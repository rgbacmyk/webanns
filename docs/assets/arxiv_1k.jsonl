{"key":"[Using Unity to Help Solve Intelligence] In the pursuit of artificial general intelligence, our most significant measurement of progress is an agent's ability to achieve goals in a wide range of environments. Existing platforms for constructing such environments are typically constrained by the technologies they are founded on, and are therefore only able to provide a subset of scenarios necessary to evaluate progress. To overcome these shortcomings, we present our use of Unity, a widely recognized and comprehensive game engine, to create more diverse, complex, virtual simulations. We describe the concepts and components developed to simplify the authoring of these environments, intended for use predominantly in the field of reinforcement learning. We also introduce a practical approach to packaging and re-distributing environments in a way that attempts to improve the robustness and reproducibility of experiment results. To illustrate the versatility of our use of Unity compared to other solutions, we highlight environments already created using our approach from published papers. We hope that others can draw inspiration from how we adapted Unity to our needs, and anticipate increasingly varied and complex environments to emerge from our approach as familiarity grows.","layer":1,"vector":[-0.0486,0.0299,0.0481,-0.0318,0.0227,0.0129,-0.0024,0.0321,0.0236,-0.0035,0.0255,-0.0362,0.0775,0.0406,-0.0069,-0.0023,-0.05,0.038,-0.0199,-0.0236,-0.0094,-0.0235,-0.0233,-0.0553,-0.0371,0.0359,-0.0419,-0.0479,-0.0355,-0.2119,0.0195,-0.0435,0.0182,-0.0324,-0.0342,0.0228,0.004,0.0725,-0.0381,0.0302,0.0375,0.0026,-0.0067,-0.0627,0.0141,-0.0468,-0.0616,-0.0174,0.002,-0.0146,0.0472,-0.0436,-0.0295,-0.0099,0.0435,0.0493,0.082,0.0823,0.0342,0.0256,-0.0083,0.0161,-0.1605,0.0758,0.0622,0.0189,-0.0441,-0.0443,0.0347,0.0514,-0.0213,0.0712,0.0387,0.0224,0.0179,0.0198,-0.0008,-0.0777,0.0399,0.0201,0.0022,-0.0404,-0.0417,-0.0063,-0.0178,-0.0268,0.02,-0.0153,0.0617,-0.009,-0.0383,0.0237,0.0026,0.0174,-0.029,0.0165,0.0124,-0.0336,-0.062,0.2363,-0.0082,-0.0188,0.0426,-0.0096,0.0751,-0.0183,-0.0239,-0.0704,-0.0097,0.0298,-0.0682,-0.022,0.0369,0.0129,0.0007,0.044,0.0368,0.0017,-0.0041,-0.0364,-0.0056,0.0295,0.0495,0.0055,0.0327,-0.0721,0.0176,0.1317,-0.0269,0.0203,0.0544,-0.0227,-0.0595,-0.0274,0.0139,0.0344,0.0067,0.0078,0.0056,0.0455,-0.0413,0.0187,-0.0013,-0.1306,-0.0231,0.1115,0.0214,0.0243,-0.0254,0.0196,-0.0299,0.0084,-0.019,-0.0216,-0.0027,-0.0095,0.0263,0.0485,-0.0448,-0.0035,-0.0256,-0.0162,-0.0147,0.0958,-0.0212,-0.0775,-0.0362,0.0147,0.0348,-0.0163,0.0209,0.034,-0.0137,0.0314,0.0684,-0.0012,-0.0856,0.0392,0.0112,0.0344,0.0296,-0.0516,-0.0345,0.0345,0.0154,-0.0632,-0.0096,-0.0242,-0.01,0.0215,-0.0487,0.0625,-0.0216,0.0237,-0.0026,-0.0033,-0.0151,-0.0287,-0.0158,-0.0348,-0.016,-0.0279,-0.0162,0.0235,-0.0102,-0.0067,-0.0025,-0.0282,0.0594,0.0169,-0.0093,-0.0222,0.0604,-0.0322,-0.0647,0.0108,0.0205,0.0064,0.0142,0.033,-0.013,-0.0134,-0.0368,-0.2292,0.0181,-0.0204,-0.0464,0.0124,-0.0214,0.0615,-0.0343,0.039,0.0266,0.0707,-0.048,-0.0162,0.0342,0.0019,-0.002,0.0023,0.0215,-0.0624,0.0479,0.0033,-0.016,-0.0115,-0.0888,0.0358,-0.0267,0.2467,0.0261,0.0548,-0.0043,0.0354,0.0439,-0.027,-0.1387,0.0596,0.0025,0.0519,-0.0052,-0.0313,-0.0427,-0.032,0.0533,-0.0078,-0.1174,-0.0275,0.0165,-0.0612,0.0413,-0.0455,-0.0136,0.0331,-0.0215,0.0312,0.0065,-0.0597,-0.0163,-0.0731,0.0389,-0.0347,0.0333,0.0023,0.006,0.0141,-0.0039,0.0807,0.0006,0.0324,-0.071,0.0951,-0.0242,0.0027,0.0281,-0.0285,-0.0055,0.0156,0.0002,0.0355,-0.0243,-0.0153,-0.0303,0.0201,-0.0303,0.0291,0.0375,0.071,-0.0595,0.0515,-0.0379,0.065,-0.004,0.0304,0.0415,-0.0609,-0.0301,0.0377,-0.0131,-0.3162,0.0811,0.0163,0.038,-0.0289,-0.0135,0.0227,0.0025,-0.06,0.0058,0.0327,0.021,0.0164,0.0094,-0.0032,0.0198,0.0648,-0.0803,0.0733,-0.0714,0.0364,0.0749,0.2458,-0.0401,0.0242,0.029,-0.0126,-0.0306,0.0165,-0.0005,-0.0129,0.0018,0.0648,-0.0831,0.0536,0.0698,-0.0377,0.0143,0.0394,0.0144,-0.048,0.0265,0.0453,0.0137,0.0554,0.0164,-0.0538,-0.0237,-0.0181,0.0126,-0.0283,-0.0103,-0.0046,-0.0367,0.0534,0.0276,-0.0441,-0.0376,-0.0499,-0.0403,-0.0153,-0.0733,0.0577,-0.0065,-0.0248]}
{"key":"[On Optimizing Interventions in Shared Autonomy] Shared autonomy refers to approaches for enabling an autonomous agent to collaborate with a human with the aim of improving human performance. However, besides improving performance, it may often also be beneficial that the agent concurrently accounts for preserving the user's experience or satisfaction of collaboration. In order to address this additional goal, we examine approaches for improving the user experience by constraining the number of interventions by the autonomous agent. We propose two model-free reinforcement learning methods that can account for both hard and soft constraints on the number of interventions. We show that not only does our method outperform the existing baseline, but also eliminates the need to manually tune a black-box hyperparameter for controlling the level of assistance. We also provide an in-depth analysis of intervention scenarios in order to further illuminate system understanding.","layer":1,"vector":[-0.0366,0.0016,0.0237,-0.0304,-0.0205,0.0424,0.0479,0.0156,0.0446,-0.0405,0.0312,-0.0353,0.0032,0.0598,0.0323,-0.0183,-0.0632,0.0693,-0.037,-0.0203,0.0034,-0.0429,-0.0112,-0.0591,-0.0279,0.0118,-0.082,-0.0972,-0.0221,-0.2177,0.049,-0.0377,0.021,0.0145,-0.0136,0.0091,-0.0423,0.0503,-0.0343,0.0202,0.0406,0.0547,-0.0125,-0.0361,-0.033,-0.0489,-0.0047,-0.0456,-0.0403,-0.027,0.029,-0.0033,0.0,0.0156,0.0382,0.0411,0.0636,0.0698,0.0386,0.031,0.002,0.0638,-0.1698,0.0915,0.0631,0.0526,-0.0596,-0.0268,0.0059,0.0271,-0.0527,0.0563,0.0446,0.0261,0.0195,0.0053,-0.0018,-0.0674,0.0407,-0.0185,-0.0069,-0.0276,-0.0252,-0.0161,0.0004,-0.0538,0.0439,-0.0359,0.0318,0.0182,-0.0078,0.0275,0.0001,-0.004,-0.0585,-0.0017,0.0133,-0.0115,-0.1146,0.2108,-0.0144,0.0029,0.0301,-0.0284,0.0325,-0.0289,-0.0144,-0.0208,-0.0065,0.0093,-0.03,-0.0204,0.0263,-0.0258,0.0369,0.0344,0.0316,0.0068,0.0338,-0.0129,-0.0022,0.0057,0.0518,0.0058,0.0469,-0.063,0.0648,0.1625,-0.0014,0.027,0.0482,-0.0182,-0.0011,-0.0316,0.0015,-0.0078,0.0077,0.0044,0.0115,-0.0038,-0.0014,-0.0136,-0.0294,-0.1395,-0.0355,0.1438,0.0261,0.0426,-0.0321,-0.0177,-0.0162,-0.0187,-0.0229,-0.0205,-0.0026,0.0261,0.0313,0.0013,-0.0837,0.0378,-0.0262,-0.0361,-0.0279,0.0896,0.0282,-0.0675,-0.034,-0.0122,-0.0006,0.0353,0.0137,0.0114,-0.0441,0.0526,0.0822,0.0153,-0.0904,0.0072,-0.0293,0.0098,0.0195,-0.0722,-0.0242,0.0135,0.0283,-0.0204,0.019,-0.0325,0.049,0.0075,-0.0106,0.023,-0.032,0.025,-0.0174,-0.0049,-0.0006,-0.0338,-0.0259,-0.0019,-0.0117,0.0132,-0.0509,0.0136,-0.0052,0.0443,-0.0243,0.0132,0.0641,-0.0051,-0.0471,0.0176,0.0508,-0.0039,-0.0343,-0.0103,-0.0069,0.0387,0.0018,0.0624,0.0488,0.0403,-0.0009,-0.2203,0.0031,-0.0079,-0.0352,0.0285,-0.0311,0.0343,-0.0005,-0.0158,0.0711,0.082,-0.0565,-0.051,0.0267,0.0103,0.0517,0.0005,0.0421,-0.0246,0.0433,0.013,-0.0106,-0.0051,-0.0581,0.0156,0.0084,0.225,-0.0118,0.004,-0.0067,0.0112,0.029,-0.0245,-0.161,0.0021,0.0503,0.0693,-0.0221,-0.0054,-0.0363,-0.0249,0.0325,-0.016,-0.1248,-0.0576,-0.0589,-0.0137,0.0105,-0.0592,-0.02,0.0206,-0.0162,-0.004,-0.0283,-0.0486,-0.0252,-0.0987,0.0253,-0.0429,0.0629,0.0113,-0.0241,-0.0071,-0.0892,0.0718,-0.0188,0.012,-0.0213,0.0478,0.0046,-0.0259,0.0578,-0.0251,0.0097,0.0189,0.0147,0.0481,-0.0243,-0.0574,-0.0223,0.0584,-0.0407,0.0296,0.0585,0.032,-0.0472,0.0682,0.0089,0.0256,-0.0451,0.0328,0.0257,-0.0369,0.0068,0.0615,-0.0339,-0.3033,0.0415,0.0088,0.0324,-0.049,0.025,0.0477,0.0261,-0.064,-0.0184,0.0181,0.0456,0.0135,0.0329,0.049,0.0377,0.0485,-0.0159,0.0167,-0.1384,0.0251,0.0641,0.2213,-0.0363,0.0167,0.0302,-0.0084,-0.0315,0.0499,-0.0348,-0.0317,0.0193,0.059,-0.0593,0.0409,0.0452,-0.0516,0.0268,0.0317,0.018,-0.0598,0.0188,0.0246,-0.0261,0.1213,-0.0092,-0.0075,-0.0395,-0.0304,0.0392,0.0157,0.0365,0.0061,0.0241,0.0283,0.007,-0.0234,-0.0597,-0.0193,-0.0421,0.0152,-0.0362,0.0123,0.0181,0.0004]}
{"key":"[Stochastic Approximate Gradient Descent via the Langevin Algorithm] We introduce a novel and efficient algorithm called the stochastic approximate gradient descent (SAGD), as an alternative to the stochastic gradient descent for cases where unbiased stochastic gradients cannot be trivially obtained. Traditional methods for such problems rely on general-purpose sampling techniques such as Markov chain Monte Carlo, which typically requires manual intervention for tuning parameters and does not work efficiently in practice. Instead, SAGD makes use of the Langevin algorithm to construct stochastic gradients that are biased in finite steps but accurate asymptotically, enabling us to theoretically establish the convergence guarantee for SAGD. Inspired by our theoretical analysis, we also provide useful guidelines for its practical implementation. Finally, we show that SAGD performs well experimentally in popular statistical and machine learning problems such as the expectation-maximization algorithm and the variational autoencoders.","layer":0,"vector":[-0.044,0.0006,0.0248,-0.0054,0.0407,0.0115,0.0125,0.0288,0.0374,0.0225,0.0378,-0.0535,0.0449,0.0949,0.0272,0.0191,0.0062,0.0209,-0.0338,-0.0261,0.0191,0.0104,0.0033,-0.0465,0.0055,0.0043,-0.0101,-0.0499,-0.0554,-0.2612,-0.0045,-0.0996,0.0335,-0.0415,0.0202,-0.0286,-0.0461,0.0465,-0.0238,0.0196,-0.043,0.0318,-0.0592,-0.0428,-0.0184,-0.0328,-0.0219,-0.0289,-0.0209,0.0024,0.0086,-0.0094,0.0327,0.017,-0.0273,0.0497,0.0404,0.0427,0.0428,0.0423,-0.0039,0.0627,-0.1447,0.0615,0.0462,0.0129,-0.0314,-0.0556,0.0149,0.054,-0.0246,0.0622,0.0045,0.0514,0.003,0.0088,0.0131,-0.0549,0.0027,0.0193,0.0312,-0.015,-0.0794,0.0036,-0.0706,-0.0367,0.0548,-0.0431,0.0727,-0.0169,-0.0003,-0.0331,-0.0492,0.0038,-0.0611,0.0217,0.0471,0.0346,-0.0204,0.214,-0.037,0.0617,0.0128,-0.0035,0.0282,-0.0285,-0.0404,-0.0107,0.008,-0.0152,0.0021,-0.0415,0.0362,-0.0747,-0.0019,-0.0085,0.0472,0.0352,-0.0201,-0.0026,-0.0201,-0.0146,0.0759,0.0128,0.0446,-0.0675,-0.0172,0.1539,0.0645,0.0023,0.0523,-0.0195,-0.0594,-0.0432,0.0195,0.0041,-0.0323,-0.0181,0.021,-0.0052,-0.0462,-0.0652,0.0058,-0.0857,-0.0632,0.1119,-0.0676,0.0392,-0.0462,-0.0568,0.0309,0.0355,0.0045,-0.0301,0.0262,0.0588,0.0324,0.0336,-0.0334,0.0254,-0.0107,-0.0774,0.0089,0.0965,0.0173,-0.0639,-0.0394,0.0169,0.0097,-0.0094,0.0766,0.0344,-0.0579,0.0397,0.0611,0.0383,-0.0734,0.007,0.0102,-0.0024,0.0069,-0.015,-0.0231,0.0536,0.0342,-0.0147,0.0302,-0.0356,0.0124,0.0323,-0.0326,0.001,-0.0149,-0.0244,-0.0222,-0.0414,-0.0329,-0.037,0.0217,-0.0005,0.0139,-0.0493,-0.05,0.0634,0.0178,0.0181,-0.0247,-0.0079,0.0581,0.0297,-0.0091,-0.0114,0.0489,-0.0034,-0.0028,0.011,0.0126,0.0088,-0.0045,0.0299,0.0451,-0.0145,-0.0724,-0.1947,-0.0184,0.0105,-0.0041,0.0471,-0.06,0.001,-0.0267,0.0645,0.0647,0.042,-0.0273,0.0242,0.0016,0.0274,0.0589,0.0234,-0.0043,0.0056,0.0461,0.0186,0.0321,-0.0122,-0.0653,0.0701,0.0017,0.1845,0.0185,0.0896,-0.0309,-0.0185,0.0347,0.0028,-0.0856,0.0538,0.0344,0.1202,-0.0153,-0.0373,-0.0276,-0.0308,0.0684,0.0253,-0.1161,-0.0661,-0.0495,-0.0501,0.0292,-0.0392,-0.0053,0.0258,-0.0187,0.0827,-0.048,0.0204,-0.0291,-0.0942,0.0316,0.0033,0.0466,0.0289,-0.08,0.0448,-0.0504,0.0297,-0.0187,-0.0437,-0.0343,-0.0038,-0.0138,-0.0132,0.077,-0.0118,0.0116,0.0515,-0.0003,0.0314,-0.0328,-0.0516,-0.0093,0.0366,-0.0203,0.0348,0.0282,0.0254,0.0095,0.0909,-0.0253,0.0089,0.0063,-0.0354,-0.0318,-0.0654,0.0241,0.0107,-0.0082,-0.3,0.0423,0.0087,0.0416,-0.0202,0.0158,0.0798,-0.0112,-0.0519,0.0069,-0.0375,0.0657,0.072,-0.0089,0.0088,0.0154,0.062,-0.0418,0.0586,-0.0846,0.0249,0.0188,0.2343,-0.0553,0.0443,0.0038,-0.0317,0.0214,0.0187,-0.0528,-0.0168,-0.0072,0.0557,-0.0475,0.0559,0.104,-0.0279,0.0384,-0.0155,-0.047,0.0076,-0.0148,-0.0114,-0.0348,0.0855,-0.0348,-0.0316,-0.0162,0.0071,0.0109,-0.0525,0.0394,0.0069,-0.0281,0.0137,0.0085,-0.0639,-0.0586,-0.0149,-0.0393,0.0132,-0.0847,-0.0606,-0.0186,-0.0138]}
{"key":"[Provable defenses against adversarial examples via the convex outer adversarial polytope] We propose a method to learn deep ReLU-based classifiers that are provably robust against norm-bounded adversarial perturbations on the training data. For previously unseen examples, the approach is guaranteed to detect all adversarial examples, though it may flag some non-adversarial examples as well. The basic idea is to consider a convex outer approximation of the set of activations reachable through a norm-bounded perturbation, and we develop a robust optimization procedure that minimizes the worst case loss over this outer region (via a linear program). Crucially, we show that the dual problem to this linear program can be represented itself as a deep network similar to the backpropagation network, leading to very efficient optimization approaches that produce guaranteed bounds on the robust loss. The end result is that by executing a few more forward and backward passes through a slightly modified version of the original network (though possibly with much larger batch sizes), we can learn a classifier that is provably robust to any norm-bounded adversarial attack. We illustrate the approach on a number of tasks to train classifiers with robust adversarial guarantees (e.g. for MNIST, we produce a convolutional classifier that provably has less than 5.8% test error for any adversarial attack with bounded $\\ell_\\infty$ norm less than $\\epsilon = 0.1$), and code for all experiments in the paper is available at https://github.com/locuslab/convex_adversarial.","layer":0,"vector":[-0.0327,-0.0414,-0.0453,-0.0158,0.0041,0.0547,0.0491,0.0124,-0.0093,-0.0363,0.0052,-0.027,0.0379,0.088,-0.0054,0.0531,0.043,0.0736,-0.0799,0.0286,0.0288,-0.0404,0.0108,-0.033,0.0437,-0.0059,0.0293,-0.0362,-0.0205,-0.2598,0.0,-0.0708,0.0044,-0.0367,-0.0041,0.0062,-0.0708,0.0406,-0.0436,0.0565,0.0105,0.0144,-0.0557,-0.0923,-0.0026,-0.0193,-0.0057,0.0037,-0.0095,-0.0495,0.0629,-0.0648,0.0517,0.046,0.0385,-0.0213,0.0473,0.0411,0.0564,0.0596,0.0049,0.0548,-0.117,0.0308,0.0257,0.0231,-0.068,-0.0182,-0.0085,0.0691,0.0028,0.0438,0.0057,0.0353,-0.0175,0.0435,0.0099,-0.0529,-0.0028,0.0063,0.0703,-0.0216,-0.0264,0.0059,-0.0024,-0.0401,0.0467,-0.01,0.0693,0.0031,-0.0216,-0.0331,-0.0124,-0.0075,-0.0537,-0.0039,0.0535,0.0214,-0.0622,0.1858,-0.0733,0.0366,0.017,-0.0178,0.0197,-0.0001,-0.0465,-0.0475,-0.0113,-0.017,-0.0375,0.0199,0.027,-0.0227,0.0258,0.0087,0.0649,-0.0061,-0.0689,-0.0131,-0.0258,-0.0206,0.0451,-0.0441,0.0555,-0.0722,0.0479,0.1388,0.0265,0.0572,0.0272,-0.0392,-0.0189,-0.0136,0.0502,0.028,-0.0317,0.037,0.0385,0.0097,-0.0542,-0.0064,0.0204,-0.0658,-0.0495,0.1014,-0.0514,0.0178,-0.0101,-0.0666,0.0058,-0.0017,-0.0088,0.0001,0.0304,0.0128,0.0225,0.0589,-0.0531,-0.0257,-0.0257,-0.0936,-0.0241,0.1189,-0.0029,-0.0815,-0.0234,-0.0076,0.0104,-0.0118,0.0416,0.0156,0.0058,0.0393,0.031,0.0189,-0.1325,-0.0305,-0.0098,0.0121,-0.0255,-0.0844,-0.0149,0.0006,0.0507,-0.0181,0.0236,-0.044,0.0449,0.0309,-0.0539,0.0168,-0.0819,-0.0393,-0.0309,-0.0462,-0.0084,-0.0209,0.0093,-0.0241,-0.0021,0.0024,-0.0394,0.0282,0.0308,0.0348,-0.0067,-0.0045,0.022,0.0362,-0.0167,-0.0328,0.0289,-0.0648,-0.0266,-0.0219,0.0095,0.069,-0.0299,0.0421,0.0272,-0.0367,-0.0686,-0.2318,-0.0299,-0.0112,-0.0662,0.0406,-0.0978,0.0442,-0.0338,0.0404,0.0119,0.0422,-0.0042,-0.0128,0.0285,-0.0132,0.0742,0.0149,0.0158,-0.0278,-0.0011,-0.0371,0.0494,-0.01,-0.0571,0.044,0.0271,0.2285,0.0519,0.0875,-0.024,0.0426,0.049,0.0028,-0.0518,0.055,-0.0108,0.0593,-0.0029,-0.0006,-0.0112,-0.024,0.0446,0.0211,-0.1383,-0.011,-0.0281,-0.0284,0.0277,-0.053,0.0418,0.0333,-0.0115,0.0691,-0.0009,0.0065,-0.0227,-0.112,0.0416,-0.0354,0.0466,0.0151,-0.0605,0.0192,-0.0811,0.0617,0.0223,-0.0096,-0.0621,0.0688,0.0071,0.0012,0.0452,0.0329,0.0258,0.0426,-0.0104,0.0274,-0.0222,-0.0553,-0.0039,0.0716,-0.0169,0.0269,-0.019,0.0455,-0.0077,0.0934,-0.016,0.0632,-0.0177,0.0008,0.0096,-0.0827,-0.0282,0.0361,0.0273,-0.2782,-0.0102,0.0384,0.0427,-0.068,0.0258,0.0351,-0.0013,-0.0625,-0.0107,0.0072,0.0665,0.0258,0.0243,-0.0101,-0.0113,0.0875,-0.023,0.0259,-0.0568,0.0106,0.0337,0.2104,-0.0481,-0.0287,-0.0032,-0.0057,0.0081,-0.0198,-0.0417,0.0124,-0.002,0.0794,-0.0258,0.0147,0.071,-0.0332,-0.0025,0.0173,-0.0026,-0.0209,0.0118,-0.0504,0.037,0.0897,-0.0116,-0.0,-0.001,0.0341,0.0378,-0.0106,-0.0014,0.0243,0.0035,0.039,0.0155,-0.0718,-0.0641,-0.0477,-0.0016,0.0555,-0.0488,-0.0052,0.0131,-0.0432]}
{"key":"[Learning Latent Trees with Stochastic Perturbations and Differentiable Dynamic Programming] We treat projective dependency trees as latent variables in our probabilistic model and induce them in such a way as to be beneficial for a downstream task, without relying on any direct tree supervision. Our approach relies on Gumbel perturbations and differentiable dynamic programming. Unlike previous approaches to latent tree learning, we stochastically sample global structures and our parser is fully differentiable. We illustrate its effectiveness on sentiment analysis and natural language inference tasks. We also study its properties on a synthetic structure induction task. Ablation studies emphasize the importance of both stochasticity and constraining latent structures to be projective trees.","layer":1,"vector":[-0.0262,-0.0142,-0.0082,-0.0128,0.0341,0.0005,0.0335,0.013,0.0354,-0.029,0.0076,-0.0186,0.0609,0.091,0.0351,0.0466,0.0029,0.0184,-0.0347,0.0203,0.0557,-0.0513,0.0206,-0.0108,0.0429,0.0099,-0.0419,-0.0292,-0.0172,-0.2339,0.0137,-0.0532,0.0181,-0.0332,0.0177,-0.0279,-0.0431,0.0459,-0.0245,0.0555,0.007,0.002,-0.017,-0.0561,0.0066,-0.0516,0.0037,-0.007,-0.0351,0.007,-0.0396,-0.0851,0.0583,-0.0169,0.0254,0.0505,0.0361,0.0261,0.0226,0.0395,0.0075,0.0729,-0.1496,0.0769,0.058,0.0292,-0.0888,-0.0008,0.0374,0.0894,-0.0327,0.0254,0.0088,0.0464,0.0185,0.0194,0.017,-0.0218,-0.0083,0.0296,0.0168,-0.013,-0.0774,-0.0012,-0.0279,-0.0623,0.0096,-0.0208,0.007,0.041,-0.0176,-0.0213,-0.0156,0.0371,-0.0684,-0.0445,0.0599,0.0024,-0.0239,0.2178,-0.0589,0.0421,0.0614,-0.0483,0.0095,-0.0192,-0.0544,-0.0364,-0.0105,-0.0036,-0.0262,-0.065,0.0106,-0.0894,0.0223,-0.0408,0.0509,0.0541,-0.0296,-0.0183,-0.0221,-0.0061,0.0137,-0.0081,0.0313,-0.0872,-0.0267,0.1186,0.0599,0.0717,0.0492,-0.0166,-0.0273,-0.0112,0.0299,0.0142,0.024,-0.0186,0.0262,-0.0276,-0.0441,-0.0255,-0.01,-0.0608,-0.0916,0.1279,-0.0156,0.0524,-0.0435,0.0031,-0.0309,0.0324,-0.0348,-0.049,0.0357,0.0598,0.0299,-0.0143,-0.0197,-0.0039,-0.003,-0.0363,-0.0082,0.0933,0.0061,-0.0813,-0.055,-0.0353,0.0212,-0.0059,0.1046,0.0523,-0.0266,0.0358,0.0155,0.0195,-0.0574,0.0377,0.0402,0.0469,0.0233,-0.0348,-0.011,0.0578,0.0227,-0.0697,0.0205,-0.0581,0.0488,0.0063,-0.0201,0.0146,-0.0359,0.0114,-0.0427,-0.0323,-0.0271,0.0013,0.0195,-0.0237,0.0002,0.0285,-0.0565,0.0165,-0.0523,0.0138,-0.0077,0.0247,0.0028,0.0336,-0.012,0.0154,0.0711,-0.0513,-0.019,-0.0048,0.0041,0.0198,0.0333,0.0281,0.0119,-0.0493,-0.0184,-0.242,0.014,0.0016,-0.0102,0.0653,-0.0657,0.0026,-0.0382,0.0268,0.0573,0.0122,-0.0385,-0.006,0.0128,0.047,0.0547,0.0152,-0.0171,0.0122,-0.0127,-0.0216,0.0121,-0.0185,-0.0917,0.0239,0.0138,0.2318,0.0318,0.0069,-0.0189,0.0373,0.0317,-0.0463,-0.1047,0.083,0.0074,0.0587,-0.0449,-0.014,0.0279,0.0249,0.032,0.0026,-0.1254,-0.0621,-0.061,-0.0094,0.0015,-0.0417,0.0308,0.0371,-0.018,0.0401,0.0168,-0.0228,-0.0686,-0.1234,0.021,-0.0336,0.0326,0.013,-0.0528,0.0172,-0.0502,0.0176,-0.0138,-0.0717,-0.0523,0.0189,0.0015,-0.0123,0.0442,-0.0164,-0.0236,0.0371,0.0217,0.0274,-0.0164,-0.0489,-0.0302,0.068,-0.0409,0.059,0.0389,0.042,0.0034,0.0828,-0.0181,0.0619,0.0402,0.0058,0.0133,-0.0366,0.0169,0.0611,-0.0196,-0.2876,0.0261,0.0277,-0.0046,0.0149,-0.0023,0.0449,0.0053,-0.0348,0.0005,-0.0206,0.0601,0.0381,-0.0008,-0.0284,0.0076,0.0792,-0.0359,0.0528,-0.0579,0.0113,0.0338,0.2204,-0.0052,0.0654,-0.0297,-0.0294,-0.0141,0.0307,-0.0147,0.0266,0.0173,0.0973,-0.0656,0.0426,0.0401,-0.0147,0.0666,-0.0169,-0.038,-0.0182,0.0079,-0.0523,-0.0445,0.1161,-0.0202,0.0114,-0.0421,0.0181,0.04,0.0142,0.0269,-0.0275,0.0105,-0.0148,0.0148,-0.0017,-0.065,-0.0324,-0.0573,-0.0631,-0.0621,-0.0151,0.0187,0.013]}
{"key":"[On the Consistency of the Bootstrap Approach for Support Vector Machines and Related Kernel Based Methods] It is shown that bootstrap approximations of support vector machines (SVMs) based on a general convex and smooth loss function and on a general kernel are consistent. This result is useful to approximate the unknown finite sample distribution of SVMs by the bootstrap approach.","layer":0,"vector":[-0.0241,-0.0049,0.0265,-0.0599,0.0017,0.0521,0.0049,0.0636,-0.0004,-0.0131,0.0402,-0.066,0.0381,0.0003,0.0223,0.0153,0.0611,0.0692,-0.0279,-0.036,-0.0094,-0.0341,-0.0113,-0.0497,-0.0136,0.0474,-0.0293,-0.0325,-0.0489,-0.2008,0.0402,-0.0708,0.0802,-0.0399,0.0079,0.0014,-0.0404,0.0516,-0.0484,0.0304,0.0237,-0.0263,-0.0822,-0.034,-0.039,-0.059,-0.005,-0.0472,-0.0215,-0.0064,0.0376,0.0119,0.062,-0.0275,0.0004,0.0484,0.0636,0.0193,0.0398,0.0601,-0.0014,0.0571,-0.1899,0.0758,0.041,0.0433,-0.0195,-0.0184,0.0223,0.0542,-0.0192,0.0412,0.0063,0.0513,-0.0399,-0.0394,0.01,-0.0396,-0.0352,0.041,0.0143,0.0127,0.0065,-0.0262,-0.0422,0.0296,0.0229,-0.0508,0.0539,0.0279,-0.0252,-0.0074,-0.0166,0.0343,-0.0803,-0.0573,0.0597,0.0228,-0.0734,0.1976,-0.0212,0.0461,0.0647,-0.0448,0.0072,-0.022,-0.0427,-0.0573,-0.0368,-0.0071,-0.0201,0.0028,0.025,-0.0379,0.0009,-0.0091,0.0626,0.0253,0.0119,-0.0325,-0.0197,0.025,0.0425,-0.006,0.0308,-0.0536,0.0192,0.1066,0.0276,0.0523,0.0576,-0.0218,-0.0863,-0.0478,0.0355,0.0285,0.0276,0.0343,-0.0137,-0.0008,-0.0235,-0.0913,-0.0011,-0.0461,-0.0158,0.137,-0.0555,-0.0144,-0.0433,-0.0431,0.0029,0.0251,-0.0192,-0.0512,0.0182,0.0297,0.0456,0.0198,-0.0439,-0.0076,-0.0116,-0.0727,-0.031,0.1005,0.0023,-0.074,-0.0342,0.0349,0.0347,-0.0018,0.0635,0.0207,-0.014,0.0233,0.0513,0.0243,-0.069,0.0131,0.033,-0.0136,0.0248,-0.0201,-0.0276,0.0366,0.0322,-0.0433,-0.0014,-0.0716,0.0431,0.0253,-0.0204,-0.033,0.0003,-0.0351,-0.025,0.0085,-0.01,-0.0426,0.0831,-0.0103,0.0138,0.0179,-0.0378,0.0237,-0.0438,0.0447,-0.0238,-0.0019,-0.0036,-0.0067,-0.0086,0.0115,0.0474,-0.0121,-0.0457,0.0382,0.0209,0.0614,-0.0025,0.0649,0.0593,-0.0408,-0.0604,-0.2123,-0.0385,-0.0023,0.0029,0.0278,-0.0882,0.0446,0.0105,-0.0107,0.0459,0.0764,0.0289,-0.0264,0.0095,0.002,0.065,0.0428,0.0476,-0.0239,0.0142,-0.0292,0.0159,-0.0155,-0.063,0.0472,-0.015,0.1715,0.0004,0.0232,-0.0516,0.0483,0.0306,0.0114,-0.0998,0.0953,0.0209,0.0505,-0.0012,0.0126,-0.0358,0.0072,0.035,0.0106,-0.0843,-0.0644,-0.0189,-0.0343,0.0045,-0.0755,0.0195,0.0641,-0.0038,0.074,-0.0029,0.0471,-0.0307,-0.1191,0.0304,-0.0134,0.0085,0.007,-0.0716,0.0468,-0.0705,0.0935,0.0097,0.0125,-0.0046,0.0085,-0.0101,-0.0361,0.1198,0.037,-0.0098,0.0689,-0.0291,0.0042,0.0061,-0.0552,-0.0148,0.0397,-0.0145,0.0207,0.0216,0.0275,0.0302,0.064,-0.0182,-0.0094,0.0031,0.0135,0.0059,-0.0432,-0.0177,0.0024,-0.0218,-0.2863,0.009,-0.0126,0.0249,-0.039,-0.0232,0.0001,-0.0398,-0.0344,0.0071,0.0061,0.0945,0.049,-0.0147,0.0105,0.0435,0.0226,-0.0757,0.0526,-0.0768,0.0425,0.0065,0.2388,-0.0741,-0.0371,-0.0105,0.015,-0.0161,0.0024,-0.0048,0.0277,-0.0505,0.0916,-0.0001,0.0226,0.0884,-0.0363,0.0428,-0.005,-0.0673,0.0587,0.0117,-0.0799,-0.0213,0.105,-0.0637,-0.0091,-0.0375,0.0454,0.0043,-0.0705,0.0185,-0.0128,0.0178,0.0307,0.036,-0.0596,-0.0464,-0.0299,-0.0583,0.0548,-0.0742,-0.0322,-0.0189,0.0004]}
{"key":"[Building Spatio-temporal Transformers for Egocentric 3D Pose Estimation] Egocentric 3D human pose estimation (HPE) from images is challenging due to severe self-occlusions and strong distortion introduced by the fish-eye view from the head mounted camera. Although existing works use intermediate heatmap-based representations to counter distortion with some success, addressing self-occlusion remains an open problem. In this work, we leverage information from past frames to guide our self-attention-based 3D HPE estimation procedure -- Ego-STAN. Specifically, we build a spatio-temporal Transformer model that attends to semantically rich convolutional neural network-based feature maps. We also propose feature map tokens: a new set of learnable parameters to attend to these feature maps. Finally, we demonstrate Ego-STAN's superior performance on the xR-EgoPose dataset where it achieves a 30.6% improvement on the overall mean per-joint position error, while leading to a 22% drop in parameters compared to the state-of-the-art.","layer":1,"vector":[-0.0083,-0.018,0.0861,-0.0785,0.0199,0.0483,0.0392,0.0126,-0.0187,-0.014,0.0336,-0.098,0.0292,0.0553,0.0153,0.0338,-0.0234,0.0706,-0.0498,-0.0059,0.0389,-0.0549,-0.0324,-0.035,0.008,-0.0051,-0.0247,-0.0437,-0.0228,-0.2292,-0.0014,-0.0515,0.0312,-0.0563,-0.0415,-0.0624,-0.0379,0.0662,0.0077,0.0144,-0.0188,0.0165,-0.0312,-0.0351,-0.0388,-0.0161,-0.0047,-0.0129,-0.0053,-0.0257,0.0506,-0.0313,0.0256,0.0341,0.0477,0.0279,0.0435,0.0375,0.0315,-0.021,0.0315,0.0343,-0.1488,0.0513,0.0398,0.0474,-0.0199,-0.0491,-0.0019,-0.0156,-0.0169,0.0458,0.028,0.0322,0.0182,-0.0087,0.0042,-0.0081,-0.0063,-0.0469,0.0393,0.0103,-0.0333,-0.0354,0.0137,-0.0436,0.0476,-0.0205,0.0027,0.0004,-0.0873,-0.0321,-0.0615,0.0208,-0.0414,0.0098,0.0256,0.0318,-0.0367,0.2338,-0.0349,0.0809,0.0633,-0.0297,0.0602,-0.0308,-0.0178,-0.0161,-0.0306,0.0244,-0.0242,0.0096,-0.0124,-0.0337,0.0732,-0.0022,0.0496,0.0695,0.0083,-0.0331,0.001,0.0264,0.0496,-0.0122,0.0019,-0.052,0.0162,0.1518,0.0759,0.0167,0.051,0.006,-0.0649,-0.0325,-0.0335,0.0643,0.0135,0.0062,0.0308,0.0045,-0.0006,-0.0537,-0.0162,-0.048,0.0028,0.1255,-0.0575,-0.0065,0.0029,-0.0323,-0.0025,0.029,-0.0371,0.0397,0.0549,0.0145,0.0166,0.0499,-0.0729,-0.0171,-0.0385,0.0111,-0.047,0.0782,0.0344,-0.1193,-0.0237,0.014,-0.0093,0.0189,0.0425,0.0144,-0.0379,0.0169,0.1251,0.0489,-0.1144,0.0207,-0.0132,0.0707,-0.0015,-0.0664,-0.008,0.0072,0.0385,-0.0529,0.0268,-0.0127,0.014,0.0843,-0.0015,0.0519,-0.0628,-0.0068,-0.0297,-0.0179,0.0058,0.0184,0.0293,-0.0215,0.0081,0.0252,0.0146,-0.0069,0.0331,0.0246,-0.0057,0.0206,0.0325,0.0366,-0.038,0.0222,0.0712,-0.0195,0.0033,-0.0315,0.0164,0.0011,-0.0652,0.0308,0.0011,-0.0318,-0.045,-0.2158,0.0178,-0.0083,-0.0193,0.0022,-0.0645,-0.0143,0.0182,0.0789,0.0222,0.0548,-0.0507,-0.0097,0.0551,0.0013,0.0641,0.0194,0.0568,-0.0566,-0.0181,-0.0176,0.0166,-0.0231,-0.0577,0.0552,-0.0095,0.2361,0.055,0.0094,0.004,0.0099,0.0223,-0.0633,-0.1381,0.0285,0.0376,0.0714,-0.0209,-0.0339,0.008,-0.0788,0.0003,0.0229,-0.0972,-0.0115,-0.0018,-0.0453,0.057,-0.0275,-0.0001,0.0454,-0.0915,0.0131,-0.0466,-0.0157,-0.0382,-0.0769,0.015,-0.0452,0.0436,-0.0069,-0.0341,0.0074,-0.0695,0.0458,-0.0032,-0.0081,-0.0759,0.0109,-0.0139,-0.0365,0.0889,0.0291,0.0034,0.0665,0.0132,0.1061,-0.0021,-0.0025,-0.0444,0.0194,-0.0447,0.0191,0.023,0.0724,-0.0132,0.059,-0.0078,0.015,-0.0469,-0.0034,0.0207,-0.0531,-0.0306,0.0631,-0.0043,-0.3168,-0.0031,-0.0184,0.0452,-0.0071,0.0249,0.0191,-0.0146,-0.0219,-0.0255,-0.0206,0.0212,0.0246,-0.021,-0.0049,0.0244,0.0701,-0.0379,0.0451,-0.058,0.0169,0.0299,0.1855,-0.0229,-0.0082,0.0169,-0.0776,-0.0304,0.0196,-0.0118,-0.0154,0.0011,0.0295,-0.0455,0.0177,0.0662,-0.026,0.0083,0.0246,0.0098,0.0105,-0.006,-0.0112,-0.0224,0.0885,0.0184,-0.0305,-0.0043,-0.0119,-0.0003,-0.0337,-0.0078,-0.0189,0.0228,0.0397,0.0363,-0.0481,-0.04,-0.063,0.0139,0.047,-0.0722,-0.0234,0.0069,0.008]}
{"key":"[Unifying Probabilistic Models for Time-Frequency Analysis] In audio signal processing, probabilistic time-frequency models have many benefits over their non-probabilistic counterparts. They adapt to the incoming signal, quantify uncertainty, and measure correlation between the signal's amplitude and phase information, making time domain resynthesis straightforward. However, these models are still not widely used since they come at a high computational cost, and because they are formulated in such a way that it can be difficult to interpret all the modelling assumptions. By showing their equivalence to Spectral Mixture Gaussian processes, we illuminate the underlying model assumptions and provide a general framework for constructing more complex models that better approximate real-world signals. Our interpretation makes it intuitive to inspect, compare, and alter the models since all prior knowledge is encoded in the Gaussian process kernel functions. We utilise a state space representation to perform efficient inference via Kalman smoothing, and we demonstrate how our interpretation allows for efficient parameter learning in the frequency domain.","layer":0,"vector":[-0.0748,-0.0401,0.0233,-0.0342,0.0269,0.0098,0.0366,0.0168,0.0876,-0.0157,0.0648,-0.0765,0.0155,0.027,0.0364,0.0405,0.01,0.0339,-0.0291,-0.0179,0.0654,0.0022,-0.019,0.0063,0.0142,0.0029,0.0025,-0.0368,-0.0388,-0.2593,0.0255,-0.0443,0.0613,-0.0231,-0.0081,-0.048,-0.0409,0.0602,-0.0189,0.0632,0.0491,0.0239,-0.0342,-0.0701,-0.0434,-0.0703,-0.0415,-0.0396,-0.0444,-0.0022,0.02,-0.0041,0.0355,0.0096,0.0356,0.0174,0.11,0.0356,0.0762,0.0657,-0.0045,0.0283,-0.2011,0.0842,0.0426,0.0597,0.013,-0.0385,0.0047,0.0189,-0.0371,0.0503,-0.0017,0.0071,-0.0228,-0.0346,-0.0073,-0.0525,-0.0319,0.0387,0.0183,-0.0209,-0.038,-0.0166,-0.0448,-0.0342,0.0037,-0.0457,0.0277,-0.0003,-0.0986,-0.0002,-0.0578,0.0319,-0.0526,-0.0052,0.0189,0.0313,0.0249,0.1719,-0.0132,0.0244,0.0369,-0.0191,0.0074,-0.0587,-0.0273,-0.0104,-0.0149,0.0087,-0.001,-0.0009,0.0069,-0.0627,0.0469,0.0213,0.0356,0.0055,0.0376,-0.0348,-0.0197,-0.0073,0.0638,-0.0253,0.0374,-0.0355,0.0454,0.1447,0.0447,0.0484,0.0234,-0.0355,-0.0279,0.0092,0.0037,0.0378,0.0355,0.0069,0.0316,0.0041,-0.0179,-0.0555,-0.0188,-0.0752,-0.0621,0.1261,-0.0782,0.0394,-0.0969,0.0463,-0.0412,0.0103,0.0221,-0.0481,0.0453,0.0268,0.0266,-0.0129,-0.0186,0.0475,-0.0471,-0.0602,0.028,0.1011,0.0164,-0.05,-0.0511,-0.0065,-0.0056,0.011,0.0263,0.0143,-0.0119,0.0106,0.0808,0.0324,-0.0543,0.0304,0.0355,0.0016,0.0122,-0.0326,-0.0001,0.0318,0.0675,-0.0725,0.0039,-0.0562,-0.0016,0.0145,-0.0066,-0.0769,-0.0052,0.0185,-0.0216,-0.0333,-0.0236,-0.0135,0.0402,-0.0192,-0.0165,-0.002,-0.0647,0.0002,-0.0082,0.0461,-0.0292,0.0346,0.051,0.0339,0.0138,-0.0171,0.0592,-0.0323,0.01,-0.0042,-0.0185,0.0523,0.0291,0.0457,0.0286,-0.0535,-0.077,-0.2006,0.0236,0.0228,0.0595,0.0579,-0.0703,0.0054,-0.0009,0.0874,0.0548,0.0277,-0.0007,-0.0208,0.0137,-0.0123,0.0119,0.016,0.023,-0.0209,0.0053,-0.0212,0.0294,-0.1039,-0.0605,0.0695,-0.0223,0.1858,-0.0273,0.0566,-0.0427,0.0016,0.0259,-0.0052,-0.0575,0.0224,0.053,0.11,0.0168,0.0026,-0.02,-0.0646,-0.0253,-0.0006,-0.068,-0.0948,-0.0274,-0.0047,-0.0113,-0.0489,-0.0036,0.0704,0.0146,0.0419,-0.0371,-0.0088,-0.0555,-0.0526,0.0237,-0.0441,0.032,0.0184,-0.023,0.0284,-0.0682,0.0331,-0.0221,-0.0317,-0.0041,0.0157,-0.0227,0.0256,0.1089,-0.0244,0.0054,0.0565,-0.017,0.0298,-0.0462,-0.0428,-0.0078,0.0612,-0.036,0.0344,0.0329,0.0253,-0.0196,0.0945,0.0056,-0.0132,-0.0246,-0.01,-0.0123,-0.0179,-0.01,0.0404,-0.0213,-0.2938,-0.0076,0.0129,0.0085,-0.0388,-0.0019,0.0045,-0.0093,-0.1068,0.0121,-0.0441,0.0453,0.0037,-0.0138,0.0296,0.0668,0.0988,-0.052,0.0341,-0.042,0.0133,0.0521,0.2018,-0.0117,0.0292,-0.0228,-0.0108,0.0371,0.0603,-0.0179,0.0109,0.0154,0.1137,-0.0525,0.0248,0.0456,-0.0272,0.0716,-0.0307,-0.0321,0.0408,-0.0097,-0.0097,-0.0409,0.1228,-0.0315,-0.042,-0.0512,-0.011,0.0354,-0.0539,-0.0207,0.0014,0.0043,-0.01,0.0541,-0.0254,-0.0335,0.0153,-0.0242,0.0104,-0.0744,-0.0106,-0.0215,-0.0496]}
{"key":"[Have I done enough planning or should I plan more?] People's decisions about how to allocate their limited computational resources are essential to human intelligence. An important component of this metacognitive ability is deciding whether to continue thinking about what to do and move on to the next decision. Here, we show that people acquire this ability through learning and reverse-engineer the underlying learning mechanisms. Using a process-tracing paradigm that externalises human planning, we find that people quickly adapt how much planning they perform to the cost and benefit of planning. To discover the underlying metacognitive learning mechanisms we augmented a set of reinforcement learning models with metacognitive features and performed Bayesian model selection. Our results suggest that the metacognitive ability to adjust the amount of planning might be learned through a policy-gradient mechanism that is guided by metacognitive pseudo-rewards that communicate the value of planning.","layer":2,"vector":[-0.0468,-0.0334,0.0247,-0.0222,0.0125,0.0248,0.0665,0.0341,0.0349,0.0267,0.0173,-0.0305,0.032,0.0239,0.0388,-0.005,-0.0575,0.0224,-0.0414,-0.0124,0.0236,-0.039,-0.0496,-0.0123,0.007,0.0208,-0.041,-0.0214,-0.0765,-0.1855,0.0453,-0.0554,0.0199,-0.0248,-0.0184,-0.0131,-0.0471,0.0685,0.009,0.056,-0.0017,0.0157,-0.0422,-0.0529,-0.059,-0.032,-0.0121,-0.0442,0.0072,-0.0283,-0.0031,-0.013,-0.0349,-0.0069,0.0636,0.0404,0.0498,0.0622,0.064,0.0194,0.0082,0.0309,-0.1951,0.0785,0.0007,0.0737,-0.0459,-0.0235,0.024,0.0481,-0.0394,0.0391,0.0192,0.0467,0.0451,0.0054,-0.0618,-0.014,0.0418,0.0161,0.0257,0.0009,-0.0413,-0.0015,0.0048,-0.0355,-0.0284,-0.0288,0.074,0.0044,0.0178,-0.0095,-0.0542,-0.0015,-0.0531,-0.0128,0.0534,0.0389,-0.0784,0.1959,0.0036,0.0294,0.012,0.0034,0.0327,-0.0667,-0.0781,0.0049,-0.0076,-0.0201,-0.0233,-0.0006,0.0503,-0.0051,0.0248,0.0448,0.0355,0.0233,-0.0141,-0.0032,-0.0025,0.0027,0.0537,-0.0238,0.0052,-0.0969,0.0309,0.1415,-0.0063,-0.0256,0.0913,-0.0622,-0.0328,0.0238,0.0311,0.0364,0.0071,-0.0317,-0.006,0.0107,-0.0353,0.03,-0.0361,-0.1167,-0.0886,0.1389,0.0157,0.0255,0.0263,0.0085,-0.0102,0.0297,0.0043,-0.0353,-0.0154,0.0062,0.0281,0.0751,-0.0399,0.0214,0.0112,-0.0336,-0.065,0.0776,-0.0054,-0.0776,-0.0492,-0.0445,0.0262,-0.0154,0.0038,0.0217,-0.029,-0.0028,0.1386,0.0242,-0.0825,-0.008,-0.0167,0.0085,0.074,-0.0628,-0.0103,0.0172,0.0672,-0.043,0.0167,-0.0615,0.0206,0.0436,0.0014,0.0787,-0.0124,-0.0305,-0.0375,-0.0145,0.0068,-0.0045,-0.0477,-0.0268,-0.0254,0.0047,-0.0695,0.0092,0.0085,0.0225,-0.0255,-0.0032,0.0998,-0.0058,-0.0519,0.0075,0.0769,-0.0002,-0.038,0.042,0.0007,0.0573,0.0156,0.0274,0.0385,-0.0431,-0.031,-0.1976,-0.0156,-0.0135,0.0021,0.0076,-0.0462,0.0271,-0.0337,-0.0236,0.0541,0.0563,-0.0825,-0.0192,0.0137,-0.0128,0.043,0.038,0.0183,-0.0331,0.0351,-0.0304,0.0145,-0.0221,-0.1386,0.0286,-0.0077,0.2585,-0.0006,0.0177,-0.0132,0.0537,0.0344,-0.0408,-0.1377,0.026,0.0425,0.0695,-0.0654,-0.0228,-0.0478,-0.0006,-0.0089,-0.0337,-0.1072,-0.0838,-0.0089,-0.0228,0.0379,-0.0416,-0.0039,0.0443,-0.0088,0.0202,-0.0171,-0.0411,-0.0164,-0.0835,0.0069,-0.0424,0.0438,0.03,-0.0279,0.0472,-0.0141,0.0624,0.0104,0.0013,-0.0471,0.0175,-0.0266,0.0064,0.0825,0.0415,-0.0355,0.0141,-0.0362,-0.005,-0.0218,-0.0475,-0.0039,0.0511,-0.0404,0.0338,0.0226,0.0429,-0.0187,0.0465,-0.0452,0.0296,-0.0205,-0.013,0.0165,-0.0457,0.0045,0.0594,-0.0295,-0.2654,0.0959,0.003,-0.0001,0.0088,0.0294,0.0682,0.0384,-0.0071,0.0297,0.0373,0.0188,0.0245,-0.0056,0.0313,0.0085,0.0733,-0.0409,0.0318,-0.0641,-0.0022,0.0623,0.2201,-0.0312,0.039,-0.0113,-0.0313,0.004,0.0436,-0.0219,-0.0046,0.0085,0.0323,-0.051,0.0441,0.064,-0.0316,0.0457,0.0297,0.0023,-0.0122,-0.0142,-0.0196,-0.0177,0.1315,0.0103,-0.0148,-0.0678,-0.023,0.0077,-0.0275,0.0376,-0.0275,-0.0542,0.0148,0.0021,-0.0074,-0.038,-0.0548,-0.0344,0.0184,-0.0187,0.0702,0.0267,0.0128]}
{"key":"[Manifold optimization for non-linear optimal transport problems] Optimal transport (OT) has recently found widespread interest in machine learning. It allows to define novel distances between probability measures, which have shown promise in several applications. In this work, we discuss how to computationally approach general non-linear OT problems within the framework of Riemannian manifold optimization. The basis of this is the manifold of doubly stochastic matrices (and their generalization). Even though the manifold geometry is not new, surprisingly, its usefulness for solving general non-linear OT problems has not been popular. To this end, we specifically discuss optimization-related ingredients that allow modeling the OT problem on smooth Riemannian manifolds by exploiting the geometry of the search space. We also discuss extensions where we reuse the developed optimization ingredients. We make available the Manifold optimization-based Optimal Transport, or MOT, repository with codes useful in solving OT problems in Python and Matlab. The codes are available at \\url{https://github.com/SatyadevNtv/MOT}.","layer":1,"vector":[-0.0232,-0.0718,0.0259,0.0454,-0.0056,0.0113,0.0039,0.0748,0.0217,-0.0241,0.0327,-0.077,-0.0036,0.0596,0.0028,0.0447,0.0279,0.0513,-0.0468,0.0392,0.0449,-0.0538,-0.0252,-0.0619,0.0282,-0.0018,-0.0095,-0.0319,-0.0499,-0.2605,0.0296,-0.051,0.0182,-0.0423,-0.021,-0.0368,0.0157,0.0544,-0.0192,0.048,0.023,0.0357,-0.0387,-0.0213,-0.0566,-0.0528,-0.0111,-0.022,-0.0224,-0.0289,-0.0058,-0.0476,0.0054,0.0387,0.0174,0.02,0.0566,0.0114,0.0362,0.0611,-0.0114,0.0119,-0.1634,0.0756,0.0698,-0.0148,-0.0618,0.0046,0.018,0.0924,-0.0276,0.012,-0.0237,0.0257,0.0501,0.0003,0.0218,-0.0065,-0.0281,0.0227,0.012,0.0035,-0.0512,0.0092,-0.0248,-0.0243,0.0192,-0.0619,0.0149,-0.0045,-0.0283,-0.0155,-0.0311,0.0356,-0.0689,-0.004,0.0443,0.072,-0.0281,0.2069,-0.0479,0.0565,0.0221,0.02,0.0137,-0.0374,-0.0446,-0.028,-0.0042,-0.0202,0.0377,-0.0152,0.027,-0.0314,-0.0056,0.0085,0.0283,0.0771,-0.0617,0.0085,-0.0718,-0.0013,0.0827,-0.0369,0.0223,-0.038,-0.0122,0.1223,0.0202,0.0377,0.046,0.0156,-0.0175,-0.0274,0.0189,-0.0184,0.0093,0.0141,-0.0025,0.0228,-0.0303,-0.0765,0.0073,-0.1226,-0.0371,0.1353,-0.0469,0.0047,-0.0192,-0.0309,0.0122,0.0174,-0.0298,-0.0411,0.006,0.0186,-0.0056,0.0233,-0.0559,0.0197,-0.0572,-0.0584,-0.0136,0.124,-0.051,-0.0729,-0.0062,-0.0191,0.0127,-0.036,0.0337,0.0304,-0.0403,0.0423,0.0606,0.0336,-0.074,-0.0074,-0.0355,-0.001,0.022,-0.0035,-0.049,-0.0412,0.055,0.0015,0.0087,-0.0576,0.0126,0.0316,-0.0118,0.0091,0.0046,-0.0296,-0.0476,-0.0405,0.001,-0.008,0.0276,0.0179,0.039,-0.0071,-0.0382,0.0379,-0.0221,0.0129,-0.0043,-0.021,-0.0078,0.05,-0.0026,-0.0634,0.0442,-0.0224,-0.0605,-0.0078,-0.0058,0.0364,0.0229,0.0296,0.0262,-0.0152,-0.0619,-0.2172,0.0095,-0.0173,0.024,0.069,-0.0486,0.0511,-0.0333,0.0603,0.0975,0.0988,-0.0251,-0.0246,0.0326,0.0167,0.0341,-0.0221,0.0526,-0.0317,-0.0219,0.0149,0.017,-0.0115,-0.0837,0.0483,-0.0525,0.2348,0.0094,0.048,-0.0195,0.0272,-0.003,0.0215,-0.0669,0.0628,-0.0019,0.0916,-0.0195,-0.0247,-0.022,0.0108,0.028,0.0297,-0.0478,-0.0121,-0.056,-0.0396,-0.0153,-0.0588,0.0339,0.0637,-0.003,0.0634,-0.0369,-0.002,-0.0185,-0.0528,-0.0127,-0.0287,0.0597,-0.0389,-0.0704,0.0147,-0.0503,0.0576,-0.0221,-0.0094,-0.0328,-0.0014,-0.0221,-0.0141,0.0939,-0.0257,0.0063,0.0755,0.0157,0.0447,0.0554,-0.0139,-0.0455,0.0516,-0.0301,0.0409,0.0244,0.0669,0.0017,0.0882,-0.0413,-0.0286,-0.0083,0.0183,-0.0199,-0.0548,0.0199,0.0324,0.0191,-0.2821,0.0396,0.0319,0.0159,-0.0376,-0.0171,0.0375,-0.0009,-0.0375,-0.0418,0.0449,0.0342,0.0144,0.0303,0.0531,0.0321,0.082,-0.0489,0.0252,-0.0671,-0.0086,0.03,0.2283,-0.0454,0.0502,0.025,-0.0034,0.0417,0.0402,-0.0591,0.0009,0.0134,0.053,-0.0828,0.0555,0.0662,-0.0409,0.0186,-0.0028,-0.0085,0.0395,-0.024,-0.0099,-0.0073,0.0953,0.001,-0.0228,-0.0632,0.0138,0.0167,-0.0004,0.0164,0.0119,0.0021,0.0229,0.0149,-0.0495,-0.0989,-0.0098,-0.0283,0.0196,-0.0951,-0.0258,-0.0355,0.0099]}
{"key":"[Query Complexity of Derivative-Free Optimization] This paper provides lower bounds on the convergence rate of Derivative Free Optimization (DFO) with noisy function evaluations, exposing a fundamental and unavoidable gap between the performance of algorithms with access to gradients and those with access to only function evaluations. However, there are situations in which DFO is unavoidable, and for such situations we propose a new DFO algorithm that is proved to be near optimal for the class of strongly convex objective functions. A distinctive feature of the algorithm is that it uses only Boolean-valued function comparisons, rather than function evaluations. This makes the algorithm useful in an even wider range of applications, such as optimization based on paired comparisons from human subjects, for example. We also show that regardless of whether DFO is based on noisy function evaluations or Boolean-valued function comparisons, the convergence rate is the same.","layer":1,"vector":[-0.0563,0.0066,0.034,0.0352,-0.0148,0.0111,0.036,0.0471,0.0519,-0.029,0.0087,-0.0369,0.0478,0.0522,0.0204,0.0238,0.0342,-0.0031,-0.0326,0.0319,0.0623,-0.0421,-0.0239,-0.1038,0.0305,-0.0171,-0.0487,-0.0232,-0.0189,-0.2612,0.0107,-0.0575,0.0161,-0.0519,0.0256,-0.0389,-0.0104,0.0418,-0.0474,0.0219,0.0086,0.0494,-0.0338,-0.0321,-0.0036,-0.056,-0.0037,0.0152,-0.0535,-0.0026,0.007,-0.0628,-0.0024,-0.0013,0.0366,0.0489,0.043,0.0208,0.0302,0.0691,0.0134,0.0598,-0.1415,0.0656,0.0369,0.0093,-0.0014,-0.0538,0.0075,0.0565,0.009,0.046,0.0372,0.0304,0.042,-0.0046,-0.049,-0.0501,-0.0353,0.0349,-0.0092,-0.0706,-0.0452,-0.0076,-0.0169,-0.0404,0.0304,-0.002,0.0488,0.0362,0.0003,-0.0031,0.002,0.0307,-0.048,0.0316,0.0324,0.0372,-0.0436,0.2133,-0.0626,0.065,0.0415,-0.0489,0.0311,-0.0432,0.0137,-0.0441,-0.0085,-0.0222,-0.0393,-0.0218,0.0986,-0.0241,-0.0104,0.016,0.0344,-0.0097,-0.0024,-0.0134,-0.0509,0.0173,0.0253,0.0426,0.031,-0.0522,-0.0026,0.1219,0.002,0.0156,0.0697,-0.0604,-0.0155,-0.0438,0.0192,-0.0055,-0.0259,-0.0062,0.024,0.0054,-0.0694,-0.0788,0.0366,-0.0999,-0.022,0.1052,-0.0324,0.0414,-0.0417,-0.0477,-0.0006,0.0228,0.002,-0.0177,0.025,0.01,0.0138,0.034,-0.0667,0.0297,-0.0197,-0.0195,0.029,0.128,-0.0387,-0.0635,-0.0219,-0.0324,-0.0016,0.0096,0.0385,0.0189,-0.0738,0.0137,0.0932,0.0162,-0.0714,0.0099,-0.015,0.0073,0.0043,-0.039,-0.0517,0.0154,0.0385,-0.0588,-0.0212,-0.0286,0.0375,0.044,-0.049,-0.0276,-0.0073,-0.0,-0.0508,-0.0814,-0.0115,0.0262,0.0159,0.0143,0.0312,0.0042,-0.0406,0.0451,0.0082,0.0111,-0.0166,-0.0033,0.0583,0.0571,-0.063,-0.0292,0.0442,-0.0312,-0.0389,0.023,0.0462,0.0084,-0.0555,0.0624,0.0161,0.0033,-0.0482,-0.2353,-0.0534,0.0056,-0.0018,0.0178,-0.0697,0.0291,-0.0146,0.0295,0.0675,0.0305,-0.0145,-0.0443,0.0696,0.0053,0.054,0.053,0.0084,-0.0061,-0.0425,0.0297,0.0176,-0.0045,-0.0849,0.0515,-0.0198,0.236,-0.0381,0.0747,-0.0229,0.0279,-0.0185,0.025,-0.0763,0.0005,0.042,0.0643,-0.0533,-0.0009,-0.0265,-0.0005,0.0138,-0.0034,-0.0787,-0.0444,-0.0615,-0.0285,0.0047,-0.0966,0.0597,0.0372,-0.0194,0.0369,-0.002,0.0117,-0.0484,-0.0882,-0.0035,-0.0499,0.0489,-0.0064,-0.0391,-0.0254,-0.0188,0.0623,-0.0015,0.0002,-0.021,-0.0079,-0.0416,-0.0285,0.0618,-0.0127,0.0289,0.0235,0.0252,0.0911,0.0045,-0.0249,-0.0364,0.0801,-0.014,0.0436,-0.0089,-0.016,-0.0071,0.104,-0.0056,-0.0059,-0.0026,-0.0128,-0.0055,-0.0509,-0.0375,0.0587,-0.0157,-0.2804,0.0017,0.0327,-0.0265,-0.0051,-0.0006,0.0564,-0.0168,-0.075,-0.0148,0.0131,0.0649,-0.0016,-0.027,0.0646,0.0085,0.0438,-0.0083,0.0615,-0.0631,0.0429,0.0465,0.196,-0.0608,0.0515,0.0363,-0.0422,-0.0114,-0.0149,-0.0404,0.0149,0.0111,0.066,-0.0611,0.0457,0.12,-0.0084,0.0287,0.0209,0.0019,-0.004,-0.0051,-0.0218,0.0522,0.1218,-0.0222,-0.025,-0.0069,0.0113,0.001,-0.0529,0.0541,0.0253,-0.0009,0.0183,0.0251,-0.0556,-0.0468,-0.0093,-0.0166,0.0203,-0.0726,-0.0099,0.0466,0.0082]}
{"key":"[Deep Imitative Models for Flexible Inference, Planning, and Control] Imitation Learning (IL) is an appealing approach to learn desirable autonomous behavior. However, directing IL to achieve arbitrary goals is difficult. In contrast, planning-based algorithms use dynamics models and reward functions to achieve goals. Yet, reward functions that evoke desirable behavior are often difficult to specify. In this paper, we propose Imitative Models to combine the benefits of IL and goal-directed planning. Imitative Models are probabilistic predictive models of desirable behavior able to plan interpretable expert-like trajectories to achieve specified goals. We derive families of flexible goal objectives, including constrained goal regions, unconstrained goal sets, and energy-based goals. We show that our method can use these objectives to successfully direct behavior. Our method substantially outperforms six IL approaches and a planning-based approach in a dynamic simulated autonomous driving task, and is efficiently learned from expert demonstrations without online data collection. We also show our approach is robust to poorly specified goals, such as goals on the wrong side of the road.","layer":2,"vector":[-0.0268,-0.0039,0.017,-0.0338,-0.0257,0.0382,0.0507,0.033,0.0152,-0.0046,0.0368,-0.0523,0.0189,0.0331,0.0074,0.0203,-0.0239,0.0641,-0.0376,0.0087,0.0352,-0.0577,-0.0259,-0.0446,0.0275,0.0197,-0.0229,-0.0223,-0.0378,-0.19,0.0137,-0.0688,0.0252,-0.0502,-0.0414,-0.0245,-0.0558,0.0844,-0.0255,0.0322,0.0618,0.0114,-0.0204,-0.0582,0.0001,-0.056,-0.0507,-0.0199,-0.0214,-0.0422,0.0154,-0.0426,-0.0099,0.0157,0.0671,0.0288,0.0707,0.0903,0.0901,0.0343,0.0042,0.0328,-0.1515,0.0469,0.0301,0.0655,-0.0373,-0.0157,0.0153,0.043,-0.0151,0.0192,-0.0129,0.0707,-0.0003,-0.0201,-0.0117,-0.0086,-0.0391,-0.0034,-0.0048,-0.0426,-0.069,0.0014,-0.018,-0.0605,0.0009,-0.0355,0.0653,0.0083,-0.0358,-0.0215,-0.0365,-0.0027,-0.0379,0.015,0.034,0.0292,-0.0473,0.2093,-0.0123,0.0542,0.0434,-0.012,-0.0095,-0.0107,-0.0667,-0.0121,-0.0273,0.0084,-0.0464,-0.0223,0.0426,-0.023,0.0362,0.0454,0.0945,0.0826,-0.017,-0.0335,0.0137,-0.0249,0.066,-0.0355,-0.0148,-0.0897,0.0114,0.1245,-0.0108,0.0397,0.0341,-0.0439,-0.056,-0.0022,0.0255,0.0067,0.0283,-0.044,0.0127,0.0171,-0.0201,-0.0164,0.0004,-0.1069,-0.037,0.0913,-0.0251,-0.0053,-0.0161,-0.0051,-0.0237,-0.0146,-0.0197,-0.0208,0.0182,0.0038,0.0354,0.0614,-0.0636,0.0078,-0.0269,-0.0423,-0.0152,0.0525,-0.0098,-0.067,-0.0428,0.0107,-0.003,-0.0142,0.0205,0.0185,-0.0324,0.0317,0.0927,0.0529,-0.0611,0.0384,-0.0279,0.0083,0.0207,-0.092,-0.0359,0.0184,0.0318,-0.0065,-0.0121,-0.0393,0.0227,0.0231,0.0184,0.0453,-0.0331,0.0147,-0.0356,-0.0079,-0.0177,0.0002,0.0208,-0.0416,-0.0431,-0.0272,-0.0358,-0.0,-0.0263,-0.0247,-0.0159,0.0095,0.0736,0.0216,-0.0656,0.02,0.0266,-0.004,-0.0698,-0.0155,-0.012,0.0246,0.0276,0.0152,-0.001,-0.0097,0.0075,-0.204,0.0021,-0.0201,-0.0024,0.028,-0.0681,0.0277,-0.0099,0.0484,0.049,0.0723,-0.0696,-0.0234,0.0517,0.0146,0.0484,-0.0065,-0.0038,-0.0068,0.0221,0.0076,0.0229,-0.0371,-0.1005,0.0422,0.0094,0.2285,0.0451,0.072,0.0113,0.0322,0.0321,-0.0395,-0.0987,0.0152,-0.003,0.099,-0.0468,-0.0316,-0.058,-0.0074,0.0245,-0.0539,-0.1025,-0.0397,0.024,-0.0256,0.0631,-0.032,0.0017,0.0777,-0.029,0.0434,-0.0462,-0.0299,-0.0378,-0.1018,0.0364,-0.0281,0.0308,-0.008,-0.0045,0.0185,-0.0812,0.048,-0.0202,0.0067,-0.0378,0.0319,0.0533,0.013,0.0546,0.0095,0.0117,0.0277,-0.0112,0.01,-0.0188,-0.0547,-0.0432,0.0482,-0.0107,0.0529,0.0338,0.0384,-0.0148,0.0771,-0.0515,0.0444,-0.0304,0.08,0.0124,-0.0478,-0.0259,0.0937,-0.0071,-0.3074,0.0343,-0.013,0.0691,-0.0133,0.0359,0.0563,-0.012,-0.0361,0.019,-0.0151,0.0338,0.0567,0.0546,0.0223,0.0399,0.1172,-0.0193,0.0363,-0.0919,-0.0152,0.0575,0.2507,-0.0462,0.0719,0.0347,0.0036,-0.0442,0.0553,-0.0491,0.0076,-0.0231,0.0758,-0.0613,0.0299,0.0663,-0.0466,0.0431,0.007,-0.009,-0.034,0.0308,0.0255,-0.0571,0.0827,0.0127,-0.0163,-0.0256,-0.0173,0.05,0.0059,0.0081,-0.0226,-0.055,0.0445,0.001,-0.0092,-0.0425,-0.0514,-0.0572,0.0416,-0.0411,0.0173,-0.0034,-0.037]}
{"key":"[Fluctuation-dissipation relations for stochastic gradient descent] The notion of the stationary equilibrium ensemble has played a central role in statistical mechanics. In machine learning as well, training serves as generalized equilibration that drives the probability distribution of model parameters toward stationarity. Here, we derive stationary fluctuation-dissipation relations that link measurable quantities and hyperparameters in the stochastic gradient descent algorithm. These relations hold exactly for any stationary state and can in particular be used to adaptively set training schedule. We can further use the relations to efficiently extract information pertaining to a loss-function landscape such as the magnitudes of its Hessian and anharmonicity. Our claims are empirically verified.","layer":2,"vector":[-0.0614,-0.0172,0.0394,0.0036,0.0198,0.0254,0.05,0.0041,0.068,-0.0214,0.0122,-0.0587,0.0656,0.0487,0.0044,-0.0086,-0.0154,0.033,-0.0636,0.0041,0.0418,-0.0242,0.0049,-0.0594,0.0239,0.0368,-0.0226,-0.0523,-0.044,-0.2389,-0.0005,-0.0274,0.0322,-0.0528,0.0569,0.0162,-0.0261,0.0405,-0.0192,0.0226,-0.0289,0.0509,-0.0457,-0.0838,0.0118,-0.0343,-0.0164,-0.0443,-0.0786,0.0058,0.0171,-0.0189,0.0393,-0.0078,0.0027,0.0257,0.0775,0.059,0.0528,0.0327,-0.0019,0.0535,-0.1713,0.0701,0.0569,-0.0199,-0.0424,-0.0337,0.0169,0.0383,0.0113,0.0202,0.0351,0.0344,0.0148,-0.0195,0.0013,0.0009,0.0239,0.0277,0.0471,-0.0108,-0.0642,-0.014,-0.0171,-0.0613,0.0803,-0.0245,0.0487,0.0365,-0.0504,-0.0422,-0.0172,0.0169,-0.0533,0.034,0.0537,0.0612,-0.0,0.1972,-0.0176,0.0285,-0.0153,-0.0046,0.0136,-0.0402,-0.0514,0.0024,-0.0079,-0.0109,-0.0272,-0.0341,0.0577,-0.068,0.0311,-0.0072,0.0384,0.0837,-0.0222,-0.0151,-0.013,0.008,0.0504,-0.015,0.035,-0.0778,0.0012,0.1673,0.0302,0.0221,0.0297,-0.0195,-0.0282,-0.044,0.021,0.026,0.0097,-0.0555,0.0262,0.0169,-0.0489,-0.0465,0.0049,-0.1243,-0.0841,0.116,-0.0101,0.0316,-0.0433,-0.0185,-0.0002,-0.0082,0.0082,-0.0554,0.0236,0.0524,0.0148,0.0324,-0.0382,0.0076,-0.0558,-0.0697,-0.0185,0.0999,0.0026,-0.035,-0.0249,0.0185,0.0119,0.0102,0.0871,0.0176,-0.0502,-0.0255,0.0834,0.0247,-0.0776,-0.0102,0.0196,-0.013,0.0191,-0.0264,-0.0455,0.0379,0.0514,0.0059,-0.0008,-0.0398,0.0073,0.0248,-0.0391,-0.0255,0.0104,-0.0088,-0.0341,0.0163,-0.037,-0.0074,0.0273,-0.0109,-0.0226,0.0138,-0.0336,0.0738,0.0107,0.0255,-0.0152,-0.026,0.0181,0.0397,-0.0294,-0.0301,0.0134,-0.0227,-0.006,0.0251,-0.0333,0.0211,-0.0115,0.0225,0.0616,-0.0279,-0.0597,-0.2569,0.0022,-0.0107,-0.0332,0.0944,-0.0455,0.0324,-0.0048,0.0555,0.0765,0.0503,0.0019,-0.0519,-0.0323,0.0283,0.0388,0.065,0.0114,-0.0004,-0.0019,-0.0047,0.0369,-0.0242,-0.1015,0.0692,0.0149,0.1571,-0.0183,0.0447,-0.0003,0.0059,0.0303,-0.0043,-0.0488,0.0602,0.0374,0.0828,-0.0317,-0.0128,-0.0489,-0.0267,0.0807,-0.0043,-0.0452,-0.0714,-0.0161,-0.0227,-0.0117,-0.0766,0.0071,0.0249,-0.0398,0.0805,-0.0372,0.0194,-0.0068,-0.0692,0.037,-0.0605,0.0229,-0.0066,-0.0391,0.039,-0.0596,0.0719,-0.0014,-0.0295,-0.0529,0.0001,-0.0231,-0.0077,0.0588,0.0118,0.0166,0.0568,-0.0321,0.0179,-0.0106,-0.0558,-0.0383,0.037,-0.0173,0.0457,0.064,0.0135,-0.0077,0.1201,-0.041,0.0167,0.011,-0.011,0.0208,-0.0684,0.0028,0.0418,-0.0272,-0.2999,0.0479,-0.0164,0.0383,-0.0341,0.0001,0.0789,0.0339,-0.0681,-0.0214,-0.0253,0.0467,0.0408,0.0154,-0.0061,0.0437,0.0633,-0.0696,0.021,-0.0528,0.0218,0.0448,0.2226,0.0219,0.02,0.0312,-0.0285,0.0234,0.0121,-0.0481,0.0107,-0.0491,0.086,-0.0445,0.0479,0.0651,-0.0527,0.0552,-0.0004,-0.0321,0.0155,-0.0277,-0.0122,-0.0306,0.1073,-0.0086,-0.0346,-0.0573,-0.0178,0.019,-0.0315,0.0243,-0.0035,-0.0154,0.0007,-0.0068,-0.0501,-0.0373,-0.0085,-0.0556,0.005,-0.0449,-0.043,-0.0122,-0.0286]}
{"key":"[Stochastic Online Convex Optimization; Application to probabilistic time series forecasting] Stochastic regret bounds for online algorithms are usually derived from an ''online to batch'' conversion. Inverting the reasoning, we start our analyze by a ''batch to online'' conversion that applies in any Stochastic Online Convex Optimization problem under stochastic exp-concavity condition. We obtain fast rate stochastic regret bounds with high probability for non-convex loss functions. Based on this approach, we provide prediction and probabilistic forecasting methods for non-stationary unbounded time series.","layer":3,"vector":[-0.055,-0.0023,0.0553,-0.0155,0.0159,0.0435,0.0239,0.0238,0.0537,0.0181,0.0065,-0.0165,-0.006,0.0697,-0.0007,0.0391,-0.042,0.0167,-0.0401,0.0172,0.0521,-0.0615,-0.0259,-0.0708,0.046,0.0114,-0.0106,-0.058,-0.0361,-0.1968,-0.0196,-0.0667,0.0304,-0.0479,-0.0016,-0.043,-0.0002,0.0824,-0.0519,0.0945,-0.0065,0.0489,-0.0573,-0.0589,0.0082,-0.074,0.0201,0.0099,-0.0411,-0.0069,0.0331,-0.0063,0.0407,0.0217,0.014,0.0428,0.0022,0.0347,0.0253,0.0463,0.0271,0.0189,-0.2073,0.0356,-0.0072,0.013,-0.0307,0.0049,-0.0116,0.0825,0.0127,0.0427,0.0093,0.0616,0.0348,0.017,-0.0202,-0.0342,-0.0308,0.0294,0.0303,-0.0178,-0.0759,0.006,-0.0398,-0.0583,0.0288,-0.0491,0.0494,0.0382,-0.0371,-0.0236,-0.0151,0.0189,-0.1031,0.023,0.0545,0.0596,-0.0298,0.1878,-0.0768,0.0726,0.0246,-0.0345,0.0333,-0.025,-0.0274,-0.0189,-0.0166,-0.0312,-0.0146,-0.0213,0.0717,-0.0614,0.018,0.037,0.0271,0.0415,0.0274,-0.0143,-0.0518,0.0351,0.0893,0.0092,-0.0033,-0.0549,0.0382,0.1544,0.0055,0.0282,0.0218,-0.0689,-0.0437,-0.035,0.0442,-0.0224,0.0025,-0.0126,0.0334,-0.0421,-0.0203,-0.0386,-0.0169,-0.0818,-0.0283,0.1384,-0.0089,0.0266,-0.0407,-0.0633,-0.0047,0.0015,0.0252,-0.0531,0.028,0.0193,0.0295,0.0125,-0.0706,0.0076,-0.0631,-0.0284,0.0323,0.1144,0.0162,-0.0481,-0.0443,0.0153,0.0113,0.009,0.0414,-0.0218,-0.0292,0.009,0.0768,0.0751,-0.0683,-0.0196,0.0147,-0.0111,-0.0038,-0.0108,-0.013,0.002,0.0024,-0.0358,0.0181,-0.0345,0.0397,0.0264,-0.0193,-0.0329,-0.0343,-0.0021,-0.0201,-0.0405,-0.0229,-0.0072,0.0562,-0.0266,0.0264,-0.0395,-0.0933,0.022,0.0285,-0.0069,0.0004,0.021,0.0573,0.0469,-0.0148,-0.0229,0.0565,-0.0271,-0.0573,0.0404,0.0576,0.0689,-0.0002,0.0262,0.0304,0.0088,-0.0146,-0.1891,0.0255,-0.0038,0.0099,0.0417,-0.0791,0.0251,-0.0101,0.0847,0.0704,0.0544,-0.062,-0.0007,0.001,0.0241,0.0421,0.0348,0.0248,-0.0236,0.0308,-0.0322,0.012,-0.0346,-0.0753,0.0501,-0.0018,0.1644,-0.0032,0.0696,-0.0617,0.0429,0.0272,0.0197,-0.064,0.0359,0.051,0.0673,-0.0067,-0.0672,-0.0429,-0.021,0.0237,0.0238,-0.083,-0.0656,-0.0454,-0.0152,0.0402,-0.0677,0.0007,0.0271,-0.0293,0.0939,-0.0304,0.0298,-0.051,-0.1014,0.0319,0.0013,0.0228,0.0136,-0.0347,0.0117,-0.0765,0.05,0.0004,-0.0163,-0.0212,-0.0012,-0.0207,-0.0233,0.0908,-0.0388,0.0199,0.0431,0.0011,0.0499,0.0009,-0.068,-0.0256,0.0435,-0.0761,0.0473,0.0481,0.0278,-0.0042,0.0743,-0.0368,0.0118,0.0267,-0.0365,-0.0168,-0.0578,0.0055,0.044,0.0025,-0.3182,0.0405,-0.0171,0.0076,-0.0212,-0.0225,0.0092,0.0261,-0.067,0.0078,0.0123,0.0518,0.0424,0.0028,0.0055,0.0365,0.0477,-0.0491,0.0406,-0.0634,-0.0019,0.0208,0.2294,-0.0366,0.0384,0.0112,-0.0282,-0.0074,0.0522,-0.0497,0.037,-0.0278,0.052,-0.0789,0.0432,0.0621,-0.0363,0.0707,0.0187,-0.017,-0.0167,0.0147,-0.0132,0.0173,0.0943,-0.0046,-0.0607,-0.0575,-0.0056,0.0177,-0.0526,0.034,-0.001,-0.0028,-0.0024,0.0313,-0.0229,-0.0883,-0.0117,-0.0416,0.0316,-0.0407,-0.0504,-0.0335,0.0055]}
{"key":"[Approximation in shift-invariant spaces with deep ReLU neural networks] We study the expressive power of deep ReLU neural networks for approximating functions in dilated shift-invariant spaces, which are widely used in signal processing, image processing, communications and so on. Approximation error bounds are estimated with respect to the width and depth of neural networks. The network construction is based on the bit extraction and data-fitting capacity of deep neural networks. As applications of our main results, the approximation rates of classical function spaces such as Sobolev spaces and Besov spaces are obtained. We also give lower bounds of the $L^p (1\\le p \\le \\infty)$ approximation error for Sobolev spaces, which show that our construction of neural network is asymptotically optimal up to a logarithmic factor.","layer":0,"vector":[-0.0542,-0.0361,0.0464,-0.0413,0.0221,0.0427,0.0242,0.0506,0.0546,-0.0381,0.0059,-0.0848,0.0382,0.0694,0.0306,0.0131,-0.0062,0.0442,-0.0816,0.0005,0.0576,0.0074,-0.0025,-0.0385,0.0317,-0.0483,-0.0214,-0.0317,-0.005,-0.2203,0.0154,-0.0369,0.0358,-0.0488,0.0473,-0.0228,-0.0303,-0.0067,-0.0508,0.0555,0.0381,0.0271,-0.0232,-0.0575,-0.0144,-0.0657,-0.018,-0.0241,-0.016,-0.0261,0.0339,-0.0181,0.0385,0.0433,0.0277,-0.0,0.0574,0.0041,0.0333,0.032,0.0399,0.021,-0.1816,0.0567,0.0142,-0.0124,-0.0147,-0.0648,0.0239,0.0353,-0.0067,0.0424,-0.0146,0.0268,0.0007,0.0063,0.0029,-0.0399,0.0315,0.0357,0.0151,-0.0264,-0.0385,-0.0218,-0.039,-0.0368,0.0115,-0.0425,0.0054,-0.0413,-0.0578,-0.0217,-0.0274,0.0014,-0.0393,-0.0126,0.0238,0.0291,-0.0586,0.2117,-0.0353,0.0408,0.0454,-0.0535,0.0094,-0.0227,-0.0419,0.0025,-0.0331,-0.0073,-0.0512,-0.0414,0.0171,-0.014,0.0012,0.0256,0.0379,0.0441,-0.017,0.0107,-0.0369,0.0067,0.0003,-0.0066,0.0219,-0.0595,-0.0032,0.1206,0.0752,0.0743,0.0169,0.0126,-0.0542,0.0026,0.0413,0.0468,-0.027,0.0227,0.0158,0.0118,-0.0258,-0.0615,0.0397,-0.0332,-0.0257,0.1027,-0.0698,0.028,-0.0668,-0.0429,-0.0294,0.0508,-0.0324,-0.0336,0.0536,0.0048,0.0385,0.0131,-0.07,0.0177,-0.0167,-0.0464,0.007,0.1194,0.0052,-0.0351,-0.0233,0.0204,0.0218,-0.0498,0.0326,0.0556,-0.0283,0.0226,0.0776,0.0359,-0.0582,-0.0223,-0.0003,0.0415,-0.0232,-0.0176,-0.0271,0.0267,0.0534,-0.025,-0.01,-0.0538,0.0148,0.0945,-0.0183,0.01,-0.0541,-0.0022,-0.0121,-0.0453,0.0157,-0.0138,0.0065,-0.0354,0.0061,-0.0062,-0.0716,0.0028,0.0214,0.0081,-0.0104,0.004,0.0187,0.0346,-0.036,-0.0104,0.063,-0.0732,0.0177,0.0105,0.0218,-0.0023,0.0239,0.0356,0.017,-0.0717,-0.0743,-0.249,-0.0437,0.0014,-0.0379,0.0699,-0.0808,0.0708,0.0251,0.0183,0.035,0.0393,0.002,0.0113,0.0154,0.0089,0.0379,0.0461,0.0345,-0.0481,-0.0464,-0.0328,0.0252,-0.0407,-0.0653,0.0773,-0.0127,0.1969,-0.0192,0.0755,-0.0552,0.0068,0.0381,0.0328,-0.0629,0.0749,0.0486,0.0978,0.0162,0.0126,-0.0829,0.0103,0.0113,0.0045,-0.0773,-0.0068,-0.0386,-0.0393,-0.009,-0.0722,0.0011,0.0331,-0.057,0.0604,-0.0167,0.0217,-0.0041,-0.0693,-0.01,-0.0002,0.0316,-0.0219,-0.0802,0.012,-0.0207,0.0615,0.0284,0.0325,-0.0166,0.0165,0.0098,0.0001,0.0724,-0.0225,0.0251,0.106,-0.0479,0.0413,-0.0181,-0.02,-0.0383,0.0895,-0.0397,0.0106,0.0083,0.0494,0.0056,0.0894,-0.0557,0.0133,-0.0499,-0.0346,-0.0164,-0.1017,0.0264,0.0166,-0.0048,-0.2797,0.0476,0.0133,0.0148,-0.0089,0.0001,0.0117,0.0133,-0.0267,-0.017,0.0069,0.0484,0.0213,-0.0016,0.0012,0.0119,0.0523,-0.0504,0.0782,-0.0772,0.0375,0.011,0.2178,-0.0728,0.0274,0.0222,-0.0411,-0.0197,0.0034,-0.0149,-0.0112,0.0234,0.0898,-0.0596,0.0634,0.1307,-0.0149,0.0621,0.0345,-0.0191,0.0369,0.0269,-0.0294,-0.0052,0.1101,-0.0218,-0.0075,-0.0263,0.0309,0.0393,-0.0179,0.0234,-0.0057,-0.0231,0.0468,0.0448,-0.08,-0.0522,-0.0283,-0.0374,0.0202,-0.1072,-0.0086,0.014,-0.024]}
{"key":"[Online k-means Clustering] We study the problem of online clustering where a clustering algorithm has to assign a new point that arrives to one of $k$ clusters. The specific formulation we use is the $k$-means objective: At each time step the algorithm has to maintain a set of k candidate centers and the loss incurred is the squared distance between the new point and the closest center. The goal is to minimize regret with respect to the best solution to the $k$-means objective ($\\mathcal{C}$) in hindsight. We show that provided the data lies in a bounded region, an implementation of the Multiplicative Weights Update Algorithm (MWUA) using a discretized grid achieves a regret bound of $\\tilde{O}(\\sqrt{T})$ in expectation. We also present an online-to-offline reduction that shows that an efficient no-regret online algorithm (despite being allowed to choose a different set of candidate centres at each round) implies an offline efficient algorithm for the $k$-means problem. In light of this hardness, we consider the slightly weaker requirement of comparing regret with respect to $(1 + \\epsilon) \\mathcal{C}$ and present a no-regret algorithm with runtime $O\\left(T(\\mathrm{poly}(log(T),k,d,1/\\epsilon)^{k(d+O(1))}\\right)$. Our algorithm is based on maintaining an incremental coreset and an adaptive variant of the MWUA. We show that na\\\"{i}ve online algorithms, such as \\emph{Follow The Leader}, fail to produce sublinear regret in the worst case. We also report preliminary experiments with synthetic and real-world data.","layer":7,"vector":[-0.0402,-0.0322,0.0475,0.0245,0.0179,0.0348,0.01,0.0139,0.0387,-0.0171,0.0723,-0.0704,0.0316,0.0171,-0.0124,0.0432,-0.002,0.0271,-0.0404,0.0158,-0.0112,-0.0653,-0.0037,-0.043,0.0346,0.0218,-0.0615,-0.0723,-0.0646,-0.2225,0.0203,-0.0388,0.0825,-0.0218,-0.0245,0.0226,0.0049,0.1014,-0.0705,0.0271,0.0222,0.0249,-0.0165,-0.0264,-0.0258,-0.0448,-0.0232,0.0177,-0.0355,-0.049,0.0316,-0.0483,0.014,0.039,0.0403,0.0382,0.0232,0.0345,0.0123,0.028,0.0701,0.0278,-0.1303,0.029,0.0246,-0.0158,-0.0387,-0.0132,0.0046,0.0524,0.0315,0.091,0.0043,0.0468,0.041,0.024,-0.0065,-0.0229,-0.0565,0.0378,0.0177,-0.0222,-0.0256,0.0394,-0.0506,-0.07,-0.01,-0.0678,0.0447,0.0249,-0.0212,0.0148,-0.0006,0.0163,-0.1036,-0.0288,-0.0013,0.0155,-0.0119,0.2259,-0.0322,0.0673,0.0383,-0.0456,-0.0128,-0.0748,-0.018,-0.0638,0.0078,-0.0336,0.0179,0.0203,-0.0084,-0.042,0.0005,0.0006,0.085,0.0616,-0.001,0.022,-0.0212,0.0187,0.0597,-0.0048,0.0653,-0.0476,-0.0032,0.1218,0.0324,0.0141,0.0326,-0.0174,-0.041,0.0012,0.0317,0.026,0.0048,-0.0226,0.0126,-0.0539,-0.0313,-0.0746,0.0274,-0.0816,0.0076,0.1696,-0.0227,0.021,-0.0155,-0.0552,-0.0063,-0.0246,-0.0735,-0.0234,0.0161,-0.0039,0.0591,0.0056,-0.0494,0.0114,-0.0367,-0.0485,0.0036,0.1025,0.0145,-0.0624,-0.0361,-0.0207,-0.0072,0.0126,0.0653,0.0384,-0.0048,0.0524,0.0824,0.0227,-0.0572,-0.0195,0.0395,0.0212,0.0162,0.0358,-0.0464,0.0324,0.0368,-0.0284,-0.0303,-0.0174,0.0162,0.0399,-0.0348,-0.0023,-0.0291,-0.0468,-0.0144,-0.0505,-0.0121,-0.0314,0.0297,-0.0224,0.0048,0.0264,-0.0552,0.0029,0.0221,0.0258,0.0287,-0.023,0.0615,0.0375,-0.0338,-0.0082,0.0811,-0.0475,-0.0379,0.0249,0.0851,0.0407,0.0004,0.0298,0.0561,-0.0219,-0.0449,-0.1827,-0.0246,-0.0265,-0.015,0.0374,-0.063,0.0658,-0.0113,0.0565,0.0838,0.0579,-0.0369,0.0075,0.0274,0.0122,0.0515,0.0402,0.0111,-0.0423,0.0299,-0.0241,0.0153,-0.0557,-0.0681,0.0139,0.0041,0.2442,0.0588,0.0384,-0.0127,0.0356,0.0414,0.011,-0.0737,0.0473,0.0406,0.0481,-0.0007,-0.0523,-0.0141,-0.0342,-0.0141,0.0116,-0.115,-0.0303,-0.0073,-0.0008,0.0075,-0.0521,-0.0107,-0.0043,-0.0208,0.0715,-0.0254,0.016,-0.0518,-0.0909,0.0103,-0.0377,0.0172,0.0473,-0.0546,-0.0303,-0.0685,0.089,-0.0352,-0.0261,-0.0289,0.0138,-0.0651,-0.0272,0.0507,-0.0127,-0.006,0.0435,0.0154,0.0029,-0.0145,-0.0624,-0.0003,0.0553,-0.0707,0.0297,0.0161,-0.0114,0.0412,0.079,0.0206,-0.0208,0.0266,0.0062,-0.0076,-0.0643,-0.0187,0.0529,0.0345,-0.2989,0.0294,-0.0082,0.0197,-0.0201,-0.0022,-0.0121,0.0097,-0.0148,0.0092,-0.0086,0.0833,0.0228,-0.0411,-0.0057,0.0813,0.0275,-0.0407,0.0559,-0.0474,0.0148,0.0211,0.2086,-0.0624,0.0335,0.0068,-0.0289,-0.0141,0.058,-0.0315,-0.0218,0.0058,0.073,-0.0595,0.0543,0.0313,-0.0448,0.0226,0.0358,-0.0582,-0.012,0.0149,-0.0436,-0.0037,0.1097,0.0408,-0.0707,-0.0782,-0.0095,-0.0181,-0.0307,0.0287,-0.0233,0.004,-0.0012,0.0333,-0.0374,-0.0478,-0.0572,-0.0608,0.0191,-0.05,-0.0225,-0.0416,0.0369]}
{"key":"[A Convolutional Neural Network Approach to the Classification of Engineering Models] This paper presents a deep learning approach for the classification of Engineering (CAD) models using Convolutional Neural Networks (CNNs). Owing to the availability of large annotated datasets and also enough computational power in the form of GPUs, many deep learning-based solutions for object classification have been proposed of late, especially in the domain of images and graphical models. Nevertheless, very few solutions have been proposed for the task of functional classification of CAD models. Hence, for this research, CAD models have been collected from Engineering Shape Benchmark (ESB), National Design Repository (NDR) and augmented with newer models created using a modelling software to form a dataset - 'CADNET'. It is proposed to use a residual network architecture for CADNET, inspired by the popular ResNet. A weighted Light Field Descriptor (LFD) scheme is chosen as the method of feature extraction, and the generated images are fed as inputs to the CNN. The problem of class imbalance in the dataset is addressed using a class weights approach. Experiments have been conducted with other signatures such as geodesic distance etc. using deep networks as well as other network architectures on the CADNET. The LFD-based CNN approach using the proposed network architecture, along with gradient boosting yielded the best classification accuracy on CADNET.","layer":0,"vector":[-0.0037,0.0297,0.0206,-0.0179,0.0583,0.0156,0.0381,0.0386,0.0055,-0.0143,-0.0031,-0.093,-0.0087,0.0633,0.0053,-0.0015,0.0084,0.0715,-0.0076,-0.0185,0.0302,-0.0535,-0.0144,-0.0615,0.022,0.01,-0.0329,0.001,-0.0509,-0.2432,-0.0078,-0.0308,0.0139,-0.0228,0.0017,-0.0407,-0.0514,0.0129,-0.0577,0.009,0.0289,-0.0264,-0.0038,-0.0749,-0.0047,-0.0254,-0.0072,-0.0595,0.0194,-0.0534,0.0898,-0.0585,0.0189,0.0224,0.0134,0.0413,0.0673,0.0368,0.0613,0.0189,0.0642,0.0119,-0.1918,0.0673,0.0191,0.0341,-0.055,-0.052,0.0186,0.0356,-0.0034,0.0233,0.0051,-0.006,-0.0027,0.0274,-0.0066,-0.0088,-0.0218,-0.0137,0.0324,0.007,-0.0356,-0.0331,0.015,0.0046,0.0332,-0.0273,0.0517,0.0251,-0.0585,-0.0487,-0.0235,0.0305,-0.0757,0.0118,0.0078,-0.0063,-0.0648,0.2173,-0.0674,0.0078,0.0482,0.0074,0.0278,0.0225,-0.0272,-0.0351,-0.0458,0.002,-0.002,0.0194,-0.0171,0.0162,-0.0037,-0.0272,0.0269,0.0455,-0.0176,0.0066,-0.0238,0.027,0.0586,-0.035,0.0382,-0.0543,0.0426,0.1305,0.0439,0.0283,0.0053,0.0007,-0.0523,-0.0069,0.0189,0.0612,0.0242,-0.0169,-0.0072,-0.0089,-0.0564,-0.0374,0.05,-0.0588,-0.0585,0.088,-0.0456,0.0366,-0.0241,-0.0395,-0.0041,0.0007,-0.0019,-0.0253,0.0486,0.0234,0.0102,0.0705,-0.072,0.041,0.0046,-0.0607,-0.0515,0.122,-0.0042,-0.1035,-0.0715,-0.0149,-0.0214,-0.0417,0.0242,0.0374,-0.0158,0.0484,0.1015,0.0503,-0.0466,-0.0434,0.0056,0.003,0.0392,-0.0344,-0.0388,0.0112,0.0595,-0.0233,-0.0111,-0.0267,0.0065,0.0686,-0.0395,0.0905,-0.0258,0.0201,-0.0487,-0.0147,-0.0455,-0.065,-0.0003,-0.0396,0.0024,0.0151,0.0022,0.0242,-0.0225,0.0101,-0.0125,0.0057,0.0048,0.0777,-0.0708,-0.0305,0.0795,-0.0772,-0.008,-0.015,0.044,0.0493,-0.0293,0.0189,0.0403,-0.0689,-0.0396,-0.2113,0.0024,0.0069,-0.0418,0.07,-0.0861,0.0461,0.0222,0.0094,0.0485,0.0878,0.012,-0.0048,-0.0268,-0.0215,0.0281,0.0423,0.0242,-0.0618,-0.028,0.0215,0.0224,0.0154,-0.0997,0.0426,-0.0045,0.2202,-0.0443,0.0569,-0.0151,0.0321,0.0463,-0.003,-0.0685,0.0462,-0.0013,0.0549,0.0306,-0.0518,-0.0136,-0.0476,0.0484,0.0225,-0.0909,-0.0218,-0.0298,-0.0275,0.0288,-0.0545,0.0517,0.0011,-0.0244,0.0397,0.0568,-0.0064,-0.0581,-0.0914,0.0438,-0.0424,-0.0015,0.0447,-0.0672,0.0154,-0.0262,0.0364,0.0099,-0.114,-0.0614,0.0046,-0.0423,0.0002,0.1012,0.0544,-0.0125,0.0886,-0.0199,0.0451,0.0115,0.0191,-0.0184,0.0405,0.0052,0.0183,0.0242,0.0478,0.0132,0.0254,-0.059,0.0208,0.0181,0.0357,0.0497,-0.0294,0.0093,0.0293,0.0113,-0.2531,0.0253,0.0561,0.0506,-0.0589,0.0053,0.0296,0.0161,0.0138,-0.0283,-0.027,0.0122,0.024,-0.0362,0.0127,0.009,0.0257,-0.0574,0.0824,-0.024,-0.0114,0.0718,0.2168,-0.0586,0.069,-0.0094,-0.0501,-0.0015,0.0004,0.0084,0.0152,0.0228,0.1107,-0.0601,0.0396,0.1203,-0.0091,0.0035,0.015,-0.0115,0.0266,0.0048,-0.0606,-0.0209,0.0629,0.0064,-0.029,-0.0441,-0.004,0.0059,-0.033,0.009,-0.0098,0.0083,0.0326,-0.0001,-0.0549,-0.04,-0.0603,-0.0137,-0.0038,-0.0514,-0.0228,-0.0224,0.0048]}
{"key":"[Optimizing Deeper Transformers on Small Datasets] It is a common belief that training deep transformers from scratch requires large datasets. Consequently, for small datasets, people usually use shallow and simple additional layers on top of pre-trained models during fine-tuning. This work shows that this does not always need to be the case: with proper initialization and optimization, the benefits of very deep transformers can carry over to challenging tasks with small datasets, including Text-to-SQL semantic parsing and logical reading comprehension. In particular, we successfully train $48$ layers of transformers, comprising $24$ fine-tuned layers from pre-trained RoBERTa and $24$ relation-aware layers trained from scratch. With fewer training steps and no task-specific pre-training, we obtain the state-of-the-art performance on the challenging cross-domain Text-to-SQL parsing benchmark Spider. We achieve this by deriving a novel Data-dependent Transformer Fixed-update initialization scheme (DT-Fixup), inspired by the prior T-Fixup work. Further error analysis shows that increasing depth can help improve generalization on small datasets for hard cases that require reasoning and structural understanding.","layer":3,"vector":[-0.0578,-0.0085,0.015,-0.031,-0.0051,-0.0036,-0.0038,0.0289,0.0156,-0.0287,0.0066,-0.0506,0.0821,0.0458,0.0185,0.0573,0.0296,0.0462,-0.0587,-0.0058,0.0379,-0.0618,0.0048,-0.0286,0.019,0.0337,0.019,-0.0285,-0.0438,-0.268,-0.0089,-0.0477,0.0241,0.0012,0.0093,-0.0296,-0.0033,0.0159,0.013,0.0037,0.0105,-0.0131,-0.0411,-0.0026,-0.0228,-0.0776,-0.0311,-0.0499,-0.0261,-0.0308,-0.0363,-0.0081,0.0317,0.0746,0.0086,0.0211,0.0805,0.025,0.0774,0.0414,0.0205,0.0906,-0.1461,0.0887,0.0266,0.019,-0.0585,-0.0158,-0.0275,0.0466,0.0126,0.0133,0.0025,0.0517,0.0138,-0.0085,-0.0179,-0.01,0.002,0.0458,0.0189,-0.0351,-0.0158,-0.0002,-0.0367,-0.0398,0.0127,-0.0527,0.0404,0.0187,-0.0418,-0.0162,-0.0227,0.0413,-0.0442,-0.036,0.0171,0.0184,-0.041,0.1959,-0.0316,0.0342,-0.0292,-0.0386,-0.0068,0.0126,-0.0106,-0.0309,-0.0328,-0.0018,-0.0321,-0.0086,-0.0026,-0.0289,0.0504,0.0021,0.103,-0.0007,-0.0648,0.0128,-0.0143,-0.0004,0.0078,0.0264,0.0296,-0.058,0.0495,0.1281,0.0343,0.0482,0.0465,0.0054,-0.0766,-0.0083,0.0132,0.0377,0.031,-0.0075,-0.0016,-0.0063,-0.0733,-0.0753,-0.016,-0.0552,-0.0526,0.1318,-0.0383,0.0272,-0.069,-0.015,-0.0389,0.0375,-0.0102,-0.0471,0.0198,0.014,0.0404,0.0311,-0.0448,-0.0674,0.0213,-0.0288,-0.066,0.0699,0.0471,-0.0996,-0.0513,-0.0177,0.0248,-0.0217,0.0744,0.0692,-0.0515,0.0117,0.0248,0.0208,-0.0384,-0.0161,-0.0155,0.0256,0.0456,-0.0289,0.0019,0.0569,-0.0127,-0.0294,0.0264,-0.0443,0.0237,0.0128,-0.0244,0.0252,-0.0391,-0.005,0.0031,-0.0057,-0.0045,0.0099,-0.0273,-0.0253,-0.0169,0.0165,-0.0504,0.0289,-0.0345,0.0054,0.0056,0.0163,0.0109,0.041,-0.0029,0.0074,0.0621,-0.0702,0.0135,-0.0053,-0.0045,-0.0074,-0.0092,0.0594,0.0121,-0.0141,-0.0102,-0.216,-0.0176,0.0168,-0.0534,0.0581,-0.0595,0.028,0.0276,0.0092,0.1109,0.011,-0.0412,-0.0467,0.0195,-0.0471,0.0525,0.0355,0.0312,-0.0236,0.0411,0.0129,0.0111,0.0017,-0.1346,0.0499,0.0096,0.2446,0.0202,0.0491,-0.0471,0.0582,0.0239,-0.0183,-0.0965,0.0556,0.0102,0.0599,-0.0187,-0.0492,-0.04,-0.043,-0.0068,-0.025,-0.1225,-0.0111,-0.0472,-0.0367,-0.0056,-0.0271,0.0564,0.0009,-0.026,0.0496,0.0341,0.0178,-0.0414,-0.1067,0.032,-0.0396,-0.0192,0.0144,-0.0309,-0.0112,-0.0477,0.0187,0.0066,-0.0119,-0.0224,0.0307,-0.0333,-0.077,0.0334,-0.0163,0.0174,0.0221,0.0321,0.0104,-0.0029,-0.0175,-0.0176,0.089,-0.0131,0.0634,-0.0103,0.0372,0.0322,0.1054,0.0038,0.0634,0.001,0.0297,0.0069,-0.0562,-0.006,0.0483,-0.0571,-0.3136,0.0408,0.0207,0.0102,-0.0362,0.0144,0.062,0.0212,-0.016,-0.0392,-0.0154,-0.0093,0.0379,0.0244,0.0032,0.0231,0.0847,-0.0391,0.0246,-0.037,0.0205,0.0649,0.2093,-0.0047,0.0099,0.0266,-0.0162,-0.0314,0.0499,0.0233,-0.0109,0.0412,0.0792,-0.0224,0.044,0.0913,-0.0616,0.0683,0.0744,0.026,-0.0338,0.0179,-0.0334,-0.0236,0.0667,-0.0296,-0.0174,-0.0857,-0.0014,0.0095,0.0097,0.0135,-0.0168,-0.028,0.0504,0.0056,-0.0572,0.0012,-0.0457,-0.0335,0.0292,-0.0356,-0.0433,0.0457,-0.008]}
{"key":"[Discriminative Adversarial Domain Generalization with Meta-learning based Cross-domain Validation] The generalization capability of machine learning models, which refers to generalizing the knowledge for an \"unseen\" domain via learning from one or multiple seen domain(s), is of great importance to develop and deploy machine learning applications in the real-world conditions. Domain Generalization (DG) techniques aim to enhance such generalization capability of machine learning models, where the learnt feature representation and the classifier are two crucial factors to improve generalization and make decisions. In this paper, we propose Discriminative Adversarial Domain Generalization (DADG) with meta-learning-based cross-domain validation. Our proposed framework contains two main components that work synergistically to build a domain-generalized DNN model: (i) discriminative adversarial learning, which proactively learns a generalized feature representation on multiple \"seen\" domains, and (ii) meta-learning based cross-domain validation, which simulates train/test domain shift via applying meta-learning techniques in the training process. In the experimental evaluation, a comprehensive comparison has been made among our proposed approach and other existing approaches on three benchmark datasets. The results shown that DADG consistently outperforms a strong baseline DeepAll, and outperforms the other existing DG algorithms in most of the evaluation cases.","layer":2,"vector":[0.0001,-0.0465,0.052,-0.0452,0.0432,0.0097,0.0229,-0.0083,-0.0266,-0.0202,-0.0272,-0.0288,0.0354,0.0999,0.0203,0.0128,0.0297,0.0616,-0.0317,-0.0023,0.0401,0.0196,0.0152,-0.0326,-0.0099,0.0102,-0.0281,-0.0151,-0.0728,-0.276,0.0561,-0.0352,0.0312,-0.0083,0.0171,-0.0388,-0.0351,0.0687,-0.0211,0.0252,0.0299,-0.0137,0.0143,-0.0639,-0.0262,-0.0255,-0.0425,-0.0088,-0.0194,-0.0229,0.0449,-0.0277,0.0043,-0.0003,-0.003,0.0472,0.0376,0.0345,0.059,0.0419,0.0071,0.0916,-0.1491,0.0703,0.0066,0.0551,-0.0192,-0.0213,-0.019,0.0437,0.0385,0.0061,-0.0111,0.0743,-0.0353,0.0418,-0.0139,-0.0489,0.0063,0.0274,0.0726,-0.0129,-0.0118,-0.0079,0.0111,-0.0251,-0.0066,-0.0254,0.0253,-0.0021,-0.0314,-0.0217,-0.0212,0.0236,-0.0666,-0.0158,0.069,0.0136,-0.074,0.2004,-0.0089,-0.0096,0.0253,-0.0187,0.0498,-0.01,-0.0526,-0.0022,-0.0051,0.0147,-0.024,0.0012,0.0141,-0.0414,0.0063,0.0184,0.0663,-0.0045,-0.0168,-0.0307,-0.0137,-0.0255,0.0192,-0.0353,0.0124,-0.0382,0.0435,0.1386,0.0279,0.0116,0.0306,-0.0423,-0.0664,-0.0168,0.0482,0.0436,0.0217,0.0395,0.0067,0.0226,-0.0535,-0.0626,-0.0006,-0.0541,-0.0553,0.1002,-0.0378,0.0122,-0.0046,0.0119,0.0215,0.0496,-0.0454,-0.0176,0.0146,0.0402,0.0294,0.042,-0.0075,-0.0266,-0.0077,-0.0018,-0.0377,0.1096,-0.0234,-0.1353,-0.0282,-0.032,0.0178,-0.0474,0.021,0.0209,-0.0433,0.0217,0.0767,-0.0033,-0.0503,-0.0236,-0.0244,-0.0079,0.0659,-0.078,-0.0178,0.0806,0.0578,-0.0332,0.0411,-0.0545,0.0133,0.0133,-0.0465,0.007,-0.0768,0.0043,-0.0315,0.0011,-0.036,-0.0054,-0.001,-0.031,0.0127,0.0483,-0.036,-0.0482,-0.0338,0.0523,-0.0098,-0.0121,0.0284,0.0443,-0.0307,-0.012,0.0685,-0.0478,0.0227,0.0018,0.0197,0.0405,-0.0334,0.0625,0.0484,-0.0397,-0.0356,-0.2339,-0.036,-0.0232,-0.008,0.0594,-0.103,0.0359,0.0367,0.0574,0.06,0.0686,-0.0215,-0.0315,0.0245,0.0157,0.0651,0.032,0.0083,-0.0387,0.0057,-0.036,0.0382,0.0235,-0.108,0.0438,-0.0304,0.1856,0.0205,0.0376,-0.0272,0.0538,0.0328,0.0002,-0.1352,0.0412,0.0109,0.0646,-0.033,-0.0515,-0.0322,-0.0087,0.0462,0.0396,-0.1408,-0.0346,-0.0398,-0.0285,0.0221,-0.0379,0.0611,0.0464,-0.0394,0.0467,0.005,-0.0206,-0.0119,-0.1301,0.0208,-0.0625,0.0078,0.0306,-0.0561,0.0523,-0.0174,0.0405,0.0084,-0.0601,-0.0574,0.0866,-0.0276,-0.0134,0.0736,0.0091,0.0216,0.046,-0.0179,0.0076,-0.0319,-0.0654,0.0001,0.0639,-0.0074,0.0104,0.0366,0.0666,0.0215,0.0728,0.0072,0.0246,-0.0569,-0.0019,0.0068,-0.0706,-0.029,0.0471,0.014,-0.2795,0.0349,0.0278,0.0584,-0.0159,0.0064,0.0272,0.0005,-0.074,0.0134,0.0083,0.0222,0.0601,-0.0277,0.0175,0.0088,0.0247,-0.0363,0.0097,-0.0379,0.0048,0.0316,0.2086,-0.0591,-0.0015,0.0003,-0.065,-0.0181,-0.0257,-0.0071,-0.0075,0.0308,0.0913,-0.0168,0.0114,0.1349,-0.0486,0.0047,0.0395,-0.0186,0.0055,-0.0285,-0.0392,0.0207,0.0521,-0.0014,0.0224,-0.0136,-0.0061,0.0344,-0.0511,0.0012,-0.0228,-0.0317,0.0219,0.0083,-0.0027,-0.0202,-0.0674,-0.0596,0.0174,-0.0199,-0.0194,0.0016,0.0049]}
{"key":"[Variational Quantum Singular Value Decomposition] Singular value decomposition is central to many problems in engineering and scientific fields. Several quantum algorithms have been proposed to determine the singular values and their associated singular vectors of a given matrix. Although these algorithms are promising, the required quantum subroutines and resources are too costly on near-term quantum devices. In this work, we propose a variational quantum algorithm for singular value decomposition (VQSVD). By exploiting the variational principles for singular values and the Ky Fan Theorem, we design a novel loss function such that two quantum neural networks (or parameterized quantum circuits) could be trained to learn the singular vectors and output the corresponding singular values. Furthermore, we conduct numerical simulations of VQSVD for random matrices as well as its applications in image compression of handwritten digits. Finally, we discuss the applications of our algorithm in recommendation systems and polar decomposition. Our work explores new avenues for quantum information processing beyond the conventional protocols that only works for Hermitian data, and reveals the capability of matrix decomposition on near-term quantum devices.","layer":2,"vector":[-0.0943,-0.0003,0.0054,-0.0187,-0.0479,0.0547,0.0568,0.0347,0.0325,0.0063,0.0253,-0.0338,0.0721,0.0553,0.0347,-0.0243,-0.0015,0.0292,-0.0465,0.0087,0.0261,-0.0138,-0.0193,-0.067,0.0055,-0.0062,0.0062,-0.0468,-0.0517,-0.1927,0.0207,-0.0282,0.0556,0.0015,0.0224,-0.0608,-0.0688,-0.005,-0.0446,-0.0027,0.0262,0.0041,-0.0064,-0.0025,-0.0429,-0.046,-0.0367,-0.007,-0.0272,-0.0802,0.0047,0.0598,0.0246,0.0052,0.0444,0.0208,0.0738,0.0643,-0.0025,0.0418,0.0145,0.0579,-0.156,0.0659,0.0759,0.0004,-0.0082,-0.0254,0.0133,0.0385,-0.0416,0.0248,0.0276,-0.0118,0.0443,-0.0276,0.0075,-0.0262,-0.035,0.0187,-0.0089,-0.0293,-0.0325,0.0088,-0.0377,0.0109,-0.005,-0.043,0.0025,-0.0174,-0.0038,-0.0614,-0.0062,0.0219,-0.0376,0.0059,0.0511,0.0506,-0.0319,0.2207,-0.0534,0.0176,0.0467,-0.0472,0.0246,-0.0639,-0.0322,-0.0358,-0.0455,-0.0221,0.0372,-0.028,0.0457,-0.0808,0.0147,0.0071,0.0705,0.0564,0.0025,-0.0296,-0.0822,0.016,0.0342,-0.0168,0.0035,-0.0486,-0.0626,0.1536,0.0253,0.0854,0.0576,-0.008,-0.0098,-0.0663,0.0174,0.024,0.0211,-0.0096,0.0508,-0.0065,-0.0194,-0.0535,0.0011,-0.0795,-0.0352,0.0904,-0.021,0.01,0.0235,-0.0303,0.0224,0.0195,-0.0238,-0.0068,0.0707,0.0697,0.0184,0.0291,-0.0849,0.0155,-0.0199,-0.065,-0.0309,0.1003,-0.0031,-0.058,-0.0146,-0.0113,0.0338,-0.0186,0.0647,0.0454,-0.0219,0.0215,0.0824,-0.0143,-0.0339,0.0376,-0.0126,0.0207,-0.0115,-0.0414,-0.0106,0.0065,0.0536,-0.0393,0.01,-0.0471,0.0189,-0.0295,-0.0515,-0.0264,-0.0305,-0.0264,-0.0066,0.0128,-0.0381,0.0099,-0.0288,-0.0285,0.0615,-0.019,-0.0264,0.0588,0.0057,0.0082,0.0268,-0.014,0.0089,0.0241,-0.0369,-0.0094,0.0432,0.0029,-0.0346,0.0,0.0143,0.016,-0.0052,0.0211,0.0425,-0.1001,-0.0985,-0.2442,-0.0334,-0.0262,-0.0302,0.0976,-0.0708,0.0155,-0.0048,0.0534,0.0773,0.0148,0.0152,0.0009,0.0122,0.0162,0.0695,0.0907,0.0388,-0.051,0.0154,-0.0242,0.0605,-0.0298,-0.0862,0.0571,0.0014,0.2118,0.0416,0.0546,0.0339,0.0262,0.0482,-0.0166,-0.0466,0.0562,0.0057,0.0324,0.0205,-0.0337,-0.0513,-0.0203,0.0014,0.0079,-0.0874,-0.0076,-0.0092,-0.0572,0.0159,-0.0528,0.0552,0.0768,-0.0255,0.008,-0.0165,0.0038,-0.056,-0.0567,0.0102,-0.0334,0.0096,0.0038,-0.0884,-0.0014,-0.0182,0.0725,0.0181,-0.0317,-0.0134,0.0732,-0.0587,-0.0474,0.0479,-0.0365,0.0182,0.0806,-0.0203,0.022,-0.0037,-0.0094,0.0232,0.063,0.0011,0.0258,0.0084,0.0109,-0.0284,0.0588,0.018,0.0192,-0.0269,0.0153,-0.0089,-0.0282,0.0459,-0.0006,-0.0577,-0.3088,0.0382,0.0222,-0.001,-0.0159,0.0254,0.0236,0.0255,-0.0701,-0.0002,-0.0425,0.0096,0.0401,-0.0295,0.0316,0.0422,0.0492,-0.0654,0.0263,-0.0268,0.0284,0.0149,0.2541,-0.018,0.0451,0.0045,-0.0267,0.0004,-0.0051,-0.0561,-0.0231,-0.0289,0.0821,-0.0794,0.0761,0.0479,-0.0598,0.0598,0.0131,-0.0126,-0.0011,-0.0072,-0.0103,-0.0347,0.1106,0.0003,-0.0179,-0.0253,0.007,-0.0032,0.0035,0.0334,-0.0144,-0.0312,0.0141,0.0742,-0.0147,-0.0695,-0.0004,-0.0079,0.0224,-0.0512,-0.0322,0.0009,-0.0231]}
{"key":"[Compositionally-Warped Gaussian Processes] The Gaussian process (GP) is a nonparametric prior distribution over functions indexed by time, space, or other high-dimensional index set. The GP is a flexible model yet its limitation is given by its very nature: it can only model Gaussian marginal distributions. To model non-Gaussian data, a GP can be warped by a nonlinear transformation (or warping) as performed by warped GPs (WGPs) and more computationally-demanding alternatives such as Bayesian WGPs and deep GPs. However, the WGP requires a numerical approximation of the inverse warping for prediction, which increases the computational complexity in practice. To sidestep this issue, we construct a novel class of warpings consisting of compositions of multiple elementary functions, for which the inverse is known explicitly. We then propose the compositionally-warped GP (CWGP), a non-Gaussian generative model whose expressiveness follows from its deep compositional architecture, and its computational efficiency is guaranteed by the analytical inverse warping. Experimental validation using synthetic and real-world datasets confirms that the proposed CWGP is robust to the choice of warpings and provides more accurate point predictions, better trained models and shorter computation times than WGP.","layer":0,"vector":[-0.0417,-0.0484,0.0475,-0.0379,0.0463,0.0337,0.0036,-0.0105,0.0239,-0.0542,0.0251,-0.04,0.0274,0.0234,0.0119,0.0125,-0.0012,0.0482,-0.0788,0.0096,0.066,-0.041,-0.0508,-0.0182,0.0199,0.0385,-0.0401,-0.0249,-0.0288,-0.2499,-0.0046,-0.0298,0.0308,-0.0015,-0.0102,0.0079,-0.0407,0.0721,-0.0361,0.0615,0.052,0.0112,-0.0909,-0.036,0.0427,-0.0754,-0.0499,-0.0087,-0.0222,-0.0241,0.0194,-0.0292,-0.0001,0.0325,0.0557,0.0623,0.0717,0.0336,0.0453,0.0467,-0.0132,0.0683,-0.1463,0.0568,0.048,0.0359,-0.0372,-0.0461,-0.0192,0.01,-0.0517,0.0644,-0.0145,0.0803,0.0374,-0.006,-0.0071,-0.0054,-0.061,-0.0105,0.0385,-0.003,-0.0067,0.0071,-0.0159,-0.0348,-0.0138,-0.0409,0.0439,0.0305,-0.0444,-0.016,-0.0219,0.0546,-0.0829,-0.0099,0.0122,0.037,0.0319,0.1942,-0.066,0.0203,0.0803,-0.0266,0.0785,-0.037,-0.036,-0.007,-0.0064,0.0377,-0.0083,-0.0536,0.0275,-0.0374,0.0166,-0.0177,0.0488,0.0355,-0.024,0.001,-0.0234,0.0189,0.0419,-0.0118,0.0079,-0.0457,0.0219,0.1562,0.044,0.0321,0.0568,-0.0451,-0.0517,-0.0017,0.0358,-0.0459,0.0495,0.0311,0.0704,0.0062,-0.0419,-0.0048,-0.0061,-0.0485,-0.0218,0.121,-0.0484,0.011,-0.0762,0.0144,-0.0518,-0.0042,0.0041,-0.0653,0.0215,0.0223,-0.024,0.0369,-0.0791,0.053,-0.0452,-0.03,-0.0444,0.1183,0.003,-0.0648,-0.0663,0.0812,0.0377,0.0335,0.0297,0.0325,-0.0326,0.0156,0.1009,0.0539,-0.0522,-0.0044,0.0068,0.0082,0.0267,-0.0348,-0.0358,0.0613,0.046,-0.0298,-0.0123,-0.0314,0.0204,0.0036,-0.0292,0.008,-0.0178,-0.0308,-0.0268,-0.0295,-0.0367,-0.0152,-0.0013,-0.0418,0.0066,-0.023,-0.0567,-0.0112,-0.0095,-0.0065,-0.035,0.0287,0.027,0.0692,0.0072,-0.0162,0.0482,0.0035,-0.037,-0.0023,-0.0079,0.0596,-0.0296,0.0124,0.0058,-0.082,-0.0649,-0.2187,0.008,0.0246,-0.0068,0.0178,-0.0235,0.0113,0.0028,0.0726,0.0331,0.0404,-0.0006,-0.0502,0.0153,-0.054,0.029,0.0092,0.039,0.0129,-0.0158,-0.0047,0.0122,-0.0395,-0.1384,0.0768,-0.0087,0.2176,0.0234,0.04,-0.0571,0.0288,-0.0024,-0.0241,-0.0597,0.0331,0.0747,0.0936,0.0129,-0.0844,-0.0294,-0.0559,-0.0195,0.0358,-0.0864,-0.0427,0.0125,-0.0536,0.0366,-0.043,0.0315,0.0386,-0.0415,0.0507,-0.0042,-0.0201,-0.0583,-0.0655,0.0354,-0.03,0.0447,0.0162,-0.0492,0.0276,-0.0467,0.0375,-0.0046,-0.0176,-0.0593,-0.0064,-0.0366,0.0143,0.0681,-0.0155,-0.0054,0.039,0.0273,0.0504,-0.0009,-0.0638,-0.0513,0.075,-0.0318,0.021,-0.0244,0.0406,0.0075,0.0589,-0.0287,0.0092,-0.046,-0.0139,0.0083,-0.0479,0.0119,0.0326,0.0157,-0.287,0.0329,-0.0296,0.0262,-0.0179,-0.0479,0.0219,0.0368,-0.0608,0.0457,-0.0104,0.0338,0.0324,-0.0024,-0.001,0.0167,0.0888,-0.0528,0.0419,-0.0761,-0.0027,0.0505,0.2357,-0.0041,0.0039,0.0147,0.001,0.0166,0.0356,-0.0418,0.0383,0.0237,0.0529,-0.0281,0.0421,0.0594,0.0176,0.0665,0.0244,-0.0292,-0.0128,0.0045,0.0189,-0.0348,0.0964,0.0103,-0.0373,-0.0222,-0.0151,0.0373,-0.0299,0.0601,0.0106,-0.0078,-0.0031,0.0123,-0.0379,-0.0375,-0.0259,-0.0271,-0.0023,-0.0426,-0.0094,-0.0175,-0.0499]}
{"key":"[Autoregressive Quantile Flows for Predictive Uncertainty Estimation] Numerous applications of machine learning involve representing probability distributions over high-dimensional data. We propose autoregressive quantile flows, a flexible class of normalizing flow models trained using a novel objective based on proper scoring rules. Our objective does not require calculating computationally expensive determinants of Jacobians during training and supports new types of neural architectures, such as neural autoregressive flows from which it is easy to sample. We leverage these models in quantile flow regression, an approach that parameterizes predictive conditional distributions with flows, resulting in improved probabilistic predictions on tasks such as time series forecasting and object detection. Our novel objective functions and neural flow parameterizations also yield improvements on popular generation and density estimation tasks, and represent a step beyond maximum likelihood learning of flows.","layer":0,"vector":[-0.0274,0.0097,0.0488,-0.049,0.0442,0.0397,0.0668,0.0028,0.0336,-0.004,-0.0025,-0.0399,0.0331,0.0503,-0.0017,0.01,-0.0262,0.0643,-0.046,-0.0085,0.0268,-0.0261,-0.0173,-0.0619,0.0533,0.0008,-0.0033,-0.0481,-0.0299,-0.2149,0.0182,-0.0441,0.0099,-0.0457,0.0111,-0.0284,-0.062,0.073,-0.0029,0.0477,0.023,0.0306,0.0095,-0.0622,-0.0089,-0.0251,-0.0071,0.0017,-0.0435,-0.0428,0.0316,-0.0255,0.0241,0.0142,0.0323,-0.014,0.0218,0.033,0.0925,0.0827,0.013,0.0405,-0.1668,0.042,0.0489,-0.0094,-0.0421,0.0095,-0.0127,-0.0082,-0.0125,0.0414,0.0227,0.0436,0.0347,-0.0168,0.029,-0.0684,-0.0272,0.0049,0.0392,-0.0067,-0.073,0.0087,-0.0119,-0.0383,0.0373,-0.0189,0.0454,-0.0132,-0.0365,-0.0017,-0.071,0.0125,-0.0789,0.0171,0.0218,-0.0017,-0.0509,0.1811,-0.0439,0.0644,0.0387,-0.0073,0.05,-0.0146,-0.0691,-0.0279,-0.0086,-0.039,-0.001,-0.0569,0.0587,-0.0194,0.0255,-0.013,0.0405,0.0117,-0.0016,0.0057,-0.015,-0.0043,0.0325,-0.0412,0.0137,-0.042,0.0274,0.1526,0.0002,0.0235,0.0411,-0.0273,-0.0894,-0.0236,0.0416,0.0229,0.0224,-0.0264,0.0134,-0.0026,-0.0449,-0.0141,-0.0194,-0.1197,-0.0797,0.1228,-0.0242,0.0292,-0.0197,-0.0485,-0.0251,0.0361,-0.0322,-0.0291,0.0399,0.0625,-0.0177,0.0405,-0.0618,-0.0168,-0.0286,-0.0841,-0.0673,0.075,0.002,-0.0627,0.0043,-0.0046,0.017,0.0324,0.0432,0.037,0.0032,0.0232,0.0771,0.004,-0.0463,0.0068,0.0225,0.0282,0.0037,-0.0543,-0.0218,0.0416,0.0557,-0.0208,-0.0,-0.0339,-0.0027,0.0768,-0.0092,-0.0033,0.0037,0.0002,0.0063,-0.0085,-0.0209,0.0181,0.0107,-0.0724,0.0028,-0.0041,-0.0135,-0.0113,-0.0308,0.0421,-0.0445,-0.0101,0.0637,0.034,0.0185,-0.0566,0.1097,-0.0226,-0.0187,0.0457,-0.0016,0.0631,0.0228,0.0257,0.037,-0.0052,-0.0714,-0.2463,-0.0116,0.0255,-0.0057,0.0692,-0.0487,0.012,-0.0014,0.029,0.1041,0.042,-0.0358,-0.0072,0.0357,-0.0116,0.0474,0.0194,0.0316,-0.0507,-0.0021,-0.0366,0.0017,-0.0747,-0.0904,0.0326,0.0246,0.1858,0.0008,0.054,-0.0306,0.059,0.0228,0.0012,-0.0564,0.0383,0.0567,0.027,-0.0017,-0.0825,-0.0233,-0.0408,0.0037,-0.0252,-0.0756,-0.0485,-0.034,0.0042,0.0466,-0.0767,-0.0096,0.0447,-0.0555,0.08,-0.0506,0.0173,-0.0486,-0.072,0.0434,-0.0034,0.0248,0.0163,-0.04,0.028,-0.0603,0.0284,-0.0103,-0.0099,-0.0129,0.0244,-0.0543,-0.0148,0.0913,-0.0031,-0.0073,0.0908,-0.0166,0.0529,-0.0266,-0.0216,-0.0436,0.0973,-0.0653,0.0676,0.0532,0.0163,0.0151,0.0496,-0.039,0.0522,-0.002,-0.0136,-0.0236,-0.0721,0.0125,0.0347,0.0321,-0.3066,-0.0054,-0.024,0.0319,-0.0299,-0.0303,0.0709,0.0029,-0.0358,0.0192,-0.0325,0.0613,0.0781,-0.0196,0.0001,0.0288,0.0659,-0.0586,0.0661,-0.0293,0.0099,0.0343,0.237,-0.0164,0.04,0.0061,-0.0509,0.0099,0.0417,-0.0414,0.0226,0.0126,0.0818,-0.0525,0.0395,0.0453,-0.0529,0.0393,0.0101,0.0034,-0.0049,-0.0091,-0.0408,-0.0365,0.081,-0.0046,-0.0293,-0.0189,-0.0231,0.0302,-0.0066,0.0183,-0.0458,0.003,0.0332,0.0362,-0.0262,-0.0467,-0.0124,-0.0341,0.0226,-0.1066,0.0152,0.0001,-0.0475]}
{"key":"[Convergence of Stein Variational Gradient Descent under a Weaker Smoothness Condition] Stein Variational Gradient Descent (SVGD) is an important alternative to the Langevin-type algorithms for sampling from probability distributions of the form $\\pi(x) \\propto \\exp(-V(x))$. In the existing theory of Langevin-type algorithms and SVGD, the potential function $V$ is often assumed to be $L$-smooth. However, this restrictive condition excludes a large class of potential functions such as polynomials of degree greater than $2$. Our paper studies the convergence of the SVGD algorithm for distributions with $(L_0,L_1)$-smooth potentials. This relaxed smoothness assumption was introduced by Zhang et al. [2019a] for the analysis of gradient clipping algorithms. With the help of trajectory-independent auxiliary conditions, we provide a descent lemma establishing that the algorithm decreases the $\\mathrm{KL}$ divergence at each iteration and prove a complexity bound for SVGD in the population limit in terms of the Stein Fisher information.","layer":1,"vector":[-0.0547,-0.0423,0.0382,-0.0332,0.0278,0.026,0.0277,0.0654,0.0663,0.0067,0.0103,-0.0217,0.0482,0.0559,0.0035,0.0328,0.0148,0.0378,-0.0345,0.012,0.0382,-0.0218,0.0055,-0.0791,0.0375,-0.0317,0.0008,-0.0509,-0.0331,-0.2377,0.048,-0.0406,0.05,-0.0222,0.0294,-0.0009,-0.0183,0.0587,-0.0281,0.0263,0.005,-0.0006,-0.0772,-0.0438,-0.0173,-0.0494,-0.0349,-0.0391,-0.03,-0.012,-0.0325,-0.0254,0.0281,0.0233,0.0487,0.0498,0.0348,0.0131,0.0379,0.067,0.0057,0.0009,-0.1737,0.0536,0.0385,0.0233,-0.0244,-0.0496,0.055,0.0547,-0.0113,0.0438,0.0101,0.0257,-0.0168,-0.0263,0.0243,-0.051,-0.0163,0.0222,0.0133,-0.0427,-0.0452,-0.0141,-0.0565,-0.0098,0.006,-0.0613,0.0728,-0.0072,-0.0046,-0.0277,-0.0161,0.0386,-0.0756,0.0084,0.058,0.0282,-0.0047,0.1938,-0.0301,0.0668,0.037,-0.0173,-0.0073,-0.022,-0.0154,-0.0569,-0.0208,-0.0115,-0.0264,0.0006,0.0674,-0.0611,-0.0293,-0.0204,0.0133,0.0531,-0.048,-0.0049,-0.0566,0.0243,0.0482,-0.01,0.0605,-0.0335,-0.0255,0.1199,0.0626,0.0449,0.0329,-0.0078,-0.0614,-0.0597,0.0242,0.0288,-0.0382,0.004,0.0246,-0.0123,-0.0285,-0.043,-0.0007,-0.105,-0.0106,0.1201,-0.0561,0.0549,-0.0359,-0.034,0.0024,0.0028,-0.0093,-0.0317,0.0106,0.0108,0.0162,0.0544,-0.071,0.0597,-0.0172,-0.0246,0.0028,0.1456,0.0126,-0.0646,-0.0139,0.0473,0.0308,0.0173,0.036,0.0534,-0.0469,0.0085,0.0761,0.0001,-0.1197,0.0047,-0.01,-0.0198,0.0002,-0.0392,-0.0086,0.0281,0.0188,-0.0383,0.0192,-0.0449,0.0451,0.0482,-0.0482,-0.0265,0.0196,0.0003,-0.0085,-0.0331,-0.0222,-0.0225,0.0042,-0.0082,-0.0,-0.0313,-0.0211,0.0448,0.0071,0.0211,-0.004,-0.017,0.0063,0.0463,-0.0378,-0.0488,0.0569,0.0007,-0.0192,0.0657,-0.0105,-0.0055,-0.008,0.0884,0.0258,-0.0391,-0.0709,-0.2053,-0.0554,-0.014,-0.0168,0.0487,-0.0632,0.0487,-0.019,0.081,0.0692,0.0247,-0.0249,-0.0134,0.0066,0.0288,0.0814,0.0373,-0.0054,-0.0041,0.0206,-0.0125,0.0309,-0.0333,-0.0235,0.069,-0.0334,0.1903,0.0253,0.0313,-0.0357,-0.016,0.0337,-0.0023,-0.0579,0.0434,0.0382,0.0655,-0.0277,-0.0208,0.0006,-0.0111,0.0117,-0.0276,-0.0566,-0.0662,-0.0487,-0.0607,0.0412,-0.0515,-0.0001,0.0497,-0.0323,0.0709,-0.0526,0.065,-0.0375,-0.073,0.0157,-0.0236,0.0517,-0.0346,-0.0672,0.0263,-0.0321,0.0316,0.0341,-0.0186,-0.0328,0.0017,-0.031,-0.0207,0.0424,0.0227,0.0321,0.0615,0.0241,-0.0041,-0.0246,-0.0536,-0.0329,0.0387,-0.0341,0.0057,0.008,0.0035,-0.0111,0.1204,-0.0325,-0.01,-0.0025,-0.0202,-0.0117,-0.0982,0.0192,0.0146,-0.0172,-0.2977,0.0368,0.0246,0.0257,-0.0109,0.0582,0.0762,-0.038,-0.0676,0.0282,-0.0345,0.061,0.039,-0.0328,0.0631,0.0326,0.0463,-0.048,0.0532,-0.0893,0.0349,0.0706,0.2156,-0.0807,-0.0048,-0.0015,0.0111,0.0265,0.0132,-0.0623,-0.0258,-0.0232,0.0952,-0.0618,0.0862,0.1094,-0.0448,0.0428,0.0143,-0.0561,0.0287,0.0002,-0.0303,-0.0074,0.071,-0.0442,-0.001,-0.0083,-0.0204,0.0053,0.0075,0.0414,0.0179,-0.0113,0.0441,0.0107,-0.0605,-0.0384,-0.0411,-0.0195,0.0604,-0.0765,-0.0591,0.0046,0.0034]}
{"key":"[There is no Accuracy-Interpretability Tradeoff in Reinforcement Learning for Mazes] Interpretability is an essential building block for trustworthiness in reinforcement learning systems. However, interpretability might come at the cost of deteriorated performance, leading many researchers to build complex models. Our goal is to analyze the cost of interpretability. We show that in certain cases, one can achieve policy interpretability while maintaining its optimality. We focus on a classical problem from reinforcement learning: mazes with $k$ obstacles in $\\mathbb{R}^d$. We prove the existence of a small decision tree with a linear function at each inner node and depth $O(\\log k + 2^d)$ that represents an optimal policy. Note that for the interesting case of a constant $d$, we have $O(\\log k)$ depth. Thus, in this setting, there is no accuracy-interpretability tradeoff. To prove this result, we use a new \"compressing\" technique that might be useful in additional settings.","layer":2,"vector":[-0.0508,-0.0056,0.0415,-0.0116,-0.0123,0.0366,0.0389,0.0316,0.0361,-0.0004,0.0512,-0.0223,0.0557,0.0548,0.0061,0.0324,-0.0345,0.0641,-0.0252,0.0378,0.0496,-0.0655,-0.0119,-0.0909,-0.0076,0.0345,-0.0319,-0.0366,-0.0153,-0.222,0.0283,-0.0485,0.0064,-0.0229,0.0266,0.029,-0.0308,0.0359,-0.0585,0.012,0.0573,0.0334,-0.022,-0.0542,-0.0251,-0.0494,-0.0199,-0.0282,-0.0297,-0.053,0.0152,-0.0051,0.0478,0.0056,0.0558,0.0234,0.0317,0.0579,0.031,0.0472,-0.0184,0.0455,-0.1606,0.0192,0.037,0.023,-0.0594,-0.0333,0.0203,0.083,0.0125,0.0045,0.009,0.0173,0.0378,-0.0378,-0.006,-0.0359,0.0113,0.0175,0.0052,-0.0349,-0.065,-0.0052,-0.0527,-0.0944,0.0159,-0.0756,0.0474,0.015,0.0092,0.0024,-0.0486,0.0056,-0.0512,-0.0014,0.0288,0.0088,-0.1018,0.175,-0.0287,0.0546,0.0022,0.0065,0.0243,-0.035,-0.0312,-0.0345,0.0171,0.0005,-0.0218,-0.0281,0.0731,0.0162,-0.0183,0.0253,0.0603,0.0156,0.0181,0.0347,-0.0106,-0.0108,0.0867,-0.0299,0.0383,-0.0615,-0.0034,0.1349,0.0393,0.0333,0.0314,-0.0595,-0.0036,0.008,0.0236,0.0176,0.0455,0.0261,0.0016,-0.0401,-0.057,-0.0035,0.0456,-0.1113,-0.0542,0.096,-0.0158,0.0196,-0.0563,0.0026,-0.0088,-0.0022,0.0111,-0.0333,0.0155,0.0228,0.0252,0.0221,-0.0551,0.0066,0.0093,-0.0404,-0.0092,0.1032,-0.0105,-0.0415,-0.0314,0.0137,0.0247,-0.0743,0.038,0.045,-0.0339,0.0042,0.0475,-0.0091,-0.1253,-0.0476,-0.0203,-0.0057,0.0603,-0.0792,-0.0569,0.0244,0.0338,-0.0016,0.0155,-0.0483,0.0397,0.0294,-0.0175,0.0149,-0.0389,0.0001,-0.0591,-0.0168,0.0207,-0.0274,-0.0093,-0.0145,0.0031,-0.0138,-0.0398,0.0323,-0.0236,0.0426,0.0103,-0.0275,0.0334,-0.0207,-0.047,0.0344,0.0353,-0.048,-0.0214,0.0285,0.0453,0.0021,0.0157,0.0245,0.0205,-0.0364,-0.0606,-0.2391,-0.0324,0.0021,0.0174,0.0394,-0.057,0.0148,-0.025,-0.0127,0.0682,0.0338,-0.0666,-0.0183,0.0517,0.0142,0.031,0.0182,0.0186,-0.0274,0.021,-0.026,0.0258,-0.0167,-0.0976,-0.0074,0.0303,0.2524,0.007,0.0237,-0.0098,0.0319,0.0468,-0.0536,-0.0948,0.0498,0.0467,0.0618,-0.0032,0.0305,-0.0576,0.0048,0.0392,-0.0198,-0.0964,-0.0459,-0.0064,-0.0228,0.07,-0.0418,0.0144,0.0395,-0.0283,0.0362,-0.0054,0.0361,-0.013,-0.0721,0.032,-0.0343,0.0505,-0.0415,-0.0464,0.0072,-0.053,0.0532,0.0037,0.0024,-0.0269,0.0213,-0.0132,-0.0046,0.0304,-0.0021,-0.0529,0.0854,0.0203,0.0204,-0.0046,-0.0573,-0.0043,0.0815,-0.0488,0.0332,0.0035,0.0176,0.0107,0.0448,-0.0383,0.021,-0.0452,-0.0002,0.015,-0.0552,-0.0237,0.0544,-0.0192,-0.3241,0.0543,0.0002,-0.0045,-0.0383,0.0052,0.0648,-0.0009,-0.0329,-0.0292,0.0453,0.0798,-0.0062,0.0004,0.0324,0.0409,0.0965,-0.0203,0.06,-0.0544,0.0662,0.0699,0.2124,-0.0494,0.0191,-0.0299,-0.0289,-0.0107,0.0349,0.0003,-0.0082,0.0055,0.0569,-0.0376,0.0716,0.0764,-0.0642,0.0559,-0.0013,-0.0066,-0.0302,-0.0068,-0.0248,-0.036,0.0939,0.0132,-0.0425,-0.0199,-0.018,0.0399,-0.0053,0.0257,0.0098,0.0035,0.0466,0.0671,-0.0193,-0.0888,-0.0231,-0.0536,0.0042,-0.0687,0.0419,0.0229,-0.0062]}
{"key":"[Use of the Deep Learning Approach to Measure Alveolar Bone Level] Abstract: Aim: The goal was to use a Deep Convolutional Neural Network to measure the radiographic alveolar bone level to aid periodontal diagnosis. Material and methods: A Deep Learning (DL) model was developed by integrating three segmentation networks (bone area, tooth, cementoenamel junction) and image analysis to measure the radiographic bone level and assign radiographic bone loss (RBL) stages. The percentage of RBL was calculated to determine the stage of RBL for each tooth. A provisional periodontal diagnosis was assigned using the 2018 periodontitis classification. RBL percentage, staging, and presumptive diagnosis were compared to the measurements and diagnoses made by the independent examiners. Results: The average Dice Similarity Coefficient (DSC) for segmentation was over 0.91. There was no significant difference in RBL percentage measurements determined by DL and examiners (p=0.65). The Area Under the Receiver Operating Characteristics Curve of RBL stage assignment for stage I, II and III was 0.89, 0.90 and 0.90, respectively. The accuracy of the case diagnosis was 0.85. Conclusion: The proposed DL model provides reliable RBL measurements and image-based periodontal diagnosis using periapical radiographic images. However, this model has to be further optimized and validated by a larger number of images to facilitate its application.","layer":1,"vector":[-0.0792,-0.0017,0.035,-0.0486,0.0035,0.0195,0.0244,0.0599,0.0322,0.0223,0.0146,-0.0668,0.0801,0.0736,-0.0032,-0.0337,0.0116,0.0269,-0.0444,0.0391,0.0456,-0.0142,-0.0023,-0.0604,0.0379,0.0359,-0.015,-0.0203,-0.0546,-0.191,0.0111,-0.0377,0.0781,-0.0584,-0.0028,-0.0098,-0.0605,0.0299,0.0033,0.0158,0.07,0.0099,-0.0474,-0.0472,-0.0089,-0.0465,-0.025,-0.0299,0.009,0.0111,0.076,-0.0418,0.0253,0.0631,-0.0327,0.0375,0.0874,0.0552,0.0683,0.0393,0.0592,0.0311,-0.2333,0.0686,0.0313,0.04,0.0086,-0.0356,0.0171,0.042,-0.0164,0.0389,0.0399,0.0172,0.0119,-0.0003,0.0421,-0.005,-0.0111,-0.0171,-0.0366,0.0361,-0.0457,-0.0202,-0.0268,-0.0623,0.0064,-0.1025,0.0061,-0.0049,-0.0309,0.0118,-0.0641,0.0525,-0.061,0.0053,0.0453,0.0464,-0.0833,0.1744,-0.0576,-0.0043,0.0503,-0.0497,0.0451,-0.0321,-0.028,0.0009,-0.0216,0.0301,-0.018,-0.0423,0.0212,0.0154,0.0087,-0.0087,0.0486,0.0825,0.0137,-0.004,-0.0405,-0.0079,0.0339,-0.0367,0.0347,-0.0313,0.0224,0.1212,0.0072,0.0367,0.0468,-0.052,-0.0311,0.02,0.0269,0.0187,0.0107,-0.0324,-0.0123,-0.005,-0.0376,-0.0701,0.0289,-0.0574,-0.0422,0.1043,-0.0682,0.0713,-0.0209,-0.0833,-0.0127,0.025,-0.0503,-0.0482,0.0217,-0.0165,0.0324,0.018,-0.0649,0.0078,0.0306,-0.0541,-0.0565,0.1104,0.0072,-0.0798,-0.0377,-0.0082,0.0578,-0.0299,0.0558,0.0493,-0.0312,-0.0427,0.0593,0.0925,-0.0662,0.0068,-0.0068,-0.0037,0.0224,-0.028,-0.0364,-0.0041,0.0691,-0.0739,0.0131,-0.0505,0.04,0.0415,-0.0127,0.0467,-0.0464,0.0216,0.0063,-0.0172,-0.0239,-0.0106,-0.0208,0.0175,0.0032,-0.0213,0.0027,0.0043,0.0305,0.0377,-0.0268,0.0287,0.0311,0.0414,-0.0413,-0.0068,0.0613,-0.0124,-0.0497,-0.0056,0.0114,-0.0071,-0.0313,0.0652,0.0358,-0.0359,-0.0546,-0.2089,-0.0258,0.0127,-0.0209,0.0369,-0.0472,0.0509,-0.0196,0.0401,0.0131,0.0845,0.0495,0.0349,-0.0089,0.01,0.0449,0.06,0.0488,-0.0657,-0.0477,0.0237,0.0226,0.0295,-0.0941,0.0016,0.0047,0.1748,0.0144,0.0008,0.0297,0.0259,0.063,-0.015,-0.1187,0.0406,-0.0112,0.0811,0.0016,-0.0812,-0.0548,-0.0439,0.0038,0.0112,-0.0718,-0.0263,-0.0377,-0.0078,0.0603,-0.0791,-0.0059,0.0427,-0.0283,0.0444,-0.0103,-0.0073,-0.0341,-0.1259,-0.0093,-0.0405,-0.0356,0.0035,-0.0724,0.0235,-0.0493,0.0265,-0.0027,-0.0386,-0.0288,0.0059,-0.0324,-0.0237,0.0831,0.0151,0.0003,0.0693,-0.0128,0.0632,-0.025,-0.0434,-0.011,0.0738,-0.0266,0.0307,0.0289,0.0649,0.0271,0.0739,0.0056,0.0223,0.0216,0.0056,-0.0023,-0.0331,-0.0594,0.0187,0.0124,-0.283,0.0368,0.0142,0.0487,-0.038,-0.0212,0.0413,-0.0036,0.0005,-0.0045,0.0166,0.0053,0.0535,-0.0758,-0.0039,-0.0044,0.0612,-0.0593,0.0568,-0.0477,0.0263,0.045,0.2011,-0.0637,0.0091,0.0178,-0.0235,-0.0235,0.0284,-0.0208,0.0392,-0.0006,0.0818,-0.0591,0.0312,0.1074,-0.0113,0.0216,0.0045,-0.0612,0.0516,0.04,-0.0385,0.0082,0.0825,-0.0501,-0.004,-0.0269,0.0194,0.0262,-0.0254,-0.0043,-0.0136,-0.01,0.0175,0.0085,-0.0106,-0.0515,-0.0059,-0.0676,0.0231,-0.1057,-0.0293,0.0172,-0.0198]}
{"key":"[Application of Quantum Annealing to Training of Deep Neural Networks] In Deep Learning, a well-known approach for training a Deep Neural Network starts by training a generative Deep Belief Network model, typically using Contrastive Divergence (CD), then fine-tuning the weights using backpropagation or other discriminative techniques. However, the generative training can be time-consuming due to the slow mixing of Gibbs sampling. We investigated an alternative approach that estimates model expectations of Restricted Boltzmann Machines using samples from a D-Wave quantum annealing machine. We tested this method on a coarse-grained version of the MNIST data set. In our tests we found that the quantum sampling-based training approach achieves comparable or better accuracy with significantly fewer iterations of generative training than conventional CD-based training. Further investigation is needed to determine whether similar improvements can be achieved for other data sets, and to what extent these improvements can be attributed to quantum effects.","layer":2,"vector":[-0.067,0.0165,0.0056,-0.0029,0.0074,0.037,0.0374,0.0082,0.0382,-0.0152,0.0194,-0.0544,0.013,0.0625,0.0631,0.0644,-0.0043,0.0027,-0.0842,-0.0165,0.006,-0.0327,-0.0106,-0.058,0.0382,-0.0056,0.0195,-0.0193,-0.0343,-0.2169,0.0094,-0.0462,0.0287,-0.0148,0.0313,-0.0121,-0.0544,0.0334,0.0004,0.0616,0.0323,0.0141,-0.028,-0.031,-0.0107,-0.0474,-0.0706,-0.02,-0.0394,-0.0357,0.049,0.0009,0.0389,0.0362,0.0691,0.0278,0.0869,0.0317,0.0251,0.0543,-0.0009,0.035,-0.1349,0.0473,0.0595,0.0417,-0.027,-0.0494,0.0337,0.052,-0.0271,0.0403,-0.0104,0.0114,0.014,-0.0177,0.0145,-0.0086,0.0062,-0.0217,-0.0133,-0.0473,-0.0753,-0.027,-0.0383,0.0122,-0.0052,-0.0223,0.0296,0.0121,-0.0104,-0.0318,-0.0514,0.0014,-0.0423,0.0355,0.0143,0.0392,-0.037,0.2005,-0.0307,0.051,0.0302,0.0066,0.0237,-0.0562,-0.0836,-0.0101,-0.0734,0.0048,0.0113,-0.0445,0.0219,-0.0495,0.0164,0.0129,0.0796,0.0227,-0.0123,-0.0118,-0.074,-0.0105,0.0279,-0.0227,-0.0295,-0.0741,-0.0381,0.1545,-0.0064,0.0269,0.0324,-0.0665,-0.0282,-0.0355,0.0273,0.0021,0.0042,-0.0005,0.0379,0.0066,-0.012,-0.0392,-0.0114,-0.0982,-0.069,0.0702,-0.0685,0.0192,-0.0258,-0.0241,0.0003,0.0348,-0.0403,-0.0537,0.0571,0.0729,0.0037,0.0722,-0.0746,0.0265,-0.0496,-0.0815,-0.0272,0.0905,0.0501,-0.0383,-0.0548,0.0046,0.0195,-0.0447,0.0367,0.0421,-0.0506,0.04,0.0618,-0.0092,-0.0693,0.0336,0.0034,0.0305,-0.0406,-0.0615,-0.0361,0.0248,0.0488,-0.042,0.0334,-0.0538,-0.0044,0.0184,-0.0216,0.0228,-0.0188,-0.0056,-0.0181,0.0066,-0.032,0.0174,-0.0231,-0.0184,0.0106,-0.0078,-0.0297,0.0034,-0.0195,-0.0038,0.027,-0.0263,0.058,0.0147,-0.0623,-0.0117,0.0632,-0.0427,-0.034,-0.0198,0.0136,0.0141,-0.0271,0.0288,0.0382,-0.0663,-0.0885,-0.238,0.0047,0.0372,-0.046,0.0982,-0.0622,0.0273,0.0232,0.0292,0.0734,-0.0005,0.001,0.019,-0.0199,-0.0035,0.0356,0.0779,0.0348,0.0065,0.0612,-0.0117,0.0339,-0.0171,-0.0837,0.0275,-0.0035,0.2021,0.0459,0.0349,0.0163,0.0125,0.052,-0.047,-0.0908,0.0617,0.0125,0.0665,0.0214,-0.0619,-0.0386,-0.0093,0.0121,-0.0184,-0.0838,-0.0133,0.0244,-0.0279,0.0299,-0.0524,0.0098,0.0763,-0.036,0.0426,-0.046,-0.0157,-0.0539,-0.0912,0.0122,-0.0611,0.0015,0.0445,-0.0298,0.0244,-0.0297,0.0007,-0.0397,-0.0302,-0.0124,0.0696,-0.0217,-0.0281,0.07,-0.0194,0.0111,0.0621,-0.0282,0.0083,-0.0262,-0.0536,0.0091,0.0678,0.0197,0.0505,0.0217,0.0069,0.0256,0.0738,-0.01,0.0026,0.0222,0.009,0.0254,-0.055,0.0565,0.0315,-0.0112,-0.2791,0.0665,-0.0046,0.045,-0.0285,-0.0182,0.0401,0.0766,-0.0514,-0.0334,-0.0345,0.0571,0.0411,-0.0058,0.0037,0.0271,0.0717,-0.0414,0.0543,-0.0266,0.0169,0.0652,0.2724,-0.0023,0.0281,0.0385,-0.024,0.0108,0.0007,-0.061,-0.0229,-0.0207,0.0811,-0.0703,0.0735,0.0782,-0.0281,0.0384,0.0335,0.0037,0.0035,-0.0019,-0.0259,-0.0168,0.1138,0.0038,-0.0154,-0.0439,-0.0135,-0.0107,-0.0344,0.0125,-0.0012,-0.0147,0.0225,0.066,-0.0204,-0.0558,-0.015,0.0059,0.0709,-0.0675,-0.0385,-0.0048,-0.0169]}
{"key":"[Data-driven detrending of nonstationary fractal time series with echo state networks] In this paper, we propose a novel data-driven approach for removing trends (detrending) from nonstationary, fractal and multifractal time series. We consider real-valued time series relative to measurements of an underlying dynamical system that evolves through time. We assume that such a dynamical process is predictable to a certain degree by means of a class of recurrent networks called Echo State Network (ESN), which are capable to model a generic dynamical process. In order to isolate the superimposed (multi)fractal component of interest, we define a data-driven filter by leveraging on the ESN prediction capability to identify the trend component of a given input time series. Specifically, the (estimated) trend is removed from the original time series and the residual signal is analyzed with the multifractal detrended fluctuation analysis procedure to verify the correctness of the detrending procedure. In order to demonstrate the effectiveness of the proposed technique, we consider several synthetic time series consisting of different types of trends and fractal noise components with known characteristics. We also process a real-world dataset, the sunspot time series, which is well-known for its multifractal features and has recently gained attention in the complex systems field. Results demonstrate the validity and generality of the proposed detrending method based on ESNs.","layer":0,"vector":[-0.0693,-0.0013,0.0401,-0.0273,0.0591,0.0054,0.0131,-0.0003,0.0423,-0.0112,0.0107,-0.0334,0.0516,0.072,0.0055,-0.0109,0.0029,0.0406,-0.0352,0.0123,0.05,0.0069,-0.0325,-0.0278,0.0358,0.0327,-0.0036,-0.0565,-0.089,-0.2496,0.0397,-0.068,0.0349,-0.0252,-0.0081,-0.0426,-0.0402,0.0801,-0.0191,0.084,0.0228,0.0207,-0.007,-0.0594,-0.0756,-0.0403,0.0179,-0.0244,-0.0161,-0.0237,-0.0208,-0.0092,0.0377,0.014,0.0116,0.0075,0.0263,0.0316,0.087,0.0474,0.0317,0.0295,-0.202,0.0471,0.0518,0.0326,0.0203,-0.0146,0.0248,0.0216,-0.0073,0.0169,-0.0183,0.0512,0.0416,-0.0037,-0.0613,-0.0454,-0.0328,0.0323,0.0174,-0.0267,-0.0347,-0.0392,-0.0085,-0.0165,0.017,-0.0492,0.0243,-0.0142,-0.0559,-0.0223,-0.0258,0.0244,-0.0429,-0.0203,0.0279,0.0378,0.0175,0.1827,-0.0348,0.062,0.0537,-0.0079,0.0197,-0.0496,-0.0198,-0.0619,-0.0275,0.0152,-0.0411,-0.0157,0.0712,-0.0449,0.0584,-0.019,0.0397,0.0421,0.0256,0.0026,-0.021,-0.0085,0.037,-0.0368,0.0159,-0.0278,0.0551,0.125,0.0281,0.0163,-0.0157,0.0088,-0.0783,-0.0349,0.0572,0.0319,-0.0033,-0.0334,0.0281,0.0293,-0.0299,-0.0491,0.0284,-0.1236,-0.0711,0.1199,-0.0086,-0.0352,-0.0441,0.0267,-0.0716,0.0339,0.0061,-0.0514,0.0321,0.0172,0.0321,0.0292,-0.0758,0.0071,-0.0354,-0.0564,-0.0213,0.1143,0.0419,-0.0737,-0.0368,-0.0075,0.0094,0.0015,0.0251,-0.0289,-0.0195,0.0153,0.1351,0.0349,-0.0471,-0.0036,-0.0146,-0.0294,0.0356,-0.0394,-0.0102,0.0469,0.0518,-0.0426,0.0357,-0.0255,0.0353,0.0444,-0.0141,-0.0205,-0.0112,0.0543,-0.0342,-0.0377,0.0096,-0.0059,0.0217,-0.0677,-0.0233,-0.0208,-0.0084,0.0087,0.0174,0.0424,-0.0195,0.0211,-0.0052,0.0119,0.0002,0.0123,0.0603,-0.0337,-0.0484,0.0285,0.0033,-0.0021,-0.0056,0.057,0.084,-0.036,-0.0919,-0.222,0.031,0.0353,-0.0226,0.0793,-0.0519,0.0124,-0.0488,0.1326,0.0772,-0.0108,0.0162,0.0101,-0.0067,0.0291,0.0538,0.0377,0.0167,-0.0284,-0.0155,-0.0071,-0.0147,-0.0261,-0.093,0.0596,0.0104,0.1742,-0.0118,0.0293,-0.0372,0.0097,-0.0233,-0.0386,-0.0648,0.0596,0.0608,0.0831,-0.0231,-0.0621,-0.0547,-0.0559,-0.0005,-0.0185,-0.0816,-0.042,-0.0242,-0.04,0.0189,-0.074,0.0033,0.0305,-0.01,0.0293,-0.0021,0.0328,-0.0221,-0.0421,0.0383,-0.004,0.0094,0.0027,-0.0108,0.0009,-0.0522,0.0524,0.0693,-0.0503,-0.0189,0.0205,-0.0233,-0.0246,0.136,0.0247,0.0299,0.0279,0.0075,-0.0001,-0.0153,-0.0414,-0.0486,0.0659,-0.0586,0.0486,0.0694,0.013,-0.0096,0.0944,-0.0101,-0.0028,-0.039,0.0369,-0.003,-0.0252,-0.0249,0.0258,-0.0298,-0.2773,0.0085,-0.0364,0.0367,0.0204,-0.0172,-0.0192,0.0674,0.0011,-0.0258,-0.0517,0.0262,0.0556,-0.0097,0.0181,0.0312,0.0493,-0.0567,0.0052,-0.038,0.0393,0.0464,0.2241,-0.0189,0.0248,-0.0164,-0.0167,-0.013,0.0067,-0.024,0.0273,0.0092,0.092,-0.0513,0.0183,0.0559,-0.0209,0.0852,0.0029,-0.0224,0.0312,-0.013,0.0017,-0.0183,0.0915,-0.0491,-0.045,-0.0648,0.002,0.0465,-0.0522,-0.0021,-0.0278,0.0463,-0.0182,0.044,-0.0453,-0.0155,-0.0112,-0.0365,0.0098,-0.0764,-0.0264,-0.0039,-0.0362]}
{"key":"[A Data Driven Method of Optimizing Feedforward Compensator for Autonomous Vehicle] A reliable controller is critical and essential for the execution of safe and smooth maneuvers of an autonomous vehicle.The controller must be robust to external disturbances, such as road surface, weather, and wind conditions, and so on.It also needs to deal with the internal parametric variations of vehicle sub-systems, including power-train efficiency, measurement errors, time delay,so on.Moreover, as in most production vehicles, the low-control commands for the engine, brake, and steering systems are delivered through separate electronic control units.These aforementioned factors introduce opaque and ineffectiveness issues in controller performance.In this paper, we design a feed-forward compensate process via a data-driven method to model and further optimize the controller performance.We apply the principal component analysis to the extraction of most influential features.Subsequently,we adopt a time delay neural network and include the accuracy of the predicted error in a future time horizon.Utilizing the predicted error,we then design a feed-forward compensate process to improve the control performance.Finally,we demonstrate the effectiveness of the proposed feed-forward compensate process in simulation scenarios.","layer":0,"vector":[-0.0101,0.0204,0.0633,-0.0333,0.0316,0.0639,0.0213,0.0351,0.0185,-0.0035,0.017,-0.047,0.0422,0.0167,-0.0188,0.0065,-0.0005,0.0526,-0.0017,-0.0028,0.0018,-0.0636,-0.0186,-0.0434,-0.0134,0.0056,-0.0472,-0.0177,-0.0558,-0.2574,-0.0483,-0.0657,0.0581,-0.0201,-0.039,-0.0134,-0.081,0.0678,-0.0032,-0.0024,0.0181,0.0347,-0.0104,-0.0395,0.0041,-0.0128,-0.0079,0.0185,-0.0035,-0.059,-0.0047,-0.0211,0.0191,0.0149,0.0386,0.0493,0.0229,0.0344,0.0533,0.0077,-0.0096,0.0499,-0.193,0.0722,0.0477,0.0487,-0.0086,-0.0414,-0.0029,0.071,-0.0507,0.0114,-0.0138,0.0683,0.013,-0.0071,-0.0024,-0.0342,-0.0312,0.0314,0.0165,-0.0234,-0.0802,0.0199,-0.0464,-0.0435,0.0201,-0.0565,0.0549,0.0333,-0.0124,-0.0455,-0.0118,-0.0068,-0.0536,0.0067,0.0315,-0.0241,-0.0577,0.1974,-0.0184,0.0402,0.0111,-0.0165,0.0109,-0.0439,0.0151,-0.0207,-0.0563,-0.0101,0.0033,-0.0205,-0.0276,-0.0391,0.0027,0.0106,0.0262,0.049,0.0259,-0.0145,0.0094,-0.0291,0.062,0.0037,0.0473,-0.0927,0.0461,0.1326,-0.0007,0.0688,0.0467,-0.0508,-0.0625,-0.0578,0.0296,0.0027,0.0427,0.0204,0.0142,0.0179,-0.0576,-0.0638,-0.0027,-0.0955,-0.0524,0.0867,-0.026,0.0184,-0.0239,-0.0319,-0.0447,0.0232,-0.0159,0.0073,0.0307,0.0479,0.0178,0.0093,-0.0453,0.0125,-0.0272,-0.0341,-0.0346,0.067,-0.0031,-0.0798,-0.04,0.0104,0.0358,0.0106,0.0138,0.0232,-0.0678,0.0246,0.0862,0.022,-0.0717,0.0187,-0.0252,0.013,0.0614,-0.0517,-0.0292,0.018,0.0692,-0.0236,-0.0176,-0.0094,-0.0159,0.0308,-0.0295,-0.0293,-0.0385,0.0145,-0.0493,-0.0126,0.015,-0.0171,0.0186,-0.0781,0.0032,0.0228,-0.0215,0.0104,-0.0151,0.0185,-0.0166,0.0065,0.0503,0.0362,-0.0471,-0.0208,0.1105,-0.0343,-0.0454,-0.0038,-0.0265,0.067,0.0128,0.0649,0.0525,0.0061,-0.0327,-0.2234,-0.0141,-0.0351,0.002,0.0635,-0.0776,-0.002,-0.014,0.0534,0.0247,0.084,-0.01,0.012,0.0662,-0.03,0.0715,0.0286,0.0231,-0.0491,0.0165,-0.0409,0.0071,0.0023,0.004,0.0273,-0.035,0.1297,-0.0267,0.0791,-0.0258,0.0212,0.0532,-0.0104,-0.0221,0.0859,-0.0153,0.0751,-0.0074,-0.0592,-0.0557,0.0113,0.0319,-0.0141,-0.0734,-0.0461,-0.0353,-0.0399,0.0265,-0.0773,-0.0084,0.0286,-0.0346,-0.0211,-0.0017,0.0088,-0.0167,-0.05,0.0975,-0.048,0.0075,-0.0151,-0.0614,-0.0133,-0.044,0.0526,0.0023,0.0211,-0.0374,0.0349,0.0314,-0.0314,0.1373,0.0332,0.0369,0.0731,-0.0292,-0.0143,-0.0043,-0.0411,0.0182,0.0455,-0.0138,0.072,0.0533,0.0359,-0.0063,0.0674,-0.0314,-0.0198,0.0157,0.0479,0.0171,-0.046,-0.0142,0.0783,0.0355,-0.2915,0.0416,0.0256,0.005,-0.0202,-0.0316,0.039,-0.0191,-0.0622,-0.0298,-0.0348,0.0803,0.0408,0.0189,0.0437,0.0765,0.0183,-0.0569,0.0578,-0.0953,0.0211,0.0715,0.2162,-0.0488,0.0108,0.0572,-0.0446,-0.0246,0.067,-0.0277,-0.0064,0.0091,0.0899,-0.0349,0.0315,0.0934,-0.0799,0.0422,0.0012,0.0301,-0.0024,0.0659,0.0423,-0.0428,0.0872,-0.0207,-0.0367,-0.0606,-0.0059,0.0427,-0.0028,0.0074,-0.0434,-0.0115,0.0256,0.018,-0.0758,-0.0525,-0.0329,-0.0353,0.0166,-0.0732,-0.0056,-0.014,0.0041]}
{"key":"[Multi-output Polynomial Networks and Factorization Machines] Factorization machines and polynomial networks are supervised polynomial models based on an efficient low-rank decomposition. We extend these models to the multi-output setting, i.e., for learning vector-valued functions, with application to multi-class or multi-task problems. We cast this as the problem of learning a 3-way tensor whose slices share a common basis and propose a convex formulation of that problem. We then develop an efficient conditional gradient algorithm and prove its global convergence, despite the fact that it involves a non-convex basis selection step. On classification tasks, we show that our algorithm achieves excellent accuracy with much sparser models than existing methods. On recommendation system tasks, we show how to combine our algorithm with a reduction from ordinal regression to multi-output classification and show that the resulting algorithm outperforms simple baselines in terms of ranking accuracy.","layer":7,"vector":[-0.0779,-0.0142,-0.0162,-0.0154,0.0122,0.0269,0.0231,0.0394,0.0313,-0.0183,0.018,-0.0504,0.0149,0.0767,0.0389,0.0032,0.0221,0.0944,-0.0501,-0.0131,0.0515,-0.0565,-0.0066,-0.0757,0.0497,0.0165,-0.021,-0.036,-0.0458,-0.2097,0.0037,-0.0192,0.0672,-0.017,0.0548,-0.0136,0.0043,0.0356,-0.0661,0.0152,0.0179,0.0143,-0.0533,-0.0318,-0.0221,-0.0287,-0.0051,-0.0136,-0.0567,-0.0463,0.0084,-0.0563,0.0352,0.0273,0.0316,0.0217,0.0051,0.0575,0.0049,0.0607,0.0712,0.0364,-0.1692,0.0179,0.052,0.0059,-0.046,-0.0387,-0.0273,0.0765,-0.0058,0.0531,0.0328,0.0194,-0.0031,-0.001,0.0454,-0.0278,-0.0134,0.0434,0.0054,-0.0477,-0.0467,-0.0183,-0.0049,-0.0141,0.0197,-0.0283,-0.0075,-0.0123,-0.0542,-0.0124,-0.0129,0.0505,-0.0678,-0.0366,0.0321,0.0446,-0.0466,0.1968,-0.0553,0.0468,0.0395,-0.033,-0.0117,-0.0283,-0.0151,-0.0259,-0.0346,0.0039,-0.0283,0.0074,0.0132,-0.061,-0.0025,-0.0089,0.0654,0.0509,-0.0029,-0.0021,-0.0203,0.0351,0.0599,-0.0188,0.0516,-0.0675,-0.0008,0.1451,0.0264,0.087,0.0195,-0.012,-0.0749,-0.0306,0.0289,0.0049,0.0095,-0.0094,0.0533,0.018,-0.0298,-0.0136,0.022,-0.0617,-0.0454,0.1436,-0.0654,-0.0267,-0.047,-0.0516,-0.0212,0.0385,-0.0232,-0.0437,0.0052,0.037,0.0522,0.0078,-0.0745,0.0235,-0.0209,-0.043,-0.0393,0.0974,0.021,-0.0885,-0.019,0.011,-0.0069,-0.0018,0.0435,-0.001,0.019,0.0022,0.0955,0.0355,-0.0606,-0.0056,0.0256,-0.0121,0.0158,-0.0332,-0.0199,0.0504,0.0148,0.0042,-0.0059,-0.0388,-0.0046,-0.0074,-0.0479,0.0197,-0.0146,-0.0388,-0.0438,-0.0047,-0.0309,0.0371,0.0076,-0.0283,0.0105,0.011,-0.018,0.0466,-0.0163,0.0406,-0.0209,0.0068,0.0279,0.029,-0.0219,-0.0054,0.0361,-0.0588,-0.0765,0.0184,0.0562,0.0389,-0.032,0.0642,0.077,-0.0646,-0.0726,-0.2351,-0.0152,0.0187,0.0146,0.0581,-0.1057,0.045,-0.0468,0.0506,0.1237,0.0537,0.0031,-0.0315,0.0414,0.0122,0.0486,0.0482,-0.0025,0.0126,-0.009,0.0025,0.0173,0.0225,-0.0604,0.0378,-0.0031,0.241,0.0368,0.0095,-0.0049,0.0369,0.0319,-0.0165,-0.0728,0.0843,-0.0095,0.103,-0.0594,-0.0342,-0.0306,-0.0272,-0.0011,0.0048,-0.0513,-0.0241,-0.0383,-0.0427,-0.0117,-0.0452,0.0613,0.0439,-0.0239,0.0349,-0.0479,-0.0218,-0.0577,-0.0809,0.0144,-0.0599,0.0501,0.0346,-0.0771,0.0136,-0.0588,0.0729,0.0207,-0.0541,-0.017,0.0095,-0.0338,0.0127,0.042,-0.0011,0.0085,0.0643,0.0055,0.0651,-0.0183,-0.051,-0.0038,0.0915,-0.0302,0.0228,-0.0166,-0.0023,-0.0127,0.0823,0.0073,0.0264,-0.0241,-0.0466,-0.0209,-0.0399,0.0448,0.0861,-0.0383,-0.2893,0.0114,0.0137,0.0425,-0.0249,0.016,0.0401,0.0016,-0.0362,0.0153,0.0428,0.0444,0.0597,-0.0441,-0.0046,0.0312,0.0474,-0.0481,-0.0016,-0.0424,0.0297,0.0537,0.1938,-0.0352,0.0505,-0.005,-0.0137,-0.0425,0.0197,0.0036,0.0487,0.0079,0.0981,-0.0612,0.0347,0.0689,-0.0286,0.0078,0.0374,-0.0395,-0.0185,-0.0309,-0.0768,-0.0381,0.0798,-0.018,-0.0042,-0.0307,0.0152,-0.0027,0.005,0.0254,-0.0238,-0.008,0.0365,0.0346,-0.0425,-0.0729,-0.02,-0.0199,-0.0107,-0.058,-0.0184,-0.0239,-0.0454]}
{"key":"[PRRS Outbreak Prediction via Deep Switching Auto-Regressive Factorization Modeling] We propose an epidemic analysis framework for the outbreak prediction in the livestock industry, focusing on the study of the most costly and viral infectious disease in the swine industry -- the PRRS virus. Using this framework, we can predict the PRRS outbreak in all farms of a swine production system by capturing the spatio-temporal dynamics of infection transmission based on the intra-farm pig-level virus transmission dynamics, and inter-farm pig shipment network. We simulate a PRRS infection epidemic based on the shipment network and the SEIR epidemic model using the statistics extracted from real data provided by the swine industry. We develop a hierarchical factorized deep generative model that approximates high dimensional data by a product between time-dependent weights and spatially dependent low dimensional factors to perform per farm time series prediction. The prediction results demonstrate the ability of the model in forecasting the virus spread progression with average error of NRMSE = 2.5\\%.","layer":3,"vector":[-0.0103,-0.0424,0.0258,0.0097,0.0671,0.0073,0.0213,0.0127,0.0588,0.008,-0.0274,-0.0491,0.0515,0.045,0.0303,-0.0132,-0.0198,0.0222,-0.0285,-0.0005,0.027,-0.0183,-0.0477,-0.0953,0.0168,0.0201,-0.0066,0.0158,-0.0721,-0.2435,-0.009,-0.0692,0.0358,-0.0277,0.0153,-0.0286,0.0152,0.0028,-0.0324,0.0464,0.0114,0.0206,-0.0108,-0.0292,-0.0211,-0.0597,-0.0598,-0.0188,-0.0088,-0.0463,0.0196,-0.0548,0.0048,0.0288,0.0275,-0.018,0.0329,0.0038,0.028,0.0625,0.0433,0.0484,-0.1969,0.056,0.0536,0.0208,-0.0436,0.0155,-0.0075,0.0246,-0.0604,0.0466,-0.0105,0.0339,0.0331,0.015,0.0108,-0.0137,-0.0109,-0.0021,0.021,-0.0082,-0.0308,-0.0405,-0.0229,-0.0459,0.0243,-0.0415,0.0548,-0.0282,-0.0609,0.0133,0.0062,-0.0027,-0.079,0.0052,0.0406,0.0039,-0.039,0.1858,-0.0636,0.0079,0.0259,-0.0188,0.0052,-0.0416,-0.0323,-0.005,-0.0209,0.0225,-0.0043,-0.0185,0.0229,-0.0633,0.0351,-0.0166,0.0886,0.0402,0.003,0.009,0.0185,0.0697,0.0444,-0.0065,0.0342,-0.0662,0.0323,0.1934,-0.0306,0.0116,0.0385,0.0046,-0.0553,0.0145,0.0185,-0.0103,0.013,-0.0037,0.0111,-0.012,-0.0132,-0.0578,0.0176,-0.0817,-0.0632,0.0715,-0.0466,0.0156,-0.0438,-0.0882,-0.0126,0.0174,-0.0193,-0.0272,0.0196,0.0326,0.0073,0.0049,-0.0612,-0.0021,-0.0552,-0.0437,-0.0464,0.0755,0.0106,-0.0717,0.0133,0.005,0.0465,-0.0073,0.0097,0.0534,-0.0433,0.009,0.0879,0.0223,-0.0619,-0.0197,0.0008,0.0234,0.0339,-0.0467,-0.0328,0.0766,0.0302,-0.0342,-0.0136,-0.0469,0.0105,0.0217,-0.0206,0.033,-0.0204,0.009,-0.0044,-0.013,-0.0414,0.0524,0.0583,-0.0297,0.0037,-0.0057,-0.0534,0.0156,-0.0043,0.018,0.0241,0.01,-0.0035,0.0375,-0.0135,-0.0061,0.0709,-0.0457,-0.0281,0.0258,-0.0077,0.0112,-0.0102,0.0467,0.0438,-0.0062,-0.0389,-0.2385,0.0011,0.037,-0.0473,0.055,-0.0698,0.0027,-0.0604,0.0707,0.0892,0.0741,-0.0124,-0.0422,0.0325,0.0109,0.084,0.0174,0.0377,-0.0545,0.0039,-0.0061,-0.0251,0.0014,-0.0854,0.0414,0.0271,0.1737,0.0193,0.0438,-0.0077,0.0098,0.0361,-0.014,-0.072,0.0635,0.0026,0.0847,-0.0051,-0.0736,-0.0365,-0.0292,0.0473,-0.0444,-0.0551,-0.0252,-0.0329,-0.0176,0.0173,-0.0551,0.0248,0.0439,-0.0665,0.0844,0.0169,0.0187,-0.0802,-0.0585,0.0654,-0.0341,0.0088,0.0103,-0.054,0.0231,-0.0943,0.0317,0.0042,-0.0089,-0.0293,-0.0022,-0.0083,-0.037,0.0948,0.0387,0.0219,0.0563,0.0144,0.0134,-0.0533,-0.0896,-0.0323,0.0836,-0.0782,0.009,0.0516,0.0572,-0.0085,0.0633,0.0343,0.0602,-0.0037,0.0285,-0.0119,-0.0804,0.0046,0.0145,-0.0151,-0.3108,0.0228,-0.0114,0.0522,-0.0301,0.0054,0.0436,0.0174,-0.0068,-0.0417,0.0299,0.0242,0.0932,-0.0174,0.0047,0.0224,0.0441,-0.0373,0.0755,-0.0559,0.0157,-0.0125,0.2442,-0.0235,0.058,0.0102,-0.007,0.0127,0.0172,-0.0157,0.0418,-0.005,0.1095,-0.0556,0.0433,0.0329,-0.0066,0.0428,0.0132,0.0002,-0.0559,0.0276,-0.0004,-0.0065,0.0551,-0.0091,-0.0247,-0.0677,0.0207,0.0713,-0.0578,-0.0272,-0.0205,0.0185,0.0084,0.0067,0.0195,-0.0435,-0.0316,-0.0074,0.0452,-0.0189,-0.0178,-0.0011,-0.0213]}
{"key":"[Multiview Based 3D Scene Understanding On Partial Point Sets] Deep learning within the context of point clouds has gained much research interest in recent years mostly due to the promising results that have been achieved on a number of challenging benchmarks, such as 3D shape recognition and scene semantic segmentation. In many realistic settings however, snapshots of the environment are often taken from a single view, which only contains a partial set of the scene due to the field of view restriction of commodity cameras. 3D scene semantic understanding on partial point clouds is considered as a challenging task. In this work, we propose a processing approach for 3D point cloud data based on a multiview representation of the existing 360{\\deg} point clouds. By fusing the original 360{\\deg} point clouds and their corresponding 3D multiview representations as input data, a neural network is able to recognize partial point sets while improving the general performance on complete point sets, resulting in an overall increase of 31.9% and 4.3% in segmentation accuracy for partial and complete scene semantic understanding, respectively. This method can also be applied in a wider 3D recognition context such as 3D part segmentation.","layer":6,"vector":[-0.0069,-0.0467,0.0526,0.0022,0.0614,0.0338,0.0411,0.0065,-0.0254,0.025,0.0074,-0.0966,0.0308,0.0506,0.0094,0.001,0.01,0.0661,-0.0498,0.0003,-0.0201,-0.0212,-0.0101,-0.0207,0.016,0.0194,0.0009,-0.0184,-0.0351,-0.2348,0.0214,-0.0155,0.0413,-0.0204,-0.0158,-0.0378,-0.0057,0.0448,0.0093,0.0295,0.0351,-0.0316,-0.0327,-0.0614,-0.0393,-0.0162,-0.0013,-0.0082,0.0171,-0.0558,0.0535,-0.0595,0.0387,-0.0134,0.0184,0.0617,0.0246,0.0499,-0.0089,-0.01,0.0806,0.0517,-0.1513,0.0494,0.0386,0.0371,-0.0075,-0.0734,0.0206,0.0381,-0.0538,0.0253,0.036,0.0266,0.0038,-0.0314,-0.0229,-0.0181,-0.0533,-0.0337,0.0337,0.0202,-0.0266,0.008,0.0013,-0.0123,0.0251,-0.0442,-0.0031,0.0146,-0.0263,-0.003,-0.0326,0.0339,-0.0278,-0.0259,0.0647,0.0067,-0.0481,0.2045,-0.0651,0.0362,0.056,-0.0417,0.0259,-0.0318,-0.0259,-0.0468,0.0056,0.0251,-0.0375,0.0085,0.017,0.0027,0.012,-0.0219,0.0611,0.0548,-0.0063,-0.0431,0.0005,0.0333,0.0472,-0.0294,0.0029,-0.0569,0.0322,0.1417,0.0202,0.0106,0.0752,0.0106,-0.0972,0.0021,0.0399,0.0466,0.0388,0.0019,0.0225,-0.0204,-0.0351,-0.0533,0.0291,-0.0603,-0.0321,0.1165,-0.0561,0.0327,-0.0889,-0.06,-0.0078,0.0837,-0.0453,-0.0167,0.0205,0.0214,0.03,0.0293,-0.0439,0.0566,-0.0318,-0.0818,-0.0098,0.1029,0.0157,-0.1207,-0.0621,0.0025,-0.0125,-0.0341,0.0235,0.0597,-0.0307,-0.0207,0.0832,0.052,-0.1244,0.0049,-0.0167,0.0684,0.0301,-0.1028,0.0002,0.0024,0.0191,-0.0736,0.0121,-0.0349,-0.0009,0.0633,-0.0413,0.0385,-0.0207,-0.0154,0.0499,-0.0287,-0.0148,0.0081,-0.0355,-0.055,0.0161,0.0024,-0.027,0.0292,-0.0331,0.0332,-0.0269,0.0175,-0.0128,0.057,-0.0277,0.0053,0.0022,-0.0412,0.0217,0.0104,0.0064,0.0041,-0.0564,0.0383,0.013,-0.0735,-0.0518,-0.2082,0.0224,0.0146,-0.0475,0.0234,-0.055,0.0164,0.0091,0.0528,0.0487,0.0945,-0.0291,0.0196,0.0224,0.038,0.0391,0.012,0.077,-0.0733,-0.0025,-0.0042,0.032,-0.0737,-0.0728,0.0675,0.0062,0.2677,0.0138,0.0362,-0.0276,0.0506,0.0195,-0.0567,-0.0923,0.0479,0.0074,0.05,-0.0007,-0.0479,-0.0292,-0.0218,0.0359,-0.0002,-0.0866,-0.0161,-0.0716,-0.0107,0.0525,-0.0101,0.0076,0.0516,-0.0659,-0.0239,0.025,-0.0373,0.0416,-0.0777,0.017,-0.0346,0.0329,-0.0118,-0.0699,0.0104,-0.0507,0.0517,-0.0096,-0.0585,-0.0467,-0.0169,-0.0132,-0.0399,0.0411,-0.003,-0.0197,0.0476,0.0253,0.0707,-0.001,-0.0175,-0.054,0.0551,-0.0371,0.0097,0.0225,0.0451,0.0302,0.058,-0.0278,-0.0141,-0.0518,0.0374,0.0058,-0.0459,-0.0062,0.0907,-0.0292,-0.266,0.0072,0.0157,0.0331,-0.0101,0.0218,0.0237,0.0129,-0.0272,-0.0178,-0.0229,0.0408,0.0261,-0.0412,0.0103,0.0386,0.0559,-0.0334,0.0534,-0.0177,-0.0187,0.0642,0.2062,-0.0405,0.0129,0.0018,-0.0395,0.0022,0.0456,0.0384,-0.0109,0.0331,0.081,-0.0558,0.0135,0.0992,-0.0028,0.0431,0.0543,0.0158,0.0264,0.0157,-0.0497,-0.064,0.0562,0.0403,-0.0143,-0.0235,0.0224,-0.002,-0.0515,0.037,0.0038,0.0044,0.0481,0.0153,-0.0232,-0.0658,-0.0769,0.0042,-0.0118,-0.0575,0.0008,-0.0243,0.0105]}
{"key":"[A Comparative study of Hyper-Parameter Optimization Tools] Most of the machine learning models have associated hyper-parameters along with their parameters. While the algorithm gives the solution for parameters, its utility for model performance is highly dependent on the choice of hyperparameters. For a robust performance of a model, it is necessary to find out the right hyper-parameter combination. Hyper-parameter optimization (HPO) is a systematic process that helps in finding the right values for them. The conventional methods for this purpose are grid search and random search and both methods create issues in industrial-scale applications. Hence a set of strategies have been recently proposed based on Bayesian optimization and evolutionary algorithm principles that help in runtime issues in a production environment and robust performance. In this paper, we compare the performance of four python libraries, namely Optuna, Hyper-opt, Optunity, and sequential model-based algorithm configuration (SMAC) that has been proposed for hyper-parameter optimization. The performance of these tools is tested using two benchmarks. The first one is to solve a combined algorithm selection and hyper-parameter optimization (CASH) problem The second one is the NeurIPS black-box optimization challenge in which a multilayer perception (MLP) architecture has to be chosen from a set of related architecture constraints and hyper-parameters. The benchmarking is done with six real-world datasets. From the experiments, we found that Optuna has better performance for CASH problem and HyperOpt for MLP problem.","layer":1,"vector":[-0.0415,0.0377,0.0344,0.0332,0.0146,0.0188,-0.0068,0.0339,0.0473,-0.0167,0.04,-0.0817,-0.0156,0.0504,0.0119,-0.0389,0.0324,0.0134,-0.0354,-0.0096,0.0274,-0.0455,-0.0375,-0.1001,0.0511,-0.0155,-0.0601,0.0046,-0.0641,-0.2735,0.0214,-0.0446,0.0599,-0.026,0.0003,0.0111,-0.0224,0.0419,0.0028,0.0555,0.0026,0.0309,-0.0058,-0.0697,-0.02,-0.0322,0.0169,-0.0224,-0.0324,-0.0087,0.0018,-0.0287,0.0079,0.0547,0.0193,0.014,0.052,0.0522,0.0416,0.0443,0.0035,0.034,-0.152,0.0205,0.0568,0.0305,-0.0671,-0.0421,0.0129,0.0329,-0.0473,0.0253,0.0118,0.0172,0.0059,0.0183,-0.0088,-0.0376,0.0439,0.006,0.0212,-0.0303,-0.0277,-0.0186,-0.0006,-0.0092,-0.011,0.0267,0.0518,-0.0075,-0.0479,-0.016,-0.0283,-0.0107,-0.069,0.0263,0.0414,0.0379,-0.0739,0.2135,-0.0214,-0.0019,0.007,-0.026,0.0316,-0.026,-0.0656,0.0091,-0.0242,-0.0212,0.0078,-0.0317,0.0047,0.0019,-0.0304,0.0537,-0.026,0.0075,-0.021,-0.009,-0.014,-0.0083,0.0747,0.0045,0.0135,-0.0362,0.0358,0.1282,-0.005,0.0258,0.051,-0.0444,-0.0302,-0.0515,0.0337,0.0144,0.0416,-0.0085,-0.0005,-0.0032,-0.0786,-0.0532,0.0218,-0.0642,-0.0149,0.1182,-0.062,0.0477,-0.0706,-0.023,-0.0125,0.0139,-0.0091,-0.0105,0.0228,0.0089,-0.0148,0.0692,-0.0042,0.0157,-0.0217,-0.0342,-0.0563,0.0801,-0.0374,-0.112,-0.0442,-0.0137,0.0005,0.0137,0.0168,0.0437,-0.0431,0.0376,0.0719,0.0272,-0.0823,0.0107,-0.0167,0.0395,0.0353,-0.0285,-0.0459,-0.0145,0.065,-0.0619,0.0313,-0.0564,0.0042,0.0373,-0.0432,-0.0036,-0.0024,0.0013,-0.0062,-0.0382,-0.0029,0.0173,0.0595,-0.0445,0.0041,0.0404,-0.0379,0.0456,-0.0111,0.0192,-0.0119,-0.0214,0.0835,0.048,-0.0713,0.0106,0.0799,-0.0185,-0.047,-0.0254,0.0125,0.0855,0.0313,0.0661,0.0322,-0.0204,-0.0499,-0.2373,0.0293,0.003,-0.0221,0.1105,-0.0384,0.0297,-0.0102,0.0513,0.0591,0.071,-0.0497,-0.0137,0.0572,-0.0303,0.027,0.0038,0.0332,-0.0506,0.0173,-0.018,0.0298,-0.014,-0.1037,0.0179,-0.025,0.1804,-0.0118,0.0308,-0.0333,0.0132,-0.0057,-0.0059,-0.0962,0.019,0.0574,0.0624,-0.0065,-0.0392,-0.0238,-0.0426,0.0457,-0.0387,-0.0978,-0.0523,-0.0438,-0.0366,0.0481,-0.0806,0.0221,0.0339,-0.0094,0.046,-0.0321,-0.0245,-0.0546,-0.1007,-0.0023,-0.0294,0.016,0.0329,-0.0505,0.009,-0.0381,0.0418,-0.0382,0.0149,-0.0069,0.0243,-0.0259,-0.0119,0.0993,0.0319,0.0226,0.0775,0.0127,0.0308,-0.0208,-0.0113,-0.0099,0.0678,0.0138,0.0442,0.0601,0.0636,-0.0277,0.1009,-0.0227,0.0304,0.0032,-0.0315,-0.0254,-0.0612,0.0339,0.0619,0.0445,-0.2996,0.0061,0.0298,0.0492,-0.0373,0.0363,0.0302,-0.0026,0.0064,-0.0153,0.0005,0.0435,0.0807,0.0132,0.0082,-0.0062,0.0569,-0.0566,0.0701,-0.0231,-0.0184,0.038,0.2287,-0.0463,0.0141,0.0245,-0.0188,-0.0191,0.0158,-0.034,0.02,0.0062,0.0669,-0.0709,0.0247,0.0853,-0.0532,0.0247,0.021,0.0175,0.0249,0.0105,-0.0527,-0.0308,0.1016,-0.0091,-0.018,-0.0473,-0.0083,0.0123,-0.0501,0.0194,-0.0356,0.0035,0.0337,-0.0007,-0.038,-0.0347,-0.0414,-0.0152,0.0244,-0.0339,0.0006,-0.0186,-0.0081]}
{"key":"[The Vision of Self-Evolving Computing Systems] Computing systems are omnipresent; their sustainability has become crucial for our society. A key aspect of this sustainability is the ability of computing systems to cope with the continuous change they face, ranging from dynamic operating conditions, to changing goals, and technological progress. While we are able to engineer smart computing systems that autonomously deal with various types of changes, handling unanticipated changes requires system evolution, which remains in essence a human-centered process. This will eventually become unmanageable. To break through the status quo, we put forward an arguable opinion for the vision of self-evolving computing systems that are equipped with an evolutionary engine enabling them to evolve autonomously. Specifically, when a self-evolving computing system detects conditions outside its operational domain, such as an anomaly or a new goal, it activates an evolutionary engine that runs online experiments to determine how the system needs to evolve to deal with the changes, thereby evolving its architecture. During this process the engine can integrate new computing elements that are provided by computing warehouses. These computing elements provide specifications and procedures enabling their automatic integration. We motivate the need for self-evolving computing systems in light of the state of the art, outline a conceptual architecture of self-evolving computing systems, and illustrate the architecture for a future smart city mobility system that needs to evolve continuously with changing conditions. To conclude, we highlight key research challenges to realize the vision of self-evolving computing systems.","layer":10,"vector":[-0.0462,-0.0177,0.0635,-0.0209,0.014,0.0236,0.063,0.0347,0.0178,-0.0045,0.0254,-0.0116,0.0557,0.0044,-0.0036,0.0124,-0.0228,0.0097,-0.0102,-0.0158,0.0128,-0.0413,-0.0143,-0.0624,-0.0167,0.0495,-0.0381,-0.0172,-0.0412,-0.204,-0.0228,-0.0696,0.044,-0.0195,0.0575,-0.023,-0.0126,0.0399,0.0014,0.0164,0.0104,0.0029,-0.0034,-0.0322,-0.0322,-0.0616,0.0109,0.0118,-0.0495,-0.0295,0.0001,-0.0208,0.0104,0.0175,0.0066,0.0562,0.0495,0.0176,0.0383,-0.0424,0.0598,0.0374,-0.1543,0.0912,0.0784,-0.0009,-0.0157,-0.0891,0.0376,0.0349,-0.0295,0.0341,-0.0005,0.044,0.0301,0.0029,0.0205,0.0288,0.022,0.0345,0.0138,-0.0463,-0.0298,0.005,-0.041,-0.0401,-0.018,-0.0167,0.0614,0.0206,-0.0512,0.0277,0.0114,0.0172,-0.034,-0.0122,0.0182,-0.0095,-0.0417,0.231,-0.0358,0.0366,0.0355,-0.0272,0.0209,-0.0652,-0.0194,-0.0854,-0.0413,-0.0097,0.003,-0.0192,0.0167,0.0099,0.0319,-0.0165,-0.0001,0.0531,0.0009,0.0078,-0.0669,0.0177,0.038,-0.0418,0.0205,-0.0714,0.0196,0.1219,-0.0319,0.0241,0.0448,-0.009,-0.0097,-0.0254,0.0266,0.0237,-0.0179,-0.0064,-0.0056,-0.0095,-0.028,-0.0339,0.0451,-0.1132,-0.0318,0.0931,0.0173,0.0377,-0.0353,-0.0193,-0.024,-0.0177,-0.04,-0.0135,-0.0107,0.0475,0.026,0.0541,-0.0681,0.02,-0.0444,0.0151,-0.058,0.1251,0.0146,-0.0895,0.0056,0.0281,0.044,0.0022,0.0148,0.0236,-0.046,0.021,0.0716,0.0353,-0.0653,0.0465,-0.0203,-0.0035,0.0468,-0.0413,-0.0595,0.0259,0.0409,-0.0732,-0.0277,-0.0338,0.0185,0.0448,-0.0276,0.0593,0.0176,-0.0222,-0.0122,-0.0678,0.0028,0.0014,0.0281,-0.065,-0.0083,0.039,-0.0522,0.0053,0.0184,-0.0289,-0.0436,-0.0209,0.0322,0.0348,-0.026,0.0181,0.0742,0.0262,-0.0312,-0.0044,0.0002,0.0293,0.0358,0.0041,0.0196,-0.0059,-0.0431,-0.2019,-0.0203,-0.0297,-0.0673,0.0251,0.0032,0.0178,-0.0368,0.0406,0.0084,0.08,-0.0065,0.0206,0.0446,0.0073,0.0156,0.0381,0.0121,-0.0192,-0.0088,0.0073,0.0301,0.0068,-0.1407,0.0293,0.005,0.2511,-0.0009,0.0355,0.0088,0.0307,-0.0262,-0.0413,-0.1348,0.0988,0.037,0.0508,-0.0548,-0.0642,-0.0096,-0.0355,0.0434,-0.0305,-0.0727,-0.0514,-0.0279,-0.0046,-0.0031,-0.0228,0.0444,0.013,-0.0065,0.0274,0.0021,-0.0312,-0.0411,-0.0221,0.0459,0.0258,0.0035,-0.0309,0.0051,-0.0151,-0.0453,0.0911,-0.0266,-0.0066,-0.068,0.0327,-0.0326,-0.0245,0.0823,-0.0005,-0.0354,0.021,0.0243,-0.0034,-0.0007,0.0046,-0.0329,0.0308,-0.0387,0.0377,0.0317,0.0052,0.0063,0.0375,0.021,-0.0089,-0.0379,0.0335,0.025,-0.0375,-0.0693,0.0869,-0.0202,-0.2954,0.0393,-0.0021,0.0144,-0.0464,0.0275,0.0283,0.0685,-0.0539,0.0282,-0.0353,0.0529,0.0201,0.0005,0.0416,0.0068,0.1013,-0.0345,0.0232,-0.0994,0.0176,0.0525,0.2462,-0.031,0.0621,0.014,-0.0306,0.0348,0.0291,-0.044,-0.0135,-0.0169,0.0706,-0.0637,0.0162,0.043,-0.053,0.0457,0.007,0.0265,-0.0251,0.037,0.0099,-0.0017,0.1297,0.0101,-0.0627,-0.0632,0.0023,0.0577,-0.0201,-0.0339,-0.0335,0.0071,0.0394,0.0045,-0.0674,-0.0496,-0.073,-0.0643,0.1009,-0.0507,0.0516,0.0074,0.0024]}
{"key":"[Problem-dependent attention and effort in neural networks with an application to image resolution] This paper introduces a new neural network-based estimation approach that is inspired by the biological phenomenon whereby humans and animals vary the levels of attention and effort that they dedicate to a problem depending upon its difficulty. The proposed approach leverages alternate models' internal levels of confidence in their own projections. If the least costly model is confident in its classification, then that is the classification used; if not, the model with the next lowest cost of implementation is run, and so on. This use of successively more complex models -- together with the models' internal propensity scores to evaluate their likelihood of being correct -- makes it possible to substantially reduce resource use while maintaining high standards for classification accuracy. The approach is applied to the digit recognition problem from Google's Street View House Numbers dataset, using Multilayer Perceptron (MLP) neural networks trained on high- and low-resolution versions of the digit images. The algorithm examines the low-resolution images first, only moving to higher resolution images if the classification from the initial low-resolution pass does not have a high degree of confidence. For the MLPs considered here, this sequential approach enables a reduction in resource usage of more than 50\\% without any sacrifice in classification accuracy.","layer":2,"vector":[-0.0376,-0.033,0.0457,-0.0406,0.0243,0.0118,0.0537,0.0268,0.0127,-0.0349,0.0116,-0.0606,0.0056,0.0678,0.0047,0.0056,0.0149,0.0362,-0.0049,0.0411,-0.0046,-0.0047,0.0092,-0.0525,0.0006,-0.0043,-0.028,-0.052,-0.0471,-0.237,0.0307,-0.0589,0.0902,-0.0047,0.0058,-0.0518,-0.0469,0.0697,-0.013,0.0527,0.0218,-0.0043,-0.0462,-0.0674,-0.0262,-0.0114,-0.0303,-0.0332,-0.0423,-0.0595,-0.0149,-0.0202,0.0109,0.0269,0.001,0.0435,0.052,0.0514,0.0551,0.0386,0.0146,0.0486,-0.1921,0.0513,0.0477,0.032,-0.0246,-0.0533,-0.0037,0.0394,-0.0449,0.0409,-0.0304,0.0442,-0.0027,-0.0519,-0.0044,-0.0341,0.0216,-0.0003,0.0293,-0.0102,-0.0383,-0.0275,0.0112,-0.0239,-0.0028,-0.004,0.0533,-0.0128,-0.0325,-0.008,-0.0476,0.0147,-0.049,-0.0305,0.0077,0.014,-0.0795,0.1898,-0.0324,0.0256,0.0418,-0.0837,0.0589,-0.0114,-0.0028,-0.0202,-0.1054,0.0016,-0.0118,-0.0242,0.01,0.013,0.0188,-0.0119,0.031,0.0204,0.02,-0.0193,0.0075,-0.033,0.0048,0.0105,0.0251,-0.0697,0.0229,0.1562,0.0176,0.0489,0.029,-0.069,-0.0558,-0.0171,0.0123,0.0352,0.051,0.0067,-0.0154,-0.0293,-0.0449,-0.0566,0.0261,-0.0354,-0.07,0.1163,-0.0395,0.0248,-0.0253,-0.053,-0.0171,0.0376,-0.0581,-0.0251,0.0252,0.0102,0.0263,-0.0171,-0.0757,0.017,0.0054,-0.02,-0.0762,0.08,0.0218,-0.0379,-0.0304,-0.0712,-0.0248,-0.018,0.0274,0.0486,-0.0218,0.0634,0.0317,0.0465,-0.0938,0.0075,-0.006,0.04,0.0278,-0.0558,-0.0172,0.0085,0.0421,-0.0449,0.001,-0.0239,0.0092,0.0406,-0.0612,0.0476,-0.0041,-0.0035,-0.0152,-0.0434,-0.042,0.0252,0.0377,-0.0309,-0.0034,-0.0057,0.0034,0.0001,0.0084,0.0403,-0.0075,0.0429,0.0367,0.0474,-0.0352,0.0103,0.0923,-0.013,-0.0418,0.0089,0.052,0.0296,0.035,0.0652,0.0525,-0.0645,-0.054,-0.2299,-0.0062,0.0306,-0.0062,0.0698,-0.1029,0.0263,-0.0077,0.0117,0.0554,0.0774,-0.0139,-0.018,0.0489,-0.0061,0.0611,0.0345,0.0171,-0.0486,-0.0416,-0.0096,0.0318,0.0016,-0.0947,0.0153,0.0125,0.2106,0.0117,0.0215,-0.0097,0.0334,0.0254,-0.082,-0.0928,0.0586,-0.0116,0.0444,-0.0029,-0.0116,-0.0095,-0.0422,0.0334,0.0249,-0.0875,-0.0415,0.016,-0.03,0.0499,-0.0591,-0.0155,0.0202,-0.009,0.061,-0.0473,-0.0118,-0.0066,-0.0712,0.0147,-0.0218,0.0267,0.0288,-0.1172,0.0185,-0.0328,0.1044,0.0045,-0.0443,-0.0417,0.0337,0.0007,0.0062,0.0735,0.0268,-0.0217,0.0147,-0.0248,0.0465,-0.0453,-0.0254,0.0144,0.0447,-0.0212,0.0317,0.0064,0.0864,0.0231,0.0938,-0.056,0.0552,-0.0021,0.036,0.0402,-0.0458,-0.0072,0.0343,0.0173,-0.2843,0.0597,0.0117,0.0337,-0.0131,0.0387,0.0021,0.0689,-0.0178,-0.0116,-0.0372,0.0253,0.0608,-0.0202,-0.003,0.0177,-0.0046,-0.0174,0.0678,-0.0377,0.0381,0.0601,0.217,-0.0587,-0.0034,0.0046,0.0017,-0.0003,0.0185,-0.0274,0.0385,0.0228,0.0807,-0.0653,0.0211,0.0852,-0.0477,0.0211,0.02,0.0083,0.009,0.0241,-0.0696,-0.0378,0.1473,0.0285,-0.0013,-0.0144,-0.0229,0.0255,-0.0202,-0.0079,0.0097,-0.0053,0.0439,0.0183,-0.039,-0.0378,-0.0331,-0.0001,0.0366,-0.0803,-0.0148,-0.0243,-0.0302]}
{"key":"[Performance Indicators Contributing To Success At The Group And Play-Off Stages Of The 2019 Rugby World Cup] Performance indicators that contributed to success at the group stage and play-off stages of the 2019 Rugby World Cup were analysed using publicly available data obtained from the official tournament website using both a non-parametric statistical technique, Wilcoxon's signed rank test, and a decision rules technique from machine learning called RIPPER. Our statistical results found that ball carry effectiveness (percentage of ball carries that penetrated the opposition gain-line) and total metres gained (kick metres plus carry metres) were found to contribute to success at both stages of the tournament and that indicators that contributed to success during the group stages (dominating possession, making more ball carries, making more passes, winning more rucks, and making less tackles) did not contribute to success at the play-off stage. Our results using RIPPER found that low ball carries and a low lineout success percentage jointly contributed to losing at the group stage, while winning a low number of rucks and carrying over the gain-line a sufficient number of times contributed to winning at the play-off stage of the tournament. The results emphasise the need for teams to adapt their playing strategies from the group stage to the play-off stage at tournament in order to be successful.","layer":0,"vector":[-0.0579,0.0221,0.0379,-0.0193,0.0306,0.0532,0.0483,0.0377,0.0272,0.0295,0.0455,-0.0738,0.069,0.0645,-0.044,-0.0333,-0.0215,0.0177,-0.0393,-0.0002,0.0031,-0.0026,-0.026,-0.0423,0.063,0.0233,-0.0578,-0.0374,-0.0595,-0.215,-0.0055,-0.0857,0.0246,-0.0377,-0.0309,0.003,-0.0183,0.0385,-0.0355,0.0037,0.0334,0.0083,-0.0194,-0.0817,-0.005,-0.019,-0.0565,0.0055,-0.0074,-0.0172,-0.0081,-0.0167,0.0178,0.0017,0.0839,0.0614,0.0631,0.0478,0.0194,0.0192,0.0604,0.0258,-0.2124,0.0251,0.0365,0.0522,-0.0466,0.0207,0.0051,0.0627,-0.0581,0.0048,0.0447,0.0305,0.0045,0.028,0.022,-0.0403,0.0027,0.0437,0.013,-0.0126,-0.0643,0.0132,-0.0002,-0.038,-0.0063,-0.0105,0.07,-0.0406,-0.0036,0.0483,-0.0587,0.0379,-0.0618,-0.0028,0.0439,0.0179,-0.0344,0.1912,-0.0442,0.0244,0.0032,0.0109,0.0768,-0.0773,-0.0206,-0.0143,-0.0288,-0.0332,0.0173,-0.0075,0.0179,-0.0233,0.0099,0.0493,-0.0076,0.0695,-0.0261,-0.0061,-0.0167,0.012,0.0548,-0.0346,0.0055,-0.0175,0.055,0.1037,0.0197,0.0026,0.0487,-0.0356,-0.0681,-0.0038,0.0073,-0.0105,0.0479,0.0133,0.0309,0.0333,-0.0068,-0.0566,0.0451,-0.1475,-0.0305,0.1308,-0.0316,0.0483,0.0353,-0.0235,-0.0302,0.0705,-0.0816,-0.0625,-0.0514,0.0508,-0.0038,0.0492,-0.0199,0.016,-0.0662,-0.0301,-0.0645,0.0717,-0.0186,-0.0927,-0.0149,-0.0125,-0.0052,-0.0685,-0.0031,0.0262,-0.066,0.0013,0.0515,-0.0117,-0.0332,-0.0167,0.0125,-0.0254,0.0737,-0.0165,-0.0443,-0.0032,0.0761,-0.0274,-0.0136,-0.0528,0.0437,0.0482,-0.0325,-0.0051,-0.0395,-0.0099,-0.0159,0.015,-0.0141,-0.0405,0.0057,-0.0026,-0.0103,0.0325,0.0166,0.0275,0.0147,0.0236,0.018,-0.0217,0.0598,-0.0023,-0.0363,-0.0435,0.0386,-0.0432,-0.0504,-0.0192,0.0169,0.0194,-0.0233,0.0743,0.0245,-0.0163,-0.0537,-0.2382,-0.0202,0.0191,-0.0283,0.0788,-0.0534,0.0039,-0.0051,0.0883,0.0632,0.1371,0.0213,-0.0348,0.0633,-0.018,0.0313,-0.0156,0.0022,-0.1001,0.047,-0.0274,0.0592,-0.0366,-0.0302,0.0281,-0.0397,0.1856,0.0289,0.0051,-0.022,0.028,0.0249,-0.0114,-0.0916,0.0734,0.0108,0.0454,-0.0583,0.0261,-0.0536,0.001,0.0481,-0.0011,-0.1,-0.0322,0.0065,-0.0202,0.0461,-0.0817,0.0112,0.0692,0.0018,0.0959,-0.011,-0.0259,-0.0211,-0.1046,0.0333,-0.02,0.0195,-0.0057,-0.0524,0.0222,-0.0719,0.027,-0.0027,0.0102,-0.04,0.0261,-0.0063,-0.0407,0.0573,-0.0152,-0.0004,0.04,-0.0149,0.0749,-0.0805,-0.0285,0.0059,0.1043,-0.0438,0.0321,0.0116,0.0712,-0.0236,0.0482,-0.0004,0.0478,-0.0136,0.0166,-0.0089,-0.0727,0.0169,-0.015,0.0128,-0.2395,0.0358,0.0203,0.0187,-0.0374,-0.0012,0.0423,0.0134,-0.06,0.0047,0.0167,0.0087,0.0063,-0.0426,-0.0061,0.0332,0.0662,-0.0638,0.0542,-0.0419,0.0549,0.0437,0.2274,-0.0241,0.0763,0.0375,-0.0033,-0.0272,0.0256,-0.0295,0.0121,0.0061,0.0852,-0.0298,0.0132,0.0587,-0.0153,-0.0022,-0.0246,-0.0529,-0.0135,0.0402,-0.0225,-0.0166,0.0939,-0.0045,-0.0364,-0.0637,0.0448,0.0131,-0.0504,0.0036,-0.0389,-0.0451,0.0107,0.0181,0.0133,-0.018,-0.0152,-0.0403,0.0082,-0.0506,-0.0088,0.0186,0.0193]}
{"key":"[Bridging Maximum Likelihood and Adversarial Learning via $\\alpha$-Divergence] Maximum likelihood (ML) and adversarial learning are two popular approaches for training generative models, and from many perspectives these techniques are complementary. ML learning encourages the capture of all data modes, and it is typically characterized by stable training. However, ML learning tends to distribute probability mass diffusely over the data space, $e.g.$, yielding blurry synthetic images. Adversarial learning is well known to synthesize highly realistic natural images, despite practical challenges like mode dropping and delicate training. We propose an $\\alpha$-Bridge to unify the advantages of ML and adversarial learning, enabling the smooth transfer from one to the other via the $\\alpha$-divergence. We reveal that generalizations of the $\\alpha$-Bridge are closely related to approaches developed recently to regularize adversarial learning, providing insights into that prior work, and further understanding of why the $\\alpha$-Bridge performs well in practice.","layer":0,"vector":[-0.0224,-0.0583,0.0275,-0.0219,0.0429,0.0299,0.0142,0.0004,0.0254,0.0148,0.0412,-0.0272,-0.0043,0.0649,0.0104,0.0382,0.0179,0.0297,-0.0439,0.0585,0.0461,-0.027,0.0175,-0.0488,0.0312,0.0401,0.0021,-0.0473,-0.0154,-0.2444,0.002,-0.0415,0.0236,-0.0318,0.0228,-0.0179,-0.0755,0.0491,-0.0142,0.0778,0.047,0.0015,-0.0237,-0.0711,-0.0204,0.0113,-0.0481,-0.0042,-0.013,-0.0291,0.0271,-0.027,-0.002,0.0122,0.0548,0.0252,0.064,0.0415,0.0414,0.1263,-0.0253,0.0744,-0.1287,0.0596,0.0314,0.0142,-0.0466,-0.0416,0.0134,0.0605,-0.0138,0.0386,0.0027,0.0496,0.0152,-0.0155,-0.0113,-0.066,0.0071,0.0091,0.0675,-0.0027,-0.0202,-0.0052,-0.0069,-0.0164,0.0279,-0.0603,0.0498,-0.021,-0.041,-0.0036,-0.0712,0.033,-0.021,-0.0003,0.0254,-0.0015,-0.0474,0.1978,-0.0524,0.0258,0.0498,-0.0033,0.0565,-0.0238,-0.0411,-0.0286,-0.0261,-0.0381,-0.0011,-0.0265,0.0168,-0.0415,-0.0118,-0.0271,0.0308,0.0123,-0.0383,-0.0255,-0.0764,0.022,0.0265,-0.0072,0.0093,-0.0533,0.02,0.1437,0.0828,0.0557,0.013,-0.0451,-0.0538,-0.0045,0.0081,0.0199,-0.0206,-0.0005,-0.0017,-0.0168,-0.0423,-0.0451,0.0021,-0.0653,-0.0308,0.1117,-0.0353,0.0183,-0.0717,-0.0291,0.017,0.0208,-0.0154,-0.0102,0.0486,0.0325,0.0258,0.0773,-0.0712,-0.0022,-0.0617,-0.0371,-0.0103,0.0773,0.0153,-0.0794,-0.018,0.0092,0.0113,0.0319,-0.0138,0.038,-0.0092,0.0348,0.0579,0.0215,-0.1302,-0.0315,-0.0014,0.0401,-0.0026,-0.0673,-0.0052,0.0547,0.0613,-0.0245,0.0296,-0.052,0.0621,0.0751,0.0,0.0083,-0.0273,-0.0203,-0.0199,-0.0256,-0.0135,-0.0176,-0.0314,-0.0178,-0.0288,0.0098,-0.0808,0.0017,-0.0104,0.0207,0.0147,-0.0106,0.0396,0.0322,-0.026,0.0016,0.0598,-0.0026,-0.0362,-0.0194,0.0022,0.0213,-0.0121,0.0407,0.0128,-0.0808,-0.0635,-0.239,0.0063,-0.0029,-0.0366,0.0565,-0.1254,0.033,0.0059,0.069,0.0501,0.0495,-0.0074,0.0003,0.007,-0.0261,0.0394,-0.0009,0.0336,-0.0116,0.0323,-0.0393,0.0535,-0.0066,-0.087,0.0615,-0.0019,0.2294,0.0331,0.0265,0.0119,0.0324,0.0425,0.0253,-0.0898,0.0265,-0.0212,0.0846,-0.0223,-0.0459,0.0235,-0.0091,0.039,-0.0022,-0.1245,-0.0035,-0.0216,-0.0276,0.0284,-0.0817,0.0045,0.0499,-0.0261,0.0428,-0.0449,-0.0271,-0.02,-0.109,0.0281,-0.0403,0.0415,0.0204,-0.0287,0.0209,-0.0683,0.0841,-0.0096,-0.0394,-0.0775,0.0356,-0.0084,-0.0092,0.0407,0.0299,0.0034,0.0511,0.051,0.0105,-0.0333,-0.0972,-0.0425,0.013,-0.0014,0.0256,0.0352,0.0324,0.0521,0.0651,-0.0112,0.0059,-0.0102,-0.0066,0.0231,-0.0671,-0.0384,0.0291,-0.0182,-0.3056,0.0148,0.0251,0.0391,-0.0525,0.0133,0.0573,0.0108,-0.0545,-0.0115,-0.0255,0.0228,0.0351,0.0012,0.0158,0.0335,0.0554,-0.0495,0.044,-0.0548,-0.0189,0.0368,0.1936,-0.0502,-0.0425,0.0272,-0.0286,0.0216,0.0249,-0.0499,0.0037,0.0404,0.0903,-0.0615,0.037,0.0735,-0.062,0.0429,0.0111,-0.0387,-0.0073,0.0133,-0.0173,0.0104,0.0813,-0.0082,-0.0282,0.0319,0.0182,0.0337,-0.0441,0.0206,-0.0212,-0.0008,0.0636,0.0102,-0.0707,-0.0015,0.0099,-0.0178,0.0338,-0.0336,-0.04,0.0265,-0.0294]}
{"key":"[Applications of Generative Adversarial Networks in Neuroimaging and Clinical Neuroscience] Generative adversarial networks (GANs) are one powerful type of deep learning models that have been successfully utilized in numerous fields. They belong to a broader family called generative methods, which generate new data with a probabilistic model by learning sample distribution from real examples. In the clinical context, GANs have shown enhanced capabilities in capturing spatially complex, nonlinear, and potentially subtle disease effects compared to traditional generative methods. This review appraises the existing literature on the applications of GANs in imaging studies of various neurological conditions, including Alzheimer's disease, brain tumors, brain aging, and multiple sclerosis. We provide an intuitive explanation of various GAN methods for each application and further discuss the main challenges, open questions, and promising future directions of leveraging GANs in neuroimaging. We aim to bridge the gap between advanced deep learning methods and neurology research by highlighting how GANs can be leveraged to support clinical decision making and contribute to a better understanding of the structural and functional patterns of brain diseases.","layer":0,"vector":[-0.0081,-0.0246,0.0253,-0.0165,0.0193,0.0863,0.055,-0.0131,0.032,-0.0053,-0.0133,-0.0229,0.0382,0.0852,-0.0051,0.0092,0.0117,0.0047,-0.0388,0.0293,0.0115,-0.0313,0.0395,-0.0152,-0.0235,0.0183,-0.0047,-0.0349,-0.0595,-0.2473,0.0373,-0.0312,0.0368,-0.0638,-0.0327,-0.0214,-0.045,0.0649,-0.0222,0.0226,0.0209,0.0554,-0.0396,-0.0762,-0.0135,-0.0236,-0.0673,-0.0094,0.0162,-0.0111,0.0163,-0.044,0.0035,0.0465,0.0422,-0.0001,0.046,0.023,0.0247,0.0876,0.0331,0.0585,-0.1459,0.0655,0.0638,0.0134,-0.0294,-0.0327,0.0019,0.0253,-0.0058,0.0316,0.0103,0.02,0.0184,-0.0017,-0.0395,-0.0451,-0.0186,0.0004,0.07,0.0408,-0.0424,0.006,0.0184,0.0178,-0.0304,-0.0467,0.0197,-0.0073,0.0105,-0.0005,-0.0764,0.0442,-0.0533,-0.013,0.0525,0.0077,-0.0553,0.2071,-0.0557,0.0304,0.0629,-0.0203,0.0067,-0.0146,-0.0554,-0.0454,-0.0531,0.0265,-0.0061,-0.0069,0.0189,-0.0091,0.021,-0.0009,0.0303,0.0083,-0.0235,-0.0268,-0.0585,0.0298,0.046,-0.0131,0.058,-0.0638,0.0043,0.118,0.0715,-0.0052,0.027,0.009,-0.0285,0.0281,0.0048,-0.0032,0.0121,0.003,-0.011,0.0229,-0.0198,-0.007,-0.0057,-0.019,-0.0451,0.0987,-0.051,-0.0088,-0.0627,0.0081,-0.0334,0.0227,-0.0449,-0.0006,-0.0102,0.0535,-0.003,0.0405,-0.0586,0.0113,-0.0311,-0.0925,-0.04,0.1243,0.017,-0.0929,-0.0329,-0.0088,0.0122,0.0212,0.0466,0.0241,-0.014,0.0316,0.0608,0.0472,-0.0605,-0.045,-0.0527,0.0244,-0.0153,-0.0513,-0.034,0.0405,0.0416,-0.0334,0.0308,-0.0642,0.0232,0.0203,-0.0218,0.0259,-0.0368,-0.0339,-0.0382,-0.0685,-0.0373,-0.0304,-0.0071,-0.0383,0.0109,-0.0117,-0.0431,-0.0051,0.0068,0.0491,-0.0251,-0.0181,0.0721,0.0606,-0.0422,0.0262,0.0935,0.0055,-0.0309,0.0286,-0.0559,0.0459,-0.0146,0.0405,0.0596,-0.0525,-0.0694,-0.2302,-0.0272,0.0077,-0.0343,0.0286,-0.1024,0.0317,-0.0001,0.051,0.0638,0.0302,0.0301,0.0048,-0.0032,0.0091,0.0279,0.0051,-0.0001,-0.027,-0.0295,-0.007,0.0235,0.0431,-0.0832,0.0434,0.0134,0.2037,0.0397,0.0354,-0.0088,0.0051,0.023,-0.032,-0.0912,0.0629,0.0254,0.085,0.003,-0.0695,-0.013,-0.0735,0.0478,0.0077,-0.0975,-0.0158,0.011,-0.0097,0.0159,-0.0312,0.029,0.0464,-0.0643,0.074,-0.0216,0.0013,-0.0367,-0.1505,0.0513,-0.0523,0.0022,0.0003,-0.054,0.023,-0.0901,0.0654,0.0269,-0.05,-0.0567,0.0264,-0.0073,0.0006,0.0698,-0.0066,0.0536,0.0648,0.0262,0.0252,-0.0343,-0.0431,0.0059,0.0362,-0.051,0.0474,0.0745,0.0132,-0.0031,0.0837,-0.0252,0.0206,-0.0538,-0.019,0.0495,-0.0726,-0.0732,0.0415,-0.0128,-0.2779,0.094,-0.0079,0.0587,0.0118,0.0304,0.0157,0.0442,-0.0423,0.0068,0.0027,0.0131,0.0661,-0.0453,-0.0021,0.0201,0.0608,-0.058,0.0635,-0.0515,0.0031,0.0322,0.1927,-0.049,0.0185,0.0194,-0.0081,0.0277,0.0314,-0.0364,-0.0166,-0.0173,0.0713,-0.0213,0.059,0.1109,-0.096,0.0264,0.0027,-0.0527,0.0229,0.041,-0.0397,0.0411,0.0485,0.0124,-0.0444,0.0235,-0.0146,-0.0012,-0.0487,0.0159,-0.0243,0.0231,0.0533,0.0173,0.0029,-0.0562,0.0183,-0.0359,0.0087,-0.017,-0.0461,0.0013,-0.0446]}
{"key":"[How Much of the Chemical Space Has Been Explored? Selecting the Right Exploration Measure for Drug Discovery] Forming a molecular candidate set that contains a wide range of potentially effective compounds is crucial to the success of drug discovery. While many aim to optimize particular chemical properties, there is limited literature on how to properly measure and encourage the exploration of the chemical space when generating drug candidates. This problem is challenging due to the lack of formal criteria to select good exploration measures. We propose a novel framework to systematically evaluate exploration measures for drug candidate generation. The procedure is built upon three formal analyses: an axiomatic analysis that validates the potential measures analytically, an empirical analysis that compares the correlations of the measures to a proxy gold standard, and a practical analysis that benchmarks the effectiveness of the measures in an optimization procedure of molecular generation. We are able to evaluate a wide range of potential exploration measures under this framework and make recommendations on existing and novel exploration measures that are suitable for the task of drug discovery.","layer":5,"vector":[-0.0743,-0.0351,0.0415,-0.0124,-0.0166,0.0275,0.0258,0.0558,-0.0019,-0.0094,0.029,-0.0542,0.0265,0.0567,0.0073,-0.0016,-0.029,0.0179,-0.0661,0.0092,0.0601,-0.0233,-0.0049,-0.0668,0.0359,0.0792,-0.0307,-0.0195,-0.0429,-0.241,-0.0032,-0.0028,0.0548,-0.0378,-0.0421,0.0004,-0.019,0.0756,-0.0558,0.0476,0.056,0.011,0.0032,-0.0192,-0.0456,-0.0438,-0.0367,-0.0323,-0.0315,-0.0533,0.0184,-0.044,0.0094,0.0416,0.0417,0.0512,-0.0026,0.0247,-0.0082,-0.0014,0.0458,0.0676,-0.1517,0.0284,0.0873,-0.0063,-0.0424,-0.0147,0.0524,0.0968,0.0161,0.0251,0.0037,0.0448,0.0383,0.0043,-0.0183,-0.026,0.0327,-0.0002,0.0255,-0.0182,-0.0512,-0.0036,-0.0321,-0.0487,0.0052,-0.0244,0.0688,0.0554,0.0012,-0.0278,-0.038,0.0136,-0.1078,-0.0152,0.02,-0.0199,-0.0274,0.1638,-0.0443,0.0363,-0.0349,-0.0544,0.0034,-0.0589,-0.013,-0.0047,0.0267,-0.0096,0.0181,0.0043,0.0312,-0.0617,-0.0317,0.0472,0.0285,0.0668,-0.0146,-0.0233,-0.0271,0.0094,0.0592,-0.0048,0.0024,-0.038,-0.0009,0.1231,0.0171,0.0272,0.0392,-0.0135,-0.0218,-0.002,0.0198,-0.0028,-0.0171,0.0435,0.0181,0.0209,-0.0341,0.0021,0.0347,-0.0967,-0.0746,0.1413,-0.0313,0.0258,-0.0615,0.0069,0.0093,0.0176,-0.035,-0.0104,0.0158,0.0705,-0.0146,0.0069,-0.0523,0.0348,-0.0439,-0.0151,-0.0369,0.1702,-0.0542,-0.0327,-0.0583,0.0508,-0.0046,0.0123,-0.0053,0.0149,-0.0785,0.0613,0.0052,-0.0245,-0.0717,0.0103,0.0018,0.0011,0.0659,0.0121,-0.0296,0.0538,0.0346,-0.0501,0.017,0.0045,0.0292,0.0308,-0.0426,0.0535,-0.0121,0.0326,-0.0217,-0.0413,-0.0072,-0.0162,0.0082,-0.033,0.0548,-0.0119,-0.0476,0.0495,-0.0014,0.0462,-0.008,0.0005,0.0649,0.0117,-0.0301,0.0242,-0.0035,-0.0269,-0.0454,-0.0318,0.0143,0.0491,0.0336,0.0356,0.0568,-0.0352,-0.0304,-0.2507,0.0117,0.0013,0.0229,0.0613,-0.0241,0.0247,-0.0308,-0.0009,0.047,0.019,-0.0181,-0.0601,0.0337,-0.0577,0.019,0.0004,-0.0131,-0.0019,0.0172,0.0075,0.0107,-0.0114,-0.0451,0.0253,-0.0109,0.204,0.0564,-0.0168,-0.0245,0.0546,-0.0175,-0.0365,-0.1312,0.0023,0.0472,0.067,-0.0164,-0.0318,-0.0681,0.0144,0.015,-0.0419,-0.0493,-0.0308,0.005,-0.0117,-0.006,-0.0139,0.0406,0.0437,-0.0077,0.0508,-0.0419,-0.0122,-0.051,-0.0694,0.0182,-0.0576,0.0398,0.0174,-0.0244,0.019,-0.0245,0.0318,-0.0563,-0.0033,-0.0421,0.0372,-0.0446,-0.0037,0.0865,-0.0143,0.0076,0.0435,0.0041,0.0064,-0.0086,-0.0279,-0.0009,0.0167,-0.0446,0.0273,-0.0434,0.003,-0.0025,0.0746,-0.0268,0.0204,-0.0347,0.0213,0.046,-0.0384,-0.0365,0.0296,0.0007,-0.2847,0.0691,0.0201,0.0414,-0.0347,-0.0378,0.0663,-0.0326,-0.0532,-0.0022,0.0583,0.0343,0.035,-0.0025,-0.0158,-0.0304,0.0942,-0.073,0.0362,-0.0836,0.0668,0.0325,0.2692,-0.0221,0.0262,0.0766,-0.0367,0.0227,-0.0176,-0.0129,0.0197,0.0236,0.073,-0.0642,0.06,0.1124,-0.0384,0.0391,-0.0159,-0.0291,-0.0118,0.0194,-0.069,-0.0036,0.0939,-0.0447,-0.0528,-0.0278,0.0037,0.0388,-0.0732,0.038,-0.0418,-0.0335,0.0605,0.0302,-0.0297,-0.0228,0.0033,-0.0188,-0.024,-0.0303,0.0051,0.0185,0.0229]}
{"key":"[Accelerating Reinforcement Learning through Implicit Imitation] Imitation can be viewed as a means of enhancing learning in multiagent environments. It augments an agent's ability to learn useful behaviors by making intelligent use of the knowledge implicit in behaviors demonstrated by cooperative teachers or other more experienced agents. We propose and study a formal model of implicit imitation that can accelerate reinforcement learning dramatically in certain cases. Roughly, by observing a mentor, a reinforcement-learning agent can extract information about its own capabilities in, and the relative value of, unvisited parts of the state space. We study two specific instantiations of this model, one in which the learning agent and the mentor have identical abilities, and one designed to deal with agents and mentors with different action sets. We illustrate the benefits of implicit imitation by integrating it with prioritized sweeping, and demonstrating improved performance and convergence through observation of single and multiple mentors. Though we make some stringent assumptions regarding observability and possible interactions, we briefly comment on extensions of the model that relax these restricitions.","layer":0,"vector":[-0.0454,-0.0331,0.028,-0.0151,-0.015,-0.0156,0.0626,0.0587,0.0219,0.0132,0.0129,-0.0653,0.0519,0.073,-0.001,0.0044,-0.0487,0.0722,-0.037,0.0049,0.0199,-0.045,-0.0229,-0.0427,0.0316,0.0351,-0.0392,-0.0691,-0.004,-0.2164,0.0353,-0.051,0.0367,-0.0198,-0.0192,-0.003,-0.0178,0.0782,-0.0245,0.0247,0.0182,0.0142,-0.0148,-0.0642,-0.044,-0.0483,-0.0229,-0.0139,-0.0195,-0.0528,-0.0017,-0.0345,-0.0056,-0.0014,0.0496,0.0456,0.0685,0.0696,0.0347,0.0362,0.012,0.0347,-0.1553,0.0882,0.0258,0.0859,-0.0071,-0.0508,0.0537,0.0433,-0.0091,0.0404,0.0497,0.0447,0.0319,0.0066,-0.0375,-0.0403,-0.0044,0.0053,-0.0251,-0.0502,-0.0691,-0.0292,-0.0279,-0.05,0.0278,-0.0436,0.0681,-0.0004,-0.0362,-0.0062,-0.0461,0.0169,-0.0409,-0.0248,0.0066,0.0295,-0.0482,0.2069,-0.0107,0.0275,0.0501,0.0,0.0508,-0.0498,-0.0083,0.0034,-0.0165,0.0108,-0.0765,-0.003,0.0179,-0.0149,0.0425,0.019,0.0789,0.0006,-0.0278,-0.0248,0.0374,0.0188,0.0529,-0.0184,-0.0013,-0.0766,0.0068,0.1452,0.0151,0.0562,0.0282,-0.0449,0.0148,0.025,0.0181,0.0289,0.0058,-0.0401,0.0054,0.0096,-0.0112,-0.0074,0.0041,-0.1123,-0.0582,0.0812,0.015,0.0395,-0.0566,0.0008,-0.0561,0.0005,-0.0103,-0.0314,-0.0093,-0.0085,0.0436,0.0736,-0.0722,0.0276,-0.0665,-0.0296,-0.0275,0.0725,-0.0081,-0.0419,-0.0363,-0.0024,-0.0105,0.0059,0.0234,0.0238,-0.0708,0.0198,0.0569,-0.0178,-0.075,-0.0282,0.0085,0.001,0.0553,-0.0646,0.0019,0.0568,0.0282,-0.0104,-0.0159,-0.0672,0.0435,0.0409,-0.0133,0.0727,-0.051,0.0161,-0.0192,-0.0077,0.0289,-0.0197,-0.0141,-0.0398,-0.032,-0.0133,-0.0484,-0.0239,-0.0397,0.0093,-0.0011,0.019,0.0489,0.0089,-0.0311,0.0044,0.0122,0.0141,-0.07,0.0057,0.0024,0.0302,-0.0237,0.0464,0.0137,-0.0,0.0125,-0.2011,-0.0265,-0.0161,0.0067,0.0486,-0.0454,0.0554,-0.0159,0.0293,0.0225,0.0472,-0.0356,0.0002,0.0342,0.0084,0.0579,0.0265,0.0243,0.016,-0.0093,0.0061,0.0126,-0.0161,-0.0775,0.0716,0.0042,0.2393,0.0373,0.0328,0.0059,0.004,0.0348,-0.0449,-0.1442,0.0494,0.022,0.0658,-0.0213,0.032,-0.055,-0.016,0.004,-0.0255,-0.0827,-0.0244,-0.0072,-0.0501,0.0473,-0.062,-0.0042,0.0507,-0.055,0.0254,-0.0305,-0.0676,-0.0462,-0.0769,0.0731,-0.0251,0.0504,0.0229,-0.0133,0.0011,-0.0639,0.0873,-0.0028,0.0119,-0.0272,0.0312,-0.0085,-0.0273,0.0383,-0.028,0.0061,-0.0177,0.026,-0.0202,-0.0227,-0.0367,-0.004,0.0417,-0.0359,0.0006,0.0471,0.0214,-0.0647,0.0674,-0.0066,0.071,-0.0262,0.018,0.0309,-0.0864,-0.0046,0.0496,-0.0175,-0.2959,0.0558,0.0657,0.026,-0.0382,0.0226,0.0297,-0.0119,-0.0431,-0.0224,0.0267,0.064,0.0313,0.0341,0.0335,0.06,0.0934,-0.0114,0.0154,-0.108,0.0144,0.0432,0.2438,-0.0078,0.0444,0.0274,-0.0183,-0.0315,0.0592,-0.0587,0.0077,0.0302,0.0828,-0.1034,0.0262,0.0676,-0.0448,-0.0001,0.0027,-0.0246,-0.0535,0.0102,0.0143,-0.0287,0.0872,0.021,-0.0214,-0.0466,-0.0441,0.0358,-0.0026,0.0118,-0.0223,-0.0267,0.0537,-0.0068,-0.0689,-0.0405,-0.0303,-0.0472,0.0163,-0.0569,0.0567,0.0251,0.0158]}
{"key":"[Implementation of Fruits Recognition Classifier using Convolutional Neural Network Algorithm for Observation of Accuracies for Various Hidden Layers] Fruit recognition using Deep Convolutional Neural Network (CNN) is one of the most promising applications in computer vision. In recent times, deep learning based classifications are making it possible to recognize fruits from images. However, fruit recognition is still a problem for the stacked fruits on weighing scale because of the complexity and similarity. In this paper, a fruit recognition system using CNN is proposed. The proposed method uses deep learning techniques for the classification. We have used Fruits-360 dataset for the evaluation purpose. From the dataset, we have established a dataset which contains 17,823 images from 25 different categories. The images are divided into training and test dataset. Moreover, for the classification accuracies, we have used various combinations of hidden layer and epochs for different cases and made a comparison between them. The overall performance losses of the network for different cases also observed. Finally, we have achieved the best test accuracy of 100% and a training accuracy of 99.79%.","layer":0,"vector":[-0.0069,-0.0317,-0.0062,-0.0513,0.0668,0.02,0.0611,0.0101,0.0343,-0.0017,0.0192,-0.078,0.0523,0.0483,0.0497,-0.0105,0.0379,0.0417,-0.0617,-0.0038,0.0307,-0.0134,-0.0175,-0.0261,0.0016,0.0145,-0.0242,-0.0274,-0.0516,-0.2132,0.0015,-0.0232,0.098,-0.0457,-0.0088,-0.0391,-0.029,0.0372,-0.0409,-0.0131,0.0495,-0.032,-0.0187,-0.0858,-0.0488,-0.0187,-0.0258,-0.033,0.0259,-0.0152,0.0417,-0.0525,0.0244,0.0163,-0.0051,0.0716,0.0725,0.035,0.0271,0.0261,0.0838,0.0122,-0.189,0.056,0.0325,0.0076,-0.0621,-0.0333,0.0328,0.0319,-0.0024,0.0462,0.0412,0.041,0.0077,-0.0136,-0.0027,-0.0502,-0.0131,-0.0118,0.0115,0.0059,-0.0263,-0.0133,-0.0055,-0.046,0.0064,-0.0579,0.0292,0.0252,-0.0415,0.0015,-0.0217,0.0163,-0.0878,0.0013,0.0123,-0.0236,-0.1103,0.2075,-0.0533,0.0163,0.0473,-0.0685,0.0105,-0.0262,-0.035,-0.049,-0.0043,0.0202,-0.0125,-0.0448,0.0356,0.0225,-0.0121,0.012,0.0467,0.0664,-0.0148,-0.0334,0.001,0.0235,0.0181,-0.0435,0.0325,-0.0218,-0.0175,0.1462,0.0233,0.0312,0.0749,-0.0397,-0.0559,-0.0277,0.032,0.0541,0.0408,-0.0024,0.0075,-0.0443,-0.0556,-0.0351,0.0615,-0.0891,-0.0032,0.0688,-0.0417,0.0726,-0.0342,-0.0526,-0.0185,0.0174,-0.0512,-0.0038,0.0164,0.0156,0.0482,0.0405,-0.0463,0.0237,-0.0379,-0.0231,-0.0298,0.0998,0.0341,-0.115,-0.0441,-0.0199,0.0019,-0.0198,0.0661,0.0331,-0.0367,-0.0102,0.0905,0.0472,-0.0386,0.0009,0.009,-0.0232,0.0171,-0.0689,-0.0473,0.0239,0.0763,-0.0288,0.0288,-0.066,0.0225,0.056,-0.0343,0.0172,-0.0805,0.0234,-0.0451,-0.062,-0.0323,-0.0196,0.0135,-0.0362,0.007,-0.0031,0.0312,0.0009,0.0147,0.0118,-0.0074,0.0205,0.0308,0.0387,-0.0324,0.0071,0.0494,-0.0137,-0.0684,-0.0068,0.0719,0.0557,-0.0026,0.0341,0.0799,-0.1027,-0.0482,-0.2067,0.0001,0.0434,-0.009,0.052,-0.0242,0.0279,-0.0107,0.0283,0.0115,0.0543,0.0068,0.0285,0.0032,0.0156,0.0467,0.0449,0.071,-0.0315,0.0139,0.0103,0.0307,0.0137,-0.0983,0.0336,-0.0059,0.184,0.001,0.0032,0.0053,-0.002,0.0181,-0.0486,-0.094,0.0515,-0.0293,0.0103,-0.0064,-0.0567,0.0105,-0.0181,0.0262,0.019,-0.1131,-0.0224,-0.0232,0.0084,0.0398,-0.0374,0.0341,0.0078,0.0044,0.0137,0.0313,0.0089,-0.0091,-0.1236,0.0483,-0.0572,0.0082,-0.0248,-0.0552,-0.0307,-0.0279,0.0379,0.031,-0.07,-0.0293,-0.0028,-0.0018,-0.0056,0.0641,0.0063,-0.0337,0.0796,-0.0007,0.0398,-0.0449,-0.0418,-0.0357,0.0449,0.038,0.0308,0.0613,0.059,0.0439,0.0932,-0.0081,0.0413,-0.0,0.063,0.001,-0.0459,-0.002,0.0305,0.0347,-0.2765,0.0735,-0.0248,0.0607,0.0003,0.0027,0.029,0.0123,-0.0353,-0.0115,0.0173,-0.0092,0.0865,-0.0704,-0.0031,0.0239,0.0165,-0.0285,0.1034,-0.027,0.0148,0.0338,0.2127,-0.0833,-0.0285,-0.0012,-0.0303,-0.027,0.0105,0.0127,0.0081,-0.011,0.0892,-0.0403,0.0013,0.1083,0.0012,0.0135,0.0064,-0.0086,-0.0157,-0.0219,-0.0605,-0.0276,0.074,-0.024,0.0055,-0.0441,-0.0331,0.0339,-0.0061,-0.0628,0.007,-0.0032,0.0588,-0.0008,-0.0761,-0.0599,-0.065,-0.0293,0.0991,-0.0413,-0.0153,0.0194,0.0052]}
{"key":"[Performance-Weighed Policy Sampling for Meta-Reinforcement Learning] This paper discusses an Enhanced Model-Agnostic Meta-Learning (E-MAML) algorithm that generates fast convergence of the policy function from a small number of training examples when applied to new learning tasks. Built on top of Model-Agnostic Meta-Learning (MAML), E-MAML maintains a set of policy parameters learned in the environment for previous tasks. We apply E-MAML to developing reinforcement learning (RL)-based online fault tolerant control schemes for dynamic systems. The enhancement is applied when a new fault occurs, to re-initialize the parameters of a new RL policy that achieves faster adaption with a small number of samples of system behavior with the new fault. This replaces the random task sampling step in MAML. Instead, it exploits the extant previously generated experiences of the controller. The enhancement is sampled to maximally span the parameter space to facilitate adaption to the new fault. We demonstrate the performance of our approach combining E-MAML with proximal policy optimization (PPO) on the well-known cart pole example, and then on the fuel transfer system of an aircraft.","layer":0,"vector":[-0.0631,-0.022,0.0458,-0.0129,-0.011,0.0234,0.0078,0.0387,0.0237,0.003,0.0249,-0.0414,0.0273,0.0781,0.0259,0.0396,-0.0034,0.0555,0.0062,-0.016,0.0292,-0.0381,-0.0362,-0.0345,-0.0221,0.042,-0.0266,-0.0431,-0.0609,-0.279,0.0195,-0.0408,-0.008,-0.0065,-0.0177,-0.0242,-0.0633,0.0483,-0.0053,0.0533,0.0475,0.0212,-0.0014,-0.0765,-0.0046,-0.013,-0.0225,-0.0326,0.0324,-0.0241,-0.0039,-0.0307,0.0005,0.0054,0.0448,0.0271,0.0393,0.0531,0.0427,0.0189,-0.0125,0.0113,-0.165,0.031,0.0421,0.0609,-0.0253,0.0036,0.0478,0.0419,-0.0715,0.0171,-0.0317,0.0712,-0.0174,0.0284,-0.0288,-0.0329,0.0288,0.0202,0.0466,-0.0896,-0.0339,0.0217,-0.0591,-0.0953,-0.0043,-0.0184,0.0618,0.042,-0.0391,0.0166,0.0266,0.0198,-0.0785,0.019,0.0175,0.0142,-0.0946,0.2077,-0.0213,0.0921,0.0384,-0.0079,0.0075,-0.0716,-0.0501,-0.0163,-0.0678,-0.0611,-0.0481,-0.0054,0.0353,-0.0414,-0.0101,0.009,0.0378,0.0301,0.0219,0.0045,-0.0057,-0.035,0.0559,-0.0369,0.0362,-0.0874,0.0311,0.187,0.0006,0.0337,0.0134,-0.0564,-0.0217,-0.0678,-0.0152,0.0176,0.0298,-0.0096,0.0461,0.0545,-0.0628,-0.017,-0.0154,-0.0854,-0.084,0.0998,0.0062,0.0083,-0.0481,-0.0038,0.0148,0.0095,-0.0119,-0.0014,0.0167,0.0333,0.0698,0.0793,-0.0097,0.0218,-0.0129,-0.0392,-0.039,0.1094,-0.014,-0.0666,-0.0458,-0.0068,-0.0087,0.0013,0.0104,-0.0143,-0.036,-0.0081,0.071,-0.0092,-0.0515,-0.0,0.009,0.0236,0.068,-0.0816,-0.0304,0.0031,0.0694,0.0001,0.0386,-0.0692,-0.0066,0.008,-0.027,0.0263,0.0022,0.0262,-0.0292,-0.0287,0.0033,-0.0259,0.0114,-0.0416,-0.0087,0.0135,-0.0498,0.0236,-0.0091,0.0403,-0.0493,-0.0227,0.0229,0.014,-0.0513,0.0043,0.0567,0.0016,-0.0391,0.0429,0.0123,0.0221,-0.0173,0.0417,0.0783,-0.0003,-0.0212,-0.1842,-0.0228,-0.0077,0.0054,0.049,-0.0836,0.0311,-0.0531,0.03,0.0285,0.0672,-0.0287,-0.0205,0.0337,-0.0052,0.0663,0.0274,-0.0093,-0.0337,-0.0275,-0.0323,0.0432,-0.0251,-0.0991,0.0585,-0.0479,0.1858,-0.0105,0.0392,0.001,0.0365,0.0401,-0.0101,-0.0755,0.0656,0.0229,0.0562,-0.0452,0.0091,-0.0833,0.0101,0.0182,-0.0289,-0.1212,-0.0341,-0.0333,-0.0301,0.0672,-0.0501,0.0042,0.029,-0.0022,0.0047,-0.0642,-0.0315,-0.0234,-0.0504,0.0307,-0.0212,0.0327,0.0419,-0.0111,-0.0024,-0.0615,0.0571,-0.0039,0.059,-0.0682,0.0222,-0.0035,0.0066,0.1028,0.0559,0.0038,0.0218,0.0091,-0.0111,-0.0156,-0.0516,-0.0235,0.0414,-0.015,0.0429,0.0693,0.0338,0.0065,0.0508,-0.0211,0.0159,-0.0163,-0.0106,0.0098,-0.0608,0.0162,0.0885,0.031,-0.2793,0.0704,0.0154,0.0071,-0.0434,0.0189,0.0468,-0.0137,-0.0677,-0.0042,-0.0013,0.0565,0.0441,0.0253,0.0293,0.0803,0.0636,-0.0193,0.0227,-0.0876,0.0105,0.0614,0.1947,-0.0228,0.0615,0.0247,-0.0273,-0.0087,0.0011,0.0146,0.0235,0.0204,0.0535,-0.0274,0.0781,0.0768,-0.0457,0.0396,0.0233,-0.0208,-0.0569,0.0142,0.0124,-0.044,0.0678,-0.0076,-0.0138,-0.069,-0.0265,0.0311,-0.0053,-0.0084,-0.0023,-0.0433,0.0309,0.0231,-0.0257,-0.0495,-0.0822,-0.0412,0.0241,-0.0315,0.0441,-0.0211,0.025]}
{"key":"[Federated Progressive Sparsification (Purge, Merge, Tune)+] To improve federated training of neural networks, we develop FedSparsify, a sparsification strategy based on progressive weight magnitude pruning. Our method has several benefits. First, since the size of the network becomes increasingly smaller, computation and communication costs during training are reduced. Second, the models are incrementally constrained to a smaller set of parameters, which facilitates alignment/merging of the local models and improved learning performance at high sparsification rates. Third, the final sparsified model is significantly smaller, which improves inference efficiency and optimizes operations latency during encrypted communication. We show experimentally that FedSparsify learns a subnetwork of both high sparsity and learning performance. Our sparse models can reach a tenth of the size of the original model with the same or better accuracy compared to existing pruning and nonpruning baselines.","layer":2,"vector":[0.0162,-0.0436,-0.02,0.014,0.0251,0.0127,-0.0228,-0.0211,0.0662,-0.0454,0.0178,-0.0458,0.0532,0.0361,0.0217,0.0308,0.0302,0.0467,-0.0304,-0.0096,0.0488,-0.0746,-0.0278,-0.0314,0.0018,0.0472,-0.0359,-0.0385,-0.0704,-0.2523,0.042,-0.0281,0.0383,-0.0141,0.0381,0.0067,-0.0398,0.0354,-0.0473,0.046,-0.0112,0.0408,-0.0391,-0.0179,-0.037,-0.0193,-0.0007,-0.0082,-0.0155,-0.04,0.0707,-0.0528,0.0265,0.0249,0.0118,-0.005,0.0411,0.0432,0.0564,0.0056,0.0115,0.0248,-0.1625,0.0634,0.0397,0.0331,-0.0131,-0.0202,0.0044,0.048,0.0008,0.0603,0.0053,0.0564,-0.01,0.0045,-0.0239,0.0159,-0.0089,0.0382,0.0324,-0.0391,-0.0662,-0.0298,-0.0313,-0.0361,0.0223,-0.0647,-0.0043,-0.0541,-0.0435,0.0007,-0.0148,0.0093,-0.0579,-0.0309,0.0262,0.0462,-0.0735,0.2316,-0.0209,0.0563,0.0351,-0.0666,0.0462,-0.0428,-0.0256,-0.0421,-0.0052,-0.0147,-0.0438,-0.0141,0.0172,-0.047,0.0263,0.005,0.0839,0.0618,-0.0426,0.0197,-0.0089,-0.0129,0.0388,-0.0219,0.0381,-0.0768,0.0071,0.1436,0.0333,0.074,0.0304,-0.0278,-0.0081,-0.0137,0.0284,0.0403,0.0381,-0.0329,0.0139,0.0114,-0.0378,-0.0732,0.0137,-0.0633,-0.0518,0.1426,-0.0319,0.0419,-0.0577,-0.0192,-0.0392,0.0016,0.002,-0.0322,0.0472,-0.0105,0.0495,0.0147,-0.0582,-0.0046,0.0051,-0.0461,-0.0069,0.0993,0.0636,-0.0993,-0.0318,-0.0138,0.0011,-0.0396,0.0412,-0.0189,0.0034,0.0355,0.0352,0.0488,-0.0437,-0.0112,0.0155,-0.0012,-0.0054,-0.0668,-0.0147,0.0412,0.0073,-0.0303,0.0318,-0.0258,-0.0129,0.0267,-0.0699,0.0347,-0.0595,-0.0393,-0.0182,-0.0338,0.006,0.0034,-0.0121,-0.0385,-0.0025,0.019,-0.006,-0.0026,0.0126,0.0123,0.0162,-0.0027,0.0101,0.0535,-0.0094,-0.0037,0.0744,-0.0413,0.0016,0.0059,0.0137,0.0325,-0.0354,0.0338,0.0383,-0.0395,-0.0688,-0.192,-0.0243,0.0466,-0.0647,0.0859,-0.0837,0.0799,0.0183,0.0236,0.0253,0.0372,-0.0059,-0.0177,0.0512,0.0087,0.0659,0.0398,0.0422,-0.0034,0.0193,-0.0167,0.0281,0.0113,-0.049,0.0852,0.0454,0.202,0.0092,0.0322,-0.0473,0.0101,0.0286,-0.0306,-0.1427,0.045,-0.0173,0.064,0.0403,-0.065,-0.0239,-0.042,0.018,0.0313,-0.1647,-0.051,-0.0498,-0.0381,-0.0072,-0.0486,0.0457,0.0057,-0.0031,0.0533,0.0085,0.0165,-0.0614,-0.0779,0.0379,-0.036,0.0572,0.0229,-0.0565,-0.0267,-0.051,0.0895,0.0181,-0.0238,-0.031,0.0466,-0.0344,0.0016,0.0417,0.0013,0.0266,0.0332,0.0541,0.0229,-0.0222,-0.0639,-0.0199,0.0926,0.0019,0.0373,-0.0018,0.0031,0.0193,0.0824,0.0424,0.021,-0.0113,0.0147,0.0196,-0.0425,-0.0143,0.0414,-0.004,-0.3192,0.0178,0.026,0.0213,-0.0446,0.0261,0.1064,0.0081,-0.0515,-0.0145,0.0101,0.0202,0.0134,0.007,-0.0123,0.0763,0.0257,-0.0361,0.0154,-0.0852,0.0162,0.0074,0.1986,-0.0515,0.0135,0.0366,-0.0164,0.0238,0.0885,0.0001,0.0157,-0.0034,0.0205,-0.0425,0.0319,0.0666,-0.0227,0.0228,0.0282,-0.0365,-0.0166,-0.0344,-0.0194,-0.023,0.0697,-0.0274,-0.051,-0.0263,0.0114,0.0187,-0.0103,-0.0116,0.007,-0.0134,0.0104,-0.012,-0.0183,-0.0289,-0.0489,-0.0264,0.0211,-0.0608,-0.0423,-0.0045,-0.0194]}
{"key":"[3D AGSE-VNet: An Automatic Brain Tumor MRI Data Segmentation Framework] Background: Glioma is the most common brain malignant tumor, with a high morbidity rate and a mortality rate of more than three percent, which seriously endangers human health. The main method of acquiring brain tumors in the clinic is MRI. Segmentation of brain tumor regions from multi-modal MRI scan images is helpful for treatment inspection, post-diagnosis monitoring, and effect evaluation of patients. However, the common operation in clinical brain tumor segmentation is still manual segmentation, lead to its time-consuming and large performance difference between different operators, a consistent and accurate automatic segmentation method is urgently needed. Methods: To meet the above challenges, we propose an automatic brain tumor MRI data segmentation framework which is called AGSE-VNet. In our study, the Squeeze and Excite (SE) module is added to each encoder, the Attention Guide Filter (AG) module is added to each decoder, using the channel relationship to automatically enhance the useful information in the channel to suppress the useless information, and use the attention mechanism to guide the edge information and remove the influence of irrelevant information such as noise. Results: We used the BraTS2020 challenge online verification tool to evaluate our approach. The focus of verification is that the Dice scores of the whole tumor (WT), tumor core (TC) and enhanced tumor (ET) are 0.68, 0.85 and 0.70, respectively. Conclusion: Although MRI images have different intensities, AGSE-VNet is not affected by the size of the tumor, and can more accurately extract the features of the three regions, it has achieved impressive results and made outstanding contributions to the clinical diagnosis and treatment of brain tumor patients.","layer":0,"vector":[-0.0629,-0.0238,0.0349,-0.0406,0.0468,0.03,0.0591,-0.0002,0.0323,-0.0112,-0.0084,-0.0326,0.0396,0.0538,-0.0039,0.0264,-0.0012,0.0332,-0.0183,0.0096,0.0092,-0.0583,-0.0024,-0.0248,-0.0004,-0.0016,-0.0107,-0.0356,-0.054,-0.2438,0.0313,-0.0247,0.0391,-0.0693,-0.0251,-0.0384,-0.0147,0.0305,-0.0739,0.0158,0.0045,0.0495,-0.0335,-0.0141,-0.0812,-0.0495,-0.0278,-0.0019,0.0099,-0.0313,0.0558,-0.0416,-0.0172,0.0288,0.0019,0.0132,0.0181,0.0452,0.0225,0.0259,0.0684,0.0772,-0.157,0.0472,0.0367,-0.0058,-0.0614,-0.0304,0.0096,0.053,-0.008,0.0166,0.0359,0.023,-0.0206,-0.0152,0.034,-0.0529,0.0037,0.0026,0.0226,-0.0163,-0.0387,-0.0371,-0.0003,-0.0189,-0.0351,-0.0861,0.0115,-0.0517,-0.0312,-0.0248,-0.0121,0.0141,-0.0355,-0.0234,0.0045,0.0045,-0.056,0.175,-0.007,0.0183,0.0334,-0.0218,0.0216,-0.0572,-0.0209,-0.0045,0.0054,0.0564,-0.0308,-0.0263,0.0696,-0.0081,0.0068,0.0431,0.0574,0.0025,-0.0145,-0.0261,-0.0527,0.0131,0.0168,-0.0353,0.0448,-0.0414,0.0649,0.1283,0.0464,0.0447,0.0586,0.031,-0.0634,-0.0408,0.0015,-0.0013,0.0217,-0.0028,0.0014,-0.03,-0.0204,-0.0692,0.0388,-0.0851,-0.0216,0.1147,-0.1081,-0.0028,-0.066,-0.0255,0.0247,0.0488,-0.0388,-0.0245,0.0163,0.0283,0.0205,0.0303,-0.0996,-0.0186,-0.0023,-0.0567,-0.0734,0.1338,0.065,-0.1237,-0.009,0.013,0.0211,-0.0202,0.0332,0.0444,-0.0344,0.0087,0.0868,-0.0147,-0.0435,-0.0017,-0.0033,0.0007,0.0518,-0.0397,-0.0097,0.0264,0.0227,-0.0602,0.0392,-0.0372,0.065,0.0033,-0.0633,0.043,-0.017,-0.0001,-0.0169,-0.0749,-0.0175,-0.0349,-0.0125,-0.0417,0.0622,-0.0152,-0.0125,0.0709,0.007,0.0319,-0.0212,-0.0061,0.004,0.0638,-0.0302,-0.0129,0.0349,-0.022,-0.0327,0.0426,0.0398,0.0576,0.0082,0.0724,0.0604,-0.02,-0.067,-0.2247,-0.0452,0.0009,-0.044,0.0367,-0.0514,0.0437,-0.0061,0.0717,0.0826,0.0838,0.0113,-0.0188,0.0107,-0.0172,0.0441,0.0282,0.0499,-0.0449,-0.0151,-0.0158,0.0456,-0.0131,-0.0087,0.0731,-0.0234,0.2187,0.0091,0.0456,-0.004,0.0322,0.0326,-0.0014,-0.1033,0.0463,0.0478,0.0237,0.0067,-0.0866,-0.0465,-0.0395,-0.0249,-0.0076,-0.0897,-0.0219,-0.0423,-0.0235,0.0343,-0.0357,0.0023,0.0652,-0.0267,0.0719,0.0184,-0.0135,-0.0319,-0.0823,0.0055,-0.034,0.0354,0.0028,-0.0426,-0.028,-0.0519,0.0262,0.0564,-0.006,-0.032,0.0098,-0.0038,-0.0093,0.0781,0.0272,-0.004,0.0559,0.029,0.0634,0.0225,-0.0114,-0.0042,0.0097,-0.0141,0.0353,-0.008,0.005,0.0217,0.069,-0.0476,-0.0097,-0.0411,0.0042,0.0425,-0.0666,-0.0507,0.0643,-0.0158,-0.2824,0.0668,0.0099,0.0115,-0.0158,0.0202,0.013,0.0227,-0.0126,-0.0238,-0.019,0.0339,0.0551,-0.0511,-0.0001,0.0371,0.0295,-0.0397,0.0569,-0.0212,-0.0389,0.05,0.2184,-0.0406,0.0158,0.0343,-0.0371,0.0189,0.0303,0.0342,0.025,0.0173,0.0903,-0.061,0.0464,0.1073,-0.0946,0.0649,-0.0174,-0.0438,0.0417,0.0692,-0.0263,0.0032,0.0657,0.0186,0.025,-0.0896,0.0045,0.0393,-0.0419,0.0109,-0.0335,-0.0062,0.0181,0.0459,0.0017,-0.0552,-0.0352,-0.014,0.0313,-0.0508,-0.0374,0.032,-0.0284]}
{"key":"[Dimensionless machine learning: Imposing exact units equivariance] Units equivariance is the exact symmetry that follows from the requirement that relationships among measured quantities of physics relevance must obey self-consistent dimensional scalings. Here, we employ dimensional analysis and ideas from equivariant machine learning to provide a two stage learning procedure for units-equivariant machine learning. For a given learning task, we first construct a dimensionless version of its inputs using classic results from dimensional analysis, and then perform inference in the dimensionless space. Our approach can be used to impose units equivariance across a broad range of machine learning methods which are equivariant to rotations and other groups. We discuss the in-sample and out-of-sample prediction accuracy gains one can obtain in contexts like symbolic regression and emulation, where symmetry is important. We illustrate our approach with simple numerical examples involving dynamical systems in physics and ecology.","layer":0,"vector":[-0.0376,-0.0345,0.0575,-0.0316,-0.0005,-0.0078,0.0207,0.0359,0.013,0.0009,0.0147,-0.0833,0.0039,0.0054,0.0359,-0.0314,0.0121,0.0767,-0.0765,0.0584,0.0453,-0.0654,-0.0137,-0.0393,0.0351,0.0535,-0.0428,-0.0432,-0.0124,-0.2317,0.0353,-0.0249,0.0269,-0.0161,0.0203,-0.0168,-0.0273,0.014,-0.0005,0.0466,0.0119,0.0107,-0.004,-0.0554,-0.0015,-0.0166,-0.0195,-0.0063,-0.0435,-0.012,0.0481,-0.0413,0.0003,0.0352,0.0164,0.0606,0.0773,0.0382,0.0615,0.0255,-0.0084,0.0252,-0.1591,0.049,0.0546,0.024,-0.0248,-0.0322,0.0176,0.0576,-0.0173,0.0471,0.0062,0.0299,0.0017,-0.0216,0.0086,-0.016,0.0095,0.0096,0.0148,0.0155,-0.0608,-0.0087,-0.0326,-0.0139,0.0193,-0.0274,0.0551,0.0353,-0.0763,-0.0103,-0.05,-0.0019,-0.0286,-0.0133,0.0468,0.033,-0.0187,0.1861,-0.0499,0.0512,0.0182,-0.0202,0.055,-0.0513,-0.0546,-0.0505,0.0198,-0.0228,0.0102,0.0007,0.0169,-0.0636,0.0284,-0.0268,0.0511,0.0396,-0.0255,0.0006,-0.0348,-0.002,0.0141,-0.0113,0.0117,-0.0678,0.0234,0.1016,0.0457,0.0027,0.0336,-0.0111,-0.0564,-0.0259,-0.0367,0.0467,0.061,0.0289,0.0011,0.0652,-0.0323,-0.0448,0.0367,-0.1115,-0.0598,0.1084,-0.0379,-0.0093,-0.0386,0.0143,0.0025,0.0469,-0.0299,-0.0493,0.0274,0.0831,-0.0205,0.0061,-0.0373,0.003,-0.0518,-0.0222,-0.0392,0.0951,-0.0126,-0.0749,-0.0423,0.0584,0.0026,-0.0182,0.0434,0.0453,-0.0507,0.0257,0.142,-0.0003,-0.0585,0.0153,-0.0034,0.0146,-0.0093,-0.0392,-0.0153,0.0754,0.0777,-0.0133,0.0101,-0.0236,0.0137,0.0114,0.0109,0.0263,-0.033,0.0116,-0.0582,0.005,0.0165,0.006,-0.0065,-0.0272,-0.0091,0.0289,-0.0213,0.0705,-0.009,0.0446,0.0272,-0.0002,0.0173,0.0468,-0.0289,-0.0105,0.0055,-0.0713,-0.0252,0.0168,-0.0163,0.0453,0.0043,0.0666,0.0232,-0.061,-0.0772,-0.2279,-0.0161,0.0332,-0.0142,0.0646,-0.0901,0.0307,-0.0589,0.0712,0.0504,0.0577,0.0282,-0.0516,-0.0039,0.0015,0.061,0.0358,0.0318,-0.0665,0.0039,-0.0193,0.046,-0.0345,-0.0784,0.0532,0.0034,0.2019,0.0583,0.0201,0.0017,-0.0209,0.0183,-0.0334,-0.0516,0.0698,0.0263,0.0393,0.0018,-0.023,-0.0782,-0.0311,-0.0053,0.024,-0.058,-0.042,-0.0392,-0.0168,0.0795,-0.0335,0.0035,0.0411,-0.0246,0.0729,-0.0226,-0.0062,-0.0384,-0.0763,0.02,-0.047,0.0028,-0.0019,-0.0147,0.006,-0.0716,0.0209,-0.0214,-0.0338,-0.0471,0.0271,-0.0188,-0.0365,0.1018,-0.0246,-0.0104,0.0453,-0.034,0.0559,0.0129,-0.0586,0.0103,0.0381,-0.012,0.0507,0.0305,0.0464,0.0143,0.0724,-0.0016,0.0625,-0.0149,-0.0353,-0.0029,-0.0656,0.0141,0.0053,0.0327,-0.2955,0.047,-0.0249,0.0508,-0.026,-0.0142,0.0305,-0.0009,-0.0652,-0.0295,0.0202,0.0054,0.0409,0.008,-0.0107,0.0488,0.0686,-0.0811,0.0201,-0.0618,0.0298,0.0218,0.2444,-0.0398,0.0258,0.0298,-0.0037,0.0301,-0.0013,-0.0449,0.037,-0.0285,0.0839,-0.0437,0.0254,0.071,-0.0379,-0.0028,0.0251,-0.0455,-0.0108,0.0163,-0.0009,-0.0521,0.1556,-0.027,-0.0116,-0.0512,-0.0019,0.0363,-0.0091,0.0229,-0.016,0.0368,0.0035,0.0223,-0.0868,-0.053,-0.0018,-0.11,-0.0103,-0.0602,-0.0325,-0.0288,-0.0359]}
{"key":"[Robust Imitation Learning from Noisy Demonstrations] Robust learning from noisy demonstrations is a practical but highly challenging problem in imitation learning. In this paper, we first theoretically show that robust imitation learning can be achieved by optimizing a classification risk with a symmetric loss. Based on this theoretical finding, we then propose a new imitation learning method that optimizes the classification risk by effectively combining pseudo-labeling with co-training. Unlike existing methods, our method does not require additional labels or strict assumptions about noise distributions. Experimental results on continuous-control benchmarks show that our method is more robust compared to state-of-the-art methods.","layer":2,"vector":[-0.0406,-0.0378,-0.0123,-0.0063,-0.0146,-0.0036,0.0517,0.0124,-0.0038,0.0025,0.0262,-0.0461,0.005,0.0556,-0.0089,0.021,0.0255,0.0866,-0.0342,0.007,0.0244,-0.0419,0.0093,-0.0594,0.019,0.0127,-0.0403,-0.0476,-0.0116,-0.2431,0.0061,-0.0652,0.0114,-0.0491,0.0079,-0.0309,-0.0488,0.0598,-0.0481,0.0274,-0.0039,0.0024,-0.001,-0.0912,-0.0111,-0.03,0.0071,-0.0598,-0.0072,-0.0161,0.0128,-0.0343,-0.0069,0.0037,0.0352,0.0343,0.1126,0.048,0.0575,0.0933,-0.014,0.0092,-0.1459,0.0635,0.0033,0.0437,-0.0587,-0.0379,0.043,0.0537,-0.0191,0.0663,0.0273,0.0989,0.0091,-0.026,-0.0232,-0.0432,-0.034,-0.0031,0.015,-0.0437,-0.0565,0.0038,-0.057,-0.0461,0.0673,-0.0583,0.0363,0.0131,-0.0179,-0.0023,-0.0186,0.0285,-0.0741,0.0098,0.0467,0.058,-0.0434,0.1938,-0.0398,0.0271,0.0517,-0.0134,0.0401,-0.0306,-0.0477,-0.0106,-0.0082,-0.007,-0.0604,-0.0156,0.0249,-0.0409,0.0132,0.0138,0.0978,0.0324,0.0089,-0.006,0.0123,0.0161,0.0805,-0.0083,0.0394,-0.0495,0.0152,0.146,0.0425,0.0343,0.0342,-0.0592,-0.0359,0.0083,0.0318,-0.0031,-0.0246,-0.02,0.0399,0.0114,0.0289,-0.0296,0.0311,-0.0718,-0.0423,0.0518,-0.0246,0.0499,-0.0142,-0.0292,-0.0277,-0.0013,0.0057,0.0089,0.0065,0.0296,0.0889,0.0914,-0.0364,-0.0104,-0.042,-0.065,-0.0032,0.0557,0.0275,-0.0774,-0.0602,-0.0038,0.0182,-0.0092,-0.0113,0.0321,-0.0563,0.0349,0.0613,0.0295,-0.0856,0.0028,0.0318,-0.0104,-0.0145,-0.0992,-0.0146,0.0089,0.0304,0.001,-0.0155,-0.0387,0.0459,0.068,-0.002,0.049,-0.0525,-0.0086,-0.0474,-0.0313,-0.0163,-0.0384,0.0201,-0.0422,-0.0225,-0.0155,-0.0076,-0.0049,-0.0031,-0.0128,0.0142,-0.015,0.065,0.0224,-0.0187,-0.0014,0.0254,0.0115,-0.0851,-0.0248,0.0287,0.087,0.0061,0.0131,0.0465,0.0134,-0.0339,-0.2354,0.0002,-0.0011,0.0189,0.0584,-0.0656,0.0362,-0.0511,0.0492,0.0231,0.0538,-0.038,-0.0383,-0.0054,-0.0129,0.0421,0.0317,0.0091,0.0039,0.0076,-0.0159,0.0287,0.0169,-0.0635,0.0681,-0.002,0.2412,0.0138,0.0705,-0.0057,0.0318,-0.0013,-0.0686,-0.0713,0.03,0.0297,0.0769,-0.0482,-0.0172,-0.0305,-0.0315,-0.0171,0.0368,-0.0938,-0.0222,-0.0104,-0.0715,0.052,-0.098,0.033,0.0445,-0.0024,0.0678,-0.0294,-0.0689,-0.0139,-0.0742,0.0228,-0.0499,0.0325,0.0326,-0.0504,0.0051,-0.06,0.0081,0.0195,0.003,-0.0558,0.0448,0.0156,-0.024,0.0607,0.002,0.0022,0.0377,0.0023,-0.0115,-0.0145,-0.0679,-0.0249,0.0301,-0.0027,0.0394,0.0244,0.0616,0.0118,0.0719,-0.0301,0.0504,-0.0452,0.0421,0.0062,-0.0562,0.0114,0.0536,-0.0055,-0.2902,-0.011,0.0282,0.0874,-0.0388,-0.0138,0.0509,-0.0307,-0.0493,0.0037,-0.0287,0.0391,0.0427,0.0306,0.0275,0.0641,0.0309,-0.0699,0.084,-0.0663,0.0003,0.0079,0.2308,-0.0442,0.0398,0.0082,-0.0261,0.0101,0.0092,-0.0388,0.0134,0.0088,0.0581,-0.0483,0.0139,0.0658,-0.0445,-0.0085,-0.002,0.0135,-0.0435,0.0042,-0.0165,-0.0512,0.1047,0.0161,0.0239,-0.0413,-0.0343,0.0373,-0.0218,0.0109,0.0202,-0.0101,-0.0154,0.0389,-0.0384,-0.0297,-0.0373,-0.0489,0.0397,-0.0404,0.039,-0.0045,-0.0094]}
{"key":"[Linked Component Analysis from Matrices to High Order Tensors: Applications to Biomedical Data] With the increasing availability of various sensor technologies, we now have access to large amounts of multi-block (also called multi-set, multi-relational, or multi-view) data that need to be jointly analyzed to explore their latent connections. Various component analysis methods have played an increasingly important role for the analysis of such coupled data. In this paper, we first provide a brief review of existing matrix-based (two-way) component analysis methods for the joint analysis of such data with a focus on biomedical applications. Then, we discuss their important extensions and generalization to multi-block multiway (tensor) data. We show how constrained multi-block tensor decomposition methods are able to extract similar or statistically dependent common features that are shared by all blocks, by incorporating the multiway nature of data. Special emphasis is given to the flexible common and individual feature analysis of multi-block data with the aim to simultaneously extract common and individual latent components with desired properties and types of diversity. Illustrative examples are given to demonstrate their effectiveness for biomedical data analysis.","layer":1,"vector":[-0.0452,0.01,0.0365,0.0117,0.0296,0.0278,0.0658,0.0232,0.032,-0.0188,0.004,-0.081,0.0157,0.0635,0.0298,0.0038,0.0057,0.0544,-0.0161,0.021,-0.009,-0.039,-0.0095,-0.0664,0.0509,0.0199,-0.0382,-0.0413,-0.0435,-0.2542,0.0343,-0.007,0.0688,-0.0155,-0.001,0.0041,-0.0208,0.0572,-0.0284,0.0254,0.0155,0.0279,-0.0123,-0.0105,-0.0317,-0.0285,-0.0384,0.0029,-0.0016,0.0019,-0.0027,-0.0266,0.0404,0.042,0.0653,0.0253,0.0437,0.0102,0.0087,0.0635,0.0472,0.0274,-0.1531,0.0535,0.1079,-0.0059,-0.0271,-0.0324,0.0212,0.0398,-0.0324,0.0501,0.0222,-0.0164,0.0207,-0.0015,0.0459,-0.0148,-0.0148,0.0208,0.0081,-0.0108,-0.0139,0.0062,0.0067,-0.0369,0.04,-0.0421,-0.0146,0.0173,-0.0622,-0.0533,-0.0296,0.0278,-0.0311,-0.07,0.0398,0.0268,-0.0036,0.2029,-0.083,0.0133,0.0296,0.021,0.0113,-0.0494,-0.0143,-0.0493,-0.0069,0.0377,0.0353,-0.0022,0.0108,-0.0516,0.0322,-0.0128,0.0435,0.043,0.0211,0.0068,-0.0431,-0.0067,0.0117,-0.0284,0.0169,-0.0666,0.0242,0.1561,0.0673,0.0488,0.0478,0.0428,-0.0741,-0.0287,-0.0023,0.0211,0.0449,0.0153,0.0071,-0.0031,-0.0366,-0.0672,0.0589,-0.0951,-0.0382,0.1771,-0.0557,-0.0176,-0.0407,0.0242,-0.0256,0.053,-0.0309,0.0079,0.004,0.0403,-0.0032,0.0179,-0.0414,0.0157,-0.0198,-0.064,0.0041,0.0816,0.0347,-0.0968,-0.003,0.0016,0.036,-0.0127,0.0686,0.0288,-0.0258,-0.0011,0.0599,-0.0181,-0.0666,0.0059,0.018,0.0311,0.0433,-0.0321,-0.0283,0.0386,-0.0069,-0.0295,-0.0159,0.0077,-0.0078,-0.0093,-0.0329,-0.0263,-0.0589,0.0027,-0.0717,-0.0141,-0.0303,0.0021,-0.003,-0.065,0.0265,0.0048,-0.05,0.0308,-0.0208,0.0271,-0.0335,0.0188,-0.0121,0.0361,-0.0186,-0.0031,0.038,-0.0049,-0.031,-0.0541,0.0117,0.0491,-0.0152,0.0554,0.0701,-0.0883,-0.0744,-0.245,-0.0013,0.0183,-0.0125,0.031,-0.0581,0.0434,-0.0039,0.0177,0.0938,0.0997,0.0624,-0.0575,0.0102,-0.0273,0.044,0.0524,0.0157,-0.0228,-0.0019,-0.0119,0.0289,-0.0025,-0.0652,0.0492,-0.0037,0.2258,0.0405,-0.0153,0.0035,0.0075,0.0529,-0.0317,-0.0611,0.0695,0.0277,0.0635,0.0134,-0.0142,-0.0407,-0.056,0.0184,0.0539,-0.0611,-0.0207,-0.0436,-0.027,-0.0109,-0.0479,-0.0202,0.0636,-0.0622,0.0083,-0.0303,0.0033,-0.0365,-0.0725,0.0015,-0.0493,-0.0033,0.0056,-0.0423,0.0246,-0.0968,0.0728,-0.0101,-0.0472,-0.0232,0.0224,-0.0607,-0.0629,0.1115,0.0042,-0.0013,0.0582,0.032,0.0591,-0.0196,-0.0332,0.0063,0.065,-0.0661,-0.0078,0.0308,0.041,-0.0241,0.0753,-0.0062,0.0263,-0.0526,0.0177,0.0006,-0.0313,-0.0048,0.0233,-0.0081,-0.2872,0.02,0.0339,-0.0073,-0.0144,-0.0192,0.0269,0.0092,-0.0528,-0.0229,-0.0161,0.0162,0.0639,-0.0368,-0.0073,0.038,0.0763,-0.0601,0.0583,-0.0354,-0.0158,0.0161,0.2073,-0.0063,0.0114,0.0066,-0.0198,0.0048,0.0145,-0.0077,0.0091,0.0088,0.0974,-0.0631,0.0212,0.0627,-0.0733,0.041,0.0281,-0.0246,0.0325,0.0084,-0.0602,-0.0588,0.0994,0.0019,-0.0258,-0.0377,0.018,-0.0202,-0.0201,0.0179,-0.0233,0.0319,0.0161,-0.0005,-0.0034,-0.0522,-0.0224,-0.0104,-0.0258,-0.0347,-0.0868,-0.0002,-0.0456]}
{"key":"[SymFormer: End-to-end symbolic regression using transformer-based architecture] Many real-world problems can be naturally described by mathematical formulas. The task of finding formulas from a set of observed inputs and outputs is called symbolic regression. Recently, neural networks have been applied to symbolic regression, among which the transformer-based ones seem to be the most promising. After training the transformer on a large number of formulas (in the order of days), the actual inference, i.e., finding a formula for new, unseen data, is very fast (in the order of seconds). This is considerably faster than state-of-the-art evolutionary methods. The main drawback of transformers is that they generate formulas without numerical constants, which have to be optimized separately, so yielding suboptimal results. We propose a transformer-based approach called SymFormer, which predicts the formula by outputting the individual symbols and the corresponding constants simultaneously. This leads to better performance in terms of fitting the available data. In addition, the constants provided by SymFormer serve as a good starting point for subsequent tuning via gradient descent to further improve the performance. We show on a set of benchmarks that SymFormer outperforms two state-of-the-art methods while having faster inference.","layer":0,"vector":[-0.0425,-0.0022,0.0491,-0.0256,0.016,0.0242,-0.0235,0.0745,0.0519,-0.0279,0.0033,-0.0675,0.0286,0.005,0.0183,-0.0345,-0.053,0.0536,-0.017,-0.0023,0.0413,-0.0295,-0.0049,-0.0905,0.0154,0.0136,-0.0349,-0.0157,-0.0475,-0.2329,-0.0221,-0.05,0.0478,-0.0325,-0.0061,0.0045,-0.0273,0.045,0.0109,0.0465,0.0046,0.0197,0.0224,-0.0049,-0.0024,-0.0249,0.0213,-0.0292,-0.0291,-0.0397,0.0358,-0.0036,0.0305,0.0843,-0.0168,0.0013,0.019,0.0497,0.0641,0.0021,0.0012,0.0433,-0.1654,0.0686,0.0548,0.0371,-0.0484,-0.0279,0.0292,0.0572,-0.0137,0.0268,0.0149,0.0208,0.0187,0.0298,0.0223,-0.0229,-0.0056,0.0484,0.0479,-0.0168,-0.0457,-0.0123,-0.0267,-0.0397,-0.0053,-0.0013,0.0401,0.0061,-0.0315,-0.0222,-0.0389,0.0414,-0.0705,-0.0172,0.048,0.0266,-0.0503,0.2125,-0.0246,0.0139,-0.0055,-0.0597,0.0243,-0.0226,-0.0279,-0.0296,-0.008,-0.001,-0.0453,-0.0476,0.0078,-0.0412,-0.0035,-0.0099,0.0426,0.0393,-0.0205,-0.0101,-0.0171,-0.0065,0.0423,0.0179,0.0543,-0.0793,0.0047,0.1245,0.0044,0.0332,0.0802,-0.0078,-0.0479,-0.0496,0.0226,0.0104,0.0261,-0.0302,-0.02,0.0187,-0.0672,-0.0371,0.032,-0.0637,-0.0411,0.1127,-0.0169,0.0073,-0.0855,-0.0107,-0.0128,0.0681,-0.0088,-0.0296,0.026,0.0433,0.0364,0.0314,-0.0764,-0.0059,-0.0505,-0.053,-0.0764,0.0852,0.0127,-0.0698,-0.0103,-0.0124,0.0553,-0.0186,0.03,0.0282,-0.0033,-0.0197,0.0676,0.0189,0.0221,0.0163,-0.0143,0.0342,0.0599,-0.0624,-0.001,0.0452,0.0285,-0.019,0.0045,-0.0596,-0.009,0.0578,-0.0007,0.0633,-0.0621,-0.0372,-0.0222,-0.0189,-0.0003,0.0225,0.0419,-0.0182,-0.0013,0.0095,-0.004,0.0158,-0.0036,-0.0114,0.0019,-0.0337,0.0255,0.042,-0.0268,-0.0281,0.099,-0.0321,-0.0295,0.0449,0.0324,0.0379,0.0167,0.0691,0.0329,-0.0437,-0.0976,-0.2189,-0.0329,0.0125,-0.0431,0.0658,-0.0678,0.0251,-0.0074,0.0242,0.0497,0.0349,-0.0011,-0.0729,0.0477,-0.0221,0.0505,0.0129,0.0022,-0.0231,0.0217,-0.0321,-0.0452,0.0514,-0.0976,0.0662,-0.0105,0.2121,0.0205,0.047,-0.0139,0.0036,0.0066,-0.0449,-0.0564,0.0634,-0.0129,0.0852,-0.0201,-0.0137,-0.0076,-0.0377,0.0259,0.0265,-0.0522,-0.0194,-0.006,0.0045,0.0067,-0.0587,0.0622,0.0101,-0.0474,0.0979,-0.0051,0.0106,-0.0501,-0.086,0.0241,-0.0091,0.0201,0.018,-0.0739,0.0284,-0.0502,0.0172,0.0069,-0.026,-0.05,0.0241,-0.0139,-0.0139,0.11,0.0248,-0.0141,0.0616,0.018,0.0171,-0.0375,-0.0272,-0.0008,0.0541,-0.0638,0.0496,-0.0098,-0.0043,-0.0214,0.0659,0.0286,0.0712,-0.0059,-0.0158,-0.0215,-0.0478,0.0143,0.0273,0.0059,-0.3029,0.0534,0.0248,0.0282,-0.0208,-0.031,0.0265,0.0297,-0.051,-0.003,-0.0168,0.0092,0.0642,0.0161,0.0522,0.0133,0.0717,-0.0743,0.0663,-0.0605,-0.0017,0.0382,0.2282,-0.0472,0.0301,-0.0039,-0.0328,-0.0138,0.0726,-0.0487,0.0417,-0.0108,0.09,-0.0594,0.0531,0.073,-0.0806,0.0053,0.037,-0.02,0.0171,0.0243,-0.0741,-0.0525,0.1072,-0.0398,-0.0483,-0.0119,0.0009,0.0457,-0.0236,0.0427,-0.0343,-0.0323,0.0365,0.011,-0.0938,-0.0629,-0.0372,-0.0529,0.003,-0.0579,-0.014,-0.0189,-0.0145]}
{"key":"[Fast Convergence for Langevin Diffusion with Manifold Structure] In this paper, we study the problem of sampling from distributions of the form p(x) \\propto e^{-\\beta f(x)} for some function f whose values and gradients we can query. This mode of access to f is natural in the scenarios in which such problems arise, for instance sampling from posteriors in parametric Bayesian models. Classical results show that a natural random walk, Langevin diffusion, mixes rapidly when f is convex. Unfortunately, even in simple examples, the applications listed above will entail working with functions f that are nonconvex -- for which sampling from p may in general require an exponential number of queries. In this paper, we focus on an aspect of nonconvexity relevant for modern machine learning applications: existence of invariances (symmetries) in the function f, as a result of which the distribution p will have manifolds of points with equal probability. First, we give a recipe for proving mixing time bounds for Langevin diffusion as a function of the geometry of these manifolds. Second, we specialize our arguments to classic matrix factorization-like Bayesian inference problems where we get noisy measurements A(XX^T), X \\in R^{d \\times k} of a low-rank matrix, i.e. f(X) = \\|A(XX^T) - b\\|^2_2, X \\in R^{d \\times k}, and \\beta the inverse of the variance of the noise. Such functions f are invariant under orthogonal transformations, and include problems like matrix factorization, sensing, completion. Beyond sampling, Langevin dynamics is a popular toy model for studying stochastic gradient descent. Along these lines, we believe that our work is an important first step towards understanding how SGD behaves when there is a high degree of symmetry in the space of parameters the produce the same output.","layer":1,"vector":[-0.061,-0.0461,0.019,0.0045,0.0177,0.0423,0.0448,0.0124,0.0552,0.0198,0.02,-0.0535,0.0086,0.0653,0.0139,0.0761,-0.0234,0.0288,-0.0309,-0.0036,0.0126,-0.0364,-0.0045,-0.0188,0.0529,0.0119,0.0211,-0.0458,-0.0345,-0.2678,0.0336,-0.0429,0.0313,-0.0218,-0.0061,-0.0206,-0.0008,-0.0033,0.0177,0.0606,-0.02,0.0358,-0.0577,-0.0142,-0.0267,-0.0397,-0.0524,-0.0351,-0.0444,-0.0447,-0.0062,-0.0268,0.0169,0.0326,0.037,0.0269,0.0629,0.0214,0.0651,0.0491,0.0118,0.0071,-0.1624,0.0518,0.0675,0.014,-0.0296,-0.0575,-0.0181,0.077,-0.0181,0.0607,-0.0173,0.0567,0.0403,-0.006,0.0188,-0.0106,-0.0235,0.0506,0.0274,-0.0041,-0.0233,0.0036,-0.0522,-0.0279,0.0551,-0.0636,0.0472,0.0125,-0.0271,0.0001,-0.0756,0.0518,-0.0704,-0.0007,0.0454,0.0728,0.0408,0.185,-0.0523,0.0776,0.0393,0.033,-0.0045,-0.0332,-0.0236,-0.0189,-0.0183,0.0127,-0.0082,-0.0626,0.0123,-0.0655,-0.006,-0.0662,0.0507,0.0402,-0.0247,0.0036,-0.0203,0.0121,0.0845,-0.0182,0.0364,-0.0609,-0.0099,0.1454,0.0697,0.042,0.0477,-0.0061,-0.0467,-0.0231,0.0177,-0.0278,-0.0353,0.0024,0.0151,0.016,-0.0372,-0.0813,0.0356,-0.0829,-0.0199,0.1334,-0.018,0.0142,-0.0396,-0.0651,0.0339,0.0424,-0.0123,-0.0368,0.0115,0.0052,0.0083,0.0211,-0.0727,0.0337,-0.0631,-0.0431,0.0165,0.1542,-0.0096,-0.0392,-0.0063,-0.0213,0.0168,0.0275,0.0692,0.0347,-0.0554,0.0247,0.0778,0.0488,-0.0976,-0.0276,0.0045,0.0098,0.002,-0.0309,-0.0388,0.0075,0.0333,-0.0426,0.0234,-0.026,0.0321,0.0428,-0.011,-0.0107,0.0041,-0.0232,-0.0464,-0.033,-0.0209,0.0188,0.0184,0.003,-0.0139,-0.02,-0.0573,0.0699,0.005,0.0295,-0.0241,0.0001,-0.0,0.0673,-0.0072,-0.0243,0.0271,0.0027,-0.0316,0.0093,-0.0009,0.0112,-0.0053,0.0382,0.0002,-0.0171,-0.0553,-0.1863,-0.0087,-0.0052,0.053,0.0668,-0.0784,0.0048,-0.0148,0.0861,0.0938,0.0414,-0.0008,-0.0083,0.0181,0.0103,0.0506,0.0194,0.0234,0.0026,0.0166,-0.0142,-0.0017,-0.0555,-0.0615,0.0721,-0.0314,0.2137,0.0305,0.0374,-0.0123,0.0162,0.0388,-0.006,-0.0593,0.044,0.027,0.0805,0.0206,-0.0328,-0.0571,-0.0451,0.0085,0.003,-0.0622,-0.0293,-0.0351,-0.0684,0.0277,-0.061,-0.0053,0.044,-0.0467,0.087,-0.0275,0.002,-0.0571,-0.053,-0.02,-0.0528,0.0503,-0.0229,-0.0524,0.0666,-0.0535,0.0467,-0.017,-0.0307,-0.0505,0.0071,-0.0274,-0.0096,0.0735,-0.0286,-0.0024,0.0724,0.0077,0.031,-0.026,-0.0588,-0.0575,0.0724,-0.0493,0.0265,0.0466,0.0311,-0.0259,0.056,-0.0305,-0.0042,-0.0061,-0.0169,0.0065,-0.0568,0.0306,0.0413,0.0197,-0.275,0.0392,0.0144,0.0187,-0.0172,-0.0077,0.0528,-0.0171,-0.0661,-0.0344,0.0178,0.0589,0.0383,-0.0043,0.0119,0.0458,0.0545,-0.0531,0.0427,-0.0981,-0.0305,0.0317,0.227,-0.0524,0.0321,0.0066,-0.0055,0.0138,0.0124,-0.0665,0.0099,-0.0088,0.0692,-0.062,0.0643,0.0823,-0.0483,0.0558,0.0146,-0.0355,0.0341,-0.0271,-0.0189,-0.0149,0.0676,-0.0542,-0.0139,-0.0117,0.0172,-0.0047,-0.0196,0.0205,0.051,-0.0004,0.0001,-0.0248,-0.0733,-0.0577,0.013,-0.0506,-0.0048,-0.0807,-0.0465,-0.0444,-0.0213]}
{"key":"[The IMS-CUBoulder System for the SIGMORPHON 2020 Shared Task on Unsupervised Morphological Paradigm Completion] In this paper, we present the systems of the University of Stuttgart IMS and the University of Colorado Boulder (IMS-CUBoulder) for SIGMORPHON 2020 Task 2 on unsupervised morphological paradigm completion (Kann et al., 2020). The task consists of generating the morphological paradigms of a set of lemmas, given only the lemmas themselves and unlabeled text. Our proposed system is a modified version of the baseline introduced together with the task. In particular, we experiment with substituting the inflection generation component with an LSTM sequence-to-sequence model and an LSTM pointer-generator network. Our pointer-generator system obtains the best score of all seven submitted systems on average over all languages, and outperforms the official baseline, which was best overall, on Bulgarian and Kannada.","layer":5,"vector":[-0.0497,-0.0412,-0.0091,0.0138,-0.0121,0.0015,0.0074,0.0553,0.0265,-0.0269,0.0484,-0.0665,0.0336,0.064,0.0364,0.0484,0.0027,0.0525,-0.0159,-0.0111,0.0641,-0.0452,-0.0213,-0.0001,0.036,0.0229,-0.0091,-0.0393,-0.0273,-0.2387,0.0334,-0.0317,0.0653,-0.0255,-0.0467,0.0087,-0.0564,0.0033,-0.0318,0.0542,0.016,0.0262,0.0042,-0.0769,-0.0048,-0.07,-0.0575,-0.0081,-0.0267,-0.0648,-0.0174,-0.0646,0.0303,0.0453,0.0141,0.0217,0.0731,-0.0187,0.0309,0.0358,0.0284,-0.0019,-0.2235,0.0607,0.005,0.0315,-0.0522,-0.0178,0.021,0.0782,-0.0222,0.0326,0.0367,0.0555,0.0058,0.0004,-0.0207,-0.0505,-0.0397,0.005,0.026,0.0111,-0.0542,-0.0355,-0.0003,-0.012,0.0038,-0.0546,0.0094,-0.0085,-0.0357,-0.0335,0.012,0.0411,-0.0699,-0.0341,-0.0106,0.0444,0.007,0.1994,-0.0852,0.0097,0.0501,-0.0608,0.0077,-0.0087,-0.04,-0.032,-0.024,-0.0044,-0.0254,-0.0387,0.0448,-0.0266,0.0385,-0.0013,0.1076,0.0186,-0.0152,-0.011,-0.0576,0.022,0.0031,-0.0298,0.0315,-0.0404,0.0476,0.0632,0.0611,0.0256,0.0558,0.0144,-0.0349,-0.0182,-0.0001,-0.0284,-0.0365,0.0048,0.0297,-0.0039,-0.016,-0.1081,0.0101,-0.0745,-0.0429,0.1603,-0.0647,0.0221,-0.0547,0.0308,-0.0364,0.013,-0.0074,-0.054,0.0216,0.0266,0.0437,0.0002,-0.047,0.0299,-0.029,-0.0509,-0.039,0.0978,0.0105,-0.0611,-0.0493,0.0348,0.0351,-0.012,0.0479,0.0655,-0.0512,0.0193,0.0661,0.0822,-0.0527,-0.0059,-0.0017,0.0232,0.0587,-0.0363,-0.0519,0.046,-0.022,-0.0666,-0.0164,-0.0665,0.0522,-0.0322,-0.0307,0.0203,-0.0217,0.0046,0.03,-0.0041,-0.0196,-0.0057,0.0052,-0.0432,0.0294,0.0256,0.0029,0.0165,0.0492,-0.0005,-0.0126,0.0167,0.0752,0.0409,-0.0214,-0.0556,0.0804,-0.0029,-0.0286,-0.0221,-0.0115,0.0412,0.0078,0.0423,-0.045,-0.0381,-0.037,-0.2217,-0.0033,0.0663,-0.0615,0.044,-0.0536,0.0168,0.0363,0.0748,-0.0071,0.0352,-0.028,-0.0419,0.0,-0.0183,0.0481,0.0494,0.0191,-0.013,0.0374,0.0138,-0.0043,-0.0051,-0.0813,0.0068,-0.0301,0.2271,0.0566,0.0383,-0.0412,0.0527,0.0222,-0.0563,-0.0881,0.0394,0.0043,0.067,-0.0213,-0.0182,-0.0127,-0.0099,0.0414,0.0026,-0.0804,-0.0267,-0.0297,-0.0795,-0.006,-0.0202,0.058,0.0199,-0.029,0.0552,0.006,-0.0333,-0.027,-0.1435,0.0168,-0.0118,0.0279,0.0154,-0.0347,0.0303,-0.086,0.0018,-0.0078,-0.0374,-0.0101,0.0089,-0.0213,-0.0368,0.0746,-0.0427,-0.0147,0.0389,0.0427,0.0429,-0.0362,-0.0482,-0.0229,0.0329,-0.0327,0.0649,-0.0024,0.0282,0.0607,0.0965,-0.0027,0.0371,-0.0003,0.063,0.0041,-0.0161,0.0449,0.0503,-0.0087,-0.2627,0.0679,-0.0108,0.0482,0.0087,0.0069,0.0308,-0.0084,-0.0392,0.0301,0.0122,0.003,0.0445,-0.0339,-0.0197,0.0508,0.0961,-0.0048,0.0904,-0.0615,-0.0274,0.0628,0.1858,-0.0463,0.0144,0.0043,-0.0226,0.012,0.0217,-0.0162,0.0586,0.0067,0.0963,-0.0227,0.0128,0.0809,-0.0296,0.0455,0.0391,-0.0175,-0.0292,0.0242,-0.0539,-0.0094,0.0986,-0.0255,0.0155,-0.0286,-0.0015,0.0339,-0.0295,0.0038,-0.0285,0.0237,0.0364,-0.0071,-0.0286,-0.0518,-0.0558,-0.0272,-0.0255,-0.0475,-0.0039,0.0278,-0.0135]}
{"key":"[Large-scale Pretraining for Visual Dialog: A Simple State-of-the-Art Baseline] Prior work in visual dialog has focused on training deep neural models on VisDial in isolation. Instead, we present an approach to leverage pretraining on related vision-language datasets before transferring to visual dialog. We adapt the recently proposed ViLBERT (Lu et al., 2019) model for multi-turn visually-grounded conversations. Our model is pretrained on the Conceptual Captions and Visual Question Answering datasets, and finetuned on VisDial. Our best single model outperforms prior published work (including model ensembles) by more than 1% absolute on NDCG and MRR. Next, we find that additional finetuning using \"dense\" annotations in VisDial leads to even higher NDCG -- more than 10% over our base model -- but hurts MRR -- more than 17% below our base model! This highlights a trade-off between the two primary metrics -- NDCG and MRR -- which we find is due to dense annotations not correlating well with the original ground-truth answers to questions.","layer":0,"vector":[-0.0407,0.0042,-0.0033,-0.0193,0.0051,0.0272,0.0315,-0.0095,-0.006,-0.0199,-0.0243,-0.0691,0.0455,0.0753,0.0838,0.0013,0.0643,-0.0319,-0.0626,0.0176,0.0196,-0.0331,-0.0216,-0.04,-0.0136,0.0106,-0.0276,-0.0665,-0.0122,-0.2297,0.0295,-0.0154,0.0484,0.0041,-0.0237,-0.0473,-0.0299,0.049,-0.0093,0.0163,-0.0089,0.0061,-0.0287,-0.0749,-0.0386,-0.0489,-0.0192,-0.0274,-0.0299,-0.0632,0.0175,-0.0747,0.0196,0.0166,-0.0083,0.0412,0.0567,0.0007,0.0605,0.0684,0.029,0.0289,-0.1873,0.0926,0.012,0.0099,-0.0341,0.0233,-0.0146,0.0287,-0.0231,0.0234,0.0186,0.013,0.0007,-0.0288,-0.0212,-0.0434,0.0318,-0.0315,0.0145,0.0149,-0.0402,-0.0266,0.0167,-0.041,0.012,-0.0154,0.0311,-0.0371,-0.0518,-0.025,-0.01,0.0284,-0.0049,-0.0105,0.0275,0.0066,-0.0423,0.2268,-0.0434,0.0326,0.0412,-0.0487,0.0284,-0.0116,0.0076,-0.0112,0.0066,0.0284,-0.0317,0.0001,0.0486,-0.0326,0.0502,0.0337,0.0954,0.0159,-0.0259,0.0167,-0.0126,0.0094,0.0192,-0.0421,0.0161,-0.0869,0.0424,0.1284,0.0471,0.0061,0.0934,-0.02,-0.0368,-0.0031,0.0193,0.0233,0.0852,-0.0021,0.0295,-0.0129,-0.0243,-0.059,-0.0106,-0.0568,-0.0498,0.1169,-0.0488,0.022,-0.054,-0.014,-0.0025,0.039,-0.0038,-0.0355,0.0493,-0.0034,0.0184,0.047,-0.0465,0.0194,0.0187,-0.0571,-0.0346,0.0737,0.0141,-0.096,-0.0685,-0.0184,-0.0052,-0.0635,0.045,0.0458,0.0233,0.0096,0.0731,0.0853,-0.118,0.0286,-0.0186,0.0116,0.0337,-0.0597,-0.0186,0.0461,0.0095,-0.1047,0.013,-0.0602,0.0115,-0.0004,0.0238,0.036,-0.0404,0.0246,-0.0396,0.0188,-0.0093,-0.0182,-0.017,-0.0442,-0.0051,-0.0323,-0.0328,-0.0082,-0.015,-0.0129,-0.0159,0.0495,0.0971,0.0415,-0.0362,-0.006,0.0426,0.0172,-0.0208,-0.0516,0.0211,0.02,-0.0288,0.0205,0.0464,-0.0366,-0.0367,-0.2596,-0.0022,0.0042,-0.0327,0.0461,-0.0227,0.031,-0.0023,0.0906,0.0513,0.0616,-0.0279,-0.0334,0.0436,-0.0193,0.0548,0.0164,0.04,-0.0245,-0.0204,0.0342,0.0234,-0.0216,-0.1182,0.0708,0.0028,0.2077,0.0409,0.0282,-0.0386,0.0477,0.0378,-0.0379,-0.107,0.1029,0.0012,0.0745,0.0169,-0.0114,-0.0149,-0.0172,0.0057,-0.0076,-0.0922,-0.0681,-0.0356,-0.0169,0.0194,-0.0634,0.0111,-0.0035,-0.0526,0.0562,0.0205,-0.0138,-0.0434,-0.1247,-0.0187,-0.0688,0.0341,0.0047,0.0005,0.0185,-0.0552,0.0175,0.0236,-0.0451,-0.0522,0.0195,-0.043,-0.0547,0.0493,-0.0327,0.0397,0.0164,0.056,0.0326,-0.0177,-0.0597,-0.0356,0.0517,0.0051,0.0144,-0.0225,0.0583,0.0238,0.0864,-0.0156,0.0661,0.039,0.0151,0.0132,-0.0667,-0.0555,0.0473,-0.0178,-0.2854,0.0349,0.0529,0.0281,-0.0006,0.082,0.0586,-0.003,-0.0463,-0.0211,-0.0015,0.0749,0.0876,-0.0246,-0.0239,0.0198,0.095,-0.0082,0.043,-0.0368,0.0206,-0.0032,0.1484,0.0052,0.0229,-0.0026,-0.029,-0.0561,0.0344,-0.0144,0.0072,-0.0039,0.1016,-0.0166,0.0581,0.0099,0.0054,0.0269,0.04,0.0091,-0.0237,0.0271,-0.0021,-0.0447,0.0715,0.0224,0.0182,0.0118,-0.0114,-0.0205,-0.0069,-0.0024,-0.0052,0.0067,0.01,0.0233,-0.0065,-0.0109,-0.0424,-0.0133,-0.017,-0.0574,-0.0072,0.0183,-0.0251]}
{"key":"[TensorNetwork: A Library for Physics and Machine Learning] TensorNetwork is an open source library for implementing tensor network algorithms. Tensor networks are sparse data structures originally designed for simulating quantum many-body physics, but are currently also applied in a number of other research areas, including machine learning. We demonstrate the use of the API with applications both physics and machine learning, with details appearing in companion papers.","layer":1,"vector":[-0.0918,-0.0012,0.0119,0.0182,0.0022,0.0101,0.0106,-0.0006,0.0298,-0.0485,0.0273,-0.0741,0.0154,0.0481,0.043,-0.0066,0.0071,0.0168,-0.0394,0.0166,0.0233,-0.0576,0.0021,-0.0669,0.0359,0.018,-0.0059,-0.0244,-0.0562,-0.2097,0.0302,-0.0607,0.0215,-0.0156,0.0109,-0.019,-0.028,0.0254,-0.0351,0.0465,0.0593,0.0305,-0.0401,-0.0184,0.0188,-0.0423,-0.0195,-0.0177,-0.041,-0.0194,0.0126,-0.0576,0.0212,0.0363,0.0527,0.0302,0.0596,0.0185,0.0402,0.0345,0.0069,0.0373,-0.165,0.059,0.0886,0.0256,-0.0147,-0.0288,0.0521,0.0715,-0.0142,0.0666,0.0223,0.0461,0.0156,-0.009,0.0431,-0.0157,0.013,-0.0115,0.0044,-0.0439,-0.018,-0.0138,-0.0279,-0.0019,0.0143,0.0059,0.0022,-0.0095,-0.0557,-0.0312,-0.048,0.0356,-0.0348,-0.0103,0.0349,0.0297,-0.0131,0.2393,-0.0639,0.0051,0.0206,0.0365,0.0192,-0.0575,-0.0066,-0.0309,-0.0338,0.0271,0.0208,0.0036,0.0252,-0.0859,0.0253,-0.0238,0.0384,0.0279,-0.0372,0.0002,-0.0661,0.0101,0.036,-0.0049,-0.0152,-0.044,-0.0377,0.1284,0.042,0.0774,0.0486,-0.0074,-0.0511,-0.0534,0.0224,0.018,0.0351,0.0112,-0.0105,0.0316,-0.0179,-0.0545,0.0288,-0.1413,-0.0671,0.1295,-0.0504,0.015,-0.0254,0.0132,-0.0345,0.0532,-0.0572,-0.0185,0.0199,0.0503,0.0158,0.0573,-0.083,0.0413,-0.0266,-0.0779,-0.0413,0.0771,0.0134,-0.0885,-0.0026,0.0219,0.0068,-0.0318,0.0461,0.0369,-0.0053,0.0413,0.0908,-0.0183,-0.0646,-0.0118,-0.0116,0.0326,-0.0183,-0.0162,-0.0299,0.0444,0.0293,-0.0184,0.0001,-0.0365,-0.0146,-0.0049,-0.0335,0.0465,-0.0725,-0.0316,-0.0587,-0.0358,-0.0097,-0.0105,-0.0363,-0.0098,0.022,0.0142,-0.0153,0.0287,-0.0324,0.0447,-0.0111,-0.0008,-0.0015,0.0481,-0.0114,-0.0294,0.024,-0.0424,-0.0616,-0.0156,0.0188,0.0524,-0.0237,0.0449,0.045,-0.0951,-0.1125,-0.2249,-0.001,-0.0018,-0.0355,0.088,-0.0831,0.0427,-0.0244,0.0222,0.0756,0.081,0.0522,-0.042,-0.0173,0.0008,0.0299,0.0192,0.001,-0.0215,0.0088,-0.0045,0.0389,-0.0222,-0.0729,0.0193,-0.0115,0.2142,0.0788,0.0161,-0.0091,0.0169,0.028,-0.027,-0.0588,0.0655,0.035,0.115,0.0506,-0.031,-0.0185,-0.0518,0.0404,0.0179,-0.0726,-0.0114,-0.0206,-0.0259,0.0069,-0.0355,0.0159,0.0315,-0.0373,0.0423,-0.0148,-0.0565,-0.0548,-0.0546,0.0298,-0.0206,0.0203,-0.0001,-0.0614,0.004,-0.0256,0.0552,-0.019,-0.0219,-0.0109,0.0553,-0.05,-0.0183,0.0677,-0.0112,-0.0145,0.0882,-0.0127,0.0243,0.0105,-0.0027,0.0124,0.0681,-0.0141,0.0152,0.0318,0.0168,0.0017,0.0973,0.0074,0.0196,-0.0409,-0.0224,0.0009,-0.0188,0.0208,0.0462,0.001,-0.3051,0.0465,0.0271,0.0509,-0.0549,-0.0172,0.0684,0.0296,-0.0562,-0.0381,0.0069,0.0426,0.0552,-0.0152,-0.0389,0.0572,0.0816,-0.0199,0.0037,-0.0417,0.0159,0.0475,0.2406,-0.0226,0.0359,0.0375,-0.0151,0.0295,0.049,-0.0296,-0.0403,0.0221,0.0675,-0.0707,0.0436,0.0314,-0.0397,0.0153,0.0624,-0.0406,0.0116,-0.0076,-0.028,-0.0414,0.0682,-0.0331,-0.0254,-0.0628,0.0176,-0.0009,-0.0042,0.0091,0.0084,0.0019,0.0017,0.0141,-0.0118,-0.044,-0.0264,-0.0165,0.0019,-0.0549,-0.0181,-0.0244,-0.0344]}
{"key":"[Group-driven Reinforcement Learning for Personalized mHealth Intervention] Due to the popularity of smartphones and wearable devices nowadays, mobile health (mHealth) technologies are promising to bring positive and wide impacts on people's health. State-of-the-art decision-making methods for mHealth rely on some ideal assumptions. Those methods either assume that the users are completely homogenous or completely heterogeneous. However, in reality, a user might be similar with some, but not all, users. In this paper, we propose a novel group-driven reinforcement learning method for the mHealth. We aim to understand how to share information among similar users to better convert the limited user information into sharper learned RL policies. Specifically, we employ the K-means clustering method to group users based on their trajectory information similarity and learn a shared RL policy for each group. Extensive experiment results have shown that our method can achieve clear gains over the state-of-the-art RL methods for mHealth.","layer":1,"vector":[-0.0231,-0.0013,0.0367,-0.0278,0.004,-0.0066,0.0315,0.0287,0.0204,0.0,0.0505,-0.0356,0.0501,0.0492,0.0132,0.0094,-0.009,0.0246,-0.049,0.0048,-0.0165,-0.0405,-0.018,-0.0399,0.0199,0.0249,-0.0382,-0.064,-0.0374,-0.1846,0.0144,-0.0348,0.0401,-0.023,-0.0523,0.0048,-0.0518,0.0605,-0.0308,0.0341,0.0026,0.0273,0.0016,-0.0584,-0.0207,-0.0193,-0.0429,0.0018,-0.0268,-0.0466,0.0438,-0.0244,-0.0031,0.0507,0.0658,0.0438,0.0165,0.0306,0.0371,0.0069,0.0061,0.0265,-0.1625,0.0939,0.02,0.0447,0.0083,-0.0061,0.0145,0.004,-0.0227,0.0889,0.0271,0.022,0.0112,0.027,0.0186,-0.0622,-0.0388,0.0248,0.0188,-0.0447,-0.01,-0.0162,0.0001,-0.075,0.0123,-0.1018,0.045,0.0003,-0.0267,0.0138,-0.0262,0.0173,-0.098,-0.0037,0.0021,0.0168,-0.0888,0.2255,0.0003,0.0688,0.0247,0.0178,0.0326,-0.0474,-0.033,-0.0325,-0.0012,-0.0076,-0.0357,0.0076,0.0254,-0.0352,0.0089,0.0349,0.0528,0.0202,0.0384,-0.0135,0.0015,-0.0269,0.0612,-0.0043,0.0471,-0.0588,0.0044,0.1611,0.0292,-0.0063,0.056,-0.023,-0.0402,-0.0315,-0.0199,0.0288,-0.0042,-0.0173,0.0085,0.0054,-0.0136,-0.0199,0.0208,-0.1484,-0.0393,0.1099,0.0103,0.0149,-0.0283,-0.0241,-0.0074,-0.012,-0.055,0.012,-0.015,0.0301,0.0724,0.0236,-0.0378,0.0054,-0.023,-0.0801,-0.0418,0.1129,0.0108,-0.0658,-0.0084,0.0145,0.0391,-0.0394,0.0236,0.0741,-0.0299,0.0642,0.0574,0.0455,-0.0731,0.0173,0.0079,-0.0278,0.0372,-0.0352,-0.0394,-0.0222,0.0542,-0.0224,-0.0038,-0.0122,0.074,0.0325,-0.0246,0.0011,-0.0185,-0.0321,-0.014,-0.0483,-0.0009,-0.0227,0.007,-0.0345,-0.0015,0.0001,-0.0456,0.0417,-0.0134,0.0173,-0.0408,-0.0273,0.0749,-0.0202,-0.0167,0.0307,0.0878,-0.015,-0.0165,-0.0089,0.0084,0.004,0.009,0.0677,0.0672,0.0317,-0.0544,-0.2025,-0.0233,0.0072,-0.0157,0.002,-0.0145,0.0236,0.0047,0.0646,0.0538,0.0954,-0.0306,-0.0262,0.0607,-0.0078,0.0333,0.0558,0.0377,-0.0376,-0.0134,-0.0091,0.0081,-0.0444,-0.0973,0.0633,0.0195,0.257,0.0678,-0.0114,0.0121,0.0257,0.0567,-0.0244,-0.1202,0.0393,0.0169,0.0527,-0.0404,-0.0405,-0.0685,-0.0399,0.0346,-0.0396,-0.0803,-0.0366,-0.028,-0.047,0.0614,-0.0921,-0.0284,0.0326,-0.0346,0.032,-0.0398,0.0102,-0.0258,-0.0875,0.0403,-0.0552,0.017,-0.012,-0.0254,0.0108,-0.0632,0.0465,-0.0569,-0.0134,-0.0173,0.0117,-0.016,-0.0092,0.0683,-0.0113,0.0053,0.012,0.0289,0.0223,-0.0114,-0.0597,-0.0204,0.0537,-0.0129,0.0344,0.0632,-0.0108,-0.0581,0.0529,0.005,-0.005,-0.0379,0.0083,-0.0336,-0.0219,-0.0305,0.036,-0.0378,-0.288,0.0638,-0.0073,0.0631,-0.0588,-0.0061,0.0214,-0.0155,-0.0259,0.0287,0.0337,0.0742,0.0938,-0.0147,0.0368,0.014,0.075,-0.0361,0.0293,-0.1012,0.0478,0.0374,0.1881,-0.0486,0.0472,0.0457,-0.0039,-0.0132,0.0492,-0.0035,-0.0529,0.0007,0.0873,-0.0214,0.0667,0.0543,-0.0187,0.0113,0.0272,0.0309,-0.0032,-0.0087,-0.0067,-0.0243,0.1104,0.0079,-0.0507,-0.0739,-0.0234,0.0464,-0.0144,0.0146,-0.0713,0.0358,0.0157,0.0568,-0.0577,-0.0661,-0.0435,-0.0444,-0.0188,-0.0233,-0.0018,-0.0183,0.023]}
{"key":"[Brain Graph Super-Resolution Using Adversarial Graph Neural Network with Application to Functional Brain Connectivity] Brain image analysis has advanced substantially in recent years with the proliferation of neuroimaging datasets acquired at different resolutions. While research on brain image super-resolution has undergone a rapid development in the recent years, brain graph super-resolution is still poorly investigated because of the complex nature of non-Euclidean graph data. In this paper, we propose the first-ever deep graph super-resolution (GSR) framework that attempts to automatically generate high-resolution (HR) brain graphs with N' nodes (i.e., anatomical regions of interest (ROIs)) from low-resolution (LR) graphs with N nodes where N < N'. First, we formalize our GSR problem as a node feature embedding learning task. Once the HR nodes' embeddings are learned, the pairwise connectivity strength between brain ROIs can be derived through an aggregation rule based on a novel Graph U-Net architecture. While typically the Graph U-Net is a node-focused architecture where graph embedding depends mainly on node attributes, we propose a graph-focused architecture where the node feature embedding is based on the graph topology. Second, inspired by graph spectral theory, we break the symmetry of the U-Net architecture by super-resolving the low-resolution brain graph structure and node content with a GSR layer and two graph convolutional network layers to further learn the node embeddings in the HR graph. Third, to handle the domain shift between the ground-truth and the predicted HR brain graphs, we incorporate adversarial regularization to align their respective distributions. Our proposed AGSR-Net framework outperformed its variants for predicting high-resolution functional brain graphs from low-resolution ones. Our AGSR-Net code is available on GitHub at https://github.com/basiralab/AGSR-Net.","layer":5,"vector":[0.0044,-0.04,-0.0047,-0.0288,0.0283,0.058,-0.0005,0.0284,0.0141,-0.0059,0.0191,-0.0566,0.0987,0.0807,0.0051,0.0381,-0.0036,0.0533,-0.0472,0.0598,0.0122,-0.0511,0.0169,-0.0499,-0.0032,-0.0044,-0.033,-0.061,-0.0403,-0.2715,0.0703,-0.0302,0.0664,-0.0239,0.0157,-0.0647,0.0117,0.0336,-0.0219,0.0373,0.0101,0.0266,0.003,-0.0318,-0.0155,-0.0114,-0.018,-0.0028,0.0053,-0.0672,0.0297,-0.0524,-0.0121,0.0511,0.0381,0.0337,0.0383,0.0073,0.0428,0.0834,0.0168,0.059,-0.1563,0.0293,0.0521,0.0176,-0.0645,-0.0015,-0.0033,0.0463,0.0093,0.0286,-0.0087,-0.0152,0.0041,0.0025,0.027,-0.0363,0.0093,-0.0081,0.0501,0.0446,-0.0124,0.0074,0.0264,-0.013,0.0097,-0.0691,0.0101,0.0175,-0.0474,-0.0313,-0.0435,0.0581,-0.0498,-0.0275,-0.0062,0.0023,-0.0344,0.1884,-0.0657,0.0053,0.0683,-0.0072,0.009,-0.0343,-0.0252,-0.0335,-0.0693,0.0244,-0.0307,-0.0439,0.0299,-0.0588,0.0475,-0.0092,0.0472,0.0619,0.0028,-0.0204,-0.0569,0.0352,0.0408,-0.0395,0.0324,-0.0721,0.0032,0.1015,0.067,0.0039,0.0355,0.0032,0.001,0.004,-0.0023,-0.0103,0.0175,0.0051,-0.022,0.0312,-0.016,-0.0062,0.0211,-0.0421,-0.0672,0.1053,-0.0744,-0.0112,-0.0548,-0.0103,-0.0351,0.0295,-0.0488,0.0193,-0.0089,0.0523,0.0101,0.0239,-0.0505,0.0348,-0.0505,-0.0626,-0.0722,0.1221,0.0545,-0.1121,-0.0049,-0.0481,-0.0075,-0.0578,0.0377,0.0243,0.0035,0.0371,0.1054,0.0452,-0.0636,-0.0305,-0.0018,-0.0286,0.0205,-0.0335,-0.0453,0.0157,0.0143,-0.0456,0.0181,-0.0286,-0.0085,-0.0249,-0.0687,0.051,-0.0498,-0.0237,-0.0388,-0.0319,-0.025,-0.0501,-0.0055,0.0035,0.0436,-0.041,-0.0248,0.0384,0.0101,0.0284,-0.0114,0.0019,-0.0136,0.0312,-0.0749,-0.0041,0.059,0.0169,-0.0105,0.0143,-0.0051,-0.012,-0.0034,0.0406,0.0556,-0.0585,-0.0892,-0.1938,-0.0297,-0.0221,-0.0254,0.0735,-0.049,0.0054,0.0336,0.093,0.0502,0.0591,0.0264,0.0132,0.0152,0.0031,0.0909,0.0103,0.0308,-0.0271,-0.0162,-0.0018,0.0417,0.0068,-0.0498,0.0372,0.005,0.2472,0.0211,0.0891,-0.0047,-0.0244,0.044,-0.0395,-0.0821,0.0401,0.0442,0.0479,0.0053,-0.0618,-0.0327,-0.0794,0.0118,-0.0087,-0.1289,-0.0489,0.0187,-0.008,-0.0118,-0.0352,-0.0013,0.0154,0.0312,0.0448,-0.0003,-0.0276,-0.01,-0.1014,0.01,-0.0431,0.0182,0.0226,-0.078,-0.0112,-0.0463,0.097,0.051,-0.0185,-0.0329,0.0168,-0.0037,0.0255,0.0433,0.026,0.05,0.0545,-0.002,0.0564,-0.0242,-0.0159,0.0156,0.0259,-0.0481,0.0334,0.0134,0.0546,0.0373,0.0874,-0.0258,0.0404,-0.0335,-0.0148,0.0207,-0.08,-0.0882,0.0629,-0.0122,-0.2932,0.0444,0.0161,0.0392,-0.0354,0.0116,0.0219,0.0665,-0.0716,-0.0126,0.012,0.0267,0.0506,-0.0204,-0.0189,0.0466,0.0596,-0.0316,0.0555,-0.0378,-0.0085,0.0506,0.2037,-0.0363,0.0221,0.0283,-0.009,0.0091,0.0077,-0.0364,0.0018,0.0221,0.0444,-0.0385,0.0201,0.0726,-0.0831,0.0097,0.0495,-0.0114,0.0189,-0.0244,-0.0369,-0.03,0.1005,0.0075,-0.0101,-0.0083,0.0243,0.0122,-0.0373,-0.0145,0.0012,0.0082,0.0182,0.0085,-0.0156,-0.0496,-0.0301,-0.0252,-0.0292,-0.0177,-0.0276,0.0148,-0.0407]}
{"key":"[Approximate Latent Force Model Inference] Physically-inspired latent force models offer an interpretable alternative to purely data driven tools for inference in dynamical systems. They carry the structure of differential equations and the flexibility of Gaussian processes, yielding interpretable parameters and dynamics-imposed latent functions. However, the existing inference techniques associated with these models rely on the exact computation of posterior kernel terms which are seldom available in analytical form. Most applications relevant to practitioners, such as Hill equations or diffusion equations, are hence intractable. In this paper, we overcome these computational problems by proposing a variational solution to a general class of non-linear and parabolic partial differential equation latent force models. Further, we show that a neural operator approach can scale our model to thousands of instances, enabling fast, distributed computation. We demonstrate the efficacy and flexibility of our framework by achieving competitive performance on several tasks where the kernels are of varying degrees of tractability.","layer":1,"vector":[-0.0891,-0.0545,0.0562,-0.008,-0.001,0.0081,0.0173,0.003,0.0308,-0.0135,-0.0078,-0.0557,0.0226,0.0329,0.0316,-0.022,-0.0041,0.0769,-0.0372,0.0302,-0.0128,-0.0091,0.0073,0.0023,-0.0172,0.0038,-0.0356,0.0247,-0.0305,-0.2505,-0.0024,-0.0631,0.0255,-0.0265,0.0421,0.0265,-0.0162,-0.0014,0.022,0.0477,0.0045,0.0066,0.0076,-0.0152,-0.0041,-0.0421,0.0206,0.0026,-0.0089,-0.0236,0.037,-0.0463,0.0301,0.0098,0.0416,0.0063,0.0765,0.009,0.0501,0.0452,0.0653,0.0628,-0.1627,0.088,0.0488,0.0174,0.0228,-0.0117,0.0201,0.0444,-0.0409,0.0523,0.0088,0.0268,0.0249,-0.0308,0.0008,-0.0481,-0.0147,-0.0214,0.025,-0.0279,-0.048,-0.0302,-0.049,-0.0278,0.0112,-0.0501,0.0458,0.0156,-0.0271,-0.0282,-0.0391,0.0136,-0.054,-0.0059,0.0424,0.0133,-0.0749,0.2221,-0.0136,0.0348,0.0443,-0.015,-0.0046,-0.0395,-0.0177,-0.0398,-0.0175,0.0565,-0.0136,0.0037,0.0041,-0.048,0.0457,-0.0443,0.0377,0.0115,0.0038,0.005,-0.0518,0.0047,0.0034,-0.0114,0.0178,-0.1,-0.0505,0.1612,0.0349,-0.0012,0.0405,-0.0207,-0.0847,-0.0383,0.0233,0.027,0.0091,-0.0158,-0.012,0.014,-0.0496,-0.0589,-0.008,-0.1052,-0.0958,0.1169,-0.0242,0.0179,-0.0573,-0.0028,0.0188,0.06,-0.0497,0.0061,0.0705,0.0335,0.0029,0.0037,-0.0652,0.0112,-0.0157,-0.0289,-0.0167,0.1136,-0.0094,-0.0354,-0.0188,-0.0042,0.0314,-0.0185,0.0642,0.0072,-0.022,-0.0006,0.0759,0.0139,-0.0488,0.0184,0.0355,0.0578,0.0062,-0.0671,-0.0202,0.0237,0.0316,-0.0313,0.0325,-0.0629,-0.0045,0.0268,0.0197,0.0047,-0.0102,-0.0029,-0.0009,-0.0097,-0.0184,-0.0294,-0.0036,-0.0371,-0.0111,0.0003,-0.0672,0.0002,-0.0354,0.0038,-0.0272,0.0103,-0.0219,0.0265,-0.0088,-0.0031,0.0828,-0.0187,-0.0211,0.0188,-0.0199,0.0219,0.0171,0.0128,0.0625,-0.076,-0.0289,-0.2306,0.0094,-0.0053,-0.0047,0.1041,-0.0446,0.0705,-0.0468,0.0463,0.0455,0.0441,0.0083,-0.0243,0.0073,0.0205,0.0422,0.0145,0.0326,-0.031,0.0006,-0.0108,-0.0179,-0.0576,-0.0756,0.0744,0.0081,0.2084,0.0521,0.0268,-0.0352,0.0195,0.0213,-0.03,-0.0501,0.0772,0.0145,0.0996,-0.0358,0.0036,-0.0068,-0.0364,0.011,0.0019,-0.0584,-0.0421,-0.0263,-0.0144,0.0494,-0.067,-0.0018,0.0885,-0.0411,0.0533,-0.0376,0.0129,-0.0464,-0.0741,0.0404,-0.0588,0.0176,-0.0436,-0.0726,0.0081,-0.034,0.0515,0.0185,-0.0419,-0.0823,0.0128,-0.0506,0.0027,0.0612,-0.0124,0.0554,0.0563,0.0155,0.0285,0.0012,-0.0521,0.0151,0.0539,-0.0119,0.0623,0.0302,0.0306,-0.0335,0.0689,-0.0134,0.0419,-0.0042,-0.0134,0.0246,-0.0729,0.0378,0.0248,-0.0121,-0.304,0.0456,0.0394,0.0562,-0.0259,0.0045,0.0543,0.0041,-0.056,-0.0095,-0.0362,0.0653,0.0409,0.0418,-0.0056,-0.0147,0.0577,-0.043,0.0227,-0.1154,0.0005,0.0491,0.2189,-0.0426,0.0735,0.0319,-0.0066,-0.0398,0.027,-0.039,0.0106,0.0211,0.0483,-0.0719,0.0423,0.0608,-0.0235,0.0616,0.0466,-0.0403,0.042,0.0055,-0.0005,-0.0181,0.0663,-0.014,-0.0011,-0.0586,-0.0353,0.0398,-0.0216,0.0175,0.016,-0.004,0.0514,0.0361,-0.0175,-0.065,-0.0484,-0.0513,-0.0274,-0.0658,-0.0061,-0.0201,0.0121]}
{"key":"[How to Certify Machine Learning Based Safety-critical Systems? A Systematic Literature Review] Context: Machine Learning (ML) has been at the heart of many innovations over the past years. However, including it in so-called 'safety-critical' systems such as automotive or aeronautic has proven to be very challenging, since the shift in paradigm that ML brings completely changes traditional certification approaches. Objective: This paper aims to elucidate challenges related to the certification of ML-based safety-critical systems, as well as the solutions that are proposed in the literature to tackle them, answering the question 'How to Certify Machine Learning Based Safety-critical Systems?'. Method: We conduct a Systematic Literature Review (SLR) of research papers published between 2015 to 2020, covering topics related to the certification of ML systems. In total, we identified 217 papers covering topics considered to be the main pillars of ML certification: Robustness, Uncertainty, Explainability, Verification, Safe Reinforcement Learning, and Direct Certification. We analyzed the main trends and problems of each sub-field and provided summaries of the papers extracted. Results: The SLR results highlighted the enthusiasm of the community for this subject, as well as the lack of diversity in terms of datasets and type of models. It also emphasized the need to further develop connections between academia and industries to deepen the domain study. Finally, it also illustrated the necessity to build connections between the above mention main pillars that are for now mainly studied separately. Conclusion: We highlighted current efforts deployed to enable the certification of ML based software systems, and discuss some future research directions.","layer":0,"vector":[-0.0647,-0.0301,0.0255,-0.0553,0.0113,0.0497,0.0495,0.0537,0.0313,-0.0195,-0.0223,-0.0787,0.0015,0.0527,-0.016,0.0292,0.0216,0.0744,0.0094,-0.0201,0.0229,0.0097,-0.0231,-0.0574,0.0081,0.0438,-0.0431,-0.0403,-0.0463,-0.2526,-0.0222,-0.0694,0.0473,-0.0207,0.0041,-0.0185,-0.0544,0.0531,0.0187,0.0175,0.0607,-0.0035,-0.0173,-0.0713,0.0049,-0.051,0.0348,-0.0042,-0.0271,-0.0608,0.0179,-0.0392,0.0266,0.045,0.0148,0.0175,0.056,0.0634,0.0417,0.053,0.0053,0.0148,-0.1933,0.014,0.0453,0.0441,-0.0798,-0.0282,0.0311,0.0431,-0.0136,0.0295,0.0148,0.064,-0.0208,0.0273,0.0136,-0.0497,-0.0017,0.0116,0.0083,-0.0567,-0.0534,0.003,-0.0273,-0.0591,0.0425,-0.0178,0.044,-0.0082,-0.0384,-0.02,0.0287,0.0325,-0.0303,-0.018,0.0352,0.0119,-0.0981,0.1872,-0.0563,0.0081,-0.0407,-0.0041,0.0326,-0.0193,-0.0279,-0.0378,-0.0105,-0.0782,-0.0217,-0.0008,0.042,-0.0322,-0.0182,0.0638,0.0632,0.0466,0.06,-0.0128,-0.0268,0.0183,0.0642,-0.0176,0.0259,-0.0439,0.0509,0.1201,0.0147,0.0346,0.021,-0.0242,-0.0525,-0.0375,0.0251,0.041,-0.0028,0.0313,0.0089,0.0205,-0.0231,-0.0345,0.0275,-0.0817,-0.0466,0.0789,-0.0434,0.0147,-0.0275,-0.0197,-0.0046,0.0237,-0.0422,-0.0082,0.0423,0.0158,0.0354,0.0338,-0.0454,0.0048,0.0049,-0.0581,-0.0739,0.1305,-0.0044,-0.0613,0.0124,0.0151,0.0272,-0.0197,0.0223,0.046,-0.0454,0.039,0.0302,0.0081,-0.0542,-0.0042,0.0021,0.017,-0.0034,-0.0305,-0.049,0.0653,0.0736,-0.021,0.0002,-0.0564,0.0161,0.0148,-0.0245,0.0204,0.0051,0.0054,-0.0039,-0.0089,-0.0112,-0.0007,0.02,-0.0135,-0.0203,0.0145,-0.0208,0.022,-0.0399,0.0156,-0.0172,-0.0108,0.0553,0.0422,-0.0413,-0.0165,0.0387,-0.0417,-0.0405,0.0144,0.0042,0.0663,-0.033,0.0325,0.0376,0.0146,-0.0355,-0.2414,-0.0164,-0.0033,0.0112,0.0363,-0.0504,0.0239,-0.0376,0.0174,0.0149,0.1139,-0.0237,-0.0668,0.0033,-0.0017,0.0349,0.0022,-0.0064,-0.0278,-0.0129,-0.0445,0.0156,-0.0301,-0.0868,0.0251,-0.0057,0.2056,0.0059,0.0447,-0.026,0.031,-0.002,-0.0433,-0.1001,0.0857,0.0063,0.036,-0.0059,-0.0279,-0.0132,-0.0245,0.054,-0.0243,-0.1402,-0.0435,-0.0664,-0.0617,0.0665,-0.0278,0.0395,0.0159,0.0209,0.0197,-0.0117,-0.0154,-0.0026,-0.0674,0.0274,-0.029,0.0031,0.008,-0.0357,-0.0059,-0.0741,0.0435,-0.0532,-0.0121,-0.0707,0.056,-0.0341,-0.058,0.1575,0.0006,-0.0427,0.0893,0.022,-0.0003,-0.0292,-0.0278,0.0078,0.052,0.003,0.0684,0.0566,0.0468,0.0169,0.0473,0.0176,0.0142,-0.0279,0.0118,-0.0228,-0.0591,-0.0193,0.0931,0.0161,-0.2711,0.0262,0.0037,0.0329,-0.063,-0.039,0.059,-0.014,-0.0324,0.017,0.0112,0.0685,0.0864,-0.0097,0.0261,0.0631,0.0696,-0.037,0.0471,-0.0436,0.0118,0.075,0.2197,-0.046,0.0057,0.0464,-0.0281,0.0252,0.05,-0.0047,0.0246,-0.0314,0.0663,-0.0538,0.0181,0.0757,-0.0645,0.0008,0.0597,-0.0117,-0.019,0.033,0.0209,-0.0375,0.0648,-0.0006,-0.0294,-0.0623,-0.0192,0.0271,0.0245,-0.0057,-0.0475,-0.0069,0.0499,0.0164,-0.0032,-0.0408,-0.0494,-0.0425,0.0347,-0.0316,0.0122,0.0076,-0.029]}
{"key":"[Insertion-Deletion Transformer] We propose the Insertion-Deletion Transformer, a novel transformer-based neural architecture and training method for sequence generation. The model consists of two phases that are executed iteratively, 1) an insertion phase and 2) a deletion phase. The insertion phase parameterizes a distribution of insertions on the current output hypothesis, while the deletion phase parameterizes a distribution of deletions over the current output hypothesis. The training method is a principled and simple algorithm, where the deletion model obtains its signal directly on-policy from the insertion model output. We demonstrate the effectiveness of our Insertion-Deletion Transformer on synthetic translation tasks, obtaining significant BLEU score improvement over an insertion-only model.","layer":0,"vector":[-0.072,-0.035,0.0203,-0.0526,-0.0027,0.0376,0.0007,0.0159,0.059,0.0028,0.0011,-0.0462,0.0322,0.0447,0.007,-0.033,-0.0396,0.0452,-0.0227,-0.0272,0.0897,-0.0388,-0.0037,-0.0079,-0.0011,-0.001,-0.0092,0.011,-0.0636,-0.2506,0.0234,-0.0359,0.0631,-0.0113,-0.0266,-0.0004,-0.0814,0.0219,-0.0366,0.0277,0.0544,0.0262,-0.0089,-0.0698,0.0026,-0.0454,-0.0195,-0.0295,0.0089,-0.0186,0.0074,-0.051,0.0168,0.079,0.0115,0.016,0.0656,0.0508,0.0235,0.0113,0.0272,0.0692,-0.1644,0.068,0.0108,0.0479,-0.027,-0.0335,0.0078,0.0524,-0.0822,0.0059,0.0241,0.0544,0.0248,0.0005,-0.0064,-0.0527,0.0105,0.0297,0.0316,-0.0688,-0.0399,-0.0209,-0.0252,-0.039,0.0111,-0.0495,0.004,0.0174,-0.0289,-0.0133,-0.0046,0.0604,-0.0765,-0.0091,0.0446,0.0291,-0.0418,0.1894,-0.0434,0.0136,0.0365,-0.0581,0.052,-0.0281,-0.0306,-0.0307,-0.0537,0.0043,-0.0336,-0.0171,0.0219,0.0026,0.036,0.0076,0.1023,0.0592,-0.0355,-0.0077,-0.0104,0.0055,-0.0196,-0.0012,-0.0049,-0.0678,0.0472,0.0983,0.0411,0.0124,0.0581,0.0266,-0.0636,0.0068,0.0122,0.0132,0.0006,-0.0569,-0.0404,-0.0387,-0.0034,-0.0407,-0.0235,-0.0549,-0.0264,0.114,-0.0119,0.0166,-0.0477,-0.0029,-0.0327,0.0443,0.0092,-0.0281,0.0345,0.0676,0.0434,0.0174,-0.0448,-0.0055,-0.0259,-0.06,-0.0231,0.0905,0.0176,-0.0507,-0.0577,0.0029,0.0635,0.003,0.0305,0.0078,-0.0436,0.0392,0.0609,0.0374,-0.0437,-0.0084,-0.0196,0.0047,0.0244,-0.0561,-0.0037,0.0426,0.0266,-0.0665,0.0231,-0.0465,0.0291,0.0228,-0.0517,0.0856,-0.0188,0.0008,-0.0335,-0.0607,-0.0274,-0.0029,0.0229,-0.0469,0.0285,-0.0045,-0.0503,-0.009,-0.0303,-0.0065,-0.0111,-0.0216,0.0475,0.0265,-0.0096,-0.0047,0.0554,0.0064,-0.0082,0.0053,-0.0243,0.0368,0.0831,0.0395,0.0025,-0.0471,-0.0136,-0.2665,0.0063,0.0372,-0.0182,0.0569,-0.0855,0.0256,0.0216,0.0741,0.0398,0.0308,-0.0354,-0.0351,0.0294,-0.0033,0.062,0.0022,-0.0116,-0.0093,-0.003,0.005,0.0205,0.0095,-0.0841,0.0526,-0.0087,0.1906,0.0734,0.0871,-0.0187,0.0332,0.0288,-0.0081,-0.0785,0.0549,0.021,0.0893,-0.0128,0.0178,-0.0382,-0.0056,-0.0182,-0.0092,-0.1115,-0.0101,-0.0446,-0.0567,-0.002,-0.034,0.0599,0.0339,-0.0394,0.0529,0.0015,-0.0306,-0.0661,-0.1272,-0.005,-0.0303,0.0035,0.005,-0.038,0.0317,-0.0379,0.0211,-0.0016,0.0041,-0.0025,0.0449,-0.0066,-0.0264,0.0797,0.0213,0.0292,0.046,0.0217,-0.0217,-0.062,-0.0469,-0.0591,0.0799,-0.0503,0.046,0.0474,0.0413,0.0181,0.0767,0.0055,0.0498,-0.0027,0.0203,0.0148,-0.028,-0.0009,0.0361,-0.0366,-0.3066,0.0356,0.0057,0.0454,0.0191,0.0355,0.0169,0.0001,-0.0254,0.014,-0.0377,-0.0113,0.0361,-0.02,0.0025,0.0326,0.0911,-0.0468,0.0304,-0.08,-0.0151,0.0286,0.2247,0.0127,0.0123,-0.0171,0.007,-0.0065,0.0514,-0.0104,0.0278,0.0143,0.0647,0.003,0.0243,0.0685,-0.0683,0.0457,0.0296,-0.0091,-0.0354,0.0505,-0.0415,-0.0347,0.1154,-0.0348,-0.0564,-0.0333,0.0,0.016,-0.0249,0.0302,-0.0038,-0.0217,0.0084,0.0523,-0.0449,-0.0451,-0.0655,-0.0044,0.0185,-0.0783,0.0132,0.0474,-0.0532]}
{"key":"[Learning to maximize global influence from local observations] We study a family online influence maximization problems where in a sequence of rounds $t=1,\\ldots,T$, a decision maker selects one from a large number of agents with the goal of maximizing influence. Upon choosing an agent, the decision maker shares a piece of information with the agent, which information then spreads in an unobserved network over which the agents communicate. The goal of the decision maker is to select the sequence of agents in a way that the total number of influenced nodes in the network. In this work, we consider a scenario where the networks are generated independently for each $t$ according to some fixed but unknown distribution, so that the set of influenced nodes corresponds to the connected component of the random graph containing the vertex corresponding to the selected agent. Furthermore, we assume that the decision maker only has access to limited feedback: instead of making the unrealistic assumption that the entire network is observable, we suppose that the available feedback is generated based on a small neighborhood of the selected vertex. Our results show that such partial local observations can be sufficient for maximizing global influence. We model the underlying random graph as a sparse inhomogeneous Erd\\H{o}s--R\\'enyi graph, and study three specific families of random graph models in detail: stochastic block models, Chung--Lu models and Kronecker random graphs. We show that in these cases one may learn to maximize influence by merely observing the degree of the selected vertex in the generated random graph. We propose sequential learning algorithms that aim at maximizing influence, and provide their theoretical analysis in both the subcritical and supercritical regimes of all considered models.","layer":1,"vector":[-0.0075,-0.014,0.046,-0.0309,0.0278,0.0017,0.0592,0.0548,0.025,-0.009,0.0257,0.0091,0.0448,0.0838,-0.0007,0.0466,-0.0494,0.0223,-0.0278,-0.0125,0.0116,-0.0719,0.0003,-0.0883,0.0429,-0.0095,-0.0317,-0.05,-0.0339,-0.2213,0.0277,-0.0057,0.0878,-0.0194,0.0094,-0.0136,0.0029,-0.0096,-0.0362,0.037,0.0103,0.0524,0.0114,-0.0481,-0.0221,-0.0298,0.031,-0.0143,-0.0483,-0.0412,0.0503,-0.026,0.029,-0.0004,0.0778,0.0097,0.049,0.0531,0.0456,0.0838,0.0089,0.0747,-0.1349,0.0383,0.025,0.0678,-0.0522,0.0608,0.0124,0.0709,0.0391,0.0545,-0.0033,0.0494,0.0051,0.0044,0.0023,-0.001,-0.0184,-0.0079,0.0031,-0.0382,-0.0625,-0.0382,-0.0287,-0.0331,0.0348,-0.0843,0.0416,0.0107,-0.0458,-0.0163,-0.0156,0.0082,-0.0788,-0.0435,0.0516,0.0029,-0.0264,0.1789,-0.0549,0.0171,0.0272,-0.0005,0.036,-0.0375,0.0074,-0.0213,-0.0197,0.0225,-0.0144,-0.0381,0.0271,-0.0486,0.0086,-0.0118,0.0504,0.0535,-0.0125,-0.0422,-0.0282,0.0478,0.0892,-0.0322,0.0553,-0.0735,-0.0087,0.1316,0.034,0.0318,0.0241,-0.0186,-0.0307,-0.016,-0.011,-0.0502,-0.0015,-0.0102,0.0058,-0.0064,-0.0133,-0.0583,-0.0066,-0.1249,-0.0468,0.1087,-0.0364,0.0123,-0.0435,-0.069,0.0082,-0.0087,0.0015,-0.0194,-0.0005,0.0283,0.0402,0.0398,-0.0582,0.0322,-0.0479,-0.0244,0.0131,0.1214,0.0137,-0.0791,-0.0257,-0.0146,0.0095,-0.0266,0.0364,0.0617,-0.0386,0.0778,0.0652,0.0495,-0.0892,0.0208,0.0107,0.0273,0.0353,-0.0236,-0.0017,0.0184,-0.0306,-0.0308,-0.0117,-0.0405,0.0015,-0.0034,-0.0407,0.0523,-0.0202,-0.0122,-0.0101,-0.0259,-0.0174,-0.0226,-0.0084,-0.0536,0.0134,-0.0206,-0.0773,-0.0259,-0.0175,0.0355,0.0014,-0.0102,0.0263,-0.0183,-0.051,0.043,0.0383,-0.0051,-0.0276,0.0298,0.036,0.0158,0.0097,0.019,0.0558,-0.0391,-0.0667,-0.2218,-0.0422,-0.0252,0.0318,0.0727,-0.093,0.0724,-0.0539,0.063,0.127,0.0371,-0.0056,-0.0169,0.0542,0.0203,0.0346,0.0022,0.061,-0.0111,0.0015,-0.0324,0.0192,-0.038,-0.0621,0.0589,-0.0001,0.2236,0.0248,0.0101,-0.0245,0.0204,0.0522,-0.0436,-0.0518,0.0538,0.0297,0.0961,-0.0129,-0.0398,-0.0091,-0.038,0.0249,-0.0287,-0.0673,-0.0277,-0.0264,-0.0083,0.0338,-0.0688,-0.0255,0.0023,0.0005,0.0873,-0.0011,0.0254,-0.0535,-0.0715,0.0447,-0.0333,0.0268,0.0154,-0.0491,-0.0423,-0.0577,0.0681,0.0073,-0.0245,-0.0351,0.0424,-0.0027,-0.008,0.0489,-0.012,0.0115,0.0472,0.0199,0.0448,-0.0505,-0.1288,-0.0105,0.0727,-0.1008,0.0153,0.0653,-0.0091,-0.0171,0.0713,-0.0129,0.0148,-0.0024,-0.0146,0.0278,-0.0594,-0.013,0.0315,-0.0604,-0.285,0.0336,-0.0206,0.0524,0.0043,0.0144,0.0501,0.0138,-0.0602,-0.0075,0.0095,0.0853,0.0305,0.0115,-0.0033,0.0449,0.0268,-0.0465,0.0263,-0.0688,0.006,0.0637,0.2122,-0.026,0.0717,0.0281,-0.0357,0.0046,-0.0126,-0.0636,0.0425,0.0273,0.0655,-0.0839,0.0458,0.0492,-0.0551,0.0128,-0.0064,-0.0195,-0.0376,-0.0347,-0.0395,-0.0041,0.1065,-0.0272,-0.024,-0.0221,-0.0207,0.0444,-0.0126,0.0193,-0.0172,-0.0097,-0.0318,0.0368,-0.0316,-0.0281,-0.0409,-0.0433,0.0196,-0.0321,-0.0103,-0.0184,0.0182]}
{"key":"[Tree-Guided Rare Feature Selection and Logic Aggregation with Electronic Health Records Data] Statistical learning with a large number of rare binary features is commonly encountered in analyzing electronic health records (EHR) data, especially in the modeling of disease onset with prior medical diagnoses and procedures. Dealing with the resulting highly sparse and large-scale binary feature matrix is notoriously challenging as conventional methods may suffer from a lack of power in testing and inconsistency in model fitting while machine learning methods may suffer from the inability of producing interpretable results or clinically-meaningful risk factors. To improve EHR-based modeling and utilize the natural hierarchical structure of disease classification, we propose a tree-guided feature selection and logic aggregation approach for large-scale regression with rare binary features, in which dimension reduction is achieved through not only a sparsity pursuit but also an aggregation promoter with the logic operator of ``or''. We convert the combinatorial problem into a convex linearly-constrained regularized estimation, which enables scalable computation with theoretical guarantees. In a suicide risk study with EHR data, our approach is able to select and aggregate prior mental health diagnoses as guided by the diagnosis hierarchy of the International Classification of Diseases. By balancing the rarity and specificity of the EHR diagnosis records, our strategy improves both prediction and model interpretation. We identify important higher-level categories and subcategories of mental health conditions and simultaneously determine the level of specificity needed for each of them in predicting suicide risk.","layer":6,"vector":[-0.0395,-0.0062,0.0535,-0.0126,0.0172,0.0137,0.0591,0.0389,0.0561,-0.0304,0.0053,-0.0409,0.0156,0.0271,0.0191,0.0465,0.017,0.0364,-0.0619,0.0631,0.005,-0.023,-0.0169,-0.0632,0.0329,-0.0082,-0.0475,-0.0306,-0.0408,-0.2213,0.0209,-0.0454,0.0412,-0.0329,0.0279,-0.0311,-0.0453,0.051,-0.0556,0.0283,0.0026,0.0299,-0.0326,-0.0531,-0.0382,-0.0377,0.0072,-0.0113,-0.0226,-0.0358,-0.0051,-0.031,0.0514,0.0333,0.061,0.0146,0.0115,0.0186,0.0266,0.0596,0.0206,0.0113,-0.1442,0.0351,0.0813,0.0425,-0.0507,-0.0235,0.0255,0.0525,0.001,0.0339,0.0088,0.0671,-0.0179,0.0111,0.0208,0.0058,-0.0012,0.0405,0.0343,-0.004,-0.0367,0.0001,-0.0303,-0.0285,0.0523,-0.0948,0.014,-0.0012,-0.0631,-0.01,-0.0323,0.0259,-0.0499,-0.0585,0.0587,0.0102,-0.0482,0.2334,-0.072,0.0141,-0.0096,-0.0249,-0.0011,-0.0425,-0.0165,-0.0784,-0.0043,-0.0459,0.0277,-0.0195,0.0618,-0.0404,0.0221,-0.0111,0.0757,0.0195,-0.0001,-0.0099,-0.0243,-0.0065,0.0718,-0.0377,0.0229,-0.0539,0.0036,0.1611,0.0594,0.003,0.0588,-0.0163,-0.0546,-0.016,0.0069,-0.0111,0.0473,0.0251,0.0128,-0.009,-0.0384,-0.0412,0.028,-0.0817,-0.0943,0.1119,-0.0751,-0.0317,-0.0532,-0.0521,0.0045,-0.0274,0.0023,-0.0175,-0.0082,0.029,0.0575,0.0308,-0.0523,0.0033,0.0043,-0.0442,0.0422,0.1252,-0.0373,-0.0553,-0.0282,-0.0184,-0.0068,0.0029,0.0644,0.0287,0.0043,0.05,0.0533,0.0529,-0.0312,0.0032,-0.0057,0.0078,0.0263,-0.0536,-0.0542,0.0482,0.0476,-0.0491,0.0095,-0.025,0.034,0.0208,-0.06,0.0192,-0.0114,-0.0202,-0.0073,-0.0486,-0.0114,-0.0063,0.0371,-0.0286,0.0512,0.012,-0.0094,0.0462,0.0268,-0.0259,0.0143,0.0144,0.0348,0.011,-0.0216,0.0121,0.117,-0.0114,-0.0079,0.0223,-0.001,0.0328,0.0555,0.065,0.0652,-0.0047,-0.075,-0.2321,-0.0552,0.0117,0.0042,0.0034,-0.0691,0.0154,-0.0145,0.0433,0.0934,0.0597,0.0175,-0.04,0.0295,-0.0204,0.0379,0.0303,-0.0137,-0.0617,0.0592,-0.0254,0.0225,0.0018,-0.0995,0.0242,-0.0069,0.2211,0.0182,0.0246,-0.0098,0.0147,0.0097,-0.0181,-0.0915,0.0815,0.0299,-0.0047,-0.0052,-0.08,-0.0156,-0.0591,0.0477,-0.0166,-0.1151,-0.067,-0.0134,-0.0081,0.0474,-0.065,0.0318,0.0417,0.0079,0.0303,-0.0274,0.0071,-0.0327,-0.0846,0.0173,-0.0521,0.0042,0.0129,-0.0609,0.0095,-0.0801,0.0424,0.003,-0.0123,-0.0116,0.0229,-0.0578,-0.023,0.0832,-0.0371,-0.0402,0.048,0.0345,0.0368,-0.0318,-0.0222,-0.0343,0.0245,-0.0706,0.015,0.0317,0.0346,0.0019,0.097,-0.0194,0.0092,-0.0372,-0.0025,-0.0364,-0.018,-0.0048,0.0472,0.041,-0.2935,0.0351,-0.0337,0.0291,-0.0006,0.0205,0.0601,0.0372,-0.0251,-0.0225,0.0008,0.0203,0.0792,-0.063,-0.0421,0.0197,0.0698,-0.0637,0.0562,-0.0416,0.0273,0.0313,0.1821,-0.0262,0.0067,0.0057,0.0143,0.0117,-0.0028,-0.0256,0.0517,-0.0312,0.0879,-0.0289,0.0366,0.0704,-0.0631,0.022,0.0239,-0.0153,0.0018,-0.0116,-0.0547,-0.0275,0.085,-0.01,-0.0307,-0.0407,-0.0053,0.0631,0.0008,0.0237,-0.0341,0.0448,0.0423,0.0171,0.0026,-0.0383,-0.0395,-0.0684,0.0285,-0.0081,-0.0611,0.0223,-0.0162]}
{"key":"[Deep Reasoning Networks: Thinking Fast and Slow] We introduce Deep Reasoning Networks (DRNets), an end-to-end framework that combines deep learning with reasoning for solving complex tasks, typically in an unsupervised or weakly-supervised setting. DRNets exploit problem structure and prior knowledge by tightly combining logic and constraint reasoning with stochastic-gradient-based neural network optimization. We illustrate the power of DRNets on de-mixing overlapping hand-written Sudokus (Multi-MNIST-Sudoku) and on a substantially more complex task in scientific discovery that concerns inferring crystal structures of materials from X-ray diffraction data under thermodynamic rules (Crystal-Structure-Phase-Mapping). At a high level, DRNets encode a structured latent space of the input data, which is constrained to adhere to prior knowledge by a reasoning module. The structured latent encoding is used by a generative decoder to generate the targeted output. Finally, an overall objective combines responses from the generative decoder (thinking fast) and the reasoning module (thinking slow), which is optimized using constraint-aware stochastic gradient descent. We show how to encode different tasks as DRNets and demonstrate DRNets' effectiveness with detailed experiments: DRNets significantly outperform the state of the art and experts' capabilities on Crystal-Structure-Phase-Mapping, recovering more precise and physically meaningful crystal structures. On Multi-MNIST-Sudoku, DRNets perfectly recovered the mixed Sudokus' digits, with 100% digit accuracy, outperforming the supervised state-of-the-art MNIST de-mixing models. Finally, as a proof of concept, we also show how DRNets can solve standard combinatorial problems -- 9-by-9 Sudoku puzzles and Boolean satisfiability problems (SAT), outperforming other specialized deep learning models. DRNets are general and can be adapted and expanded to tackle other tasks.","layer":2,"vector":[-0.0799,0.0337,0.0558,-0.0352,0.0251,0.0185,0.0239,0.0084,0.028,-0.0346,0.0054,-0.0605,0.0978,0.0898,0.0238,-0.0032,-0.0351,0.0424,-0.0202,0.0076,0.039,-0.0356,-0.0321,-0.0928,0.0481,0.023,-0.0124,-0.0277,-0.0403,-0.2565,0.0108,-0.031,0.0163,-0.0191,0.0329,-0.0607,-0.0299,0.0473,-0.0427,0.0352,0.0495,0.0363,0.0022,-0.0459,0.0079,-0.0289,-0.0165,-0.0118,-0.0025,-0.0356,0.0056,-0.0471,-0.013,0.0169,0.0411,0.0603,0.07,0.0444,0.0295,0.0308,0.0558,0.0224,-0.163,0.0839,0.0428,0.0207,-0.0341,-0.0135,-0.0123,0.0717,-0.0106,0.0354,0.0271,0.0839,0.0227,0.0158,-0.0346,-0.0408,-0.0072,0.0224,0.0039,-0.0097,-0.0389,-0.0084,-0.0012,-0.0215,-0.0172,-0.0058,0.0086,0.0074,0.0098,0.0048,-0.0273,0.0062,-0.0522,-0.0147,0.0348,0.0409,-0.0473,0.2005,-0.0653,0.0262,0.0185,-0.0698,-0.0313,-0.0082,-0.0416,-0.0443,-0.0325,0.0128,-0.0282,-0.0301,0.0318,0.0025,0.0314,0.0355,0.0896,0.0189,-0.0449,0.0011,-0.0104,0.021,0.0301,0.0075,0.0051,-0.0924,-0.0582,0.1321,-0.0228,0.0282,0.0689,-0.0423,-0.0391,-0.0204,0.0579,-0.0086,0.055,-0.0004,-0.002,-0.0135,-0.0984,0.0032,0.0168,-0.0846,-0.0691,0.0873,-0.0204,-0.0096,-0.0245,-0.0168,-0.0203,-0.0051,-0.0451,-0.0202,-0.0165,-0.0011,0.0457,0.0367,-0.0274,-0.0019,-0.0121,-0.0353,-0.0332,0.0803,0.0209,-0.0731,-0.0239,-0.011,0.0056,-0.0897,0.0498,0.0447,-0.0152,0.006,0.0611,0.0493,-0.0462,0.0058,-0.0048,0.0536,0.0167,-0.0543,-0.0236,0.0281,0.0461,-0.073,0.0087,-0.037,0.0039,0.0272,-0.0107,0.0417,-0.0384,0.0535,-0.0243,-0.0042,-0.0284,0.0212,0.0072,0.0107,0.024,-0.0086,-0.0465,0.0205,0.012,-0.014,0.0051,0.0046,0.0501,0.0656,-0.0442,0.0133,-0.0009,-0.0543,-0.0065,-0.0146,0.0195,-0.0023,-0.0004,0.02,0.0571,-0.0689,-0.0428,-0.2183,-0.0041,-0.0068,-0.0439,0.0374,-0.0148,0.0416,0.0064,0.013,0.0462,0.0328,-0.0095,-0.0122,-0.0058,-0.0216,-0.0075,0.0575,0.0196,-0.0071,0.0215,-0.0261,0.0031,0.0231,-0.0928,0.0469,0.032,0.2742,0.0437,0.0173,-0.017,0.0143,0.0094,-0.0266,-0.0974,0.0438,0.0283,0.1114,-0.0049,-0.0541,-0.0233,-0.0578,0.0449,-0.0125,-0.1188,-0.0075,-0.0116,-0.0197,0.0068,-0.0024,0.0173,0.0481,-0.0382,0.0443,0.0126,-0.0485,0.0014,-0.1039,0.0198,-0.0635,-0.0011,-0.0121,-0.0417,-0.0224,-0.0363,0.0326,-0.0064,-0.0155,-0.0486,0.0367,-0.0401,0.0177,0.0914,-0.0115,0.0273,0.0548,0.0251,0.0426,-0.0351,-0.0248,0.017,0.0525,-0.0072,0.0293,0.0013,0.0451,0.0238,0.0804,-0.0248,0.0294,-0.0031,-0.0175,0.0333,-0.0538,-0.0208,0.0226,-0.0015,-0.2895,0.0771,-0.0229,0.0061,-0.0249,0.0099,0.0805,0.0247,-0.0192,-0.0391,-0.0123,0.0248,0.0125,-0.0287,-0.0388,0.0242,0.0855,-0.0209,0.0711,-0.0489,0.0319,0.0525,0.2238,-0.0178,0.0491,0.013,0.031,-0.0228,0.0422,0.0051,0.0161,0.0157,0.0373,-0.0452,0.019,0.1223,-0.0561,0.0414,0.1008,-0.0361,-0.0157,-0.035,-0.0651,-0.0214,0.0819,-0.0103,-0.0404,-0.028,-0.0016,0.0073,-0.0097,0.0001,-0.0766,-0.0351,-0.0103,-0.0167,0.0084,-0.038,-0.0372,-0.0501,-0.0053,-0.0701,0.0061,0.0139,-0.0209]}
{"key":"[Let the Model Decide its Curriculum for Multitask Learning] Curriculum learning strategies in prior multi-task learning approaches arrange datasets in a difficulty hierarchy either based on human perception or by exhaustively searching the optimal arrangement. However, human perception of difficulty may not always correlate well with machine interpretation leading to poor performance and exhaustive search is computationally expensive. Addressing these concerns, we propose two classes of techniques to arrange training instances into a learning curriculum based on difficulty scores computed via model-based approaches. The two classes i.e Dataset-level and Instance-level differ in granularity of arrangement. Through comprehensive experiments with 12 datasets, we show that instance-level and dataset-level techniques result in strong representations as they lead to an average performance improvement of 4.17% and 3.15% over their respective baselines. Furthermore, we find that most of this improvement comes from correctly answering the difficult instances, implying a greater efficacy of our techniques on difficult tasks.","layer":2,"vector":[-0.0134,-0.0299,0.0238,-0.0396,-0.0188,0.0494,0.0136,0.0008,0.019,-0.0598,-0.0094,-0.0657,0.0259,0.0845,0.0474,0.0105,-0.0009,0.063,-0.0476,-0.027,0.0206,-0.0078,-0.0406,-0.0331,0.0239,0.0334,-0.025,-0.0304,-0.023,-0.2694,0.0424,-0.0235,0.0726,-0.0113,0.0196,0.0075,-0.0311,0.0506,-0.0395,0.0301,0.0005,-0.0146,-0.0467,-0.0689,-0.0406,-0.0531,-0.0339,-0.0364,-0.001,-0.0412,0.0015,-0.0832,0.0332,0.0231,0.0272,0.0205,0.0562,0.0393,0.0481,0.0607,0.0132,0.0192,-0.1633,0.0649,0.0665,0.0198,-0.038,-0.0322,0.0254,0.0707,-0.0429,-0.015,0.0197,0.0651,0.0014,-0.0003,0.0035,-0.0578,0.0069,-0.0173,-0.0017,-0.0365,-0.0368,-0.025,0.0141,-0.0607,-0.0275,-0.0573,0.0541,0.0383,-0.0053,-0.0435,-0.0095,0.0251,-0.0209,-0.0462,0.0245,0.0418,-0.0436,0.1914,-0.0341,0.0094,0.0596,-0.0615,0.0225,-0.0192,-0.0099,-0.0271,-0.0172,-0.0283,-0.0196,-0.0143,0.0124,-0.0247,0.0121,0.0431,0.1014,-0.0144,-0.003,-0.0199,0.0042,0.0069,0.0669,-0.0427,0.027,-0.1093,0.0387,0.1494,0.0358,-0.0136,0.0456,-0.0354,-0.0596,-0.0294,0.0309,0.0012,0.0081,0.0081,0.0235,0.0049,-0.0377,-0.0184,0.0291,-0.0578,-0.0705,0.1585,-0.0336,0.0418,-0.0591,-0.017,-0.0344,0.0017,-0.002,-0.0452,0.0139,0.0213,0.0352,0.0286,-0.04,-0.0151,-0.0251,-0.0515,-0.0315,0.0732,0.044,-0.0548,-0.0205,-0.0401,0.005,-0.0205,0.0521,0.0398,-0.0175,0.0851,0.0805,0.0078,-0.0851,0.0048,0.0291,0.0411,0.0588,-0.0776,-0.0141,0.0054,0.0373,-0.018,0.0236,-0.0229,0.0389,0.0203,-0.0183,0.0549,0.0148,0.0123,-0.0368,-0.0197,0.0302,-0.015,0.0291,-0.0082,-0.0381,0.0014,-0.0157,0.0421,-0.014,-0.0156,0.0009,-0.0228,0.1002,0.0263,-0.0387,-0.0027,0.0603,-0.0569,-0.0242,-0.0361,0.0451,0.0638,-0.0119,0.0349,0.0189,-0.0392,-0.0161,-0.2402,-0.0011,0.0408,-0.0059,0.0427,-0.028,0.0408,-0.0007,0.0026,0.0577,0.0838,-0.0601,-0.0171,0.0057,-0.0083,0.0191,0.0402,0.0076,-0.0226,-0.0006,-0.0086,0.0133,-0.0093,-0.1066,0.0191,0.0075,0.1988,0.0025,0.0115,-0.0344,0.0215,0.0593,-0.0447,-0.0765,0.0527,-0.0021,0.0686,-0.0277,-0.003,-0.0395,-0.0027,0.0207,-0.0109,-0.14,-0.0594,0.003,-0.0075,0.0437,-0.0463,-0.0011,0.0209,-0.0603,0.0182,-0.0523,-0.0681,-0.0054,-0.1312,0.026,-0.0338,0.0411,0.0187,-0.0504,0.0104,-0.0655,0.0494,-0.0012,-0.008,-0.0194,0.0265,-0.0258,-0.0243,0.0663,-0.0204,0.0116,-0.0064,-0.0103,0.0174,-0.0054,-0.0139,-0.0028,0.0679,-0.0075,0.0047,-0.0104,0.0585,0.0116,0.0776,0.0161,0.051,-0.0192,-0.0321,0.0098,-0.0586,0.0234,0.0584,-0.0061,-0.2785,0.0443,0.0139,0.0247,-0.0427,0.0229,0.0468,0.0039,-0.009,-0.0088,0.0181,0.0429,0.0244,0.0044,0.0028,0.0299,0.0844,-0.0056,0.0397,-0.0322,0.0382,0.0754,0.2296,-0.0367,0.0047,0.0079,-0.0085,-0.0364,0.0449,0.014,0.0324,-0.02,0.0768,-0.0243,0.0257,0.0801,0.011,0.0511,0.045,0.021,-0.0168,0.0081,-0.0688,-0.0868,0.0921,0.0329,0.009,-0.0842,0.0002,0.0068,0.0055,-0.0084,-0.0089,-0.0205,0.015,0.0111,-0.0013,-0.0392,-0.0513,-0.0748,0.0244,-0.0447,0.0513,-0.0129,-0.0362]}
{"key":"[Learning visual groups from co-occurrences in space and time] We propose a self-supervised framework that learns to group visual entities based on their rate of co-occurrence in space and time. To model statistical dependencies between the entities, we set up a simple binary classification problem in which the goal is to predict if two visual primitives occur in the same spatial or temporal context. We apply this framework to three domains: learning patch affinities from spatial adjacency in images, learning frame affinities from temporal adjacency in videos, and learning photo affinities from geospatial proximity in image collections. We demonstrate that in each case the learned affinities uncover meaningful semantic groupings. From patch affinities we generate object proposals that are competitive with state-of-the-art supervised methods. From frame affinities we generate movie scene segmentations that correlate well with DVD chapter structure. Finally, from geospatial affinities we learn groups that relate well to semantic place categories.","layer":3,"vector":[0.0042,-0.0201,0.0173,0.0173,0.0406,0.0005,0.0201,-0.0347,-0.0209,-0.0319,0.0102,-0.0866,0.0348,0.1044,0.006,0.0045,-0.0015,0.0432,-0.05,-0.0208,0.0343,-0.0454,0.0012,-0.0402,-0.0119,0.0298,-0.0205,-0.072,-0.041,-0.1929,-0.0122,-0.0066,0.0135,0.0065,-0.0078,-0.0531,-0.0206,0.0571,-0.062,-0.0074,0.0097,0.024,0.0102,-0.0484,-0.0638,-0.0522,-0.009,-0.0375,-0.0338,-0.0098,0.0074,-0.056,0.0367,0.0549,-0.0226,0.0719,0.0547,0.0264,0.0324,-0.0183,0.0476,0.0232,-0.1327,0.0732,0.053,0.026,-0.0136,0.0153,-0.0005,0.021,-0.0309,0.0736,-0.0339,0.0201,0.0211,0.0213,-0.0176,-0.0199,-0.025,-0.0256,-0.0315,0.0207,-0.0345,-0.033,0.0069,-0.0504,0.0266,-0.0436,0.0516,0.0316,-0.0749,0.0146,-0.0397,0.0397,-0.0714,-0.0167,0.0226,0.0065,0.0218,0.2278,-0.0654,0.0555,0.0708,-0.0201,0.0282,-0.0445,-0.0118,-0.0363,-0.0159,0.0189,-0.0075,0.022,-0.0043,-0.0583,0.0259,-0.0242,0.062,0.051,-0.016,-0.0125,0.0155,-0.0024,0.0319,-0.0069,0.0016,-0.0876,0.039,0.1467,0.0881,-0.0127,0.0252,0.0197,-0.0219,0.0218,-0.0309,0.054,0.0273,-0.0084,0.0107,-0.0496,-0.0126,-0.0536,0.0386,-0.0523,-0.0204,0.1352,-0.0407,0.0659,-0.0724,-0.0105,-0.0382,0.0068,-0.0797,-0.0047,-0.0388,-0.0054,0.0522,-0.0058,-0.0444,0.0446,-0.0278,-0.0286,-0.0061,0.1095,0.003,-0.1009,-0.0372,0.0373,0.0378,-0.0095,0.0127,0.0495,0.012,0.04,0.1028,0.0558,-0.1214,0.0181,-0.0032,-0.0139,-0.0009,-0.042,-0.0237,0.0395,0.07,-0.039,-0.0332,-0.0569,0.0293,0.0595,0.0047,0.0159,-0.0232,0.0236,-0.026,-0.011,0.0049,-0.0249,0.0228,0.0014,-0.0022,-0.0335,-0.0505,0.0395,0.0157,-0.0148,-0.0214,-0.0046,0.0289,0.0094,-0.0186,-0.033,0.0203,0.0002,0.0069,-0.0303,-0.0008,0.0141,-0.0392,0.0371,0.0616,-0.0881,-0.0382,-0.2283,0.0105,0.0265,-0.0115,0.0026,-0.0609,0.0498,-0.0029,0.0322,0.0588,0.0426,-0.0537,0.0049,0.0144,-0.0226,0.018,0.0178,0.0581,-0.0259,-0.0006,-0.0297,0.0352,-0.0222,-0.0589,0.0723,0.0121,0.2455,0.0764,0.0048,-0.0437,0.0531,0.0284,-0.0695,-0.0837,0.0228,0.004,0.0633,-0.0201,-0.0093,-0.0484,-0.0327,0.0387,0.0288,-0.0789,-0.0569,-0.0512,-0.0031,0.0736,-0.0187,-0.0047,0.0217,-0.1033,0.032,0.0235,-0.0438,0.0037,-0.0341,0.0184,-0.0156,0.0368,0.0117,-0.0424,0.0386,-0.0399,0.1128,-0.0171,-0.0643,-0.0364,0.0064,-0.0059,-0.0235,0.0655,-0.0162,-0.0413,0.0641,-0.0177,0.0537,0.0045,-0.048,-0.0423,0.0604,-0.0292,-0.011,-0.0192,0.0563,0.0286,0.0577,-0.0219,0.014,-0.0147,0.0062,0.0111,-0.0642,0.0024,0.0288,-0.0011,-0.315,0.0717,0.0101,0.0398,-0.0076,0.0403,0.043,0.0281,-0.01,-0.0212,0.0114,0.0167,0.0316,0.0084,-0.0418,0.0789,0.0875,-0.066,0.0311,-0.043,0.0345,0.0343,0.2227,-0.016,0.021,0.0177,-0.0553,-0.0138,0.0414,0.0163,0.0086,0.0234,0.0937,-0.0629,0.0127,0.0519,-0.0057,0.0067,0.0126,0.0079,-0.034,0.0093,-0.0322,-0.0411,0.1017,0.008,0.0089,-0.0268,-0.0127,0.0272,-0.0153,-0.0337,-0.0066,0.0032,0.0275,-0.0026,-0.0553,0.0037,-0.0377,-0.0285,0.0025,-0.0738,-0.0058,0.0014,-0.0166]}
{"key":"[Object-Oriented Dynamics Predictor] Generalization has been one of the major challenges for learning dynamics models in model-based reinforcement learning. However, previous work on action-conditioned dynamics prediction focuses on learning the pixel-level motion and thus does not generalize well to novel environments with different object layouts. In this paper, we present a novel object-oriented framework, called object-oriented dynamics predictor (OODP), which decomposes the environment into objects and predicts the dynamics of objects conditioned on both actions and object-to-object relations. It is an end-to-end neural network and can be trained in an unsupervised manner. To enable the generalization ability of dynamics learning, we design a novel CNN-based relation mechanism that is class-specific (rather than object-specific) and exploits the locality principle. Empirical results show that OODP significantly outperforms previous methods in terms of generalization over novel environments with various object layouts. OODP is able to learn from very few environments and accurately predict dynamics in a large number of unseen environments. In addition, OODP learns semantically and visually interpretable dynamics models.","layer":4,"vector":[-0.0391,-0.016,0.0401,-0.0355,0.0053,0.0142,0.0072,-0.0053,0.0379,0.0468,0.001,-0.0434,0.0283,0.0735,0.0347,-0.0397,-0.0122,0.0774,-0.0193,-0.0235,0.0422,-0.0392,0.001,-0.0516,-0.0141,0.0409,-0.0278,-0.0082,-0.0069,-0.2341,-0.0112,-0.0393,0.0188,-0.0338,-0.0018,-0.0138,0.0014,0.042,-0.0543,0.0253,0.0222,0.0246,-0.0053,-0.0674,0.0029,-0.0048,0.0177,-0.0446,-0.006,-0.0284,0.0395,-0.0445,-0.0105,0.0327,0.0388,0.014,0.082,0.0359,0.0322,0.0193,0.0046,0.0191,-0.1668,0.0449,0.0223,0.0236,-0.0378,0.0157,0.0385,0.0351,-0.0118,0.0671,0.0325,0.0229,0.0254,-0.0082,-0.0229,-0.0389,-0.0052,-0.0227,0.0321,-0.0137,-0.0371,-0.0475,0.0038,-0.0769,0.0294,-0.0702,0.0578,0.0304,-0.1006,-0.0134,-0.0079,-0.0285,-0.0642,0.0393,0.0429,0.0085,-0.1063,0.2022,-0.0491,0.0276,0.0707,-0.0209,0.0305,-0.025,-0.0236,-0.0163,-0.0288,0.0083,-0.0471,-0.0143,0.0155,-0.0259,0.0449,0.0167,0.0827,0.0233,-0.0054,-0.0034,0.0069,-0.0384,0.0117,-0.0315,0.0084,-0.0697,0.0102,0.158,0.0256,-0.0299,0.0078,-0.0133,-0.0652,-0.0189,0.0234,0.0455,0.0401,-0.0477,-0.011,0.0148,-0.0165,0.0062,-0.0109,-0.1127,-0.0544,0.1042,0.0005,0.031,-0.0244,0.0028,0.0036,0.0685,-0.028,-0.0239,-0.0171,0.0323,0.032,0.0498,-0.0448,-0.001,-0.0578,-0.0623,-0.0392,0.0469,-0.0261,-0.085,-0.0382,-0.0115,0.0252,-0.0258,0.0508,0.0283,-0.0412,0.0123,0.1269,0.0319,-0.0703,0.0128,-0.0312,-0.0151,0.0524,-0.0515,-0.0332,0.0439,0.0392,-0.0184,0.0092,-0.0539,0.0327,-0.018,-0.0185,0.0054,-0.0622,0.0411,-0.0245,0.0191,-0.0199,-0.0072,0.0186,-0.0487,0.006,0.0195,-0.0782,0.042,-0.0435,-0.0089,0.016,-0.002,0.0058,0.0109,-0.0404,-0.0087,0.0439,-0.0151,-0.0058,-0.0056,-0.0262,-0.005,-0.024,0.0163,0.0065,-0.0348,-0.0114,-0.2114,0.0405,-0.0107,-0.0171,0.0394,-0.063,0.0592,-0.0204,0.0507,0.0442,0.0216,-0.0427,0.0233,0.0045,-0.0101,0.0828,0.0531,0.0416,-0.0187,0.0273,-0.002,-0.0275,-0.0332,-0.0996,0.0505,-0.0095,0.2124,0.0387,0.0993,-0.0217,0.0316,0.039,-0.0219,-0.1053,0.088,0.0067,0.0759,-0.0324,-0.0114,-0.028,-0.0289,0.0146,0.0035,-0.103,-0.0277,-0.021,0.005,0.0653,-0.0629,-0.0034,0.0215,-0.0516,0.0563,0.0367,-0.0042,-0.0135,-0.0656,0.027,-0.0436,0.0215,-0.0229,-0.0444,-0.012,-0.0495,0.0407,-0.0071,-0.0012,-0.0824,0.0484,-0.0381,-0.0077,0.0643,0.025,-0.02,0.0616,0.0141,0.022,0.0024,-0.0607,0.0176,0.0707,-0.0273,0.0401,0.0503,0.0296,-0.0378,0.0527,-0.0493,0.0403,-0.031,-0.0081,0.0111,-0.0647,-0.0065,0.0535,0.0117,-0.3313,0.0323,0.0354,0.0651,-0.0413,0.0375,0.0437,0.0099,-0.0188,-0.0335,0.0179,0.0323,0.0789,0.0407,0.0104,0.0078,0.1148,-0.0232,0.0692,-0.0669,0.0411,0.0639,0.1946,-0.0125,0.0394,-0.0346,-0.0412,-0.0141,0.014,0.0008,0.0214,0.0444,0.0776,-0.0775,0.0381,0.0647,0.0117,0.0347,-0.0122,0.0164,-0.0134,-0.0012,0.0088,-0.0534,0.0829,0.0002,-0.0474,-0.0209,-0.0282,0.0438,-0.0233,-0.0133,-0.0323,-0.021,0.0736,0.0246,-0.0275,-0.0343,-0.044,-0.0542,0.0576,-0.0529,0.0027,-0.0137,-0.0136]}
{"key":"[Guided Policy Search Based Control of a High Dimensional Advanced Manufacturing Process] In this paper we apply guided policy search (GPS) based reinforcement learning framework for a high dimensional optimal control problem arising in an additive manufacturing process. The problem comprises of controlling the process parameters so that layer-wise deposition of material leads to desired geometric characteristics of the resulting part surface while minimizing the material deposited. A realistic simulation model of the deposition process along with carefully selected set of guiding distributions generated based on iterative Linear Quadratic Regulator is used to train a neural network policy using GPS. A closed loop control based on the trained policy and in-situ measurement of the deposition profile is tested experimentally, and shows promising performance.","layer":0,"vector":[-0.0486,0.007,0.0628,-0.0097,-0.0154,0.0297,0.0112,0.0485,-0.0098,0.0054,0.0277,-0.0186,0.0237,0.0614,0.0128,0.0148,-0.0058,0.0443,-0.0053,0.0071,0.0842,-0.0734,-0.063,-0.0769,0.0,-0.0068,-0.0536,-0.0453,-0.032,-0.2534,0.0232,-0.0466,0.0175,-0.0436,-0.0369,-0.0214,-0.0293,0.0673,-0.0052,0.0095,0.0486,0.0253,-0.0404,-0.0689,-0.0176,-0.0093,-0.0016,-0.0113,0.0148,-0.0555,0.0215,-0.0289,0.0052,-0.0138,0.0518,0.0429,0.0501,0.048,0.0143,-0.0216,-0.0234,0.0009,-0.1939,0.0898,0.0543,0.078,-0.0347,-0.0344,0.0067,0.0743,-0.0603,0.0533,0.0066,0.0463,0.0299,-0.0162,0.0163,-0.0428,-0.0408,0.0247,0.0249,0.0089,-0.0539,-0.0162,-0.028,-0.0365,0.0196,-0.0339,0.0453,0.0209,-0.0058,0.013,-0.0441,-0.0072,-0.0581,-0.0134,0.0452,0.0217,-0.0846,0.1871,-0.0168,0.0101,0.0258,0.0114,-0.005,-0.0312,-0.0416,0.0092,-0.0485,-0.016,0.0104,-0.0193,0.0128,0.004,-0.0212,0.0131,0.0292,0.0367,-0.0002,-0.0258,-0.0074,-0.0002,0.0434,0.0006,0.01,-0.0958,-0.0058,0.1462,-0.0026,0.0027,0.0557,-0.0302,-0.0512,0.0112,0.0039,0.0365,0.0267,-0.0268,0.0307,-0.0169,-0.0742,0.0305,0.0155,-0.1206,0.0115,0.0861,-0.0484,0.0112,-0.0345,-0.0143,-0.0405,0.0376,0.0006,-0.0009,-0.0231,0.0328,0.0186,0.061,-0.0904,0.0254,-0.0265,-0.0272,-0.0469,0.1121,-0.0275,-0.096,-0.0627,0.0181,0.0563,-0.0166,-0.0119,0.053,-0.0656,0.0047,0.093,0.0534,-0.0863,-0.0007,-0.0102,0.0335,0.0525,-0.0517,-0.0035,-0.0385,0.0351,-0.0646,0.0655,-0.03,0.0112,0.0457,-0.0228,0.0477,-0.0331,-0.0252,0.001,-0.0555,-0.001,-0.0093,0.0362,-0.0313,0.0376,0.001,-0.0374,0.0238,0.0055,0.0595,0.0013,-0.0228,0.0394,0.0637,-0.0539,0.0128,0.0701,-0.0435,-0.0007,0.0164,-0.0356,0.0105,-0.0292,0.0479,0.0811,0.0038,-0.0587,-0.2246,0.0143,-0.0176,0.0295,0.0368,-0.0564,0.0154,-0.0175,0.0094,0.0202,0.0865,0.0227,0.0061,0.0327,-0.0062,0.0193,0.0657,0.0272,-0.0079,0.0001,-0.0548,0.0218,-0.0439,-0.0774,0.0431,-0.0351,0.1715,-0.0095,-0.0068,0.0166,0.0142,0.0342,-0.0458,-0.0706,0.0474,0.0447,0.0471,0.0171,-0.0382,-0.0592,0.0096,-0.0094,-0.0446,-0.0875,0.0375,-0.0165,-0.0741,0.0869,-0.0712,0.0124,0.0489,0.0002,0.0117,-0.0354,-0.0334,-0.0206,-0.0786,0.0547,-0.0466,0.022,0.0239,-0.058,0.0104,-0.0266,0.0542,0.0277,0.0207,-0.0365,0.0382,0.0027,0.0076,0.0779,0.0497,0.0196,0.0356,0.0014,0.0087,0.0489,-0.0125,-0.0553,0.0872,-0.0368,0.0349,0.0268,0.0091,-0.027,0.0467,-0.0296,0.0081,-0.0195,-0.0326,0.0085,-0.0572,0.0151,0.0764,-0.0071,-0.289,0.0335,0.0422,0.0928,-0.0731,-0.0157,0.0277,0.0193,-0.003,0.0324,-0.0275,0.0372,0.0007,0.0101,0.0288,0.0216,0.0334,-0.0643,0.0546,-0.0847,0.0227,0.0463,0.2235,-0.0391,0.0344,-0.0094,-0.0449,-0.0298,0.0236,0.0013,0.0212,0.0339,0.0503,-0.0491,0.0747,0.1094,-0.0434,0.0477,-0.0175,0.0202,-0.0368,0.018,0.0042,-0.0037,0.0547,0.033,-0.0772,-0.0228,-0.0398,0.024,-0.0497,0.0394,-0.0189,0.012,0.0177,0.0313,-0.0634,-0.0666,-0.0473,0.0139,0.0313,-0.0513,-0.0089,-0.0059,0.0147]}
{"key":"[SQ-VAE: Variational Bayes on Discrete Representation with Self-annealed Stochastic Quantization] One noted issue of vector-quantized variational autoencoder (VQ-VAE) is that the learned discrete representation uses only a fraction of the full capacity of the codebook, also known as codebook collapse. We hypothesize that the training scheme of VQ-VAE, which involves some carefully designed heuristics, underlies this issue. In this paper, we propose a new training scheme that extends the standard VAE via novel stochastic dequantization and quantization, called stochastically quantized variational autoencoder (SQ-VAE). In SQ-VAE, we observe a trend that the quantization is stochastic at the initial stage of the training but gradually converges toward a deterministic quantization, which we call self-annealing. Our experiments show that SQ-VAE improves codebook utilization without using common heuristics. Furthermore, we empirically show that SQ-VAE is superior to VAE and VQ-VAE in vision- and speech-related tasks.","layer":0,"vector":[-0.0394,-0.0253,0.0302,-0.0524,0.0326,0.0301,-0.0002,0.031,0.0101,-0.016,0.0318,-0.0752,0.0502,0.0857,0.0706,0.0427,-0.0063,0.0402,-0.0381,-0.029,0.0266,-0.0165,-0.0072,-0.0446,0.0185,-0.0374,-0.0244,-0.0537,-0.0011,-0.2473,0.039,-0.0228,0.0536,-0.0429,0.0032,-0.0536,-0.0691,0.0852,-0.0275,0.0477,0.018,0.0267,0.0005,-0.047,-0.0315,-0.0714,-0.0134,-0.0316,-0.0198,-0.0209,0.038,-0.0179,0.0549,0.0243,-0.0004,0.018,0.0401,0.0713,0.024,0.0448,0.0415,0.0373,-0.1524,0.0798,0.0208,0.0207,-0.0325,-0.0045,0.0288,0.0246,-0.0102,0.0168,0.0129,0.0476,0.0415,0.0087,0.0128,-0.0334,-0.0214,0.0111,0.0045,-0.013,-0.0373,0.0101,-0.0534,-0.0401,0.0249,-0.0362,0.0551,0.0099,-0.0441,-0.0478,-0.0094,0.0292,-0.0584,-0.0095,-0.0019,0.0468,-0.0304,0.2126,-0.0234,0.0431,0.0439,-0.0365,0.0588,-0.0553,-0.037,-0.051,-0.027,0.0144,0.0057,-0.0301,0.038,-0.0937,0.0126,-0.0133,0.0437,0.0234,0.0118,0.0049,-0.0222,0.0184,0.0228,-0.0064,-0.0008,-0.0611,0.0237,0.1671,-0.0173,0.0231,0.0612,-0.0045,0.0081,-0.0466,0.0347,0.0107,0.013,-0.0303,0.0205,-0.0175,-0.0231,-0.0524,-0.006,-0.082,-0.0556,0.0894,-0.085,0.0105,-0.0379,-0.0317,0.0689,0.0369,0.0078,-0.0257,0.067,0.0684,0.0151,0.0325,-0.078,-0.0369,-0.005,-0.0574,-0.014,0.0773,0.0005,-0.0507,-0.0632,-0.0166,0.0038,-0.0642,0.064,0.0118,-0.0352,0.0414,0.0656,0.0545,-0.0837,0.021,0.0014,0.0192,-0.0102,-0.063,-0.0057,0.0491,0.0442,-0.0432,0.0206,-0.0057,0.0108,0.0092,0.0391,0.0049,-0.022,-0.0245,0.0149,0.0061,-0.0134,-0.0305,-0.0087,-0.0259,0.0167,-0.0102,-0.0636,0.0751,0.0207,-0.017,-0.0262,-0.0185,0.0834,0.0353,-0.0333,-0.0163,0.0734,-0.0138,-0.0068,0.0105,-0.0036,0.0561,-0.0212,0.0144,0.0154,-0.0709,-0.032,-0.2238,-0.0193,0.0005,-0.0762,0.0165,-0.0614,-0.0124,-0.0031,0.0611,0.0595,0.0018,-0.0669,-0.0,0.0376,0.0361,0.0412,0.0614,0.0487,-0.0038,0.0381,-0.0076,0.0124,-0.0499,-0.0707,0.0377,0.0302,0.2244,0.0128,0.0729,0.0155,0.0315,0.0537,-0.0604,-0.0653,0.0737,0.0092,0.0753,-0.0016,-0.0605,-0.0006,-0.0072,0.008,-0.0144,-0.1223,-0.0531,-0.0601,-0.0482,0.0271,-0.0829,-0.0064,0.0442,-0.0414,0.0405,-0.0242,-0.0059,-0.0608,-0.0718,-0.0255,-0.0286,0.0234,0.0034,-0.0679,-0.0033,-0.0234,0.0325,0.0021,-0.0225,-0.0351,0.03,-0.0128,-0.0477,0.0596,-0.0225,-0.0296,0.0551,0.0003,0.0289,-0.0108,-0.0294,-0.0539,0.0377,-0.0136,0.0499,0.0057,0.0217,0.0356,0.062,0.0113,0.0294,0.0336,-0.0385,0.0212,-0.0482,-0.0039,0.0082,-0.0359,-0.2972,0.0268,0.055,0.0241,-0.0321,0.0162,0.0473,-0.0016,-0.0348,-0.0068,-0.0582,0.0317,0.0222,-0.0109,0.0265,0.0142,0.1035,-0.0235,0.0729,-0.0209,-0.005,0.0239,0.2224,-0.002,0.0282,0.0058,-0.0288,0.0555,0.0583,-0.0466,-0.0148,0.0046,0.117,-0.0432,-0.0063,0.0607,-0.0386,0.0537,0.0131,-0.0009,0.0086,-0.0303,-0.018,-0.0362,0.1045,0.005,0.0328,-0.0019,-0.037,0.0176,-0.0122,0.0417,-0.0368,-0.0291,0.016,0.0422,-0.0457,-0.0442,-0.0066,-0.0327,0.0355,-0.0579,-0.0288,-0.0022,-0.0376]}
{"key":"[AR-Net: A simple Auto-Regressive Neural Network for time-series] In this paper we present a new framework for time-series modeling that combines the best of traditional statistical models and neural networks. We focus on time-series with long-range dependencies, needed for monitoring fine granularity data (e.g. minutes, seconds, milliseconds), prevalent in operational use-cases. Traditional models, such as auto-regression fitted with least squares (Classic-AR) can model time-series with a concise and interpretable model. When dealing with long-range dependencies, Classic-AR models can become intractably slow to fit for large data. Recently, sequence-to-sequence models, such as Recurrent Neural Networks, which were originally intended for natural language processing, have become popular for time-series. However, they can be overly complex for typical time-series data and lack interpretability. A scalable and interpretable model is needed to bridge the statistical and deep learning-based approaches. As a first step towards this goal, we propose modelling AR-process dynamics using a feed-forward neural network approach, termed AR-Net. We show that AR-Net is as interpretable as Classic-AR but also scales to long-range dependencies. Our results lead to three major conclusions: First, AR-Net learns identical AR-coefficients as Classic-AR, thus being equally interpretable. Second, the computational complexity with respect to the order of the AR process, is linear for AR-Net as compared to a quadratic for Classic-AR. This makes it possible to model long-range dependencies within fine granularity data. Third, by introducing regularization, AR-Net automatically selects and learns sparse AR-coefficients. This eliminates the need to know the exact order of the AR-process and allows to learn sparse weights for a model with long-range dependencies.","layer":2,"vector":[-0.0435,-0.0124,0.0299,-0.0179,0.0595,0.0208,0.0122,-0.0159,0.0734,-0.0455,-0.0009,-0.0261,0.039,0.0514,0.032,0.0386,-0.0191,0.0627,-0.0016,-0.0328,0.0317,-0.0256,-0.0169,-0.0217,0.0565,0.018,-0.0038,-0.024,-0.0468,-0.2693,0.0352,-0.0438,0.0307,0.0157,0.0415,-0.0122,-0.0477,0.0404,-0.0237,0.0534,0.0322,0.0266,-0.0085,-0.0384,-0.0094,-0.0664,-0.0001,-0.016,-0.018,-0.0223,0.0,-0.0368,0.0381,0.0286,0.0384,0.0106,0.0528,-0.0164,0.0646,-0.0005,0.0468,0.0219,-0.185,0.0388,0.0414,0.0421,-0.0132,0.037,0.0127,0.0329,-0.0136,-0.0009,0.0096,0.0384,0.0033,0.0362,0.0068,-0.0201,-0.0165,-0.0005,0.0529,-0.0338,-0.046,-0.0485,-0.0199,-0.0832,0.0124,-0.0652,0.0435,-0.0087,-0.0273,-0.0161,-0.0059,0.029,-0.0596,-0.0288,0.0295,0.0414,-0.0576,0.2099,-0.0611,0.0394,0.0322,-0.0002,0.0136,0.0225,-0.0435,-0.0356,-0.0038,0.0391,-0.0059,0.0158,0.0424,-0.0854,0.0472,0.0063,0.0382,0.0286,0.0044,0.0079,0.0217,0.0016,0.0244,-0.0227,0.0487,-0.0534,0.079,0.1782,0.0151,0.0098,0.0445,-0.0155,-0.0666,0.0056,0.034,0.0348,0.0208,-0.0312,0.0151,-0.0137,-0.0546,-0.044,0.0347,-0.0809,-0.0496,0.1245,-0.0219,-0.0154,-0.0637,-0.0235,-0.074,0.0283,0.0044,-0.0374,0.0413,0.0585,0.0452,-0.0181,-0.0537,-0.0175,-0.0404,-0.0569,-0.0202,0.0561,0.0308,-0.0652,-0.0497,-0.0235,0.0146,0.0211,0.0558,0.0238,-0.0224,0.036,0.0835,0.0454,-0.0272,-0.0033,0.0054,0.0134,0.0351,-0.0418,-0.0218,0.074,0.0516,-0.0457,0.0146,-0.0448,0.001,-0.0057,-0.0559,0.0229,-0.0192,0.0394,0.0108,-0.0121,-0.0118,0.0305,0.0456,-0.0547,-0.0021,-0.0163,-0.0399,0.0052,-0.0001,0.0145,-0.0689,0.0162,0.0164,0.0152,-0.0188,0.0344,0.0668,-0.0394,-0.0295,0.0152,-0.0233,0.0315,0.0196,0.0403,0.0272,-0.0399,-0.0745,-0.2379,0.0205,0.0213,-0.0077,0.1036,-0.0668,0.0249,-0.0376,0.0545,0.0428,0.0533,-0.0255,0.002,0.0126,-0.0245,0.0716,-0.0042,0.0445,-0.0415,0.0317,-0.0372,-0.0049,-0.0433,-0.1022,-0.0005,0.0255,0.1644,0.0003,0.0398,-0.0763,0.0219,0.016,-0.0178,-0.0486,0.0539,0.007,0.0685,-0.0021,-0.0621,-0.0369,-0.0831,0.0284,0.0076,-0.0703,-0.0474,-0.0002,0.014,-0.0173,-0.1027,-0.0022,0.0295,-0.053,0.0601,-0.0093,-0.02,-0.039,-0.0778,0.0573,-0.0428,0.0103,0.0106,-0.048,0.0136,-0.0217,0.0325,0.0203,-0.0511,-0.027,0.0011,-0.0029,-0.0051,0.1164,-0.0229,-0.0002,0.0561,-0.0091,-0.0092,0.0098,-0.0462,-0.0347,0.0737,-0.0449,0.0734,0.0088,0.0207,-0.0227,0.0523,0.0238,0.008,0.0042,0.0024,-0.0132,-0.074,-0.0673,0.0701,0.0151,-0.2898,0.0723,0.0085,0.0431,-0.0005,-0.0181,-0.003,0.0346,-0.0277,0.0018,-0.0411,0.0273,0.0645,-0.0375,-0.0026,0.0379,0.0749,-0.0378,0.0389,-0.0696,0.039,0.0512,0.1796,-0.0493,0.0299,-0.0095,-0.0114,-0.0321,0.0491,0.0017,0.0274,0.0071,0.1166,-0.0346,-0.0073,0.0692,-0.0535,0.0939,0.0043,-0.0095,0.037,-0.012,-0.0604,-0.039,0.0688,-0.0318,-0.0314,-0.0724,-0.0371,0.0671,-0.0547,0.0009,0.011,0.0569,0.0311,0.0177,-0.0547,-0.0194,-0.0111,-0.0125,-0.0203,-0.0897,-0.0069,-0.0131,-0.0421]}
{"key":"[Multi Layered-Parallel Graph Convolutional Network (ML-PGCN) for Disease Prediction] Structural data from Electronic Health Records as complementary information to imaging data for disease prediction. We incorporate novel weighting layer into the Graph Convolutional Networks, which weights every element of structural data by exploring its relation to the underlying disease. We demonstrate the superiority of our developed technique in terms of computational speed and obtained encouraging results where our method outperforms the state-of-the-art methods when applied to two publicly available datasets ABIDE and Chest X-ray in terms of relative performance for the accuracy of prediction by 5.31 % and 8.15 % and for the area under the ROC curve by 4.96 % and 10.36 % respectively. Additionally, the model is lightweight, fast and easily trainable.","layer":4,"vector":[0.0008,-0.0076,0.0352,-0.0188,0.0315,0.0544,0.0474,0.0346,0.0234,0.0023,0.0222,-0.0742,0.0389,0.0632,0.0259,0.0218,0.0277,0.0295,-0.0349,0.0438,0.0128,0.001,-0.0,-0.0741,0.0741,0.0452,0.0107,-0.0588,-0.0715,-0.218,0.0068,-0.0735,0.0772,-0.0212,-0.0244,-0.0716,-0.0034,0.0363,-0.0362,0.0278,0.003,-0.0075,-0.0219,-0.0375,-0.0208,-0.0062,-0.0535,-0.0211,0.0257,0.0146,0.0247,-0.0376,0.0258,0.0357,0.0337,0.0277,0.0516,0.0118,0.0299,0.0669,0.0461,0.0197,-0.1647,0.0834,0.0529,0.0273,-0.0781,-0.0341,0.0025,0.0616,0.0203,0.0416,0.0246,0.0383,-0.0058,0.0487,0.0105,-0.0114,-0.0462,0.0079,0.0065,0.0225,-0.0361,-0.0143,-0.0197,-0.0428,-0.0028,-0.0535,0.0055,0.025,-0.0416,-0.0467,-0.0646,0.0573,-0.092,-0.0063,0.0358,0.0277,-0.0714,0.1918,-0.0823,0.0011,0.0498,-0.0033,0.0281,-0.009,0.0161,-0.0426,-0.0573,0.0026,0.0277,-0.0266,0.0367,-0.0573,0.0079,0.0278,0.0346,0.0737,-0.0305,-0.0026,0.0035,-0.0222,0.0519,-0.0442,0.0364,-0.0434,-0.0024,0.1384,0.0429,0.0083,0.0387,0.0101,-0.0191,-0.0182,-0.0455,-0.009,0.056,-0.0225,-0.0275,-0.0062,-0.0585,-0.0516,-0.0106,-0.0673,-0.0775,0.1092,-0.0512,-0.0296,-0.0357,-0.0708,-0.0379,0.0043,-0.0158,0.0023,0.0182,-0.0184,-0.0278,0.0359,-0.0392,0.0192,-0.0199,-0.0651,-0.0647,0.1156,0.0402,-0.0826,-0.0565,-0.025,0.0139,-0.0096,0.0496,0.0171,-0.0215,0.0085,0.0698,0.0428,-0.0696,-0.0537,0.0027,0.0057,0.0178,-0.0375,-0.0242,0.0159,0.0569,-0.0236,-0.0009,-0.0381,0.0635,0.0556,-0.0405,0.0614,-0.0686,0.0167,-0.0169,-0.007,-0.0421,-0.0198,0.0112,-0.0022,0.0109,-0.0121,0.0094,0.0269,-0.04,0.0035,-0.0171,0.0408,-0.0205,0.0027,-0.0208,0.0152,0.0758,-0.0229,-0.0516,-0.0134,0.0313,0.0324,0.0054,0.0637,0.0364,-0.0463,-0.0383,-0.1956,-0.004,0.0435,-0.0176,0.0146,-0.0625,0.0416,-0.0058,0.0593,0.0796,0.0763,0.0274,-0.068,-0.0255,0.0044,0.0563,0.0313,0.027,-0.0163,-0.039,-0.0195,-0.0015,0.0299,-0.1098,0.0678,0.0523,0.2128,0.0247,-0.0028,0.0317,-0.0161,0.0135,-0.0545,-0.098,0.0555,0.0141,0.0282,0.0153,-0.1034,0.0092,-0.0752,0.0296,0.0157,-0.1044,-0.0407,0.0036,-0.0152,0.0364,-0.0778,0.0206,0.0455,-0.0214,0.0489,0.0407,0.0185,-0.0339,-0.1283,0.0523,-0.0397,0.0249,0.0172,-0.0673,0.0042,-0.0579,0.0171,0.0018,-0.0513,-0.0534,-0.0109,-0.0337,-0.0332,0.0722,0.0286,-0.0125,0.0784,0.0129,0.0277,-0.0196,-0.0372,0.0124,0.0662,-0.0264,0.0231,0.0503,0.0513,0.0091,0.0994,0.0393,0.0348,-0.0458,-0.0279,0.0136,-0.0306,-0.031,-0.0099,0.0022,-0.3066,0.0495,-0.0212,0.0605,-0.0073,0.0281,0.0591,0.0444,-0.0161,0.0024,0.0204,-0.0017,0.0456,-0.0588,-0.0454,0.0188,0.0896,-0.0577,0.0605,-0.0266,-0.0189,0.0317,0.2136,-0.0401,0.0112,0.0443,-0.0083,-0.0227,-0.008,0.0169,0.0112,-0.0061,0.0167,-0.0737,0.0473,0.1024,-0.0071,0.052,0.0096,-0.0322,0.0198,0.0249,-0.0284,0.0095,0.0567,-0.0039,-0.0356,-0.0304,0.0184,0.0082,-0.0048,-0.0001,0.013,-0.0001,0.0364,0.0075,-0.0158,-0.0185,-0.0297,-0.0328,0.0236,-0.0244,-0.0661,-0.0035,-0.0237]}
{"key":"[User-Device Authentication in Mobile Banking using APHEN for Paratuck2 Tensor Decomposition] The new financial European regulations such as PSD2 are changing the retail banking services. Noticeably, the monitoring of the personal expenses is now opened to other institutions than retail banks. Nonetheless, the retail banks are looking to leverage the user-device authentication on the mobile banking applications to enhance the personal financial advertisement. To address the profiling of the authentication, we rely on tensor decomposition, a higher dimensional analogue of matrix decomposition. We use Paratuck2, which expresses a tensor as a multiplication of matrices and diagonal tensors, because of the imbalance between the number of users and devices. We highlight why Paratuck2 is more appropriate in this case than the popular CP tensor decomposition, which decomposes a tensor as a sum of rank-one tensors. However, the computation of Paratuck2 is computational intensive. We propose a new APproximate HEssian-based Newton resolution algorithm, APHEN, capable of solving Paratuck2 more accurately and faster than the other popular approaches based on alternating least square or gradient descent. The results of Paratuck2 are used for the predictions of users' authentication with neural networks. We apply our method for the concrete case of targeting clients for financial advertising campaigns based on the authentication events generated by mobile banking applications.","layer":1,"vector":[-0.0271,-0.0007,-0.0036,0.0081,0.0003,0.0087,0.0378,0.0232,0.0426,-0.0305,-0.0224,-0.0525,0.0285,0.0223,0.0578,-0.0302,0.0343,0.0423,-0.0198,0.0745,0.0293,-0.052,-0.0313,-0.0371,0.0374,0.026,-0.0269,-0.0291,-0.0621,-0.226,0.0436,-0.0936,0.0682,-0.0486,0.03,-0.0394,-0.0401,0.0476,-0.0353,0.0239,-0.0235,0.025,-0.0368,-0.0266,-0.0134,-0.0406,-0.0248,-0.0027,-0.0415,-0.0231,0.0323,-0.0108,0.0295,0.0397,0.0514,0.0109,0.0236,0.0178,0.0221,0.0732,0.0302,0.0052,-0.1874,0.0811,0.0397,0.0229,0.0156,-0.0448,0.001,0.0858,-0.0411,0.0228,0.009,-0.0006,0.0274,0.0027,0.008,-0.0356,-0.0052,-0.0101,0.0255,-0.0066,-0.0129,0.0087,-0.026,0.0065,0.0546,-0.0207,0.0265,0.0053,-0.0278,0.0022,-0.0543,0.0156,-0.0614,-0.0659,0.0222,0.0755,-0.0313,0.2365,-0.0285,0.0284,0.0414,-0.0395,0.0155,-0.042,-0.0525,-0.0151,-0.0206,-0.0053,-0.033,-0.031,0.0439,-0.0386,0.0117,0.0052,0.0153,0.0379,-0.0172,0.0133,-0.0285,0.0095,0.0358,-0.0099,0.0042,-0.0342,-0.004,0.1199,0.0105,0.0646,0.0259,-0.006,-0.0666,-0.007,0.0252,-0.0031,-0.023,0.0434,0.0062,0.0037,-0.0468,-0.0324,0.0183,-0.0755,-0.057,0.1381,-0.0329,0.0368,-0.026,-0.0179,-0.0061,0.0565,-0.0523,-0.0155,0.0239,0.0389,0.0296,0.0274,-0.0837,0.0139,0.0023,-0.074,-0.0449,0.0997,0.0303,-0.0707,0.0067,0.0035,0.0148,-0.0144,0.0486,0.0031,-0.0327,0.013,0.056,-0.0018,-0.0644,0.0059,0.006,0.0222,0.0096,-0.0437,-0.0363,0.0314,0.0424,-0.0342,-0.0025,-0.0482,0.0087,-0.0036,-0.0843,0.0114,-0.0853,-0.0564,-0.0315,-0.0182,-0.0354,-0.0159,-0.0205,-0.0178,0.0218,0.028,-0.0191,0.0123,0.0222,0.0656,-0.0479,-0.0139,0.0131,0.0465,-0.0076,0.0038,0.0398,-0.0187,-0.0091,-0.031,0.024,0.0598,-0.0016,0.0509,0.0399,-0.0552,-0.0704,-0.2242,-0.0316,0.0046,-0.0112,0.0503,-0.0759,0.0278,-0.0144,0.048,0.06,0.0673,0.0267,-0.0002,0.0334,0.0001,0.0526,0.0195,0.0252,-0.0106,-0.0176,-0.0039,0.0276,-0.0189,-0.0308,0.0448,0.0015,0.2318,0.0519,0.0255,0.0179,0.0264,0.02,-0.0116,-0.0733,0.0962,0.0282,0.077,0.0011,-0.0346,-0.0383,-0.0249,0.0183,0.0156,-0.0502,-0.0161,-0.0229,-0.0506,0.0246,-0.0095,0.0304,0.0851,-0.011,0.0777,-0.0366,-0.0075,-0.0912,-0.0494,0.022,-0.0591,0.0374,-0.0281,-0.0901,0.0027,-0.0478,0.0765,-0.017,-0.0477,-0.0162,0.0113,0.0204,-0.0407,0.0775,0.0238,-0.0401,0.085,-0.0056,0.0693,-0.0173,-0.0247,-0.015,0.0357,-0.0368,0.0762,0.0393,0.0121,-0.0089,0.0758,-0.0094,0.0402,-0.0639,-0.038,-0.0092,-0.0574,0.0296,0.0192,-0.0165,-0.3167,0.0294,0.0199,0.0538,-0.0298,-0.0055,0.0423,0.011,-0.0597,-0.0235,0.0136,0.0287,0.0899,-0.0288,0.0184,0.0215,0.0421,-0.0625,0.0054,-0.0671,0.0243,0.042,0.2222,-0.0436,0.0218,0.0223,0.0099,0.0146,0.0787,-0.0019,-0.019,0.0051,0.1081,-0.0795,0.038,0.0423,-0.0578,0.0038,0.0472,-0.0591,0.0217,-0.0014,-0.0648,-0.0383,0.0779,-0.0163,-0.0125,-0.043,0.0255,0.0158,0.0108,-0.0044,-0.0242,0.0315,-0.0033,0.0096,-0.0757,-0.028,-0.042,-0.0126,-0.0049,-0.0278,-0.0252,-0.0279,-0.0278]}
{"key":"[Sparse Centroid-Encoder: A Nonlinear Model for Feature Selection] We develop a sparse optimization problem for the determination of the total set of features that discriminate two or more classes. This is a sparse implementation of the centroid-encoder for nonlinear data reduction and visualization called Sparse Centroid-Encoder (SCE). We also provide a feature selection framework that first ranks each feature by its occurrence, and the optimal number of features is chosen using a validation set. The algorithm is applied to a wide variety of data sets including, single-cell biological data, high dimensional infectious disease data, hyperspectral data, image data, and speech data. We compared our method to various state-of-the-art feature selection techniques, including two neural network-based models (DFS, and LassoNet), Sparse SVM, and Random Forest. We empirically showed that SCE features produced better classification accuracy on the unseen test data, often with fewer features.","layer":5,"vector":[-0.0017,0.0232,0.0271,0.006,0.0594,0.0431,0.0265,0.0672,-0.0136,-0.0251,-0.0284,-0.0304,0.0599,0.0459,0.0232,0.0148,0.0386,0.0719,-0.0368,0.0288,0.0309,-0.0363,-0.0037,-0.075,0.007,-0.0012,-0.0116,-0.0269,-0.0763,-0.2445,0.033,-0.0301,0.0783,-0.0497,0.007,-0.0277,0.0038,0.0685,-0.0474,0.0246,0.0013,-0.0024,-0.0362,-0.0374,-0.0232,-0.0601,-0.0017,-0.0057,0.0015,-0.0395,0.0302,-0.0841,-0.027,0.0211,0.0233,0.0247,0.0494,0.0413,0.0064,0.0489,0.0324,0.0646,-0.1434,0.0511,0.062,0.0336,-0.0267,-0.0256,0.0234,0.0161,-0.0281,0.0222,-0.0034,0.0442,-0.0161,-0.0339,-0.019,-0.0456,0.0148,0.0246,0.0455,-0.0153,-0.0359,-0.0169,-0.0023,-0.008,0.02,-0.0657,0.0392,0.0181,-0.0468,-0.0183,-0.0662,0.0113,-0.0929,-0.0402,0.0547,0.0204,-0.0517,0.2193,-0.0538,0.0133,-0.0197,-0.0591,0.0413,-0.0844,-0.0378,-0.0344,-0.0334,0.0318,0.0113,-0.0056,-0.0284,-0.0107,0.0101,-0.0418,0.0465,0.0099,-0.0155,0.0397,0.0213,-0.01,0.0853,-0.0485,0.0777,-0.0675,0.0269,0.1361,0.0404,0.0092,0.0861,0.015,-0.0413,-0.0344,0.0094,0.0425,0.0455,0.0399,-0.0343,0.004,-0.0559,-0.0636,0.0215,-0.0897,-0.0774,0.1086,-0.0784,-0.0382,-0.0442,-0.0772,-0.0129,-0.0099,-0.0193,-0.0386,-0.0168,0.0184,0.0288,0.0248,-0.0359,0.0266,0.0194,-0.0648,0.0121,0.0937,0.0074,-0.1081,-0.0637,0.0144,0.0233,0.0093,-0.0061,0.0311,-0.0485,0.0475,0.0792,0.0243,-0.0569,-0.0025,0.0263,0.0101,0.0151,-0.0285,-0.0445,0.0372,0.0593,-0.0094,0.001,-0.028,0.0041,0.0292,-0.0247,0.0124,-0.0135,-0.0178,-0.0325,-0.0458,0.0255,-0.0261,-0.0027,-0.0607,0.0471,0.0151,-0.0498,0.0241,0.0087,0.0218,-0.002,0.0001,0.0521,0.0449,-0.013,0.0162,0.0626,-0.0384,-0.0499,0.019,-0.0071,0.0253,0.01,0.047,0.0606,-0.0166,-0.0878,-0.2096,-0.0139,0.0479,-0.0327,0.0166,-0.0636,0.0233,-0.0045,0.0709,0.042,0.0307,0.0246,-0.0632,0.0237,-0.0262,0.0519,0.0502,0.0294,-0.0302,0.0493,0.0135,0.0051,0.0077,-0.0499,0.0412,0.0235,0.1988,0.0266,0.0231,-0.0239,0.0277,0.048,-0.0353,-0.101,0.04,0.0304,0.0391,-0.006,-0.0531,-0.0124,0.0023,0.0223,0.0011,-0.0933,-0.001,-0.0601,-0.0303,0.0222,-0.077,0.0172,0.0818,0.0038,0.0654,-0.0206,0.0519,-0.0259,-0.0965,0.045,-0.0319,0.0334,0.0147,-0.0848,-0.0091,-0.0486,0.0272,0.0468,-0.0506,-0.011,0.0385,-0.035,-0.0083,0.0988,-0.0071,-0.013,0.0433,0.0383,0.0356,-0.0488,0.0255,-0.0156,0.072,-0.0255,0.0356,-0.0067,0.0275,0.009,0.1157,-0.0282,0.0201,-0.0507,-0.004,-0.0045,-0.0421,-0.0326,0.0234,-0.0044,-0.2945,0.0235,0.0461,0.0132,0.0089,0.0222,0.0397,-0.0149,-0.0309,0.005,0.0276,0.018,0.0498,-0.0315,-0.0059,0.0498,0.0601,-0.0578,0.0435,-0.0928,0.0319,0.0374,0.1853,-0.0324,0.0262,0.0285,-0.0033,0.0261,-0.0289,-0.0335,0.0567,0.0437,0.0939,-0.0442,-0.0018,0.1117,-0.05,0.0238,0.0042,-0.0352,0.0299,0.0052,-0.0581,-0.0231,0.0905,-0.0352,-0.0112,-0.0388,-0.0352,0.0059,-0.0306,-0.0114,-0.0174,-0.0304,0.0034,0.0568,-0.0409,-0.0038,-0.0511,-0.0634,0.0317,-0.0461,-0.0341,-0.007,-0.0174]}
{"key":"[Automatic Expert Selection for Multi-Scenario and Multi-Task Search] Multi-scenario learning (MSL) enables a service provider to cater for users' fine-grained demands by separating services for different user sectors, e.g., by user's geographical region. Under each scenario there is a need to optimize multiple task-specific targets e.g., click through rate and conversion rate, known as multi-task learning (MTL). Recent solutions for MSL and MTL are mostly based on the multi-gate mixture-of-experts (MMoE) architecture. MMoE structure is typically static and its design requires domain-specific knowledge, making it less effective in handling both MSL and MTL. In this paper, we propose a novel Automatic Expert Selection framework for Multi-scenario and Multi-task search, named AESM^{2}. AESM^{2} integrates both MSL and MTL into a unified framework with an automatic structure learning. Specifically, AESM^{2} stacks multi-task layers over multi-scenario layers. This hierarchical design enables us to flexibly establish intrinsic connections between different scenarios, and at the same time also supports high-level feature extraction for different tasks. At each multi-scenario/multi-task layer, a novel expert selection algorithm is proposed to automatically identify scenario-/task-specific and shared experts for each input. Experiments over two real-world large-scale datasets demonstrate the effectiveness of AESM^{2} over a battery of strong baselines. Online A/B test also shows substantial performance gain on multiple metrics. Currently, AESM^{2} has been deployed online for serving major traffic.","layer":11,"vector":[-0.0665,-0.0012,-0.0087,0.0006,0.0138,0.0089,0.0279,0.0412,0.0258,-0.0478,-0.0593,-0.0536,0.0274,0.0726,0.0509,0.0416,-0.0029,0.0276,-0.0049,-0.0079,0.0432,-0.0397,0.0037,-0.068,-0.0009,0.0225,-0.0352,-0.0247,-0.0577,-0.2267,-0.0017,-0.0173,0.0476,-0.0153,-0.0302,0.0044,-0.054,0.0663,0.0351,0.0123,0.0367,-0.0054,0.0045,-0.0739,-0.0375,-0.0267,-0.0163,0.0331,-0.0262,-0.0059,0.068,-0.0641,0.0304,0.0281,0.0145,0.0409,-0.0119,0.0518,0.0489,0.0406,0.0321,0.0394,-0.1799,0.0403,0.035,0.0469,-0.0306,-0.0017,0.0105,0.0218,-0.0353,0.0689,-0.019,0.0668,0.0234,0.0237,-0.0131,-0.0083,0.0069,0.0018,0.0287,-0.0462,-0.0555,-0.0181,-0.0109,-0.0009,-0.0203,-0.039,0.0474,-0.0423,-0.0338,-0.0437,-0.028,0.0171,-0.0158,-0.0018,0.0437,0.0207,-0.0487,0.2045,-0.0349,0.0278,0.0474,-0.0626,0.0296,-0.0487,-0.0221,-0.0075,-0.0231,-0.009,-0.0043,-0.0311,0.019,0.0181,0.035,0.0381,0.0342,0.0333,-0.0227,-0.0027,0.0019,0.0143,0.0732,-0.0384,0.0382,-0.082,0.0317,0.151,0.0031,0.0101,0.0313,-0.0096,-0.0887,-0.0259,0.0339,-0.0086,0.041,0.0103,-0.0303,-0.0117,-0.0537,-0.031,0.0233,-0.0698,-0.0328,0.1549,-0.0173,0.0047,-0.0579,-0.0368,-0.0357,-0.0013,-0.0564,-0.0198,-0.0203,0.0432,0.0498,0.043,-0.0661,-0.0163,0.0049,-0.0491,-0.0275,0.1198,-0.0119,-0.1208,-0.0506,-0.032,-0.0192,-0.0068,0.0589,0.0271,-0.0747,0.0421,0.0754,0.0365,-0.0649,0.0592,-0.0181,-0.009,0.0079,-0.0244,-0.0029,0.0105,0.0446,-0.0293,0.0096,-0.0219,0.0134,0.0129,-0.0306,0.0362,0.0455,0.0074,0.0084,-0.03,-0.0247,0.0086,0.0558,-0.0049,0.0462,0.0095,-0.0439,-0.0156,-0.0193,0.04,-0.0457,0.0077,0.0466,0.0271,-0.009,-0.034,0.0288,-0.0056,-0.013,-0.0438,0.053,0.0577,0.0124,0.0575,0.0154,-0.0066,-0.0414,-0.23,-0.0103,-0.0115,0.0002,0.0213,-0.0066,0.0344,-0.002,0.0436,0.0852,0.0933,-0.0271,-0.0301,0.0449,-0.0174,0.0374,0.0281,-0.0251,0.0013,0.0126,0.0295,0.0385,-0.0168,-0.0811,0.0806,0.0057,0.1955,0.0302,0.0069,-0.0604,0.0438,0.0202,0.0031,-0.0856,0.0577,-0.0077,0.1153,-0.0347,-0.0209,-0.0427,-0.0503,0.0302,-0.0231,-0.0988,-0.0811,-0.0451,-0.0076,0.0287,-0.0684,0.0094,0.0703,-0.0158,0.0109,-0.0446,-0.0489,-0.0299,-0.091,0.023,-0.0208,0.0009,0.0343,-0.0145,-0.037,-0.0357,0.0614,0.0022,-0.0375,-0.0241,0.0025,-0.0324,-0.0325,0.0673,-0.0324,-0.0154,0.0092,-0.0299,0.0418,-0.0326,-0.0316,-0.0162,0.0662,-0.022,0.0363,0.008,0.0048,0.0292,0.0608,-0.0138,0.023,-0.0335,0.0079,-0.0262,-0.0085,-0.022,0.0405,-0.0348,-0.3117,0.0813,0.0118,0.0256,-0.0336,0.0279,0.0237,-0.0159,-0.0161,0.0131,0.007,0.0327,0.0146,0.0102,0.0005,0.0556,0.0883,0.0077,0.0162,-0.0322,0.0003,0.0569,0.2415,-0.0639,0.048,0.0186,-0.055,-0.0304,0.0272,-0.0013,0.053,-0.0348,0.0719,-0.0776,0.0229,0.0767,-0.0032,0.0582,0.0085,-0.0071,-0.0059,0.02,-0.0222,-0.0498,0.1078,0.003,-0.0151,-0.0848,-0.0212,0.0249,0.0121,0.0086,-0.0405,-0.0191,0.0427,0.0294,-0.0286,-0.0331,-0.0728,-0.0469,0.0442,-0.0006,-0.0147,-0.0199,-0.0057]}
{"key":"[Class-wise Thresholding for Detecting Out-of-Distribution Data] We consider the problem of detecting OoD(Out-of-Distribution) input data when using deep neural networks, and we propose a simple yet effective way to improve the robustness of several popular OoD detection methods against label shift. Our work is motivated by the observation that most existing OoD detection algorithms consider all training/test data as a whole, regardless of which class entry each input activates (inter-class differences). Through extensive experimentation, we have found that such practice leads to a detector whose performance is sensitive and vulnerable to label shift. To address this issue, we propose a class-wise thresholding scheme that can apply to most existing OoD detection algorithms and can maintain similar OoD detection performance even in the presence of label shift in the test distribution.","layer":3,"vector":[-0.0144,-0.0043,0.0293,-0.0428,0.0678,0.0222,0.0604,0.0035,0.0475,-0.0288,0.0236,-0.0131,-0.0092,0.068,0.0036,0.0077,0.0378,0.0632,-0.0293,-0.0375,0.0286,-0.0405,0.0026,0.0019,0.0374,0.0217,-0.0022,-0.0453,-0.087,-0.274,0.0124,-0.0385,0.0673,-0.0585,0.0354,-0.0454,-0.0473,0.0109,-0.0223,0.013,0.0106,0.0082,-0.0566,-0.0891,-0.0438,-0.0264,-0.0097,-0.0274,-0.0207,-0.0302,0.0185,-0.0448,0.0424,0.026,0.0307,0.0022,0.0583,0.0364,0.0535,0.0643,-0.0075,0.0715,-0.1435,0.0189,0.0383,0.0219,-0.0279,-0.0346,-0.0253,-0.0052,0.012,0.0394,0.0095,0.0355,0.0076,0.0188,0.0295,-0.0314,0.0027,-0.0062,0.0088,-0.0263,0.008,-0.0333,-0.0167,-0.0712,0.0306,-0.0626,0.0491,-0.0314,-0.0553,-0.0065,0.0077,0.0202,-0.0361,-0.0107,0.0174,-0.0067,-0.0755,0.216,-0.0564,0.0195,0.0234,-0.0286,0.0306,-0.0397,-0.0287,-0.0311,-0.0314,-0.0001,-0.0023,-0.0442,0.0059,-0.0369,-0.0126,0.0238,0.0646,0.0106,-0.0186,0.0132,-0.0319,-0.0304,0.0432,-0.0261,0.0435,-0.0524,0.0693,0.1001,0.0386,-0.0011,-0.0058,-0.0626,-0.0748,-0.0115,0.0285,0.0637,0.016,0.0176,0.029,0.0056,-0.0205,-0.0416,0.0663,-0.084,-0.043,0.1092,-0.0465,0.0511,-0.0284,-0.0754,0.0142,0.0306,-0.0478,-0.0219,0.0181,0.0266,0.0571,0.0586,-0.0397,0.026,-0.0324,-0.0448,-0.0245,0.1185,0.0056,-0.0535,-0.0684,-0.0012,-0.0088,-0.0046,0.0491,0.0508,-0.024,0.0406,0.0614,-0.0192,-0.06,-0.006,-0.019,0.0097,0.0158,-0.0149,-0.0339,0.0386,0.0577,-0.0359,0.0085,-0.0679,-0.0007,0.0547,-0.0385,0.0112,-0.0908,-0.0208,-0.0386,-0.0453,-0.0029,-0.0071,0.0198,-0.0472,-0.0071,0.0022,-0.0249,0.031,-0.0291,0.0338,0.0139,-0.0118,0.0611,0.0643,-0.0154,0.0029,0.0534,0.0172,-0.0122,-0.0058,0.0167,0.0554,-0.0116,0.0407,0.0235,-0.0499,-0.0114,-0.2275,-0.0266,0.0451,-0.0265,0.0185,-0.0673,0.0326,0.0223,0.0807,0.0591,0.0629,0.0049,-0.0226,-0.0013,0.006,0.0503,0.024,0.019,-0.0137,0.0096,0.0222,0.0477,-0.0082,-0.0954,0.0534,-0.0201,0.237,0.0024,0.059,-0.0163,0.0055,0.0215,0.0019,-0.0683,0.1069,0.014,0.0278,0.0209,-0.0246,-0.0316,-0.01,0.037,0.0181,-0.1279,0.005,-0.0722,-0.0296,0.015,-0.0835,0.0357,0.0269,-0.0534,0.0332,0.021,0.0192,-0.0799,-0.0943,0.0241,-0.0258,-0.0144,0.0212,-0.0354,0.007,-0.0576,0.0459,-0.0061,-0.0576,-0.0798,0.0361,-0.0279,-0.0021,0.0953,-0.0218,0.0292,0.0557,0.0046,0.0166,-0.0187,-0.0384,-0.0321,0.09,0.0023,-0.0172,-0.0048,0.0254,0.0308,0.0806,0.0105,0.0651,0.0015,0.0182,0.0526,-0.0116,-0.0018,0.0335,0.0024,-0.2642,0.0267,0.0048,0.0681,0.011,0.0453,0.0378,0.0383,-0.0424,-0.0103,-0.0266,0.0298,0.0176,-0.0296,0.007,0.0639,0.0879,-0.0554,0.0469,-0.0625,0.0094,0.031,0.1905,-0.0388,0.0012,-0.0534,0.0088,0.0146,0.0179,-0.0608,0.019,0.0045,0.0739,-0.0376,-0.0066,0.1195,-0.031,0.0264,0.0154,-0.0281,0.0151,-0.0332,-0.08,-0.0584,0.0813,-0.043,-0.0371,-0.0436,0.0296,0.0428,-0.0055,-0.0431,-0.0096,0.0464,0.0193,0.0374,-0.046,-0.0543,-0.0458,-0.0477,0.0376,-0.0681,0.0081,0.0257,-0.0168]}
{"key":"[GLM: General Language Model Pretraining with Autoregressive Blank Infilling] There have been various types of pretraining architectures including autoencoding models (e.g., BERT), autoregressive models (e.g., GPT), and encoder-decoder models (e.g., T5). However, none of the pretraining frameworks performs the best for all tasks of three main categories including natural language understanding (NLU), unconditional generation, and conditional generation. We propose a General Language Model (GLM) based on autoregressive blank infilling to address this challenge. GLM improves blank filling pretraining by adding 2D positional encodings and allowing an arbitrary order to predict spans, which results in performance gains over BERT and T5 on NLU tasks. Meanwhile, GLM can be pretrained for different types of tasks by varying the number and lengths of blanks. On a wide range of tasks across NLU, conditional and unconditional generation, GLM outperforms BERT, T5, and GPT given the same model sizes and data, and achieves the best performance from a single pretrained model with 1.25x parameters of BERT Large , demonstrating its generalizability to different downstream tasks.","layer":1,"vector":[-0.0451,-0.0358,0.0276,-0.0179,0.0394,0.0183,-0.0127,-0.0017,0.0125,-0.0477,0.0121,-0.0782,0.0218,0.0809,0.0015,0.0176,-0.0051,0.0406,-0.0455,-0.0186,0.046,-0.0423,0.0147,-0.0247,0.0295,-0.0178,-0.0221,-0.0406,-0.0047,-0.2313,-0.0069,-0.0186,0.0363,-0.0202,-0.0276,-0.0485,-0.0678,0.0366,0.003,-0.006,0.0074,-0.0189,-0.0391,-0.0306,0.0161,-0.0527,-0.0233,-0.014,-0.0391,-0.0407,0.0302,-0.0351,-0.0399,-0.0009,-0.0018,-0.001,0.065,0.0312,0.0592,0.0497,0.0164,0.0227,-0.1828,0.0766,0.0271,0.0565,-0.0728,0.0232,-0.0067,0.0447,-0.0158,0.037,0.0274,0.0499,0.0436,0.008,0.0428,-0.0083,0.0007,0.0104,0.0251,-0.0478,-0.0336,-0.0225,-0.0089,-0.0635,0.0373,-0.0685,0.017,-0.0086,-0.0206,-0.0173,-0.0291,0.0819,-0.0281,0.0178,0.0566,0.0326,-0.0634,0.1967,-0.0491,-0.001,0.042,-0.0355,0.0502,0.0042,-0.0256,-0.01,-0.0108,-0.0447,-0.0252,0.0042,0.0363,-0.0358,0.0726,0.0114,0.1153,-0.0006,0.0032,0.0338,-0.0068,-0.0017,0.0097,-0.0165,0.0204,-0.0622,0.0326,0.1315,0.0539,0.0442,0.0479,0.0067,-0.0693,-0.0214,-0.0001,0.0231,0.0267,0.0011,0.0318,-0.0174,-0.0099,-0.0344,-0.0433,-0.083,-0.0436,0.138,-0.056,0.0172,-0.0486,-0.0254,-0.0076,0.0349,0.0415,-0.0467,0.0665,0.0439,0.0234,0.03,-0.029,0.016,-0.0121,-0.0733,-0.0875,0.0583,0.029,-0.0783,-0.0589,0.0002,0.0158,-0.0351,0.0799,0.0304,-0.0329,0.002,0.0875,0.0491,-0.0637,0.0021,-0.0137,0.0069,0.0378,-0.048,-0.0036,0.0724,0.0077,-0.0705,0.0053,-0.0077,0.0602,-0.0036,-0.0138,0.027,-0.0288,-0.0102,-0.011,-0.0095,-0.0378,-0.0317,-0.0073,-0.0277,0.0092,0.0249,-0.0433,0.0104,0.0229,0.0204,-0.0025,-0.0272,0.0883,0.0431,-0.0095,-0.0131,0.0405,-0.0215,-0.0027,0.0132,-0.0209,0.0032,0.0043,0.0142,-0.0294,-0.0259,-0.0432,-0.2286,0.0045,0.0156,-0.03,0.0445,-0.0457,0.0341,0.0187,0.0772,0.0646,0.0103,-0.0242,-0.0096,0.057,-0.038,0.049,0.0037,0.0403,0.0041,0.0417,0.0341,-0.0051,-0.0611,-0.1073,0.0514,-0.007,0.2246,0.0243,0.0434,-0.0642,0.0122,0.0115,-0.0318,-0.0948,0.0873,0.0446,0.0478,0.0355,0.0056,-0.0247,-0.0049,0.0055,0.0022,-0.1315,-0.055,-0.0577,-0.0561,-0.0282,-0.0564,0.0432,0.0309,-0.0282,0.064,0.02,-0.0132,-0.0238,-0.0919,-0.0263,-0.0592,0.0423,-0.0084,-0.0599,-0.0135,-0.0747,0.0201,0.0103,-0.0355,-0.0353,0.003,-0.0166,0.0073,0.0556,-0.0042,0.0097,0.0597,0.0245,-0.0018,-0.0645,-0.0687,-0.073,0.061,-0.0572,0.0532,0.0165,0.0357,0.0088,0.0991,-0.0499,0.0212,0.0356,-0.0112,0.0181,-0.0187,-0.0255,0.0376,-0.0431,-0.3002,0.0316,0.0277,0.0205,-0.0024,0.0241,0.0068,0.0086,-0.0492,0.0254,-0.0371,0.0161,0.0674,-0.0201,-0.0179,0.038,0.1214,-0.042,0.0316,-0.0398,0.031,0.0439,0.1911,0.0147,0.0482,0.0352,-0.0008,-0.053,0.0582,0.0213,0.0239,0.0031,0.1223,-0.0127,0.0132,0.0444,-0.0691,0.0549,0.0414,-0.0038,-0.0332,0.0153,-0.0132,-0.0521,0.0404,0.0196,-0.003,-0.0466,-0.0161,0.0153,0.0082,0.0329,-0.0051,0.0232,0.0222,0.0453,-0.019,-0.0433,-0.0195,-0.042,0.0105,-0.0771,-0.028,0.0254,-0.0125]}
{"key":"[A Unifying Framework for Information Processing in Stochastically Driven Dynamical Systems] A dynamical system can be regarded as an information processing apparatus that encodes input streams from the external environment to its state and processes them through state transitions. The information processing capacity (IPC) is an excellent tool that comprehensively evaluates these processed inputs, providing details of unknown information processing in black box systems; however, this measure can be applied to only time-invariant systems. This paper extends the applicable range to time-variant systems and further reveals that the IPC is equivalent to coefficients of polynomial chaos (PC) expansion in more general dynamical systems. To achieve this objective, we tackle three issues. First, we establish a connection between the IPC for time-invariant systems and PC expansion, which is a type of polynomial expansion using orthogonal functions of input history as bases. We prove that the IPC corresponds to the squared norm of the coefficient vector of the basis in the PC expansion. Second, we show that an input following an arbitrary distribution can be used for the IPC, removing previous restrictions to specific input distributions. Third, we extend the conventional orthogonal bases to functions of both time and input history and propose the IPC for time-variant systems. To show the significance of our approach, we demonstrate that our measure can reveal information representations in not only machine learning networks but also a real, cultured neural network. Our generalized measure paves the way for unveiling the information processing capabilities of a wide variety of physical dynamics which has been left behind in nature.","layer":5,"vector":[-0.0605,-0.0139,0.073,-0.0428,0.0262,0.0213,0.0734,0.0079,0.0599,-0.0013,-0.0016,-0.0455,0.0402,0.0674,0.022,0.0054,-0.0538,0.0303,-0.0705,-0.0094,0.0562,-0.0346,-0.0073,-0.0037,0.0117,-0.0062,-0.0612,-0.0234,-0.0469,-0.223,0.0332,-0.0373,0.0582,-0.0402,0.0044,-0.0567,-0.0486,0.0607,-0.0159,0.0484,0.0235,0.0524,-0.0141,-0.0478,-0.0427,-0.0791,-0.0121,0.0016,-0.0359,-0.0449,-0.0134,-0.0027,0.0507,-0.0007,0.0543,0.0495,0.0622,0.0463,0.0649,0.0344,0.0223,0.0317,-0.1754,0.0473,0.0324,0.0439,-0.0546,-0.0237,0.0351,0.0198,-0.0055,0.0423,0.0159,0.0271,0.0098,0.0112,-0.0469,-0.0431,0.0038,0.0452,0.035,-0.0419,-0.0199,-0.0466,-0.0406,-0.0651,-0.0119,-0.0035,0.0491,0.0159,-0.0104,-0.0154,-0.0424,-0.0035,-0.0565,0.01,0.0314,0.0323,-0.0252,0.1702,-0.0609,-0.014,0.0655,-0.0339,0.0296,-0.0293,-0.018,-0.0153,-0.0659,0.014,-0.0035,-0.0366,0.026,-0.022,0.0524,0.0265,0.0251,0.0026,-0.0022,-0.0385,-0.0311,-0.0056,0.0366,0.007,0.0184,-0.0265,0.0368,0.1467,0.0206,0.0307,0.0461,0.0018,-0.0548,-0.048,0.0127,0.0238,-0.0029,-0.0029,0.0433,0.0209,-0.0307,-0.0233,0.0323,-0.0781,-0.0813,0.1435,-0.0326,0.0442,-0.0301,0.0004,-0.0141,0.016,-0.0189,-0.0829,-0.0022,0.0609,0.0282,-0.0074,-0.0763,0.0526,-0.0368,-0.0273,-0.0102,0.1146,0.0478,-0.0595,-0.0398,0.0187,0.0404,-0.0658,0.042,-0.0173,-0.0159,0.0074,0.0842,0.0236,-0.0488,-0.0076,-0.0038,0.0098,0.0238,-0.0296,0.0055,0.0129,0.008,-0.0298,0.0058,-0.0076,0.0368,0.006,-0.0408,-0.021,0.003,0.0284,-0.0233,-0.0101,0.0276,-0.0052,0.0091,-0.0786,-0.0108,-0.0227,-0.0571,0.0483,-0.0002,0.0114,-0.0551,0.0377,-0.0286,0.0118,-0.0218,-0.0163,0.0543,-0.0531,-0.0499,0.0409,-0.0041,0.013,0.0238,-0.0113,0.0434,-0.0379,-0.0668,-0.2532,-0.0025,0.0064,-0.0137,0.0753,-0.0742,0.0226,-0.0468,0.0635,0.0435,0.021,0.02,-0.0168,0.0037,0.0051,0.05,0.0455,0.0359,-0.0404,0.0513,-0.0322,0.0672,-0.0227,-0.064,0.0207,-0.0219,0.2033,-0.0373,0.0566,-0.0135,-0.0017,0.0489,-0.0159,-0.0424,0.0732,0.0378,0.0507,-0.0027,-0.0408,-0.0058,-0.0164,0.0034,-0.0177,-0.0789,-0.0419,-0.0206,0.0029,-0.018,-0.0724,-0.0279,0.0235,-0.0007,0.0121,0.0124,0.0286,-0.0448,-0.0493,0.0254,-0.0299,0.0409,-0.0578,-0.0609,-0.0143,-0.0175,0.0258,0.0167,-0.0305,-0.0635,0.0444,-0.0425,-0.0171,0.1061,0.0232,0.0068,0.0858,0.0064,0.0409,-0.025,-0.0156,-0.0055,0.0232,-0.0734,0.0576,0.0623,0.0268,-0.0427,0.0662,-0.0474,0.0042,-0.0096,-0.0121,0.0147,-0.059,0.001,0.008,-0.0191,-0.3048,0.0502,0.0394,0.043,-0.0122,0.0098,0.0316,0.0296,-0.0685,-0.0099,0.018,0.0396,0.02,-0.001,0.0156,0.0627,0.0235,-0.0746,0.0553,-0.078,0.0255,0.0727,0.2376,-0.0131,0.0105,-0.0109,0.0028,0.0393,-0.0216,-0.0295,0.0692,0.0312,0.0734,-0.0473,0.0403,0.0361,-0.0293,0.0535,-0.0241,0.0039,0.0362,-0.0099,-0.0282,-0.0188,0.1135,0.0015,-0.0445,-0.0779,-0.006,0.0774,-0.0129,0.0436,-0.017,-0.0171,0.0586,0.0648,-0.0408,-0.0666,0.0193,-0.0455,-0.0041,-0.0782,0.0159,0.001,-0.057]}
{"key":"[Deep Reinforcement Learning for Adaptive Network Slicing in 5G for Intelligent Vehicular Systems and Smart Cities] Intelligent vehicular systems and smart city applications are the fastest growing Internet of things (IoT) implementations at a compound annual growth rate of 30%. In view of the recent advances in IoT devices and the emerging new breed of IoT applications driven by artificial intelligence (AI), fog radio access network (F-RAN) has been recently introduced for the fifth generation (5G) wireless communications to overcome the latency limitations of cloud-RAN (C-RAN). We consider the network slicing problem of allocating the limited resources at the network edge (fog nodes) to vehicular and smart city users with heterogeneous latency and computing demands in dynamic environments. We develop a network slicing model based on a cluster of fog nodes (FNs) coordinated with an edge controller (EC) to efficiently utilize the limited resources at the network edge. For each service request in a cluster, the EC decides which FN to execute the task, i.e., locally serve the request at the edge, or to reject the task and refer it to the cloud. We formulate the problem as infinite-horizon Markov decision process (MDP) and propose a deep reinforcement learning (DRL) solution to adaptively learn the optimal slicing policy. The performance of the proposed DRL-based slicing method is evaluated by comparing it with other slicing approaches in dynamic environments and for different scenarios of design objectives. Comprehensive simulation results corroborate that the proposed DRL-based EC quickly learns the optimal policy through interaction with the environment, which enables adaptive and automated network slicing for efficient resource allocation in dynamic vehicular and smart city environments.","layer":1,"vector":[-0.0621,0.0005,0.0662,-0.0221,0.007,0.0722,0.0226,0.0659,0.0331,-0.0114,0.0346,0.0008,0.0037,0.0393,0.0279,0.0201,-0.0245,0.0172,-0.0365,-0.0387,0.0269,-0.088,-0.0334,-0.0429,0.0454,0.0241,-0.0154,-0.0463,-0.0394,-0.224,0.0143,-0.0532,0.0573,-0.0001,-0.0182,-0.062,-0.0031,0.0504,-0.0103,0.0413,0.061,-0.0071,-0.0343,-0.0573,-0.0448,-0.0601,-0.0327,-0.0013,-0.0092,-0.0626,0.0217,0.0007,-0.0124,0.0398,0.0405,0.0591,0.0498,0.0926,0.0483,0.0303,0.0006,0.0277,-0.181,0.0817,0.0609,0.0087,-0.0394,-0.0098,0.0196,0.0347,-0.0558,0.0566,0.0032,0.0449,0.0102,0.0662,-0.0169,-0.0206,0.0085,-0.0,0.0044,-0.0296,-0.0441,-0.0009,-0.0148,-0.058,-0.0221,-0.0478,0.0194,-0.0018,-0.0439,0.0029,-0.026,0.037,-0.0656,0.0163,-0.0029,-0.0179,-0.0891,0.2076,-0.0419,0.0643,0.0121,-0.0047,0.0474,-0.0139,-0.0421,-0.0379,-0.0636,0.0537,-0.0069,-0.0225,0.0461,-0.0266,0.0102,0.0314,0.0492,0.0733,0.0119,-0.0123,-0.0252,0.0131,0.07,-0.0089,0.0393,-0.0868,0.0075,0.1486,-0.0152,0.0308,0.0569,-0.053,-0.0388,-0.0216,0.0364,0.0102,-0.0049,-0.0211,-0.0203,0.0252,0.0103,-0.0253,0.0226,-0.0989,-0.0129,0.0837,-0.0006,0.0251,-0.0197,-0.0526,-0.0347,0.0391,-0.0218,-0.0097,0.0165,0.0343,-0.0012,0.0749,-0.0264,0.0124,-0.0262,-0.0232,-0.0713,0.1387,-0.0329,-0.1095,-0.0081,0.0238,-0.0166,-0.0191,0.0217,0.0416,-0.06,0.0457,0.0638,0.0398,-0.1084,0.034,-0.0215,-0.0181,-0.018,-0.0164,-0.0025,0.0006,0.0325,-0.0622,-0.0008,-0.0348,0.0235,-0.0071,-0.0342,0.0248,-0.0204,-0.0154,-0.0345,-0.0316,0.0021,0.0056,0.0102,-0.0147,-0.0005,-0.0117,-0.0355,0.0006,-0.0315,-0.0113,-0.0238,-0.0097,0.0338,0.0177,0.0027,-0.0118,0.046,-0.0074,0.0033,-0.0002,0.0256,0.0591,0.0046,0.0068,0.0096,-0.0174,-0.0504,-0.1962,-0.0159,-0.0416,-0.0164,0.0558,-0.0277,0.0215,-0.0346,0.0449,0.0141,0.0794,-0.0471,-0.0013,0.0221,0.0204,0.0418,0.0328,0.0447,-0.0059,0.0078,0.0244,-0.0056,-0.0689,-0.0929,0.047,0.0189,0.2125,-0.0182,0.0545,0.0105,0.0558,0.0879,-0.0343,-0.0839,0.0325,0.0021,0.116,0.0079,-0.0671,-0.0499,-0.0168,-0.0196,-0.0496,-0.1123,-0.0135,-0.0106,-0.0436,0.0419,-0.0356,-0.0458,0.0294,-0.0198,0.0346,-0.0159,-0.0067,-0.0079,-0.0871,0.0414,-0.0248,-0.0248,-0.015,-0.0312,0.0048,-0.0464,0.0209,0.0154,-0.0319,-0.03,-0.0346,-0.006,0.0041,0.0584,-0.0058,0.008,0.0418,0.0113,0.0333,-0.0044,-0.0162,-0.0128,0.1014,-0.0482,0.0423,0.06,0.0118,-0.0025,0.0497,0.0236,0.0232,-0.0086,0.0144,-0.0115,-0.0572,-0.0699,0.0818,-0.0226,-0.3011,0.0786,0.0162,0.0114,-0.008,0.0336,0.006,0.0312,-0.063,-0.0009,-0.0049,0.0852,0.0625,-0.0107,0.0416,0.0212,0.0669,-0.0273,0.0177,-0.0484,-0.0003,0.0311,0.211,-0.0528,0.0762,0.0704,-0.0572,0.0043,0.0158,-0.0257,-0.0153,0.0183,0.0602,-0.0717,0.0517,0.0466,-0.0397,0.0367,0.0351,0.0563,-0.0133,0.0191,0.0106,0.0249,0.0629,-0.0152,-0.035,-0.0609,0.0088,0.069,-0.0115,-0.0286,-0.0165,-0.0395,0.0636,0.0085,-0.0758,-0.0654,-0.0364,-0.0264,0.0272,-0.0954,-0.0111,-0.0119,0.0287]}
{"key":"[A Two-Step Learning Method For Detecting Landmarks on Faces From Different Domains] The detection of fiducial points on faces has significantly been favored by the rapid progress in the field of machine learning, in particular in the convolution networks. However, the accuracy of most of the detectors strongly depends on an enormous amount of annotated data. In this work, we present a domain adaptation approach based on a two-step learning to detect fiducial points on human and animal faces. We evaluate our method on three different datasets composed of different animal faces (cats, dogs, and horses). The experiments show that our method performs better than state of the art and can use few annotated data to leverage the detection of landmarks reducing the demand for large volume of annotated data.","layer":4,"vector":[-0.0429,-0.0268,0.0697,0.0001,0.0074,0.0406,0.0598,0.028,0.0184,-0.0092,0.0242,-0.0947,-0.0003,0.0541,0.0098,0.0151,-0.0065,0.1004,-0.0341,0.0373,0.031,-0.0287,0.0041,-0.0421,0.0082,0.048,0.004,-0.0223,-0.0276,-0.2053,0.0413,-0.0308,0.0339,-0.0143,-0.0154,-0.0318,-0.0408,0.0228,-0.0021,0.0422,-0.0,0.0073,-0.0446,-0.0673,-0.0218,-0.0519,-0.0229,0.0102,0.0114,-0.0547,0.0498,-0.0733,0.0569,0.0299,0.0263,0.0555,0.0655,0.0579,0.0485,0.0402,-0.0055,0.0559,-0.1625,0.0716,0.0257,0.038,-0.0161,-0.0229,-0.0297,0.0603,-0.0121,0.0355,0.0025,-0.0154,-0.0167,-0.0015,-0.0108,0.0138,-0.0129,0.0138,0.0114,0.008,-0.043,-0.0065,-0.0131,-0.052,0.0198,-0.0562,0.0026,0.0137,-0.0252,-0.0092,-0.0245,0.0093,-0.0639,-0.0137,0.0294,0.0181,-0.0184,0.1915,-0.0154,0.0489,0.0111,-0.0283,0.0652,0.007,-0.0366,-0.0154,0.038,0.0065,-0.0174,-0.0223,0.0027,-0.0275,0.0034,0.0121,0.053,0.0488,-0.0048,-0.0088,0.016,0.0148,0.0559,-0.0621,0.0467,-0.0153,0.0574,0.1005,0.0454,0.0208,0.0346,-0.0073,-0.0599,-0.0224,-0.0241,0.0655,0.0003,0.0045,0.0224,-0.0079,-0.0409,-0.0938,0.0512,-0.0738,-0.0143,0.1235,-0.0139,0.0132,-0.047,-0.0039,-0.0105,0.0524,-0.0603,0.0239,0.0014,0.0093,0.0245,0.0142,-0.029,0.0077,-0.0122,-0.0518,-0.0379,0.0635,0.0248,-0.1349,-0.0349,-0.0358,0.0043,0.0164,0.0436,0.045,-0.0345,0.0503,0.0497,0.0493,-0.08,-0.0324,0.0406,0.0348,0.0232,-0.0952,-0.0572,0.0482,0.0411,-0.0501,0.0018,-0.0335,0.0113,0.047,-0.0145,0.0407,-0.0482,-0.0495,-0.0474,-0.0241,-0.0,0.0316,0.0277,-0.0376,-0.0222,-0.0116,-0.0364,0.0021,-0.0,0.0004,-0.0004,0.007,0.0525,0.001,-0.0563,0.0268,0.0235,-0.0523,-0.0252,0.0259,0.019,0.0223,-0.0144,0.0226,0.044,-0.039,-0.033,-0.2594,0.0107,-0.0087,-0.0361,0.0077,-0.0644,0.0638,0.0501,0.1037,0.049,0.0646,-0.0668,-0.0172,0.0241,-0.0271,0.0642,0.0279,0.0186,-0.0283,-0.0608,-0.0125,0.0118,0.0294,-0.0922,0.0568,0.0055,0.2308,0.0475,0.0306,-0.0354,-0.0056,0.0156,-0.0477,-0.1214,0.0521,0.0189,0.0541,-0.0415,-0.0543,-0.0246,-0.0225,0.039,0.0629,-0.0781,-0.0376,-0.0083,-0.0285,0.0051,-0.0427,-0.0093,0.0647,-0.0482,0.0336,-0.0008,-0.0726,-0.0504,-0.074,0.011,-0.0668,0.0652,-0.0077,-0.0597,0.0325,-0.0589,0.0575,0.0159,-0.0703,-0.0288,0.0371,0.0087,-0.0266,0.066,0.0343,-0.0104,0.012,-0.0045,0.0809,-0.0261,-0.0447,-0.0307,0.0445,-0.042,0.0238,0.0068,0.046,0.0194,0.0802,-0.0229,0.0483,-0.0597,0.0032,-0.0017,-0.0318,-0.0457,0.0237,0.0522,-0.2901,-0.0235,0.0391,0.0473,-0.0334,0.0232,0.0646,0.0136,0.0002,-0.035,-0.0437,0.017,0.0288,-0.0327,-0.0038,0.0405,0.0452,0.0033,0.0334,-0.0493,-0.0153,0.0167,0.2364,-0.0973,0.0416,-0.0167,-0.0225,-0.0494,0.0307,0.0035,0.0385,-0.0162,0.0553,-0.0347,-0.0026,0.1096,-0.0512,-0.005,0.0008,-0.0125,0.0177,0.0472,-0.0263,-0.0327,0.0965,0.0054,-0.0053,0.0168,0.0648,0.0511,-0.0319,-0.0338,-0.0307,-0.0093,0.0197,-0.023,-0.0301,-0.0122,-0.0666,-0.0271,0.0336,-0.0575,-0.0376,0.0042,-0.0073]}
{"key":"[A Theory of Diagnostic Interpretation in Supervised Classification] Interpretable deep learning is a fundamental building block towards safer AI, especially when the deployment possibilities of deep learning-based computer-aided medical diagnostic systems are so eminent. However, without a computational formulation of black-box interpretation, general interpretability research rely heavily on subjective bias. Clear decision structure of the medical diagnostics lets us approximate the decision process of a radiologist as a model - removed from subjective bias. We define the process of interpretation as a finite communication between a known model and a black-box model to optimally map the black box's decision process in the known model. Consequently, we define interpretability as maximal information gain over the initial uncertainty about the black-box's decision within finite communication. We relax this definition based on the observation that diagnostic interpretation is typically achieved by a process of minimal querying. We derive an algorithm to calculate diagnostic interpretability. The usual question of accuracy-interpretability tradeoff, i.e. whether a black-box model's prediction accuracy is dependent on its ability to be interpreted by a known source model, does not arise in this theory. With multiple example simulation experiments of various complexity levels, we demonstrate the working of such a theoretical model in synthetic supervised classification scenarios.","layer":1,"vector":[-0.0122,0.0076,0.0133,0.0112,0.0212,-0.0259,0.0652,0.0326,0.0313,-0.0052,0.0232,-0.0681,0.0291,0.0704,-0.0402,0.023,-0.0213,0.0331,-0.021,0.0209,0.032,-0.0012,-0.0405,-0.0568,0.0198,0.0377,-0.0262,0.0015,-0.0572,-0.2134,0.0209,-0.0618,0.0564,-0.0207,0.0263,-0.0466,-0.0077,0.0239,-0.0422,0.0318,0.0512,-0.0267,-0.0237,-0.0418,-0.0237,-0.0341,0.0013,0.0052,-0.0172,-0.0352,0.0025,-0.0161,0.0433,0.0402,0.0109,0.0288,0.0845,0.0274,0.0217,-0.0104,0.0298,0.0967,-0.1445,0.052,0.0531,0.0211,-0.061,-0.0553,0.0235,0.0502,0.0476,0.0205,0.008,0.0558,0.0192,-0.0426,0.0086,0.0135,0.0192,0.0194,-0.0153,-0.0139,-0.0273,0.0085,-0.0216,-0.0729,-0.0147,-0.0567,0.0166,0.0296,-0.0414,-0.0192,-0.0644,0.0269,-0.0606,-0.0135,0.0432,-0.0045,-0.0562,0.1533,-0.0601,-0.0114,0.0284,-0.0257,-0.0034,-0.022,-0.055,-0.0276,-0.0196,-0.0015,-0.0242,-0.0314,0.0333,-0.0213,-0.0025,0.0651,0.0776,0.0432,0.0244,-0.0187,-0.03,-0.0016,0.0807,-0.0031,0.0158,-0.0366,0.0341,0.1808,-0.0028,0.0017,0.0416,-0.0532,-0.0513,-0.0191,0.0246,-0.0146,0.044,0.008,0.0016,0.0369,-0.0521,-0.0587,-0.0107,-0.0763,-0.0803,0.0981,-0.0812,0.017,-0.0673,-0.05,-0.0288,-0.016,-0.0369,-0.0278,0.0277,0.0239,0.0263,0.0044,-0.0801,0.0223,0.0385,-0.1021,-0.0397,0.1166,-0.0063,-0.0301,-0.0287,-0.0051,0.0673,-0.0499,0.0589,0.0369,-0.0143,-0.0063,0.0466,0.0355,-0.0667,-0.0257,-0.0143,-0.0058,0.0322,-0.038,-0.0381,0.0675,0.0061,-0.0392,0.024,-0.0403,0.0839,0.0425,-0.0083,0.012,-0.0379,0.0137,-0.0161,-0.0319,0.0033,-0.0193,-0.0146,-0.0075,-0.0121,-0.0036,-0.0277,0.0498,-0.0172,0.0296,-0.024,0.0041,0.0257,0.024,-0.0346,0.0135,0.0468,-0.0302,-0.0161,0.0133,0.0069,0.0454,-0.0038,0.0336,0.038,-0.0342,-0.0291,-0.2417,-0.0174,0.0353,-0.0075,0.0241,-0.0809,-0.0023,0.0126,-0.0172,0.0786,0.0594,0.0028,-0.0157,-0.0088,-0.0142,0.0299,0.0087,0.0366,-0.0478,0.0019,-0.0104,0.027,-0.0055,-0.1344,-0.0054,0.0235,0.234,0.0141,0.0681,0.0055,0.018,0.0033,-0.0061,-0.1097,0.0442,0.033,0.038,0.0116,-0.0299,0.011,-0.0158,0.0024,-0.0106,-0.0817,-0.0375,-0.0291,-0.0141,0.0421,-0.0777,0.0409,0.0194,-0.0398,0.0299,0.0159,0.0268,-0.0068,-0.0921,-0.0243,-0.0321,0.0114,-0.0128,-0.0377,-0.013,-0.076,0.0514,-0.0108,-0.0316,-0.0381,0.045,-0.0333,0.0158,0.0773,-0.0374,-0.0085,0.0716,0.0383,0.0732,-0.0395,-0.0441,0.0016,0.0625,-0.0081,0.0501,0.034,-0.0016,0.0266,0.0797,-0.0107,-0.0075,-0.0135,-0.0254,0.0485,-0.0548,-0.0525,0.0198,-0.0306,-0.3234,0.0537,-0.0168,0.036,-0.0661,0.0204,0.043,0.0188,-0.0397,-0.0016,0.0065,0.0111,0.0479,-0.0284,0.0191,0.04,0.0932,-0.0641,0.067,-0.0097,0.0377,0.0453,0.2298,-0.0597,0.0198,0.0206,-0.03,-0.0411,0.0146,0.0169,0.0328,-0.0278,0.0771,-0.0485,0.0389,0.0886,-0.0047,0.0519,0.0329,-0.0004,0.0324,0.0056,-0.042,-0.013,0.118,0.0218,-0.064,-0.0112,-0.0352,0.0311,-0.0377,0.0169,-0.0101,0.0077,0.047,0.0077,-0.0021,-0.0561,0.0151,-0.036,0.0296,-0.0876,0.0233,0.0479,-0.014]}
{"key":"[Physics-Aware Downsampling with Deep Learning for Scalable Flood Modeling] Background: Floods are the most common natural disaster in the world, affecting the lives of hundreds of millions. Flood forecasting is therefore a vitally important endeavor, typically achieved using physical water flow simulations, which rely on accurate terrain elevation maps. However, such simulations, based on solving partial differential equations, are computationally prohibitive on a large scale. This scalability issue is commonly alleviated using a coarse grid representation of the elevation map, though this representation may distort crucial terrain details, leading to significant inaccuracies in the simulation. Contributions: We train a deep neural network to perform physics-informed downsampling of the terrain map: we optimize the coarse grid representation of the terrain maps, so that the flood prediction will match the fine grid solution. For the learning process to succeed, we configure a dataset specifically for this task. We demonstrate that with this method, it is possible to achieve a significant reduction in computational cost, while maintaining an accurate solution. A reference implementation accompanies the paper as well as documentation and code for dataset reproduction.","layer":2,"vector":[-0.027,-0.0374,0.0349,-0.0215,0.0039,0.0116,-0.0164,-0.0018,0.0207,-0.0074,-0.0097,-0.107,0.0382,0.0257,-0.0347,-0.0035,0.0198,0.0451,-0.0109,0.0404,0.042,0.0092,-0.011,-0.0133,0.0256,-0.0005,-0.0215,-0.0249,-0.068,-0.2581,0.0117,-0.0534,-0.0138,-0.0391,0.0155,-0.0251,-0.0319,0.0386,0.0245,0.0257,0.013,-0.005,-0.0285,-0.0579,0.0147,-0.0588,-0.0142,-0.0281,0.0016,-0.0068,0.0716,-0.0373,0.0137,0.0451,0.0417,0.0405,0.0334,0.0471,0.0408,0.0367,0.0364,0.0193,-0.1669,0.0751,0.0297,0.0431,-0.0285,0.0198,0.032,0.0185,-0.0195,0.0346,0.05,0.0305,0.029,-0.0235,-0.0269,-0.0027,0.003,-0.0154,0.0343,-0.0056,-0.026,-0.0214,0.021,-0.0682,0.0459,-0.0165,0.0246,-0.0133,-0.0362,-0.0481,-0.0227,0.0456,-0.0766,0.021,0.0435,0.0032,-0.0298,0.2405,-0.0754,0.0471,0.0229,-0.0037,0.0188,-0.0614,-0.042,-0.0222,-0.0043,-0.0003,-0.0361,-0.0405,0.0097,-0.0331,0.0619,-0.0186,0.0424,0.0179,-0.0126,-0.0111,-0.0424,0.0248,0.0446,0.0118,0.0442,-0.0781,0.0025,0.1251,0.0211,0.0073,0.0503,-0.0313,-0.0707,-0.0553,0.0381,0.0065,0.0133,-0.0064,-0.0331,0.025,-0.0382,0.0028,-0.0039,-0.1093,-0.0545,0.1001,-0.1034,0.0409,-0.0104,-0.0536,-0.0542,0.0418,-0.0302,-0.0482,0.0933,0.0327,0.0033,0.0568,-0.0665,-0.0006,0.0041,-0.0089,-0.015,0.092,-0.0145,-0.0522,-0.0294,0.0081,-0.0097,-0.0216,0.0005,-0.0068,0.0195,-0.008,0.1016,-0.0147,-0.081,-0.0069,0.0086,0.0252,-0.0035,-0.0274,-0.0297,0.0482,0.0295,-0.0161,-0.0122,-0.0277,0.0299,0.0313,-0.0191,0.0093,0.0312,-0.0233,-0.0102,0.0186,-0.0167,0.0081,0.017,-0.0441,-0.0224,0.0169,-0.0584,-0.0039,-0.0209,0.0223,0.0272,0.0037,-0.0067,0.0437,-0.0078,-0.0266,0.0675,-0.0499,-0.0336,0.0312,0.0213,0.0258,-0.0298,0.0603,0.0684,-0.0941,-0.0346,-0.2069,-0.0189,0.0287,-0.0559,0.0346,-0.0433,0.0255,-0.0314,0.0321,0.0703,0.0758,-0.0198,-0.0183,-0.0017,-0.0099,0.0321,0.0284,0.0237,-0.0259,0.0253,0.0172,0.078,-0.0263,-0.0805,0.0212,0.0185,0.2182,0.0454,0.0607,-0.026,0.0175,0.0144,0.0173,-0.1029,0.0444,0.0465,0.0913,0.036,-0.0432,-0.062,-0.0357,0.0247,0.0309,-0.0657,-0.0437,0.0067,-0.0302,0.0604,-0.0423,0.0051,0.0313,-0.0697,0.0716,-0.0161,0.0212,-0.054,-0.123,0.0693,-0.0362,-0.0168,0.0191,-0.0632,0.0284,-0.0508,0.0082,-0.0108,-0.0386,-0.0528,0.0091,-0.0267,-0.0403,0.0583,-0.0321,0.0058,0.0569,0.0072,0.0161,-0.0074,-0.0044,-0.0155,0.0428,-0.0007,0.0385,0.0334,0.0291,0.0354,0.0292,-0.0138,-0.0209,-0.0315,0.0035,0.0206,-0.0524,-0.0267,0.0145,-0.0067,-0.294,0.0256,0.0124,0.0163,-0.0311,-0.0262,0.062,0.0471,-0.0155,-0.0023,-0.0284,0.0466,0.0233,-0.0043,0.0304,0.0118,0.0563,-0.0053,0.0649,-0.0476,0.0233,0.0506,0.2315,-0.0435,0.0348,0.0543,-0.0397,-0.0059,0.061,-0.0361,-0.0069,0.0354,0.1005,-0.0625,0.0507,0.1164,0.0026,0.0175,0.0566,-0.0144,0.0072,0.0414,-0.0137,-0.0092,0.0586,-0.0037,-0.0287,-0.0339,0.0347,0.0343,-0.0593,-0.0202,-0.0058,0.0015,0.0315,0.0568,-0.0761,-0.0461,-0.078,-0.0335,0.0102,-0.0925,-0.0014,-0.0473,-0.0161]}
{"key":"[Second-Order Stochastic Optimization for Machine Learning in Linear Time] First-order stochastic methods are the state-of-the-art in large-scale machine learning optimization owing to efficient per-iteration complexity. Second-order methods, while able to provide faster convergence, have been much less explored due to the high cost of computing the second-order information. In this paper we develop second-order stochastic methods for optimization problems in machine learning that match the per-iteration cost of gradient based methods, and in certain settings improve upon the overall running time over popular first-order methods. Furthermore, our algorithm has the desirable property of being implementable in time linear in the sparsity of the input data.","layer":0,"vector":[-0.0209,-0.0014,0.0035,-0.0057,0.0149,0.0387,-0.0119,0.018,0.0984,-0.02,0.0381,-0.031,0.021,0.081,0.0091,0.0297,0.0212,0.0252,-0.0436,-0.0077,0.0299,-0.0268,-0.0114,-0.0815,0.0234,0.007,-0.0555,-0.0095,-0.0469,-0.253,0.0271,-0.0541,0.0503,-0.0292,-0.0045,-0.0297,-0.0063,0.0985,-0.0648,0.0468,0.0105,0.0149,-0.0725,-0.0881,0.0095,-0.0382,-0.0135,-0.0357,-0.0367,-0.0374,0.0007,-0.0064,0.0484,-0.0215,0.0419,0.0278,0.0542,0.0362,0.0357,0.0308,-0.0065,0.0518,-0.1774,0.0537,0.0294,0.0234,-0.0333,-0.0158,0.008,0.099,-0.0119,0.0399,0.0274,0.0479,0.0078,0.0146,0.0038,-0.0264,-0.0075,0.0338,0.0224,-0.0154,-0.0259,0.0111,-0.0231,-0.0309,0.0113,-0.0337,0.0594,0.0276,0.0308,-0.033,-0.015,0.0098,-0.0703,0.028,0.0448,0.0539,-0.0088,0.1614,-0.0269,0.034,0.0054,-0.0096,0.0395,-0.0769,-0.0457,-0.0729,-0.0023,-0.0139,-0.0138,-0.0187,0.0632,-0.022,0.0097,0.0287,0.0242,0.0123,-0.0257,-0.0031,-0.0067,-0.0104,0.0715,0.0077,0.044,-0.0853,0.0166,0.1313,0.0137,0.0492,0.0457,-0.0486,-0.0565,-0.0429,0.0196,0.0111,0.0085,-0.0082,0.0464,-0.0104,-0.0134,-0.0622,0.0023,-0.0803,-0.0579,0.1378,-0.0409,0.0244,-0.0596,-0.101,0.0012,-0.0252,-0.0085,-0.0438,0.0433,0.0314,0.0261,0.0183,-0.037,-0.0091,-0.0391,-0.0536,-0.0035,0.079,-0.0153,-0.0982,-0.0321,-0.0217,0.005,-0.0334,0.0576,0.0724,-0.0333,0.035,0.081,0.0637,-0.0806,-0.0044,-0.0185,0.0255,0.0055,-0.02,-0.022,0.0528,0.0402,-0.035,0.0273,-0.0364,0.0079,0.0284,-0.0541,0.0201,-0.0049,-0.0072,-0.0573,-0.0666,-0.0151,0.0063,0.022,0.0006,-0.0025,0.0023,-0.071,0.0345,0.0071,0.0107,0.0029,-0.0153,0.0411,0.0189,-0.0074,-0.0198,0.0577,-0.0503,-0.0137,-0.015,0.006,0.0485,0.0128,0.0584,0.0303,-0.0138,-0.0749,-0.2059,-0.0093,0.0037,0.0128,0.0244,-0.0662,-0.0056,-0.0469,0.072,0.0667,0.0601,-0.0145,-0.0169,0.0243,0.0079,0.0348,0.0565,-0.0176,0.0037,-0.0325,0.0465,0.0311,0.0117,-0.0831,0.0753,0.0277,0.1823,-0.0126,0.0293,-0.0314,0.0581,0.0157,0.0167,-0.063,0.0471,0.0077,0.0959,-0.0369,-0.029,-0.0074,-0.0056,0.0437,0.011,-0.0972,-0.0707,-0.0325,-0.0202,0.0275,-0.049,-0.0355,0.0242,-0.0171,0.0573,-0.0644,0.014,-0.0351,-0.0936,0.0004,-0.0353,0.0113,-0.0077,-0.0789,0.0162,-0.0729,0.0138,-0.0334,0.0072,-0.0245,-0.0084,-0.0307,0.0078,0.0758,0.0117,0.0127,0.0761,0.0091,0.0085,0.0008,-0.0581,-0.0092,0.0546,-0.0374,0.0642,0.0451,0.0546,0.0027,0.0892,-0.0234,-0.0271,0.0293,-0.0484,-0.0137,-0.0678,0.0603,0.0462,0.0068,-0.2977,0.0441,0.0133,0.0152,-0.022,0.0085,0.0667,-0.0084,-0.0356,0.0197,-0.0217,0.0911,0.0746,0.033,0.0313,0.0163,0.0475,-0.047,0.0712,-0.0975,0.0143,0.0281,0.2303,-0.0575,-0.0025,-0.0025,-0.0093,0.0141,0.0422,-0.0359,0.0222,-0.0336,0.0614,-0.0738,0.0258,0.054,-0.0704,0.0461,0.0162,-0.0355,0.019,0.0106,-0.0686,-0.0167,0.096,-0.0065,-0.0112,-0.0384,0.008,0.0495,-0.0619,0.0141,-0.0104,0.0025,0.016,0.0155,-0.0536,-0.0504,-0.0,-0.0345,0.0279,-0.0621,-0.038,0.0019,-0.0094]}
{"key":"[Hybrid Handcrafted and Learnable Audio Representation for Analysis of Speech Under Cognitive and Physical Load] As a neurophysiological response to threat or adverse conditions, stress can affect cognition, emotion and behaviour with potentially detrimental effects on health in the case of sustained exposure. Since the affective content of speech is inherently modulated by an individual's physical and mental state, a substantial body of research has been devoted to the study of paralinguistic correlates of stress-inducing task load. Historically, voice stress analysis (VSA) has been conducted using conventional digital signal processing (DSP) techniques. Despite the development of modern methods based on deep neural networks (DNNs), accurately detecting stress in speech remains difficult due to the wide variety of stressors and considerable variability in the individual stress perception. To that end, we introduce a set of five datasets for task load detection in speech. The voice recordings were collected as either cognitive or physical stress was induced in the cohort of volunteers, with a cumulative number of more than a hundred speakers. We used the datasets to design and evaluate a novel self-supervised audio representation that leverages the effectiveness of handcrafted features (DSP-based) and the complexity of data-driven DNN representations. Notably, the proposed approach outperformed both extensive handcrafted feature sets and novel DNN-based audio representation learning approaches.","layer":0,"vector":[-0.0301,-0.0264,0.064,-0.0049,0.0141,0.0542,0.0739,0.0003,0.0193,-0.0813,-0.0039,-0.0448,0.0553,0.0587,0.054,-0.0067,0.0729,0.0548,-0.0627,0.0174,0.0209,0.0131,0.0218,-0.0313,-0.0056,-0.0097,-0.055,0.0175,-0.0296,-0.2332,0.0473,-0.052,0.07,-0.0648,-0.0117,-0.0308,-0.0455,0.0609,-0.0398,0.0428,0.0094,0.0012,-0.0279,-0.0937,-0.0479,-0.0787,-0.0467,-0.0515,0.0252,-0.0248,-0.012,-0.0393,0.0571,0.0187,0.0083,-0.0035,0.0929,0.0418,0.0448,0.034,0.0067,0.0444,-0.1874,0.066,0.0171,0.0657,-0.0215,-0.0078,-0.001,0.0148,-0.0516,0.0389,0.0317,0.0243,0.0176,0.0167,0.0321,-0.0478,0.0278,0.0018,0.045,0.0162,-0.0263,0.0048,-0.0096,-0.0424,-0.0085,-0.0503,0.0251,0.0005,-0.1103,0.0039,0.0054,0.0684,-0.051,-0.0386,0.0406,0.016,-0.0772,0.236,-0.0486,0.0191,0.024,-0.0374,0.0291,-0.0208,-0.0128,-0.0531,-0.0594,0.0054,-0.006,0.0039,0.0063,-0.0429,0.0617,0.0022,0.0607,0.0267,0.0197,0.0211,0.006,-0.0215,0.0164,-0.0288,0.0773,-0.067,0.0519,0.1273,0.054,-0.0133,0.0869,0.019,-0.0347,0.024,0.0093,0.0202,0.0062,0.0018,0.0108,0.0103,-0.0292,-0.0355,-0.0326,-0.0671,-0.066,0.1264,-0.0586,-0.0015,-0.0331,-0.0137,-0.0208,0.0185,-0.0284,-0.0458,0.0192,0.0194,0.0548,0.0494,-0.0742,-0.0341,0.0368,-0.0528,-0.0098,0.0866,0.0429,-0.0656,-0.05,-0.0225,0.0032,-0.0556,0.0256,-0.0133,-0.0261,0.0169,0.0209,0.0404,-0.0611,-0.0104,-0.0178,0.0535,0.0284,-0.0513,-0.0368,-0.0141,0.0061,-0.0441,-0.0017,-0.0662,0.0123,0.0335,-0.0215,-0.0158,-0.0342,0.019,-0.0258,0.0045,-0.006,0.0059,-0.0046,0.0062,-0.0213,0.0077,-0.0167,0.0661,0.0228,0.0226,-0.0193,0.03,0.0526,0.0215,0.0055,-0.003,0.1151,-0.0081,-0.0528,-0.0264,0.0269,0.0349,-0.0015,0.0379,0.0095,-0.0887,-0.0617,-0.2547,-0.0307,0.0451,-0.027,0.0451,-0.0187,0.0591,-0.003,0.073,0.0718,0.0669,-0.0123,-0.0313,0.0132,0.0048,0.0426,0.0116,-0.0028,-0.0376,0.0137,0.0123,0.0043,-0.0264,-0.0777,0.0294,0.005,0.2175,-0.0117,0.0161,0.0148,-0.0236,-0.0143,-0.0233,-0.1522,0.0641,0.0179,0.0689,0.0044,-0.0496,-0.0145,-0.0646,0.0091,-0.0078,-0.0852,-0.0326,-0.0192,-0.0021,-0.0032,-0.0971,-0.0138,0.0505,-0.0227,0.0535,-0.0128,-0.01,-0.0441,-0.105,0.0009,-0.0637,0.0124,-0.0012,-0.0191,0.0673,-0.0488,0.0049,-0.0084,-0.0356,-0.0189,-0.0049,-0.0154,-0.0017,0.1102,-0.0002,0.0112,0.0554,-0.0175,0.0215,-0.0454,-0.0267,-0.0139,0.0537,-0.0056,0.0492,0.0269,0.0176,-0.0036,0.08,0.0338,0.0575,-0.0221,-0.0229,0.032,-0.0207,-0.028,0.0648,0.0354,-0.2639,0.0474,0.0163,0.0284,-0.0316,-0.0052,-0.0262,0.0496,-0.0795,-0.0144,-0.0215,0.0518,0.0197,-0.0157,-0.0053,0.01,0.085,0.0031,0.0579,-0.0154,-0.0042,0.0758,0.1677,0.012,0.0344,-0.0021,-0.0127,0.0221,0.0123,-0.0452,0.0079,-0.0207,0.0874,-0.0706,-0.0365,0.0243,-0.0702,0.0248,0.0223,-0.0116,0.0271,-0.0045,-0.0013,-0.0537,0.0701,-0.0103,-0.0053,-0.0557,0.0032,0.0108,0.0181,0.0091,0.024,-0.001,0.0342,0.0467,-0.0283,-0.0134,0.0131,-0.0427,0.0225,0.0004,-0.0404,-0.0131,0.0019]}
{"key":"[Neural network models and deep learning - a primer for biologists] Originally inspired by neurobiology, deep neural network models have become a powerful tool of machine learning and artificial intelligence, where they are used to approximate functions and dynamics by learning from examples. Here we give a brief introduction to neural network models and deep learning for biologists. We introduce feedforward and recurrent networks and explain the expressive power of this modeling framework and the backpropagation algorithm for setting the parameters. Finally, we consider how deep neural networks might help us understand the brain's computations.","layer":1,"vector":[-0.0329,-0.0227,0.0119,-0.0293,0.041,0.0331,0.049,0.0113,0.0744,-0.027,-0.0078,-0.0547,0.0586,0.0366,0.0231,-0.0157,-0.0179,0.0498,-0.0494,-0.0237,0.0649,-0.0272,-0.0319,-0.0257,0.0013,-0.0107,-0.0527,-0.0158,-0.031,-0.2133,0.0194,-0.0413,0.0569,-0.0473,-0.0046,-0.0322,-0.0152,0.0051,-0.0521,0.048,0.0469,0.0114,-0.0184,-0.0294,0.0075,-0.0576,-0.0282,-0.0394,0.0065,-0.0748,0.0066,-0.0339,-0.0062,-0.0107,0.0352,0.0105,0.1056,0.0472,0.0484,0.0448,0.0306,0.0484,-0.177,0.0466,0.0418,0.0142,-0.039,-0.0157,0.0324,0.0406,-0.0385,0.0306,0.0109,0.0057,0.0014,0.0121,0.0079,-0.0319,0.0316,0.0081,0.0371,-0.003,0.0003,-0.0517,0.014,-0.0183,0.013,-0.0453,0.0087,0.0187,-0.0254,-0.0142,-0.0448,-0.0124,-0.0842,0.0133,0.0746,-0.022,-0.0702,0.1937,-0.0473,0.0498,0.0485,-0.024,0.0148,-0.0189,-0.0475,-0.0437,-0.0347,0.0458,-0.0279,-0.0074,0.0076,-0.0062,0.0068,-0.0117,0.0473,0.0371,0.0065,-0.0232,-0.0411,-0.0126,0.032,-0.0216,0.0598,-0.0465,-0.0038,0.1582,0.0623,0.0258,0.0953,-0.0081,-0.0459,-0.0332,0.0144,0.0521,0.0126,-0.0344,-0.0231,0.0314,-0.0669,-0.0496,0.0041,-0.0757,-0.0861,0.0912,-0.0093,-0.0059,-0.0531,-0.0457,-0.0716,0.0165,0.0005,-0.0114,0.0434,0.0257,0.0039,0.0697,-0.0709,0.0026,-0.0549,-0.015,-0.0053,0.0827,0.0433,-0.0459,-0.0253,-0.0106,0.0135,-0.0044,0.0613,0.0205,-0.0119,-0.0168,0.045,0.044,-0.0526,-0.0259,-0.0151,0.0315,0.0323,-0.0902,-0.05,0.0412,0.0417,-0.0465,-0.017,-0.0747,-0.0238,0.0466,-0.0242,0.0212,-0.022,0.0013,-0.0346,-0.0637,-0.0392,-0.0223,0.0012,-0.0634,-0.0214,-0.0208,0.0187,0.0195,-0.0058,0.0211,-0.0079,0.019,0.0418,0.0294,-0.0269,0.0187,0.0468,-0.0618,-0.0681,0.0053,-0.0064,0.0261,0.0329,0.0449,0.0436,-0.0683,-0.0712,-0.188,-0.0085,0.0357,-0.0499,0.0469,-0.0592,0.0321,-0.0066,0.0479,0.0719,0.0347,-0.0016,0.0134,-0.0016,0.0302,0.0694,0.0435,0.0394,-0.0616,0.0147,-0.0061,-0.0202,0.0409,-0.0914,-0.0034,0.0021,0.1825,0.0319,0.0743,-0.0201,0.0101,0.0202,-0.0158,-0.1075,0.0671,-0.0003,0.0865,-0.0067,-0.0133,-0.0398,-0.0404,0.037,-0.0216,-0.0765,-0.0808,0.0188,-0.0375,0.0192,-0.0511,-0.0112,0.0288,-0.0156,0.0473,0.0223,-0.0246,-0.0324,-0.1186,0.0454,-0.085,0.0544,0.0195,-0.055,-0.0336,-0.057,0.0493,0.0254,-0.0051,-0.0685,0.0408,-0.0343,0.0269,0.1298,0.0531,0.0217,0.0708,0.0025,0.0065,-0.0439,-0.0397,0.0097,0.072,-0.0468,0.0338,0.0163,0.0345,-0.0112,0.0878,-0.0593,0.0545,-0.0164,0.0039,0.0123,-0.0618,-0.0394,0.0211,0.0093,-0.2725,0.0859,-0.0066,0.0808,-0.0056,0.016,0.0448,0.0188,-0.0179,0.0114,0.0033,0.0289,0.0624,0.0284,0.0101,0.0337,0.0831,-0.0637,0.077,-0.0663,0.0436,0.0531,0.1835,-0.0539,0.0458,0.0221,0.0038,0.015,0.0608,-0.0232,0.031,0.0211,0.1078,-0.064,0.0558,0.124,-0.0383,0.0103,0.0032,-0.01,0.0346,-0.0192,-0.0084,0.0235,0.0818,-0.031,-0.0448,-0.0201,-0.0148,0.0441,-0.0689,-0.0131,0.0025,0.0026,0.0065,-0.0151,-0.0401,-0.0412,-0.0388,-0.0375,0.0478,-0.1055,-0.0224,0.0373,-0.0354]}
{"key":"[On Efficient Multilevel Clustering via Wasserstein Distances] We propose a novel approach to the problem of multilevel clustering, which aims to simultaneously partition data in each group and discover grouping patterns among groups in a potentially large hierarchically structured corpus of data. Our method involves a joint optimization formulation over several spaces of discrete probability measures, which are endowed with Wasserstein distance metrics. We propose several variants of this problem, which admit fast optimization algorithms, by exploiting the connection to the problem of finding Wasserstein barycenters. Consistency properties are established for the estimates of both local and global clusters. Finally, experimental results with both synthetic and real data are presented to demonstrate the flexibility and scalability of the proposed approach.","layer":2,"vector":[-0.0189,-0.0221,0.0015,0.0101,0.0141,0.0115,0.0119,-0.0147,0.0372,-0.0133,0.0355,-0.0653,0.0156,0.0335,0.0301,0.0178,0.0365,0.0405,-0.0707,0.0242,-0.0151,-0.0569,-0.0083,-0.0702,0.0527,0.0669,-0.0257,-0.0215,-0.0587,-0.2536,0.0082,-0.0355,0.0564,0.0054,0.0015,-0.0014,-0.0169,0.0759,0.0012,0.039,0.0419,-0.006,-0.0348,-0.0347,-0.0474,-0.0008,-0.0756,-0.0048,-0.0639,-0.052,-0.0187,-0.0383,0.0161,0.0135,0.0129,0.0481,0.0607,0.0127,0.0392,0.0168,0.0404,0.0454,-0.1411,0.0246,0.0358,0.0182,-0.0076,-0.0012,0.0065,0.0462,-0.0144,0.0523,0.0279,0.043,0.0344,0.0212,-0.0055,-0.0552,-0.0451,0.0103,-0.0037,-0.0324,-0.0295,0.0076,-0.0356,-0.0545,0.0199,-0.0531,0.0076,0.0154,-0.0511,-0.036,-0.0202,0.031,-0.0975,-0.0593,0.0125,0.0133,0.0097,0.2171,-0.0583,0.0514,0.0636,-0.0663,-0.0029,-0.0416,0.0146,-0.006,-0.0081,-0.0097,-0.0343,0.0124,0.0119,-0.0592,0.037,0.0011,0.0953,0.0423,-0.0254,-0.0385,0.0131,-0.0183,0.0307,-0.0181,0.0566,-0.0492,0.0196,0.1055,0.0272,0.0238,0.0615,0.0329,-0.0522,-0.0155,0.0398,0.0085,-0.0149,-0.0285,0.0323,-0.0157,-0.0217,-0.0947,0.0491,-0.0874,-0.0278,0.147,-0.0548,0.03,-0.0695,-0.0234,-0.045,-0.0483,-0.0408,-0.0562,0.0365,0.0334,0.0351,0.0174,-0.0306,0.0179,-0.0167,-0.0765,0.0153,0.1349,0.0145,-0.1027,-0.0284,0.0255,-0.0055,-0.0455,0.0612,0.0408,0.0206,0.087,0.0888,0.003,-0.0794,-0.0021,0.0118,0.0097,0.0543,-0.0266,-0.0281,0.0087,0.053,-0.013,-0.0304,-0.0303,0.0448,0.0511,-0.0461,0.0046,-0.0126,0.0164,0.0214,-0.0196,-0.0153,-0.0184,0.0379,-0.0153,0.0257,0.0378,-0.096,0.0531,-0.0018,0.0666,0.0032,-0.0133,0.067,-0.0199,-0.003,0.0063,0.0465,-0.0607,0.0292,-0.0137,0.0265,0.0291,0.0183,0.0688,0.0271,-0.054,-0.0865,-0.2103,-0.0241,0.0377,-0.0265,0.0044,-0.0532,0.017,0.0115,0.0769,0.1074,0.0202,-0.0218,-0.0161,0.0365,-0.0305,0.0554,0.0564,0.0234,-0.0375,0.0331,0.0195,0.0152,-0.0129,-0.0276,0.0348,-0.0119,0.2232,0.0449,-0.0131,-0.0152,0.0329,0.002,-0.0118,-0.0441,0.0273,-0.0138,0.0289,0.0003,-0.0624,-0.0347,-0.0144,-0.0047,0.0322,-0.1082,-0.0294,-0.0535,-0.0207,0.0033,-0.0321,-0.0612,0.018,-0.0505,0.0693,-0.0193,-0.0152,-0.0495,-0.1134,-0.0255,-0.0726,0.0232,0.0275,-0.0375,-0.0033,-0.05,0.0678,-0.0068,-0.0351,0.0025,-0.0037,-0.0508,-0.0329,0.0576,-0.0213,-0.0197,0.0511,0.0144,0.0116,-0.0137,-0.03,-0.0131,0.07,-0.0347,0.0462,0.025,0.0251,-0.005,0.0604,0.0082,0.0674,0.012,0.0288,-0.0273,-0.0528,0.0193,0.05,-0.0178,-0.2841,0.0289,-0.0252,0.0119,0.0079,-0.0043,0.0212,0.0302,0.0128,0.0169,0.0495,0.0348,0.0471,-0.0357,0.0095,0.0568,0.0264,-0.0241,0.0213,-0.0822,0.0295,0.0369,0.2299,-0.0418,0.019,-0.0019,0.0143,0.0097,0.0045,-0.0296,-0.0421,-0.0163,0.0813,-0.0571,0.0442,0.0594,0.0058,0.0206,0.013,-0.0319,0.0038,0.0558,-0.0288,-0.0458,0.0953,0.0115,-0.0239,-0.0694,0.055,0.064,-0.0391,0.0168,-0.037,0.0263,0.0049,0.0422,-0.0055,-0.0556,-0.0487,-0.0549,-0.0236,-0.0148,-0.0263,-0.0099,-0.0095]}
{"key":"[Making Non-Stochastic Control (Almost) as Easy as Stochastic] Recent literature has made much progress in understanding \\emph{online LQR}: a modern learning-theoretic take on the classical control problem in which a learner attempts to optimally control an unknown linear dynamical system with fully observed state, perturbed by i.i.d. Gaussian noise. It is now understood that the optimal regret on time horizon $T$ against the optimal control law scales as $\\widetilde{\\Theta}(\\sqrt{T})$. In this paper, we show that the same regret rate (against a suitable benchmark) is attainable even in the considerably more general non-stochastic control model, where the system is driven by \\emph{arbitrary adversarial} noise (Agarwal et al. 2019). In other words, \\emph{stochasticity confers little benefit in online LQR}. We attain the optimal $\\widetilde{\\mathcal{O}}(\\sqrt{T})$ regret when the dynamics are unknown to the learner, and $\\mathrm{poly}(\\log T)$ regret when known, provided that the cost functions are strongly convex (as in LQR). Our algorithm is based on a novel variant of online Newton step (Hazan et al. 2007), which adapts to the geometry induced by possibly adversarial disturbances, and our analysis hinges on generic \"policy regret\" bounds for certain structured losses in the OCO-with-memory framework (Anava et al. 2015). Moreover, our results accomodate the full generality of the non-stochastic control setting: adversarially chosen (possibly non-quadratic) costs, partial state observation, and fully adversarial process and observation noise.","layer":2,"vector":[-0.0569,-0.0335,0.0231,0.0217,-0.0234,0.0029,0.0305,0.0236,0.0441,0.0197,0.0104,0.0139,0.0147,0.1048,0.0048,0.0347,-0.0291,0.0048,-0.0228,0.0427,0.0481,-0.0738,0.0171,-0.0307,-0.0065,0.0262,-0.0559,-0.0649,-0.0288,-0.2386,0.0116,-0.0614,0.0075,-0.0447,-0.0354,-0.0256,-0.0236,0.088,-0.0285,0.0524,0.0169,0.0434,-0.0324,-0.0673,-0.007,-0.0536,0.0152,-0.0121,-0.0404,-0.0335,-0.0111,-0.0168,0.033,-0.0123,0.0167,0.0348,0.0168,0.0762,0.0197,0.0647,0.0065,0.0193,-0.1406,0.0314,-0.0054,0.0348,-0.0396,-0.0154,0.0166,0.0658,-0.0166,0.0424,0.0116,0.0533,0.0471,-0.014,-0.0147,-0.0387,-0.0402,0.0343,0.0655,-0.0137,-0.0907,0.0112,-0.0554,-0.084,0.0117,-0.0466,0.0891,0.0092,-0.0149,-0.0237,0.0129,0.0192,-0.0283,-0.0012,0.032,0.0547,-0.0449,0.2001,-0.0322,0.0657,0.0225,-0.0136,0.0283,-0.0296,-0.0259,-0.037,-0.0284,-0.0141,-0.022,-0.0205,0.0406,-0.0376,0.0192,0.0168,0.027,0.0416,0.0101,0.0023,-0.0571,0.0212,0.0754,0.0005,0.03,-0.0792,-0.0081,0.1523,0.0245,0.0464,0.0247,-0.0968,-0.0196,-0.0114,0.008,-0.0156,0.0067,0.0033,0.059,-0.0378,-0.0491,-0.053,0.012,-0.1071,-0.0394,0.1187,0.0192,0.0218,-0.0502,-0.0412,-0.0026,0.0153,-0.0156,0.0096,0.016,0.039,0.0257,0.0437,-0.0687,0.0142,-0.088,-0.0317,0.0342,0.0976,-0.0383,-0.0688,-0.0462,-0.018,-0.0162,-0.0013,0.0204,-0.0034,-0.0559,-0.0095,0.08,0.0458,-0.0897,-0.0187,-0.0168,0.0377,-0.0115,-0.0447,-0.0043,-0.0019,0.0397,-0.0197,0.0285,-0.0309,0.0088,0.0367,-0.0197,0.0069,-0.039,-0.0292,-0.0372,-0.0529,-0.0088,-0.0176,0.0086,-0.0079,-0.0073,-0.0166,-0.0403,0.0412,0.03,-0.0057,0.013,-0.0057,0.0352,0.036,-0.0325,0.0153,0.0512,-0.0313,-0.0505,0.0265,0.0382,0.0381,-0.0251,-0.0011,0.0391,0.0117,-0.0325,-0.193,-0.001,-0.0407,-0.0151,0.0779,-0.0686,0.0157,-0.0403,0.0559,0.0554,0.0639,-0.0302,0.0283,0.0084,0.0257,0.0556,0.0599,0.0122,-0.0554,0.0171,-0.0287,0.0146,-0.0369,-0.0676,0.0662,-0.0472,0.2426,0.0035,0.1065,0.0183,0.0508,0.0428,0.0041,-0.049,0.0444,0.0324,0.0765,-0.0154,-0.037,-0.033,-0.0005,-0.026,0.0174,-0.1099,-0.0333,-0.0341,-0.0413,0.0753,-0.0777,-0.0358,0.0437,-0.0251,0.0933,-0.0177,-0.0148,-0.0283,-0.1038,0.0297,0.007,0.0579,-0.0214,-0.034,-0.0104,-0.0908,0.0688,0.007,0.0147,-0.0702,0.0605,-0.0279,-0.0084,0.04,-0.0044,0.0319,0.0475,0.0118,0.035,-0.0542,-0.0686,-0.0287,0.0567,-0.0316,0.0033,0.0601,-0.0033,-0.02,0.0571,-0.0282,-0.0029,0.0059,-0.0326,0.0133,-0.0675,-0.0096,0.0513,0.0153,-0.3205,0.041,0.0217,0.0684,-0.0414,-0.0149,0.0329,0.0136,-0.0527,0.0283,-0.0017,0.0781,0.016,0.0076,0.0195,0.0215,0.0142,-0.0408,0.0631,-0.0351,0.0025,0.0492,0.1798,-0.0471,0.0286,-0.0073,-0.0288,0.0147,0.0582,-0.0528,0.0237,0.0238,0.0557,-0.0688,0.0494,0.0443,-0.043,0.0499,-0.0008,0.0077,-0.0291,0.023,0.0003,0.0388,0.1005,0.0152,-0.053,-0.0256,-0.0168,0.0334,-0.0143,0.0353,-0.0161,0.0302,-0.0021,0.0263,-0.0358,-0.061,-0.0284,-0.049,0.0076,-0.0232,0.0006,-0.0132,-0.0044]}
{"key":"[Hyperparameter Sensitivity in Deep Outlier Detection: Analysis and a Scalable Hyper-Ensemble Solution] Outlier detection (OD) literature exhibits numerous algorithms as it applies to diverse domains. However, given a new detection task, it is unclear how to choose an algorithm to use, nor how to set its hyperparameter(s) (HPs) in unsupervised settings. HP tuning is an ever-growing problem with the arrival of many new detectors based on deep learning. While they have appealing properties such as task- driven representation learning and end-to-end optimization, deep models come with a long list of HPs. Surprisingly, the issue of model selection in the outlier mining literature has been \"the elephant in the room\"; a significant factor in unlocking the utmost potential of deep methods, yet little said or done to systematically tackle the issue. In the first part of this paper, we conduct the first large-scale analysis on the HP sensitivity of deep OD methods, and through more than 35,000 trained models, quantitatively demonstrate that model selection is inevitable. Next, we design a HP-robust and scalable deep hyper-ensemble model called ROBOD that assembles models with varying HP configurations, bypassing the choice paralysis. Importantly, we introduce novel strategies to speed up ensemble training, such as parameter sharing, batch/simultaneous training, and data subsampling, that allow us to train fewer models with fewer parameters. Extensive experiments on both image and tabular datasets show that ROBOD achieves and retains robust, state-of-the-art detection performance as compared to its modern counterparts, while taking only 2-10% of the time by the naive hyper-ensemble with independent training.","layer":1,"vector":[0.0022,-0.0164,-0.0048,-0.0052,0.0657,-0.0242,0.0433,0.0267,0.036,-0.0235,0.0398,-0.0433,0.0104,0.073,0.0168,-0.0249,0.0345,0.0716,-0.0571,0.0045,0.0154,-0.0639,-0.0224,-0.0299,0.0556,0.0088,-0.0288,-0.0526,-0.1004,-0.2768,-0.0032,-0.0659,0.06,-0.0498,0.0532,0.0297,-0.0167,-0.0213,-0.0109,0.0179,-0.0284,0.0171,-0.0252,-0.0755,-0.0323,-0.0393,0.0247,0.0071,-0.0132,-0.0275,0.0212,-0.0307,0.0247,0.0532,0.0477,0.0087,0.0608,0.0123,0.0593,0.0574,0.0517,0.0431,-0.1292,-0.0069,0.0612,0.0165,-0.0255,-0.0321,0.0241,-0.019,0.0049,0.0246,-0.02,0.0259,0.0115,0.0137,-0.0094,-0.0271,-0.0019,0.02,0.0415,-0.0414,-0.0158,-0.0265,0.0222,-0.064,-0.0008,-0.0061,0.0695,-0.0246,-0.0044,-0.0009,-0.0116,0.0076,-0.0594,-0.017,0.06,0.0227,-0.0271,0.2304,-0.0329,0.0121,0.0009,-0.0043,0.0396,-0.0433,-0.0558,-0.0429,-0.0042,-0.0123,0.01,-0.0095,0.0421,-0.0463,-0.0176,-0.0003,0.0688,0.0219,-0.0404,0.0047,-0.0228,-0.0176,0.1068,-0.0457,0.0396,-0.0428,0.0293,0.1256,0.0177,-0.0044,0.0282,-0.0086,-0.081,-0.0357,0.015,0.0526,0.0315,0.0331,0.0291,-0.0066,-0.0468,-0.0269,-0.008,-0.0675,-0.0213,0.1086,-0.067,0.0047,-0.0352,-0.0641,-0.0217,0.0169,-0.0287,-0.0008,0.0348,0.0084,0.036,0.0732,-0.0127,-0.0048,-0.028,-0.0695,0.0054,0.0827,-0.0344,-0.0617,-0.0371,-0.0104,-0.0098,0.0213,0.0909,0.0187,-0.028,0.0379,0.0892,0.0003,-0.0631,-0.035,0.0077,-0.0023,0.0108,-0.0081,-0.0439,0.0491,0.071,-0.0367,0.0078,-0.0362,0.0302,0.0549,-0.0692,0.0136,-0.0192,-0.0088,0.0014,0.0042,0.0033,-0.01,0.0322,-0.0552,-0.0161,0.0074,-0.015,0.0359,-0.0327,0.0291,0.0096,0.0013,0.0384,0.0559,-0.0558,0.0222,0.0346,0.0025,-0.053,-0.0194,0.0141,0.0605,-0.0162,0.056,0.0129,-0.044,-0.0514,-0.2323,-0.0127,0.0606,-0.0453,0.0169,-0.0387,0.05,0.0155,0.0752,0.1049,0.0514,-0.0106,-0.0225,0.0597,-0.0306,0.0854,0.0083,0.0501,-0.0692,0.0328,-0.0319,0.0532,-0.0313,-0.0933,0.0354,0.011,0.1674,0.0276,0.0153,-0.0652,0.0236,0.0268,-0.0057,-0.0774,0.0593,0.0125,0.08,0.0119,-0.0642,-0.0085,-0.0521,0.0663,-0.0002,-0.1191,-0.0422,-0.0078,-0.03,-0.0117,-0.0454,0.0531,0.0471,-0.0516,0.0732,-0.0056,0.0165,-0.0579,-0.0797,0.0588,-0.0262,-0.0079,0.0176,-0.0506,0.021,-0.1048,0.0401,-0.0306,-0.0326,-0.0576,0.0584,-0.0483,-0.0123,0.0713,-0.0128,0.0038,0.062,0.0076,0.0063,-0.0504,-0.0334,-0.0287,0.0827,0.0307,0.0073,0.0406,0.0268,-0.021,0.0833,-0.0041,0.0546,-0.0351,0.0052,-0.0075,-0.0302,-0.0268,0.0237,0.0166,-0.2747,0.0404,-0.0059,0.0503,-0.0221,0.022,0.0072,-0.008,0.0286,-0.0189,-0.0377,0.0424,-0.0002,-0.0155,-0.005,0.0336,0.0398,-0.0323,0.055,-0.0237,0.026,0.0801,0.2062,-0.0249,0.0007,0.044,0.0136,-0.0403,0.0237,-0.0769,0.0192,-0.0105,0.0775,-0.0349,0.0206,0.0972,-0.0679,0.0115,-0.0241,-0.0394,-0.0014,0.0097,-0.0295,-0.0426,0.1039,-0.0412,-0.0429,-0.0429,0.0446,0.0576,-0.0488,-0.0267,-0.0711,0.024,0.0086,0.0132,-0.0468,-0.0236,-0.0342,-0.033,0.0167,-0.0248,-0.0108,0.0081,-0.0421]}
{"key":"[PSL is Dead. Long Live PSL] Property Specification Language (PSL) is a form of temporal logic that has been mainly used in discrete domains (e.g. formal hardware verification). In this paper, we show that by merging machine learning techniques with PSL monitors, we can extend PSL to work on continuous domains. We apply this technique in machine learning-based anomaly detection to analyze scenarios of real-time streaming events from continuous variables in order to detect abnormal behaviors of a system. By using machine learning with formal models, we leverage the strengths of both machine learning methods and formal semantics of time. On one hand, machine learning techniques can produce distributions on continuous variables, where abnormalities can be captured as deviations from the distributions. On the other hand, formal methods can characterize discrete temporal behaviors and relations that cannot be easily learned by machine learning techniques. Interestingly, the anomalies detected by machine learning and the underlying time representation used are discrete events. We implemented a temporal monitoring package (TEF) that operates in conjunction with normal data science packages for anomaly detection machine learning systems, and we show that TEF can be used to perform accurate interpretation of temporal correlation between events.","layer":2,"vector":[-0.0625,0.035,0.0327,-0.0525,0.0423,0.0017,0.0313,0.0377,0.0585,-0.0307,-0.0049,-0.0525,0.0159,0.0387,-0.0142,0.0041,-0.0291,0.0472,-0.0229,-0.0184,0.0869,-0.033,-0.0457,-0.0588,0.0027,0.041,0.0039,-0.0058,-0.0619,-0.2107,0.01,-0.0636,0.0038,-0.0288,0.0325,-0.0707,-0.0667,0.0576,-0.0051,0.0609,0.0076,0.0332,0.0197,-0.0826,-0.0285,-0.0782,0.0368,-0.0303,-0.0447,-0.0337,-0.0338,-0.0105,0.0938,0.018,0.0022,0.0338,0.0798,0.0373,0.0949,0.0009,0.0367,0.0384,-0.1757,0.0312,0.0644,0.0336,-0.0139,0.0,0.0754,0.0494,-0.0096,0.0249,-0.0224,0.0724,-0.014,-0.0032,0.0068,-0.0469,-0.0186,0.0561,-0.0247,-0.05,-0.0292,-0.0348,-0.0683,-0.0392,-0.005,-0.016,0.067,-0.0032,-0.0676,-0.0292,0.0061,0.0442,-0.0408,0.0287,0.0489,-0.0277,0.0149,0.2165,-0.0521,0.0216,-0.0062,-0.0175,0.0643,-0.0482,-0.0115,-0.0431,-0.0131,-0.0627,-0.0344,0.0052,0.047,-0.0486,0.0477,0.0139,0.031,0.0055,0.0127,-0.0066,-0.0096,0.027,0.0377,-0.0542,0.0364,-0.064,0.0437,0.147,-0.0042,0.0276,-0.0001,-0.004,-0.0645,-0.0334,0.0393,0.0528,0.0008,0.0121,0.0518,-0.0079,-0.0711,0.0,0.0393,-0.057,-0.0314,0.0931,-0.0129,0.0221,-0.0382,-0.0042,-0.0295,0.0205,-0.0458,-0.0813,0.0369,0.0421,0.0212,0.0331,-0.0232,0.0379,-0.0189,-0.0373,-0.0311,0.1123,0.0045,-0.1021,-0.0261,-0.0002,0.0103,-0.0221,0.0367,0.0458,-0.0428,-0.0085,0.0247,-0.0065,-0.0409,-0.004,-0.0113,-0.0076,0.024,-0.0473,-0.0365,0.081,0.0647,-0.053,-0.0032,-0.0078,0.0083,0.039,-0.0195,-0.0063,-0.0316,0.0023,-0.0044,-0.0204,0.0068,0.0081,0.0324,-0.0582,0.0172,0.0015,-0.0238,0.0145,-0.0265,0.0177,-0.0409,0.0044,0.0241,0.0345,-0.0052,0.0002,0.0047,-0.0287,-0.0354,0.0024,-0.0048,0.0442,-0.004,0.0228,0.0306,0.0214,-0.0294,-0.2476,-0.0102,0.0093,-0.0462,0.022,-0.0464,-0.0192,-0.0259,0.0005,0.0412,0.0432,-0.0333,-0.042,-0.0375,-0.0375,0.0774,0.0372,0.0253,-0.0699,0.0294,-0.0608,0.0084,-0.0327,-0.1183,0.0367,0.0153,0.1937,0.0162,0.0768,-0.0729,0.0227,-0.0273,-0.0289,-0.0726,0.0705,0.0472,0.0136,0.0355,0.0093,-0.0429,-0.0462,0.058,-0.0449,-0.053,-0.014,-0.0327,-0.0096,0.0194,-0.0551,0.0446,0.0155,-0.0472,0.0652,0.0294,0.0105,-0.0712,-0.0428,0.0035,-0.0066,0.0221,0.043,-0.0025,0.0072,-0.0663,0.0743,-0.0376,-0.0291,-0.0834,0.0125,-0.0019,-0.0178,0.1347,-0.0118,-0.0615,0.071,0.0504,0.001,-0.0361,-0.0216,-0.0043,0.0817,-0.0482,0.0422,0.0119,0.0427,-0.0123,0.0413,0.0473,0.0367,-0.0191,-0.0136,0.0162,-0.0394,-0.0386,0.0348,0.0205,-0.3007,0.0199,0.0067,0.004,-0.0428,-0.0002,0.0393,0.0586,-0.0459,-0.0037,0.0252,0.0295,0.0221,-0.0254,0.0257,0.0428,0.0827,-0.0624,0.0366,-0.0288,0.021,0.0453,0.2364,-0.0074,0.0415,0.014,-0.0018,0.0265,0.068,0.0051,-0.0006,0.0222,0.0842,-0.0189,-0.0124,0.05,-0.0301,0.0344,0.0428,0.0001,0.0078,-0.0236,-0.0363,-0.044,0.0714,0.0232,-0.0334,-0.0742,0.022,0.0535,-0.0304,-0.0047,-0.0423,0.0227,0.0101,0.0035,-0.0086,-0.0005,-0.0269,-0.0444,0.0349,-0.0641,0.0283,-0.0185,-0.0069]}
{"key":"[Skill Machines: Temporal Logic Composition in Reinforcement Learning] A major challenge in reinforcement learning is specifying tasks in a manner that is both interpretable and verifiable. One common approach is to specify tasks through reward machines -- finite state machines that encode the task to be solved. We introduce skill machines, a representation that can be learned directly from these reward machines that encode the solution to such tasks. We propose a framework where an agent first learns a set of base skills in a reward-free setting, and then combines these skills with the learned skill machine to produce composite behaviours specified by any regular language, such as linear temporal logics. This provides the agent with the ability to map from complex logical task specifications to near-optimal behaviours zero-shot. We demonstrate our approach in both a tabular and high-dimensional video game environment, where an agent is faced with several of these complex, long-horizon tasks. Our results indicate that the agent is capable of satisfying extremely complex task specifications, producing near optimal performance with no further learning. Finally, we demonstrate that the performance of skill machines can be improved with regular offline reinforcement learning algorithms when optimal behaviours are desired.","layer":0,"vector":[-0.0415,0.007,0.0304,-0.0201,-0.0205,0.0166,0.0243,0.0407,0.0277,-0.0115,0.0239,-0.071,-0.0072,0.0716,0.0378,0.0393,-0.0209,0.0628,-0.0064,-0.0218,0.0347,-0.0613,-0.0485,-0.0394,-0.0018,0.0311,-0.0443,-0.0411,-0.0096,-0.2074,0.0102,-0.0575,-0.0101,-0.0241,-0.0184,-0.0269,-0.0585,0.0374,-0.0509,0.0105,0.0594,0.0213,0.0006,-0.0612,-0.0147,-0.0656,-0.0076,-0.0547,-0.0185,-0.0503,-0.0121,-0.0258,0.0242,0.0309,0.0434,0.0148,0.0482,0.0817,0.023,0.013,-0.0049,0.0284,-0.1494,0.058,0.03,0.0423,-0.0696,-0.0087,0.0316,0.0639,-0.0318,0.0428,0.0046,0.0469,0.0255,0.0084,-0.0166,-0.0354,0.0262,0.0123,-0.0218,-0.0483,-0.0128,-0.0017,-0.0306,-0.0287,0.0084,-0.0416,0.0519,0.017,-0.0464,0.0,-0.0109,0.0463,-0.0242,-0.0108,0.0465,-0.0043,-0.0773,0.2335,-0.0241,-0.0033,0.0125,-0.0458,0.042,-0.0158,-0.037,-0.0489,-0.0033,-0.0293,-0.0616,-0.0004,0.0458,0.0311,-0.0078,0.0129,0.058,0.0182,0.0035,0.0214,0.0319,0.0141,0.046,-0.0278,0.0266,-0.0687,0.0008,0.1483,0.0216,0.0125,0.0406,-0.0498,0.0079,-0.001,0.0371,0.0349,0.0748,-0.0045,0.0103,-0.0122,-0.0484,0.0004,0.0166,-0.1,-0.043,0.0794,0.0221,0.0116,-0.0282,0.0057,-0.0584,0.0145,0.0209,-0.0288,-0.0069,0.0541,0.0337,0.0276,-0.0718,0.0148,-0.0192,-0.0736,0.0043,0.0893,-0.0388,-0.0959,-0.0321,-0.0278,0.0182,-0.0233,0.0411,0.0045,-0.0567,0.0097,0.0822,0.0161,-0.0879,0.0009,-0.0114,0.0134,0.0392,-0.0705,-0.0358,0.0463,-0.0016,0.0106,0.0262,-0.041,-0.0092,0.0053,-0.0165,0.0494,-0.0249,-0.0116,-0.0108,-0.0298,0.0221,0.0027,0.0199,-0.0474,-0.0006,-0.0232,-0.0248,0.0084,0.0107,-0.0273,-0.005,-0.0031,0.0755,0.058,-0.0362,-0.0104,0.0269,-0.0304,-0.0321,-0.0008,0.0459,-0.0285,-0.028,0.0065,0.0015,0.0023,-0.042,-0.2007,-0.0105,-0.046,0.0022,0.0504,0.0081,0.0118,-0.0314,0.0102,0.0311,0.0718,-0.0723,-0.0422,-0.0053,-0.004,0.0636,0.0601,0.0073,-0.0058,0.0478,0.0006,-0.0139,-0.0215,-0.1006,0.032,-0.0154,0.2697,0.0363,0.0452,-0.0093,0.0237,0.0346,-0.0332,-0.0914,0.0687,-0.0029,0.0568,0.0193,0.0246,-0.043,-0.0309,0.039,-0.0295,-0.1283,-0.0432,0.0045,-0.0473,0.0338,-0.0523,-0.0044,0.0543,-0.0074,0.0248,-0.0035,-0.0676,-0.0305,-0.0565,0.0307,-0.0491,0.0319,0.0375,-0.0407,-0.0157,-0.062,0.073,-0.0049,-0.0029,-0.0761,0.0483,0.0005,0.0164,0.0405,-0.0206,-0.0213,0.0518,0.0086,0.0271,-0.0229,-0.0401,0.0058,0.0533,-0.0297,0.0235,0.0611,0.0248,-0.0124,0.0514,-0.0297,0.0345,0.0267,0.0007,0.0144,-0.0702,0.0273,0.0198,-0.0106,-0.326,0.0676,0.0134,0.0282,-0.0349,0.0257,0.0209,-0.0045,-0.0809,0.0154,0.0277,0.0593,0.0049,0.029,-0.0204,0.0017,0.1406,-0.0286,0.0648,-0.0448,0.0019,0.0839,0.2496,-0.0414,0.0675,-0.002,-0.0296,-0.0119,0.054,0.0083,0.0334,0.0173,0.0636,0.0274,-0.0063,0.0838,-0.0181,0.024,0.0163,0.0122,-0.0583,0.0122,0.002,-0.0464,0.0551,0.0417,-0.0227,-0.0617,-0.0344,0.0048,-0.0159,0.0274,0.0029,-0.017,0.0553,0.0064,-0.0081,-0.0762,-0.0325,-0.0397,0.0076,-0.0515,0.06,-0.0068,-0.0191]}
{"key":"[Instance Segmentation as Image Segmentation Annotation] The instance segmentation problem intends to precisely detect and delineate objects in images. Most of the current solutions rely on deep convolutional neural networks but despite this fact proposed solutions are very diverse. Some solutions approach the problem as a network problem, where they use several networks or specialize a single network to solve several tasks. A different approach tries to solve the problem as an annotation problem, where the instance information is encoded in a mathematical representation. This work proposes a solution based in the DCME technique to solve the instance segmentation with a single segmentation network. Different from others, the segmentation network decoder is not specialized in a multi-task network. Instead, the network encoder is repurposed to classify image objects, reducing the computational cost of the solution.","layer":0,"vector":[-0.053,-0.0242,0.0501,-0.0189,0.0234,0.011,0.0714,-0.0052,0.0124,-0.0425,-0.0023,-0.0909,0.0307,0.0774,0.0124,0.0214,0.0571,0.0534,0.0108,0.0035,0.0181,-0.0389,-0.0343,-0.056,-0.0211,0.0142,0.033,-0.0005,-0.0502,-0.2452,0.0032,-0.0296,0.0605,-0.0309,0.011,-0.0265,-0.0477,0.0455,-0.0871,0.0442,0.0328,-0.004,-0.0476,-0.0277,-0.0406,-0.0513,-0.0241,-0.0652,0.0075,-0.0176,0.1018,-0.0022,0.061,0.0238,-0.0211,0.0421,0.0598,0.0802,0.0451,-0.0006,0.0654,0.0311,-0.1741,0.0716,0.0451,0.011,-0.0337,-0.0311,-0.0026,0.0559,0.0005,0.0282,0.002,0.0267,0.0264,0.0028,0.0143,-0.0547,-0.0303,-0.0177,-0.0556,0.0249,-0.0234,-0.0156,0.0097,-0.0193,0.0146,-0.0584,0.0595,-0.023,-0.0266,-0.0189,0.0189,0.0319,-0.042,-0.025,0.0137,0.0196,-0.0297,0.2216,-0.065,0.0115,0.0947,-0.0509,0.0008,-0.0168,-0.0438,-0.0426,-0.0036,0.0092,-0.0147,0.0199,0.0081,-0.0161,0.0415,0.0073,0.0622,0.0477,-0.0218,0.0384,-0.0282,0.0201,0.0265,-0.0273,0.0069,-0.0686,0.0527,0.1434,0.0625,0.0324,0.0515,0.0044,-0.0514,-0.0364,-0.0165,0.0377,0.0605,-0.0421,-0.0408,-0.0227,-0.0467,-0.0813,0.023,-0.0407,-0.0396,0.0866,-0.0735,0.0796,-0.0274,-0.0407,0.0106,0.0247,-0.0489,-0.0022,0.0571,0.0135,0.0554,0.0074,-0.0156,0.0023,-0.0102,-0.0566,-0.0355,0.1364,0.0356,-0.098,-0.0425,-0.0023,0.0063,-0.0315,0.0047,-0.0058,-0.0351,0.0258,0.0644,0.022,-0.0806,0.0035,0.0003,0.0485,0.0308,-0.0747,-0.0446,-0.0019,-0.0002,-0.0696,0.0092,-0.0681,0.0121,0.0635,-0.0229,0.088,-0.0152,-0.0108,-0.021,-0.0335,-0.0202,-0.0025,-0.005,-0.0,-0.0167,-0.0159,0.0076,0.0545,-0.0034,0.0183,-0.0293,-0.0104,-0.0049,0.0214,-0.0558,0.0073,0.0192,-0.0377,-0.0092,0.028,0.043,0.0423,0.0291,0.0455,0.0486,-0.0758,-0.0481,-0.2303,-0.0155,0.0503,-0.0746,-0.0059,-0.016,0.0316,-0.0102,0.049,0.0404,0.0238,-0.0491,-0.0291,0.0063,0.0111,0.0132,0.0152,0.0834,-0.0195,-0.0204,0.0249,0.0241,0.0139,-0.0524,0.0612,-0.0083,0.2476,0.0134,0.06,0.0089,0.0193,0.0376,-0.0404,-0.0688,0.042,-0.0064,0.051,-0.0058,-0.0325,-0.0294,-0.0225,0.0151,0.0228,-0.085,-0.0359,-0.0249,-0.0411,0.0515,-0.0252,0.0235,0.0132,-0.0535,0.0399,0.04,-0.0272,-0.0441,-0.0701,-0.0065,-0.0325,0.0276,0.0023,-0.0429,0.0232,-0.11,0.0746,0.0363,-0.0381,-0.0306,-0.0047,0.0076,-0.0379,0.0692,0.0138,-0.0118,0.0701,-0.0014,0.0357,-0.0099,-0.0372,-0.0111,0.0356,-0.0384,0.0225,-0.0018,0.0115,0.0333,0.0282,-0.0213,0.0102,-0.0377,0.0181,-0.0248,-0.0485,-0.0048,0.0004,-0.0026,-0.2692,0.0617,0.0051,0.0408,0.0042,0.0291,0.0647,0.0521,-0.0071,-0.0184,0.0084,0.0164,0.0248,-0.0316,0.0101,0.0346,0.0304,-0.0661,0.068,-0.0513,-0.01,0.0526,0.217,-0.04,-0.0043,0.0085,-0.0504,-0.0173,0.034,0.0304,-0.0061,0.0196,0.1105,-0.0627,0.0317,0.079,0.0168,-0.0121,0.0292,-0.0348,-0.0103,0.0319,-0.034,-0.0438,0.0849,0.0266,-0.0149,-0.0605,0.0064,-0.0103,-0.0164,-0.0397,-0.0053,0.0104,0.0243,-0.0075,-0.0646,-0.0374,-0.0601,-0.0157,0.0389,-0.0809,0.0012,-0.0004,0.002]}
{"key":"[Towards Fair Federated Recommendation Learning: Characterizing the Inter-Dependence of System and Data Heterogeneity] Federated learning (FL) is an effective mechanism for data privacy in recommender systems by running machine learning model training on-device. While prior FL optimizations tackled the data and system heterogeneity challenges faced by FL, they assume the two are independent of each other. This fundamental assumption is not reflective of real-world, large-scale recommender systems -- data and system heterogeneity are tightly intertwined. This paper takes a data-driven approach to show the inter-dependence of data and system heterogeneity in real-world data and quantifies its impact on the overall model quality and fairness. We design a framework, RF^2, to model the inter-dependence and evaluate its impact on state-of-the-art model optimization techniques for federated recommendation tasks. We demonstrate that the impact on fairness can be severe under realistic heterogeneity scenarios, by up to 15.8--41x compared to a simple setup assumed in most (if not all) prior work. It means when realistic system-induced data heterogeneity is not properly modeled, the fairness impact of an optimization can be downplayed by up to 41x. The result shows that modeling realistic system-induced data heterogeneity is essential to achieving fair federated recommendation learning. We plan to open-source RF^2 to enable future design and evaluation of FL innovations.","layer":6,"vector":[-0.0049,-0.0336,-0.0138,-0.0506,0.0199,0.0172,0.0114,0.0648,0.0618,-0.0324,0.0193,-0.0261,0.0008,0.0893,0.0592,0.0206,0.0498,-0.0126,-0.0487,-0.0303,0.0369,-0.0617,-0.0391,-0.0667,-0.0241,0.0538,-0.0626,-0.0223,-0.1207,-0.2252,-0.0005,-0.0784,0.0466,0.0141,0.0017,-0.0328,0.0042,0.0406,-0.0209,0.0521,-0.0075,0.0205,-0.0252,0.0032,-0.0279,-0.0111,-0.0312,0.0314,-0.0636,-0.026,0.049,-0.0284,0.0234,0.024,0.0166,0.0484,0.0531,0.0623,0.0205,0.02,0.0568,0.0409,-0.1396,0.067,0.0482,0.0418,-0.0423,-0.0024,-0.008,0.0316,0.0264,0.071,-0.0087,0.0209,0.0032,0.0191,-0.0161,-0.0085,-0.0323,0.0424,-0.0228,-0.0224,-0.0555,-0.049,0.0065,-0.027,0.0016,-0.0517,0.0158,-0.0183,-0.0336,0.0154,-0.0151,0.0072,-0.0316,-0.0096,0.0274,0.0599,-0.0609,0.1874,-0.0587,0.0542,0.0299,-0.0289,0.0388,-0.0248,0.0068,-0.0424,0.0263,-0.0042,-0.0197,-0.0206,0.0338,-0.0595,0.0157,0.0198,0.0896,0.053,0.0071,-0.0494,-0.0345,0.0122,0.0868,-0.0048,0.0332,-0.0524,-0.0043,0.1307,0.0127,0.0313,0.046,-0.0542,-0.0767,-0.0307,0.0223,0.0768,0.0058,-0.044,0.0692,0.033,-0.0426,-0.0642,-0.0106,-0.0544,-0.0526,0.1379,-0.0314,0.0309,-0.0795,-0.0201,-0.0027,0.0053,-0.0204,-0.0256,0.0471,0.0049,0.0579,0.0613,-0.0641,-0.0032,-0.0317,-0.0582,-0.0081,0.1137,-0.012,-0.0854,-0.0319,0.0298,0.0099,-0.0181,0.0121,0.019,0.003,0.026,0.0689,0.0344,-0.063,-0.0038,-0.0045,-0.034,0.0041,-0.0278,-0.0329,0.0255,0.014,-0.0211,0.0183,-0.049,-0.0049,0.0548,-0.0518,0.0056,-0.0048,-0.0037,0.0158,-0.0313,-0.0088,-0.0072,0.0297,-0.0422,0.0066,0.0414,-0.0219,0.0223,-0.0607,0.0511,-0.0078,-0.0182,0.0397,-0.0306,-0.0408,0.0157,0.0289,-0.033,-0.0599,-0.0008,0.026,0.0419,0.0398,0.0644,0.0315,-0.0251,-0.0635,-0.1946,-0.0353,0.0059,0.0039,0.044,-0.0294,0.0439,-0.034,0.0319,0.0997,0.07,0.0052,-0.0623,0.0907,-0.0042,0.0622,0.0117,0.0408,-0.0188,0.0052,-0.0057,0.0443,-0.0025,-0.099,0.0249,0.0637,0.2,0.0279,-0.0313,-0.0595,0.0183,0.0492,-0.0182,-0.1282,0.0111,0.006,0.0293,-0.043,-0.0319,-0.0352,0.004,0.0238,0.0114,-0.1455,-0.0323,-0.0379,-0.0317,0.0078,-0.0999,0.0603,0.0068,-0.0242,0.0382,-0.0148,0.0104,-0.0497,-0.0485,0.0422,-0.0316,0.0625,0.0116,-0.0505,0.0127,-0.0958,0.056,-0.0109,-0.0338,-0.0017,0.0305,-0.0366,-0.004,0.076,-0.0072,0.0003,0.0418,0.0277,0.0318,-0.0147,-0.0562,-0.0539,0.1161,-0.0273,0.0579,0.0084,-0.0103,-0.0474,0.0623,0.0148,0.0476,-0.0485,-0.0125,-0.0197,-0.0796,-0.0367,0.0729,-0.0225,-0.3043,0.0004,-0.0013,0.066,-0.0188,-0.027,0.0748,0.0174,-0.0048,0.0284,0.0149,0.0295,0.0323,-0.0331,-0.0036,0.0472,0.0456,-0.0593,0.021,-0.0492,0.0567,0.0226,0.2103,0.0136,0.0367,0.0519,0.0199,0.0307,0.0396,0.0086,-0.0018,-0.0167,0.0722,-0.0537,0.0317,0.0744,-0.0364,0.0144,0.0117,-0.013,0.0076,-0.0259,-0.0128,0.0129,0.0526,0.0149,-0.0511,-0.0337,-0.018,0.0313,-0.0181,-0.0217,-0.0353,-0.0085,0.0345,0.0565,-0.063,-0.0124,-0.0408,-0.0254,0.0,-0.0368,-0.0301,-0.0199,-0.0274]}
{"key":"[A Pathology-Based Machine Learning Method to Assist in Epithelial Dysplasia Diagnosis] The Epithelial Dysplasia (ED) is a tissue alteration commonly present in lesions preceding oral cancer, being its presence one of the most important factors in the progression toward carcinoma. This study proposes a method to design a low computational cost classification system to support the detection of dysplastic epithelia, contributing to reduce the variability of pathologist assessments. We employ a multilayer artificial neural network (MLP-ANN) and defining the regions of the epithelium to be assessed based on the knowledge of the pathologist. The performance of the proposed solution was statistically evaluated. The implemented MLP-ANN presented an average accuracy of 87%, with a variability much inferior to that obtained from three trained evaluators. Moreover, the proposed solution led to results which are very close to those obtained using a convolutional neural network (CNN) implemented by transfer learning, with 100 times less computational complexity. In conclusion, our results show that a simple neural network structure can lead to a performance equivalent to that of much more complex structures, which are routinely used in the literature.","layer":7,"vector":[-0.0229,-0.0453,0.0801,0.0187,-0.0215,0.022,0.0541,0.0268,0.0187,-0.0324,0.0144,-0.058,0.0303,0.0083,0.0238,0.0019,0.0109,0.0134,-0.0404,-0.0072,0.0263,-0.0275,0.0044,-0.0582,0.0539,0.0129,-0.0391,-0.0592,-0.0777,-0.2032,0.0451,-0.0827,0.0457,-0.0305,-0.0319,-0.0547,-0.0385,0.0663,0.0267,-0.0012,0.0397,-0.018,-0.0451,-0.0878,0.0327,-0.0434,-0.0385,-0.0216,-0.0048,-0.0229,0.0129,-0.0825,0.028,0.0276,0.0201,0.0142,0.0498,0.049,0.0682,0.056,-0.0002,0.0449,-0.185,0.1056,0.0497,0.0002,-0.0337,-0.0513,0.0662,0.0091,-0.056,0.0317,0.0425,0.0424,-0.0469,-0.045,0.0443,-0.0175,0.0112,0.0609,-0.0043,0.0357,-0.0393,-0.0336,-0.0096,-0.0019,0.0121,-0.0699,-0.0409,0.0034,-0.0017,0.0224,-0.0131,0.0362,-0.041,-0.0085,0.0343,0.0275,-0.0359,0.1799,-0.0649,0.0091,0.0177,-0.0482,0.02,0.0119,-0.0591,-0.0271,-0.0555,0.027,0.0285,-0.0342,0.0153,-0.0016,-0.0201,-0.0072,0.0449,0.0324,-0.0025,0.0102,-0.0214,-0.0183,0.0234,-0.0125,0.0504,-0.0303,0.0311,0.1504,0.0255,0.042,0.0042,0.0052,-0.0283,-0.0096,0.02,0.0046,-0.0173,-0.0367,-0.0475,-0.0374,-0.0875,-0.077,0.0258,-0.0582,-0.0327,0.1141,-0.0653,0.037,-0.0294,-0.0223,-0.0386,-0.006,-0.0485,-0.0348,0.0298,0.013,0.0198,0.0309,-0.0476,0.0045,0.0336,-0.0558,-0.0761,0.1457,0.0341,-0.0614,-0.0638,-0.0493,0.0356,-0.0336,0.0565,0.0512,-0.0102,0.0007,0.0494,0.0615,-0.0661,-0.0088,-0.0138,0.017,0.0456,-0.0134,-0.0713,0.021,0.0489,-0.0365,0.017,-0.0392,0.0403,0.0621,-0.059,0.0313,-0.054,-0.0261,-0.0586,-0.0159,0.0161,-0.0118,0.0152,-0.0101,0.0082,0.0014,-0.0046,0.0037,-0.0162,0.0485,-0.0108,0.0336,0.0482,0.0065,-0.0427,-0.023,0.0674,-0.0073,-0.0303,0.013,0.0145,0.0603,0.0288,0.016,0.0641,0.0007,-0.0829,-0.205,-0.0173,0.0161,0.0183,0.029,-0.0778,0.0845,-0.0005,0.0579,0.0555,0.0634,0.0061,0.0411,0.0295,-0.0403,0.0324,0.0538,0.0206,-0.0675,-0.0058,0.021,0.0033,0.0208,-0.068,0.047,0.0248,0.1873,0.0135,-0.003,0.0311,0.0294,0.0301,-0.0292,-0.1196,0.0867,-0.017,0.0375,-0.0267,-0.0719,-0.023,-0.0304,-0.0055,-0.0122,-0.072,-0.0158,-0.0049,-0.0258,0.0289,-0.0509,-0.0083,0.04,-0.0461,0.0459,-0.0022,0.0351,-0.0266,-0.0997,0.073,-0.0622,-0.035,0.0511,-0.0451,0.0191,-0.0642,0.0341,0.0014,-0.0354,-0.0896,0.0169,-0.0434,-0.0377,0.0915,0.0109,0.0084,0.0277,-0.0211,0.0294,-0.0439,-0.0593,-0.0321,0.0946,-0.0241,0.0327,0.0284,0.0472,0.0384,0.0594,-0.0154,-0.0197,0.0061,-0.0085,0.0155,-0.0495,0.0356,0.0261,0.0081,-0.2764,0.0464,-0.0097,0.0752,-0.0346,0.0129,-0.0158,0.0024,-0.0143,-0.0151,0.0235,-0.0155,0.0728,-0.0546,0.0122,0.0245,0.028,-0.0494,0.0718,-0.0181,0.0663,0.0562,0.2287,-0.0726,-0.0049,0.0238,-0.0132,-0.0045,0.0321,-0.0297,0.0463,0.0166,0.0525,-0.0716,0.0379,0.0965,-0.0246,0.0355,-0.0344,-0.0401,0.0105,0.0257,-0.0595,-0.0028,0.0889,-0.0126,0.0115,-0.013,0.0058,0.0517,-0.0054,0.0288,-0.0181,-0.0119,0.0283,0.02,-0.0272,-0.0264,0.0108,-0.0582,0.0824,-0.0817,-0.0054,0.0511,-0.0104]}
{"key":"[DAEMA: Denoising Autoencoder with Mask Attention] Missing data is a recurrent and challenging problem, especially when using machine learning algorithms for real-world applications. For this reason, missing data imputation has become an active research area, in which recent deep learning approaches have achieved state-of-the-art results. We propose DAEMA (Denoising Autoencoder with Mask Attention), an algorithm based on a denoising autoencoder architecture with an attention mechanism. While most imputation algorithms use incomplete inputs as they would use complete data - up to basic preprocessing (e.g. mean imputation) - DAEMA leverages a mask-based attention mechanism to focus on the observed values of its inputs. We evaluate DAEMA both in terms of reconstruction capabilities and downstream prediction and show that it achieves superior performance to state-of-the-art algorithms on several publicly available real-world datasets under various missingness settings.","layer":0,"vector":[-0.0006,0.0022,0.0464,-0.0246,0.0553,0.0404,0.0145,0.0226,0.0129,-0.0057,0.0544,-0.0726,0.0656,0.0532,0.0367,-0.0062,0.0258,0.0535,-0.0494,-0.0274,-0.0043,-0.0327,0.0064,-0.0518,0.024,-0.0215,-0.0281,-0.0655,-0.0582,-0.252,0.0233,-0.0507,0.0556,-0.0,0.0208,-0.0778,-0.0603,0.0559,-0.0619,0.0363,-0.0025,0.0182,-0.0231,-0.0614,-0.0426,-0.0524,-0.0183,-0.0863,-0.0063,-0.0494,0.032,-0.0641,0.0068,0.0317,0.0495,0.0367,0.0512,0.0476,0.0537,0.0372,0.0351,0.0444,-0.1172,0.0403,0.0462,0.0319,-0.0374,-0.0536,0.0014,0.0411,-0.0172,0.0186,0.0181,0.0733,0.0014,0.0028,0.016,-0.0264,-0.022,0.0182,0.0488,-0.0361,-0.0098,-0.006,-0.0329,-0.0565,0.0313,-0.0412,0.0104,-0.0296,-0.0197,-0.0346,0.0002,0.0608,-0.0611,0.0271,0.0191,0.0468,-0.0723,0.2309,-0.0584,0.0284,0.032,-0.0762,0.0276,-0.0614,0.0053,-0.0171,0.0073,0.0068,-0.0005,-0.0604,0.0687,-0.0285,0.0389,0.0034,0.0683,0.033,-0.0058,-0.0047,0.0147,-0.0344,0.0731,0.0074,0.0411,-0.0265,0.0201,0.1763,0.0398,0.0405,0.0295,-0.048,-0.047,-0.0455,0.0208,-0.0091,0.0243,-0.0009,-0.0201,-0.0517,-0.0412,-0.0357,-0.0109,-0.0505,-0.0676,0.0907,-0.0339,0.0491,-0.0452,-0.0579,-0.0265,0.023,-0.0304,-0.0002,0.0544,0.0537,0.004,0.0343,-0.0546,-0.0161,0.0111,-0.0607,-0.0374,0.0731,0.001,-0.0637,-0.084,0.0139,-0.0024,0.0115,0.0153,-0.0064,-0.0305,0.0479,0.0962,0.0297,-0.0544,0.024,-0.025,0.0334,0.0214,-0.0245,-0.0251,0.0423,0.0196,-0.0626,0.0305,-0.021,-0.0101,0.055,-0.0432,0.0247,-0.0272,-0.0147,-0.0234,-0.033,-0.0289,-0.0406,-0.0348,-0.0535,0.02,0.0045,0.0101,0.0302,0.0077,0.0554,-0.0545,-0.026,0.0581,0.0341,-0.0221,-0.0084,0.0772,-0.0465,-0.0173,-0.005,0.0335,-0.0238,-0.0066,0.0433,0.0525,-0.0377,-0.0463,-0.2196,-0.0179,0.004,-0.0279,0.0144,-0.0889,0.0382,0.0035,0.1028,0.1022,0.0445,-0.0293,-0.0128,0.0534,-0.0396,0.0658,0.0329,0.045,-0.0515,-0.0084,0.0161,0.0426,0.0245,-0.0444,0.0331,0.0083,0.229,-0.0011,0.0349,-0.0501,-0.0466,0.0606,-0.0012,-0.1058,0.0412,-0.0061,0.0282,0.0022,-0.0376,-0.0106,-0.0187,0.0169,-0.0022,-0.0733,-0.0149,-0.0528,-0.0498,0.0293,-0.0802,0.0653,0.0518,-0.0356,0.0532,-0.0243,-0.0273,-0.0305,-0.112,0.0048,-0.0441,0.0447,0.0207,-0.0309,0.002,-0.0689,0.0212,0.0381,-0.029,-0.028,0.0221,-0.0204,-0.0209,0.1007,-0.0306,-0.0177,0.0506,0.0195,0.0386,-0.0381,-0.0437,-0.0477,0.0736,-0.0141,0.0589,0.0306,0.0853,0.0894,0.0982,0.0045,0.0407,-0.0199,-0.0168,0.0152,-0.0307,-0.0431,0.017,-0.0016,-0.2675,-0.0318,0.0254,0.0029,-0.0091,0.0021,0.0236,0.0197,-0.0207,-0.0299,-0.0603,0.0285,0.0406,-0.0456,0.0147,0.0476,0.0663,-0.0628,0.0675,-0.0197,-0.0022,0.0319,0.1763,-0.0325,0.0128,0.0271,-0.0224,-0.0061,0.0586,-0.031,-0.0003,0.0317,0.0606,-0.0336,-0.0054,0.0623,-0.0498,0.0665,0.013,-0.0283,-0.0228,-0.0143,0.0001,-0.0087,0.0679,-0.0137,-0.0194,-0.0219,0.0317,0.056,-0.021,0.0315,0.0232,0.028,0.034,0.0424,-0.0514,-0.0107,-0.0065,-0.0158,-0.0321,-0.0481,-0.049,-0.029,-0.0325]}
{"key":"[No News is Good News: A Critique of the One Billion Word Benchmark] The One Billion Word Benchmark is a dataset derived from the WMT 2011 News Crawl, commonly used to measure language modeling ability in natural language processing. We train models solely on Common Crawl web scrapes partitioned by year, and demonstrate that they perform worse on this task over time due to distributional shift. Analysis of this corpus reveals that it contains several examples of harmful text, as well as outdated references to current events. We suggest that the temporal nature of news and its distribution shift over time makes it poorly suited for measuring language modeling ability, and discuss potential impact and considerations for researchers building language models and evaluation datasets.","layer":1,"vector":[-0.0422,-0.0103,0.0332,-0.0055,0.0366,0.0219,0.0111,0.0139,0.0254,0.02,-0.0015,-0.0235,0.0204,0.0449,0.0185,0.0423,0.0241,-0.0553,-0.0368,-0.0063,0.0941,-0.0139,0.0139,-0.0137,0.0399,0.0156,-0.0561,-0.0346,-0.0467,-0.2323,-0.0227,-0.0614,0.0312,0.0042,0.0157,0.0012,-0.0454,0.0113,-0.0239,0.0457,0.0212,0.0495,-0.0192,-0.0485,-0.0161,-0.0774,-0.0283,-0.004,-0.0556,-0.0177,0.0084,-0.0627,0.0509,0.018,0.0625,0.031,0.0634,0.0235,0.0314,0.0246,0.0095,0.0277,-0.1952,0.0797,0.0231,0.0006,-0.0681,0.0021,-0.0115,0.0523,0.0172,0.0229,0.0693,0.0699,0.0243,0.0168,0.0232,-0.0182,0.0191,0.0073,-0.0037,-0.0605,-0.0512,-0.0606,-0.0188,-0.0554,-0.0035,-0.0111,0.0099,-0.0218,0.0003,-0.0035,-0.0035,0.0662,-0.0371,-0.0314,0.0213,0.0169,-0.0439,0.1975,-0.0575,0.0138,0.0222,-0.0953,0.0391,-0.0398,-0.0093,-0.0308,0.0022,-0.0181,-0.0055,-0.0015,0.0244,-0.0622,0.0845,-0.0074,0.0971,0.0079,-0.0359,0.027,-0.0824,0.0041,0.0489,-0.0395,0.0117,-0.0116,0.0814,0.1345,0.0718,-0.0026,0.0335,-0.0073,-0.0286,-0.0023,0.0237,0.0268,0.0131,0.0044,0.039,-0.0261,-0.0436,-0.08,-0.0336,-0.0644,-0.0727,0.1461,-0.0393,0.0409,-0.0443,0.0164,-0.0472,0.0487,0.0207,-0.0779,0.0302,0.0138,0.0164,0.0456,-0.0768,0.001,0.0402,-0.0497,-0.0541,0.0539,0.0151,-0.0407,-0.0438,0.0126,0.0174,-0.0332,0.084,0.0122,-0.0302,0.0364,0.0487,0.0177,-0.0335,-0.0158,0.0166,0.0254,0.0487,-0.0283,-0.017,0.0827,0.0145,-0.0579,-0.0196,-0.0387,0.0476,0.0552,-0.0358,0.0178,-0.0059,0.005,-0.0371,-0.0129,-0.0377,0.0106,0.0064,-0.0327,-0.0086,0.0045,-0.0452,-0.0071,0.0169,0.0005,-0.0157,0.0001,0.0454,0.0259,-0.0596,-0.0169,0.0333,-0.0431,0.0119,-0.0341,0.0232,0.011,0.0153,0.0434,-0.0105,-0.0437,-0.0146,-0.2401,-0.0078,0.0179,-0.0283,0.0906,-0.039,0.0339,-0.0271,0.0713,0.11,0.0025,-0.057,-0.0357,-0.0004,0.0014,0.0376,0.0185,0.0435,-0.0133,0.0358,0.0203,-0.0107,-0.0324,-0.1032,0.0246,-0.0352,0.2047,0.0548,0.0282,-0.0631,0.0608,0.0183,-0.0189,-0.1126,0.0607,0.038,0.0433,-0.0049,-0.0477,-0.0129,-0.0253,0.0104,0.019,-0.098,-0.0434,-0.016,0.0106,-0.0386,-0.0798,0.0722,-0.0143,-0.0474,0.0452,0.061,-0.0464,-0.0787,-0.0785,0.0367,-0.023,0.0057,0.0211,0.003,0.0323,-0.0408,0.0594,-0.0035,-0.0152,-0.079,-0.0132,-0.0248,-0.0518,0.1071,-0.0261,-0.0388,0.0238,0.0337,-0.0159,-0.0547,-0.0339,-0.0663,0.0827,-0.009,0.0786,0.0258,0.0225,-0.0015,0.0836,-0.0337,0.0338,-0.0121,0.0115,0.0227,-0.0822,-0.0289,0.052,-0.0109,-0.2711,0.0445,0.0227,0.028,0.0261,0.0004,0.0224,0.0458,-0.014,0.0226,0.0173,0.0835,0.0455,-0.0448,-0.007,0.0296,0.0442,-0.0281,0.005,-0.0141,0.0363,0.0408,0.2062,0.0127,0.0131,0.0138,-0.0337,0.0171,0.01,-0.0251,-0.0318,-0.0187,0.0685,-0.016,0.0069,0.0517,-0.0211,0.0285,0.0266,-0.0018,-0.0142,0.0462,0.0118,-0.0367,0.0895,-0.015,0.0287,-0.0932,-0.011,-0.001,-0.0594,0.0217,-0.0093,0.0252,0.013,0.0454,-0.0104,-0.0441,-0.0386,-0.0521,0.002,-0.0605,-0.0263,0.0424,-0.0172]}
{"key":"[A robust kernel machine regression towards biomarker selection in multi-omics datasets of osteoporosis for drug discovery] Many statistical machine approaches could ultimately highlight novel features of the etiology of complex diseases by analyzing multi-omics data. However, they are sensitive to some deviations in distribution when the observed samples are potentially contaminated with adversarial corrupted outliers (e.g., a fictional data distribution). Likewise, statistical advances lag in supporting comprehensive data-driven analyses of complex multi-omics data integration. We propose a novel non-linear M-estimator-based approach, \"robust kernel machine regression (RobKMR),\" to improve the robustness of statistical machine regression and the diversity of fictional data to examine the higher-order composite effect of multi-omics datasets. We address a robust kernel-centered Gram matrix to estimate the model parameters accurately. We also propose a robust score test to assess the marginal and joint Hadamard product of features from multi-omics data. We apply our proposed approach to a multi-omics dataset of osteoporosis (OP) from Caucasian females. Experiments demonstrate that the proposed approach effectively identifies the inter-related risk factors of OP. With solid evidence (p-value = 0.00001), biological validations, network-based analysis, causal inference, and drug repurposing, the selected three triplets ((DKK1, SMTN, DRGX), (MTND5, FASTKD2, CSMD3), (MTND5, COG3, CSMD3)) are significant biomarkers and directly relate to BMD. Overall, the top three selected genes (DKK1, MTND5, FASTKD2) and one gene (SIDT1 at p-value= 0.001) significantly bond with four drugs- Tacrolimus, Ibandronate, Alendronate, and Bazedoxifene out of 30 candidates for drug repurposing in OP. Further, the proposed approach can be applied to any disease model where multi-omics datasets are available.","layer":1,"vector":[-0.0493,-0.0567,0.0099,-0.0002,0.0413,-0.0055,0.0199,0.0711,-0.0048,-0.0427,0.0332,-0.0754,0.0067,0.0435,0.0087,0.009,0.0103,0.0534,-0.0704,0.0266,0.0197,-0.0459,0.0338,-0.0594,0.0545,0.0319,-0.0247,-0.0511,-0.0465,-0.2448,0.0098,-0.0399,0.0869,-0.0754,0.0489,0.0219,-0.0552,0.0408,-0.0148,0.0111,0.0272,-0.0124,-0.008,-0.014,-0.0042,-0.0311,-0.0285,-0.0244,-0.0235,-0.0078,0.0282,-0.0199,0.0236,0.0786,0.0558,0.0091,0.038,0.0452,0.0497,0.0653,0.024,0.0745,-0.1784,0.0461,0.0974,0.0158,-0.0295,-0.02,0.0273,0.0941,-0.0102,0.0399,0.0339,0.0445,-0.0147,0.0124,0.0326,-0.0364,-0.023,0.0433,0.0476,0.0034,0.0004,-0.035,-0.0362,-0.0416,-0.0085,-0.0644,0.0246,0.0076,-0.0039,-0.0189,0.0081,0.0125,-0.071,-0.039,0.0501,0.0335,-0.0417,0.2048,-0.0521,0.0386,-0.0075,-0.0315,0.0414,-0.0924,-0.0293,-0.0318,0.0144,0.0019,0.0225,-0.0128,0.046,-0.0321,0.0321,-0.0007,0.0656,0.0388,0.0004,-0.0121,-0.0083,-0.0001,0.0519,-0.0106,0.0462,-0.0176,0.0011,0.1265,0.0587,-0.048,0.0258,-0.0155,-0.0615,-0.0157,0.0093,-0.0152,0.0094,0.0144,0.0331,-0.0041,-0.0172,-0.0922,0.0349,-0.0706,-0.0608,0.1553,-0.0375,0.006,-0.0395,-0.0194,-0.0177,0.0424,-0.029,0.0003,0.001,-0.0061,0.0501,-0.0065,-0.0572,-0.0013,-0.0541,-0.0352,-0.0431,0.082,0.0033,-0.0817,-0.0064,0.0532,0.0268,0.0213,0.0832,-0.0091,-0.0256,0.0242,0.0452,0.0122,-0.0765,0.0296,-0.0235,-0.0382,0.0117,-0.0134,-0.0235,0.0517,0.0225,-0.0617,-0.035,-0.0013,0.0239,0.0208,-0.0027,0.0327,-0.0348,-0.0145,-0.0536,-0.0294,-0.035,0.0113,0.0387,-0.0432,0.0179,0.0096,0.0024,0.004,-0.0162,0.0298,-0.0537,0.004,0.0532,0.0175,-0.0075,-0.0227,0.0496,0.0123,-0.0581,-0.0072,0.0025,0.0347,0.0261,0.0431,0.0711,-0.0385,-0.0736,-0.2251,-0.0105,0.0278,0.0406,0.0013,-0.0492,0.0759,-0.048,0.0359,0.0741,0.0251,0.0396,-0.0549,0.0226,-0.0359,0.0361,0.0196,0.0064,-0.0221,-0.0436,-0.0135,0.0043,-0.0083,-0.0368,0.0679,0.0052,0.1649,0.0649,-0.0062,0.0013,0.0443,0.0282,-0.0098,-0.1041,0.072,0.018,0.0907,-0.0275,-0.0735,-0.0196,-0.0332,0.0136,0.0236,-0.0861,-0.0473,-0.0413,-0.0299,0.0165,-0.0209,0.0713,0.0402,-0.0048,0.043,-0.0087,0.0069,-0.01,-0.1187,0.0755,-0.016,-0.0189,0.0411,-0.0861,0.0109,-0.0607,0.0442,-0.0337,-0.0448,-0.015,0.0434,-0.0536,-0.0048,0.0896,-0.0145,0.0219,0.0697,0.0396,-0.0273,-0.0638,-0.0516,0.0072,0.059,-0.0209,-0.0028,0.015,0.0562,-0.0284,0.0639,0.0501,0.0374,-0.0405,-0.0192,-0.0098,-0.0613,-0.0415,0.058,0.0202,-0.2929,0.0396,-0.01,-0.0028,-0.0223,-0.0485,0.012,-0.0129,-0.0152,-0.0009,0.0157,0.0681,0.0443,-0.0356,-0.013,0.0108,0.1244,-0.0615,0.0107,-0.0484,0.0339,0.0315,0.1759,-0.0444,0.0197,0.0542,0.0009,0.0296,-0.0057,-0.0364,0.0253,0.0134,0.0398,-0.0111,0.012,0.1002,-0.0637,0.0086,-0.0056,-0.028,0.0292,-0.0051,-0.0543,-0.0328,0.1063,-0.0566,-0.0278,-0.0299,0.022,0.0304,-0.0388,-0.0056,-0.0168,0.0264,0.0036,0.0141,0.0039,-0.0447,-0.0533,-0.0678,-0.0252,-0.021,-0.086,0.0321,-0.0425]}
{"key":"[Topic Analysis of Superconductivity Literature by Semantic Non-negative Matrix Factorization] We utilize a recently developed topic modeling method called SeNMFk, extending the standard Non-negative Matrix Factorization (NMF) methods by incorporating the semantic structure of the text, and adding a robust system for determining the number of topics. With SeNMFk, we were able to extract coherent topics validated by human experts. From these topics, a few are relatively general and cover broad concepts, while the majority can be precisely mapped to specific scientific effects or measurement techniques. The topics also differ by ubiquity, with only three topics prevalent in almost 40 percent of the abstract, while each specific topic tends to dominate a small subset of the abstracts. These results demonstrate the ability of SeNMFk to produce a layered and nuanced analysis of large scientific corpora.","layer":1,"vector":[-0.0385,0.0261,-0.0227,-0.011,0.0326,0.0143,0.0315,0.0493,-0.0219,-0.0118,-0.0222,-0.0656,0.0582,0.0359,0.0486,0.0256,0.0458,-0.0046,-0.0201,0.0392,0.0449,-0.0144,-0.0059,-0.0419,0.0333,0.033,-0.013,-0.0199,-0.0647,-0.2383,-0.0759,-0.0142,0.0798,0.021,0.0027,-0.013,-0.0191,0.0371,0.0121,0.0185,-0.0033,-0.0169,0.0089,-0.0047,-0.0561,-0.0336,-0.0349,-0.0132,-0.0076,-0.0298,0.0184,-0.0327,-0.0066,0.0326,0.0372,0.033,0.0447,-0.0185,0.0508,0.0183,0.0453,0.0534,-0.1828,0.0749,0.0969,-0.0386,-0.0257,0.007,0.0199,0.0433,-0.0241,0.0512,0.0204,0.061,0.0327,0.0282,0.0065,-0.0131,-0.009,-0.0037,-0.0175,-0.0087,-0.0608,-0.0163,-0.0207,-0.0213,-0.0092,-0.0089,0.0093,0.0077,-0.0191,-0.0279,0.0006,0.0206,-0.0859,-0.0202,0.0141,-0.021,-0.0041,0.1939,-0.0413,0.0306,0.044,-0.0452,-0.0017,-0.0047,-0.0229,0.0009,0.0015,-0.0133,0.0365,0.0088,0.0061,-0.0809,0.1082,0.0024,0.1086,0.0408,-0.0113,0.0108,-0.0559,0.0543,0.0158,-0.0185,0.0533,-0.0696,0.0317,0.1015,0.044,-0.0064,0.0629,0.0019,-0.0765,-0.0264,0.0207,0.017,-0.0217,0.0161,0.0209,0.0186,-0.014,-0.0457,-0.0088,-0.0954,-0.081,0.1118,-0.0225,-0.0117,-0.058,0.0374,0.0099,0.0063,-0.0205,-0.0368,0.0467,0.023,0.0162,0.0066,-0.0425,0.0144,-0.0006,-0.0648,-0.0244,0.1393,0.0228,-0.0805,-0.0186,0.0131,0.0019,-0.0502,0.0239,0.0363,-0.014,0.0595,0.0872,0.0172,-0.0272,0.0031,0.023,-0.0037,0.0495,-0.0323,-0.0712,0.0407,0.0191,-0.0697,-0.0236,-0.0583,0.0052,0.0335,-0.0646,0.021,-0.0436,0.0391,-0.0073,-0.0087,-0.0169,-0.0022,0.0032,-0.0101,0.0248,0.0003,-0.0659,0.0317,-0.0263,0.0215,0.0007,0.0065,0.0221,0.0092,-0.0516,-0.0467,0.0157,-0.0198,-0.0433,-0.0214,0.0248,0.0069,0.0091,0.046,0.0919,-0.0776,-0.0825,-0.2491,-0.0284,0.0206,-0.0262,0.0495,-0.0302,0.0202,-0.0348,0.0358,0.1113,0.0356,-0.0043,0.0057,-0.0159,-0.0271,0.0442,0.0229,-0.0062,-0.0264,-0.0242,-0.0156,0.0416,-0.0242,-0.0359,0.0025,-0.0505,0.1963,0.0441,-0.0248,-0.0084,0.0603,0.0107,-0.0134,-0.1058,0.0334,0.005,0.0772,0.01,-0.0386,-0.02,-0.0647,0.0248,-0.0068,-0.0663,-0.0597,-0.0241,-0.0144,-0.0085,-0.0592,0.0258,0.0412,-0.0164,0.0303,0.0169,-0.0486,-0.0483,-0.0595,0.0229,-0.037,-0.0148,-0.0002,0.0005,0.0353,-0.0779,0.005,0.0451,-0.0372,0.0541,0.0189,-0.0732,-0.0082,0.1216,-0.0044,0.0058,0.0321,-0.0134,0.0595,-0.053,-0.0395,-0.0013,0.1111,-0.0123,0.0349,-0.0084,0.0211,0.0181,0.0761,-0.0334,0.0046,-0.0009,0.0155,-0.0022,-0.0796,0.0178,0.0269,-0.0288,-0.2959,0.0298,0.0427,0.0076,-0.0005,0.0137,0.027,0.0103,-0.0319,0.0197,-0.004,0.0569,0.0025,-0.012,-0.0331,0.0179,0.0521,-0.0413,0.0306,-0.0221,0.035,0.0457,0.2616,-0.0133,0.0544,0.0175,-0.0189,0.0257,0.0099,-0.0439,0.0248,-0.0079,0.0897,-0.0436,0.0569,0.0426,-0.0037,-0.0038,0.0311,-0.0061,-0.0325,0.0547,-0.0236,-0.0516,0.1257,-0.0396,0.003,-0.0817,-0.001,-0.0285,-0.0373,0.027,0.0121,0.0048,0.0025,-0.0054,-0.0185,-0.0804,0.0127,-0.0197,-0.0362,-0.0434,-0.0537,0.0359,0.0061]}
{"key":"[Is Self-Supervised Learning More Robust Than Supervised Learning?] Self-supervised contrastive learning is a powerful tool to learn visual representation without labels. Prior work has primarily focused on evaluating the recognition accuracy of various pre-training algorithms, but has overlooked other behavioral aspects. In addition to accuracy, distributional robustness plays a critical role in the reliability of machine learning models. We design and conduct a series of robustness tests to quantify the behavioral differences between contrastive learning and supervised learning to downstream or pre-training data distribution changes. These tests leverage data corruptions at multiple levels, ranging from pixel-level gamma distortion to patch-level shuffling and to dataset-level distribution shift. Our tests unveil intriguing robustness behaviors of contrastive and supervised learning. On the one hand, under downstream corruptions, we generally observe that contrastive learning is surprisingly more robust than supervised learning. On the other hand, under pre-training corruptions, we find contrastive learning vulnerable to patch shuffling and pixel intensity change, yet less sensitive to dataset-level distribution change. We attempt to explain these results through the role of data augmentation and feature space properties. Our insight has implications in improving the downstream robustness of supervised learning.","layer":4,"vector":[-0.0249,-0.0533,-0.0113,-0.0067,0.041,0.0126,0.0221,0.0173,0.0036,0.0072,0.0252,-0.0465,0.0294,0.0649,-0.0162,0.0108,0.0054,0.0237,-0.0552,0.0196,-0.0212,-0.0553,-0.0247,-0.0426,0.0455,-0.0227,-0.0288,-0.0194,-0.013,-0.2912,0.0108,-0.0231,-0.0044,-0.0216,0.0207,-0.0507,-0.0351,0.0512,-0.0331,0.0131,0.0391,-0.001,-0.0146,-0.0846,-0.0734,-0.0762,0.0201,-0.0164,-0.026,-0.0359,0.002,-0.0425,0.044,0.0659,0.0287,0.0416,0.0888,0.0709,0.0465,0.0371,0.0318,0.0892,-0.15,0.0657,0.0354,0.0563,-0.0836,-0.0123,0.039,0.0182,0.0302,0.04,0.012,0.0296,0.0064,-0.02,0.013,-0.0183,0.0181,0.0053,0.0232,-0.041,-0.044,-0.0143,-0.0379,-0.037,0.0198,-0.0568,0.0258,0.0365,-0.0081,-0.006,-0.045,0.0444,-0.0193,-0.03,0.0524,-0.0156,-0.0106,0.2077,-0.0482,0.0353,0.0404,-0.0089,0.0216,-0.0611,0.0063,-0.02,-0.0472,-0.005,0.0026,-0.0218,0.0145,-0.0248,0.015,0.0044,0.0817,-0.0107,-0.0052,0.0156,-0.0268,0.0157,0.0653,-0.0155,0.0254,-0.0266,0.0411,0.1336,-0.0129,-0.0163,0.001,-0.0382,-0.0157,-0.0089,-0.0096,0.0359,0.0252,0.007,0.0255,-0.0079,-0.0238,-0.0017,0.0031,-0.0844,-0.0659,0.1084,-0.0708,0.0694,-0.0103,-0.0198,-0.0445,0.0085,-0.043,-0.0152,0.0065,0.0575,-0.012,0.0619,-0.0428,-0.0279,-0.0031,-0.0836,0.0014,0.071,-0.0019,-0.0274,-0.0152,0.0213,0.0341,0.0027,0.0341,0.0185,-0.0281,0.0562,0.0181,-0.0018,-0.0993,-0.0133,0.0045,0.0343,0.035,-0.028,-0.0023,0.0761,0.0689,-0.0493,-0.015,-0.0239,0.0483,0.071,-0.0032,0.0096,-0.0415,-0.0348,-0.0499,0.0258,-0.0102,-0.0213,-0.0139,0.0052,-0.0097,0.0143,-0.0168,0.0221,0.0128,-0.0024,0.0094,-0.0383,0.0452,0.0299,-0.0532,-0.0062,0.0643,-0.0116,-0.0436,-0.0037,0.0226,0.0576,0.0006,0.0179,0.0229,-0.0486,-0.0325,-0.2212,-0.0367,-0.0163,-0.0081,0.0447,-0.1034,0.044,-0.0077,0.0951,0.0639,0.0348,-0.0082,-0.0296,-0.0101,0.003,0.024,0.0423,0.0283,0.0086,-0.0022,-0.0683,0.0624,-0.0212,-0.0583,0.051,-0.0038,0.2043,0.0145,0.0002,-0.037,0.0231,0.0163,-0.0758,-0.1031,0.068,0.0026,0.0619,-0.0299,-0.0474,-0.033,-0.0148,0.0027,0.0237,-0.0977,-0.0307,-0.0342,-0.0418,0.0222,-0.0966,0.0678,0.0454,-0.034,0.0238,-0.0281,0.0102,-0.0058,-0.0735,0.0545,-0.0654,0.0036,-0.0004,-0.0872,0.0194,-0.0753,0.0332,0.0421,-0.004,-0.0632,0.042,-0.0328,-0.0002,0.0949,-0.0192,-0.0367,0.0413,-0.0037,0.0087,-0.0137,-0.0645,-0.0278,0.076,0.0381,0.0699,0.0003,0.053,0.0456,0.029,0.0382,0.0165,-0.0225,0.0124,0.0246,-0.0458,-0.0584,0.0669,0.0324,-0.2787,0.0251,0.0048,0.0496,-0.0116,0.0467,0.0326,0.0039,-0.0175,-0.0271,-0.0272,0.066,0.0327,-0.064,0.0136,0.0187,0.0915,-0.0523,0.0891,-0.0508,0.0377,0.0585,0.2115,-0.044,-0.0005,0.0228,-0.0118,0.0182,-0.0265,-0.0578,0.0116,0.013,0.0909,-0.0315,0.0156,0.0972,-0.0553,0.0274,-0.0065,0.0015,-0.0087,0.0213,-0.039,-0.062,0.0971,-0.039,0.0315,0.0067,-0.0359,0.0,-0.0376,0.0068,-0.0019,0.006,0.0045,0.0313,-0.0765,-0.024,-0.0422,-0.0266,0.063,-0.0275,-0.0081,0.0044,-0.0222]}
{"key":"[Increasing Expressivity of a Hyperspherical VAE] Learning suitable latent representations for observed, high-dimensional data is an important research topic underlying many recent advances in machine learning. While traditionally the Gaussian normal distribution has been the go-to latent parameterization, recently a variety of works have successfully proposed the use of manifold-valued latents. In one such work (Davidson et al., 2018), the authors empirically show the potential benefits of using a hyperspherical von Mises-Fisher (vMF) distribution in low dimensionality. However, due to the unique distributional form of the vMF, expressivity in higher dimensional space is limited as a result of its scalar concentration parameter leading to a 'hyperspherical bottleneck'. In this work we propose to extend the usability of hyperspherical parameterizations to higher dimensions using a product-space instead, showing improved results on a selection of image datasets.","layer":4,"vector":[0.0092,-0.0073,0.0215,-0.01,0.0187,0.0243,0.0395,0.0237,0.0289,-0.002,0.0062,-0.0909,0.0267,0.0309,0.0233,-0.0029,0.0331,0.0488,-0.0774,0.033,0.0302,-0.0337,-0.0212,-0.0596,0.0327,-0.0068,-0.0084,-0.0704,-0.0228,-0.2758,0.0378,-0.0506,0.0433,-0.0002,0.0145,-0.0437,-0.026,-0.0082,-0.016,0.0157,0.0043,0.0421,-0.0218,-0.028,-0.0548,-0.051,-0.0137,-0.0578,-0.0451,-0.026,0.0157,-0.0066,0.0254,0.0503,0.0535,0.0302,0.0578,0.0224,0.042,0.052,0.0343,0.0373,-0.1379,0.0015,0.0583,0.0114,-0.0782,-0.0321,0.0429,0.0214,-0.0313,0.0007,0.0132,0.0482,0.0511,0.0063,0.0419,-0.0576,-0.0099,-0.0061,0.0288,-0.0107,-0.0094,-0.0064,-0.0299,-0.0439,0.0239,-0.0621,0.0481,-0.0345,-0.0589,-0.0387,-0.0726,0.0154,-0.0407,-0.0381,0.0271,-0.0035,0.0013,0.1905,-0.0178,0.0158,0.0473,-0.0347,0.0118,-0.0444,-0.074,0.0383,-0.0246,0.005,-0.0017,-0.0388,0.0191,-0.0332,-0.0039,-0.0238,0.0521,0.0634,0.0174,-0.0212,-0.0056,-0.0077,0.0325,-0.0129,0.0148,-0.041,0.0158,0.1152,0.0464,0.0252,0.0353,0.0078,-0.0186,-0.0306,0.0111,-0.006,0.0567,0.0174,0.0088,0.0307,-0.0144,-0.0546,-0.0271,-0.0332,-0.0318,0.1547,-0.0655,0.0244,-0.0269,0.0179,0.0011,0.0148,-0.0237,-0.0357,0.0442,0.0224,-0.0013,0.03,-0.0728,0.0407,-0.0414,-0.0691,-0.0074,0.0913,-0.0282,-0.0408,-0.0432,-0.0191,0.0581,0.0248,0.0311,0.0264,-0.0522,0.0236,0.1147,0.007,-0.0874,-0.0219,0.0347,-0.0041,0.0273,-0.0494,-0.0222,0.0298,0.0452,-0.0538,0.0214,-0.047,0.008,0.0361,-0.0174,-0.0419,-0.0322,-0.0091,0.0007,-0.013,-0.0166,-0.007,0.033,-0.022,0.0178,0.0271,-0.044,0.0663,-0.0092,0.0407,0.0256,-0.0201,0.0508,0.0324,-0.0033,0.0189,0.0468,-0.0094,-0.0212,-0.0358,0.021,0.0584,0.0176,0.0139,0.0285,-0.0718,-0.0796,-0.2304,-0.0153,0.013,-0.0627,0.0446,-0.0797,0.0445,0.0058,0.0494,0.087,0.0336,-0.0356,-0.0312,0.0453,0.0037,0.0554,0.0289,0.0586,-0.0286,0.0012,-0.0058,0.0228,-0.0552,-0.0684,0.075,-0.0232,0.2008,0.0419,-0.0164,-0.0448,0.0481,-0.0088,-0.0229,-0.0516,0.023,-0.0166,0.0553,-0.008,-0.0399,-0.0275,-0.0248,0.0077,0.0111,-0.1155,-0.0053,-0.0103,-0.0317,0.0377,-0.036,0.0438,0.0677,-0.0624,0.04,-0.0295,0.049,-0.0379,-0.0871,0.0019,-0.038,0.0495,-0.0046,-0.0768,0.0147,-0.0422,0.0381,-0.0249,-0.0366,-0.0482,0.0467,-0.0151,-0.0658,0.0908,-0.0128,0.0193,0.1058,0.0243,0.0174,0.0255,-0.024,-0.0119,0.0804,0.0081,0.0086,0.0013,0.0728,-0.0124,0.0884,-0.0313,-0.0052,-0.0376,0.0228,0.0033,-0.0411,0.0233,0.0525,-0.0019,-0.2942,0.036,0.0526,0.0583,-0.0441,-0.0076,0.0115,-0.021,-0.0319,-0.0563,-0.0138,0.0373,0.0344,0.0071,0.0113,0.0202,0.0614,-0.05,0.0813,-0.0296,0.0406,0.0095,0.2317,-0.002,0.0002,0.0082,-0.0147,-0.0088,0.0274,-0.0431,0.038,0.0159,0.1168,-0.0508,0.0564,0.0844,-0.0578,0.0343,-0.0142,-0.0575,0.0566,-0.0078,-0.0396,-0.0175,0.1088,-0.0179,0.0087,-0.0831,-0.0018,0.021,-0.0241,-0.0221,-0.0352,0.0314,-0.0028,0.0157,-0.0037,-0.0917,-0.0098,-0.0492,-0.0105,-0.0325,-0.0363,0.0066,-0.0108]}
{"key":"[Finding the creatures of habit; Clustering households based on their flexibility in using electricity] Changes in the UK electricity market, particularly with the roll out of smart meters, will provide greatly increased opportunities for initiatives intended to change households' electricity usage patterns for the benefit of the overall system. Users show differences in their regular behaviours and clustering households into similar groupings based on this variability provides for efficient targeting of initiatives. Those people who are stuck into a regular pattern of activity may be the least receptive to an initiative to change behaviour. A sample of 180 households from the UK are clustered into four groups as an initial test of the concept and useful, actionable groupings are found.","layer":2,"vector":[0.0158,0.0024,0.0358,0.0085,0.0334,0.0194,0.0895,0.0013,0.0235,-0.0034,0.0114,-0.0717,-0.0036,-0.0123,-0.0019,0.0023,-0.0199,0.0008,0.0072,0.0342,-0.0003,-0.0556,-0.0223,-0.0194,0.0587,0.0361,0.0013,-0.0117,-0.048,-0.1813,0.0011,-0.0064,0.0558,-0.016,-0.0327,-0.0421,-0.0169,0.0504,-0.0245,0.0451,0.0151,0.0006,0.0141,-0.0903,-0.0691,-0.0315,-0.008,0.0055,-0.0136,-0.0492,0.0221,-0.0514,-0.0057,0.0386,0.068,0.0244,0.0435,0.0357,0.066,-0.0142,0.0144,0.0743,-0.2099,0.0854,0.0463,0.0262,-0.0005,-0.0128,0.0029,0.027,-0.0353,0.0409,0.0125,0.0194,0.0377,-0.0054,-0.0404,-0.0282,-0.0305,-0.0001,-0.0044,-0.0212,-0.0068,-0.0081,-0.0099,-0.0337,0.008,-0.0744,0.074,0.0007,-0.0466,0.0435,-0.0323,0.016,-0.0617,-0.0721,0.021,-0.0042,-0.0537,0.1952,-0.0256,0.0622,0.0628,0.005,0.0144,-0.0571,-0.0244,-0.0242,-0.0215,-0.0166,-0.0212,-0.0254,0.0108,-0.0679,0.0561,-0.0236,0.0146,0.0518,0.0805,0.022,-0.0292,0.0677,0.0592,-0.0093,0.0584,-0.0311,0.0532,0.1248,-0.0145,0.0186,0.0347,-0.0143,-0.0608,0.0258,0.0179,0.0405,0.0306,0.0082,0.0161,0.0298,-0.0078,-0.0276,0.0119,-0.1582,0.0237,0.1154,-0.0029,0.0344,-0.0553,0.0628,0.0073,0.008,-0.0334,-0.0805,0.0238,0.0323,0.0908,0.0319,-0.0264,0.0031,-0.0328,0.0024,-0.0489,0.0747,0.0419,-0.0841,-0.0123,-0.0283,-0.0086,-0.0057,0.055,0.0653,-0.0694,0.0561,0.082,0.0387,-0.0384,-0.004,-0.0257,0.0058,0.0349,0.0002,-0.037,-0.0067,0.0326,-0.0382,-0.0272,-0.0261,0.0422,0.0482,-0.0458,0.0081,-0.014,-0.0097,0.0157,-0.0636,-0.0278,-0.0189,0.0068,-0.0031,-0.0297,0.0029,-0.0262,0.0455,0.0169,0.0683,-0.0317,-0.0484,0.0871,0.0038,0.0124,0.0161,0.0876,-0.0252,-0.077,0.0159,0.0313,0.0308,0.0187,0.0405,0.0129,-0.0223,-0.0641,-0.2433,-0.0396,0.0275,-0.0329,0.0278,-0.0256,0.0266,-0.0401,-0.0018,0.0535,0.0367,-0.0759,-0.0428,0.051,-0.0172,0.069,0.0258,-0.0152,-0.0231,0.0451,-0.0023,0.0135,0.0027,-0.0929,0.025,0.0029,0.236,0.0375,-0.0153,-0.0043,0.0137,0.0138,-0.0423,-0.1298,0.0134,0.0646,0.0336,-0.0122,-0.013,-0.0665,-0.0552,0.041,-0.0125,-0.0575,-0.0449,-0.0232,-0.0058,0.0176,-0.0572,-0.0145,0.0452,-0.0299,0.0362,-0.0071,-0.017,-0.052,-0.0459,0.0685,-0.0199,0.0197,-0.0225,-0.0218,0.0298,-0.0474,0.0595,-0.037,-0.0412,0.0031,-0.0101,-0.0029,-0.0125,0.0973,-0.0162,-0.0609,-0.0035,-0.0155,-0.0055,-0.0126,-0.0272,-0.0163,0.0464,-0.0429,0.0175,0.0224,0.009,-0.0203,0.01,-0.0181,0.0576,-0.0208,0.0114,0.002,-0.0447,-0.018,0.0263,-0.0001,-0.2358,0.0484,-0.0129,0.0134,-0.0237,0.0367,-0.0151,0.0472,0.0048,-0.0246,-0.0075,0.0524,0.0204,0.0151,0.0379,0.0336,0.0332,-0.0489,0.0281,-0.1072,0.0336,0.0566,0.2515,-0.0629,0.0347,0.026,0.0039,0.0335,0.02,-0.0284,-0.013,-0.0126,0.0997,-0.0244,0.0288,-0.0113,-0.0136,-0.0239,0.0388,-0.0498,-0.0577,0.0414,-0.0612,-0.0266,0.1598,-0.018,-0.0517,-0.0904,0.017,0.0395,-0.0481,-0.0047,-0.0691,0.0292,0.0412,0.0424,-0.072,-0.017,-0.0205,-0.0689,0.0212,-0.0332,0.0068,-0.0235,0.0324]}
{"key":"[Learnt Sparsification for Interpretable Graph Neural Networks] Graph neural networks (GNNs) have achieved great success on various tasks and fields that require relational modeling. GNNs aggregate node features using the graph structure as inductive biases resulting in flexible and powerful models. However, GNNs remain hard to interpret as the interplay between node features and graph structure is only implicitly learned. In this paper, we propose a novel method called Kedge for explicitly sparsifying the underlying graph by removing unnecessary neighbors. Our key idea is based on a tractable method for sparsification using the Hard Kumaraswamy distribution that can be used in conjugation with any GNN model. Kedge learns edge masks in a modular fashion trained with any GNN allowing for gradient based optimization in an end-to-end fashion. We demonstrate through extensive experiments that our model Kedge can prune a large proportion of the edges with only a minor effect on the test accuracy. Specifically, in the PubMed dataset, Kedge learns to drop more than 80% of the edges with an accuracy drop of merely 2% showing that graph structure has only a small contribution in comparison to node features. Finally, we also show that Kedge effectively counters the over-smoothing phenomena in deep GNNs by maintaining good task performance with increasing GNN layers.","layer":5,"vector":[0.0113,-0.0055,0.0012,0.0466,0.0497,0.009,0.0282,0.0033,0.0139,-0.0338,0.0078,-0.0623,0.0865,0.0387,0.0319,0.0353,-0.0155,0.0905,-0.0349,-0.0253,0.0148,-0.0436,-0.0127,-0.0592,0.0487,0.0184,-0.0174,-0.044,-0.0678,-0.2568,0.0144,-0.0747,0.0247,-0.0082,0.0009,-0.0298,0.0232,0.009,-0.0611,0.027,0.0199,-0.0078,-0.0513,-0.0325,-0.021,0.0013,0.0008,-0.0285,-0.026,-0.0612,0.0094,-0.0436,0.0455,0.012,0.0487,0.0142,0.0403,0.0463,0.0578,0.0729,0.0069,0.0453,-0.134,0.046,0.029,0.045,-0.0619,-0.0065,0.0291,0.0965,0.0232,0.0287,-0.001,0.0253,-0.0025,0.0035,0.0151,-0.0085,0.0207,0.009,0.0332,-0.0492,-0.0157,-0.0258,0.0145,-0.0474,0.0235,-0.0857,0.0302,0.0193,-0.0164,-0.0308,-0.0348,0.0207,-0.0368,-0.0196,0.0447,0.0228,-0.0858,0.2108,-0.0396,-0.0036,0.0324,-0.0169,0.0027,-0.0327,-0.0267,-0.0427,-0.0176,-0.019,-0.0634,-0.0156,-0.0071,-0.052,0.0077,0.0069,0.0759,0.0707,-0.0261,-0.0309,-0.0212,0.0562,-0.0099,-0.0278,0.0475,-0.0414,-0.011,0.0918,0.0701,0.0271,0.0372,-0.0151,-0.01,-0.0172,0.0164,-0.0041,0.0314,0.0054,-0.0254,0.0171,-0.0373,-0.0503,0.0172,-0.0431,-0.0622,0.1186,-0.0718,-0.0273,-0.0452,-0.0313,-0.0276,0.0119,-0.0314,-0.0152,0.0248,0.0052,0.0064,0.0336,-0.049,0.026,0.0173,-0.0286,-0.0992,0.087,0.083,-0.1115,-0.0265,-0.0247,0.0055,-0.0277,0.0411,0.0328,-0.0293,0.0454,0.069,0.0321,-0.0631,-0.0092,-0.0042,-0.0047,0.053,-0.0233,-0.0602,0.0552,0.0292,-0.0366,-0.0083,-0.0202,-0.0015,0.0327,-0.0464,0.0396,-0.0349,-0.0029,0.0022,-0.0101,-0.0451,-0.0308,-0.0175,-0.0016,0.0025,-0.0426,-0.0213,0.0342,-0.0427,0.0354,-0.0052,0.0153,0.0197,0.033,-0.0103,-0.0383,0.0402,-0.0307,0.0278,0.0297,-0.0015,0.0213,-0.0064,0.0486,0.039,-0.0411,-0.0499,-0.2123,-0.0239,0.014,-0.0402,0.0761,-0.0849,0.01,0.045,0.0661,0.0792,0.0505,-0.0205,-0.0277,0.0121,0.0291,0.0572,0.0286,0.0343,-0.0064,-0.0095,-0.0028,0.0142,0.0132,-0.0762,0.0291,0.0583,0.2689,-0.0094,0.0299,-0.006,0.015,0.0651,-0.0257,-0.0954,0.0641,0.0257,0.0293,0.0239,-0.0444,-0.0482,-0.0574,-0.0059,0.0207,-0.1176,-0.0335,-0.018,-0.0075,-0.0114,-0.0502,0.0303,0.0125,-0.0066,0.0949,0.0269,0.0028,-0.0398,-0.1131,0.0374,-0.0566,0.0423,0.0242,-0.0536,-0.0153,-0.0685,0.0763,0.0241,-0.0129,-0.0461,0.0302,-0.0047,0.0114,0.0568,0.0247,-0.0277,0.0557,0.0206,0.0368,-0.0095,-0.0501,-0.0121,0.0273,-0.0088,0.0447,-0.0312,0.0363,0.0141,0.0681,-0.0021,0.048,-0.0068,0.0171,0.0237,-0.0316,-0.0286,0.0675,-0.0422,-0.3081,0.0437,0.0164,0.0541,-0.024,0.0293,0.0433,0.0215,-0.0489,-0.0226,0.002,0.0286,0.0129,-0.0083,-0.0197,0.0551,0.0464,-0.013,0.0473,-0.0285,0.0553,0.0217,0.2249,-0.0156,0.0203,0.0582,-0.0355,-0.0311,0.0342,-0.0013,0.0068,-0.0018,0.0689,-0.0366,0.0184,0.0837,-0.0457,0.0599,0.0257,-0.0197,0.0189,-0.0071,-0.055,-0.038,0.0745,-0.0587,-0.0698,-0.0497,0.0141,0.0376,-0.0036,-0.0139,-0.0078,0.021,-0.0093,-0.0185,-0.0197,-0.0322,-0.062,-0.045,0.0077,-0.0657,-0.028,0.0022,-0.0493]}
{"key":"[Learning De-biased Representations with Biased Representations] Many machine learning algorithms are trained and evaluated by splitting data from a single source into training and test sets. While such focus on in-distribution learning scenarios has led to interesting advancement, it has not been able to tell if models are relying on dataset biases as shortcuts for successful prediction (e.g., using snow cues for recognising snowmobiles), resulting in biased models that fail to generalise when the bias shifts to a different class. The cross-bias generalisation problem has been addressed by de-biasing training data through augmentation or re-sampling, which are often prohibitive due to the data collection cost (e.g., collecting images of a snowmobile on a desert) and the difficulty of quantifying or expressing biases in the first place. In this work, we propose a novel framework to train a de-biased representation by encouraging it to be different from a set of representations that are biased by design. This tactic is feasible in many scenarios where it is much easier to define a set of biased representations than to define and quantify bias. We demonstrate the efficacy of our method across a variety of synthetic and real-world biases; our experiments show that the method discourages models from taking bias shortcuts, resulting in improved generalisation. Source code is available at https://github.com/clovaai/rebias.","layer":0,"vector":[-0.0237,-0.0185,0.0692,-0.0128,0.0639,0.0371,0.0251,0.0309,-0.0069,0.0064,0.0247,-0.0719,0.0452,0.0624,0.015,0.0351,0.0004,0.0531,-0.0246,-0.0076,-0.0231,-0.0109,-0.015,-0.0418,0.0201,0.03,-0.0225,-0.0151,-0.0611,-0.2358,0.0496,-0.072,0.0217,-0.0329,0.0009,0.0027,-0.0352,0.0762,-0.078,0.068,0.0007,-0.0232,-0.0253,-0.0507,-0.0286,-0.0219,-0.0642,-0.0118,-0.0587,-0.0206,0.0435,-0.0333,0.0173,-0.0069,0.0227,0.0543,0.0442,0.0306,0.0806,0.0387,0.0072,0.0424,-0.1417,0.0482,0.0246,0.0662,-0.0724,-0.0109,-0.0225,0.0228,0.0171,0.0237,0.0115,0.0627,-0.0389,-0.01,0.0088,0.0076,-0.0184,-0.0239,-0.0064,-0.0255,-0.0146,0.0108,0.0161,-0.0496,0.0264,-0.0563,0.0599,0.0125,-0.0381,-0.0366,-0.0229,0.0526,-0.0662,0.0139,0.0552,0.0067,-0.0531,0.2055,-0.0701,0.02,0.021,0.0133,0.0246,-0.0506,-0.0282,-0.0388,-0.067,-0.032,0.0068,-0.0198,0.0054,-0.0433,-0.0036,0.0136,0.0439,0.0267,-0.0236,0.0191,-0.0802,-0.005,0.0292,0.0005,0.0229,-0.0306,0.0181,0.1562,0.0255,0.0215,0.013,-0.0673,-0.0498,-0.0273,0.0402,0.0639,0.0574,0.0058,0.0212,0.02,-0.0535,-0.0349,-0.0092,-0.0416,-0.0656,0.0966,-0.0719,0.0372,-0.0295,-0.0123,-0.0175,0.0099,-0.0151,-0.032,0.0768,0.0576,-0.0011,0.0503,-0.0532,0.0527,0.0125,-0.0395,-0.0369,0.0529,0.0033,-0.0495,-0.0062,0.0041,0.0238,-0.0173,0.0267,-0.014,-0.0398,0.0632,0.0931,0.0091,-0.0809,-0.0241,-0.0044,-0.0021,0.0321,-0.0508,-0.0378,0.0813,0.0287,-0.0224,0.0,-0.0309,0.0236,0.0618,-0.0173,0.0151,-0.0336,-0.0122,-0.016,-0.0168,-0.0241,0.0156,-0.0009,0.0099,-0.0001,0.0092,-0.0603,-0.0104,-0.027,0.0143,0.009,-0.0201,0.097,-0.0095,-0.0424,-0.0285,0.0405,-0.0399,-0.027,-0.0123,0.038,0.0477,0.0257,0.0154,-0.0176,-0.0774,-0.0258,-0.2377,-0.0172,0.0279,-0.0046,0.0502,-0.0748,0.0588,0.007,0.0533,0.0885,0.0644,-0.0551,-0.0201,0.0359,0.0232,0.0409,0.0036,0.0352,-0.0235,0.0293,-0.0216,0.0251,0.0082,-0.0637,0.0927,-0.0279,0.2488,-0.0002,0.0389,-0.0247,0.0222,-0.0156,-0.0138,-0.1018,0.0552,-0.0153,0.0493,-0.025,-0.0253,0.0081,0.0087,0.0384,0.0324,-0.0846,-0.0105,-0.0299,-0.0588,0.0465,-0.044,0.0143,0.0031,-0.0692,0.0815,0.0122,-0.0181,-0.0065,-0.1277,0.0413,-0.0337,0.0182,0.0622,-0.0872,0.0227,-0.0945,0.0419,-0.0091,-0.0163,-0.043,0.046,0.0142,-0.0205,0.0945,-0.0117,-0.0363,0.0411,0.0115,0.0182,-0.0458,-0.039,-0.0348,0.0731,-0.0187,0.009,0.0305,0.0506,0.0357,0.0145,-0.0284,0.0474,-0.0535,-0.018,-0.0,-0.0548,-0.0249,0.0597,0.0007,-0.2727,-0.0016,0.0303,0.0292,0.0054,-0.0161,0.0166,0.0089,-0.0598,-0.0545,-0.0228,0.0698,0.0934,-0.0278,0.0308,0.0039,0.0766,-0.0234,0.0538,-0.0319,0.0031,0.0379,0.2115,-0.0334,0.0197,0.0246,-0.0486,-0.049,0.0147,-0.004,0.0151,0.0257,0.0851,-0.0178,0.0074,0.069,-0.0308,0.0048,0.0609,0.0081,0.0001,0.0295,-0.0291,-0.0105,0.0956,-0.0055,0.0051,-0.032,-0.0002,0.0331,-0.0545,0.0093,-0.0363,-0.0064,0.0418,0.0003,-0.0494,-0.0509,-0.0401,-0.0484,0.0376,-0.0749,-0.0337,-0.0194,0.0087]}
{"key":"[Assessing the Scalability of Biologically-Motivated Deep Learning Algorithms and Architectures] The backpropagation of error algorithm (BP) is impossible to implement in a real brain. The recent success of deep networks in machine learning and AI, however, has inspired proposals for understanding how the brain might learn across multiple layers, and hence how it might approximate BP. As of yet, none of these proposals have been rigorously evaluated on tasks where BP-guided deep learning has proved critical, or in architectures more structured than simple fully-connected networks. Here we present results on scaling up biologically motivated models of deep learning on datasets which need deep networks with appropriate architectures to achieve good performance. We present results on the MNIST, CIFAR-10, and ImageNet datasets and explore variants of target-propagation (TP) and feedback alignment (FA) algorithms, and explore performance in both fully- and locally-connected architectures. We also introduce weight-transport-free variants of difference target propagation (DTP) modified to remove backpropagation from the penultimate layer. Many of these algorithms perform well for MNIST, but for CIFAR and ImageNet we find that TP and FA variants perform significantly worse than BP, especially for networks composed of locally connected units, opening questions about whether new architectures and algorithms are required to scale these approaches. Our results and implementation details help establish baselines for biologically motivated deep learning schemes going forward.","layer":3,"vector":[-0.0599,-0.0366,0.0278,-0.0041,0.02,0.0182,-0.0004,0.0046,0.0525,-0.0382,0.0394,-0.0819,-0.0044,0.0726,0.0098,0.0362,-0.0203,0.0223,-0.0278,0.0361,-0.0111,-0.0359,0.0309,-0.0308,0.013,0.006,-0.0224,-0.0591,-0.0453,-0.2758,0.0127,-0.025,0.0565,-0.0155,-0.0222,-0.0096,-0.003,-0.0024,-0.0135,0.0001,0.0671,0.0532,-0.0179,-0.0387,0.0175,-0.0196,-0.057,0.0081,0.0156,-0.0354,0.0257,-0.0505,0.0294,0.0711,0.008,0.0302,0.0981,0.0674,0.0731,0.0315,0.0407,0.0651,-0.1584,0.0257,0.0348,-0.012,-0.0573,-0.0203,-0.0058,0.0494,-0.0114,0.0276,0.0514,0.0368,0.0346,0.0222,0.0051,-0.0107,0.001,-0.0035,0.0432,-0.0275,-0.0131,-0.0491,0.0096,-0.0219,0.0064,-0.0237,-0.0034,0.0084,-0.0154,-0.0163,-0.0196,0.0164,-0.0727,-0.039,0.0286,-0.0088,-0.0684,0.177,-0.0087,0.0236,0.0264,-0.0136,0.044,-0.0419,-0.0523,-0.0367,-0.0541,-0.0117,-0.0119,-0.0514,0.0195,-0.0198,-0.003,0.0098,0.0618,0.0505,0.0018,-0.0346,-0.0522,0.021,0.0589,-0.0074,0.0116,-0.0673,-0.0108,0.1514,0.0304,0.057,0.0298,-0.0165,-0.0508,-0.0112,0.0286,0.0312,0.0102,-0.0576,-0.0032,0.0141,-0.043,-0.0583,0.0262,-0.0605,-0.0738,0.1238,-0.0259,0.0254,-0.0278,-0.0539,-0.0508,0.0019,-0.0238,0.0203,0.0003,0.0329,-0.015,0.0263,-0.0403,-0.0221,0.0007,-0.0559,-0.0625,0.0932,0.0556,-0.0731,-0.0237,-0.0439,0.0303,-0.035,0.0292,0.0168,-0.0012,0.0068,-0.0186,0.0472,-0.1152,-0.0278,0.0136,0.0417,0.0112,-0.0623,-0.0112,0.0372,0.0537,-0.0083,0.0206,-0.0616,0.0131,0.0245,-0.0435,0.0325,-0.0132,-0.0166,-0.039,-0.0513,-0.0261,-0.0252,0.0017,-0.0149,-0.0259,-0.0252,-0.0118,0.0138,-0.0237,-0.0061,-0.037,0.0342,0.0281,0.0362,-0.0432,0.0258,0.0746,-0.0324,-0.0532,-0.0167,0.019,0.0328,0.0048,0.0556,0.0398,-0.0271,-0.0491,-0.1981,-0.0118,0.028,-0.0699,0.0559,-0.0551,0.0452,0.0273,0.0029,0.0626,0.059,0.0102,0.0126,-0.002,-0.0055,0.0786,0.0058,0.0391,-0.0258,-0.0036,0.0245,0.0236,0.0066,-0.0852,0.0465,-0.0051,0.2027,0.0047,0.0675,0.0019,0.0185,0.0544,-0.0015,-0.1155,0.1004,0.0088,0.087,-0.0247,-0.0164,-0.0265,-0.0645,0.0362,0.0277,-0.1289,-0.0639,0.0157,-0.012,0.0005,-0.0559,-0.025,0.0559,-0.0384,0.0238,0.0149,-0.0094,-0.0198,-0.0986,0.0605,-0.0267,0.0315,0.0378,-0.0319,-0.0103,-0.0087,0.0427,0.0031,0.014,-0.0715,0.0444,0.0136,0.0106,0.086,-0.005,0.0348,0.0656,-0.0244,0.0269,-0.0499,-0.0309,0.0117,0.0879,-0.0129,0.0288,0.0039,0.0491,0.0199,0.0705,-0.0475,0.0424,-0.0299,0.0424,0.0391,-0.0786,-0.0291,0.0116,-0.015,-0.3,0.035,0.0114,0.0629,-0.0285,0.0065,0.0186,0.0112,-0.0292,-0.0195,-0.0355,0.0266,0.0622,0.0293,0.0222,0.0535,0.0552,-0.0354,0.0844,-0.0412,0.0091,0.0464,0.2099,-0.0596,0.006,0.0218,-0.0197,-0.0226,0.0391,-0.0295,0.0063,0.0317,0.0609,-0.0741,0.0484,0.0931,-0.0443,0.0308,0.0597,-0.0314,0.0028,0.0081,-0.0252,0.0234,0.0852,-0.017,-0.032,-0.0118,-0.01,0.055,-0.0617,0.0059,-0.0029,-0.0199,0.0046,-0.011,-0.0712,-0.0814,-0.0859,-0.0267,0.0331,-0.075,-0.011,0.0153,-0.0116]}
{"key":"[Random Projection and Its Applications] Random Projection is a foundational research topic that connects a bunch of machine learning algorithms under a similar mathematical basis. It is used to reduce the dimensionality of the dataset by projecting the data points efficiently to a smaller dimensions while preserving the original relative distance between the data points. In this paper, we are intended to explain random projection method, by explaining its mathematical background and foundation, the applications that are currently adopting it, and an overview on its current research perspective.","layer":1,"vector":[-0.0356,-0.0138,0.0383,-0.0283,0.0235,0.0225,0.0152,0.0178,0.0427,-0.0113,0.0306,-0.0699,0.0283,0.0611,0.02,0.0311,0.0208,0.0381,-0.054,0.0201,0.074,-0.024,-0.0126,-0.0673,0.0205,0.0285,-0.0006,-0.0559,-0.0352,-0.233,0.0193,-0.0378,0.0533,-0.0563,-0.0132,-0.0375,-0.0241,0.108,-0.0185,0.0239,0.0658,0.0154,-0.0118,-0.0676,0.0062,-0.0336,-0.0497,-0.0169,-0.0159,0.0017,0.0084,-0.0161,0.0095,0.0258,0.0125,0.0427,0.0238,0.0639,0.0507,0.0389,0.0547,0.0394,-0.1582,0.0368,0.0687,0.0459,-0.0196,-0.0411,0.0182,0.0402,-0.0124,0.0307,0.0155,0.0259,0.0205,-0.0201,0.013,-0.0148,-0.0023,0.0183,0.0039,0.0091,-0.0459,0.042,0.0045,-0.0213,0.0155,-0.0707,0.0568,-0.0,-0.0471,0.0197,-0.076,0.0257,-0.0538,-0.039,0.0862,0.0308,-0.0245,0.1901,-0.0368,0.0134,0.0408,-0.0151,-0.0008,-0.0398,-0.0656,-0.0413,-0.0092,-0.029,0.0201,-0.0473,0.0166,-0.0417,-0.0078,-0.0434,0.0817,0.0596,-0.0063,-0.0576,-0.0306,-0.0031,0.0119,0.0101,0.0075,-0.0226,0.027,0.1453,0.018,0.0352,0.0411,-0.0324,-0.0675,-0.042,-0.0072,0.0283,0.0293,0.0052,0.0573,-0.0391,0.0166,-0.0495,0.0224,-0.0956,-0.0723,0.138,-0.0533,0.0455,-0.0093,-0.0673,0.0116,0.0062,-0.0217,-0.0504,0.0172,0.0136,0.0246,0.0685,-0.0625,0.0394,-0.0157,-0.0346,0.0013,0.0745,0.0214,-0.1076,-0.0363,0.0226,0.0362,-0.0214,-0.0117,0.0746,-0.0271,0.0319,0.0447,0.0256,-0.0639,-0.0124,0.0131,0.0174,0.0155,-0.0123,-0.0445,0.0411,0.0022,-0.0607,0.0333,-0.0,0.0094,0.0309,-0.0394,0.0147,-0.0476,-0.0375,-0.0371,-0.0718,-0.0075,-0.0153,0.0307,-0.0046,-0.0139,0.0235,-0.0655,0.0581,0.0044,0.0077,0.0408,-0.049,0.0186,0.0698,-0.035,-0.013,0.0066,-0.0014,-0.0266,0.0361,0.0151,0.0409,0.0167,0.0211,0.043,-0.0175,-0.0811,-0.2354,0.0023,-0.0084,-0.0181,0.0297,-0.0846,-0.0274,-0.041,0.0741,0.0767,0.075,-0.0405,-0.0246,0.029,-0.0407,0.0597,0.0327,0.01,-0.0674,-0.0052,0.0108,0.0396,0.0162,-0.1059,0.0247,0.0306,0.1619,0.0056,0.0052,-0.0026,0.065,0.0182,-0.0219,-0.0982,0.0452,0.0243,0.045,-0.0243,-0.0455,-0.0371,-0.0094,0.023,0.0471,-0.1168,-0.0838,-0.0748,-0.0409,0.0301,-0.0317,0.0525,0.0079,-0.0079,0.0544,-0.0429,0.029,-0.0314,-0.0522,0.0251,-0.0072,0.0194,0.0105,-0.0468,-0.0,-0.0709,0.0522,-0.0046,-0.0159,-0.027,0.0314,0.0064,-0.0494,0.0892,-0.0194,-0.0123,0.1057,-0.0159,0.0246,0.0231,-0.0326,-0.0127,0.063,-0.0092,0.0196,0.0247,0.0158,-0.0108,0.1022,-0.0105,-0.0008,0.0063,0.0217,0.0109,-0.054,-0.0209,0.0412,0.0073,-0.2897,0.0043,-0.0303,0.033,-0.0429,-0.0173,0.0403,0.0173,-0.0203,0.0156,0.0029,0.0268,0.0741,-0.0467,0.0176,0.0316,0.0777,-0.076,0.0603,-0.0457,0.0107,0.0005,0.2378,-0.0297,0.0093,0.0322,-0.003,0.0202,0.0274,-0.0335,-0.0175,0.0161,0.0952,-0.0457,0.0476,0.0752,-0.0206,0.0317,0.0375,-0.0823,0.0036,-0.0364,-0.0477,-0.0193,0.1252,0.0337,0.0034,-0.0376,0.0107,0.0101,-0.0332,-0.0293,-0.0284,0.0027,0.0029,0.0413,-0.0285,-0.027,-0.0317,-0.0252,-0.0312,-0.067,-0.0317,-0.0509,0.0221]}
{"key":"[On Learning Nominal Automata with Binders] We investigate a learning algorithm in the context of nominal automata, an extension of classical automata to alphabets featuring names. This class of automata captures nominal regular languages; analogously to the classical language theory, nominal automata have been shown to characterise nominal regular expressions with binders. These formalisms are amenable to abstract modelling resource-aware computations. We propose a learning algorithm on nominal regular languages with binders. Our algorithm generalises Angluin's L* algorithm with respect to nominal regular languages with binders. We show the correctness and study the theoretical complexity of our algorithm.","layer":2,"vector":[-0.0808,-0.0163,0.0152,-0.0255,-0.0209,0.0261,0.0339,-0.0017,0.0156,-0.0548,0.0163,-0.0609,0.0436,0.0456,0.0168,-0.0086,-0.0209,0.0286,-0.0321,-0.0368,0.0895,-0.0214,-0.0324,-0.026,0.0047,0.0685,-0.014,-0.0389,-0.0038,-0.2075,-0.0271,-0.0198,0.0475,0.0159,-0.0141,-0.0024,-0.0771,0.0519,-0.0355,0.0498,-0.0215,-0.0106,-0.0011,-0.0374,-0.0229,-0.0667,0.024,-0.0022,-0.0635,-0.0468,-0.0304,-0.0359,0.0455,0.0037,0.0122,0.0133,0.0635,0.0196,0.0349,0.029,0.0184,0.0542,-0.1664,0.1136,0.0216,-0.0015,-0.0188,-0.0117,0.0223,0.1119,-0.0083,0.0454,0.0274,0.0402,0.0279,0.0121,0.0024,-0.0396,0.0081,-0.0083,-0.0311,-0.0501,-0.0309,0.0035,-0.0316,0.0011,-0.0088,-0.076,0.0383,0.0225,-0.0537,-0.0426,0.0582,0.0279,-0.0451,-0.047,-0.0135,0.0585,-0.0253,0.2207,-0.0505,0.0386,-0.0009,-0.0528,0.0292,-0.0439,0.0058,-0.024,-0.0013,-0.0597,0.0014,0.0032,0.058,-0.0683,0.0092,0.0011,0.0902,0.0022,-0.0028,0.014,-0.0102,-0.0004,-0.0174,-0.0161,0.0304,-0.0296,0.0277,0.0995,0.0181,0.0712,0.0772,-0.0345,-0.0095,-0.0313,0.0753,0.0095,-0.0036,-0.0162,0.0116,0.0057,-0.0325,-0.0769,-0.0139,-0.0572,-0.0553,0.1307,-0.0269,0.0403,-0.0186,-0.002,-0.0023,0.002,-0.0042,-0.0551,0.0175,0.0549,0.0536,0.0356,-0.0266,0.0065,-0.007,-0.0155,-0.022,0.0645,-0.0279,-0.0796,-0.0471,-0.0262,0.0181,-0.0092,0.0696,0.0269,-0.0703,0.0345,0.0423,0.0566,-0.0396,-0.0296,-0.0043,0.0264,0.0335,-0.0458,0.0106,0.0698,0.0417,-0.0244,-0.011,-0.0247,0.0437,-0.0334,-0.0095,0.0695,-0.0128,-0.0226,-0.0392,-0.0285,-0.0059,-0.0114,0.0132,-0.0084,0.0364,0.0292,-0.0029,0.0537,0.0285,0.0126,-0.0254,0.0082,0.0321,0.0209,-0.028,-0.0229,0.0579,-0.0089,-0.0418,-0.0241,0.033,0.0144,0.0365,0.0326,0.0266,-0.0705,-0.0439,-0.1848,-0.0309,-0.0017,-0.0415,0.0337,-0.0657,0.0483,-0.0214,0.0236,0.0638,-0.0029,-0.0384,-0.0259,0.0345,-0.0242,0.0562,0.0635,0.017,-0.0041,0.0337,-0.034,0.0147,-0.0334,-0.0784,0.0085,0.0249,0.2649,0.0064,0.0271,-0.0571,0.029,0.0026,-0.0325,-0.0773,0.0392,0.0006,-0.0136,-0.004,0.0434,-0.059,0.0009,0.0352,-0.0594,-0.06,0.0102,-0.0062,-0.0379,-0.0192,-0.064,0.0579,0.0311,-0.0115,0.0378,0.0214,-0.0644,-0.0227,-0.0732,0.0146,-0.0478,0.0439,0.0363,-0.0425,-0.0026,-0.086,0.0526,0.0383,0.0232,-0.0678,0.0036,-0.0323,-0.0386,0.0859,0.0072,-0.0493,0.0694,0.0194,0.0006,-0.0083,-0.0532,-0.0201,0.0898,-0.0825,0.0709,0.0322,0.0362,0.0198,0.0825,0.0069,0.0378,-0.0127,-0.0209,0.0656,-0.0383,0.008,0.0394,-0.0204,-0.3241,0.0712,0.0179,0.0417,-0.0454,-0.0054,0.0493,-0.006,-0.0438,0.0092,0.0214,0.0373,0.0148,-0.0265,-0.0032,0.0225,0.0765,-0.0548,0.0367,-0.0738,0.0043,0.0529,0.2495,-0.0383,0.0203,0.0046,-0.0567,0.0208,0.0249,0.0043,0.0142,0.0259,0.1095,-0.0659,0.0289,0.0294,-0.0416,0.0118,0.0048,-0.0133,-0.0209,-0.0201,-0.0468,0.0009,0.0587,0.0291,0.0124,-0.0439,-0.022,0.0259,-0.0249,0.0111,-0.0431,-0.004,0.0206,-0.0045,-0.0351,-0.0594,-0.0564,-0.0502,0.001,-0.0535,0.0179,0.0659,-0.0009]}
{"key":"[Stock Movement Prediction with Financial News using Contextualized Embedding from BERT] News events can greatly influence equity markets. In this paper, we are interested in predicting the short-term movement of stock prices after financial news events using only the headlines of the news. To achieve this goal, we introduce a new text mining method called Fine-Tuned Contextualized-Embedding Recurrent Neural Network (FT-CE-RNN). Compared with previous approaches which use static vector representations of the news (static embedding), our model uses contextualized vector representations of the headlines (contextualized embeddings) generated from Bidirectional Encoder Representations from Transformers (BERT). Our model obtains the state-of-the-art result on this stock movement prediction task. It shows significant improvement compared with other baseline models, in both accuracy and trading simulations. Through various trading simulations based on millions of headlines from Bloomberg News, we demonstrate the ability of this model in real scenarios.","layer":3,"vector":[-0.056,-0.0312,0.0085,-0.0068,0.0154,0.0258,0.0393,0.0273,0.0187,0.0166,-0.0368,-0.0067,0.0225,0.0263,0.0126,0.0234,0.0137,0.0412,-0.0697,0.0075,0.0267,-0.0306,0.0083,-0.0528,0.0531,-0.0054,-0.0071,-0.0227,-0.0689,-0.2096,-0.002,-0.0885,0.0368,-0.02,0.0052,-0.0036,-0.0626,0.0351,0.0153,0.0689,-0.0289,0.0537,0.002,-0.0794,0.0231,-0.053,-0.0023,0.0047,-0.0211,-0.0131,0.0299,-0.0489,0.0272,0.0332,0.0445,0.0612,0.0334,-0.0088,0.0456,0.0359,0.0411,0.0498,-0.1758,0.0423,-0.0178,-0.0022,-0.0192,0.0166,0.0162,0.0548,0.012,0.0164,0.0229,0.0299,0.0235,-0.0235,0.0073,-0.0081,-0.0366,0.0046,0.0506,0.0014,-0.0224,-0.0283,-0.0244,-0.0383,0.0108,-0.0527,0.0314,-0.0044,-0.0172,0.0121,-0.01,0.0223,-0.0831,-0.0384,0.0367,0.0046,-0.0746,0.1652,-0.0288,0.0326,0.0313,-0.0489,0.0433,-0.0349,-0.0227,-0.0199,-0.0252,0.0025,-0.0209,-0.0105,0.0244,-0.0425,0.027,-0.0069,0.0902,0.0067,-0.0146,0.0152,-0.0191,0.0189,0.0121,-0.0382,0.0179,-0.0427,0.0441,0.1347,0.0566,0.0291,0.0461,-0.0023,-0.0713,-0.0365,0.0183,0.0536,-0.0142,-0.0186,0.0117,-0.0174,-0.0821,-0.0287,0.0044,-0.1054,-0.0874,0.1432,-0.022,-0.0035,-0.0321,-0.0152,-0.0757,-0.0038,0.0015,-0.054,0.0538,0.0308,0.0527,0.0645,-0.0434,-0.0418,-0.0413,-0.0278,-0.0742,0.0911,0.0203,-0.1038,-0.0123,0.0218,0.0345,-0.0352,0.0439,0.0215,-0.0448,0.0374,0.0989,0.0522,-0.0264,-0.0401,0.0001,0.0327,0.0337,-0.0954,-0.0446,0.058,0.0236,-0.0389,-0.0054,-0.0595,0.0293,0.0407,-0.0128,0.0226,-0.0437,0.0265,-0.0545,-0.0035,-0.0034,0.0027,0.0009,-0.01,0.0125,-0.0069,-0.0259,0.0263,-0.0285,0.0079,-0.0339,0.0136,0.0353,0.016,-0.0377,0.0208,0.0683,-0.0565,-0.0597,-0.0172,0.0054,0.0629,0.0301,0.0564,0.0196,-0.0327,-0.0346,-0.2624,-0.0332,-0.0075,-0.0264,0.0296,-0.0607,0.0013,0.0014,0.1051,0.0798,0.0251,-0.0597,0.0057,-0.0139,0.0321,0.0239,-0.0032,0.0474,-0.0242,0.0177,-0.0176,0.0006,-0.0074,-0.0426,0.0639,0.0042,0.1924,0.0544,0.0571,-0.0508,0.0236,0.0196,-0.0603,-0.0794,0.0838,0.017,0.1016,0.012,-0.0429,-0.0396,-0.0543,-0.0047,-0.0078,-0.0457,-0.0402,0.0001,-0.01,-0.0021,-0.0403,0.0693,0.0499,-0.038,0.0905,0.0339,-0.0205,-0.0528,-0.0459,0.0094,-0.0426,-0.0187,-0.003,-0.038,0.0486,-0.0375,0.0239,0.0069,0.0257,-0.0149,0.0061,0.0182,-0.0416,0.1003,-0.0099,0.0055,0.0456,-0.0002,0.0188,-0.0294,-0.0449,-0.0025,0.0793,-0.0147,0.0945,0.0752,0.0105,-0.0127,0.0988,-0.0619,0.0304,0.0106,0.004,0.0117,-0.0788,-0.0113,0.0324,-0.0409,-0.2969,0.0294,0.0042,0.0406,0.0242,0.0124,0.0131,0.0244,-0.0432,0.0261,-0.0083,0.0249,0.0822,-0.0154,-0.0324,0.0138,0.0815,-0.0553,-0.0071,-0.0205,0.0593,0.04,0.2201,-0.0445,0.02,0.0011,-0.0461,-0.0059,0.056,0.0328,0.041,-0.0175,0.0816,-0.0558,0.0159,0.0522,-0.0212,0.0672,0.0509,-0.0063,-0.0106,0.0153,-0.0346,-0.0416,0.0358,0.0067,0.0264,-0.0434,-0.0339,0.0192,-0.0816,0.0358,-0.0249,-0.0058,-0.0007,0.029,-0.0579,-0.0585,0.0098,-0.0518,0.0164,-0.1047,-0.0502,-0.0097,0.015]}
{"key":"[Development of Interpretable Machine Learning Models to Detect Arrhythmia based on ECG Data] The analysis of electrocardiogram (ECG) signals can be time consuming as it is performed manually by cardiologists. Therefore, automation through machine learning (ML) classification is being increasingly proposed which would allow ML models to learn the features of a heartbeat and detect abnormalities. The lack of interpretability hinders the application of Deep Learning in healthcare. Through interpretability of these models, we would understand how a machine learning algorithm makes its decisions and what patterns are being followed for classification. This thesis builds Convolutional Neural Network (CNN) and Long Short-Term Memory (LSTM) classifiers based on state-of-the-art models and compares their performance and interpretability to shallow classifiers. Here, both global and local interpretability methods are exploited to understand the interaction between dependent and independent variables across the entire dataset and to examine model decisions in each sample, respectively. Partial Dependence Plots, Shapley Additive Explanations, Permutation Feature Importance, and Gradient Weighted Class Activation Maps (Grad-Cam) are the four interpretability techniques implemented on time-series ML models classifying ECG rhythms. In particular, we exploit Grad-Cam, which is a local interpretability technique and examine whether its interpretability varies between correctly and incorrectly classified ECG beats within each class. Furthermore, the classifiers are evaluated using K-Fold cross-validation and Leave Groups Out techniques, and we use non-parametric statistical testing to examine whether differences are significant. It was found that Grad-CAM was the most effective interpretability technique at explaining predictions of proposed CNN and LSTM models. We concluded that all high performing classifiers looked at the QRS complex of the ECG rhythm when making predictions.","layer":1,"vector":[0.0017,-0.0021,0.0007,0.0014,0.0558,-0.0233,0.0255,0.0042,0.0276,-0.0193,0.0156,-0.0607,0.0139,0.0463,-0.0039,0.0051,-0.0165,0.0233,-0.0326,0.0226,0.0012,0.0208,-0.0154,-0.0303,0.0478,0.0274,0.0196,0.0009,-0.066,-0.2323,-0.0066,-0.043,0.0621,0.0099,-0.0172,-0.0274,0.0076,0.0527,-0.077,0.0265,-0.0007,-0.0507,-0.0134,-0.0291,-0.0175,-0.0757,-0.0464,-0.0475,0.0021,-0.0283,0.028,-0.0003,0.0402,0.0175,0.0179,0.0301,0.0636,0.0604,0.0598,0.0231,0.0317,0.0342,-0.1697,0.0497,-0.0068,0.0474,-0.0316,0.0101,0.0161,0.0664,0.0039,-0.0226,0.012,0.0462,-0.0113,0.0263,0.0123,0.003,-0.0007,-0.0198,0.0192,-0.0287,-0.0328,-0.0329,0.0062,-0.0517,0.0376,-0.0798,0.0284,-0.0024,-0.0375,-0.0143,-0.0561,0.0465,-0.0286,0.0086,0.0291,0.015,-0.0831,0.1856,-0.0722,0.0107,-0.0059,0.0201,0.0409,-0.0016,-0.0467,-0.0423,-0.0053,0.0055,-0.0092,-0.0116,0.0257,-0.0059,0.0214,0.0608,0.0353,0.0308,0.0669,0.0353,-0.0288,0.0192,0.0597,-0.0194,0.0315,-0.0402,0.032,0.1649,0.0396,-0.0032,0.0355,0.0142,-0.0621,-0.0051,0.0102,0.0091,0.0599,-0.007,0.0047,-0.0231,-0.0493,-0.0344,0.0251,-0.0948,-0.0788,0.094,-0.062,0.027,-0.0495,-0.0087,-0.048,0.0029,-0.0683,-0.0348,0.0489,0.0394,0.013,0.0355,-0.0249,0.0013,-0.0231,-0.082,-0.0104,0.1067,0.0119,-0.0325,-0.0201,-0.0246,-0.0047,-0.0088,0.0676,0.0577,-0.0369,0.0332,0.0749,0.02,-0.0416,-0.0283,0.0003,0.0084,0.0169,-0.0441,-0.0393,0.0393,0.0383,-0.0383,0.0411,-0.0502,0.022,0.061,-0.0113,0.0053,-0.0242,0.0269,-0.0399,-0.0174,-0.018,-0.0262,0.0221,-0.0208,-0.0107,0.0128,-0.0023,0.0415,-0.0453,0.0264,-0.006,0.0129,0.0089,0.0419,0.0175,0.0404,0.0436,-0.041,-0.0321,-0.0149,-0.0017,0.0527,0.0028,0.0047,0.0349,-0.0473,-0.0429,-0.2371,-0.0329,0.0398,-0.0373,0.0317,-0.0926,-0.0012,0.0046,0.0361,0.0641,0.0512,0.0208,-0.0446,0.0095,0.0135,0.0654,0.0213,0.0427,-0.0489,-0.0006,-0.0053,0.0245,0.0005,-0.0823,0.0335,0.022,0.2108,0.0011,0.051,-0.0204,0.0065,0.0057,-0.017,-0.1142,0.064,0.0114,0.0634,0.0126,-0.0304,-0.0184,-0.0785,0.0386,0.008,-0.0933,-0.0785,-0.0113,-0.0078,0.0039,-0.05,0.0155,0.0286,-0.0747,0.046,0.0152,0.0556,-0.03,-0.1178,0.023,-0.037,0.0071,-0.0337,-0.0296,0.01,-0.0848,0.0382,0.0029,-0.0834,-0.0165,0.0133,-0.0726,0.0107,0.1001,0.0128,-0.0326,0.0974,0.0079,0.0572,-0.037,-0.0273,0.005,0.0523,-0.0087,-0.0097,-0.0003,0.0272,0.0011,0.0605,0.0021,0.0011,-0.0315,-0.0035,0.0502,-0.0463,-0.0685,0.0406,-0.0648,-0.289,0.0355,-0.0003,0.0305,-0.0486,-0.0298,0.0053,-0.0045,-0.0508,-0.0016,-0.0287,0.0087,0.0643,-0.0489,0.0092,0.0508,0.0833,-0.0585,0.0682,-0.018,0.0616,0.0685,0.2259,-0.0503,0.0241,0.033,-0.0028,-0.0195,0.0515,-0.0099,0.039,-0.016,0.083,-0.0309,0.0278,0.067,-0.0368,0.0358,0.0377,-0.0063,0.0451,0.037,-0.0983,-0.0636,0.1069,-0.01,-0.069,-0.0402,-0.0096,0.0774,0.0116,-0.0225,-0.003,0.0235,0.013,0.0289,-0.0588,-0.0612,0.0059,-0.0178,0.0298,-0.0845,-0.0064,0.0344,-0.0266]}
{"key":"[Improving Confidence Estimation on Out-of-Domain Data for End-to-End Speech Recognition] As end-to-end automatic speech recognition (ASR) models reach promising performance, various downstream tasks rely on good confidence estimators for these systems. Recent research has shown that model-based confidence estimators have a significant advantage over using the output softmax probabilities. If the input data to the speech recogniser is from mismatched acoustic and linguistic conditions, the ASR performance and the corresponding confidence estimators may exhibit severe degradation. Since confidence models are often trained on the same in-domain data as the ASR, generalising to out-of-domain (OOD) scenarios is challenging. By keeping the ASR model untouched, this paper proposes two approaches to improve the model-based confidence estimators on OOD data: using pseudo transcriptions and an additional OOD language model. With an ASR model trained on LibriSpeech, experiments show that the proposed methods can greatly improve the confidence metrics on TED-LIUM and Switchboard datasets while preserving in-domain performance. Furthermore, the improved confidence estimators are better calibrated on OOD data and can provide a much more reliable criterion for data selection.","layer":9,"vector":[-0.0212,-0.0435,0.031,-0.0206,0.0129,-0.0005,0.012,0.0198,-0.0004,-0.0209,0.0033,-0.0442,0.0065,0.068,0.0371,0.0372,0.013,0.0596,-0.01,0.0487,0.0403,-0.0441,0.0269,-0.0114,0.0016,0.0068,-0.0592,-0.0427,-0.0401,-0.2642,-0.021,-0.0607,0.0347,-0.082,0.0003,-0.0111,-0.0605,0.0135,-0.0364,0.0022,-0.0006,0.0042,0.0111,-0.0741,0.0049,-0.0352,-0.0041,0.0033,-0.0293,-0.0096,-0.0007,0.0129,0.024,0.0011,0.0021,0.0305,0.0511,0.047,0.0231,0.0612,0.0087,0.0402,-0.2063,0.0605,0.0198,0.0365,-0.0191,-0.0356,0.0157,0.0334,-0.0117,0.0439,0.0205,0.075,0.0211,-0.0001,0.0248,-0.0619,0.0151,0.0349,0.0245,-0.041,-0.0336,-0.0359,-0.0235,-0.0887,0.031,-0.025,0.0206,-0.0162,-0.0675,-0.0004,-0.0168,0.041,-0.0222,-0.0341,0.028,0.033,-0.0399,0.1786,-0.0315,-0.0133,-0.0362,-0.0554,0.078,-0.0424,-0.0215,-0.0296,0.0032,-0.0058,-0.0202,-0.0211,0.0218,-0.0033,0.0631,0.0083,0.0822,-0.0189,-0.0255,-0.0268,-0.0423,0.009,0.0353,-0.0043,0.0042,-0.0286,0.063,0.1154,0.026,-0.0042,0.0792,-0.0616,-0.0232,-0.0129,0.0256,0.0337,0.0458,0.0312,0.0345,-0.0056,0.0036,-0.1323,0.0247,-0.0478,-0.054,0.1312,-0.0499,-0.0002,-0.0523,-0.0204,0.0257,0.0991,-0.0128,0.0179,0.069,-0.0021,0.0427,0.0423,-0.0499,-0.0198,-0.0041,-0.0621,-0.0236,0.0457,0.002,-0.0725,-0.0649,0.008,0.0337,-0.0231,0.0267,-0.0045,-0.0288,0.014,0.0743,-0.005,-0.0245,-0.0233,0.0206,0.0105,0.0003,-0.0494,-0.0295,0.0386,0.0228,-0.0516,-0.0007,-0.0794,-0.011,0.029,-0.0262,0.0066,-0.035,0.0146,-0.0271,-0.0565,-0.0242,-0.0146,0.0457,-0.0343,-0.0074,0.0044,-0.0342,0.0138,-0.0384,0.0337,-0.0035,0.0288,0.0464,0.0236,-0.0227,-0.0002,0.1085,-0.0061,-0.0373,-0.0082,0.0351,0.0438,0.0076,0.0334,0.022,-0.0402,-0.0286,-0.2413,0.0359,0.041,-0.0085,0.0647,-0.0995,0.0546,0.0176,0.0852,0.0699,0.0348,-0.0268,0.0057,0.062,-0.0312,0.0398,0.0541,-0.0184,-0.0345,0.0537,-0.0318,0.0271,-0.0595,-0.0895,0.0492,-0.0121,0.1811,-0.0373,0.0561,-0.0254,0.0197,0.0366,-0.0237,-0.1197,0.0948,-0.007,0.0907,-0.0142,0.0021,-0.0249,-0.0271,0.0242,0.0225,-0.0916,-0.0345,-0.024,-0.0837,-0.0086,-0.0482,0.0362,-0.0052,-0.0195,0.086,0.0136,-0.0179,-0.0313,-0.0869,-0.0042,-0.0147,0.017,0.0189,-0.0261,0.0206,-0.0535,0.0014,-0.0337,-0.0526,-0.0584,0.0501,-0.0105,-0.0328,0.096,0.0211,0.0181,0.0229,0.0381,-0.0122,-0.0718,-0.0583,-0.0449,0.064,-0.0394,0.0236,0.021,0.0613,0.0475,0.0963,0.0151,0.0263,-0.0013,0.0146,0.0225,0.0163,-0.0061,0.0426,0.015,-0.3003,0.0092,0.0042,0.0378,-0.0448,0.0189,0.0315,0.0152,-0.0723,0.0329,-0.0112,0.0836,0.0042,-0.0276,0.019,0.0717,0.075,-0.0239,0.0387,-0.0335,0.0451,0.0231,0.1799,-0.0213,0.0487,0.0021,-0.0336,-0.0072,0.0188,-0.0387,0.0192,-0.0072,0.0877,-0.01,0.0123,0.0803,-0.1024,0.0269,0.0075,0.0035,0.0334,0.0004,-0.0429,-0.0142,0.0675,-0.019,-0.0023,-0.0433,0.0299,0.037,-0.0346,-0.0073,-0.0347,-0.004,-0.0015,0.0694,-0.0347,-0.0329,-0.0302,-0.052,0.0422,-0.0414,-0.0295,0.0112,-0.0091]}
{"key":"[Deep Clustering With Intra-class Distance Constraint for Hyperspectral Images] The high dimensionality of hyperspectral images often results in the degradation of clustering performance. Due to the powerful ability of deep feature extraction and non-linear feature representation, the clustering algorithm based on deep learning has become a hot research topic in the field of hyperspectral remote sensing. However, most deep clustering algorithms for hyperspectral images utilize deep neural networks as feature extractor without considering prior knowledge constraints that are suitable for clustering. To solve this problem, we propose an intra-class distance constrained deep clustering algorithm for high-dimensional hyperspectral images. The proposed algorithm constrains the feature mapping procedure of the auto-encoder network by intra-class distance so that raw images are transformed from the original high-dimensional space to the low-dimensional feature space that is more conducive to clustering. Furthermore, the related learning process is treated as a joint optimization problem of deep feature extraction and clustering. Experimental results demonstrate the intense competitiveness of the proposed algorithm in comparison with state-of-the-art clustering methods of hyperspectral images.","layer":2,"vector":[-0.0197,-0.0368,0.0422,-0.0157,0.0621,0.0422,0.0494,-0.0051,-0.001,-0.0284,0.0238,-0.0931,0.0288,0.0387,0.0395,0.0388,0.0228,0.0575,-0.0544,0.0237,0.0208,-0.0232,-0.0395,-0.0571,0.0354,0.0179,-0.013,-0.0542,-0.0628,-0.2427,0.0319,0.0078,0.1091,-0.0118,-0.0014,-0.0227,-0.0381,0.0525,-0.0423,0.0193,0.0389,0.0214,-0.0185,-0.0206,-0.0141,-0.0643,-0.041,-0.0246,-0.0006,-0.0591,0.0367,-0.0518,-0.0303,0.0597,0.0026,0.0375,0.0823,0.0278,0.0388,0.0124,0.0739,0.0583,-0.1797,0.0213,0.0331,0.016,-0.0423,-0.0306,0.0514,0.0149,0.0029,0.0758,0.0235,0.0297,0.0232,0.03,0.0283,-0.0122,-0.0232,-0.0122,0.043,-0.0176,-0.0244,0.0076,0.0261,-0.0249,0.0189,-0.0853,0.0561,0.0017,-0.0443,-0.0538,-0.0071,0.0033,-0.0936,-0.0469,0.0204,-0.0125,-0.0373,0.1748,-0.0467,0.0418,0.0246,-0.0776,-0.0246,-0.0763,-0.0472,0.0123,-0.0263,-0.0149,-0.0089,-0.0107,-0.0049,-0.0103,-0.0485,-0.0157,0.0611,0.0087,0.0004,0.0037,-0.0065,-0.0142,0.0526,0.0035,0.0505,-0.0589,0.0518,0.1159,0.0555,0.0409,0.0563,-0.0086,-0.0331,-0.0283,0.0328,0.037,0.0375,0.0005,-0.006,0.0011,-0.0553,-0.0752,0.0451,-0.0728,0.0064,0.094,-0.0639,0.0014,-0.0382,-0.0254,-0.0166,-0.0276,-0.0452,-0.0345,0.0142,0.0465,0.0282,0.0498,-0.0601,0.0287,-0.0156,-0.062,0.0047,0.1296,0.022,-0.071,-0.0129,-0.0116,0.0171,0.0165,0.0345,0.0728,-0.0142,0.019,0.1458,0.0362,-0.0372,-0.0048,-0.0342,0.0009,-0.0181,0.0167,-0.0164,0.0348,0.0736,-0.039,0.0194,-0.0119,-0.0028,0.035,-0.0419,0.0118,-0.0608,-0.0112,-0.0027,-0.0295,-0.0019,-0.0137,-0.004,-0.0278,0.0045,0.0231,-0.0222,0.0527,0.0182,0.0277,0.0318,-0.0252,0.0028,-0.0017,-0.0382,0.02,0.0462,-0.0443,-0.017,-0.029,0.0126,0.0672,0.0122,0.0775,0.0424,-0.0549,-0.0791,-0.2282,0.0021,0.0074,-0.0271,0.0434,-0.0576,0.0427,0.0017,0.0746,0.052,0.0641,0.0209,-0.0125,0.0079,-0.0192,0.038,0.0587,0.062,-0.0197,0.0339,-0.0205,0.0204,-0.0135,-0.0683,0.0364,0.0052,0.1864,-0.0131,0.0278,-0.0103,0.0011,0.0143,-0.0334,-0.1081,0.0095,-0.0243,0.0744,0.029,-0.0445,-0.0411,0.0089,0.0054,-0.0382,-0.0986,-0.0183,-0.0141,-0.0109,0.0418,-0.0348,-0.0098,0.0501,-0.025,0.0238,-0.0153,-0.0192,-0.0243,-0.1008,0.0246,-0.0522,0.0019,-0.0015,-0.0829,-0.0433,-0.0798,0.0989,0.0109,-0.0289,0.0052,0.0386,-0.0118,-0.0252,0.1395,0.0014,0.0253,0.0936,-0.0006,0.046,-0.0068,0.0058,-0.034,0.1019,-0.0122,0.0309,0.0222,0.0222,0.018,0.0659,-0.0345,0.008,-0.0032,0.0212,0.0208,-0.0936,-0.0041,0.0404,-0.0383,-0.2664,-0.0024,-0.0137,0.0128,0.0037,0.0151,0.016,0.0316,-0.0465,-0.0252,0.0138,0.0038,0.0322,-0.0302,-0.0121,0.0165,0.0206,-0.0431,0.0788,-0.0433,-0.0201,0.0006,0.2107,-0.0258,0.0087,0.0258,-0.0144,-0.0141,0.03,-0.0568,0.0171,0.0111,0.1134,-0.0603,0.0103,0.1101,0.0039,0.0355,0.0614,0.005,-0.0116,0.0136,-0.0529,-0.0243,0.0851,-0.0167,-0.0022,-0.0643,-0.0325,0.0164,-0.0428,-0.0164,-0.0391,0.0066,-0.0113,0.0285,-0.0667,-0.0633,-0.0502,-0.0591,0.0211,-0.0842,-0.0328,-0.0101,-0.0105]}
{"key":"[Lifelong Generative Modeling] Lifelong learning is the problem of learning multiple consecutive tasks in a sequential manner, where knowledge gained from previous tasks is retained and used to aid future learning over the lifetime of the learner. It is essential towards the development of intelligent machines that can adapt to their surroundings. In this work we focus on a lifelong learning approach to unsupervised generative modeling, where we continuously incorporate newly observed distributions into a learned model. We do so through a student-teacher Variational Autoencoder architecture which allows us to learn and preserve all the distributions seen so far, without the need to retain the past data nor the past models. Through the introduction of a novel cross-model regularizer, inspired by a Bayesian update rule, the student model leverages the information learned by the teacher, which acts as a probabilistic knowledge store. The regularizer reduces the effect of catastrophic interference that appears when we learn over sequences of distributions. We validate our model's performance on sequential variants of MNIST, FashionMNIST, PermutedMNIST, SVHN and Celeb-A and demonstrate that our model mitigates the effects of catastrophic interference faced by neural networks in sequential learning scenarios.","layer":1,"vector":[-0.034,-0.0289,0.0517,-0.0226,0.016,0.0514,-0.014,-0.0269,0.0105,-0.0134,0.0251,-0.0185,0.0553,0.0841,0.0234,0.0498,-0.034,0.0681,-0.0414,-0.0419,0.0341,-0.0428,0.0051,-0.022,-0.0069,0.0172,-0.0495,-0.047,-0.0271,-0.2609,0.0468,-0.0469,0.0403,-0.0025,-0.0282,-0.0177,-0.0615,0.0355,-0.0205,0.0589,0.0313,0.0136,-0.0321,-0.0563,-0.0213,-0.0396,-0.0128,-0.0406,-0.0143,-0.0563,0.0043,-0.0384,0.0384,0.0403,0.0143,0.0177,0.0532,0.0508,0.038,0.0866,0.018,0.0584,-0.1518,0.0814,0.0119,0.0525,-0.0709,0.0087,0.0278,0.0363,-0.027,0.0226,0.0088,0.0882,0.0516,-0.0079,-0.0008,-0.0369,-0.0116,0.0342,0.0217,-0.0197,-0.0045,-0.0633,-0.0009,-0.0186,-0.0202,-0.0879,0.0257,0.0183,-0.0462,-0.007,-0.0237,0.0637,-0.0348,-0.0388,0.0468,0.0374,-0.0604,0.2067,-0.0551,0.0217,0.0671,-0.0057,0.0098,0.0027,-0.0512,0.0096,-0.0098,-0.0079,-0.0087,0.0145,0.0299,-0.0737,0.0048,0.0197,0.0734,0.0126,-0.0219,0.0139,-0.0557,0.037,0.0432,0.0095,0.0319,-0.0671,0.0121,0.1474,0.0167,0.0487,0.0587,-0.0087,-0.0684,-0.0105,0.0512,0.0094,0.0069,-0.0292,0.0384,-0.0469,-0.0085,-0.011,0.0076,-0.0538,-0.0381,0.0841,-0.0041,0.0472,-0.0196,-0.0106,0.017,-0.0051,-0.0025,-0.0238,0.038,0.1004,0.0498,0.0135,-0.0539,-0.0096,-0.039,-0.0637,-0.0181,0.0816,-0.0128,-0.0515,-0.0242,0.0134,0.0173,0.0063,0.0705,0.0496,-0.0023,0.0415,0.0621,0.0593,-0.076,0.005,0.0151,0.0269,0.0006,-0.0522,-0.0137,0.0561,0.0074,-0.0518,0.0165,-0.0606,0.0301,0.0433,0.0377,0.0466,-0.0077,-0.0038,-0.0123,-0.0012,-0.0329,-0.0113,0.0161,-0.0668,0.0066,-0.0036,-0.0748,-0.0189,-0.0282,0.0349,0.01,0.0237,0.0987,0.0045,-0.0359,-0.041,0.0711,-0.0272,-0.0212,0.0091,-0.0031,0.0129,0.0309,0.001,0.0241,-0.0012,-0.028,-0.2181,0.0198,0.0165,-0.0524,0.0456,-0.0656,0.0262,-0.0402,0.0274,0.0137,0.0054,-0.0114,0.0072,0.0042,-0.0335,0.0326,0.0428,0.0451,-0.0374,0.0092,-0.008,-0.0104,-0.0145,-0.1328,0.0438,-0.0367,0.2133,0.0529,0.0469,-0.0243,0.0468,0.0624,-0.0653,-0.0772,0.0531,0.0083,0.0535,-0.0247,-0.0187,-0.0242,-0.0117,0.0265,0.0031,-0.1087,-0.0527,-0.0584,-0.0435,0.0084,-0.0502,0.0354,0.0053,-0.0424,0.0362,-0.0461,-0.0266,-0.0673,-0.1136,0.0387,-0.0302,0.0077,0.0356,-0.0284,-0.0008,-0.0708,0.0513,-0.008,-0.0373,-0.0469,0.0418,-0.0582,-0.0137,0.0723,-0.0029,-0.0154,0.0264,-0.005,0.0036,-0.0244,-0.0858,-0.0201,0.0622,-0.0289,0.0266,0.0392,0.0412,-0.008,0.0791,0.0017,0.037,-0.0032,-0.0161,0.0582,-0.071,-0.0034,0.0245,-0.0306,-0.2846,0.0028,-0.0228,0.0286,0.0005,0.0272,0.0401,0.0107,-0.0504,-0.014,-0.0181,0.0794,0.0588,0.0274,0.0212,0.0197,0.0971,-0.0746,0.0239,-0.067,0.0044,0.039,0.1983,-0.0371,0.0238,-0.01,-0.0139,0.0192,0.0403,-0.0431,-0.0094,0.0096,0.095,-0.0362,0.0268,0.0776,-0.0466,0.0721,-0.0044,-0.0323,-0.031,-0.0362,-0.0516,-0.0054,0.1188,-0.0037,0.0288,-0.0045,-0.0674,0.0067,0.016,-0.0274,-0.0264,0.0112,0.0412,0.0225,-0.0377,-0.0368,-0.0305,-0.0432,0.0375,-0.0492,-0.0122,-0.0085,-0.0363]}
{"key":"[Machine learning applications for noisy intermediate-scale quantum computers] Quantum machine learning has proven to be a fruitful area in which to search for potential applications of quantum computers. This is particularly true for those available in the near term, so called noisy intermediate-scale quantum (NISQ) devices. In this Thesis, we develop and study three quantum machine learning applications suitable for NISQ computers, ordered in terms of increasing complexity of data presented to them. These algorithms are variational in nature and use parameterised quantum circuits (PQCs) as the underlying quantum machine learning model. The first application area is quantum classification using PQCs, where the data is classical feature vectors and their corresponding labels. Here, we study the robustness of certain data encoding strategies in such models against noise present in a quantum computer. The second area is generative modelling using quantum computers, where we use quantum circuit Born machines to learn and sample from complex probability distributions. We discuss and present a framework for quantum advantage for such models, propose gradient-based training methods and demonstrate these both numerically and on the Rigetti quantum computer up to 28 qubits. For our final application, we propose a variational algorithm in the area of approximate quantum cloning, where the data becomes quantum in nature. For the algorithm, we derive differentiable cost functions, prove theoretical guarantees such as faithfulness, and incorporate state of the art methods such as quantum architecture search. Furthermore, we demonstrate how this algorithm is useful in discovering novel implementable attacks on quantum cryptographic protocols, focusing on quantum coin flipping and key distribution as examples.","layer":0,"vector":[-0.0908,-0.0035,-0.0307,0.023,-0.0512,0.0284,0.0679,0.0172,0.0191,0.0258,0.0124,-0.0724,0.0426,0.0273,0.0534,0.0021,-0.0254,0.0164,-0.0427,0.0136,-0.0004,-0.0017,0.0045,-0.0678,0.0186,0.0006,0.0026,-0.0324,-0.0258,-0.203,0.0069,-0.041,0.0175,-0.032,0.0334,-0.0462,-0.0354,0.0369,-0.012,0.0216,0.032,0.0056,-0.0004,-0.0204,0.0125,-0.0667,-0.0417,-0.0539,-0.0416,-0.0546,0.0058,0.0177,0.0261,0.06,0.0683,0.0188,0.074,0.0752,0.0329,0.0476,0.0088,0.0844,-0.1212,0.0592,0.0447,0.0357,-0.0254,-0.0577,0.0353,0.0489,-0.0425,0.0334,-0.0092,0.039,0.0251,-0.0106,-0.0141,-0.0407,-0.0281,0.0129,-0.0182,-0.0657,-0.0424,0.0186,-0.0877,-0.0263,-0.0059,-0.0123,0.0144,-0.0257,-0.0055,-0.033,-0.0398,-0.0068,-0.017,0.0163,0.0185,0.0329,-0.0633,0.2308,-0.0438,0.0384,-0.0013,-0.0367,0.0481,-0.0703,-0.0438,-0.0461,-0.0518,-0.0363,0.0132,-0.0151,0.0415,-0.0514,-0.0215,-0.0132,0.0475,0.0217,0.002,0.0061,-0.0686,-0.0225,0.0181,-0.0219,-0.001,-0.0314,-0.0372,0.1566,0.0171,0.065,0.0221,-0.0129,0.0221,-0.0326,0.0267,0.0234,0.023,0.0011,0.078,-0.0199,-0.0258,-0.034,0.033,-0.0932,-0.0466,0.0644,-0.033,0.0587,0.0092,-0.0577,0.0235,0.0142,-0.0057,-0.0278,0.0586,0.0973,0.0142,0.0443,-0.0504,0.0432,-0.0354,-0.0433,0.0046,0.1141,0.0267,-0.0991,-0.0335,0.0033,0.0049,-0.0022,0.0496,0.0587,-0.0518,0.0325,0.0202,-0.0311,-0.0348,0.0117,-0.0162,0.0335,-0.0473,-0.0827,-0.0382,0.011,0.0498,-0.0285,0.0024,-0.0475,0.0359,0.0008,-0.0258,0.0214,-0.0493,-0.0207,-0.0218,-0.0274,-0.0632,0.0054,-0.0076,-0.032,0.0232,-0.0038,-0.0156,0.0313,-0.0575,-0.0273,0.0309,-0.0087,0.0524,0.0144,-0.0059,-0.0372,0.0175,-0.0392,-0.0822,0.0096,0.0467,0.0249,-0.0168,0.0253,0.0249,-0.062,-0.0904,-0.2436,-0.0313,-0.0184,-0.0336,0.088,-0.0711,0.0321,-0.0119,0.0353,0.0494,0.0324,0.0196,0.0017,0.0226,-0.0181,0.0685,0.078,0.0341,-0.0201,0.0438,-0.031,0.0239,-0.0171,-0.1128,0.0027,0.005,0.2193,0.0101,0.037,0.0325,0.0214,0.0307,-0.0601,-0.0644,0.0623,0.0118,0.0362,0.0381,-0.0278,0.009,-0.0115,0.0352,0.0061,-0.1155,0.0001,-0.0086,-0.0514,0.0196,-0.0395,0.0362,0.0528,-0.0117,0.0494,-0.0157,-0.011,-0.0708,-0.0774,0.0121,-0.0254,0.0332,0.0089,-0.0502,0.0245,-0.0058,0.0264,-0.0362,-0.024,-0.0132,0.0914,-0.0455,-0.0357,0.0759,0.0029,0.0091,0.0498,-0.0204,0.0466,-0.0349,-0.0268,0.0264,0.0576,0.0257,0.0398,0.0181,0.0129,-0.0157,0.0742,0.0201,0.0218,-0.0152,0.0125,-0.0016,-0.0426,0.0354,0.0136,0.0071,-0.3005,0.0196,0.0259,0.0491,-0.0278,-0.0081,0.0483,0.0283,-0.0739,0.0178,-0.0227,0.0225,0.0402,-0.0108,0.0255,0.055,0.0697,-0.0378,0.0003,-0.0161,0.037,0.0572,0.2385,-0.0162,0.0464,0.0026,-0.0069,0.0403,0.003,-0.0764,-0.0134,-0.0356,0.0633,-0.0588,0.0647,0.0426,-0.0357,0.0218,0.0009,-0.0246,-0.0014,0.0144,-0.0731,-0.0058,0.137,0.0038,-0.0232,-0.0661,0.017,0.0254,-0.0142,0.0404,-0.0158,0.0059,0.0191,0.0564,-0.0373,-0.0542,-0.0135,0.0065,0.0646,-0.0503,0.003,0.0171,-0.0354]}
{"key":"[An Improved Subject-Independent Stress Detection Model Applied to Consumer-grade Wearable Devices] Stress is a complex issue with wide-ranging physical and psychological impacts on human daily performance. Specifically, acute stress detection is becoming a valuable application in contextual human understanding. Two common approaches to training a stress detection model are subject-dependent and subject-independent training methods. Although subject-dependent training methods have proven to be the most accurate approach to build stress detection models, subject-independent models are a more practical and cost-efficient method, as they allow for the deployment of stress level detection and management systems in consumer-grade wearable devices without requiring training data for the end-user. To improve the performance of subject-independent stress detection models, in this paper, we introduce a stress-related bio-signal processing pipeline with a simple neural network architecture using statistical features extracted from multimodal contextual sensing sources including Electrodermal Activity (EDA), Blood Volume Pulse (BVP), and Skin Temperature (ST) captured from a consumer-grade wearable device. Using our proposed model architecture, we compare the accuracy between stress detection models that use measures from each individual signal source, and one model employing the fusion of multiple sensor sources. Extensive experiments on the publicly available WESAD dataset demonstrate that our proposed model outperforms conventional methods as well as providing 1.63% higher mean accuracy score compared to the state-of-the-art model while maintaining a low standard deviation. Our experiments also show that combining features from multiple sources produce more accurate predictions than using only one sensor source individually.","layer":0,"vector":[-0.0355,0.018,0.0728,-0.0165,0.0302,0.0352,0.086,-0.0014,0.0119,-0.0363,-0.0062,-0.0408,0.015,0.0369,0.0651,-0.0186,0.071,0.0219,-0.0385,0.0422,0.0294,0.0117,0.0108,-0.024,0.0082,0.0074,-0.0286,0.0156,-0.0892,-0.2399,0.018,-0.0616,0.0709,-0.0175,0.0121,-0.0306,-0.0543,0.0206,-0.0021,0.0435,-0.0243,0.0112,-0.0435,-0.0611,-0.0201,-0.0565,-0.0186,-0.0108,-0.0204,-0.0302,0.0597,-0.0098,0.0506,0.0165,0.0379,0.0087,0.0554,0.0349,0.0442,0.0324,0.0043,0.0741,-0.1551,0.0459,0.0516,0.0347,-0.0259,-0.0169,0.0072,-0.0016,-0.0347,0.0228,0.0642,0.0188,0.0011,0.0012,0.0278,-0.066,0.0146,0.0208,0.0123,0.0035,-0.0289,-0.0012,0.0113,-0.0567,-0.0129,-0.0593,0.0274,0.0,-0.0881,0.0129,-0.0181,0.0374,-0.0573,-0.0463,0.0095,0.0118,-0.0861,0.2058,-0.0467,0.0263,0.0357,-0.0028,0.0481,-0.0121,-0.0227,-0.04,-0.0809,0.0165,0.043,-0.0083,0.0252,-0.0127,0.0439,0.0196,0.0633,0.0428,0.0556,-0.0124,-0.036,-0.016,0.0436,-0.0479,0.0108,-0.0691,0.0568,0.1301,0.0462,-0.0172,0.084,-0.0194,-0.0812,-0.0174,0.0024,-0.008,0.0308,0.0179,0.0395,0.0156,-0.03,-0.0127,0.0476,-0.1092,-0.0578,0.1371,-0.0111,0.0249,-0.0625,-0.0245,-0.0372,0.0223,-0.0567,-0.0031,0.0531,0.0371,0.0244,-0.0011,-0.0672,-0.012,-0.0027,-0.0298,-0.0284,0.0608,0.0454,-0.0552,-0.0099,0.0154,0.0009,-0.0246,0.0805,-0.0103,-0.0348,0.0508,0.0596,0.0271,-0.0403,-0.0109,-0.0113,0.0279,0.041,-0.0618,-0.0372,-0.0167,0.0029,-0.0829,0.022,-0.0558,-0.0025,0.0605,-0.0377,-0.0482,-0.0151,0.0462,-0.0067,-0.0083,-0.0318,0.0046,0.0131,0.0037,0.0215,-0.0422,-0.01,0.052,0.0064,0.0431,-0.045,-0.0073,0.0371,0.0132,0.0485,0.0032,0.1147,-0.0519,-0.0203,-0.027,0.0417,0.0659,0.0047,0.052,0.025,-0.0415,-0.0779,-0.2676,-0.0209,0.0419,-0.0021,0.0338,-0.0388,0.0506,-0.0079,0.0349,0.0707,0.1207,-0.0045,-0.0195,-0.0022,0.0051,0.0852,0.0365,-0.009,-0.0404,-0.0084,-0.0114,0.0359,-0.0023,-0.0529,0.0234,-0.0097,0.185,0.0042,0.0158,-0.0161,-0.0114,0.0226,-0.0542,-0.1242,0.062,0.0172,0.0532,-0.0231,-0.0485,-0.0367,-0.0527,0.0422,-0.0,-0.07,-0.0744,-0.0328,0.0055,0.0158,-0.0868,0.0091,0.0399,-0.0284,0.053,-0.0568,0.0102,-0.0667,-0.1144,0.0194,-0.0749,-0.0007,-0.0126,-0.0196,0.0678,-0.0533,0.0452,0.0015,-0.0131,-0.0179,0.0283,-0.0505,-0.0064,0.1241,0.0305,-0.0153,0.0468,-0.0419,0.0351,-0.0322,-0.0383,-0.0116,0.0548,-0.037,0.0102,0.046,0.0248,-0.0207,0.1041,-0.0197,0.0626,-0.0444,-0.0217,0.0213,-0.0327,-0.0119,0.0488,-0.012,-0.2857,0.0334,0.0067,-0.0017,-0.0412,0.0191,-0.0116,0.0412,-0.0466,-0.0,-0.0422,0.0461,0.0159,-0.0141,0.0139,0.0443,0.0559,-0.0455,0.0376,-0.0439,0.0238,0.0578,0.1587,-0.0272,0.0232,0.0744,-0.0151,-0.0033,0.0402,-0.0176,0.0111,0.0102,0.0603,-0.0289,0.0295,0.0394,-0.0454,-0.0035,-0.0097,-0.0175,0.0435,-0.0082,-0.026,-0.0764,0.0865,0.0129,-0.0465,-0.0294,-0.0464,0.0132,0.0176,0.0017,0.0289,0.0031,-0.0087,0.0086,-0.0028,-0.0608,0.0019,-0.0539,0.0169,-0.0403,-0.0443,-0.0016,-0.0191]}
{"key":"[Automatic Cough Classification for Tuberculosis Screening in a Real-World Environment] Objective: The automatic discrimination between the coughing sounds produced by patients with tuberculosis (TB) and those produced by patients with other lung ailments. Approach: We present experiments based on a dataset of 1358 forced cough recordings obtained in a developing-world clinic from 16 patients with confirmed active pulmonary TB and 35 patients suffering from respiratory conditions suggestive of TB but confirmed to be TB negative. Using nested cross-validation, we have trained and evaluated five machine learning classifiers: logistic regression (LR), support vector machines (SVM), k-nearest neighbour (KNN), multilayer perceptrons (MLP) and convolutional neural networks (CNN). Main Results: Although classification is possible in all cases, the best performance is achieved using LR. In combination with feature selection by sequential forward selection (SFS), our best LR system achieves an area under the ROC curve (AUC) of 0.94 using 23 features selected from a set of 78 high-resolution mel-frequency cepstral coefficients (MFCCs). This system achieves a sensitivity of 93\\% at a specificity of 95\\% and thus exceeds the 90\\% sensitivity at 70\\% specificity specification considered by the World Health Organisation (WHO) as a minimal requirement for a community-based TB triage test. Significance: The automatic classification of cough audio sounds, when applied to symptomatic patients requiring investigation for TB, can meet the WHO triage specifications for the identification of patients who should undergo expensive molecular downstream testing. This makes it a promising and viable means of low cost, easily deployable frontline screening for TB, which can benefit especially developing countries with a heavy TB burden.","layer":6,"vector":[-0.0526,0.0109,0.0015,-0.0322,0.0733,-0.0187,0.0684,0.0127,-0.0154,-0.0069,-0.0146,-0.0454,-0.0031,0.0387,0.0121,0.0139,0.0457,0.0156,-0.0519,0.0298,0.0069,0.0483,0.0064,-0.0618,0.0759,-0.0052,-0.0226,-0.0839,-0.0731,-0.2114,0.045,-0.0528,0.0263,-0.0452,0.0124,-0.0169,-0.0046,0.071,-0.0773,0.0127,0.0077,0.0096,0.0393,-0.121,-0.0406,-0.0679,-0.0441,-0.0175,-0.015,-0.0445,0.0294,-0.0607,0.0262,0.0217,-0.0302,-0.0181,0.0348,0.0472,0.0101,0.0654,0.0273,0.0416,-0.1767,0.0364,0.0141,0.0356,-0.0115,-0.0236,0.0266,0.0516,0.0016,0.0416,0.0319,0.0629,-0.0122,0.0036,0.0102,-0.0231,-0.0271,0.0443,0.0172,0.0316,-0.0301,-0.0345,-0.0133,-0.0181,0.03,-0.0632,0.0002,-0.0068,-0.0311,0.0138,-0.0273,0.0664,-0.0874,-0.0128,0.0477,0.0084,-0.0498,0.1983,-0.0361,-0.0325,0.004,-0.0325,0.0448,-0.0337,-0.0467,-0.0894,-0.0126,-0.0089,-0.0046,-0.0407,0.014,-0.0125,-0.0217,0.0178,0.0355,-0.0078,0.0143,0.011,-0.0182,-0.0286,0.0455,-0.0026,0.0563,-0.044,0.0112,0.1458,-0.0053,0.0106,0.0938,-0.0281,-0.056,-0.0297,-0.0031,0.0217,-0.0027,0.0195,-0.0151,0.001,-0.0281,-0.1032,0.0144,-0.0718,-0.1126,0.0618,-0.0848,0.0486,-0.0329,-0.0746,0.0057,0.0134,-0.0071,0.0049,0.0697,0.0277,0.0204,0.0551,-0.0025,0.0554,-0.0215,-0.0918,-0.015,0.1117,-0.013,-0.0845,-0.0414,-0.0387,-0.0091,-0.0233,0.0445,0.0504,-0.049,0.0248,0.0617,0.0296,-0.0413,-0.0058,-0.0106,-0.012,0.0056,-0.0544,0.0096,0.0024,0.0586,-0.057,-0.0026,-0.0587,0.0784,0.0303,-0.0218,0.0209,-0.0194,-0.0053,0.0269,-0.0381,0.0077,0.0028,0.0472,0.0114,0.0561,0.018,0.0114,0.0122,0.0059,0.0391,0.0072,0.0304,0.0352,0.0208,0.0041,0.0204,0.0895,-0.027,-0.0476,-0.0134,-0.0038,0.0413,-0.0177,0.0964,0.0392,-0.0121,-0.0721,-0.2198,0.0159,0.0536,-0.0096,0.0686,-0.0421,0.0315,0.0061,0.0395,0.0602,0.0948,0.0123,-0.0308,0.0365,0.0051,0.0855,0.0413,0.0088,-0.0245,0.003,0.0184,0.009,0.0091,-0.0968,0.0363,-0.0076,0.228,0.0174,0.0441,-0.0034,-0.0217,-0.0127,0.0117,-0.1465,0.036,0.0477,0.018,-0.0042,-0.0705,-0.0248,-0.0718,0.0393,0.0096,-0.0945,-0.0403,-0.0131,-0.0041,0.0341,-0.0779,-0.0209,-0.0008,0.0233,0.0261,0.0142,0.007,-0.0461,-0.0882,0.02,-0.0419,0.023,0.0012,-0.0386,0.0176,-0.0444,-0.0214,-0.0278,-0.0253,-0.0172,0.0723,0.0135,0.0022,0.1057,-0.0221,-0.0432,0.0768,-0.0164,0.0562,-0.0765,-0.0642,-0.0101,0.0503,0.0015,0.0196,-0.0125,0.0114,0.0058,0.068,0.0202,0.0082,-0.0189,-0.0017,0.0483,-0.0343,-0.0033,0.0219,-0.0078,-0.2715,0.0305,0.0067,0.0406,-0.0364,0.0252,0.004,0.0212,-0.0276,0.0132,0.0079,0.0245,0.0349,-0.0536,0.0088,0.0126,0.0604,-0.0797,0.0836,-0.0462,-0.0288,0.04,0.2068,-0.0409,0.0268,0.0399,0.0158,0.0173,-0.0017,-0.0225,0.0812,-0.0093,0.0981,-0.0399,-0.032,0.0696,-0.0389,0.0262,0.0056,-0.0027,-0.009,0.0101,-0.0333,-0.0395,0.0849,-0.0194,-0.0119,-0.0492,-0.0005,0.0478,-0.0133,0.022,-0.0078,0.0241,0.0159,0.0443,0.002,-0.023,-0.0135,-0.0414,0.0257,-0.081,-0.0188,0.0053,-0.0017]}
{"key":"[Matrix Tile Analysis] Many tasks require finding groups of elements in a matrix of numbers, symbols or class likelihoods. One approach is to use efficient bi- or tri-linear factorization techniques including PCA, ICA, sparse matrix factorization and plaid analysis. These techniques are not appropriate when addition and multiplication of matrix elements are not sensibly defined. More directly, methods like bi-clustering can be used to classify matrix elements, but these methods make the overly-restrictive assumption that the class of each element is a function of a row class and a column class. We introduce a general computational problem, `matrix tile analysis' (MTA), which consists of decomposing a matrix into a set of non-overlapping tiles, each of which is defined by a subset of usually nonadjacent rows and columns. MTA does not require an algebra for combining tiles, but must search over discrete combinations of tile assignments. Exact MTA is a computationally intractable integer programming problem, but we describe an approximate iterative technique and a computationally efficient sum-product relaxation of the integer program. We compare the effectiveness of these methods to PCA and plaid on hundreds of randomly generated tasks. Using double-gene-knockout data, we show that MTA finds groups of interacting yeast genes that have biologically-related functions.","layer":1,"vector":[-0.091,0.0132,0.0177,-0.0109,0.0064,0.0237,0.0499,0.0396,0.0074,-0.0099,0.0197,-0.0796,0.0185,0.0334,-0.0098,0.0044,0.0196,0.091,-0.0639,-0.0351,0.0259,-0.0607,-0.0359,-0.0461,0.0475,0.0485,-0.0343,-0.0322,-0.0453,-0.252,-0.0196,0.022,0.0955,-0.0493,0.0052,-0.0082,-0.0278,0.002,-0.0354,0.006,0.0421,0.0448,-0.0305,0.0209,-0.0342,-0.0274,-0.0323,0.017,0.0015,-0.0381,0.0193,-0.0451,0.0214,0.0591,0.0209,-0.0128,0.0838,0.0304,0.0244,0.0369,0.0335,0.003,-0.1322,0.0969,0.0889,-0.0127,-0.058,-0.0373,0.052,0.053,-0.0532,0.082,-0.0018,0.0454,0.0445,0.0183,0.0003,-0.0168,0.0187,-0.0122,-0.021,-0.0105,-0.0073,0.0313,-0.0161,-0.0278,0.0171,-0.0402,0.0449,-0.0238,-0.0438,0.0152,-0.0126,0.0433,-0.0905,-0.0075,0.0494,0.0163,-0.0659,0.2192,-0.0635,0.0165,0.0599,-0.0002,0.0037,-0.0368,-0.0206,-0.0373,-0.0121,-0.0138,0.0116,-0.045,0.0103,-0.0584,0.0392,-0.0298,0.0767,0.0281,-0.034,0.0182,-0.005,-0.0149,0.0581,0.0172,0.0352,-0.0399,-0.0104,0.1391,0.0345,0.0204,0.0677,0.0216,-0.0694,-0.0214,0.027,-0.001,0.0168,-0.0064,-0.0344,-0.0093,-0.0154,-0.083,0.0616,-0.0687,-0.0154,0.1283,-0.0268,-0.0079,-0.0249,-0.0217,-0.0284,0.0383,-0.0351,0.0054,-0.0021,-0.0037,0.0404,0.0327,-0.0398,0.025,-0.0597,-0.0299,-0.0101,0.0873,0.0415,-0.0651,-0.0337,-0.0015,0.0069,-0.0324,0.0424,0.0478,-0.0169,0.0661,0.0117,0.0054,-0.0853,0.0145,0.0525,0.0604,0.0473,-0.0187,-0.0463,0.0521,0.0065,-0.02,-0.0148,0.0009,-0.028,0.008,-0.0339,0.0353,-0.0298,0.0193,-0.0239,-0.0376,0.0208,-0.0319,0.009,-0.0166,0.0462,0.0468,-0.046,0.045,-0.0138,-0.0096,-0.0332,-0.0177,0.0464,0.0433,-0.0264,-0.0382,0.0224,-0.0233,-0.0019,0.0091,0.0349,0.04,0.0194,0.0508,0.0415,-0.0473,-0.1008,-0.2548,-0.0172,0.0114,-0.0106,-0.0289,-0.0837,0.0131,-0.0507,0.0011,0.0768,0.0273,0.0004,-0.0043,0.0428,-0.0307,0.0487,0.0337,0.018,-0.0376,-0.0222,-0.0055,0.0069,0.0063,-0.0391,0.0485,0.0029,0.2317,0.0437,0.0309,-0.0076,0.0628,0.0235,-0.0037,-0.0433,0.0665,0.0021,0.065,-0.0095,-0.0155,-0.018,-0.0287,-0.0139,-0.0419,-0.0888,-0.0154,-0.0298,0.0095,0.0066,-0.0155,-0.0111,0.0753,-0.0756,0.069,-0.0328,0.0219,-0.0302,-0.0546,0.0083,-0.0449,0.0155,-0.0039,-0.0907,0.0252,-0.0172,0.0476,-0.0289,-0.0516,0.0033,-0.0016,-0.0609,-0.0313,0.0692,0.0252,-0.0096,0.0448,-0.02,0.0155,-0.0498,-0.0276,-0.0231,0.0732,-0.0691,0.0308,0.0072,0.04,0.0167,0.0427,0.0288,0.0332,-0.0318,-0.0091,0.0265,-0.0334,0.0034,0.0211,-0.0038,-0.2912,0.0591,-0.0188,-0.0121,-0.0314,-0.0299,0.0364,-0.0149,-0.0273,-0.0312,0.0652,0.0389,0.0805,0.005,0.0225,0.0563,0.0905,-0.063,0.0296,-0.0541,0.0252,0.0068,0.2261,-0.0167,0.0508,-0.0026,-0.0417,-0.0122,-0.0023,-0.0106,0.0068,0.0161,0.0841,-0.0487,0.0186,0.044,-0.0068,0.0499,0.0004,-0.0079,-0.0312,-0.0047,-0.096,-0.0404,0.1124,-0.0211,-0.0286,-0.051,0.017,0.03,-0.0312,0.0066,-0.0328,-0.0051,-0.0269,0.0268,-0.0614,-0.0425,-0.014,0.0128,-0.0066,-0.052,-0.019,0.0405,-0.0518]}
{"key":"[CapillaryX: A Software Design Pattern for Analyzing Medical Images in Real-time using Deep Learning] Recent advances in digital imaging, e.g., increased number of pixels captured, have meant that the volume of data to be processed and analyzed from these images has also increased. Deep learning algorithms are state-of-the-art for analyzing such images, given their high accuracy when trained with a large data volume of data. Nevertheless, such analysis requires considerable computational power, making such algorithms time- and resource-demanding. Such high demands can be met by using third-party cloud service providers. However, analyzing medical images using such services raises several legal and privacy challenges and does not necessarily provide real-time results. This paper provides a computing architecture that locally and in parallel can analyze medical images in real-time using deep learning thus avoiding the legal and privacy challenges stemming from uploading data to a third-party cloud provider. To make local image processing efficient on modern multi-core processors, we utilize parallel execution to offset the resource-intensive demands of deep neural networks. We focus on a specific medical-industrial case study, namely the quantifying of blood vessels in microcirculation images for which we have developed a working system. It is currently used in an industrial, clinical research setting as part of an e-health application. Our results show that our system is approximately 78% faster than its serial system counterpart and 12% faster than a master-slave parallel system architecture.","layer":4,"vector":[-0.0401,-0.0144,-0.0032,-0.0377,0.0356,0.0191,0.0773,0.0104,0.0294,-0.0018,-0.0037,-0.0698,0.0207,0.0208,-0.0116,-0.0273,0.001,0.0016,-0.0335,-0.0033,0.0194,-0.0366,-0.0589,-0.1009,0.024,-0.0117,-0.0281,-0.0531,-0.0513,-0.2089,0.0344,-0.0518,0.0354,0.0041,0.0516,-0.0266,-0.0044,0.0323,-0.0379,-0.01,0.0223,0.0336,-0.0617,0.0196,-0.015,-0.0354,-0.0332,-0.0225,0.0189,-0.0022,0.0437,-0.0396,0.0502,0.0635,0.0258,0.0085,0.0801,-0.0032,0.0464,0.0172,0.0472,0.0678,-0.169,0.0761,0.0416,-0.0006,0.0103,-0.0333,0.0218,0.0127,-0.0074,0.0332,0.0235,0.0485,-0.0117,0.007,0.015,-0.0308,-0.0287,-0.0154,0.0412,-0.0069,-0.0172,0.005,-0.0509,-0.0107,0.0043,-0.0657,0.0221,-0.003,-0.0721,-0.0179,-0.0478,0.0066,-0.045,0.0121,-0.0054,0.0139,-0.0583,0.1994,-0.0756,0.0108,0.0405,-0.031,0.0726,-0.0403,-0.0382,-0.0292,-0.0082,0.0229,0.0203,-0.0282,0.0327,0.0431,0.0144,0.0366,0.0114,0.0264,-0.0067,0.0233,-0.0241,0.0044,0.0498,0.0079,0.0639,-0.0645,0.0374,0.1547,-0.0132,0.031,0.0439,0.0145,-0.0361,-0.0332,0.0096,0.0342,0.0021,0.0124,0.0034,-0.0164,-0.0873,-0.0097,-0.0021,-0.0615,-0.0124,0.136,0.0071,0.0384,-0.0704,-0.0912,-0.0677,-0.0078,-0.0837,-0.0092,0.0132,0.0048,0.006,0.0408,-0.0253,0.0045,0.0067,-0.0656,-0.0108,0.1441,0.0444,-0.0927,0.0291,-0.0063,0.0124,0.0019,0.0346,0.0165,-0.0302,-0.0069,0.021,0.0211,-0.0539,-0.0462,0.0166,0.0131,0.0275,-0.0154,0.0134,0.0333,0.0395,-0.0776,0.0199,-0.0458,0.0191,0.0558,-0.0482,0.0677,-0.0529,0.0056,0.0069,-0.0642,-0.0059,-0.005,0.0339,0.0019,0.0512,-0.0053,0.0085,0.0306,0.0283,0.0268,-0.0549,0.0374,0.0128,0.0303,-0.0268,0.0376,0.0461,-0.0272,-0.0047,0.0211,0.0359,0.0037,0.0073,0.0748,0.053,-0.0479,-0.0621,-0.2092,0.0081,-0.0099,-0.0214,0.0472,-0.0567,0.0515,-0.0199,0.0149,0.0565,0.0822,-0.0367,-0.0122,-0.0231,-0.0082,0.0594,0.0556,0.029,-0.0462,-0.0144,0.0097,0.0054,-0.021,-0.0877,0.0287,0.0238,0.257,-0.0211,0.0083,-0.0336,0.0128,0.0029,-0.0154,-0.1394,0.0093,-0.0142,0.0499,-0.0187,-0.027,0.0236,-0.0617,0.0359,0.0027,-0.0796,-0.0185,-0.029,-0.0058,0.0283,-0.0756,-0.0218,0.0116,-0.0841,-0.0119,0.0286,0.0188,-0.0685,-0.1022,0.0538,-0.0521,0.0446,0.0043,-0.0559,-0.0085,-0.0596,0.0673,-0.0034,-0.0071,-0.0639,0.0088,-0.0414,-0.003,0.0798,0.0192,-0.0075,0.0899,0.0359,0.0794,-0.0276,-0.0085,-0.0096,0.0709,-0.0281,0.0281,0.0669,0.0251,0.033,0.0643,0.0297,-0.0143,-0.0429,0.0214,0.0171,-0.0888,-0.0149,0.0223,-0.0027,-0.2971,0.0411,-0.0137,0.0344,-0.0137,0.0011,0.0248,0.0172,-0.0126,0.0045,-0.0202,0.0371,0.033,-0.0228,-0.0226,0.0416,0.0869,-0.0039,0.0629,-0.0277,-0.0192,0.0112,0.2098,-0.0773,-0.0236,0.0321,-0.0334,0.0041,0.0355,0.0151,0.0088,-0.0034,0.066,-0.092,0.0277,0.0769,-0.0529,0.0392,0.0355,0.0128,0.0433,0.0128,-0.0249,-0.0037,0.0691,0.0218,-0.0222,-0.0328,0.0157,0.0309,-0.0225,-0.0062,-0.0016,-0.0134,0.0388,0.0179,-0.0349,-0.0912,-0.0472,-0.0487,0.0291,-0.0704,-0.0177,0.0336,-0.0204]}
{"key":"[Beating Attackers At Their Own Games: Adversarial Example Detection Using Adversarial Gradient Directions] Adversarial examples are input examples that are specifically crafted to deceive machine learning classifiers. State-of-the-art adversarial example detection methods characterize an input example as adversarial either by quantifying the magnitude of feature variations under multiple perturbations or by measuring its distance from estimated benign example distribution. Instead of using such metrics, the proposed method is based on the observation that the directions of adversarial gradients when crafting (new) adversarial examples play a key role in characterizing the adversarial space. Compared to detection methods that use multiple perturbations, the proposed method is efficient as it only applies a single random perturbation on the input example. Experiments conducted on two different databases, CIFAR-10 and ImageNet, show that the proposed detection method achieves, respectively, 97.9% and 98.6% AUC-ROC (on average) on five different adversarial attacks, and outperforms multiple state-of-the-art detection methods. Results demonstrate the effectiveness of using adversarial gradient directions for adversarial example detection.","layer":1,"vector":[-0.0204,-0.02,0.0044,-0.0265,0.0237,0.0064,0.0541,-0.0052,-0.0116,-0.0264,-0.0072,-0.0266,-0.009,0.0677,0.0113,0.0042,-0.0023,0.0291,-0.0565,0.0089,0.0371,-0.0066,0.0074,-0.0212,0.0149,0.0262,-0.006,-0.0294,-0.049,-0.226,0.0103,-0.0822,0.0247,-0.0481,0.0253,-0.0331,-0.073,0.0562,-0.0625,0.0109,0.0199,0.0384,-0.0328,-0.0409,-0.0239,-0.0382,-0.0243,-0.0201,0.001,-0.0564,0.0676,-0.036,0.065,0.0093,0.0427,-0.0294,0.0893,0.0668,0.0353,0.0649,0.0462,0.0547,-0.1591,0.0244,0.0388,0.0579,-0.005,-0.0454,0.0011,0.0545,-0.0041,0.0422,-0.0055,0.0264,-0.0208,0.0498,-0.0224,-0.0308,-0.018,-0.0261,0.068,0.0057,-0.0056,0.0156,0.0046,-0.043,-0.0025,-0.0111,0.0964,-0.0084,-0.0048,0.0195,-0.0273,0.0518,-0.0233,-0.0174,0.0277,0.0111,-0.076,0.2031,-0.062,0.014,0.0298,-0.0519,0.0532,-0.017,-0.057,-0.0592,0.0019,-0.0004,-0.0205,0.0056,0.0398,-0.0578,0.0329,-0.0157,0.0342,0.0189,-0.0672,-0.0183,-0.0361,-0.0015,0.031,-0.0251,0.0441,-0.0674,0.0213,0.1206,0.0474,0.0154,0.0292,-0.0135,-0.0374,0.0111,-0.0021,0.0378,-0.0051,0.0554,0.0453,0.0105,-0.0522,-0.0538,0.0457,-0.0788,-0.0361,0.097,-0.0334,0.0609,-0.0171,-0.042,0.0245,0.0316,-0.0492,0.0155,0.0192,0.038,0.0337,0.0815,-0.0287,-0.0028,-0.0328,-0.058,-0.0218,0.1391,0.0383,-0.1142,-0.0254,-0.0155,-0.0006,-0.0246,0.0219,0.0218,-0.048,0.0246,0.0509,0.0162,-0.106,-0.03,-0.0229,0.034,0.0235,-0.0526,-0.0418,0.0552,0.0632,-0.0293,0.0132,-0.0576,0.0435,0.0492,-0.0668,0.0388,-0.0296,-0.0172,-0.0597,-0.0146,0.0022,-0.023,-0.0031,-0.046,-0.0223,-0.0066,-0.032,0.0132,-0.0114,0.0189,-0.011,-0.047,0.0261,0.0162,-0.0184,-0.0063,0.0058,-0.0323,-0.0224,-0.0487,-0.0183,0.0872,-0.0239,0.0581,0.0048,-0.0408,-0.0199,-0.2585,-0.0442,-0.0088,-0.0439,0.0016,-0.1006,0.0066,-0.0003,0.0553,0.0277,0.0687,-0.0082,0.0095,-0.0105,0.0306,0.0705,-0.0167,0.0501,0.0097,0.0208,-0.0291,0.0026,0.0065,-0.0661,0.0399,0.0118,0.216,0.0714,0.0187,-0.0502,0.0045,-0.0191,-0.0207,-0.0833,0.0634,0.008,0.0635,-0.0179,-0.0348,-0.0182,-0.0115,0.0651,0.0216,-0.0883,-0.0245,-0.0071,-0.0717,-0.0001,-0.0559,0.067,0.0347,-0.0015,0.0553,0.014,0.0349,-0.0447,-0.1083,0.029,-0.0193,0.0451,0.0142,-0.0604,0.029,-0.0857,0.0665,0.0222,-0.0337,-0.0659,0.0415,-0.0093,-0.0062,0.1007,0.0554,-0.0266,0.052,-0.0068,0.0408,-0.0667,-0.0688,-0.0171,0.0337,0.015,-0.0155,0.0187,0.029,-0.0122,0.0532,0.0238,0.0786,-0.0613,-0.0138,0.0366,-0.0494,-0.0225,0.043,0.0245,-0.2924,0.0337,0.0528,0.0703,-0.0356,0.041,0.0545,0.0012,-0.0485,-0.0157,-0.0257,0.0129,0.0326,-0.0476,-0.0081,0.0064,0.021,-0.031,0.0454,-0.0369,0.0336,0.044,0.2084,-0.0175,-0.0064,0.0033,-0.0101,0.0455,0.0232,-0.0311,0.0138,-0.0077,0.0843,-0.0278,0.0075,0.0917,-0.0236,-0.0085,0.0001,-0.0142,-0.0405,0.0284,-0.0747,0.0241,0.0735,0.013,0.0289,-0.0207,0.0098,0.0274,-0.0177,-0.0205,0.0168,-0.0326,0.0632,0.0042,-0.0561,-0.0358,-0.0408,-0.0164,0.0152,-0.0253,0.0031,0.0076,-0.0093]}
{"key":"[Axiomatic Explanations for Visual Search, Retrieval, and Similarity Learning] Visual search, recommendation, and contrastive similarity learning power technologies that impact billions of users worldwide. Modern model architectures can be complex and difficult to interpret, and there are several competing techniques one can use to explain a search engine's behavior. We show that the theory of fair credit assignment provides a $\\textit{unique}$ axiomatic solution that generalizes several existing recommendation- and metric-explainability techniques in the literature. Using this formalism, we show when existing approaches violate \"fairness\" and derive methods that sidestep these shortcomings and naturally handle counterfactual information. More specifically, we show existing approaches implicitly approximate second-order Shapley-Taylor indices and extend CAM, GradCAM, LIME, SHAP, SBSM, and other methods to search engines. These extensions can extract pairwise correspondences between images from trained $\\textit{opaque-box}$ models. We also introduce a fast kernel-based method for estimating Shapley-Taylor indices that require orders of magnitude fewer function evaluations to converge. Finally, we show that these game-theoretic measures yield more consistent explanations for image similarity architectures.","layer":5,"vector":[-0.0269,-0.0545,-0.0311,-0.0132,0.0355,0.0094,0.0328,0.0394,0.0277,-0.0352,0.0288,-0.0743,0.0056,0.0519,0.0202,0.0246,0.0135,0.0273,-0.0817,-0.0155,0.0723,-0.0051,-0.0285,-0.0561,-0.015,0.0261,-0.0091,-0.0357,-0.0423,-0.2218,0.0196,-0.0451,0.0721,0.0114,-0.0035,-0.0478,-0.0275,0.0578,-0.0588,0.0183,-0.0127,-0.0094,-0.0328,-0.0071,-0.0272,-0.0403,-0.0211,0.0069,-0.0271,-0.067,0.0198,-0.0044,0.0027,-0.0059,0.0333,0.0256,0.0675,0.0673,0.0146,0.0553,0.0699,0.02,-0.146,0.0786,0.064,0.0155,-0.0101,-0.0114,0.0093,0.055,-0.0131,0.0778,-0.0342,0.0532,-0.0161,-0.0394,-0.0124,-0.0328,-0.0428,-0.0194,-0.0056,-0.0203,-0.0184,-0.0269,0.006,-0.0289,0.0359,-0.042,0.0643,0.012,-0.0176,-0.0126,-0.0523,-0.0209,-0.046,0.0015,0.0263,0.0332,-0.051,0.1983,-0.0269,0.0689,0.0363,-0.0478,0.0717,-0.0104,-0.0336,-0.0032,-0.0011,-0.0123,-0.0048,-0.0,0.055,0.0005,0.037,0.033,0.0224,0.0366,0.0394,-0.0243,-0.0398,0.0213,0.0431,-0.0351,0.0243,-0.0386,0.0009,0.1451,0.019,0.0017,0.0214,-0.0533,-0.0432,-0.0235,0.023,0.0354,0.004,0.0128,0.0302,0.0311,-0.0268,-0.0578,0.0164,-0.0592,-0.068,0.1173,-0.0357,0.0275,-0.0445,-0.0001,-0.0073,0.0149,-0.0195,-0.0442,0.0167,0.0306,0.0675,0.0412,-0.0526,0.0004,-0.0164,-0.0899,-0.0381,0.0984,-0.0015,-0.09,0.0112,0.0285,-0.0161,-0.0266,0.0095,-0.0034,-0.0524,0.0041,0.0587,-0.0139,-0.1043,-0.0039,0.0189,-0.0212,-0.015,-0.0741,-0.0483,0.0664,0.0524,-0.0358,0.0051,-0.0663,0.006,0.0443,-0.024,0.0033,-0.0319,0.0268,-0.006,0.0022,0.0033,-0.0499,0.0105,-0.01,0.0103,-0.0021,-0.0241,0.0122,-0.0405,0.0304,-0.0054,-0.0348,0.0543,0.0095,-0.0166,0.021,-0.016,0.0018,-0.009,-0.0033,0.0287,0.019,0.0214,0.0526,0.0379,-0.0539,-0.0263,-0.2135,-0.0336,-0.0244,-0.0054,0.065,-0.0668,0.0021,-0.0388,0.023,0.0963,0.0354,-0.0317,-0.0345,0.0236,-0.0043,0.0532,0.0113,0.0439,-0.0005,-0.0453,0.0211,0.0453,-0.0025,-0.1295,0.0457,0.0306,0.2561,0.0676,-0.017,-0.0426,0.0217,0.0043,-0.0378,-0.1052,0.0249,0.0309,0.0744,-0.0392,-0.0218,-0.0202,-0.0255,0.0302,-0.0054,-0.0722,-0.0183,0.0039,-0.0316,0.0237,-0.023,0.0712,0.0538,-0.0242,0.0378,-0.0019,0.0007,-0.0226,-0.0822,-0.0083,-0.0409,0.065,0.0012,-0.0923,0.0064,-0.0599,0.0949,-0.0043,-0.0288,-0.0528,0.0201,-0.0093,-0.0149,0.0353,-0.0269,0.0218,0.0423,0.0461,0.0651,-0.0168,-0.0122,-0.0335,0.0917,0.0038,0.0254,-0.024,0.0128,-0.0099,0.0441,-0.0279,0.0487,-0.014,0.0065,-0.0089,-0.0831,-0.0294,0.043,-0.026,-0.3236,0.0627,0.0004,0.0188,-0.0277,0.0546,0.0644,-0.0126,0.0198,-0.0292,0.0133,0.0584,0.0122,-0.0104,-0.0004,0.0124,0.0996,-0.072,0.0639,-0.0114,0.0393,0.0008,0.23,-0.0397,-0.0244,0.0103,-0.0101,-0.0216,0.0432,0.0003,0.0448,0.0181,0.1124,-0.0348,0.0476,0.0862,-0.0036,0.0358,-0.0091,-0.0357,-0.0314,-0.0022,-0.0556,-0.0252,0.0849,0.028,-0.008,-0.0352,-0.0261,0.0269,-0.0192,-0.0207,-0.039,-0.0138,0.0381,0.0325,-0.0868,-0.0168,-0.0173,-0.0378,0.0087,-0.0136,0.0101,0.0279,0.0016]}
{"key":"[Computing the Information Content of Trained Neural Networks] How much information does a learning algorithm extract from the training data and store in a neural network's weights? Too much, and the network would overfit to the training data. Too little, and the network would not fit to anything at all. Na\\\"ively, the amount of information the network stores should scale in proportion to the number of trainable weights. This raises the question: how can neural networks with vastly more weights than training data still generalise? A simple resolution to this conundrum is that the number of weights is usually a bad proxy for the actual amount of information stored. For instance, typical weight vectors may be highly compressible. Then another question occurs: is it possible to compute the actual amount of information stored? This paper derives both a consistent estimator and a closed-form upper bound on the information content of infinitely wide neural networks. The derivation is based on an identification between neural information content and the negative log probability of a Gaussian orthant. This identification yields bounds that analytically control the generalisation behaviour of the entire solution space of infinitely wide networks. The bounds have a simple dependence on both the network architecture and the training data. Corroborating the findings of Valle-P\\'erez et al. (2019), who conducted a similar analysis using approximate Gaussian integration techniques, the bounds are found to be both non-vacuous and correlated with the empirical generalisation behaviour at finite width.","layer":0,"vector":[-0.0217,0.0002,0.0522,-0.0288,0.0121,0.0247,0.0438,0.0148,0.0585,0.0004,-0.0106,-0.0304,0.06,0.0467,0.0169,0.0388,0.0051,-0.0182,-0.059,0.0161,0.0484,-0.0439,-0.0188,-0.03,0.0046,-0.0157,-0.0582,-0.0175,-0.0045,-0.2569,0.0113,-0.0537,0.1001,-0.03,0.0379,-0.0026,-0.028,0.0335,-0.0225,0.0303,0.0125,0.0467,-0.0241,-0.0415,0.0125,-0.0307,-0.0097,-0.0097,-0.0194,-0.0568,0.0266,0.0019,0.0243,0.0311,-0.0006,0.0499,0.012,0.0292,0.0001,0.0585,-0.0179,0.0492,-0.1632,0.038,0.0162,0.0141,-0.0772,-0.0378,0.0208,0.0696,-0.0085,0.0472,-0.0052,0.0189,0.0233,0.0066,-0.0042,-0.0164,0.0038,0.0119,0.0311,-0.0621,-0.0164,-0.0209,-0.021,-0.0339,0.0323,-0.0175,0.0053,-0.0095,-0.0139,-0.0136,-0.0495,0.0141,-0.0426,-0.0107,-0.0039,0.0433,-0.0528,0.18,-0.0387,-0.0245,0.0557,-0.0229,0.0495,0.0119,-0.0434,-0.0156,-0.0459,-0.0248,-0.0002,-0.0241,-0.0114,-0.0532,0.0411,0.0498,0.0153,0.0057,-0.035,-0.0143,-0.0483,0.0111,0.0591,0.0421,0.0247,-0.0195,-0.0478,0.109,0.0201,0.043,0.0461,-0.031,-0.0722,-0.0082,0.0469,0.0275,0.0643,0.0197,-0.0052,0.0145,-0.0233,-0.086,0.0382,-0.0626,-0.0731,0.1536,-0.0399,0.0396,-0.0207,-0.0545,-0.0323,0.0318,0.0062,-0.076,0.0179,0.0532,0.0018,-0.0077,-0.0901,0.0121,-0.0383,-0.0251,-0.0107,0.1068,0.0315,-0.0461,-0.0408,0.0255,0.0293,-0.0541,0.0527,-0.0005,0.0084,0.0223,0.0546,-0.0225,-0.1086,0.0025,0.0076,0.0288,-0.003,-0.0423,-0.0187,0.0392,0.0132,-0.0147,0.0084,-0.0423,0.0359,0.0266,-0.0442,0.0186,-0.021,-0.0148,-0.0525,-0.0549,-0.0012,-0.0081,0.0146,-0.0237,-0.0067,-0.0184,-0.0708,0.0143,0.02,0.0175,-0.0315,0.0081,0.0358,0.0191,-0.021,-0.0228,0.0369,-0.059,-0.0467,0.0244,0.0316,0.0268,-0.0308,0.0021,0.0248,-0.0786,-0.0764,-0.2578,-0.0052,0.013,-0.0116,0.0912,-0.0774,0.0971,0.0067,0.0078,0.0487,0.0367,0.0103,-0.0593,0.0297,-0.0371,0.012,0.0463,0.0294,-0.0385,0.0423,-0.0396,0.047,-0.0542,-0.0612,0.0589,-0.0145,0.2188,-0.0158,0.0677,-0.0471,-0.0014,0.0539,-0.0719,-0.0618,0.0922,0.021,0.0191,0.0336,-0.0022,-0.019,-0.0209,0.0228,0.019,-0.0857,-0.0466,0.0001,-0.036,-0.0188,-0.114,-0.0103,0.0589,0.0062,0.0551,0.023,0.0461,-0.0373,-0.0715,0.0222,-0.034,0.0579,-0.0142,-0.0614,0.0169,-0.0475,0.0429,0.0155,0.0131,-0.0068,0.0315,-0.0193,-0.0139,0.1023,0.0149,-0.0077,0.0583,-0.0342,0.0378,-0.002,-0.0247,0.0108,0.0288,-0.0482,0.0473,0.0466,0.0288,0.0103,0.0645,-0.0644,0.0333,-0.0053,0.0169,0.0446,-0.0404,0.0003,0.0278,-0.0141,-0.2981,0.0614,0.0104,0.0427,-0.0014,0.024,0.0237,0.0203,-0.0421,-0.0289,0.0239,0.0359,0.0555,-0.0285,-0.0005,0.058,0.0513,-0.0321,0.0667,-0.0217,-0.003,0.056,0.2076,-0.062,0.0126,0.0331,-0.0674,-0.005,-0.01,-0.0302,0.038,0.0127,0.0868,-0.0752,0.0562,0.1096,-0.0288,0.0583,0.0316,-0.0255,0.0444,-0.0232,-0.0523,-0.0033,0.1021,0.0038,-0.0183,-0.0307,-0.0306,0.011,-0.034,0.0287,0.0277,-0.029,0.0356,0.023,-0.0543,-0.0584,-0.0222,-0.052,0.0205,-0.0506,-0.0067,0.0132,-0.0181]}
{"key":"[Optimal Policy Trees] We propose an approach for learning optimal tree-based prescription policies directly from data, combining methods for counterfactual estimation from the causal inference literature with recent advances in training globally-optimal decision trees. The resulting method, Optimal Policy Trees, yields interpretable prescription policies, is highly scalable, and handles both discrete and continuous treatments. We conduct extensive experiments on both synthetic and real-world datasets and demonstrate that these trees offer best-in-class performance across a wide variety of problems.","layer":1,"vector":[0.0035,0.0128,0.0362,-0.0338,0.0508,0.034,0.0232,0.0708,0.0293,-0.0102,0.0517,-0.0403,0.02,0.0863,-0.0198,0.0507,-0.0278,0.0441,-0.0435,0.0594,0.0623,-0.0488,-0.0322,-0.071,0.0223,0.0463,-0.0517,-0.041,-0.0428,-0.2063,0.0348,-0.0262,0.0207,-0.0086,0.0086,-0.025,-0.0384,0.0587,-0.0244,0.0386,0.0369,0.0216,-0.0449,-0.0487,-0.0382,-0.0763,0.0015,0.0117,-0.009,0.0039,-0.0115,-0.0165,0.0446,0.0245,0.0546,-0.0023,0.0357,0.0451,0.0293,0.0701,-0.0012,0.0051,-0.1628,0.0539,0.0398,0.0733,-0.0017,-0.0007,0.0443,0.0938,-0.0338,0.0295,0.0023,0.0155,0.0123,-0.0234,-0.0044,-0.0289,-0.0027,-0.0049,0.0132,-0.0329,-0.0058,0.0027,0.0108,-0.0865,0.025,-0.0539,0.083,0.0229,-0.0526,-0.0085,-0.0031,-0.0127,-0.0812,0.0133,0.0308,0.0117,-0.0832,0.1843,-0.0185,0.0224,0.0075,-0.0253,0.0282,-0.0438,-0.0551,-0.0552,0.008,0.0132,0.0335,-0.045,0.0514,-0.012,-0.003,0.0216,0.0661,0.0256,-0.003,0.0126,-0.0112,0.0189,0.0422,-0.0215,0.0059,-0.0871,-0.0039,0.1526,0.0018,0.0032,0.0501,-0.052,-0.0293,-0.008,0.0102,-0.0151,0.0462,0.0306,0.0383,0.0132,-0.0061,-0.0156,0.0024,-0.0998,-0.0875,0.1511,-0.0091,0.0354,-0.0718,-0.0217,-0.0073,0.0024,-0.0422,-0.011,-0.0113,0.0504,0.0403,0.0215,-0.0242,0.0197,-0.0244,-0.0417,-0.0141,0.0913,-0.0336,-0.0624,-0.0114,0.0029,-0.0052,0.0069,0.0654,0.0742,-0.0305,0.0121,0.0486,-0.0007,-0.0574,-0.0149,-0.006,-0.002,0.0498,-0.0646,-0.0511,0.0619,0.0158,-0.0005,0.0326,-0.0026,0.0089,0.0162,-0.0088,0.0013,-0.0468,-0.0007,-0.0346,-0.0277,-0.0208,-0.0029,0.0247,-0.0147,0.0047,0.017,-0.0539,-0.0052,-0.0271,0.0113,-0.0514,0.0092,0.079,-0.0254,-0.007,0.0721,0.1001,-0.0055,-0.0475,0.0241,0.0122,0.0059,-0.0116,0.025,0.0395,-0.0407,-0.021,-0.2462,-0.0209,-0.0137,0.0039,0.0177,-0.0594,0.0095,-0.0308,-0.0018,0.1162,0.0207,-0.0166,-0.0341,0.0333,-0.0179,0.0307,0.0332,0.0304,-0.0556,-0.0054,-0.0086,0.0116,-0.0046,-0.1082,0.0472,-0.0009,0.2414,0.0368,0.0313,0.011,0.0479,0.0171,-0.0071,-0.1277,0.0422,0.0285,0.0559,-0.0523,-0.0346,-0.0341,-0.0079,0.0418,-0.0547,-0.1169,-0.0616,-0.0155,-0.046,0.0338,-0.0572,-0.0118,0.021,0.0045,0.046,0.01,0.0038,-0.0483,-0.1058,0.0173,-0.0677,-0.0001,0.027,-0.0098,0.0051,-0.0584,0.0234,-0.031,-0.0152,-0.0434,0.0136,0.0319,-0.0196,0.0654,-0.0434,-0.062,0.0505,0.0617,0.0162,-0.0107,-0.0893,-0.0275,0.0971,-0.0336,0.0306,0.0081,-0.0132,-0.0152,0.0787,0.0019,0.0639,-0.0235,0.0049,0.0259,-0.0541,0.0014,0.0327,0.0112,-0.2848,0.0511,-0.0023,0.0149,-0.022,-0.0124,0.0264,0.0219,-0.0228,-0.0388,0.0103,0.038,0.0223,0.0175,-0.0223,0.01,0.128,-0.0283,0.048,-0.0432,0.0575,0.015,0.1811,-0.0273,0.0225,0.0325,-0.0461,0.0247,0.0452,-0.01,0.005,0.0257,0.0513,-0.0251,0.0775,0.0569,-0.0571,0.0561,-0.0012,-0.0335,-0.045,0.0003,-0.0092,-0.0195,0.1075,-0.0363,-0.0605,-0.071,0.0026,0.0416,-0.0284,0.0349,-0.0416,-0.0069,-0.0016,0.016,-0.0205,-0.0757,-0.0265,-0.0668,-0.0179,-0.0409,-0.0198,0.0137,0.001]}
{"key":"[Revisiting Vicinal Risk Minimization for Partially Supervised Multi-Label Classification Under Data Scarcity] Due to the high human cost of annotation, it is non-trivial to curate a large-scale medical dataset that is fully labeled for all classes of interest. Instead, it would be convenient to collect multiple small partially labeled datasets from different matching sources, where the medical images may have only been annotated for a subset of classes of interest. This paper offers an empirical understanding of an under-explored problem, namely partially supervised multi-label classification (PSMLC), where a multi-label classifier is trained with only partially labeled medical images. In contrast to the fully supervised counterpart, the partial supervision caused by medical data scarcity has non-trivial negative impacts on the model performance. A potential remedy could be augmenting the partial labels. Though vicinal risk minimization (VRM) has been a promising solution to improve the generalization ability of the model, its application to PSMLC remains an open question. To bridge the methodological gap, we provide the first VRM-based solution to PSMLC. The empirical results also provide insights into future research directions on partially supervised learning under data scarcity.","layer":1,"vector":[0.0088,-0.0307,-0.0109,-0.0237,0.0344,0.0164,0.0144,0.0513,0.0017,-0.001,0.0166,-0.07,0.0319,0.0596,0.0384,0.0346,0.0283,0.0474,-0.0484,0.0114,-0.0245,0.0175,-0.0226,-0.0495,0.0347,0.0278,-0.025,-0.0454,-0.034,-0.254,0.0322,-0.0176,0.0681,-0.0601,0.0114,-0.0596,-0.0321,0.0579,-0.0409,0.0568,-0.0055,-0.0192,-0.0337,-0.0816,-0.0127,-0.0494,-0.0285,-0.0526,-0.0195,-0.0087,0.0048,-0.0153,-0.0024,0.0386,-0.005,0.0314,0.0314,-0.0024,0.0178,0.0643,0.0402,0.0372,-0.1457,0.0246,0.0432,-0.0056,-0.0479,0.0043,0.0176,0.0912,-0.0122,0.0618,0.0474,0.0551,-0.0205,-0.0356,-0.0058,0.0005,-0.0315,0.0402,0.0147,-0.0022,-0.0635,-0.0526,-0.0465,-0.0739,0.0135,-0.0749,0.0171,0.0101,-0.0175,-0.0245,-0.0341,0.023,-0.0662,-0.0563,0.0261,0.01,-0.0602,0.1882,-0.0856,0.0238,0.0344,-0.0549,0.0586,-0.0416,-0.0203,-0.0343,-0.041,-0.0129,-0.0029,0.0147,0.0256,-0.0561,0.0317,0.0305,0.0833,0.0366,0.0055,0.0075,-0.004,-0.0461,0.0786,-0.0103,0.0177,-0.0147,0.0622,0.1384,0.0493,-0.0154,0.0391,-0.0399,-0.0185,-0.0138,0.0367,-0.0015,0.0585,0.0225,0.0293,0.0067,-0.0169,-0.0222,-0.0098,-0.065,-0.0801,0.1186,-0.0222,0.0272,-0.0588,-0.0532,-0.003,0.0133,-0.0664,0.0072,0.0427,-0.0055,0.0624,0.0299,-0.0257,0.0114,-0.0011,-0.0954,-0.0137,0.1303,0.0147,-0.0485,-0.0729,-0.0353,-0.0061,-0.0487,0.0505,0.0664,0.0019,-0.0068,0.0588,0.0646,-0.0882,-0.0077,-0.0257,0.0162,0.003,-0.0226,-0.0344,0.0585,0.0081,-0.0072,0.004,-0.0258,0.0245,0.0358,-0.0093,-0.0013,0.0125,-0.044,-0.0283,-0.0353,-0.0347,-0.0,0.0239,-0.045,-0.0006,0.0006,0.0266,0.0624,0.0234,0.0149,-0.0174,0.0146,0.0263,0.0323,0.0014,0.0458,0.0575,0.0038,-0.059,0.0074,0.0271,0.0696,0.0125,0.0353,0.0569,0.0081,-0.0341,-0.2373,0.0136,-0.0138,-0.0025,0.0062,-0.0714,0.0104,0.0067,0.0289,0.0746,0.0555,-0.0076,-0.0616,0.0047,-0.0139,0.0239,0.0375,0.0133,-0.0044,-0.0067,0.0169,0.0347,-0.0119,-0.0441,0.0768,-0.0063,0.2353,0.0389,0.0028,-0.0314,0.0248,0.0101,-0.0796,-0.0917,0.0637,0.0139,0.0079,-0.0464,-0.0598,-0.0052,-0.0224,0.0204,0.0472,-0.107,-0.0394,-0.0821,-0.0479,0.0045,-0.1028,0.0266,0.0506,-0.0185,0.0391,-0.0123,-0.0134,-0.008,-0.1072,0.0215,-0.051,0.015,0.0119,-0.047,0.0312,-0.0384,0.0236,0.0072,-0.0554,-0.0328,0.0189,-0.0514,-0.0481,0.0903,-0.0081,-0.0538,0.0437,0.0321,-0.0004,-0.0038,-0.0563,0.0143,0.0399,-0.0055,0.0029,0.031,0.0244,0.0303,0.0867,0.0027,0.0257,-0.0301,0.0258,0.0203,-0.0622,0.0528,0.0456,-0.0133,-0.2658,0.0228,0.0291,0.0478,-0.0325,0.0185,0.0527,0.0161,-0.011,0.0168,0.0247,0.0207,0.0631,-0.0226,-0.0042,0.0509,0.0734,-0.0773,0.0383,-0.0202,0.0411,0.0104,0.1939,-0.0494,-0.0188,0.0347,-0.08,0.0069,0.06,0.0227,-0.0017,0.0224,0.074,-0.0741,0.0695,0.0714,-0.041,0.0068,0.0232,-0.0025,0.0336,-0.0068,-0.0926,-0.0487,0.0913,0.0431,0.0062,-0.0264,-0.0367,0.0229,0.0133,-0.0148,-0.0135,0.0197,0.0498,0.0209,-0.0133,-0.0781,-0.0293,-0.0402,0.0341,-0.0323,-0.0612,0.0054,-0.0228]}
{"key":"[Integrating Transductive And Inductive Embeddings Improves Link Prediction Accuracy] In recent years, inductive graph embedding models, \\emph{viz.}, graph neural networks (GNNs) have become increasingly accurate at link prediction (LP) in online social networks. The performance of such networks depends strongly on the input node features, which vary across networks and applications. Selecting appropriate node features remains application-dependent and generally an open question. Moreover, owing to privacy and ethical issues, use of personalized node features is often restricted. In fact, many publicly available data from online social network do not contain any node features (e.g., demography). In this work, we provide a comprehensive experimental analysis which shows that harnessing a transductive technique (e.g., Node2Vec) for obtaining initial node representations, after which an inductive node embedding technique takes over, leads to substantial improvements in link prediction accuracy. We demonstrate that, for a wide variety of GNN variants, node representation vectors obtained from Node2Vec serve as high quality input features to GNNs, thereby improving LP performance.","layer":3,"vector":[-0.0036,-0.0436,-0.0083,0.0107,0.0332,0.0252,0.0224,0.0462,0.0311,-0.0185,0.0105,-0.0438,0.0458,0.0307,0.017,0.0371,0.0026,0.0641,-0.0263,0.0284,0.0093,-0.0192,0.0157,-0.0816,0.0657,0.0284,-0.013,-0.0144,-0.0427,-0.2515,0.03,-0.1018,0.0419,0.0062,-0.0197,-0.0585,0.0382,0.0006,-0.0144,0.0445,0.0028,0.0079,-0.0253,-0.0261,-0.0123,0.0202,-0.004,-0.0141,-0.032,-0.0164,-0.0002,-0.0189,-0.0056,0.0586,0.0524,0.0682,0.0659,0.025,-0.0055,0.0748,0.0338,0.053,-0.1325,0.0619,0.0288,0.0545,-0.0556,0.0059,-0.0131,0.0524,0.0226,0.0329,0.0112,0.0157,0.019,0.0361,0.0243,0.0292,-0.0213,-0.0067,0.0288,-0.0282,-0.0033,-0.0389,-0.0032,-0.0301,0.0246,-0.0448,-0.0198,0.0487,-0.0278,-0.0354,-0.0232,0.0125,-0.0415,-0.0134,0.0544,0.0159,-0.0595,0.1871,-0.075,0.0609,0.0194,-0.0538,0.0222,-0.0486,-0.0099,-0.0324,-0.0344,-0.0064,-0.0522,-0.0767,0.0026,-0.0414,0.0311,0.0347,0.0949,0.061,-0.0226,-0.0165,-0.0468,0.0329,0.0423,-0.0217,0.0412,-0.0433,0.0139,0.1283,0.025,0.0377,0.0017,0.0282,-0.0243,-0.0242,-0.018,0.0399,0.0112,-0.0173,-0.0187,-0.0264,-0.0106,-0.0424,0.0034,-0.0399,-0.0717,0.1357,-0.0272,-0.0211,-0.0123,0.0034,0.0033,0.0322,-0.0511,-0.0139,0.0241,0.0332,0.012,0.0334,-0.064,0.0438,-0.0197,-0.0315,-0.0783,0.0549,0.0496,-0.1427,-0.0134,0.0353,0.0234,-0.067,0.0434,0.004,-0.039,0.0282,0.1077,0.0669,-0.0669,0.0004,0.0224,0.0125,0.0317,-0.0384,-0.0354,0.0486,0.0311,-0.0149,-0.0156,-0.0442,0.0204,0.0497,-0.0422,0.0131,-0.0609,-0.0018,-0.0437,0.0079,-0.0378,-0.0234,0.0196,-0.0605,0.0139,-0.0274,-0.0401,0.0086,-0.0392,-0.0047,0.0233,-0.004,-0.0017,-0.021,-0.0715,-0.0122,0.0663,-0.0144,-0.0249,-0.0307,0.011,0.0128,0.0421,0.0663,0.0173,-0.0407,-0.0277,-0.2132,-0.0269,0.0023,-0.0097,0.0697,-0.1038,0.0058,0.0023,0.0523,0.1012,0.0668,0.0051,-0.0268,-0.0174,-0.0017,0.0745,0.0315,0.0086,0.032,-0.0017,-0.0322,0.0084,0.0003,-0.095,0.0464,0.0049,0.2203,0.0377,0.0172,-0.048,-0.0195,0.0644,-0.0736,-0.1507,0.081,0.0098,0.0331,0.0043,-0.0266,-0.0306,-0.0525,0.0103,-0.0013,-0.1187,-0.0151,-0.0093,-0.042,-0.0028,-0.069,0.0113,0.0541,-0.0012,0.0851,-0.0001,-0.0317,-0.0147,-0.0927,0.0161,-0.0555,0.0335,-0.0223,-0.0657,-0.0303,-0.0872,0.0502,0.0115,-0.0092,-0.0078,0.0179,0.0014,-0.006,0.0751,-0.0043,-0.0133,0.0496,-0.0547,0.0042,-0.0377,-0.0286,0.0058,0.0569,-0.028,0.0408,0.0588,0.0051,-0.012,0.0911,0.0222,0.0615,-0.0125,0.001,-0.0113,-0.0061,-0.0373,0.0656,-0.0575,-0.3004,0.0347,-0.0017,0.0333,-0.0221,0.004,0.0847,0.0228,-0.0301,-0.0234,0.0516,0.0183,0.0569,0.0272,-0.0143,0.0275,0.0299,-0.0695,0.0029,-0.0406,0.0506,-0.0139,0.2279,-0.0144,0.078,0.0289,-0.0402,-0.0175,0.0228,-0.0004,-0.0065,0.0249,0.0466,-0.0632,0.0192,0.0486,-0.0157,0.0131,0.0371,-0.0089,-0.0076,-0.0097,-0.0648,-0.0307,0.0631,-0.0047,-0.0148,-0.0219,0.0348,0.0463,-0.0414,0.0083,-0.0314,-0.009,0.0367,0.0245,-0.0162,-0.0189,-0.0631,-0.0237,0.0174,-0.0449,-0.0096,-0.0083,0.0246]}
{"key":"[Data-Driven Discovery of Coarse-Grained Equations] Statistical (machine learning) tools for equation discovery require large amounts of data that are typically computer generated rather than experimentally observed. Multiscale modeling and stochastic simulations are two areas where learning on simulated data can lead to such discovery. In both, the data are generated with a reliable but impractical model, e.g., molecular dynamics simulations, while a model on the scale of interest is uncertain, requiring phenomenological constitutive relations and ad-hoc approximations. We replace the human discovery of such models, which typically involves spatial/stochastic averaging or coarse-graining, with a machine-learning strategy based on sparse regression that can be executed in two modes. The first, direct equation-learning, discovers a differential operator from the whole dictionary. The second, constrained equation-learning, discovers only those terms in the differential operator that need to be discovered, i.e., learns closure approximations. We illustrate our approach by learning a deterministic equation that governs the spatiotemporal evolution of the probability density function of a system state whose dynamics are described by a nonlinear partial differential equation with random inputs. A series of examples demonstrates the accuracy, robustness, and limitations of our approach to equation discovery.","layer":2,"vector":[-0.0816,-0.0062,0.0192,-0.0203,0.0065,0.0195,0.0046,-0.0033,0.0429,-0.0402,-0.0097,-0.0385,0.0394,0.0214,0.025,0.0098,-0.0275,0.0498,-0.0651,-0.0371,0.023,0.0245,-0.0133,-0.0406,0.0318,0.0529,-0.0054,-0.0248,-0.0651,-0.2377,0.0124,-0.043,0.0402,-0.0471,0.0445,-0.0138,-0.0235,0.0929,-0.0169,0.0472,-0.0151,-0.0072,0.0042,-0.0458,0.0022,-0.0313,0.0021,-0.0224,-0.0224,-0.0422,-0.01,-0.033,0.0093,0.0166,0.0132,0.0199,0.0562,0.0292,0.0436,0.0276,0.0185,0.0182,-0.1824,0.0804,0.0961,0.0369,-0.0055,-0.0093,0.0384,0.0691,0.0043,0.0516,0.0158,0.0417,-0.0027,-0.0022,0.0103,-0.0339,0.0072,0.0462,0.0434,-0.016,-0.0494,-0.0161,-0.0563,-0.0844,0.0526,-0.0311,0.0559,-0.0046,-0.0438,-0.0261,-0.0662,0.0047,-0.0997,0.0033,0.0153,0.0232,0.0083,0.1993,-0.0312,0.0612,-0.0035,-0.0319,-0.0046,-0.0442,-0.0365,-0.0012,-0.0019,-0.0044,-0.0043,-0.0002,0.0254,-0.0661,0.0286,-0.0522,0.0377,0.021,-0.0109,-0.0021,-0.03,0.0032,0.0458,-0.0065,0.0158,-0.052,-0.0249,0.1573,0.0132,0.0015,0.0519,0.0037,-0.0636,-0.0163,0.0082,0.0144,0.0027,-0.0081,0.0175,-0.0097,-0.0705,-0.0522,0.0006,-0.1249,-0.127,0.1302,-0.0256,0.0324,-0.0523,-0.0328,0.0017,0.025,-0.0369,0.0033,0.045,0.0447,0.0103,-0.0091,-0.0327,0.0354,-0.0473,-0.0429,-0.0005,0.1003,-0.017,-0.0303,-0.018,0.0748,0.0143,-0.0062,0.0422,0.0482,-0.0404,0.0248,0.0376,0.0488,-0.0501,-0.0031,0.0399,0.0383,0.0084,-0.0023,-0.0201,0.0418,0.0729,-0.0388,0.0007,-0.0237,0.0195,0.0155,0.013,0.0219,0.0095,0.003,-0.0426,0.0079,0.0103,-0.0411,0.0132,0.0056,0.0207,-0.024,-0.0466,0.0512,-0.0459,-0.0081,0.0098,0.0031,0.0116,0.0365,-0.0235,0.0306,0.0366,-0.042,-0.0347,0.0123,0.0056,0.0338,-0.0053,0.0271,0.0449,-0.0829,-0.0698,-0.2428,-0.0267,0.0205,-0.0209,0.0685,-0.0649,0.058,-0.0644,0.0364,0.0706,0.0438,0.0288,-0.0088,-0.0094,0.0044,0.0257,0.0146,0.036,-0.0302,0.0082,0.0078,-0.0137,-0.0248,-0.0747,0.0263,0.0101,0.2059,0.0186,0.0204,-0.0196,0.0248,-0.0009,-0.0235,-0.0483,0.0451,0.0233,0.0958,-0.0203,-0.0201,-0.073,-0.008,0.0382,-0.004,-0.0931,-0.0597,-0.0186,0.003,0.0207,-0.0506,0.0179,0.0294,-0.016,0.1046,-0.0616,-0.0259,-0.0362,-0.0508,0.0439,-0.0151,0.0147,-0.0209,-0.049,-0.0174,-0.0529,0.0286,-0.0092,-0.0025,-0.0671,0.0203,-0.0686,0.016,0.0885,-0.0162,0.0184,0.037,0.0302,0.0277,-0.0242,-0.0622,0.0001,0.0641,-0.0118,0.0695,0.0411,0.014,-0.005,0.0694,-0.0248,0.0183,-0.0198,-0.0398,-0.002,-0.0253,-0.0138,0.0354,0.0207,-0.3124,0.0496,0.0321,0.0408,0.0088,0.001,0.1096,0.0056,-0.0434,0.0047,-0.0412,0.0181,0.0518,0.0234,0.0219,0.0266,0.0747,-0.065,0.0568,-0.0973,0.0294,0.046,0.2226,-0.0545,0.0349,0.0319,-0.022,-0.0126,0.0275,-0.0217,0.0283,0.0216,0.0631,-0.0995,0.0438,0.0612,-0.0284,0.0457,0.0316,-0.0012,0.011,-0.0032,-0.0421,-0.0408,0.0616,-0.0393,0.0073,-0.0584,0.0078,0.0554,-0.0332,0.003,-0.0239,0.009,0.0462,0.0335,-0.0497,-0.0287,-0.0054,-0.0307,-0.0148,-0.0499,-0.0242,-0.0323,-0.0168]}
{"key":"[Query Complexity of k-NN based Mode Estimation] Motivated by the mode estimation problem of an unknown multivariate probability density function, we study the problem of identifying the point with the minimum k-th nearest neighbor distance for a given dataset of n points. We study the case where the pairwise distances are apriori unknown, but we have access to an oracle which we can query to get noisy information about the distance between any pair of points. For two natural oracle models, we design a sequential learning algorithm, based on the idea of confidence intervals, which adaptively decides which queries to send to the oracle and is able to correctly solve the problem with high probability. We derive instance-dependent upper bounds on the query complexity of our proposed scheme and also demonstrate significant improvement over the performance of other baselines via extensive numerical evaluations.","layer":2,"vector":[-0.057,0.0002,0.0552,0.0058,0.0144,0.0374,0.0342,0.0031,0.0246,-0.0202,-0.0127,-0.03,-0.0074,0.0654,0.0054,0.028,0.0033,0.0328,-0.0003,0.0491,0.0587,-0.0279,-0.0119,-0.0492,0.0205,0.0015,-0.0186,-0.0568,-0.0361,-0.2453,0.0089,-0.0127,0.057,-0.0278,0.005,-0.0304,-0.0165,0.0377,0.0272,0.0005,0.0232,0.029,0.0109,-0.048,-0.026,-0.0296,0.0189,-0.0393,-0.0213,-0.0649,-0.0218,-0.0303,0.0399,0.0279,0.0678,0.0416,0.0226,0.0637,0.0148,0.0628,0.0207,0.0147,-0.1557,0.0352,0.0416,-0.0019,-0.0146,-0.0397,-0.0158,0.0354,-0.0006,0.0426,0.0183,0.0435,0.0028,-0.0028,-0.0016,-0.0137,-0.0329,0.0114,-0.0266,-0.0462,-0.0386,-0.0043,-0.0417,-0.0912,0.0058,-0.0666,0.0223,0.0452,-0.04,-0.0145,-0.0435,0.0027,-0.0584,-0.0467,0.0462,0.0131,0.0103,0.1966,-0.0368,0.0731,0.0181,-0.0116,0.0186,-0.052,-0.0375,-0.0551,-0.0104,0.0042,-0.0331,-0.0397,0.0204,-0.0605,0.0082,0.0141,0.065,0.0139,-0.0081,-0.0287,-0.0528,0.0081,0.0376,0.0134,0.0519,-0.0712,0.0001,0.1078,0.0475,0.0469,0.0512,-0.0478,-0.0534,-0.028,-0.0029,0.0227,0.0203,-0.0015,-0.0008,0.0021,-0.0681,-0.098,0.082,-0.0672,-0.0444,0.1343,-0.0336,0.0167,-0.049,-0.1021,-0.0102,0.0375,-0.0345,-0.0259,0.0368,-0.0005,0.0374,0.0475,-0.0727,0.0283,-0.0261,-0.0474,0.0282,0.1124,0.0078,-0.0856,-0.0276,0.0172,0.0307,0.0245,0.0747,0.0648,-0.0286,0.0633,0.0794,0.0218,-0.0878,0.0148,0.0443,0.0088,-0.0042,-0.0518,-0.0222,0.0111,0.0271,-0.0796,-0.0095,-0.0106,0.0391,0.0172,-0.0003,-0.003,0.0069,0.0223,-0.0477,-0.054,-0.0067,0.0155,0.0387,-0.0688,0.0062,-0.0315,-0.0359,0.0435,-0.0397,0.0021,-0.0105,0.0395,0.0286,0.019,-0.0247,-0.0082,0.0371,-0.0398,-0.0088,-0.0112,0.0344,0.03,-0.0274,0.0576,0.0422,-0.0269,-0.0352,-0.2385,0.004,0.0302,0.0339,0.0587,-0.0563,0.0267,0.0057,0.0568,0.0542,0.0261,0.009,-0.0347,0.07,-0.0126,0.0524,0.0701,0.0076,-0.0368,-0.0204,0.017,0.0399,-0.0554,-0.1012,0.0394,-0.0,0.2202,-0.0025,0.0233,-0.0261,0.0242,0.0324,-0.0493,-0.0714,0.0561,0.0116,0.0782,0.0084,-0.0357,-0.0227,-0.0223,0.011,0.0152,-0.0291,-0.0382,-0.0409,-0.0222,0.0418,-0.0433,0.0057,0.0619,0.0117,0.088,-0.0438,0.0074,-0.0517,-0.094,-0.0417,-0.0301,0.0357,0.0015,-0.0416,0.0127,-0.0984,0.0594,-0.0156,-0.007,-0.0275,0.0623,-0.0159,-0.0561,0.0489,-0.0109,0.0391,0.0608,-0.0091,0.0466,-0.0205,-0.0487,-0.0387,0.098,-0.0572,0.0527,-0.0041,-0.0067,-0.0064,0.1324,0.0344,0.0442,-0.0105,0.0083,-0.0238,-0.034,-0.0588,0.0352,-0.0096,-0.2968,0.0227,-0.0065,0.0218,-0.0255,-0.0024,0.0648,0.0004,-0.0341,-0.0164,0.0159,0.0589,0.0146,-0.0566,0.0302,0.0387,0.0263,-0.0481,0.0247,-0.053,0.0506,0.0413,0.1847,-0.0371,0.0241,0.0171,-0.0309,0.005,0.0144,0.0095,0.0397,-0.0259,0.0753,-0.082,0.0238,0.0773,-0.0061,0.0264,0.0264,-0.0236,0.0075,-0.0004,-0.0377,0.0067,0.1338,0.0014,-0.0337,-0.0501,-0.0114,0.0016,-0.0408,0.0138,0.0287,0.0289,0.0151,0.0241,-0.0532,-0.039,-0.0203,-0.0703,0.0316,-0.083,-0.001,-0.029,0.0032]}
{"key":"[Accelerating likelihood optimization for ICA on real signals] We study optimization methods for solving the maximum likelihood formulation of independent component analysis (ICA). We consider both the the problem constrained to white signals and the unconstrained problem. The Hessian of the objective function is costly to compute, which renders Newton's method impractical for large data sets. Many algorithms proposed in the literature can be rewritten as quasi-Newton methods, for which the Hessian approximation is cheap to compute. These algorithms are very fast on simulated data where the linear mixture assumption really holds. However, on real signals, we observe that their rate of convergence can be severely impaired. In this paper, we investigate the origins of this behavior, and show that the recently proposed Preconditioned ICA for Real Data (Picard) algorithm overcomes this issue on both constrained and unconstrained problems.","layer":2,"vector":[-0.0919,-0.0152,0.0264,-0.0288,0.0499,0.053,0.0364,0.0166,0.0588,-0.0115,0.0454,-0.081,0.0012,0.0393,0.0568,0.0406,0.0553,0.0349,0.0072,0.047,0.0328,-0.0511,-0.0355,-0.0464,0.0651,0.0377,-0.0446,-0.0408,-0.0496,-0.2821,0.0012,-0.0388,0.0868,-0.0207,0.0252,-0.0175,-0.0238,0.0515,-0.0561,0.0593,-0.0017,0.0368,-0.0373,0.0031,-0.0319,-0.0689,-0.0568,-0.0128,-0.0015,-0.0389,0.0352,-0.0462,0.0476,0.0452,0.0369,0.0021,0.0705,-0.0047,0.0528,0.0359,-0.0061,0.0536,-0.1816,0.08,0.0679,0.0347,-0.0051,-0.0463,0.0385,0.0646,-0.0344,0.0622,0.0067,-0.0125,0.0215,-0.0168,0.0267,-0.0231,-0.0231,0.0178,0.0323,-0.0056,-0.0305,0.0249,-0.0297,-0.0346,0.022,-0.0457,0.0338,0.0113,-0.0746,0.0045,-0.006,0.0096,-0.0589,-0.0195,-0.0031,0.0272,-0.0356,0.2134,-0.0289,0.0283,0.0196,-0.0214,-0.0165,-0.0111,-0.0172,-0.0142,-0.0063,-0.0126,0.0397,-0.0277,0.0078,-0.0426,0.0087,0.0038,0.0662,-0.0092,0.0227,-0.0164,-0.0481,0.0016,0.0511,0.0208,0.0631,-0.0969,0.0156,0.1086,0.0162,0.0447,0.0661,-0.0024,-0.0087,-0.0003,-0.0009,0.0237,0.0213,0.0284,0.0182,0.0066,-0.0537,-0.0691,0.0109,-0.0544,-0.0096,0.1189,-0.0218,0.0048,-0.0709,-0.0388,-0.0035,0.0082,0.0148,-0.0055,0.0354,-0.0101,0.0125,0.0143,-0.0368,0.0271,-0.0172,-0.0354,0.0367,0.0985,-0.0379,-0.0528,0.0187,0.0041,0.0078,0.041,0.0527,0.0667,-0.0426,0.0299,0.094,-0.0013,-0.076,0.0251,0.0154,0.0533,0.0137,-0.0632,-0.0241,0.0427,0.0481,-0.0676,-0.0229,-0.0251,-0.0136,0.0091,-0.0606,-0.0054,-0.0458,0.0559,-0.0048,-0.0604,0.0043,-0.019,0.0024,-0.0301,0.0021,-0.0201,-0.0617,0.0249,-0.0086,-0.0142,-0.012,-0.0029,0.0412,-0.0058,-0.0201,-0.0076,0.0747,-0.0402,0.0064,-0.0121,0.0378,0.0267,0.0112,0.0713,0.0341,-0.0588,-0.0756,-0.2441,-0.0553,0.0227,-0.0154,0.0116,-0.0985,0.036,0.0263,0.0695,0.1183,0.0521,0.0149,-0.0736,0.039,-0.0429,0.046,0.0058,0.0286,-0.0038,-0.0033,-0.0112,0.0244,-0.0097,-0.006,0.0635,-0.0001,0.1939,-0.0172,0.0211,-0.0122,0.0102,0.0466,0.0136,-0.0593,0.0515,0.0113,0.0641,-0.0338,-0.0193,-0.0317,-0.0158,0.0014,0.0168,-0.0658,-0.0467,-0.0386,-0.022,0.025,-0.0767,0.0234,0.076,-0.0363,0.0204,-0.0079,0.0034,-0.0704,-0.086,-0.0021,-0.0657,0.0148,-0.0269,-0.0385,0.0256,-0.077,0.0505,-0.014,-0.0144,-0.0359,0.0037,-0.0115,-0.0231,0.0855,-0.0011,0.0006,0.0803,0.0055,0.0139,-0.0674,-0.0566,-0.029,0.0504,-0.0616,0.0315,0.0374,0.005,0.022,0.1256,-0.0024,-0.02,0.0136,-0.0159,0.016,-0.0329,-0.0063,0.0015,-0.0172,-0.2741,-0.0065,0.0099,-0.0135,-0.0278,0.0239,-0.0027,-0.0188,-0.0785,0.0079,-0.0515,0.041,0.019,-0.0355,0.0067,0.0585,0.0363,-0.0226,0.0396,-0.07,-0.0238,0.0585,0.2149,-0.0206,-0.0042,0.0358,-0.0297,0.0093,-0.0079,-0.0905,0.0164,-0.0174,0.0643,-0.0491,0.0509,0.0907,-0.0531,0.0732,0.0132,-0.0352,0.0144,0.0263,-0.0169,-0.0345,0.0713,-0.0147,-0.0291,-0.0358,-0.004,0.0365,-0.0223,0.0121,-0.0013,-0.0063,0.0127,0.0082,-0.0486,-0.0021,0.0101,-0.0231,0.0096,-0.0378,-0.0455,0.0001,-0.0058]}
{"key":"[A probability theoretic approach to drifting data in continuous time domains] The notion of drift refers to the phenomenon that the distribution, which is underlying the observed data, changes over time. Albeit many attempts were made to deal with drift, formal notions of drift are application-dependent and formulated in various degrees of abstraction and mathematical coherence. In this contribution, we provide a probability theoretical framework, that allows a formalization of drift in continuous time, which subsumes popular notions of drift. In particular, it sheds some light on common practice such as change-point detection or machine learning methodologies in the presence of drift. It gives rise to a new characterization of drift in terms of stochastic dependency between data and time. This particularly intuitive formalization enables us to design a new, efficient drift detection method. Further, it induces a technology, to decompose observed data into a drifting and a non-drifting part.","layer":3,"vector":[-0.0713,0.0297,0.0547,-0.0427,0.055,0.0352,0.0674,-0.0143,0.0488,-0.0115,0.059,-0.041,0.0329,0.0679,-0.0467,0.0345,-0.0443,0.0305,-0.0086,-0.0005,0.0241,-0.0197,0.0064,-0.0421,0.0084,0.0496,-0.0386,-0.0328,-0.0334,-0.2214,0.0065,-0.0837,0.008,-0.0277,0.0057,-0.01,-0.0442,0.0487,0.01,0.0583,0.0028,0.0302,-0.0457,-0.0671,-0.0537,-0.0514,0.0214,-0.0098,-0.0351,-0.0156,-0.0437,-0.024,0.058,0.0261,0.0423,0.0779,0.0622,0.0116,0.0675,0.0262,0.0196,0.0493,-0.1631,0.0288,0.0653,0.0396,-0.0128,-0.0527,0.027,0.0356,-0.0294,0.0305,-0.0316,0.0821,0.0232,-0.0328,0.0208,-0.043,-0.046,0.048,0.0419,-0.0141,-0.0336,-0.0194,-0.0592,-0.0582,0.0378,-0.0556,0.064,-0.0304,-0.0415,-0.0157,-0.0295,0.0312,-0.0496,-0.0095,0.0422,0.0362,0.0245,0.1824,-0.0471,0.02,0.0113,-0.0236,0.0249,-0.0566,-0.0173,-0.061,-0.0066,0.004,0.0335,-0.0159,0.0069,-0.0423,0.0226,-0.0057,-0.0006,0.0574,-0.0244,0.0128,-0.0243,0.0436,0.0679,-0.0166,0.0368,-0.0355,0.0428,0.1132,0.0462,0.0164,0.0553,-0.0256,-0.0827,-0.0293,-0.0029,0.0469,0.019,0.0216,0.0283,-0.0267,-0.045,-0.0593,0.0431,-0.0758,-0.0402,0.1372,0.0046,0.0513,-0.0321,-0.0395,-0.022,0.0016,-0.0183,-0.0479,0.0261,0.0366,0.0323,0.0485,-0.0611,0.0272,-0.0522,-0.053,-0.0061,0.1298,0.0071,-0.0548,-0.0211,-0.0299,0.0201,-0.0222,0.0391,0.0554,-0.029,0.0018,0.0586,0.0235,-0.042,0.0065,0.029,-0.0352,0.0304,-0.0218,-0.0438,0.0893,0.0379,-0.0663,-0.0479,-0.0217,0.0193,0.0789,0.0005,-0.0323,-0.0034,-0.0091,-0.0294,-0.0522,-0.015,0.0317,0.028,-0.0358,-0.012,-0.0191,-0.0582,0.023,-0.0162,0.0044,-0.0346,-0.002,0.0321,0.0469,0.016,-0.0149,0.0529,-0.0213,-0.0084,0.0032,-0.0092,0.0244,-0.0045,0.0114,0.0312,-0.035,-0.0095,-0.2431,-0.0383,-0.0022,0.0194,0.0436,-0.0571,0.0036,-0.0234,0.0886,0.0732,0.0746,-0.0491,-0.0128,-0.0115,-0.0026,0.0308,0.0194,0.0132,-0.0351,-0.0093,-0.0243,-0.0097,-0.0462,-0.1006,0.043,-0.001,0.2316,0.0008,0.0479,-0.0298,0.009,0.0241,0.0077,-0.0673,0.0464,0.0279,0.0309,0.0013,-0.0426,-0.0383,0.0057,0.0471,0.0109,-0.0584,-0.0448,-0.0411,-0.027,-0.0223,-0.0409,0.0001,-0.0005,-0.0396,0.0628,-0.021,0.004,-0.0637,-0.0123,0.0183,-0.0213,0.0144,-0.0032,-0.0187,0.0102,-0.0521,0.0606,-0.0488,-0.0356,-0.0751,0.0097,-0.0363,0.014,0.1103,-0.0188,-0.0364,0.0498,-0.0024,0.0021,-0.0125,-0.0639,-0.0088,0.058,-0.0643,0.0373,0.0371,0.0259,-0.0197,0.0726,-0.0009,0.0038,0.0317,0.0352,0.0253,-0.0302,-0.0158,0.041,-0.0152,-0.2927,0.0467,0.008,0.0224,-0.0002,0.0279,0.0125,0.0526,-0.0599,0.0059,-0.0272,0.0496,0.0473,-0.0025,0.0174,0.0534,0.048,-0.0655,0.0007,-0.0832,0.0512,0.0528,0.2191,-0.0314,0.0017,0.0184,-0.0093,0.0498,0.0143,-0.0199,-0.0538,0.0338,0.0938,-0.0375,0.0018,0.0725,-0.047,0.0783,0.0158,-0.0662,0.0254,0.0253,-0.0273,-0.0382,0.1353,0.006,-0.0097,-0.0576,-0.0051,0.0633,-0.0372,-0.0285,-0.0328,0.0049,-0.0041,0.0159,-0.0724,-0.0301,-0.0176,-0.0234,0.0501,-0.091,0.0142,-0.0407,-0.0097]}
{"key":"[A Novel Multi-Layer Modular Approach for Real-Time Gravitational-Wave Detection] Advanced LIGO and Advanced Virgo ground-based interferometers are poised to probe an unprecedentedly large volume of space, enhancing the discovery power of the observations to even new sources of gravitational wave emitters. In this scenario, the development of highly optimized gravitational wave detection algorithms is crucial. We propose a novel layered framework for real-time detection of gravitational waves inspired by speech processing techniques and, in the present implementation, based on a state-of-the-art machine learning approach involving a hybridization of genetic programming and neural networks. The key aspects of the newly proposed framework are: the well structured, layered approach, and the low computational complexity. The paper describes the basic concepts of the framework and the derivation of the first three layers. Even if, in the present implementation, the layers are based on models derived using a machine learning approach, the proposed layered structure has a universal nature. To train and test the models, we used simulated binary black hole gravitational wave waveforms in synthetic Gaussian noise representative of Advanced LIGO sensitivity design. Compared to more complex approaches, such as convolutional neural networks, our framework, even using the simple ground model described in the paper, has similar performance but with a much lower computational complexity and a higher degree of modularity. Furthermore, the underlying exploitation of short-term features makes the results of the new framework virtually independent against time-position of gravitational wave signals, simplifying its future exploitation in real-time multi-layer pipelines for gravitational-wave detection with second generation interferometers.","layer":1,"vector":[-0.0548,0.0101,-0.0325,-0.0238,0.0133,-0.0005,0.0166,-0.0097,0.0046,-0.07,-0.0097,-0.0629,-0.005,0.0354,0.044,-0.0178,0.0303,-0.0038,-0.0267,0.0423,0.0418,-0.006,-0.0087,-0.0513,0.0412,0.0065,-0.0611,-0.0278,-0.0731,-0.2134,0.0301,-0.0704,0.0937,-0.0252,0.0028,0.0123,0.0056,0.0619,-0.0523,0.0577,0.0715,-0.0163,-0.0187,-0.068,0.0185,-0.0583,-0.0173,-0.0276,-0.034,-0.046,0.0129,-0.0567,0.0259,0.0069,0.0019,0.0379,0.0207,-0.0134,0.0645,0.037,-0.0098,0.0482,-0.2064,0.0527,0.0089,0.0714,-0.0063,-0.0411,0.0371,0.0068,-0.0154,0.0513,0.0411,0.0294,-0.0132,-0.0044,0.0344,-0.0735,0.0307,0.0352,0.0384,-0.0487,-0.0232,-0.011,-0.0326,-0.0529,0.0262,0.0098,0.0215,0.0001,-0.049,-0.0513,-0.0285,0.0555,-0.0628,-0.0179,0.0356,0.0189,-0.0121,0.1966,-0.0621,0.005,0.0344,-0.0274,0.0457,-0.0225,-0.0276,-0.0462,-0.0367,0.0026,0.0234,-0.0327,0.0443,0.0165,-0.0112,0.0319,0.0115,0.0199,0.0101,0.0041,-0.0342,-0.0235,0.0102,-0.0188,0.0462,-0.0482,0.0371,0.1323,0.0136,0.043,0.0578,0.0351,-0.0425,-0.0145,-0.0034,0.0047,-0.0169,0.0165,-0.0105,-0.0005,-0.078,-0.0561,0.0351,-0.0694,-0.0227,0.0665,-0.0719,0.0147,-0.0607,-0.0152,-0.0488,-0.0086,-0.049,-0.0326,0.0356,0.0195,0.0219,0.0254,-0.0903,0.0069,-0.0139,-0.0298,-0.0401,0.1125,0.0442,-0.0889,-0.0648,-0.0073,0.021,-0.0407,0.0297,0.0175,-0.0213,-0.0006,0.0815,0.0523,-0.0637,-0.0149,-0.0026,0.0332,-0.0171,-0.0629,-0.0111,0.0241,0.0438,-0.0569,0.0102,-0.0379,-0.0001,0.0123,-0.0465,0.0271,-0.011,0.024,-0.0233,-0.0307,0.0106,0.0085,0.0071,-0.0408,0.0147,0.0091,-0.0178,0.0131,-0.0275,0.0103,-0.0027,0.0123,0.034,0.0432,0.0367,-0.0058,0.0833,-0.0114,-0.0185,-0.0205,-0.0009,0.0279,-0.0362,0.0677,0.0233,-0.0409,-0.1039,-0.2563,-0.0267,0.0122,0.016,0.0925,-0.0211,0.061,0.0159,0.0064,0.0382,0.0852,0.0285,-0.0279,0.0217,0.0077,0.029,0.0309,0.0059,-0.0041,-0.0396,-0.0375,0.0176,-0.0241,-0.0591,0.0266,-0.0015,0.2002,0.0231,0.0583,-0.0288,0.0349,0.0154,0.0174,-0.0741,0.0672,0.048,0.1068,0.0354,-0.0629,-0.0404,-0.0441,0.0322,0.036,-0.062,-0.036,-0.051,-0.0115,0.0361,-0.0328,-0.0072,0.0357,-0.0352,0.0347,-0.0226,0.0154,-0.0492,-0.0988,-0.0241,-0.0291,0.0525,0.0488,0.0295,-0.0044,-0.0569,0.0643,0.0001,-0.0122,-0.0184,0.0185,-0.0189,-0.0035,0.0698,-0.0124,-0.026,0.0518,-0.0171,0.0829,-0.0539,-0.0448,-0.0222,0.0856,-0.0075,0.0236,0.0693,-0.0153,0.0398,0.0697,0.0113,0.0392,-0.0206,-0.0124,0.027,-0.0023,0.0069,0.0103,-0.0051,-0.3028,0.0247,0.021,-0.0071,0.0157,0.02,0.0204,0.0104,-0.049,-0.0034,-0.1037,0.058,-0.001,-0.0398,0.0122,0.0709,0.0695,-0.0477,0.0502,-0.0378,0.0452,0.0601,0.2352,-0.0102,0.0366,0.0148,-0.0254,0.033,0.0009,-0.0248,0.0531,0.0358,0.0603,-0.0588,-0.0159,0.0551,-0.0206,0.0557,0.0152,-0.0074,0.0515,0.0089,-0.049,-0.0471,0.1117,-0.0302,-0.0207,-0.0305,-0.0283,0.0296,-0.0044,0.0141,0.0415,-0.0084,0.0121,0.0648,-0.0349,-0.0205,-0.0331,-0.0207,0.057,-0.0786,-0.0097,-0.0329,-0.0627]}
{"key":"[Predictive Process Model Monitoring using Recurrent Neural Networks] The field of predictive process monitoring focuses on modelling future characteristics of running business process instances, typically by either predicting the outcome of particular objectives (e.g. completion (time), cost), or next-in-sequence prediction (e.g. what is the next activity to execute). This paper introduces Processes-As-Movies (PAM), a technique that provides a middle ground between these predictive monitoring. It does so by capturing declarative process constraints between activities in various windows of a process execution trace, which represent a declarative process model at subsequent stages of execution. This high-dimensional representation of a process model allows the application of predictive modelling on how such constraints appear and vanish throughout a process' execution. Various recurrent neural network topologies tailored to high-dimensional input are used to model the process model evolution with windows as time steps, including encoder-decoder long short-term memory networks, and convolutional long short-term memory networks. Results show that these topologies are very effective in terms of accuracy and precision to predict a process model's future state, which allows process owners to simultaneously verify what linear temporal logic rules hold in a predicted process window (objective-based), and verify what future execution traces are allowed by all the constraints together (trace-based).","layer":5,"vector":[-0.0792,-0.0188,0.0361,-0.045,-0.0049,0.0005,0.0501,-0.0101,0.0581,-0.0391,-0.0103,-0.0375,0.0102,0.0133,0.0076,-0.0204,0.0047,0.0581,-0.0171,-0.0002,0.0288,-0.0082,-0.0345,-0.0282,0.0263,0.0258,-0.0161,-0.0322,-0.069,-0.2299,0.0057,-0.0671,0.029,-0.0143,0.0441,-0.0172,-0.0471,0.0343,-0.0185,0.0319,0.0092,0.0331,-0.0042,-0.0602,-0.0376,-0.054,0.0057,-0.0053,-0.0404,-0.047,0.0149,-0.0408,0.0054,-0.0047,0.024,0.0361,0.0618,0.0343,0.041,0.0143,0.0153,-0.0001,-0.1808,0.0456,0.0335,0.0296,-0.0593,-0.0141,0.0037,0.0537,-0.0464,0.017,0.0243,0.0593,0.0076,-0.0134,-0.0142,0.0113,-0.038,0.0215,0.042,0.0164,-0.0485,-0.0127,-0.0724,-0.0208,0.0093,0.014,0.0841,-0.013,-0.0572,-0.0251,-0.0167,0.0186,-0.0508,-0.007,0.0642,0.0392,-0.0278,0.2179,-0.0499,0.0183,0.0714,-0.022,0.0208,-0.0115,-0.0792,-0.0042,-0.0081,-0.0102,-0.0185,-0.0122,0.0571,-0.0562,0.0547,-0.0026,0.0477,0.0002,-0.0099,0.0241,0.0218,0.0276,0.0445,-0.0359,-0.0057,-0.051,0.0369,0.1297,0.0078,0.0216,0.0425,-0.0385,-0.0298,0.034,0.0202,0.0132,0.0449,-0.0209,0.0059,-0.0626,-0.0908,0.0134,0.0232,-0.093,-0.0343,0.1531,-0.0001,0.0059,-0.0352,-0.0406,-0.0741,0.0635,0.0088,-0.0378,0.0162,-0.0007,-0.0004,0.0255,-0.0657,0.0569,-0.0368,-0.0202,-0.0357,0.0856,0.0183,-0.0861,-0.0016,-0.0317,0.0263,-0.0105,0.035,0.0215,-0.0344,-0.0458,0.0529,0.0527,-0.0417,-0.0248,-0.0209,0.0291,0.0474,-0.0676,-0.0277,0.0131,0.0517,-0.0491,0.0643,-0.072,0.0154,0.0629,-0.0493,0.0274,-0.0284,0.0438,0.001,0.0257,0.016,0.0103,0.0008,-0.0499,0.0044,0.0356,-0.0445,0.0101,0.0307,0.0022,-0.0452,0.0374,0.0331,0.0481,-0.0605,0.0119,0.0751,-0.0099,-0.0345,0.0121,-0.0211,0.0387,0.0272,0.0229,0.0468,-0.0122,-0.0381,-0.2459,0.0372,0.0138,-0.0014,0.023,-0.0558,0.0165,-0.0116,0.035,0.0317,0.052,-0.0021,-0.0173,-0.0172,0.0196,0.0375,0.0302,0.0276,-0.0479,0.0485,-0.0435,-0.0253,-0.0099,-0.0753,0.0122,0.0035,0.1954,0.0019,0.0777,-0.0388,0.0533,0.031,-0.0547,-0.0888,0.0723,0.0071,0.0613,0.0313,-0.0165,-0.0521,-0.0344,0.0261,-0.0101,-0.1105,-0.0382,-0.0262,-0.0032,0.0149,-0.0509,0.0138,0.0116,-0.0437,0.036,-0.0219,-0.0348,-0.0096,-0.0392,0.0447,-0.0552,0.0087,0.0296,-0.0554,0.006,0.0065,0.0548,0.0246,0.009,-0.0277,0.0172,0.0142,-0.0033,0.1199,-0.0183,-0.0636,0.0718,0.0042,-0.0268,-0.0449,-0.0736,-0.0236,0.0692,-0.0544,0.0461,0.0192,0.0494,-0.0017,0.0494,-0.0277,0.0252,-0.0296,0.004,-0.0328,-0.0491,0.0223,0.0558,-0.0119,-0.3033,0.03,-0.0096,0.0615,-0.0142,-0.0082,0.0469,0.0416,-0.0259,0.0125,0.0061,0.0083,0.0416,-0.022,-0.0062,0.0147,0.1034,-0.058,0.0505,-0.0487,0.0278,0.0505,0.2084,-0.0391,0.0617,-0.0211,-0.0145,-0.0144,0.0697,-0.0172,0.0601,0.0061,0.0955,-0.0434,0.0308,0.0657,-0.0457,0.0826,0.0629,-0.0009,0.0143,-0.0173,-0.024,-0.0702,0.0706,-0.0131,-0.0502,-0.0657,-0.0249,0.0422,-0.0628,0.008,-0.0117,0.0409,0.0232,0.0154,-0.0362,-0.0484,-0.0583,-0.0185,0.0077,-0.0759,0.0448,0.0044,-0.0129]}
{"key":"[Classification using margin pursuit] In this work, we study a new approach to optimizing the margin distribution realized by binary classifiers. The classical approach to this problem is simply maximization of the expected margin, while more recent proposals consider simultaneous variance control and proxy objectives based on robust location estimates, in the vein of keeping the margin distribution sharply concentrated in a desirable region. While conceptually appealing, these new approaches are often computationally unwieldy, and theoretical guarantees are limited. Given this context, we propose an algorithm which searches the hypothesis space in such a way that a pre-set \"margin level\" ends up being a distribution-robust estimator of the margin location. This procedure is easily implemented using gradient descent, and admits finite-sample bounds on the excess risk under unbounded inputs. Empirical tests on real-world benchmark data reinforce the basic principles highlighted by the theory, and are suggestive of a promising new technique for classification.","layer":3,"vector":[-0.0071,-0.0082,0.0145,0.002,0.0548,-0.0101,0.0343,0.0335,0.0557,-0.0161,0.008,-0.0284,-0.0044,0.0293,0.0077,0.0256,0.0254,0.0501,-0.0113,-0.0003,0.0331,-0.0188,0.0106,-0.0551,0.0316,-0.0203,-0.057,-0.0458,-0.0635,-0.2402,0.0014,-0.0549,0.0601,-0.0612,0.0302,-0.0197,-0.0528,0.0472,-0.0326,0.038,0.0199,-0.0175,-0.0376,-0.0278,-0.0135,-0.0525,0.0255,-0.0154,-0.0554,-0.0498,0.0208,-0.0329,0.014,0.0538,0.0267,0.0387,0.0026,0.0469,0.0362,0.082,0.0146,0.0147,-0.1562,0.0154,0.0399,0.0274,-0.0472,0.0073,0.0144,0.0714,0.0106,0.0302,0.0156,0.0858,-0.0263,-0.0169,0.0503,-0.0174,-0.0504,-0.0183,0.0289,-0.0277,-0.0558,0.0137,-0.0209,-0.022,0.0658,-0.0527,0.0849,0.0115,-0.0423,-0.0404,-0.0336,0.0205,-0.0941,-0.0277,0.021,-0.001,-0.013,0.1809,-0.0477,0.0313,0.0148,-0.0363,0.0386,-0.0238,-0.0135,-0.0253,-0.0307,-0.0368,0.0094,0.0341,0.0102,-0.0033,-0.0261,0.0001,0.0423,0.0189,-0.0253,0.0049,-0.031,0.0182,0.0859,-0.0185,0.0255,-0.0247,0.0221,0.1538,0.0242,0.0117,0.0297,-0.0514,-0.0518,-0.01,0.0368,0.0441,-0.0281,0.0461,0.0476,-0.0447,-0.0259,-0.0335,0.0305,-0.0725,-0.0596,0.1403,-0.0897,0.0258,-0.0363,-0.082,-0.0245,-0.0111,-0.0437,-0.0355,0.0253,0.0184,0.0354,0.0695,-0.0649,-0.0005,-0.0239,-0.0622,-0.0137,0.1421,0.0092,-0.0749,-0.0398,0.0076,-0.0146,0.0199,0.0377,0.065,-0.0588,0.0605,0.0572,0.0234,-0.0867,0.0224,0.0264,0.0077,-0.0339,-0.0489,-0.0466,0.0019,0.0295,-0.0403,-0.025,-0.0268,0.0436,0.0441,-0.0341,-0.0177,0.0026,0.0054,-0.011,-0.0248,-0.0013,0.0107,0.0226,-0.0186,0.0218,-0.0073,-0.033,0.0551,0.0128,0.0219,-0.0005,-0.0318,0.032,0.0176,-0.0203,-0.0094,0.0869,-0.0157,-0.0411,-0.0046,0.0401,0.0453,0.0077,0.0573,0.0742,-0.0167,-0.0403,-0.2443,0.0052,-0.0127,0.0167,0.0232,-0.0858,0.0165,0.0056,0.0476,0.0289,0.0552,-0.0342,-0.0226,0.0557,-0.0229,0.035,0.0224,-0.0021,-0.0468,0.0184,-0.0108,0.0299,-0.0531,-0.0196,0.0835,0.0025,0.1763,0.0131,0.0557,-0.0321,0.0284,0.0067,0.0216,-0.0574,0.0438,0.0039,0.009,-0.0641,-0.0856,-0.021,-0.0176,0.0296,-0.0046,-0.0741,-0.0525,-0.018,-0.0584,0.0558,-0.041,0.0297,0.0386,-0.0276,0.0616,-0.0105,0.0046,-0.0425,-0.0662,0.0436,-0.0427,0.0268,0.0038,-0.0968,0.0211,-0.0493,0.0659,0.0187,-0.0086,-0.0295,0.0122,-0.0448,-0.0289,0.0481,-0.0115,-0.0365,0.0548,0.0003,0.0279,-0.0178,-0.0459,-0.0268,0.0678,-0.0015,0.0476,0.0065,0.022,0.009,0.1442,-0.0027,0.0298,0.021,0.0016,0.0196,-0.0673,-0.0132,0.0137,0.0093,-0.286,0.0156,0.003,-0.0043,0.0006,-0.0297,0.0549,-0.0033,-0.057,0.0278,-0.0095,0.043,0.0303,-0.0146,0.0185,0.0111,0.0319,-0.0687,0.0586,-0.0499,0.061,0.0812,0.2391,-0.0194,0.0144,0.0491,-0.0125,-0.0149,0.0096,-0.0354,0.0127,-0.0099,0.0952,-0.0762,0.0134,0.0978,-0.0232,0.0338,0.025,-0.0381,0.0118,0.0079,-0.0247,-0.0101,0.1242,-0.0384,0.0003,-0.0192,-0.0052,0.0448,-0.0142,0.0348,-0.031,0.012,0.0468,0.0217,-0.0593,-0.0538,-0.0676,-0.061,0.0129,-0.0868,-0.0535,-0.0069,-0.0166]}
{"key":"[Task Selection Policies for Multitask Learning] One of the questions that arises when designing models that learn to solve multiple tasks simultaneously is how much of the available training budget should be devoted to each individual task. We refer to any formalized approach to addressing this problem (learned or otherwise) as a task selection policy. In this work we provide an empirical evaluation of the performance of some common task selection policies in a synthetic bandit-style setting, as well as on the GLUE benchmark for natural language understanding. We connect task selection policy learning to existing work on automated curriculum learning and off-policy evaluation, and suggest a method based on counterfactual estimation that leads to improved model performance in our experimental settings.","layer":6,"vector":[-0.0454,0.0039,0.0035,-0.0289,-0.0087,0.0074,0.0265,0.0372,-0.0102,-0.0238,-0.0045,-0.0572,0.0157,0.0826,0.0228,0.0359,-0.0334,0.0169,-0.0302,0.0199,0.0328,-0.0771,-0.0218,-0.0822,0.0181,0.0255,-0.0511,-0.0726,-0.0205,-0.2123,0.0341,-0.0175,0.0341,0.0077,0.0388,0.0038,-0.0359,0.0163,-0.0268,0.0283,0.0056,0.0243,-0.0555,-0.0691,-0.0395,-0.0458,-0.0387,-0.022,-0.0374,-0.0051,0.0138,-0.0371,0.0695,0.0171,0.0221,0.0194,0.0357,0.0551,0.0333,0.0473,-0.0197,0.0273,-0.1927,0.0744,0.0325,0.0327,-0.0291,-0.0216,0.0465,0.0711,-0.0345,0.0305,0.0425,0.0626,0.0384,0.0133,-0.0073,-0.0303,0.0238,0.014,-0.01,-0.0514,-0.009,-0.0067,-0.0177,-0.0442,0.0096,-0.0214,0.0561,0.0034,0.0021,-0.0227,-0.0243,0.0174,-0.0413,-0.0018,0.0023,0.0133,-0.0569,0.1939,-0.0303,0.0064,0.0227,-0.0529,0.0115,-0.0615,-0.0251,-0.0361,0.0046,-0.0175,-0.002,0.0008,0.0122,-0.04,0.0061,0.0581,0.1004,-0.01,0.0047,-0.0028,0.0224,0.0111,0.0589,-0.0544,-0.0123,-0.073,0.0302,0.1546,0.0483,-0.0002,0.0436,-0.0506,-0.0507,-0.04,0.0267,0.0107,-0.0005,0.0525,0.0374,-0.0115,-0.0126,-0.0293,-0.0018,-0.0885,-0.052,0.1554,0.0091,0.0439,-0.0293,-0.0222,-0.0446,0.0088,0.0207,-0.0417,0.0023,0.0471,0.005,0.0303,-0.0662,0.0099,-0.0113,-0.0603,-0.0066,0.0658,-0.0233,-0.0457,-0.0136,-0.0179,-0.0336,-0.0126,0.0458,0.0248,-0.0628,0.0109,0.0528,0.0181,-0.0909,0.0052,0.0352,0.0036,0.041,-0.0758,-0.0351,0.0417,0.0112,-0.0336,-0.01,-0.0453,0.0309,0.0453,-0.0072,0.0371,0.0411,-0.0049,-0.04,0.0013,0.0203,-0.0162,0.0366,-0.0172,-0.0247,0.0293,-0.0673,-0.0301,-0.0002,-0.0017,-0.003,0.0067,0.1121,0.0286,-0.0195,0.0163,0.031,-0.0233,-0.0446,0.0123,0.0249,0.0406,0.0139,0.0031,0.006,-0.0156,-0.0103,-0.2485,0.0008,0.0259,0.0266,0.074,-0.0476,0.0631,-0.0377,0.0264,0.0662,0.0249,-0.0713,-0.0427,0.0218,-0.0273,0.0539,0.0256,0.0139,-0.0162,0.0061,0.0313,-0.0008,-0.0208,-0.1068,0.0433,-0.0053,0.2127,0.0329,0.027,-0.0192,0.0699,0.0525,0.0115,-0.1067,0.0461,0.0337,0.0834,-0.0505,-0.0152,-0.0163,-0.018,0.0693,-0.0275,-0.1227,-0.0675,-0.0499,-0.0264,0.0075,-0.0419,0.014,0.0301,-0.0141,0.012,-0.0084,-0.0693,-0.0157,-0.1354,0.0138,-0.0605,0.0213,0.0352,-0.023,0.005,-0.0528,0.0741,0.0458,-0.0039,-0.0368,0.0203,0.0324,-0.0097,0.0513,0.001,-0.0183,-0.0231,0.0203,0.0171,-0.0441,-0.0484,-0.0459,0.0901,-0.0517,0.0061,-0.0101,0.0059,-0.0163,0.1096,0.019,0.0166,-0.0335,0.0015,0.0284,-0.035,0.0255,0.0657,-0.0028,-0.3058,0.056,0.028,0.0248,-0.0003,0.0092,0.0291,0.0012,-0.0205,0.0212,-0.0006,0.0593,-0.0097,0.0045,0.0014,0.0426,0.1046,-0.0052,0.0278,-0.0462,0.0111,0.0623,0.1878,-0.0261,0.0488,0.0141,-0.0547,-0.0127,0.0368,0.0001,0.0253,0.0186,0.082,-0.0338,0.0614,0.0942,-0.0123,0.0335,0.0291,0.0156,-0.0528,0.0141,-0.0322,-0.0611,0.0908,-0.0068,-0.0173,-0.0796,-0.0345,0.0225,0.0031,0.0188,-0.0477,-0.0288,0.0515,0.0279,-0.0127,-0.0629,-0.0528,-0.0462,0.0024,-0.0349,0.034,-0.012,-0.0366]}
{"key":"[Can Hybrid Geometric Scattering Networks Help Solve the Maximal Clique Problem?] We propose a geometric scattering-based graph neural network (GNN) for approximating solutions of the NP-hard maximal clique (MC) problem. We construct a loss function with two terms, one which encourages the network to find a large set of nodes and the other which acts as a surrogate for the constraint that the nodes form a clique. We then use this loss to train a novel GNN architecture that outputs a vector representing the probability for each node to be part of the MC and apply a rule-based decoder to make our final prediction. The incorporation of the scattering transform alleviates the so-called oversmoothing problem that is often encountered in GNNs and would degrade the performance of our proposed setup. Our empirical results demonstrate that our method outperforms representative GNN baselines in terms of solution accuracy and inference speed as well as conventional solvers like GUROBI with limited time budgets.","layer":1,"vector":[-0.0579,-0.0334,0.0417,0.0012,0.0347,0.0233,0.0021,0.0137,-0.0169,-0.0229,0.041,-0.0421,0.0842,0.0265,0.0048,0.0362,0.0053,0.0494,-0.0204,-0.0069,0.003,-0.0079,-0.0583,-0.0761,0.028,0.031,-0.0371,-0.0053,-0.0525,-0.2672,-0.0003,-0.0403,0.0192,-0.0455,0.0055,-0.0333,0.0273,0.0187,-0.0495,0.0384,0.0453,0.0131,-0.0058,-0.0606,0.0005,-0.0093,-0.0094,-0.0085,-0.0215,-0.059,0.0107,-0.0643,0.0067,-0.0091,0.0456,0.0363,0.0583,0.0174,-0.008,0.0535,0.0108,0.0348,-0.1392,0.0961,0.0629,0.0386,-0.0056,-0.0227,0.0197,0.113,0.0134,0.031,0.04,0.034,0.0581,0.0368,-0.0132,-0.0414,-0.038,0.0009,-0.032,-0.0179,-0.0496,-0.0016,0.0103,-0.0149,-0.0038,-0.0725,-0.0186,-0.0274,-0.0139,-0.02,-0.0374,0.0414,-0.0782,-0.0198,0.0408,0.0144,-0.0349,0.2252,0.0051,-0.0195,0.0575,-0.0159,-0.0007,-0.0552,-0.0467,-0.0355,-0.0201,-0.0143,-0.0126,-0.0136,0.0234,-0.0284,-0.0225,0.0257,0.0715,0.0198,-0.0387,-0.0429,-0.0427,-0.0057,0.0329,0.0111,0.0222,-0.0507,-0.0262,0.0939,0.0507,0.0517,0.0369,0.0245,-0.0041,-0.035,0.0307,0.0101,0.0187,0.0099,-0.0623,0.0587,-0.0281,-0.0615,0.0196,-0.0592,-0.0589,0.1046,-0.056,-0.0124,-0.0528,-0.0588,-0.0103,-0.0141,-0.0522,-0.0397,-0.0143,0.0425,0.0423,0.0486,-0.0774,0.0335,-0.0228,0.0122,-0.0737,0.0909,-0.0063,-0.0869,0.0083,-0.0296,0.0295,-0.0301,-0.0163,0.0229,-0.007,0.0545,0.058,0.0249,-0.1042,-0.0092,-0.0058,0.0079,-0.0007,-0.0268,-0.0279,0.0091,0.0259,-0.036,0.0197,-0.024,0.0196,0.0214,-0.023,0.0385,-0.0298,0.0209,-0.0508,-0.0145,0.0058,0.0006,0.0585,-0.0132,0.027,-0.0131,-0.068,0.044,0.0037,0.0314,0.0085,-0.0045,0.0255,0.056,-0.0173,-0.0021,0.0496,-0.054,-0.0236,0.0212,-0.0056,0.0322,0.0158,0.0435,0.0582,-0.0507,-0.0662,-0.2206,-0.0046,0.0286,-0.0103,0.0813,-0.0647,0.0239,-0.0075,0.0307,0.0917,0.0821,0.0044,-0.0588,-0.0032,0.0124,0.0396,0.0302,0.0482,-0.0394,-0.0,-0.0158,-0.0183,-0.0286,-0.0825,0.0478,0.0293,0.2454,0.0618,0.035,-0.0062,0.026,0.0184,-0.062,-0.0563,0.0504,0.0323,0.0717,-0.0249,-0.0302,-0.0362,-0.059,0.0155,-0.0123,-0.0961,-0.0147,0.0009,-0.0101,0.0395,-0.0812,0.0161,0.0799,-0.0504,0.0382,-0.0044,-0.0031,-0.0244,-0.0523,0.0213,-0.0093,0.0023,0.0212,-0.0605,-0.0336,-0.0627,0.0669,0.014,0.0064,0.0013,-0.0072,-0.0339,-0.007,0.0758,-0.0098,0.0137,0.0363,0.0119,0.0474,-0.0102,-0.0036,-0.0425,0.1093,-0.0281,0.0351,0.0098,0.0409,0.0257,0.0558,-0.0102,0.0544,-0.0218,0.0141,0.0015,-0.0205,0.0092,0.0333,-0.0208,-0.3063,0.0717,-0.0083,0.0498,-0.0395,0.0308,0.0351,0.0572,-0.0696,-0.0428,0.025,0.0244,-0.0031,-0.0015,0.0293,0.011,0.0029,-0.0663,-0.0084,-0.0561,0.0496,0.0118,0.2407,-0.0403,0.0609,0.0309,-0.0128,-0.0057,0.0418,-0.024,0.0075,0.0333,0.0475,-0.0787,0.0142,0.0828,0.001,0.0207,0.0401,-0.0222,0.0045,-0.027,-0.0155,-0.0115,0.0933,0.0089,-0.0406,-0.0658,0.0429,0.0014,-0.0343,0.0391,-0.047,-0.0136,-0.0135,0.012,-0.0538,-0.0324,-0.061,-0.0362,0.0366,-0.0411,0.0315,0.0034,-0.005]}
{"key":"[Multi-layer Relation Networks] Relational Networks (RN) as introduced by Santoro et al. (2017) have demonstrated strong relational reasoning capabilities with a rather shallow architecture. Its single-layer design, however, only considers pairs of information objects, making it unsuitable for problems requiring reasoning across a higher number of facts. To overcome this limitation, we propose a multi-layer relation network architecture which enables successive refinements of relational information through multiple layers. We show that the increased depth allows for more complex relational reasoning by applying it to the bAbI 20 QA dataset, solving all 20 tasks with joint training and surpassing the state-of-the-art results.","layer":1,"vector":[-0.0723,0.0281,0.0267,-0.0148,0.0069,-0.0159,0.0222,0.042,-0.0003,-0.0494,-0.0335,-0.0858,0.1017,0.0681,0.0754,0.0249,0.0275,0.0341,-0.0428,-0.0096,0.0152,-0.0617,-0.0245,-0.0759,0.0515,0.0555,0.0007,-0.0465,-0.0374,-0.2374,-0.0222,-0.0161,0.033,0.0151,-0.0215,-0.0271,-0.0009,0.0034,0.0081,-0.0008,0.062,0.0238,0.0314,-0.0599,-0.0028,-0.0154,-0.0165,-0.0027,-0.0152,-0.0664,-0.0074,-0.0557,-0.0163,0.0033,0.0168,0.0393,0.0598,0.0451,0.0301,0.0697,0.0528,0.0478,-0.1693,0.1003,0.0279,0.0024,-0.061,0.0039,-0.0072,0.0415,0.0004,0.0281,0.0121,0.0454,0.0506,-0.0157,-0.0206,-0.0239,0.0184,0.0206,-0.0116,-0.0234,-0.0372,-0.008,-0.0068,-0.0469,-0.0398,-0.0398,0.01,0.0045,-0.0489,-0.0385,-0.0078,-0.0015,-0.0208,-0.0222,-0.0023,-0.0118,-0.0696,0.1687,-0.0259,-0.0154,0.0343,-0.0165,0.0164,-0.0232,-0.0271,-0.052,-0.0125,0.0359,-0.0066,-0.0401,0.0068,-0.0344,0.0552,0.0306,0.071,0.0391,-0.0472,-0.0278,0.0032,0.0039,0.0363,0.0133,-0.0116,-0.0735,0.0152,0.1077,-0.0014,0.0374,0.0721,-0.0048,-0.0421,-0.0275,0.0421,-0.005,0.0144,-0.0193,-0.0208,0.0321,-0.02,-0.0628,0.0355,-0.0756,-0.0932,0.1138,-0.0154,0.0245,-0.0666,0.0141,-0.036,0.041,0.0047,-0.0622,0.0315,0.0115,0.0413,0.0319,-0.0639,0.0182,0.0228,-0.0825,-0.0438,0.0936,0.031,-0.1189,-0.0021,-0.0101,0.0212,-0.063,0.0861,0.0611,-0.0266,-0.0127,0.0735,0.0116,-0.0201,0.0255,-0.0054,0.0059,0.0539,-0.0365,-0.0217,0.0696,-0.0,-0.0232,0.0166,-0.0181,-0.0012,0.0187,-0.0421,0.0296,-0.0173,0.0324,-0.0007,-0.0179,-0.0058,0.0243,-0.0131,-0.0248,0.0079,-0.0242,-0.0759,0.0146,-0.0375,0.0338,-0.0274,-0.0098,0.0562,0.0208,-0.0149,0.014,0.0021,-0.0797,-0.0089,0.0025,-0.0355,0.0284,-0.0272,0.0369,0.0485,-0.0299,-0.0182,-0.2568,0.0197,0.0164,-0.0249,0.0532,-0.0148,0.0215,0.0304,-0.0252,0.1155,0.054,-0.0103,-0.0338,-0.0012,-0.0332,0.0586,0.0615,0.0259,-0.0432,-0.0064,-0.0461,0.0266,-0.0048,-0.0895,0.0659,0.0075,0.2422,-0.0205,0.0003,0.0207,0.0439,0.0321,-0.0385,-0.1015,0.076,0.0138,0.063,-0.0177,-0.0284,-0.0659,-0.0446,0.0227,0.0094,-0.1261,-0.0205,-0.0379,-0.0073,0.0417,-0.0299,0.0105,-0.0005,-0.0195,0.0367,0.0435,-0.0046,-0.0344,-0.1024,0.0304,-0.0513,0.0282,0.0337,-0.0115,-0.0269,-0.0143,0.0794,-0.0095,-0.0257,-0.0121,0.0215,-0.0468,-0.0239,0.0884,0.0086,-0.0177,0.0259,-0.0088,0.0616,-0.0272,0.0016,-0.0246,0.0823,-0.0487,0.0515,0.0,0.0522,0.0307,0.1068,-0.0007,0.0755,-0.0195,-0.0083,-0.013,-0.0602,-0.0264,0.0284,-0.0281,-0.3021,0.039,-0.0259,0.0212,-0.0289,0.0411,0.0204,0.0514,-0.0313,-0.0197,0.0152,0.012,0.031,-0.0022,-0.0277,0.0159,0.0688,-0.0406,0.0523,0.0029,0.0137,0.0445,0.2025,0.0102,0.0488,0.0325,-0.0114,-0.02,0.0306,0.0066,0.0117,0.0265,0.0551,-0.0404,0.016,0.0681,-0.0332,0.049,0.0654,-0.0128,-0.0665,-0.0423,-0.0404,-0.0402,0.1148,0.0218,-0.0363,-0.0454,-0.0026,0.0102,0.0179,-0.0004,-0.0212,-0.0002,0.0264,-0.0175,-0.0137,-0.0272,-0.036,-0.0458,0.0037,-0.0441,-0.0069,0.0243,-0.0321]}
{"key":"[Towards Safer Self-Driving Through Great PAIN (Physically Adversarial Intelligent Networks)] Automated vehicles' neural networks suffer from overfit, poor generalizability, and untrained edge cases due to limited data availability. Researchers synthesize randomized edge-case scenarios to assist in the training process, though simulation introduces potential for overfit to latent rules and features. Automating worst-case scenario generation could yield informative data for improving self driving. To this end, we introduce a \"Physically Adversarial Intelligent Network\" (PAIN), wherein self-driving vehicles interact aggressively in the CARLA simulation environment. We train two agents, a protagonist and an adversary, using dueling double deep Q networks (DDDQNs) with prioritized experience replay. The coupled networks alternately seek-to-collide and to avoid collisions such that the \"defensive\" avoidance algorithm increases the mean-time-to-failure and distance traveled under non-hostile operating conditions. The trained protagonist becomes more resilient to environmental uncertainty and less prone to corner case failures resulting in collisions than the agent trained without an adversary.","layer":3,"vector":[-0.0676,-0.0216,0.0396,0.0048,0.0056,0.0634,0.048,0.0066,-0.0202,-0.0395,-0.0298,-0.0302,0.0594,0.0742,0.0169,0.0228,-0.0018,0.0081,-0.0367,0.015,0.0178,-0.0507,0.0268,-0.0523,-0.0068,0.0031,-0.0013,-0.0189,-0.0326,-0.2261,-0.0212,-0.0401,-0.0006,-0.0176,-0.0224,-0.0237,-0.0449,0.0545,0.0278,-0.0145,0.0571,0.0156,-0.0393,-0.0345,0.0237,-0.0263,-0.0075,0.0081,0.0188,-0.0653,0.0367,-0.0247,0.0381,-0.002,0.0337,-0.0265,0.0543,0.0288,0.031,0.0551,0.0327,0.0388,-0.1437,0.0566,0.038,0.0257,-0.0614,-0.0378,0.0134,0.0472,-0.0314,-0.0076,0.01,0.0419,0.017,0.007,0.0074,-0.046,-0.0064,0.0313,0.0132,-0.0185,-0.0665,0.0395,-0.0201,-0.0593,0.0152,-0.0411,0.0441,-0.0122,-0.0329,0.0007,0.0018,0.0381,-0.0317,-0.0079,-0.0102,-0.018,-0.0702,0.2366,-0.0309,0.0093,0.0004,0.0048,0.0078,-0.0034,-0.0571,-0.0759,-0.057,0.0332,0.005,-0.0073,0.0326,-0.0091,0.0212,0.0361,0.0321,0.074,-0.0179,-0.0094,-0.0413,0.0031,0.0833,-0.0258,0.0388,-0.0891,0.0188,0.1243,0.0065,0.0206,0.0231,-0.0496,-0.0342,-0.0224,0.0147,-0.0278,-0.0213,0.0165,-0.0155,-0.0081,0.0024,-0.0645,0.0098,-0.082,-0.0572,0.0348,-0.0269,0.0026,0.0049,-0.0079,-0.0016,-0.0201,-0.0391,-0.0171,0.0361,0.0322,0.0442,0.0285,-0.0587,0.0025,-0.0147,-0.0328,-0.0302,0.1521,0.0238,-0.0666,-0.0015,0.0378,0.0348,-0.0053,0.0379,-0.0005,-0.0337,0.0399,0.0856,0.0112,-0.1051,0.0135,-0.0679,0.0303,-0.0073,-0.0265,-0.0271,0.0186,0.0337,0.0084,-0.0187,-0.0391,-0.0208,0.0243,-0.0082,-0.0017,-0.0821,0.0096,-0.0493,-0.0316,0.0269,0.0012,-0.01,0.0004,-0.0336,-0.0224,-0.0457,0.0194,0.0142,-0.0358,-0.027,-0.0025,0.0091,0.0257,-0.0558,-0.0195,0.0676,-0.0218,-0.0321,-0.0199,-0.0398,0.0406,-0.0461,0.0582,0.0052,-0.0207,-0.0129,-0.2282,0.0031,-0.0207,-0.0145,0.0452,-0.0667,0.0176,-0.0096,0.0619,0.0544,0.078,-0.0371,-0.0029,0.0087,-0.0002,0.0676,-0.012,0.0462,-0.0383,0.0371,-0.0306,0.0363,-0.0156,-0.0564,0.0385,0.0622,0.2576,0.0428,0.0654,0.0091,0.0349,0.056,-0.0175,-0.092,0.0718,-0.0362,0.1274,0.0123,-0.0617,-0.0603,-0.0533,0.0389,-0.0276,-0.1064,-0.0229,-0.0561,-0.0359,0.0345,-0.0604,0.0261,0.0558,-0.0266,0.0628,-0.0136,-0.0327,-0.0471,-0.1261,0.0455,-0.0298,0.0113,-0.0094,-0.0279,0.0146,-0.0814,0.0749,0.0329,-0.0025,-0.053,0.0533,0.0021,-0.0233,0.0502,-0.0001,0.0075,0.055,-0.0157,0.0491,-0.0206,-0.0287,-0.0393,0.0618,-0.002,0.0493,0.044,0.0502,-0.0168,0.0641,-0.0081,0.0216,-0.0329,0.0112,0.0139,-0.0482,-0.0334,0.0629,-0.0106,-0.2969,0.0348,0.0097,0.0617,-0.0735,0.0043,0.0608,0.0326,-0.0526,-0.0026,-0.0239,0.0706,0.0538,0.029,0.0105,0.0088,0.0552,0.006,0.019,-0.0578,0.0415,0.0495,0.2046,-0.0476,0.0578,0.0594,-0.0511,0.0019,0.03,-0.0493,-0.0186,0.0177,0.0558,-0.0741,0.0342,0.0798,-0.0466,0.0238,0.0547,0.0326,-0.0098,0.0201,0.0205,-0.0143,0.0881,-0.0207,-0.0192,-0.0543,0.0101,0.047,-0.0032,-0.009,-0.021,-0.0063,0.0517,0.0207,-0.0049,-0.0453,-0.0012,-0.0613,0.021,-0.0366,-0.0069,0.0174,-0.0215]}
{"key":"[Exploring Techniques for the Analysis of Spontaneous Asynchronicity in MPI-Parallel Applications] This paper studies the utility of using data analytics and machine learning techniques for identifying, classifying, and characterizing the dynamics of large-scale parallel (MPI) programs. To this end, we run microbenchmarks and realistic proxy applications with the regular compute-communicate structure on two different supercomputing platforms and choose the per-process performance and MPI time per time step as relevant observables. Using principal component analysis, clustering techniques, correlation functions, and a new \"phase space plot,\" we show how desynchronization patterns (or lack thereof) can be readily identified from a data set that is much smaller than a full MPI trace. Our methods also lead the way towards a more general classification of parallel program dynamics.","layer":5,"vector":[-0.1079,-0.0033,0.0006,-0.0131,0.0098,0.0148,0.0342,0.0075,0.0666,-0.026,0.0189,-0.0374,0.0259,0.0204,0.0001,0.0102,0.0156,-0.0177,-0.0018,-0.0319,0.0352,-0.0442,-0.0847,-0.0718,0.0292,0.0258,-0.0328,-0.01,-0.0507,-0.2546,0.0222,-0.0225,0.0563,-0.0366,0.0371,-0.0168,-0.015,0.0802,-0.0167,0.0256,0.0062,0.0155,-0.022,-0.0494,-0.0116,-0.0388,-0.0302,-0.0183,-0.0082,-0.0069,-0.0153,-0.0336,0.0266,0.0027,0.0421,0.0289,0.0662,0.0392,0.0475,0.0167,0.0409,0.0449,-0.1771,0.0622,0.1042,0.0337,-0.0152,-0.0621,0.0092,0.0729,-0.0299,0.0337,0.0225,0.0569,0.0319,-0.0301,-0.0175,0.013,-0.0134,0.0375,0.012,-0.0439,-0.0471,-0.0397,-0.047,-0.0121,0.0292,0.024,0.0827,-0.0281,-0.0532,-0.0007,-0.0188,0.0197,-0.051,0.0273,0.0467,0.0313,-0.0053,0.2062,-0.0882,0.0104,0.0273,0.0161,0.0357,-0.0691,-0.0059,-0.0292,-0.0399,-0.0218,-0.0121,-0.0011,0.0381,-0.0365,0.1001,-0.0102,0.0415,0.002,-0.0068,0.032,-0.0079,0.0061,0.042,-0.0191,0.0415,-0.0537,0.0324,0.1343,0.0111,-0.0272,0.0365,0.0513,-0.0258,-0.0106,0.0252,0.0294,0.018,-0.0144,0.0194,-0.0436,-0.0773,0.0133,0.0063,-0.0821,-0.0097,0.1642,0.0195,0.0592,-0.0045,-0.0037,-0.0551,0.0144,-0.0819,-0.0538,0.0011,0.0547,-0.005,0.0286,-0.0111,0.0317,-0.0589,-0.0063,-0.0074,0.1031,0.0127,-0.0716,-0.0419,0.0177,-0.0059,-0.0319,0.0239,0.0565,-0.0188,-0.0267,0.0239,0.0037,-0.0597,-0.0321,-0.0211,0.0237,0.0438,-0.0319,-0.0513,0.0262,0.0665,-0.0775,-0.0016,0.0245,0.0123,0.0239,-0.0503,-0.0301,-0.0267,0.0157,-0.001,-0.0563,0.0267,-0.0072,0.0315,-0.03,-0.0082,0.0451,-0.041,0.0836,-0.0422,0.0094,-0.0533,0.0255,0.0374,0.0519,-0.0179,-0.0144,0.0279,0.0097,-0.0456,0.0234,-0.0098,0.0326,0.0393,0.0652,0.0738,-0.0531,-0.0629,-0.2288,-0.0108,-0.0063,-0.0034,0.0454,-0.0634,0.0241,-0.0575,0.0445,0.046,0.0407,0.013,-0.0252,-0.0203,0.0093,0.0537,0.0028,0.0704,-0.0321,0.0002,-0.0285,0.014,-0.0471,-0.0687,0.0021,0.0178,0.202,-0.0115,0.0389,-0.0337,0.0259,0.0092,-0.0471,-0.0668,0.0519,0.036,0.0788,-0.0225,-0.0053,-0.0063,-0.0523,0.0641,-0.006,-0.0726,-0.0085,-0.0378,0.0241,-0.0268,-0.0565,-0.007,0.0254,-0.0615,0.0099,0.0097,-0.0109,-0.0504,-0.0569,0.0304,-0.004,0.0137,0.0167,-0.0552,0.0023,-0.0504,0.0813,-0.0081,-0.0646,-0.0225,0.0048,-0.0511,-0.0139,0.1034,-0.0591,-0.0543,0.0665,-0.0086,0.0457,-0.0222,-0.0511,0.0009,0.0845,-0.0457,-0.0082,0.0212,0.0,-0.0337,0.0419,0.0034,0.0047,-0.0291,0.0089,0.0531,-0.0687,-0.0108,0.0259,0.0041,-0.2882,0.0288,-0.0269,0.0055,-0.023,-0.0157,0.046,0.0325,-0.0422,0.023,0.0222,0.0499,0.0232,-0.0027,-0.0072,0.0931,0.0894,-0.0251,0.0034,-0.0509,0.0045,0.0065,0.201,-0.0226,0.0449,0.0277,-0.0406,0.0498,0.0273,-0.0091,0.031,-0.0335,0.0521,-0.0666,0.0503,0.0487,-0.0194,0.0301,0.0235,0.0184,0.0335,0.0518,-0.0063,-0.0596,0.0974,-0.0089,-0.0478,-0.0975,0.0195,0.042,-0.0786,0.0056,-0.0129,-0.0154,0.0014,0.0807,-0.0431,-0.0203,-0.0605,-0.0292,0.0354,-0.0439,-0.0028,-0.0097,0.0011]}
{"key":"[Towards FPGA Implementation of Neural Network-Based Nonlinearity Mitigation Equalizers in Coherent Optical Transmission Systems] For the first time, recurrent and feedforward neural network-based equalizers for nonlinearity compensation are implemented in an FPGA, with a level of complexity comparable to that of a dispersion equalizer. We demonstrate that the NN-based equalizers can outperform a 1 step-per-span DBP.","layer":0,"vector":[-0.0947,-0.0075,0.0167,-0.0662,0.0107,0.0388,0.0057,0.0361,0.0078,-0.0421,0.0233,-0.0709,0.0312,0.0285,-0.0115,-0.0296,-0.0002,0.0239,-0.0329,0.0042,0.0738,-0.0528,-0.0583,-0.0229,-0.0073,-0.0232,-0.0416,-0.061,-0.0271,-0.2389,0.0005,-0.041,0.0182,-0.0298,-0.0414,-0.0516,-0.0548,-0.0054,-0.0416,0.0675,0.0761,0.0268,0.0081,-0.0643,-0.015,-0.0685,-0.0334,0.0021,0.0016,-0.0794,-0.0142,0.0122,-0.0049,0.0198,0.0188,0.0194,0.0316,0.0692,0.0692,0.0277,0.012,0.0781,-0.1764,0.0615,0.0165,0.0271,-0.0091,-0.061,0.0533,0.0298,-0.0341,0.0262,0.0457,0.0297,0.0512,-0.023,-0.007,-0.0307,-0.003,0.0485,0.0621,-0.065,-0.0129,-0.03,0.0183,-0.0218,-0.017,-0.0196,-0.0225,0.0266,-0.033,0.0255,-0.0031,-0.0078,-0.0199,-0.0158,-0.0091,-0.0094,-0.0235,0.1738,-0.0368,0.0109,0.0028,-0.0854,0.037,-0.0036,-0.0316,-0.0139,-0.0497,0.014,-0.011,-0.0741,0.0166,0.0099,0.0087,-0.0091,0.0066,-0.0037,0.0532,-0.008,-0.0287,0.0164,0.0098,0.0048,0.017,-0.0537,0.0129,0.1116,0.0146,0.0756,-0.0061,-0.0125,-0.026,-0.026,0.0507,0.0682,-0.0019,-0.03,-0.0012,0.0366,-0.0682,-0.0936,0.0325,-0.069,-0.0565,0.1029,-0.0409,0.0191,0.0203,-0.0013,-0.0161,0.0292,-0.0106,-0.053,0.0236,0.0705,0.0499,0.055,-0.0546,0.0329,-0.0062,-0.0849,-0.0446,0.1134,0.028,-0.0761,-0.0189,0.0147,-0.0035,-0.0252,0.0521,-0.0086,-0.0557,-0.01,0.0546,0.0273,-0.0409,-0.0359,-0.0189,-0.0011,0.0218,-0.0758,-0.0015,0.0027,0.0878,-0.0287,0.0022,-0.052,-0.0079,0.015,-0.0522,0.0032,-0.0449,0.0005,-0.0269,0.0061,-0.0178,0.0141,0.0088,-0.0313,0.0313,-0.0437,-0.0192,0.0315,0.0165,0.0242,0.02,-0.014,0.0449,0.0403,-0.0284,0.0088,0.1286,-0.0051,-0.0389,-0.0169,-0.073,0.0363,0.0278,0.0673,0.061,-0.0112,-0.1146,-0.2351,0.0365,0.0147,-0.0565,0.1029,-0.067,0.0202,0.0112,0.0305,0.0906,0.0299,0.0406,0.0191,0.0142,-0.0465,0.0358,0.0406,0.0301,-0.0141,-0.032,-0.0102,0.0227,-0.0074,-0.0576,0.0413,-0.0113,0.1737,-0.0227,0.0695,-0.062,0.0795,0.062,-0.0234,-0.0703,0.0373,0.0387,0.0824,-0.0126,0.0003,-0.0342,-0.0058,-0.0051,0.0025,-0.1095,-0.0118,-0.0307,-0.0414,0.0325,-0.0314,0.0144,0.0176,-0.0575,0.0593,0.0453,0.0515,-0.0308,-0.092,0.0269,-0.0391,0.0154,0.0521,-0.0372,0.0008,-0.039,0.0718,0.0378,0.0162,0.0036,0.0564,0.0194,0.0046,0.1022,0.0202,0.0389,0.0725,-0.0145,0.0195,-0.0371,-0.0167,-0.0061,0.0885,-0.0081,0.0376,0.0166,0.0123,0.0034,0.0671,0.0066,-0.0164,-0.0102,-0.0144,0.0262,-0.0223,-0.0203,0.0085,0.0054,-0.2964,0.0349,0.0204,0.0051,-0.0105,0.0316,0.0378,0.0292,-0.0491,-0.0168,-0.0706,0.0583,0.0147,0.0118,0.0511,0.0782,0.0287,-0.0744,0.0344,-0.0431,0.041,0.0318,0.2004,-0.003,0.0188,-0.0045,-0.0135,0.0397,0.0156,-0.0172,0.0346,0.0207,0.0777,-0.0477,-0.0131,0.0696,-0.0055,0.033,0.0276,0.0114,-0.0239,0.0027,-0.0242,0.0045,0.0925,-0.0256,-0.0769,-0.0179,0.0069,-0.0401,0.0042,0.0691,0.0455,-0.0185,-0.0433,-0.0358,-0.1052,-0.0536,-0.0436,-0.0433,0.0251,-0.0802,-0.0198,0.0063,0.024]}
{"key":"[A JIT Compiler for Neural Network Inference] This paper describes a C++ library that compiles neural network models at runtime into machine code that performs inference. This approach in general promises to achieve the best performance possible since it is able to integrate statically known properties of the network directly into the code. In our experiments on the NAO V6 platform, it outperforms existing implementations significantly on small networks, while being inferior on large networks. The library was already part of the B-Human code release 2018, but has been extended since and is now available as a standalone version that can be integrated into any C++14 code base.","layer":4,"vector":[-0.0719,-0.005,0.0118,-0.0064,0.0482,0.0426,0.0174,0.0403,0.0506,-0.0505,0.018,-0.0641,0.0381,0.035,0.0302,-0.0187,-0.0231,0.0449,-0.0104,0.0033,0.004,-0.037,-0.0102,-0.0642,0.0078,-0.0012,-0.0066,-0.0186,-0.0353,-0.2188,0.0116,-0.0401,0.0801,-0.0101,-0.011,-0.0172,-0.0086,-0.0033,-0.0155,0.0602,0.0474,0.0383,-0.0283,-0.066,0.0192,-0.0272,-0.026,-0.0131,-0.035,-0.0507,0.003,-0.0311,0.0042,0.0315,0.0266,0.0302,0.0651,0.0625,0.0559,0.0436,-0.0113,0.0361,-0.1656,0.039,0.0485,0.0082,-0.0354,-0.0219,0.0344,0.0266,-0.0025,0.0583,0.0235,0.0477,0.0146,0.0115,0.0001,-0.0191,0.0125,0.0051,0.0077,-0.0064,-0.0197,-0.0085,0.0508,0.0134,-0.0178,-0.0059,-0.0016,-0.0035,-0.0436,-0.0316,-0.0114,0.048,-0.0432,0.0249,0.04,0.0031,-0.0782,0.2352,-0.0364,0.0005,0.0237,-0.0178,0.0308,-0.0197,-0.0282,-0.0484,-0.0541,-0.0092,-0.036,-0.0179,0.0162,-0.0494,0.0585,0.005,0.0922,0.0056,-0.0222,0.0092,-0.0101,0.0284,0.0338,-0.0085,0.0168,-0.0919,0.0046,0.1365,0.0216,0.0278,0.0563,-0.0142,-0.0402,-0.0389,0.0341,0.033,0.0326,-0.0267,-0.0451,-0.0062,-0.0358,-0.0191,0.0402,-0.0411,-0.0573,0.0795,-0.0241,-0.0058,-0.0191,-0.0299,-0.0232,0.0697,-0.0513,-0.0249,0.0407,0.0421,-0.0075,0.03,-0.0821,0.0218,-0.043,-0.0147,-0.048,0.072,0.0228,-0.0708,-0.0427,-0.0056,0.0491,-0.04,0.0351,-0.0327,0.0221,0.0347,0.0386,0.037,-0.0604,0.0116,-0.0215,0.0672,0.0116,-0.0381,-0.0526,0.0442,0.0028,-0.0514,-0.0086,-0.0634,-0.0351,0.0371,-0.057,0.0526,-0.0425,-0.0178,-0.0281,-0.0331,0.0048,0.0081,0.0416,-0.0594,0.0059,0.0297,-0.0101,0.0125,-0.0336,0.0022,-0.0297,0.0127,0.0055,0.0306,-0.0195,-0.0046,0.0775,-0.0108,-0.0159,0.0263,-0.0194,-0.001,0.0247,0.054,0.0308,-0.0859,-0.0752,-0.2566,0.0058,0.0339,-0.0339,0.0736,-0.0815,0.0488,-0.0079,0.033,0.0613,0.047,0.0039,-0.0156,0.0128,-0.0149,0.0534,0.0305,0.0063,-0.0591,-0.0145,-0.026,0.0304,-0.0261,-0.1072,0.046,0.0155,0.2014,0.0371,0.0927,-0.0071,0.0185,0.0332,-0.0284,-0.1233,0.1002,0.045,0.0777,0.0261,-0.03,-0.0178,-0.0614,0.0405,0.0093,-0.1184,-0.042,-0.0137,-0.0231,0.016,-0.0463,-0.017,0.0383,-0.037,0.0447,0.016,-0.0219,-0.0792,-0.1077,0.0029,-0.0519,0.0452,0.0239,-0.053,-0.0121,-0.0548,0.0664,-0.0084,0.0163,-0.0301,0.0376,-0.0139,-0.0069,0.0488,-0.0011,0.0149,0.0738,0.015,0.016,-0.0383,-0.035,0.0088,0.0585,-0.0059,0.0478,0.0255,0.0522,-0.0099,0.0701,-0.002,0.0363,-0.0172,-0.013,0.0158,-0.029,-0.0311,0.0739,0.0371,-0.3121,0.0376,0.0199,0.0265,-0.0108,0.0304,0.0211,-0.0131,-0.0581,-0.0168,0.0242,0.0139,0.0646,-0.0171,0.0189,0.0301,0.0684,-0.0765,0.0408,-0.0454,-0.0002,0.0225,0.2052,-0.0293,0.056,0.0489,-0.0329,-0.0106,0.071,-0.0058,-0.0047,0.0163,0.0783,-0.0626,-0.0062,0.0963,-0.0541,0.0404,0.0367,-0.0053,0.0073,-0.0156,-0.0442,-0.0111,0.0521,-0.0115,-0.0464,-0.0464,-0.0227,0.0092,-0.0323,-0.0054,0.0278,-0.0129,0.0328,0.0161,-0.0416,-0.0697,-0.0577,-0.0198,0.0409,-0.1056,0.0213,0.0172,-0.0457]}
{"key":"[Self-supervised Speaker Diarization] Over the last few years, deep learning has grown in popularity for speaker verification, identification, and diarization. Inarguably, a significant part of this success is due to the demonstrated effectiveness of their speaker representations. These, however, are heavily dependent on large amounts of annotated data and can be sensitive to new domains. This study proposes an entirely unsupervised deep-learning model for speaker diarization. Specifically, the study focuses on generating high-quality neural speaker representations without any annotated data, as well as on estimating secondary hyperparameters of the model without annotations. The speaker embeddings are represented by an encoder trained in a self-supervised fashion using pairs of adjacent segments assumed to be of the same speaker. The trained encoder model is then used to self-generate pseudo-labels to subsequently train a similarity score between different segments of the same call using probabilistic linear discriminant analysis (PLDA) and further to learn a clustering stopping threshold. We compared our model to state-of-the-art unsupervised as well as supervised baselines on the CallHome benchmarks. According to empirical results, our approach outperforms unsupervised methods when only two speakers are present in the call, and is only slightly worse than recent supervised models.","layer":0,"vector":[-0.0649,-0.0537,0.0155,-0.0311,0.0089,0.0129,0.0218,0.016,-0.01,-0.0376,0.0155,-0.0514,0.0262,0.0548,0.0362,0.0378,0.0212,0.0957,-0.0612,-0.0237,0.0404,-0.0371,-0.0322,-0.0043,0.0579,-0.0074,-0.0455,-0.0686,-0.0255,-0.2491,0.0324,0.0001,0.0743,-0.0238,0.0067,-0.0303,-0.0242,0.0497,-0.0477,-0.0087,-0.0012,-0.002,-0.0101,-0.055,-0.0018,-0.0746,-0.0464,-0.0301,-0.0321,-0.0211,0.03,-0.0343,0.0397,-0.0014,-0.0318,0.044,0.0566,0.0278,0.0362,0.0307,0.0257,0.0399,-0.1929,0.0279,0.0167,0.0548,-0.0349,-0.0293,-0.0001,0.0279,-0.0118,0.0442,0.0238,0.0238,0.0224,-0.0149,0.0339,-0.0254,-0.0047,0.0519,-0.0033,-0.0218,-0.0226,-0.0276,0.0135,-0.0452,0.0529,-0.0429,0.0107,-0.0002,-0.0808,0.0046,-0.0183,0.0545,-0.0842,-0.0391,0.0384,0.0028,-0.0409,0.2041,-0.0474,0.0416,0.053,-0.0614,0.0059,-0.0379,-0.0398,-0.0208,-0.0211,0.0205,-0.0145,-0.0238,0.0205,-0.0487,0.0665,0.0025,0.0902,0.0465,-0.0181,-0.0072,0.0072,-0.0301,0.059,-0.0219,0.0623,-0.0598,0.0203,0.0944,0.0115,0.0514,0.0375,-0.026,-0.0318,0.0104,0.0268,0.0266,0.0288,0.031,0.029,-0.0357,0.0041,-0.0745,-0.0095,-0.0307,-0.0366,0.0994,-0.1243,0.0119,-0.0384,-0.0004,-0.0219,-0.0041,0.0177,-0.0676,0.0066,0.003,0.0712,0.005,-0.0349,0.0413,0.0141,-0.0848,0.007,0.0787,0.053,-0.0983,-0.0412,0.0209,0.0045,-0.0308,0.0115,0.0247,-0.0392,0.0036,0.0773,0.048,-0.1083,-0.0129,0.0217,0.0411,0.006,-0.0395,-0.0259,0.0443,0.0231,-0.0588,-0.0119,-0.0584,0.035,0.0627,-0.0441,0.0223,-0.0838,-0.0364,-0.0328,-0.0302,-0.0093,-0.0296,0.013,-0.0322,-0.025,0.017,-0.0099,0.039,0.0181,0.0536,-0.0189,0.0293,0.0735,0.0382,-0.0718,0.0182,0.0663,0.017,-0.0083,-0.0562,0.0048,0.0191,-0.0034,0.0628,0.0106,-0.054,-0.0217,-0.2047,0.0353,0.0447,-0.0826,0.0132,-0.0443,0.0171,0.016,0.0847,0.0738,0.0091,-0.0251,-0.0121,0.0444,-0.0261,0.051,0.0293,0.0721,-0.0188,0.029,-0.0071,0.0297,-0.0053,-0.0818,0.0232,-0.0295,0.2233,0.0107,0.0773,0.0082,-0.0153,0.0084,-0.0363,-0.1179,0.0753,-0.0088,0.0848,-0.0287,-0.0423,-0.0434,-0.0498,0.0338,0.0147,-0.0861,-0.0654,-0.0542,-0.072,0.0337,-0.0355,0.0299,0.0273,-0.0324,0.0642,0.0019,-0.0238,-0.0444,-0.0849,0.0225,-0.0593,0.0112,-0.0117,-0.0651,0.0375,-0.0651,0.0214,-0.006,-0.034,0.0145,0.0467,0.0222,-0.0194,0.0781,-0.0368,0.017,0.0672,-0.0123,0.0387,-0.0884,-0.107,0.0064,0.0607,-0.0055,0.0494,0.0291,0.0222,0.0315,0.0776,0.0298,0.0268,-0.0273,0.0124,0.0245,-0.0421,0.0296,0.054,0.0092,-0.2507,0.0282,0.0167,0.034,-0.0439,0.0023,0.0244,0.0177,-0.0404,0.0189,0.0024,0.0729,0.071,-0.0021,-0.0164,0.0598,0.0939,-0.0445,0.0296,-0.0239,0.0236,0.0206,0.1574,-0.0339,0.0282,-0.0045,-0.0558,0.0196,0.0436,-0.0218,-0.0213,0.0046,0.129,-0.0636,-0.0393,0.0643,-0.0076,-0.0297,0.0339,0.0017,-0.0124,-0.0425,-0.0099,-0.0352,0.0841,-0.0275,0.0329,0.0164,0.0193,0.0397,0.0013,-0.0067,-0.0043,-0.0119,0.0273,0.0466,-0.061,-0.0434,-0.048,-0.0109,0.0392,-0.0226,-0.0339,-0.0077,0.0489]}
{"key":"[A Hierarchical Graphical Model for Record Linkage] The task of matching co-referent records is known among other names as rocord linkage. For large record-linkage problems, often there is little or no labeled data available, but unlabeled data shows a reasonable clear structure. For such problems, unsupervised or semi-supervised methods are preferable to supervised methods. In this paper, we describe a hierarchical graphical model framework for the linakge-problem in an unsupervised setting. In addition to proposing new methods, we also cast existing unsupervised probabilistic record-linkage methods in this framework. Some of the techniques we propose to minimize overfitting in the above model are of interest in the general graphical model setting. We describe a method for incorporating monotinicity constraints in a graphical model. We also outline a bootstrapping approach of using \"single-field\" classifiers to noisily label latent variables in a hierarchical model. Experimental results show that our proposed unsupervised methods perform quite competitively even with fully supervised record-linkage methods.","layer":2,"vector":[-0.0085,-0.0225,0.0411,-0.0295,0.0102,0.0094,0.0297,0.0045,0.0373,-0.0221,0.031,-0.0405,0.0352,0.0757,-0.0046,0.0134,0.011,0.1273,-0.0408,-0.0036,-0.0072,-0.0412,-0.0145,-0.0647,0.045,0.0456,-0.0259,-0.0471,-0.0369,-0.2378,-0.0004,-0.0262,0.0564,-0.0437,0.0147,-0.0401,-0.0192,0.0466,-0.0077,0.0264,0.0488,0.032,-0.013,-0.0516,-0.0445,-0.028,-0.0075,-0.0283,-0.0612,-0.0213,-0.027,-0.0244,-0.0194,-0.0143,0.0041,0.0687,0.0909,-0.0241,0.0436,0.057,0.0306,0.0486,-0.1746,0.0507,0.0714,0.0625,-0.0224,0.0033,-0.0058,0.0409,-0.0288,0.0297,-0.0046,0.0531,-0.0032,-0.0332,-0.0044,-0.0354,-0.0388,-0.0343,-0.006,-0.0144,-0.0356,-0.0283,-0.0494,-0.0411,0.0228,-0.0829,0.018,-0.0069,-0.0313,-0.0177,-0.0088,0.0193,-0.0737,-0.0294,0.0421,0.0376,-0.0321,0.191,-0.0865,0.0582,0.0625,-0.0225,-0.0016,-0.0376,-0.0305,-0.0506,0.0045,-0.0313,-0.0124,-0.0191,-0.0062,-0.0484,0.0908,0.0097,0.0981,0.014,-0.0056,-0.0398,-0.0082,0.0217,0.0018,-0.0109,0.0416,-0.0428,0.0212,0.1119,0.0474,0.018,0.0577,-0.0229,-0.0645,-0.021,-0.0413,0.0057,0.0284,-0.0166,0.0095,0.0018,-0.033,-0.0886,0.0151,-0.0582,-0.0674,0.1775,-0.0423,0.0418,-0.0561,-0.0231,-0.013,0.0059,-0.0076,-0.0486,0.0023,-0.0038,0.0678,0.0455,-0.0317,0.017,0.0122,-0.0792,0.0021,0.099,-0.0031,-0.1002,-0.015,-0.003,0.0116,-0.0183,0.0236,0.0394,-0.0127,0.0411,0.0417,0.0464,-0.0771,0.0027,0.0263,0.0068,0.0491,-0.0545,-0.0409,0.0438,0.0055,-0.0049,0.0022,-0.0437,0.0186,0.0075,-0.0146,-0.0057,0.0063,-0.0121,-0.014,-0.0344,-0.0217,0.0077,0.042,-0.0495,0.0364,-0.0009,-0.0451,0.0297,-0.0336,0.0226,-0.0301,-0.0266,0.0267,-0.0022,-0.0085,-0.0047,0.0662,0.0179,-0.0026,0.0167,0.0256,0.0267,0.0113,0.079,0.0334,-0.0654,0.0031,-0.2482,0.0027,0.0129,0.0415,0.0471,-0.065,0.0223,0.011,-0.0074,0.1072,0.0394,0.0208,-0.0342,0.0381,-0.0472,0.0595,0.0334,0.0352,0.0082,0.0219,0.0206,-0.0063,-0.0221,-0.072,0.0599,-0.0086,0.2244,0.0195,-0.0045,-0.0377,0.007,0.0076,-0.0413,-0.0864,0.0586,0.0389,0.0148,-0.0311,-0.0192,-0.0303,-0.0504,0.0441,0.0409,-0.0738,-0.0424,-0.0123,-0.0411,-0.0192,-0.0503,0.059,0.0202,-0.0147,0.0452,-0.013,-0.0065,-0.0385,-0.0995,0.0019,-0.0125,0.0203,0.0376,-0.0437,0.0321,-0.0802,0.0663,-0.001,-0.0422,-0.0412,-0.0002,-0.0348,-0.0228,0.0619,-0.0077,0.0113,0.072,-0.0003,0.0045,-0.0533,-0.069,-0.045,0.0457,-0.0692,0.0208,0.0275,0.039,-0.0009,0.099,-0.0161,0.0902,-0.0173,0.0334,-0.0357,-0.0218,-0.0097,0.0274,0.0032,-0.2927,0.0461,0.0298,0.0406,-0.0189,0.0051,0.0447,0.0495,-0.0063,-0.0022,0.0065,0.028,0.0605,-0.0144,-0.0271,0.033,0.0508,-0.0424,0.0585,-0.0179,0.0287,-0.002,0.1878,0.0281,0.0301,0.0123,-0.0169,0.0119,0.0496,-0.0371,0.042,-0.0163,0.0955,-0.0063,0.0473,0.0971,-0.0055,-0.0044,0.0125,-0.0252,-0.0316,0.0164,-0.0751,-0.0742,0.1101,-0.008,-0.0236,-0.0082,0.0118,0.0287,-0.0329,-0.0364,0.0173,0.025,0.0126,0.0112,-0.0412,-0.0627,-0.0105,-0.0772,0.0231,-0.0289,-0.0365,0.0211,-0.0104]}
{"key":"[SafeRL-Kit: Evaluating Efficient Reinforcement Learning Methods for Safe Autonomous Driving] Safe reinforcement learning (RL) has achieved significant success on risk-sensitive tasks and shown promise in autonomous driving (AD) as well. Considering the distinctiveness of this community, efficient and reproducible baselines are still lacking for safe AD. In this paper, we release SafeRL-Kit to benchmark safe RL methods for AD-oriented tasks. Concretely, SafeRL-Kit contains several latest algorithms specific to zero-constraint-violation tasks, including Safety Layer, Recovery RL, off-policy Lagrangian method, and Feasible Actor-Critic. In addition to existing approaches, we propose a novel first-order method named Exact Penalty Optimization (EPO) and sufficiently demonstrate its capability in safe AD. All algorithms in SafeRL-Kit are implemented (i) under the off-policy setting, which improves sample efficiency and can better leverage past logs; (ii) with a unified learning framework, providing off-the-shelf interfaces for researchers to incorporate their domain-specific knowledge into fundamental safe RL methods. Conclusively, we conduct a comparative evaluation of the above algorithms in SafeRL-Kit and shed light on their efficacy for safe autonomous driving. The source code is available at \\href{ https://github.com/zlr20/saferl_kit}{this https URL}.","layer":1,"vector":[-0.0575,-0.0108,0.0419,-0.058,-0.004,0.0308,0.0312,0.0511,0.0313,0.0139,0.0107,-0.0534,0.0297,0.0716,-0.0264,0.0008,-0.005,0.0593,-0.0105,0.0031,0.0212,-0.0656,0.0055,-0.1012,0.0055,0.0395,-0.023,-0.0383,-0.0233,-0.2547,-0.023,-0.0407,-0.0078,-0.0362,-0.0318,-0.0054,-0.0393,0.0555,-0.0019,-0.0055,0.0495,0.0454,-0.0474,-0.041,-0.018,-0.0271,0.0143,-0.0015,0.0045,-0.0504,0.0118,-0.0224,0.0475,0.0359,0.0006,-0.0124,0.0587,0.0584,0.0536,0.0749,-0.0253,0.0208,-0.1608,0.0535,0.0384,0.0479,-0.0714,-0.0061,0.0349,0.0551,-0.0301,0.0296,0.0125,0.0254,0.0132,-0.0353,0.0249,-0.087,0.0029,-0.0009,0.0284,-0.0469,-0.0876,0.0209,0.0013,-0.0986,-0.0058,-0.0402,0.0383,0.0262,-0.0012,0.0045,0.0104,0.0147,-0.0435,0.0013,0.008,0.004,-0.081,0.1967,-0.0349,0.0575,-0.001,0.0208,0.0147,-0.0283,-0.0216,-0.0209,-0.0193,-0.0267,0.0156,-0.0027,0.0599,-0.0052,0.02,0.0595,0.0629,0.0548,0.0212,0.0051,-0.0166,-0.017,0.0597,-0.0145,0.0368,-0.0696,0.0426,0.1252,0.0288,0.0497,0.0271,-0.0653,-0.0028,-0.0386,0.0044,-0.0017,-0.024,0.0066,0.0245,-0.0356,-0.018,-0.0247,-0.0121,-0.1374,-0.0351,0.0399,-0.0059,0.0118,-0.0237,-0.0003,0.015,-0.009,-0.0235,-0.0116,0.0186,0.019,0.0519,0.0541,-0.0442,0.0307,-0.0181,-0.0954,-0.0097,0.1,-0.0025,-0.0547,-0.0293,0.0371,0.0082,0.0027,-0.0011,0.0205,-0.0513,0.022,0.0717,0.0237,-0.0529,0.0117,-0.0338,-0.0062,0.0094,-0.0658,-0.034,0.0044,0.0441,0.0107,-0.0183,-0.0375,0.0082,-0.0033,-0.0322,0.0038,-0.0031,-0.0078,-0.0221,-0.0081,0.0208,-0.0185,-0.0082,-0.0033,0.0124,0.0079,-0.0463,0.0108,-0.028,-0.0141,-0.0089,-0.0223,0.0619,0.0498,-0.0665,-0.0186,0.0411,0.0243,-0.0481,-0.0333,-0.0312,0.0224,-0.0153,0.0664,0.0393,0.0381,-0.0126,-0.2527,-0.0001,-0.0595,-0.0187,0.0509,-0.0546,0.0302,-0.0489,0.0369,0.0463,0.0652,-0.0624,-0.0582,0.0507,0.0032,0.0773,0.0018,-0.0099,-0.0439,0.0132,-0.0132,0.013,-0.0393,-0.0896,0.0683,0.0187,0.224,0.0186,0.078,-0.0324,0.039,0.0191,0.0146,-0.0898,0.0617,0.0099,0.0683,-0.0071,-0.0259,-0.0489,0.0155,0.034,-0.0435,-0.088,-0.0487,-0.0313,-0.0742,0.0624,-0.0528,0.0049,0.0436,-0.0309,0.0388,-0.0145,-0.0245,-0.0163,-0.0652,0.0498,-0.0282,-0.0082,0.0136,-0.036,-0.0106,-0.0688,0.0552,0.0052,0.0248,-0.0884,0.0396,0.0153,-0.0401,0.0677,-0.0022,-0.0043,0.0317,0.0105,0.0013,-0.0057,-0.0269,-0.0355,0.0551,-0.0013,0.0758,0.0251,0.012,-0.0158,0.0693,0.0028,-0.0071,-0.0145,0.0451,0.0002,-0.0446,-0.0267,0.1004,0.0153,-0.2902,0.019,0.0058,0.0396,-0.0282,0.0034,0.061,-0.0022,-0.0477,0.0057,-0.0185,0.1095,0.0508,0.0548,0.0177,0.0085,0.0657,-0.0015,0.041,-0.0729,0.0336,0.0581,0.2446,-0.0347,0.031,0.0535,-0.036,-0.0205,0.0386,-0.0084,0.0009,0.0134,0.0343,-0.088,0.0572,0.0732,-0.0483,0.0179,0.0199,0.0122,0.0018,0.0581,0.0118,0.0082,0.0606,-0.0143,-0.0411,-0.0606,-0.0116,0.0252,0.0084,-0.011,-0.0394,-0.0196,0.0467,0.0318,-0.0155,-0.0745,-0.0355,-0.0617,0.0118,-0.0082,0.0073,0.01,-0.022]}
{"key":"[deepregression: a Flexible Neural Network Framework for Semi-Structured Deep Distributional Regression] In this paper we describe the implementation of semi-structured deep distributional regression, a flexible framework to learn conditional distributions based on the combination of additive regression models and deep networks. Our implementation encompasses (1) a modular neural network building system based on the deep learning library \\pkg{TensorFlow} for the fusion of various statistical and deep learning approaches, (2) an orthogonalization cell to allow for an interpretable combination of different subnetworks, as well as (3) pre-processing steps necessary to set up such models. The software package allows to define models in a user-friendly manner via a formula interface that is inspired by classical statistical model frameworks such as \\pkg{mgcv}. The packages' modular design and functionality provides a unique resource for both scalable estimation of complex statistical models and the combination of approaches from deep learning and statistics. This allows for state-of-the-art predictive performance while simultaneously retaining the indispensable interpretability of classical statistical models.","layer":0,"vector":[-0.0019,0.0153,0.0425,0.0087,0.0476,0.0469,-0.0065,-0.0036,0.0625,-0.019,0.0283,-0.0724,0.0192,0.03,0.0252,0.0032,-0.013,0.0437,-0.0598,0.0066,0.0219,-0.0283,-0.0118,-0.0619,0.0751,0.0075,-0.026,-0.0332,-0.0671,-0.2399,0.0309,-0.0542,0.0232,-0.0325,0.0181,-0.0455,-0.0541,0.045,-0.0081,0.0534,0.0106,-0.0082,-0.0174,-0.0211,-0.0036,-0.0698,-0.0174,-0.02,-0.0476,-0.0196,0.0397,-0.0114,-0.0028,0.0599,0.0249,0.0192,0.0565,0.0404,0.052,0.0479,0.0156,0.0346,-0.1835,0.0228,0.0483,0.0709,-0.071,-0.0155,0.0185,0.026,0.005,0.0648,0.0194,0.0389,-0.001,-0.0045,0.0602,-0.0265,-0.0138,0.0214,0.0447,-0.019,-0.0161,-0.0343,-0.0057,-0.0689,0.0384,-0.0545,0.0316,-0.0236,-0.025,0.0253,-0.0529,0.0147,-0.0443,-0.0119,0.0538,-0.007,-0.0362,0.2132,-0.0668,0.0548,0.0348,0.0001,0.0041,0.0176,-0.0086,0.0068,-0.014,0.0165,0.0105,-0.0162,0.0487,-0.0831,0.0179,-0.0245,0.0505,0.0624,0.021,-0.0046,-0.0234,-0.0085,0.0384,-0.0147,-0.0032,-0.043,0.0062,0.1479,0.0066,0.0256,0.0425,-0.015,-0.0637,-0.0459,0.02,0.0086,0.0126,-0.0036,0.031,0.0411,-0.0606,-0.0055,0.0159,-0.0943,-0.0456,0.1522,-0.0363,-0.0036,-0.0397,-0.0245,-0.011,0.0495,0.0001,-0.0354,0.0227,0.0186,-0.0003,0.0285,-0.0392,0.0265,-0.0394,-0.0798,-0.0485,0.0858,-0.0003,-0.0835,-0.007,0.0338,0.0463,-0.0018,0.041,0.0297,-0.0231,-0.0105,0.0396,0.0298,-0.0364,-0.0058,-0.0297,0.0461,0.0248,-0.0183,-0.038,0.0559,0.0089,-0.0336,0.0354,-0.0485,-0.0283,0.0454,0.0077,0.0659,-0.0558,-0.0022,0.016,-0.0102,-0.0466,0.0224,0.0281,-0.0301,0.0017,-0.0024,-0.042,0.0175,-0.0306,0.0484,-0.008,0.0048,0.0212,-0.0095,0.012,-0.021,0.0981,0.0014,-0.0316,0.0503,0.0193,0.0405,0.0138,0.0365,0.0231,-0.0787,-0.0474,-0.2444,0.0218,0.0569,-0.0508,0.0655,-0.0694,0.048,-0.0138,0.0273,0.12,0.0808,-0.0107,-0.0134,0.0294,0.0039,0.0378,0.0245,0.0157,-0.0235,-0.028,0.0111,-0.0443,-0.0456,-0.0694,0.0711,0.0015,0.1865,0.0191,0.0674,-0.0236,0.0051,0.0237,0.0082,-0.0886,0.0796,-0.0195,0.0719,-0.003,-0.0703,-0.0102,-0.0264,0.0295,0.0485,-0.0954,-0.0531,-0.0413,-0.0323,0.0405,-0.0393,0.0462,0.0142,-0.0647,0.0643,0.0195,-0.0048,-0.0197,-0.0846,0.0509,-0.0516,0.0275,-0.0016,-0.0602,0.0015,-0.0609,0.0208,-0.0043,-0.0239,-0.0674,0.0261,-0.0226,0.0097,0.0848,-0.0448,0.0243,0.0777,0.0189,0.0282,-0.0105,-0.0334,-0.025,0.088,-0.0572,0.0199,-0.0069,0.0399,-0.018,0.0515,-0.0038,0.0515,-0.0213,-0.0105,0.0168,-0.0698,-0.0203,0.0245,0.0472,-0.296,-0.0021,-0.014,0.0201,-0.02,-0.0335,0.0139,0.0138,-0.034,-0.0145,-0.014,0.0592,0.0748,-0.0422,-0.048,-0.0159,0.0983,-0.08,0.0599,-0.0441,0.0012,0.0472,0.1847,-0.0507,0.0081,-0.0051,-0.0261,0.008,0.0267,-0.0393,-0.0314,0.0311,0.0825,-0.0561,0.0101,0.0483,-0.0143,0.0316,0.0342,-0.0191,0.0121,-0.0001,-0.0422,-0.002,0.0797,-0.0324,-0.0281,-0.0655,0.0191,0.0374,-0.0232,-0.0002,-0.0376,0.0203,0.0176,-0.0325,0.01,-0.0799,-0.0284,-0.0261,0.0223,-0.0951,-0.0254,-0.0237,-0.054]}
{"key":"[SELC: Self-Ensemble Label Correction Improves Learning with Noisy Labels] Deep neural networks are prone to overfitting noisy labels, resulting in poor generalization performance. To overcome this problem, we present a simple and effective method self-ensemble label correction (SELC) to progressively correct noisy labels and refine the model. We look deeper into the memorization behavior in training with noisy labels and observe that the network outputs are reliable in the early stage. To retain this reliable knowledge, SELC uses ensemble predictions formed by an exponential moving average of network outputs to update the original noisy labels. We show that training with SELC refines the model by gradually reducing supervision from noisy labels and increasing supervision from ensemble predictions. Despite its simplicity, compared with many state-of-the-art methods, SELC obtains more promising and stable results in the presence of class-conditional, instance-dependent, and real-world label noise. The code is available at https://github.com/MacLLL/SELC.","layer":11,"vector":[-0.0135,-0.0454,0.0337,-0.0011,0.0295,0.0265,0.0349,0.0052,0.0097,-0.0197,0.0322,-0.0618,0.0717,0.0544,-0.0126,0.0201,0.032,0.0846,-0.0503,-0.047,-0.0129,-0.0502,0.0138,-0.0044,0.013,0.0151,-0.0195,-0.0636,-0.0451,-0.2878,0.0301,-0.0251,0.0277,0.0007,0.0138,-0.0354,-0.0352,0.0199,-0.0554,0.0566,0.0299,0.0307,-0.0215,-0.0915,0.0223,-0.0299,-0.0071,-0.0644,-0.0123,-0.0007,0.0208,-0.0155,-0.0303,0.0231,-0.022,0.0641,0.0652,0.0496,0.0632,0.0367,0.0346,0.0581,-0.1642,0.079,0.0433,-0.0062,-0.0513,-0.0159,-0.0078,0.0655,-0.0101,0.0588,0.0061,0.0389,0.0335,0.0084,0.0365,0.0018,-0.0051,0.0121,0.0164,0.0064,-0.0487,-0.0673,0.0122,-0.0815,0.0313,-0.0305,0.089,0.0267,-0.0184,0.0008,-0.0203,0.0691,-0.0537,-0.0115,0.0104,0.0192,-0.0575,0.2079,-0.0068,0.0185,0.0216,-0.0244,0.0342,-0.061,-0.0183,-0.0049,-0.0447,-0.0229,-0.0287,0.0037,0.0308,-0.049,0.0374,0.0259,0.1162,0.0465,-0.0244,-0.0122,0.0193,-0.0144,0.0345,0.0034,-0.0007,-0.0523,0.0094,0.1369,0.0103,0.012,0.0519,-0.0005,-0.0455,-0.0112,0.0354,0.0141,0.0133,0.0014,0.0556,-0.0484,-0.0351,0.0032,-0.0001,-0.0855,-0.068,0.1158,-0.0279,0.0354,-0.0203,-0.0342,-0.03,0.0233,-0.0333,-0.0065,0.0396,0.0323,0.0552,0.033,-0.0456,0.0078,-0.0268,-0.0404,-0.0039,0.0983,0.0127,-0.0416,-0.0322,-0.037,0.0036,-0.0563,0.0216,0.0453,-0.0142,0.0226,0.0491,0.0677,-0.0628,0.0129,-0.0111,0.0008,0.0182,-0.0464,-0.0447,0.0411,0.0489,-0.0463,-0.019,-0.0959,0.0407,0.0853,-0.0268,0.0571,0.0107,-0.0351,-0.0186,-0.0393,-0.0009,0.0107,0.0284,-0.0165,-0.0221,0.0347,0.0031,0.0104,0.014,0.018,-0.0215,0.0001,0.0364,-0.0172,-0.0372,0.0021,0.0286,-0.042,-0.0552,-0.0092,-0.0045,0.0622,-0.0006,0.0175,0.0709,-0.0203,-0.0576,-0.232,0.0414,0.0534,-0.0584,0.0493,-0.0913,0.0617,0.0303,0.0669,0.0834,0.0167,-0.0075,-0.0081,-0.0027,-0.042,0.0517,0.0366,0.0317,-0.0072,0.0144,-0.0153,0.0349,-0.0081,-0.0502,0.0621,-0.0202,0.196,0.014,0.0298,-0.0261,0.02,0.0307,-0.0162,-0.1069,0.0882,0.0458,0.0769,-0.0256,-0.0581,-0.0496,0.0055,0.0388,-0.0037,-0.1515,-0.0683,-0.0325,-0.0403,-0.0287,-0.0595,0.0069,0.0357,-0.0469,0.0486,0.0171,-0.0007,-0.0182,-0.095,-0.008,-0.0668,-0.004,-0.0341,-0.0411,0.0333,-0.0465,0.0346,0.0299,-0.0287,-0.0731,0.0462,-0.0174,-0.0511,0.0615,-0.0082,-0.0137,0.0664,-0.0084,-0.0102,-0.0551,-0.0578,-0.0256,0.0475,0.009,0.0451,0.0217,0.0366,0.0469,0.0711,0.0021,0.0156,-0.002,0.0085,0.0048,-0.0387,0.0053,0.0768,-0.0524,-0.2543,0.0353,-0.0185,0.0618,-0.0165,0.021,0.0315,0.0373,-0.0431,-0.0031,-0.0594,0.0246,0.061,-0.0112,-0.0178,0.049,0.0646,-0.0653,0.0766,-0.0419,0.0177,0.0362,0.1775,-0.0149,0.0045,0.0195,-0.0669,-0.004,0.051,-0.0057,0.0096,-0.0077,0.0555,-0.0288,0.0342,0.0466,-0.0403,-0.0759,-0.013,0.0246,0.0084,-0.0148,-0.0462,-0.043,0.0978,0.0131,0.0064,-0.0486,0.0005,0.0211,-0.0184,-0.0178,-0.0131,0.0017,-0.0166,0.0309,-0.0536,-0.063,-0.0399,-0.0282,0.0137,-0.0591,-0.0096,-0.02,-0.041]}
{"key":"[Small-footprint Deep Neural Networks with Highway Connections for Speech Recognition] For speech recognition, deep neural networks (DNNs) have significantly improved the recognition accuracy in most of benchmark datasets and application domains. However, compared to the conventional Gaussian mixture models, DNN-based acoustic models usually have much larger number of model parameters, making it challenging for their applications in resource constrained platforms, e.g., mobile devices. In this paper, we study the application of the recently proposed highway network to train small-footprint DNNs, which are {\\it thinner} and {\\it deeper}, and have significantly smaller number of model parameters compared to conventional DNNs. We investigated this approach on the AMI meeting speech transcription corpus which has around 70 hours of audio data. The highway neural networks constantly outperformed their plain DNN counterparts, and the number of model parameters can be reduced significantly without sacrificing the recognition accuracy.","layer":6,"vector":[-0.0229,-0.0167,0.028,-0.0122,0.0257,0.0451,-0.0148,-0.0178,0.0108,-0.0478,0.0011,-0.0166,0.0592,0.0617,0.0237,0.019,0.0476,-0.0019,-0.0291,0.0116,-0.002,0.012,-0.0038,-0.034,0.0211,0.0094,-0.0241,-0.0047,-0.0232,-0.2571,0.0196,-0.0313,0.0736,-0.0229,-0.036,-0.0239,-0.0326,0.0419,-0.0101,0.0073,0.0565,0.0353,0.0259,-0.0592,-0.0217,-0.0354,-0.0167,-0.018,-0.0176,-0.0526,0.0027,-0.0527,0.0227,-0.0201,-0.0017,0.0539,0.0306,0.0491,0.0294,0.0414,-0.0164,0.0409,-0.2289,0.0572,0.0334,0.0384,-0.0326,-0.0219,0.0278,0.0278,0.0049,0.0523,0.0233,-0.009,0.0144,-0.0095,0.0103,-0.0044,0.0184,-0.0079,0.0138,-0.024,-0.0326,-0.0423,0.0103,-0.0614,0.0256,-0.0137,-0.0335,-0.022,-0.0701,0.0016,-0.0256,0.0446,-0.0492,-0.0421,0.0315,0.003,-0.0473,0.2329,-0.033,0.0531,-0.014,-0.0616,-0.0036,0.0103,-0.0509,-0.0346,-0.0434,0.0471,0.0347,-0.0147,-0.0034,-0.0235,0.0319,-0.0005,0.0709,0.0564,-0.0258,0.004,-0.0186,-0.0164,0.0164,-0.0072,0.0402,-0.0607,0.0365,0.1008,0.0566,0.0592,0.0537,-0.0147,-0.0351,-0.0164,0.0676,-0.0031,0.0379,0.0189,-0.0112,-0.005,-0.0232,-0.0827,-0.007,-0.0494,-0.0443,0.1082,-0.0564,-0.0076,-0.0431,-0.0556,0.0222,0.0035,-0.0306,-0.0502,0.0246,0.0141,0.0808,0.0277,-0.029,0.0554,0.0249,-0.0487,-0.0112,0.0688,0.0214,-0.1013,-0.06,-0.0168,0.0174,-0.0381,0.0268,0.0099,-0.0032,0.025,0.064,0.0479,-0.0727,0.0112,-0.0717,0.0295,-0.0272,-0.0635,-0.0553,0.0556,0.0241,-0.0092,0.0059,-0.0702,0.0156,0.0779,-0.0599,0.0518,-0.0297,-0.009,-0.0568,-0.0442,-0.0167,0.016,0.0235,-0.0401,0.0192,-0.0088,-0.0182,0.0247,0.0009,0.0049,-0.0135,0.0228,0.0105,0.0076,-0.0351,-0.0023,0.1047,-0.0295,-0.0068,-0.016,0.0166,0.0064,-0.0076,0.0454,0.0299,-0.0545,-0.0406,-0.228,0.0138,0.0141,-0.004,0.0925,-0.0616,0.0397,0.0503,0.0603,0.0628,0.0398,0.002,0.01,0.0371,-0.0388,0.0484,0.0231,0.0312,-0.0273,0.0324,0.0185,0.0331,-0.0784,-0.0896,0.071,0.0264,0.2065,-0.0386,0.0564,-0.0286,0.0101,0.0477,-0.042,-0.1176,0.0425,-0.009,0.1003,-0.0089,-0.0238,-0.0347,-0.0653,0.0121,-0.0022,-0.0835,-0.0479,-0.002,-0.0824,0.0084,-0.051,-0.002,0.0213,-0.0307,0.0655,0.0113,0.009,-0.0261,-0.0726,0.0275,-0.0559,0.0006,0.0063,-0.0479,-0.0293,-0.0353,0.0416,0.0169,-0.0567,-0.0476,0.0035,0.0251,-0.0173,0.0729,0.0063,0.0155,0.0594,-0.0293,0.0249,-0.0698,-0.0256,-0.0519,0.0892,-0.002,0.0767,0.0369,0.004,0.0679,0.0907,-0.0044,0.0391,-0.0059,0.0356,0.0116,0.0295,-0.0185,0.055,-0.0127,-0.2805,0.0452,-0.0157,0.0287,-0.048,0.0278,-0.0024,0.0281,-0.0533,0.0192,0.0111,0.0511,0.0588,-0.0618,0.0437,0.0652,0.0911,-0.0193,0.0226,-0.0481,-0.0142,0.0353,0.1769,-0.0473,0.0488,0.0034,-0.0629,-0.001,0.0453,-0.0424,-0.0095,0.0062,0.1081,-0.0678,0.0126,0.045,-0.0601,0.0206,0.0442,0.0283,0.0165,-0.012,-0.0483,-0.0189,0.0522,-0.0312,0.0165,-0.0465,0.0036,0.0255,-0.01,0.0084,-0.0105,-0.0258,0.0492,0.0413,0.002,-0.0394,-0.0581,-0.0397,0.0399,-0.0693,-0.028,-0.026,0.0046]}
{"key":"[Complex Network based Supervised Keyword Extractor] In this paper, we present a supervised framework for automatic keyword extraction from single document. We model the text as complex network, and construct the feature set by extracting select node properties from it. Several node properties have been exploited by unsupervised, graph-based keyword extraction methods to discriminate keywords from non-keywords. We exploit the complex interplay of node properties to design a supervised keyword extraction method. The training set is created from the feature set by assigning a label to each candidate keyword depending on whether the candidate is listed as a gold-standard keyword or not. Since the number of keywords in a document is much less than non-keywords, the curated training set is naturally imbalanced. We train a binary classifier to predict keywords after balancing the training set. The model is trained using two public datasets from scientific domain and tested using three unseen scientific corpora and one news corpus. Comparative study of the results with several recent keyword and keyphrase extraction methods establishes that the proposed method performs better in most cases. This substantiates our claim that graph-theoretic properties of words are effective discriminators between keywords and non-keywords. We support our argument by showing that the improved performance of the proposed method is statistically significant for all datasets. We also evaluate the effectiveness of the pre-trained model on Hindi and Assamese language documents. We observe that the model performs equally well for the cross-language text even though it was trained only on English language documents. This shows that the proposed method is independent of the domain, collection, and language of the training corpora.","layer":1,"vector":[-0.0257,0.0072,-0.0415,0.0061,0.0264,0.0064,0.0655,0.0471,0.0035,-0.051,0.0029,-0.0117,0.0782,0.0421,0.0562,-0.0085,0.0393,0.0254,-0.023,0.0005,0.083,0.0182,0.0096,-0.0331,0.0393,0.0029,-0.0343,-0.0275,-0.0524,-0.2299,-0.0054,-0.0549,0.0586,0.0125,-0.0024,-0.0222,-0.0192,0.0479,-0.0258,0.0562,0.0149,-0.0106,-0.0043,-0.0149,-0.0301,-0.033,-0.0446,-0.0341,-0.032,-0.0553,-0.0157,-0.028,0.0117,0.0457,0.0286,0.0129,0.0404,0.0115,0.0065,0.0773,0.0455,0.0718,-0.1813,0.0611,0.052,0.0203,-0.0481,-0.0171,0.0252,0.0864,0.0328,0.0438,0.0269,0.018,-0.0186,0.0212,-0.025,-0.0175,-0.0453,0.0002,-0.0251,-0.015,-0.0448,0.006,-0.0202,-0.0179,0.0063,-0.0417,0.017,0.0085,-0.0439,-0.0492,-0.0093,0.0167,-0.0451,-0.0188,0.0325,-0.0058,-0.0272,0.2037,-0.0416,0.036,0.0133,-0.0943,-0.006,-0.0599,-0.0088,-0.0399,-0.021,-0.0081,0.0042,-0.0027,-0.0069,-0.0338,0.0522,-0.0057,0.0947,0.0439,-0.0036,0.0119,-0.042,-0.0144,0.0366,-0.0398,0.0113,-0.0469,0.0311,0.0929,0.0547,-0.0211,0.0633,0.0183,-0.0549,-0.0111,0.0247,0.0369,0.0036,0.0254,0.011,-0.0013,-0.0041,-0.0715,-0.0094,-0.0687,-0.0968,0.1247,-0.0334,0.0086,-0.0261,-0.0345,-0.0356,0.0062,-0.0151,-0.0235,0.0224,0.0297,0.0698,0.0234,-0.0702,0.0378,0.0223,-0.0393,-0.0412,0.1026,0.0276,-0.1356,-0.0582,0.0012,-0.049,-0.0555,0.0709,0.0208,-0.0301,0.0539,0.0536,0.0322,-0.0467,-0.0197,-0.0029,0.0083,0.022,-0.0358,-0.058,0.1,0.0625,-0.0312,-0.0057,-0.0053,0.0176,0.012,-0.0329,0.0702,-0.0285,-0.0107,-0.0506,-0.0314,-0.0253,-0.0341,0.0053,-0.0839,0.0165,0.012,-0.006,-0.0176,-0.0149,0.0167,0.0052,-0.0064,0.0031,0.0366,-0.0361,-0.0042,0.0503,-0.0538,-0.0315,-0.027,0.0646,0.0688,0.0028,0.0398,-0.0027,-0.0615,-0.0369,-0.192,0.0294,-0.0032,-0.0314,0.0775,-0.0218,0.0553,-0.0162,0.0733,0.079,0.0318,-0.0172,-0.0279,0.0391,0.0237,0.0388,0.0495,0.0208,-0.001,0.0087,0.0501,-0.0151,-0.0074,-0.0782,0.0447,0.028,0.2055,0.0153,0.0401,-0.034,0.0067,0.023,-0.0586,-0.1544,0.0498,0.043,0.0139,-0.0144,-0.082,0.0005,0.0174,0.0408,-0.0132,-0.0747,-0.0368,-0.0254,-0.0424,0.0025,-0.0732,0.0395,0.0406,0.0186,0.0495,0.0681,-0.0572,-0.0447,-0.0498,0.0347,-0.0598,-0.0062,0.0009,-0.0383,0.0081,-0.0513,0.0511,0.0225,-0.0369,0.0089,-0.0059,-0.043,-0.0569,0.0969,0.0048,0.004,0.0369,0.0351,0.0184,-0.0656,-0.0181,-0.0206,0.0757,-0.0413,0.0816,0.0134,-0.004,0.0372,0.099,-0.0305,0.0427,-0.0199,0.0106,0.0623,-0.0061,0.0002,0.028,0.0114,-0.2968,0.0536,0.0032,0.0372,0.0022,0.0229,0.0264,0.0353,-0.0255,0.0589,-0.0092,0.0619,-0.0075,-0.0917,-0.0403,0.0376,0.062,0.0191,0.0188,-0.0005,0.0287,0.0248,0.2123,-0.0261,0.0469,-0.013,0.0022,-0.0067,-0.0171,-0.023,0.0054,-0.024,0.0875,0.0169,0.0311,0.0877,0.0167,0.0335,-0.0145,0.0072,-0.0064,0.0034,-0.0693,-0.0766,0.0563,-0.0426,-0.0237,-0.068,-0.0215,0.0272,-0.0819,-0.0156,-0.0249,0.0206,0.0213,0.0216,-0.0074,-0.0481,-0.067,-0.0304,0.0044,-0.0951,-0.0273,0.0129,-0.0282]}
{"key":"[CollaGAN : Collaborative GAN for Missing Image Data Imputation] In many applications requiring multiple inputs to obtain a desired output, if any of the input data is missing, it often introduces large amounts of bias. Although many techniques have been developed for imputing missing data, the image imputation is still difficult due to complicated nature of natural images. To address this problem, here we proposed a novel framework for missing image data imputation, called Collaborative Generative Adversarial Network (CollaGAN). CollaGAN converts an image imputation problem to a multi-domain images-to-image translation task so that a single generator and discriminator network can successfully estimate the missing data using the remaining clean data set. We demonstrate that CollaGAN produces the images with a higher visual quality compared to the existing competing approaches in various image imputation tasks.","layer":0,"vector":[-0.013,-0.0329,0.0386,-0.0316,0.0413,0.0561,0.0187,-0.0244,-0.0151,0.0058,0.0259,-0.0818,0.0532,0.0697,-0.0081,0.0168,0.0185,0.0237,-0.0439,0.0225,0.0054,-0.0358,0.0171,-0.084,0.0162,-0.0176,-0.0249,-0.0926,-0.0447,-0.2414,0.0415,-0.0412,0.0498,-0.0149,-0.0031,-0.0342,-0.0554,0.0649,-0.0343,0.032,-0.0094,0.0394,-0.0385,-0.0548,-0.0313,-0.0159,-0.0317,-0.0335,0.0127,-0.0311,0.014,-0.0436,0.0357,0.0419,0.0268,0.0692,0.0562,0.0137,0.0378,0.0401,0.0218,0.0831,-0.144,0.0384,0.0418,-0.0028,-0.0052,-0.0365,-0.0068,0.0264,-0.0204,0.037,0.0356,0.0605,-0.0085,-0.0289,-0.0014,-0.0606,-0.0626,-0.0116,0.058,0.0147,-0.0301,0.026,0.0146,-0.0162,0.0119,-0.0397,0.0144,0.0078,-0.0004,-0.0223,-0.0452,0.0507,-0.0804,0.0121,0.0348,0.0256,-0.0838,0.261,-0.0561,0.0193,0.0972,-0.0471,0.0175,-0.0233,-0.0341,-0.0255,-0.0434,0.041,0.0076,-0.0332,0.0265,-0.0061,0.0486,-0.0302,0.0281,0.0426,-0.0105,-0.0144,-0.038,0.0183,0.0761,-0.0236,0.0329,-0.0348,0.0057,0.1322,0.0637,0.0407,0.0214,-0.0438,-0.038,-0.0046,-0.0018,-0.0181,0.016,0.0008,-0.0303,-0.0233,0.0169,-0.0451,-0.017,-0.017,-0.0102,0.0942,-0.0108,0.0407,-0.0521,-0.04,-0.0289,0.0129,-0.0622,0.0178,0.0381,0.0289,0.0457,0.042,-0.044,0.0156,-0.0191,-0.0762,-0.0868,0.0795,0.0259,-0.0864,-0.0292,0.0578,0.0031,0.0096,-0.0113,-0.0175,-0.0256,0.0318,0.0909,0.0182,-0.0555,0.0118,-0.0165,0.0148,-0.0056,-0.0466,-0.0108,0.0562,0.0049,-0.0295,0.0499,-0.0413,-0.0043,0.0426,-0.0445,0.011,-0.0423,-0.0158,-0.0236,-0.0303,-0.0361,-0.0516,0.0029,-0.0403,0.0087,-0.017,0.0198,-0.0127,-0.0113,0.048,-0.0006,-0.0266,0.0325,0.0599,0.0033,-0.0007,0.0553,-0.0174,-0.0179,-0.0023,0.0096,0.0168,-0.0309,0.018,0.0432,-0.0328,-0.0387,-0.2558,-0.0032,-0.0021,-0.0323,0.0282,-0.0938,0.0272,0.0291,0.0198,0.0688,0.0683,0.0033,-0.0088,0.0563,-0.0257,0.0399,0.023,0.045,-0.0492,-0.0485,-0.0224,0.0549,0.0178,-0.0732,0.062,-0.011,0.2238,0.0499,0.0005,-0.0103,-0.0077,0.0486,-0.0393,-0.0798,0.0162,0.0282,0.0578,-0.0124,-0.038,-0.0223,-0.012,0.0284,0.0078,-0.1074,0.0261,-0.0296,-0.0346,0.0416,-0.0714,0.0498,0.043,-0.0158,0.0301,-0.015,-0.0149,-0.0047,-0.1582,0.0535,-0.0426,0.0387,0.0209,-0.0424,-0.0025,-0.0637,0.0688,0.0095,-0.0419,-0.0512,0.0727,-0.0065,-0.0088,0.0657,-0.0308,0.0218,0.0421,0.0279,0.0426,-0.0445,-0.0569,-0.0239,0.059,0.0071,0.0486,0.0407,0.0596,0.0439,0.0672,0.0133,0.032,-0.0286,-0.0113,0.0118,-0.0133,-0.0313,0.0377,-0.0019,-0.2716,0.0238,0.0217,0.0509,-0.0313,0.0531,0.0222,0.0571,-0.0295,-0.0264,-0.0049,-0.0127,0.0491,-0.0508,0.0162,0.045,0.0483,-0.0569,0.0548,-0.0494,0.0111,0.0135,0.2101,-0.0846,-0.0182,-0.0252,-0.0064,-0.0012,0.0274,0.0027,-0.0333,0.0451,0.0703,-0.0273,0.0079,0.0997,-0.0708,0.0421,0.0082,-0.0499,-0.0608,0.0257,-0.0047,-0.0023,0.0545,0.0072,0.0107,0.0021,0.0244,0.0141,-0.0646,0.011,-0.014,0.0244,0.0228,0.0351,-0.0457,-0.0612,-0.0196,0.0013,-0.016,-0.0321,-0.0251,-0.0131,-0.0478]}
{"key":"[Callisto: Entropy based test generation and data quality assessment for Machine Learning Systems] Machine Learning (ML) has seen massive progress in the last decade and as a result, there is a pressing need for validating ML-based systems. To this end, we propose, design and evaluate CALLISTO - a novel test generation and data quality assessment framework. To the best of our knowledge, CALLISTO is the first blackbox framework to leverage the uncertainty in the prediction and systematically generate new test cases for ML classifiers. Our evaluation of CALLISTO on four real world data sets reveals thousands of errors. We also show that leveraging the uncertainty in prediction can increase the number of erroneous test cases up to a factor of 20, as compared to when no such knowledge is used for testing. CALLISTO has the capability to detect low quality data in the datasets that may contain mislabelled data. We conduct and present an extensive user study to validate the results of CALLISTO on identifying low quality data from four state-of-the-art real world datasets.","layer":1,"vector":[-0.011,0.0032,0.0383,-0.0228,0.0099,0.0425,0.031,0.0413,-0.004,-0.0646,0.0386,-0.0441,0.0275,0.0341,0.0115,0.0015,0.0358,0.0113,-0.0439,0.0049,0.0522,-0.021,-0.0095,-0.0662,0.0027,0.0348,0.0022,-0.0566,-0.0621,-0.2808,0.0171,-0.0739,0.0533,-0.0115,0.0373,-0.0141,-0.0474,0.0254,-0.0095,0.0269,-0.0047,-0.0159,-0.0124,-0.0415,-0.0092,-0.0412,-0.0014,-0.0114,-0.0604,-0.0346,-0.0127,-0.04,0.0192,0.0185,0.0592,0.0406,0.0514,0.0516,0.0281,0.0657,0.0239,0.0849,-0.1693,0.0238,0.0217,0.0298,-0.0489,-0.0108,-0.0246,0.0362,-0.007,0.0403,0.0481,0.095,-0.0193,0.0304,0.0015,-0.02,-0.0082,0.0311,-0.0173,-0.0363,-0.0349,-0.0014,0.0115,-0.0569,0.0112,0.0085,0.0302,0.0038,-0.0125,0.0052,-0.0248,0.0542,-0.0496,-0.0093,0.0195,0.0339,-0.0654,0.2076,-0.087,0.0336,0.0045,-0.0358,0.0028,-0.0533,-0.0162,-0.0806,-0.029,-0.0243,-0.0012,-0.0419,0.0206,-0.0226,-0.0054,0.0102,0.0679,0.0367,-0.0284,-0.0076,-0.0327,0.0273,0.0517,-0.0174,0.0442,-0.0469,-0.0083,0.1441,-0.025,0.0108,-0.0175,-0.0229,-0.0713,-0.0572,0.035,0.0248,0.0386,0.0527,0.05,0.006,-0.0288,-0.0633,0.0151,-0.0821,-0.0628,0.0922,-0.0579,0.0672,-0.0319,-0.0254,0.0239,0.0123,-0.0359,-0.0185,0.0102,0.0338,0.0079,0.0381,-0.0361,0.0359,0.0173,-0.0421,-0.0591,0.0676,-0.0029,-0.0698,-0.0195,0.0211,0.0238,-0.0003,0.0583,0.0169,-0.0627,0.022,0.0097,-0.0198,-0.0436,0.0014,0.0099,0.0068,0.0251,-0.0378,-0.0358,0.0736,0.0439,-0.0487,0.0193,-0.0225,0.0081,0.0523,-0.0153,0.0028,-0.0065,0.011,-0.026,-0.042,-0.0303,0.0023,0.0166,-0.0502,0.0137,0.0199,0.0003,0.0141,-0.0162,0.0859,-0.0329,-0.0158,0.0556,0.0405,0.0008,-0.0254,0.0547,-0.0113,-0.0354,-0.0281,0.0175,0.0129,-0.0154,0.0425,0.0328,-0.0403,-0.0434,-0.2326,-0.012,-0.0032,0.0042,0.0481,-0.0369,0.0435,0.0135,0.0361,0.0449,0.0536,-0.0018,-0.0492,0.0049,-0.0226,0.0252,0.0347,0.0095,-0.0712,0.0044,-0.029,0.0406,0.0292,-0.1234,0.0746,-0.0279,0.2123,-0.016,0.0395,-0.0257,0.0405,0.0225,-0.005,-0.1173,0.0855,0.0165,0.0359,0.0175,-0.0572,-0.0319,0.0117,0.0208,-0.0136,-0.1484,-0.0157,-0.0451,-0.0355,0.0317,-0.0763,0.0183,0.0322,-0.0187,0.0571,-0.0391,0.009,-0.0273,-0.0957,0.0145,0.0149,0.0422,0.0175,-0.0517,0.0616,-0.0344,-0.0064,-0.0433,-0.0127,-0.0576,0.057,-0.0282,-0.0337,0.1285,0.0047,-0.0418,0.0567,-0.0058,0.019,-0.0715,-0.0217,-0.0236,0.0613,-0.0101,0.019,0.0274,0.0106,0.0411,0.0599,0.0162,0.0663,-0.0558,-0.0141,0.0511,-0.0705,-0.0058,0.0369,0.0046,-0.2696,0.0202,-0.0074,0.0491,-0.0409,-0.0159,0.0518,-0.0231,-0.0238,0.0431,0.0425,0.0203,0.0655,-0.0439,0.0096,0.0389,0.0766,-0.0246,0.0773,-0.0537,-0.0039,0.0415,0.2445,-0.0264,0.0098,0.0301,0.0143,0.0217,0.0579,-0.0533,0.0093,-0.0168,0.0746,-0.0296,0.022,0.0744,-0.0512,0.001,0.0366,-0.0372,0.0452,0.0005,-0.0211,-0.0214,0.0667,-0.005,0.0035,-0.0598,0.0115,0.0297,-0.0143,-0.0118,-0.0182,-0.0066,0.0204,0.0201,-0.0162,-0.0263,-0.0434,-0.0587,0.0179,-0.06,0.0285,-0.0119,-0.0035]}
{"key":"[Shared Experience Actor-Critic for Multi-Agent Reinforcement Learning] Exploration in multi-agent reinforcement learning is a challenging problem, especially in environments with sparse rewards. We propose a general method for efficient exploration by sharing experience amongst agents. Our proposed algorithm, called Shared Experience Actor-Critic (SEAC), applies experience sharing in an actor-critic framework. We evaluate SEAC in a collection of sparse-reward multi-agent environments and find that it consistently outperforms two baselines and two state-of-the-art algorithms by learning in fewer steps and converging to higher returns. In some harder environments, experience sharing makes the difference between learning to solve the task and not learning at all.","layer":6,"vector":[-0.0495,0.018,0.0214,0.0061,-0.0344,0.0424,0.0326,0.0332,0.0496,-0.0,0.0116,-0.0686,0.0596,0.0791,0.0159,0.002,-0.0699,0.0531,-0.0285,-0.04,0.0247,-0.0305,0.0117,-0.0715,0.0085,0.0255,-0.0463,-0.0796,-0.0347,-0.2028,0.0251,-0.0165,0.0032,-0.0038,-0.0231,0.0022,-0.0217,0.0631,-0.0336,0.0484,0.0439,0.0285,-0.0065,-0.0453,-0.03,-0.0669,0.0034,-0.0623,-0.0133,-0.0371,0.0439,-0.0366,0.0126,0.0005,0.0113,0.0382,0.0144,0.1099,0.0339,0.0137,0.0214,0.0334,-0.1564,0.0544,0.077,0.0959,-0.0458,-0.0084,0.0385,0.022,-0.0306,0.0611,0.0216,0.0267,0.0493,0.0068,-0.0091,-0.0316,0.0203,-0.0087,-0.0001,-0.0394,-0.0502,-0.0226,-0.016,-0.0879,0.0177,-0.0237,0.0403,0.0281,-0.0189,0.0254,0.0187,0.0274,-0.0458,-0.008,0.015,0.0424,-0.0817,0.217,-0.0271,0.0596,0.0213,0.0093,0.0084,-0.0504,0.0105,-0.009,-0.0004,-0.0049,-0.0592,-0.0108,0.0414,-0.0243,0.0089,0.0327,0.0531,0.0477,-0.0056,-0.0176,0.017,0.0154,0.0508,-0.0232,0.0207,-0.0724,0.0155,0.1559,0.0089,0.0224,0.0516,-0.0353,0.0094,-0.0088,-0.0057,0.0028,0.018,0.0012,0.0162,-0.0027,-0.0054,-0.0137,0.0048,-0.1327,-0.0713,0.0775,0.0342,0.0271,-0.0468,-0.0129,-0.0676,-0.0171,0.0081,-0.0033,-0.005,0.0072,0.0455,0.0381,-0.0836,0.052,-0.0286,-0.0514,-0.0041,0.0999,-0.0035,-0.0815,-0.0423,-0.0137,0.0212,0.0192,0.0043,0.0215,-0.084,0.0458,0.0606,0.0179,-0.0643,0.0296,-0.0019,-0.0725,0.047,-0.067,-0.0252,0.027,0.0319,-0.0205,-0.0082,-0.0425,-0.0103,0.0066,-0.0116,0.021,-0.0354,0.039,-0.0216,-0.0249,0.0157,-0.0408,-0.0103,-0.0332,0.017,-0.0248,-0.061,0.0025,-0.0208,0.0313,-0.0272,-0.0036,0.032,0.0224,-0.0528,0.0002,0.0397,-0.0176,-0.0816,-0.0036,0.0487,0.0382,-0.0224,0.0433,0.028,0.034,-0.0322,-0.2325,-0.0128,-0.0204,-0.0546,0.0277,-0.0304,0.0227,-0.0477,0.0076,0.0435,0.0962,-0.0709,-0.0255,0.02,0.0115,0.0543,0.0548,0.0513,-0.0018,0.0484,0.0329,-0.0099,-0.0045,-0.0948,0.0714,0.0171,0.2303,0.0378,-0.0017,-0.0114,0.019,0.0317,-0.0354,-0.123,0.0082,0.0152,0.0939,-0.0443,0.003,-0.0546,-0.0121,0.0226,-0.0193,-0.1047,-0.0118,-0.0418,-0.0479,0.0445,-0.0548,-0.0052,0.0424,-0.0266,0.004,-0.0312,-0.0529,-0.0326,-0.0715,0.034,-0.0355,0.0764,0.0025,-0.0217,0.022,-0.0685,0.0664,0.0048,0.0275,-0.0448,0.0392,-0.0097,-0.0378,0.0384,-0.0551,-0.0069,-0.0279,0.0052,0.0171,-0.0171,-0.0032,-0.0099,0.0594,-0.0411,0.0201,0.0134,0.0327,-0.0254,0.0431,0.0015,0.0265,-0.0546,0.0243,-0.0293,-0.0635,0.0082,0.0572,-0.0347,-0.3024,0.067,0.0492,0.0327,-0.0385,0.0299,0.0317,-0.0152,-0.0619,-0.0038,0.0389,0.0394,0.0201,0.0366,0.0252,0.0396,0.0704,-0.0113,0.043,-0.0672,0.0276,0.0756,0.2174,-0.0314,0.0407,0.0152,-0.0167,-0.0132,0.0171,-0.0213,-0.0027,0.0046,0.0993,-0.0807,0.0496,0.0945,-0.0449,0.0181,0.0097,0.0059,-0.0302,0.0077,0.0181,-0.0301,0.1002,-0.0054,-0.0031,-0.0444,-0.0568,0.0241,0.0132,0.033,0.0086,-0.0134,0.0555,-0.0154,-0.0358,-0.0466,-0.0279,-0.0242,-0.0205,-0.0365,0.0064,-0.0018,0.016]}
{"key":"[A Deep Embedded Refined Clustering Approach for Breast Cancer Distinction based on DNA Methylation] Epigenetic alterations have an important role in the development of several types of cancer. Epigenetic studies generate a large amount of data, which makes it essential to develop novel models capable of dealing with large-scale data. In this work, we propose a deep embedded refined clustering method for breast cancer differentiation based on DNA methylation. In concrete, the deep learning system presented here uses the levels of CpG island methylation between 0 and 1. The proposed approach is composed of two main stages. The first stage consists in the dimensionality reduction of the methylation data based on an autoencoder. The second stage is a clustering algorithm based on the soft-assignment of the latent space provided by the autoencoder. The whole method is optimized through a weighted loss function composed of two terms: reconstruction and classification terms. To the best of the authors' knowledge, no previous studies have focused on the dimensionality reduction algorithms linked to classification trained end-to-end for DNA methylation analysis. The proposed method achieves an unsupervised clustering accuracy of 0.9927 and an error rate (%) of 0.73 on 137 breast tissue samples. After a second test of the deep-learning-based method using a different methylation database, an accuracy of 0.9343 and an error rate (%) of 6.57 on 45 breast tissue samples is obtained. Based on these results, the proposed algorithm outperforms other state-of-the-art methods evaluated under the same conditions for breast cancer classification based on DNA methylation data.","layer":0,"vector":[-0.0286,-0.0154,0.0151,0.0044,0.0246,0.0668,0.0381,0.0031,0.0304,0.0019,0.0295,-0.0592,0.0288,-0.0063,0.0351,0.0315,0.009,0.0678,-0.0573,-0.0123,0.0128,-0.0204,-0.0142,-0.0061,0.0153,-0.0272,-0.0212,-0.0339,-0.09,-0.2541,0.0472,-0.0128,0.0536,-0.0109,0.0043,-0.0701,-0.0298,0.0222,-0.0433,0.0435,0.021,0.0099,-0.0027,-0.0013,0.0013,0.0067,-0.0473,-0.0294,-0.0109,-0.0393,-0.0106,-0.0248,-0.0203,0.0688,0.006,0.0534,0.0649,0.0191,0.0394,0.0436,0.0672,0.0492,-0.1379,0.064,0.0775,0.0049,-0.0215,-0.0361,0.089,0.0232,-0.0505,0.0721,0.0407,0.0332,0.0413,0.0259,0.0301,-0.0028,-0.0136,0.0264,0.006,0.0013,-0.0089,-0.0587,0.0085,-0.0495,0.053,-0.09,0.0275,0.0171,-0.0016,-0.0329,-0.0351,0.0088,-0.0753,-0.08,0.0469,0.0325,-0.019,0.1677,-0.0658,0.0245,0.0068,-0.0523,-0.0039,-0.0653,-0.0207,0.0068,0.0027,0.0095,-0.0044,0.0317,0.0242,-0.0589,-0.0094,0.0307,0.062,0.0147,-0.017,-0.007,-0.0263,-0.0277,0.0345,0.0255,0.0521,-0.0123,0.0238,0.1403,0.0485,0.0371,0.049,-0.001,-0.0627,0.0211,-0.016,0.0184,-0.0291,-0.0139,0.0172,0.0007,-0.0453,-0.0946,0.023,-0.101,-0.0285,0.0997,-0.0673,0.0227,-0.0226,-0.0008,-0.0108,-0.0085,-0.1027,-0.0309,0.0363,0.0264,0.0145,0.0056,-0.0758,-0.0023,0.0081,-0.0352,-0.04,0.1422,0.0295,-0.0997,-0.0637,-0.0459,0.0215,-0.0119,0.0502,0.0423,-0.0192,0.059,0.0811,-0.0028,-0.0093,0.0165,-0.0049,0.0202,0.0472,-0.0361,-0.0543,0.0351,0.0692,-0.0669,-0.0195,-0.0356,0.0311,0.0129,-0.0013,0.0137,-0.0271,-0.0022,-0.0179,-0.0605,-0.0002,-0.0012,0.0056,-0.0085,0.0405,0.0436,-0.0167,0.0412,-0.0152,0.0315,-0.0158,-0.0245,0.0688,0.0386,-0.0417,0.025,0.0685,-0.0765,0.0096,0.0114,0.0245,0.025,-0.0035,0.0482,0.0684,-0.0648,-0.1054,-0.2122,-0.023,0.0012,-0.0702,-0.0117,-0.0722,0.0066,0.0279,0.0652,0.0444,0.0236,0.032,-0.0289,0.0398,-0.0192,0.05,0.0789,0.04,-0.0356,-0.0021,0.0029,0.0406,-0.0004,-0.0648,0.082,0.0068,0.1894,0.0224,0.0196,0.0212,-0.0007,0.0306,-0.0174,-0.0978,0.0501,-0.0506,0.0264,0.009,-0.039,0.0371,0.004,-0.0253,-0.0241,-0.1472,-0.0552,-0.0291,-0.0067,0.0132,-0.0269,0.0097,0.0304,-0.0305,0.0385,-0.0252,0.015,-0.0199,-0.1156,0.038,-0.0826,-0.0173,0.0192,-0.0939,-0.0037,-0.0669,0.0295,0.0046,-0.0531,0.0073,-0.0048,-0.0624,-0.0044,0.0887,-0.0049,-0.0315,0.0775,-0.0064,-0.0339,0.0118,-0.0604,0.0241,0.0787,-0.0154,0.0216,0.0129,0.0254,0.0455,0.0727,0.0011,0.0227,0.0058,0.0251,0.0078,-0.0297,-0.0225,0.0133,-0.0126,-0.2511,0.0261,-0.0256,0.0441,-0.0279,-0.0302,0.0036,0.0175,-0.0422,0.0283,-0.0056,0.0051,0.0976,-0.0024,0.0158,0.031,0.0723,-0.0384,0.0764,-0.0452,0.0069,0.0078,0.2236,-0.0736,-0.0054,0.0172,-0.0259,0.0323,0.0375,-0.028,-0.0066,0.0441,0.0923,-0.0382,0.0251,0.0854,-0.0186,0.0161,0.0059,-0.0382,0.0093,0.0377,-0.0281,-0.0296,0.1071,-0.0309,-0.0384,-0.051,-0.0018,0.0608,-0.0188,0.017,-0.0336,-0.0379,0.0061,0.0386,-0.0404,-0.0686,-0.028,-0.0573,-0.0112,-0.032,-0.06,0.0302,-0.0294]}
{"key":"[Attentive cross-modal paratope prediction] Antibodies are a critical part of the immune system, having the function of directly neutralising or tagging undesirable objects (the antigens) for future destruction. Being able to predict which amino acids belong to the paratope, the region on the antibody which binds to the antigen, can facilitate antibody design and contribute to the development of personalised medicine. The suitability of deep neural networks has recently been confirmed for this task, with Parapred outperforming all prior physical models. Our contribution is twofold: first, we significantly outperform the computational efficiency of Parapred by leveraging \\`a trous convolutions and self-attention. Secondly, we implement cross-modal attention by allowing the antibody residues to attend over antigen residues. This leads to new state-of-the-art results on this task, along with insightful interpretations.","layer":0,"vector":[-0.0741,0.0005,0.0224,0.0228,0.0111,0.0526,0.098,0.0442,0.0302,-0.0198,-0.0374,-0.0756,0.0575,0.0627,0.0119,0.0006,-0.0136,0.0892,-0.0569,-0.0024,0.0053,0.0145,-0.0219,-0.0738,0.0386,-0.0007,-0.01,-0.0217,-0.0501,-0.2208,0.0241,-0.051,0.0058,-0.0445,0.0135,-0.0381,-0.01,0.0406,-0.0506,-0.0023,0.0426,0.0096,-0.0261,-0.0229,-0.0059,-0.0898,-0.0411,-0.0279,-0.0001,-0.0618,0.0416,-0.0302,0.0282,0.0578,0.0294,0.0202,0.0322,0.0289,0.0376,0.0118,0.0149,0.056,-0.1091,0.0874,0.0582,0.0679,-0.0143,-0.0285,-0.0208,0.0689,-0.0123,0.0058,0.0206,0.0319,0.0255,0.0271,-0.0318,-0.019,0.0297,-0.013,0.013,-0.0046,-0.003,-0.0176,-0.0104,-0.0293,0.0175,0.0048,0.0804,0.0176,-0.0216,-0.0421,-0.011,0.0217,-0.0487,0.0283,0.0384,0.0172,-0.0363,0.2377,-0.0372,-0.0031,-0.0019,-0.0608,0.0286,-0.0418,-0.0397,-0.0078,-0.0178,0.0195,-0.0185,-0.0131,0.0499,-0.0359,0.0295,0.0138,0.0595,0.0108,0.0237,-0.0161,-0.0157,-0.0533,0.0136,-0.0161,0.0149,-0.003,0.0107,0.1476,0.032,0.0408,0.0959,0.0029,-0.0196,-0.036,0.0077,-0.015,0.0093,0.0124,0.0349,0.0053,-0.05,-0.0681,-0.0128,-0.0908,-0.0302,0.0761,-0.0604,0.0211,-0.0476,-0.0208,-0.0166,0.03,-0.0053,-0.0176,0.0219,0.0337,-0.0168,0.0303,-0.0432,-0.0073,-0.0324,-0.0411,-0.0431,0.0835,0.0092,-0.0686,-0.0189,0.0062,0.0293,-0.0055,0.024,0.0377,-0.0548,-0.0196,0.0661,0.0258,-0.0998,-0.0124,0.0018,0.0278,0.024,-0.0645,-0.0452,0.0292,0.0406,-0.0711,0.0241,-0.0717,0.0128,0.0441,0.0082,0.0449,-0.0689,0.0138,-0.0228,-0.0117,-0.0595,-0.0207,0.0408,-0.0066,0.0207,0.0196,-0.0411,0.0122,-0.011,-0.0147,0.0177,0.0562,0.0666,0.0408,-0.0551,0.0234,0.0337,-0.0508,-0.0359,-0.0585,-0.008,0.0149,0.0243,0.0503,0.0458,-0.0474,-0.0451,-0.2529,0.011,0.0354,-0.0679,0.0428,-0.0546,0.0102,0.0169,0.0749,0.0969,0.0985,-0.0133,-0.023,-0.0027,-0.0211,0.034,0.0417,0.0139,-0.0186,0.0021,-0.0064,0.0188,-0.0128,-0.0534,0.0026,-0.029,0.2299,0.0857,0.0013,0.007,0.0294,0.0346,-0.0511,-0.0778,0.022,0.0,0.0428,-0.0232,-0.0358,-0.0398,-0.0642,-0.0023,0.0048,-0.1016,-0.0485,-0.0249,-0.0345,0.0224,-0.0074,0.026,0.0892,-0.0869,0.0145,0.0388,-0.0019,-0.0748,-0.0766,-0.0082,-0.0563,0.0429,0.0089,-0.0365,-0.0099,-0.0532,-0.0143,-0.0165,-0.0229,-0.0401,0.0348,-0.0118,-0.0251,0.0866,-0.0065,0.0513,0.0483,0.0024,0.0155,0.0113,-0.0636,0.0281,0.0658,0.0147,0.0191,0.0079,0.0428,0.0094,0.0744,-0.0297,0.0468,-0.0162,-0.0125,0.0054,-0.0717,-0.0249,0.0165,-0.0191,-0.3117,0.0785,0.0281,0.0363,0.0197,-0.0085,0.0463,-0.0357,-0.046,0.0323,-0.0034,0.0279,0.0635,-0.0031,-0.0152,0.0224,0.0856,-0.0553,0.025,-0.05,0.0339,0.0033,0.2486,-0.0185,0.0125,0.017,-0.0358,0.016,-0.0073,-0.0403,-0.0047,-0.0035,0.0703,-0.0283,0.0133,0.0629,-0.0437,0.0294,0.0145,0.0031,0.0238,0.0374,-0.0145,-0.0538,0.0968,-0.0401,-0.0453,-0.0297,-0.0045,0.0189,0.0209,0.0049,-0.0231,-0.033,0.0427,-0.0125,-0.0403,-0.0295,-0.0375,-0.0036,0.039,-0.0234,-0.024,0.0337,-0.0043]}
{"key":"[RPnet: A Deep Learning approach for robust R Peak detection in noisy ECG] Automatic detection of R-peaks in an Electrocardiogram signal is crucial in a multitude of applications including Heart Rate Variability (HRV) analysis and Cardio Vascular Disease(CVD) diagnosis. Although there have been numerous approaches that have successfully addressed the problem, there has been a notable dip in the performance of these existing detectors on ECG episodes that contain noise and HRV Irregulates. On the other hand, Deep Learning(DL) based methods have shown to be adept at modelling data that contain noise. In image to image translation, Unet is the fundamental block in many of the networks. In this work, a novel application of the Unet combined with Inception and Residual blocks is proposed to perform the extraction of R-peaks from an ECG. Furthermore, the problem formulation also robustly deals with issues of variability and sparsity of ECG R-peaks. The proposed network was trained on a database containing ECG episodes that have CVD and was tested against three traditional ECG detectors on a validation set. The model achieved an F1 score of 0.9837, which is a substantial improvement over the other beat detectors. Furthermore, the model was also evaluated on three other databases. The proposed network achieved high F1 scores across all datasets which established its generalizing capacity. Additionally, a thorough analysis of the model's performance in the presence of different levels of noise was carried out.","layer":0,"vector":[-0.0502,-0.0068,0.0354,0.0273,0.025,0.0132,0.048,-0.0154,0.0605,-0.0317,0.0189,-0.0502,-0.0015,0.0671,-0.0087,0.0196,0.0269,0.0189,-0.0186,0.046,-0.0171,0.0026,0.0373,-0.0444,0.0449,-0.0033,-0.0125,-0.0523,-0.0735,-0.2371,0.0193,-0.0306,0.0744,0.0025,0.0199,-0.0289,-0.0389,0.0212,-0.0468,0.0168,0.0005,-0.0229,-0.0234,-0.066,-0.0207,-0.0743,-0.0296,-0.0453,0.0051,-0.0041,0.0498,-0.0039,0.0512,0.0209,0.0109,0.0423,0.0732,0.0275,0.0717,0.0076,0.0154,0.0967,-0.1751,0.0364,0.0111,0.0077,-0.0549,0.0237,0.0295,0.0144,-0.0104,0.0078,-0.0064,0.0267,-0.003,0.0289,-0.0183,-0.0352,-0.0333,0.028,0.0161,-0.0069,-0.0193,-0.0453,0.0181,-0.0325,0.0259,-0.0843,0.0062,0.0231,-0.0358,-0.0204,-0.0042,0.0124,-0.0518,0.0006,0.0133,0.0249,-0.0514,0.2083,-0.0619,-0.0145,0.0108,0.0141,0.0282,-0.0319,-0.0462,-0.0271,-0.0279,0.023,-0.0011,-0.0328,0.0612,-0.0403,0.0412,0.0274,0.0364,0.0115,0.0236,0.0087,-0.066,-0.0079,0.0723,-0.0593,0.0444,-0.01,0.0261,0.1403,0.0563,0.0567,0.0341,0.0125,-0.0756,-0.0112,-0.0065,0.0204,0.0366,0.0123,0.007,0.0004,-0.0547,-0.0472,0.0522,-0.1322,-0.0833,0.078,-0.0478,0.0487,-0.0452,-0.0335,-0.0029,-0.0037,-0.0477,-0.0033,0.0383,0.0274,0.0219,0.0341,-0.0522,-0.037,-0.0413,-0.0766,-0.0392,0.0942,0.0097,-0.0778,-0.0272,-0.0489,-0.0222,-0.0178,0.0404,0.0151,0.0205,0.069,0.0738,0.0385,-0.0283,-0.005,-0.0027,0.018,0.0246,-0.0534,0.0158,-0.0073,0.0462,-0.046,0.0211,-0.055,0.076,0.0374,-0.0245,0.0145,-0.005,0.0443,-0.0113,-0.0422,-0.0158,0.0197,-0.0062,-0.0076,0.0219,-0.0423,-0.0134,0.0492,-0.0149,0.0063,-0.0139,0.0041,0.0031,0.07,0.0189,0.0142,0.0926,-0.0193,-0.0639,-0.0039,0.0191,0.0225,-0.0088,0.0399,0.0356,-0.0373,-0.0551,-0.2486,0.0004,0.0407,-0.0081,0.0679,-0.0471,0.0332,0.0048,0.0677,0.0605,0.0416,0.0162,-0.033,0.0099,0.0051,0.0693,0.0377,0.0255,-0.0415,-0.0271,-0.0097,0.0172,-0.0298,-0.0493,0.047,0.0154,0.2176,-0.011,0.0111,-0.0326,0.0118,0.0218,-0.0005,-0.1138,0.0501,0.0081,0.0494,0.0537,-0.0406,-0.055,-0.0637,0.0421,0.0182,-0.0819,-0.1033,-0.0363,-0.0318,0.0131,-0.032,0.0525,0.0259,-0.07,0.0515,0.0437,0.0371,-0.0424,-0.1261,0.0351,-0.0385,0.0055,0.0036,-0.0427,0.0054,-0.0338,0.0389,0.0319,-0.0012,-0.0156,-0.0005,-0.0427,-0.0403,0.0942,0.0071,0.023,0.0504,0.0116,0.0308,-0.022,-0.0541,-0.0192,0.0124,-0.0392,0.0167,0.0072,0.013,0.005,0.0541,0.0136,0.012,-0.0117,0.027,0.0152,-0.0102,-0.0569,0.0136,-0.014,-0.2742,-0.0124,0.0019,-0.0023,-0.0232,-0.0346,0.0119,0.018,-0.071,-0.0058,-0.0666,0.007,0.0808,-0.0353,-0.0081,0.0901,0.0534,-0.064,0.1051,-0.0363,-0.0328,0.0897,0.1983,-0.0623,0.0211,0.0468,-0.0113,0.01,0.0232,-0.0105,-0.0148,0.0445,0.0787,-0.0337,0.0028,0.0853,-0.0216,0.0561,0.0275,-0.0146,0.0254,0.025,-0.0293,-0.0328,0.0943,-0.0326,-0.0304,-0.0034,0.0187,0.0625,-0.0478,-0.0122,-0.0101,0.0376,-0.017,0.041,-0.0835,-0.0526,-0.0177,-0.0444,0.0243,-0.0694,-0.0253,0.0284,-0.0476]}
{"key":"[Provable Gaussian Embedding with One Observation] The success of machine learning methods heavily relies on having an appropriate representation for data at hand. Traditionally, machine learning approaches relied on user-defined heuristics to extract features encoding structural information about data. However, recently there has been a surge in approaches that learn how to encode the data automatically in a low dimensional space. Exponential family embedding provides a probabilistic framework for learning low-dimensional representation for various types of high-dimensional data. Though successful in practice, theoretical underpinnings for exponential family embeddings have not been established. In this paper, we study the Gaussian embedding model and develop the first theoretical results for exponential family embedding models. First, we show that, under mild condition, the embedding structure can be learned from one observation by leveraging the parameter sharing between different contexts even though the data are dependent with each other. Second, we study properties of two algorithms used for learning the embedding structure and establish convergence results for each of them. The first algorithm is based on a convex relaxation, while the other solved the non-convex formulation of the problem directly. Experiments demonstrate the effectiveness of our approach.","layer":2,"vector":[-0.0201,0.0139,0.0582,-0.0328,0.0247,0.0593,0.0329,0.0139,-0.0128,-0.0089,-0.0193,-0.0544,0.0382,0.0725,0.036,0.0479,-0.0293,0.0988,-0.0749,0.0067,0.0366,-0.078,0.0075,-0.052,0.0197,0.0667,-0.008,-0.0554,-0.0171,-0.2472,0.0197,-0.0686,0.0671,0.0226,-0.0136,-0.0577,0.0022,0.0336,-0.0236,0.0626,0.0309,0.0276,-0.0592,-0.0612,-0.0056,-0.0449,-0.0336,-0.0217,-0.0384,-0.0327,0.0489,-0.0243,0.0557,0.036,0.0162,0.0375,0.0541,0.0121,-0.0063,0.0468,0.0274,0.0475,-0.1266,0.0565,0.0546,0.0493,-0.0322,-0.0202,-0.0055,0.0478,-0.0204,0.0091,-0.0257,0.0933,0.0043,0.0203,0.029,-0.0155,-0.0527,-0.0122,0.007,-0.0246,-0.0171,0.0055,-0.0231,-0.0358,0.0297,-0.0829,0.0293,0.0075,-0.051,-0.0158,-0.0657,0.0338,-0.0678,-0.0199,0.0327,0.0422,-0.0155,0.1892,-0.0563,0.042,0.0597,-0.014,0.0394,-0.0709,-0.0106,-0.0082,-0.039,-0.0529,-0.0589,-0.0199,0.0004,-0.0248,0.0039,-0.0339,0.0296,0.023,0.0048,-0.041,-0.0135,-0.0214,0.0033,-0.0247,0.0343,-0.0764,0.0355,0.1581,0.0339,0.0602,0.0303,-0.011,-0.0515,-0.0178,-0.0052,0.0306,0.016,-0.0034,0.0133,-0.0165,-0.0073,-0.0524,0.0097,-0.0898,-0.0802,0.1514,-0.0651,0.0007,-0.056,-0.0214,0.0124,-0.0141,0.0269,-0.0176,0.0266,0.0726,0.0222,0.0073,-0.0602,0.0038,-0.0264,-0.0367,-0.0111,0.0893,0.0405,-0.0944,-0.0493,-0.0032,0.0499,-0.0275,0.0724,0.0142,-0.0436,0.0492,0.0807,0.0366,-0.0841,-0.0005,0.0586,0.0078,0.0116,-0.0726,-0.0085,0.0501,0.0503,-0.0386,0.0035,-0.0599,0.045,0.0086,-0.015,-0.0148,-0.0215,0.002,-0.0435,-0.0428,0.0113,-0.0033,0.0012,-0.0222,0.0558,0.0179,-0.0636,0.0414,-0.0506,-0.0086,0.0265,0.0579,0.0366,0.0215,-0.0032,-0.0017,0.0237,-0.0365,-0.0316,0.0292,0.0107,0.0513,0.0216,0.0271,0.0056,-0.0491,-0.0662,-0.2362,-0.0291,-0.0094,-0.0257,0.009,-0.0713,0.0528,0.0197,0.0747,0.0539,0.0384,-0.0235,-0.0363,0.0177,0.0158,0.0418,0.049,0.0504,-0.0124,0.0173,-0.015,0.0295,-0.0249,-0.0623,0.0715,-0.0251,0.215,0.0063,0.0355,-0.0265,0.0112,-0.0183,-0.0619,-0.1002,0.0245,0.0204,0.0232,0.001,-0.022,-0.0281,-0.0636,-0.0131,0.0199,-0.0976,-0.0484,-0.0471,-0.0232,0.0525,-0.0695,0.0245,0.0674,-0.0589,0.0959,-0.0291,-0.048,-0.0396,-0.0697,0.052,-0.051,0.0138,0.0432,-0.0516,0.006,-0.0602,0.0457,-0.0333,-0.0396,-0.0225,0.0062,-0.0307,-0.0305,0.0919,-0.0298,-0.0099,0.0568,0.0402,0.0663,-0.0226,-0.0752,0.0184,0.051,-0.0548,0.0335,0.0398,0.0649,0.0423,0.0669,-0.0026,-0.0006,-0.0204,-0.0016,-0.0094,-0.037,0.0161,0.0164,-0.0116,-0.2891,0.0283,0.0318,-0.0073,-0.0172,0.002,0.0074,-0.0089,-0.0432,-0.009,0.0181,0.0548,0.0927,0.0199,0.0076,0.0173,0.0471,-0.0436,0.0308,-0.0377,-0.0051,0.0341,0.2079,-0.0237,0.0599,0.0078,-0.0052,-0.032,0.0303,-0.0223,0.0076,0.0164,0.089,-0.0423,0.0304,0.0583,-0.0026,0.0232,0.0215,-0.0201,0.0346,-0.0136,-0.0582,-0.0112,0.0699,-0.003,0.0357,0.015,0.0111,0.0141,-0.0256,0.0128,0.0059,-0.0009,0.0105,0.0126,-0.0449,-0.0345,-0.0162,-0.0284,0.0154,-0.0322,-0.0547,-0.0096,-0.0024]}
{"key":"[Reliable Fleet Analytics for Edge IoT Solutions] In recent years we have witnessed a boom in Internet of Things (IoT) device deployments, which has resulted in big data and demand for low-latency communication. This shift in the demand for infrastructure is also enabling real-time decision making using artificial intelligence for IoT applications. Artificial Intelligence of Things (AIoT) is the combination of Artificial Intelligence (AI) technologies and the IoT infrastructure to provide robust and efficient operations and decision making. Edge computing is emerging to enable AIoT applications. Edge computing enables generating insights and making decisions at or near the data source, reducing the amount of data sent to the cloud or a central repository. In this paper, we propose a framework for facilitating machine learning at the edge for AIoT applications, to enable continuous delivery, deployment, and monitoring of machine learning models at the edge (Edge MLOps). The contribution is an architecture that includes services, tools, and methods for delivering fleet analytics at scale. We present a preliminary validation of the framework by performing experiments with IoT devices on a university campus's rooms. For the machine learning experiments, we forecast multivariate time series for predicting air quality in the respective rooms by using the models deployed in respective edge devices. By these experiments, we validate the proposed fleet analytics framework for efficiency and robustness.","layer":1,"vector":[0.001,-0.0094,0.0769,-0.0177,0.0567,0.013,0.0654,0.0706,0.0213,-0.0205,0.0074,-0.0282,0.0004,0.0077,0.0443,0.0247,-0.0234,-0.0034,-0.0337,-0.0126,0.0354,-0.0127,-0.0284,-0.0699,0.0389,0.0544,0.0105,-0.0401,-0.077,-0.1859,-0.0106,-0.0649,0.0694,-0.0046,0.0309,-0.0489,0.0233,0.039,0.009,0.0316,0.0454,-0.048,-0.0393,-0.0368,-0.0424,-0.0319,-0.0061,-0.019,-0.0475,-0.0555,0.0537,-0.0202,0.0024,0.0028,0.0235,0.0316,0.0265,-0.0019,0.06,-0.0017,-0.0023,0.056,-0.1813,0.0724,0.0389,-0.0044,-0.0424,-0.0194,0.0003,0.0056,-0.0081,0.0169,0.0278,0.0593,-0.0077,0.0456,0.0325,-0.0324,0.0212,0.0337,0.0156,-0.0347,-0.0825,0.0124,-0.0253,-0.0492,-0.0209,-0.0502,0.0657,-0.0159,-0.0552,-0.0063,-0.0489,0.0304,-0.0244,-0.0028,-0.0031,-0.0148,-0.0601,0.193,-0.0597,0.0409,0.0023,-0.0275,-0.0049,-0.0465,-0.048,-0.085,-0.0448,0.0241,-0.0516,-0.0237,0.0397,-0.0092,0.0176,0.0172,0.0498,0.0679,0.0166,-0.0138,-0.0251,0.0329,0.0827,0.0089,0.04,-0.0392,0.0529,0.1347,-0.0502,0.0079,0.011,-0.0427,-0.0883,-0.0327,0.0432,0.0455,0.0168,-0.0072,-0.0054,0.0189,-0.0441,-0.0453,0.0644,-0.0945,-0.0324,0.1262,-0.0004,0.0458,-0.0031,-0.0476,-0.041,0.0313,-0.0525,-0.0316,0.0287,0.0317,-0.0145,0.0521,-0.0113,0.0395,-0.0333,-0.002,-0.039,0.0987,-0.029,-0.097,0.0365,0.0463,-0.0012,0.0288,0.0708,0.0418,-0.0541,0.033,0.0725,0.0157,-0.0511,-0.0007,-0.0184,0.0269,0.0007,-0.0386,-0.0131,0.0332,0.0461,-0.056,0.0051,-0.0459,0.0088,0.0363,-0.0539,0.0152,-0.0197,0.0359,-0.0006,-0.0385,-0.0243,0.0005,0.0306,-0.0086,0.041,-0.0116,-0.0223,0.0316,-0.0061,-0.006,-0.0518,0.0233,0.0237,-0.0008,0.0182,-0.0482,0.0664,-0.0133,-0.0247,-0.0063,0.0177,0.0336,0.0071,0.0211,0.0381,-0.003,-0.0947,-0.2339,-0.0249,0.0058,0.0184,0.0292,-0.0168,0.0132,0.0033,0.0245,0.0267,0.087,-0.0292,-0.0024,0.0159,-0.0434,0.0786,0.0319,0.0533,-0.0613,0.0171,-0.0134,0.0313,-0.0327,-0.1067,-0.0076,0.0414,0.2177,-0.036,0.0356,-0.0415,0.0394,0.0144,-0.0098,-0.0792,0.0348,0.0191,0.0766,0.0131,-0.0873,-0.0349,-0.0589,0.076,-0.0135,-0.107,-0.0358,-0.0305,-0.0366,0.0247,-0.0692,-0.0243,0.0101,-0.0273,0.0581,0.0044,-0.001,-0.0179,-0.0339,0.0625,-0.0038,0.0133,-0.016,-0.0503,0.0032,-0.0692,0.0569,-0.0175,-0.0361,-0.034,-0.0161,-0.0229,-0.0053,0.1064,-0.0255,-0.0384,0.0506,-0.0248,0.0312,-0.0089,-0.0428,-0.0184,0.0653,-0.0423,0.1031,0.0431,0.0434,0.0497,0.0443,-0.0206,0.0229,-0.0257,-0.018,0.0186,-0.035,-0.0354,0.0521,0.0117,-0.3123,0.0381,0.0108,0.063,-0.02,0.0053,0.0188,0.0397,-0.0362,0.0148,0.0047,0.06,0.0366,-0.0373,0.0325,0.0239,0.0833,-0.0802,0.0205,-0.0164,0.0457,0.029,0.2347,-0.0389,0.0389,0.027,-0.0152,0.0055,0.0279,-0.0253,-0.0057,-0.0255,0.0834,-0.0782,0.0029,0.0754,-0.0272,0.0251,0.0248,0.0043,0.017,0.0061,-0.0071,0.0063,0.0677,0.0062,-0.0323,-0.0604,0.0244,0.0394,-0.0269,-0.028,-0.0265,-0.0169,0.0172,0.0399,-0.0532,-0.0309,-0.0491,-0.0471,0.0271,-0.0875,0.0066,0.0004,0.0134]}
{"key":"[Location reference identification from tweets during emergencies: A deep learning approach] Twitter is recently being used during crises to communicate with officials and provide rescue and relief operation in real time. The geographical location information of the event, as well as users, are vitally important in such scenarios. The identification of geographic location is one of the challenging tasks as the location information fields, such as user location and place name of tweets are not reliable. The extraction of location information from tweet text is difficult as it contains a lot of non-standard English, grammatical errors, spelling mistakes, non-standard abbreviations, and so on. This research aims to extract location words used in the tweet using a Convolutional Neural Network (CNN) based model. We achieved the exact matching score of 0.929, Hamming loss of 0.002, and $F_1$-score of 0.96 for the tweets related to the earthquake. Our model was able to extract even three- to four-word long location references which is also evident from the exact matching score of over 92\\%. The findings of this paper can help in early event localization, emergency situations, real-time road traffic management, localized advertisement, and in various location-based services.","layer":0,"vector":[-0.0158,-0.0173,0.0237,-0.0106,0.0611,0.0212,0.0686,0.0456,0.0317,-0.0397,-0.0096,-0.0488,0.0393,0.0538,0.0117,0.0175,0.0215,0.0008,-0.0218,-0.0325,0.0572,-0.029,0.0147,-0.0394,0.0639,-0.0143,-0.0105,-0.0017,-0.0553,-0.1957,-0.0061,-0.0244,0.0717,-0.0161,-0.0437,0.0019,-0.039,0.0621,0.0153,0.0465,0.0452,-0.0212,-0.0087,-0.0567,-0.0233,-0.0524,0.0004,-0.0295,-0.0211,-0.0536,0.0233,-0.0328,0.0355,0.0178,0.0252,0.0165,0.0619,0.0365,0.0306,0.02,0.087,0.0332,-0.2119,0.0641,0.0194,-0.0157,-0.0175,0.0278,0.0282,0.027,-0.0323,0.0646,0.0186,0.073,-0.0031,-0.0092,0.0226,-0.0066,0.0008,-0.0123,0.0342,-0.022,-0.0147,-0.011,-0.0107,-0.0557,0.0184,-0.0205,0.009,0.0057,-0.0573,-0.0207,-0.0138,0.0728,-0.0718,-0.0243,-0.0054,0.0392,-0.0536,0.2097,-0.0673,0.0489,0.016,-0.0034,0.0426,-0.0161,0.0051,-0.0574,-0.0227,0.0104,-0.0279,-0.0473,0.006,-0.0084,0.0641,0.0116,0.0982,0.057,-0.032,-0.023,-0.0084,0.0727,0.0239,-0.0333,0.008,-0.0255,0.0439,0.1202,0.0557,0.0246,0.0288,-0.0066,-0.0585,0.0052,0.0122,0.0323,-0.0016,-0.0126,-0.0268,-0.0467,0.0328,-0.0775,0.0181,-0.0531,-0.0374,0.1055,-0.0572,-0.0316,-0.0454,-0.0519,-0.018,0.0128,-0.0614,0.0133,0.0328,0.0329,0.0461,0.0389,-0.0621,-0.0107,0.0136,-0.0369,-0.041,0.09,-0.0257,-0.1405,-0.0484,-0.0284,0.0057,-0.0457,0.0397,0.0292,0.0022,0.0188,0.0677,0.049,-0.0479,-0.0068,-0.009,0.0326,-0.0179,-0.0611,-0.0193,0.0607,0.0314,-0.0323,-0.0136,-0.0659,0.0265,0.0627,-0.0112,0.0219,-0.0173,-0.0508,-0.0292,-0.0106,-0.0224,-0.0193,0.064,-0.0628,0.0111,-0.0152,-0.0765,-0.0132,0.0128,-0.0057,0.0063,-0.0152,-0.0079,0.0451,-0.0133,-0.0058,0.0639,-0.0672,0.008,-0.0484,0.0511,0.041,0.0151,0.0622,0.0413,-0.0558,-0.0114,-0.2391,-0.0059,0.0277,-0.007,0.0245,-0.0605,0.0049,0.0012,0.0487,0.0366,0.0896,-0.0843,0.0072,0.0126,0.0041,0.0438,0.027,0.0514,0.0317,-0.0252,0.0192,0.0168,-0.0529,-0.0898,0.0583,0.0076,0.1944,0.0138,0.0264,-0.0358,0.0287,0.0205,0.0134,-0.1581,0.0641,0.0225,0.0615,0.0157,-0.0504,-0.0309,-0.0438,0.0463,0.0459,-0.0467,0.0013,-0.0431,-0.0674,0.009,-0.0119,0.0414,0.0303,-0.0276,0.029,-0.025,0.0468,-0.0363,-0.094,0.0285,-0.0642,-0.0249,-0.0194,-0.0639,0.0089,0.0007,0.0664,0.0511,-0.0602,-0.0435,0.0031,0.0154,-0.0299,0.0984,0.0017,0.0275,0.0341,-0.0382,0.0477,-0.058,-0.026,-0.0298,0.0694,-0.0157,0.0618,0.0201,0.0113,0.004,0.074,-0.0025,0.0229,-0.0188,0.0097,0.0097,-0.0137,-0.0526,0.004,-0.0106,-0.3055,0.0717,0.0167,0.0075,-0.0334,-0.0263,0.0018,0.0251,-0.0045,-0.0072,-0.0051,0.0236,0.0466,-0.0771,-0.0179,0.0314,0.0183,-0.0423,0.0478,-0.0515,0.0151,0.0196,0.2366,-0.0242,0.0156,0.0179,-0.0369,-0.009,0.052,0.0181,0.0033,0.0044,0.0736,-0.0395,0.0362,0.0188,0.0005,0.0486,0.0216,0.0222,-0.0514,0.0622,-0.0106,-0.033,0.0664,-0.025,-0.0284,-0.0395,-0.0073,0.037,-0.0018,-0.0307,-0.0326,-0.0076,0.0404,0.0171,-0.0671,-0.0234,-0.0675,-0.0516,0.0255,-0.0611,0.0096,-0.0399,0.0148]}
{"key":"[The Symmetry of a Simple Optimization Problem in Lasso Screening] Recently dictionary screening has been proposed as an effective way to improve the computational efficiency of solving the lasso problem, which is one of the most commonly used method for learning sparse representations. To address today's ever increasing large dataset, effective screening relies on a tight region bound on the solution to the dual lasso. Typical region bounds are in the form of an intersection of a sphere and multiple half spaces. One way to tighten the region bound is using more half spaces, which however, adds to the overhead of solving the high dimensional optimization problem in lasso screening. This paper reveals the interesting property that the optimization problem only depends on the projection of features onto the subspace spanned by the normals of the half spaces. This property converts an optimization problem in high dimension to much lower dimension, and thus sheds light on reducing the computation overhead of lasso screening based on tighter region bounds.","layer":8,"vector":[-0.019,-0.0066,0.0502,-0.0133,0.0231,0.0196,0.0042,0.0576,0.0138,-0.0404,0.0049,-0.0578,0.0268,0.0685,0.0085,0.0218,0.0543,0.0541,-0.0465,0.0387,0.0269,-0.0064,-0.0431,-0.0636,0.0669,0.029,-0.0461,-0.0355,-0.0368,-0.2855,0.0245,0.0011,0.0382,-0.0182,0.0193,-0.0171,-0.0152,0.0441,-0.0611,0.0475,-0.0147,0.0008,-0.0189,-0.0403,-0.0065,-0.0305,-0.0431,-0.0251,-0.0478,-0.0122,0.02,-0.0472,-0.0312,0.0362,0.0352,0.0079,0.0071,0.0164,0.0196,0.0305,0.0149,0.0238,-0.1767,0.0343,0.1212,-0.0091,-0.0224,-0.0374,0.0174,0.0918,-0.0028,0.0314,0.0417,0.031,-0.0042,-0.0056,-0.0152,-0.007,0.0133,0.027,0.0519,-0.0001,-0.0108,0.0327,-0.0056,-0.0291,0.0196,-0.0459,0.033,-0.0148,-0.0631,-0.0131,-0.0383,0.0268,-0.0959,-0.0212,0.0579,0.0392,-0.0512,0.2226,-0.0619,0.0423,0.0221,-0.0436,0.0481,-0.0818,-0.0411,-0.0314,-0.0126,0.013,0.0318,0.0107,0.011,-0.0118,-0.0088,-0.0307,0.0534,0.0557,-0.0235,-0.0035,-0.0067,0.0146,0.057,-0.0134,0.0211,-0.0343,0.0072,0.1164,0.0533,0.0252,0.0296,-0.0247,-0.0316,-0.0052,0.0059,0.0067,-0.0129,0.04,0.0092,-0.0002,-0.047,-0.0557,0.0371,-0.0599,-0.0149,0.1404,-0.086,0.0025,-0.039,-0.0313,0.0216,0.0033,-0.0392,-0.0299,-0.0208,-0.0031,0.0492,0.0417,-0.036,0.0273,-0.0375,-0.0468,-0.0233,0.0567,0.0019,-0.08,-0.0259,-0.0202,0.0026,-0.0089,0.0555,-0.0109,-0.0381,0.0673,0.0945,0.0377,-0.0951,0.0237,-0.0174,-0.0176,0.0087,-0.0222,-0.0364,0.0215,0.063,-0.0441,0.0347,-0.0115,0.0088,0.0244,-0.0689,0.0051,-0.0331,-0.0257,0.0063,-0.0225,0.0319,0.0109,0.0153,0.0098,0.0514,0.024,-0.0676,0.0311,0.048,0.0412,0.0096,-0.0118,0.0097,0.0576,-0.0321,-0.0169,0.0664,-0.0283,-0.0272,-0.0041,0.0234,0.0325,0.0036,0.0754,-0.0022,-0.045,-0.0711,-0.2567,-0.0282,-0.0187,0.0198,-0.0025,-0.038,0.064,-0.052,0.0546,0.0641,0.0109,-0.0375,-0.0701,0.0207,0.0024,0.0581,0.0305,0.0426,-0.0528,0.0374,-0.0262,0.0106,-0.0054,-0.0517,0.0292,0.0191,0.1971,0.0211,0.0146,-0.0451,0.0214,0.0218,-0.0216,-0.1111,0.0506,0.0097,0.0215,-0.0083,-0.042,-0.0321,-0.0083,-0.0023,-0.002,-0.0497,-0.0039,-0.0655,-0.0345,0.027,-0.0463,0.0039,0.029,-0.027,0.0476,-0.0357,0.0499,-0.0073,-0.102,0.0216,-0.0414,0.0547,0.0201,-0.0715,0.0133,-0.0385,0.041,0.0154,-0.0068,-0.0093,0.0179,-0.0212,-0.0473,0.0268,-0.0388,-0.0067,0.0457,0.0126,0.0618,0.0024,-0.0141,-0.0094,0.0859,-0.0353,0.0122,-0.0123,0.048,0.0219,0.1025,0.0165,0.0197,-0.0343,-0.0138,0.0181,-0.0955,0.0092,0.0139,0.011,-0.2943,0.0125,0.0083,0.0018,0.0106,-0.0072,0.0771,-0.0244,-0.0164,-0.0055,0.0165,0.0392,0.0198,-0.0017,-0.009,0.0148,0.0749,-0.0348,0.0226,-0.0837,0.0065,0.0396,0.2232,-0.078,0.0334,0.0408,-0.0167,-0.0205,-0.019,-0.0284,0.0167,0.0197,0.0623,-0.0651,0.0344,0.083,-0.0643,0.0002,0.0451,-0.0266,0.0587,0.0097,-0.0801,-0.0164,0.1122,-0.0211,0.0041,-0.0252,0.0181,-0.0319,-0.034,0.027,0.0213,-0.0326,0.0109,0.0393,-0.0567,-0.0417,-0.0113,-0.0292,0.0484,-0.053,-0.0315,0.0174,0.0145]}
{"key":"[Robust Counterfactual Explanations on Graph Neural Networks] Massive deployment of Graph Neural Networks (GNNs) in high-stake applications generates a strong demand for explanations that are robust to noise and align well with human intuition. Most existing methods generate explanations by identifying a subgraph of an input graph that has a strong correlation with the prediction. These explanations are not robust to noise because independently optimizing the correlation for a single input can easily overfit noise. Moreover, they do not align well with human intuition because removing an identified subgraph from an input graph does not necessarily change the prediction result. In this paper, we propose a novel method to generate robust counterfactual explanations on GNNs by explicitly modelling the common decision logic of GNNs on similar input graphs. Our explanations are naturally robust to noise because they are produced from the common decision boundaries of a GNN that govern the predictions of many similar input graphs. The explanations also align well with human intuition because removing the set of edges identified by an explanation from the input graph changes the prediction significantly. Exhaustive experiments on many public datasets demonstrate the superior performance of our method.","layer":1,"vector":[0.0074,-0.0238,-0.0037,0.0337,0.0497,0.0035,0.0495,0.0231,-0.0034,-0.0256,0.0022,-0.0725,0.0525,0.0768,0.0094,0.0273,-0.0347,0.0608,-0.0343,0.005,0.0433,-0.0359,-0.0179,-0.0546,0.0292,0.0083,-0.02,-0.0538,-0.0287,-0.2195,-0.0021,-0.0287,0.066,-0.0405,0.0289,-0.0651,0.0046,0.0356,-0.0251,0.0585,0.0301,0.0146,-0.0139,-0.048,-0.0108,-0.0172,0.0322,-0.0058,-0.0369,-0.0454,0.0644,-0.0305,0.004,0.0012,0.0675,0.0401,0.0624,0.0604,0.0248,0.0902,0.0138,0.0431,-0.139,0.0808,0.0437,0.0247,-0.0683,-0.0222,0.0206,0.0664,0.0333,0.0527,0.0346,0.0449,0.0134,0.0061,0.0055,-0.0161,-0.0212,0.0116,0.025,-0.0181,-0.0206,0.0015,0.0262,-0.026,0.0308,0.0025,0.0211,-0.0153,-0.0254,-0.0392,0.0004,-0.0019,-0.053,-0.0015,0.0541,0.0058,-0.0899,0.1729,-0.0446,-0.0326,0.0282,-0.0109,0.0141,-0.0551,-0.0479,-0.048,-0.0133,-0.0281,-0.0137,-0.0273,0.0296,-0.0386,0.0017,0.0323,0.0925,0.0139,-0.0209,-0.0078,-0.0633,0.0183,0.0314,-0.021,0.0254,-0.0543,-0.0049,0.1301,0.0081,-0.0195,0.0393,-0.0347,-0.0234,-0.018,0.0408,-0.02,0.0284,0.0529,0.0013,0.0161,0.007,-0.0264,-0.0083,-0.0686,-0.0959,0.0771,-0.057,0.0006,0.0022,-0.0153,-0.0247,0.0116,-0.0075,-0.0213,-0.0117,0.0413,0.0306,0.0277,-0.0608,0.0251,-0.0011,-0.0414,-0.0384,0.0909,0.0193,-0.1202,-0.0102,-0.0054,0.0056,-0.0115,0.0312,0.04,-0.027,0.0311,0.0598,0.0066,-0.0781,-0.0435,0.0109,-0.0089,0.0137,-0.0598,-0.0645,0.0517,0.0151,-0.0462,-0.0111,-0.0421,0.0041,0.0366,-0.0458,0.0466,-0.02,0.0411,-0.0662,-0.0084,-0.0389,-0.023,-0.0013,-0.0307,-0.0038,-0.0231,-0.0546,-0.0018,-0.0531,0.0262,-0.0138,0.0004,0.0352,0.0376,-0.0168,0.0065,0.0065,-0.0311,-0.0091,0.0003,0.0386,0.0164,0.01,0.0319,0.0784,-0.0001,-0.0486,-0.2289,0.0046,-0.0029,-0.0012,0.0915,-0.0478,0.0378,-0.0122,0.0226,0.0751,0.0461,0.0025,-0.0577,-0.034,-0.0213,0.0864,0.0155,0.0281,-0.0318,0.009,-0.0261,0.0392,0.0074,-0.1242,0.0549,0.0834,0.2424,0.018,0.0757,-0.0353,0.0072,0.018,-0.0406,-0.1001,0.0802,0.0645,0.0562,-0.023,-0.0342,-0.0388,-0.0396,0.0505,-0.0318,-0.1053,-0.0646,-0.0148,0.001,0.0017,-0.0321,0.0086,0.0387,0.0082,0.0813,0.075,-0.0101,-0.0463,-0.095,0.0157,-0.0548,0.013,0.0101,-0.0592,-0.0166,-0.0453,0.0596,0.0326,-0.0415,-0.0394,0.025,0.0473,-0.0279,0.0971,-0.0024,-0.0287,0.056,0.0467,0.0456,-0.0354,-0.0736,0.0064,0.0547,-0.0318,0.0056,-0.0204,0.0249,-0.0103,0.0589,-0.033,0.043,-0.0297,0.0602,0.013,-0.0107,-0.0301,0.0377,-0.0164,-0.3168,0.0326,0.0354,0.046,-0.0221,0.0329,0.0468,0.0703,-0.0293,-0.0134,-0.0131,0.0309,0.0275,-0.0208,-0.015,0.01,0.0332,-0.0333,0.0796,-0.0104,0.0637,0.0141,0.2167,-0.0288,0.0413,0.033,-0.0384,-0.0527,0.0384,-0.0124,-0.003,-0.0019,0.0628,-0.0288,0.0362,0.0687,-0.0596,0.04,0.0597,-0.0374,-0.0308,-0.0393,-0.0084,-0.062,0.0838,-0.0223,-0.0396,-0.041,0.0399,0.011,-0.035,0.0073,-0.0655,-0.0121,-0.0075,0.0073,-0.0235,-0.0596,-0.0498,-0.0507,-0.0086,-0.0683,0.0211,0.008,-0.0139]}
{"key":"[A Learning Convolutional Neural Network Approach for Network Robustness Prediction] Network robustness is critical for various societal and industrial networks again malicious attacks. In particular, connectivity robustness and controllability robustness reflect how well a networked system can maintain its connectedness and controllability against destructive attacks, which can be quantified by a sequence of values that record the remaining connectivity and controllability of the network after a sequence of node- or edge-removal attacks. Traditionally, robustness is determined by attack simulations, which are computationally very time-consuming or even practically infeasible. In this paper, an improved method for network robustness prediction is developed based on learning feature representation using convolutional neural network (LFR-CNN). In this scheme, higher-dimensional network data are compressed to lower-dimensional representations, and then passed to a CNN to perform robustness prediction. Extensive experimental studies on both synthetic and real-world networks, both directed and undirected, demonstrate that 1) the proposed LFR-CNN performs better than other two state-of-the-art prediction methods, with significantly lower prediction errors; 2) LFR-CNN is insensitive to the variation of the network size, which significantly extends its applicability; 3) although LFR-CNN needs more time to perform feature learning, it can achieve accurate prediction faster than attack simulations; 4) LFR-CNN not only can accurately predict network robustness, but also provides a good indicator for connectivity robustness, better than the classical spectral measures.","layer":2,"vector":[-0.0059,-0.0727,-0.0402,0.0021,0.0482,0.0031,0.0759,0.0469,0.0139,-0.0136,-0.0054,-0.0467,0.0862,0.0755,0.0006,0.0163,-0.0029,0.0725,-0.0293,0.0173,0.0266,-0.0517,0.0115,-0.0404,0.03,0.0079,0.027,0.0035,-0.0654,-0.2283,-0.0087,-0.045,0.0486,-0.0397,0.0194,-0.0143,-0.0438,0.0316,-0.0451,0.0311,0.0216,0.0348,-0.0215,-0.0759,-0.0565,-0.0675,0.0076,-0.0145,0.0112,-0.0852,0.0406,-0.052,0.0382,0.0512,0.0537,-0.0136,0.0962,0.0391,0.0604,0.0551,0.0408,0.0351,-0.1726,0.0345,0.0102,0.0273,-0.0745,0.0312,0.0164,0.0286,-0.0121,0.0565,-0.0117,0.0416,0.0343,0.0486,0.0063,-0.0031,-0.0216,-0.0005,0.0627,-0.0064,-0.0488,0.0049,-0.0128,-0.0365,0.0041,-0.0513,0.0191,0.0155,-0.0479,-0.017,-0.0215,0.0247,-0.0588,0.003,0.0395,-0.0155,-0.0845,0.1754,-0.0552,-0.0127,0.0254,-0.0415,0.0484,-0.0062,-0.0079,-0.0471,-0.0672,-0.0032,0.0057,-0.038,0.0024,-0.0249,0.0444,0.0241,0.0636,0.0318,-0.0128,-0.0163,-0.0178,0.0298,0.0975,-0.0689,0.0966,-0.0457,0.0264,0.1638,0.037,0.0305,-0.0179,-0.0268,-0.0068,-0.0088,0.0205,0.0503,-0.0217,-0.0182,-0.0153,-0.0163,-0.0261,-0.0343,0.0155,-0.0798,-0.0342,0.0901,-0.0621,-0.0049,-0.0113,-0.0399,-0.0204,-0.0122,-0.0377,-0.0218,0.0222,0.0228,0.027,0.0647,-0.0386,-0.0119,-0.0043,-0.0643,-0.048,0.1131,0.0077,-0.0919,-0.0008,0.0115,-0.0083,-0.0022,0.0048,0.0115,-0.017,0.0216,0.0463,0.0362,-0.0825,-0.0283,-0.0362,0.027,0.0057,-0.0581,-0.027,0.0318,0.0491,-0.002,0.0016,-0.0217,0.0278,0.0428,-0.0744,0.0386,-0.0539,0.0097,-0.0408,-0.0246,-0.0294,-0.0305,-0.0221,-0.0099,-0.0037,0.0255,-0.0549,0.0156,-0.0124,0.0412,0.0037,0.0248,-0.0091,0.0094,-0.0433,-0.0278,0.0613,-0.0409,-0.0224,-0.0297,0.0102,0.0535,-0.0054,0.0431,0.0534,-0.0285,-0.0594,-0.2314,-0.0025,-0.0325,-0.0325,0.0702,-0.0811,0.0347,-0.0162,0.0471,0.0395,0.0644,0.0175,-0.0289,-0.0042,-0.0075,0.0822,0.0157,0.0273,-0.0271,0.027,-0.0485,0.0092,-0.0151,-0.0729,0.0338,0.0251,0.1835,-0.0179,0.058,-0.0315,0.0291,0.0561,-0.039,-0.081,0.0765,-0.0022,0.0589,-0.0004,-0.0432,-0.0068,-0.0714,0.0109,0.0227,-0.1282,-0.0236,-0.0082,-0.0384,0.0549,-0.0735,0.0316,0.0378,0.0034,0.0581,0.0132,0.0373,-0.0165,-0.0976,0.0669,-0.0463,0.0084,-0.0122,-0.0488,-0.0383,-0.0866,0.0637,0.0431,-0.0051,-0.0563,0.0433,-0.0169,-0.0113,0.0965,0.0594,-0.0042,0.0908,-0.0354,-0.005,-0.0168,-0.0396,-0.0109,0.0714,-0.0194,0.0272,-0.0165,0.0499,-0.0052,0.0467,0.0169,0.0851,-0.0346,-0.01,-0.0266,-0.019,-0.0552,0.0377,0.0144,-0.2877,0.0491,0.0152,0.0557,-0.046,-0.0089,0.0682,0.0371,-0.0333,-0.0009,0.033,0.0238,0.0506,-0.0494,0.0043,-0.0252,0.0451,-0.0531,0.0515,-0.0121,0.0249,0.0372,0.2228,-0.0428,0.0389,0.0337,-0.0062,0.0378,-0.0099,-0.0497,0.0159,-0.0061,0.0474,-0.0622,0.0168,0.0834,-0.0201,0.0018,0.0008,0.0085,-0.0249,0.0403,-0.0232,-0.0101,0.0897,-0.0229,-0.0205,-0.0343,0.031,0.0217,-0.0606,-0.024,0.0187,0.0027,0.0116,0.0172,-0.0375,-0.0296,-0.0686,0.0196,0.0353,-0.0388,0.0145,0.0021,-0.0449]}
{"key":"[T4PdM: a Deep Neural Network based on the Transformer Architecture for Fault Diagnosis of Rotating Machinery] Deep learning and big data algorithms have become widely used in industrial applications to optimize several tasks in many complex systems. Particularly, deep learning model for diagnosing and prognosing machinery health has leveraged predictive maintenance (PdM) to be more accurate and reliable in decision making, in this way avoiding unnecessary interventions, machinery accidents, and environment catastrophes. Recently, Transformer Neural Networks have gained notoriety and have been increasingly the favorite choice for Natural Language Processing (NLP) tasks. Thus, given their recent major achievements in NLP, this paper proposes the development of an automatic fault classifier model for predictive maintenance based on a modified version of the Transformer architecture, namely T4PdM, to identify multiple types of faults in rotating machinery. Experimental results are developed and presented for the MaFaulDa and CWRU databases. T4PdM was able to achieve an overall accuracy of 99.98% and 98% for both datasets, respectively. In addition, the performance of the proposed model is compared to other previously published works. It has demonstrated the superiority of the model in detecting and classifying faults in rotating industrial machinery. Therefore, the proposed Transformer-based model can improve the performance of machinery fault analysis and diagnostic processes and leverage companies to a new era of the Industry 4.0. In addition, this methodology can be adapted to any other task of time series classification.","layer":0,"vector":[-0.0754,0.0018,0.0315,-0.0229,0.044,0.0025,0.0278,0.0409,0.024,-0.0293,-0.0213,-0.0477,0.0363,0.0147,-0.0032,0.0235,-0.0099,0.04,0.0027,0.0014,0.0679,-0.0219,-0.0245,-0.0021,-0.0026,0.0408,0.0026,-0.017,-0.046,-0.2244,-0.0272,-0.0446,0.0523,-0.017,0.0365,-0.0096,-0.0448,0.0374,0.0117,0.0274,0.0145,0.0076,0.0043,-0.0508,-0.0005,-0.0597,0.0185,-0.0399,-0.0002,-0.0653,0.0436,-0.0519,0.0513,0.0412,0.0035,0.0334,0.0987,0.053,0.0319,-0.0107,0.0809,0.0679,-0.213,0.0653,0.0158,0.0521,-0.0554,-0.0375,0.0118,0.0473,0.0046,-0.0009,0.0377,0.0237,-0.0186,0.0574,0.0027,-0.0437,-0.0401,0.0129,0.0395,-0.0404,-0.0158,-0.0422,-0.0736,-0.074,0.0174,-0.0365,0.0254,0.0136,-0.0509,0.0017,0.006,0.0417,-0.0461,0.0013,0.035,0.0359,-0.0814,0.1842,-0.0655,0.0083,0.0196,-0.0612,-0.0052,0.0078,-0.0497,-0.0326,-0.031,-0.0199,-0.0083,-0.0041,0.0076,-0.0301,0.0175,-0.0177,0.0646,0.0231,-0.0339,0.0115,-0.0121,0.0105,0.0593,-0.0179,0.0241,-0.0521,0.0448,0.1625,0.0064,0.0014,0.0178,0.0284,-0.0773,0.0159,0.0607,0.0203,0.0361,-0.0108,0.008,0.0083,-0.0604,-0.0884,-0.0131,-0.0352,-0.0435,0.0825,-0.0452,-0.0435,-0.0389,-0.0216,-0.0489,0.0104,-0.013,-0.0175,0.0554,0.0123,0.0245,0.0167,-0.0306,0.0016,0.0013,-0.0512,-0.0816,0.1096,0.0127,-0.0838,-0.0272,-0.0106,-0.0192,-0.0418,0.0251,0.0128,-0.013,-0.0361,0.0691,0.0614,0.0147,-0.0053,-0.0182,0.0319,0.0801,-0.0021,-0.0105,0.0523,0.0463,-0.0195,0.005,-0.0838,0.0079,0.0217,-0.0524,0.0416,-0.0406,-0.0087,0.0022,0.0034,-0.0403,0.0143,0.0361,-0.0563,-0.0367,0.0095,-0.0206,0.0324,-0.0359,0.022,-0.0238,0.0223,0.0008,0.0461,-0.0133,-0.0278,0.0942,-0.0371,-0.0535,0.0225,0.0198,0.0712,-0.0337,0.0499,0.0552,-0.0066,-0.068,-0.2025,-0.0179,0.0063,0.0127,0.0551,-0.0451,-0.0183,0.0064,0.0628,0.0344,0.0688,-0.0142,-0.0351,-0.021,-0.006,0.0719,0.0505,0.0172,-0.0413,-0.019,0.0294,0.0053,0.0425,-0.1054,0.0332,-0.0062,0.2045,-0.0308,0.045,-0.0348,0.0535,0.0551,-0.0317,-0.0865,0.1075,-0.0141,0.0349,-0.0223,-0.0781,-0.0343,-0.019,0.0207,-0.0076,-0.0742,-0.0338,-0.0764,-0.0275,0.0319,-0.059,0.0467,0.0529,-0.0413,0.0598,0.0128,0.0026,-0.0451,-0.0905,-0.0076,-0.0327,-0.0206,0.0269,-0.0475,0.0527,-0.0401,0.0613,-0.014,-0.0542,-0.0418,0.0074,-0.0422,-0.0177,0.1131,0.0352,0.0144,0.0306,-0.0051,0.0258,-0.0263,-0.0193,-0.0296,0.0786,-0.0673,0.0474,0.0399,0.0326,-0.028,0.0677,-0.0174,0.0518,0.0091,0.0212,0.0045,-0.0342,-0.0171,0.0517,0.0021,-0.2808,0.0613,0.0151,0.0315,-0.0128,-0.0244,-0.0287,0.0493,0.0105,-0.013,-0.0171,0.0263,0.0521,-0.0548,0.0134,0.0461,0.0666,-0.0776,0.0406,-0.055,0.0163,0.0637,0.2192,-0.0432,0.0434,-0.0108,-0.0185,-0.0098,0.0223,-0.0274,-0.0129,-0.0049,0.1166,-0.0155,0.0432,0.0903,-0.0387,0.0207,0.0586,0.0051,0.0015,0.0175,-0.041,-0.0465,0.065,-0.0116,-0.046,-0.0609,-0.0312,0.0379,0.0191,0.0151,-0.0156,0.0146,0.0155,0.0137,-0.0407,-0.0147,-0.0315,-0.0589,0.0353,-0.0892,0.0119,0.0102,0.0022]}
{"key":"[Fed-NILM: A Federated Learning-based Non-Intrusive Load Monitoring Method for Privacy-Protection] Non-intrusive load monitoring (NILM) is essential for understanding customer's power consumption patterns and may find wide applications like carbon emission reduction and energy conservation. The training of NILM models requires massive load data containing different types of appliances. However, inadequate load data and the risk of power consumer privacy breaches may be encountered by local data owners during the NILM model training. To prevent such potential risks, a novel NILM method named Fed-NILM which is based on Federated Learning (FL) is proposed in this paper. In Fed-NILM, local model parameters instead of local load data are shared among multiple data owners. The global model is obtained by weighted averaging the parameters. Experiments based on two measured load datasets are conducted to explore the generalization ability of Fed-NILM. Besides, a comparison of Fed-NILM with locally-trained NILMs and the centrally-trained NILM is conducted. The experimental results show that Fed-NILM has superior performance in scalability and convergence. Fed-NILM outperforms locally-trained NILMs operated by local data owners and approximates the centrally-trained NILM which is trained on the entire load dataset without privacy protection. The proposed Fed-NILM significantly improves the co-modeling capabilities of local data owners while protecting power consumers' privacy.","layer":10,"vector":[-0.0135,-0.0237,0.0358,-0.0058,0.0213,0.0335,0.0315,0.0022,0.0574,-0.0536,0.0166,-0.0487,0.0058,0.0491,0.0026,0.0211,0.0024,0.0346,-0.0421,-0.0234,0.0601,-0.0571,-0.0538,-0.029,0.0192,0.0472,-0.0409,-0.0052,-0.0713,-0.228,0.0453,-0.0666,0.0452,-0.0043,0.0163,-0.0191,-0.0175,0.0625,-0.0092,0.0167,-0.0149,-0.0024,-0.0015,-0.045,-0.034,-0.0282,0.0136,-0.0254,-0.0197,-0.0218,0.08,-0.0372,-0.0295,0.0229,0.0235,0.0552,0.0764,0.0018,0.1195,0.023,0.0236,0.0555,-0.1916,0.0712,0.0544,0.0592,-0.0107,-0.0129,0.0314,0.0121,-0.016,0.0203,-0.0226,0.0199,0.0246,0.0182,-0.0234,-0.0098,-0.0623,-0.022,0.0327,-0.0184,-0.0266,-0.0177,-0.0527,-0.0402,-0.0151,-0.0784,0.0321,0.0006,-0.0499,-0.0017,0.0017,0.0465,-0.0442,-0.0478,0.0169,0.0117,-0.0659,0.1827,-0.05,0.0484,0.0389,-0.0466,0.0167,-0.0569,-0.023,-0.0122,-0.0157,-0.0212,-0.0107,-0.0109,0.0152,-0.0536,0.0502,-0.0032,0.0825,0.0367,0.0139,0.0121,0.0152,-0.0036,0.0761,-0.0017,0.0527,-0.0739,0.03,0.1264,0.0146,0.0239,0.0416,-0.0347,-0.0397,-0.0123,0.0509,0.0675,0.0383,-0.0006,0.0486,0.035,-0.023,-0.0522,-0.0112,-0.1055,-0.0308,0.0835,0.0091,0.0806,-0.0735,-0.0497,-0.0136,0.0197,-0.0187,-0.0538,0.0442,0.0057,0.0667,0.0665,-0.0464,0.005,-0.0137,-0.0063,-0.0303,0.0965,0.0387,-0.1169,-0.0021,0.0204,0.0039,-0.0619,0.0474,0.0324,-0.0735,0.0459,0.0945,0.0508,-0.0341,0.0025,-0.0148,-0.0164,-0.005,-0.0334,-0.0157,-0.0231,0.0159,-0.0357,0.0282,-0.0557,-0.0132,0.0175,-0.0554,0.0018,-0.0388,0.0148,-0.0209,-0.046,-0.0274,-0.0154,0.0371,-0.0557,0.0034,0.0013,-0.016,0.0279,-0.0241,0.0506,-0.0023,-0.0183,0.0368,-0.0071,-0.0017,0.0106,0.0338,-0.0137,-0.0727,0.0168,0.0238,0.0175,0.0293,0.0285,0.0535,0.001,-0.0791,-0.2319,0.0014,0.0167,-0.005,0.0371,-0.0341,0.0904,-0.0298,0.0014,0.0678,0.0784,-0.0038,-0.0666,0.0393,-0.022,0.0668,0.0721,0.011,-0.0985,0.0058,0.0021,0.0107,0.0192,-0.0878,0.0733,0.0095,0.1586,-0.0256,0.0551,-0.0528,0.0056,-0.0252,-0.0491,-0.1351,0.0666,0.0149,0.0154,0.0028,-0.033,-0.0301,0.01,0.0138,0.0098,-0.0989,-0.0201,-0.0616,-0.0216,0.0124,-0.1379,0.0011,0.0315,-0.0112,0.0612,-0.0096,0.0207,-0.0299,-0.0553,0.0737,-0.0223,0.0674,-0.0106,-0.046,0.0366,-0.0754,0.0736,-0.0436,-0.0577,-0.0343,-0.0081,-0.0137,-0.0219,0.1032,0.0165,-0.0061,0.028,0.0533,0.0203,-0.0396,-0.0523,-0.0417,0.0775,0.0024,0.0495,0.038,-0.031,-0.0163,0.0603,0.0478,0.0252,-0.0306,0.0194,0.0163,-0.0173,-0.0615,0.0643,0.0058,-0.2984,0.0043,-0.0265,0.0274,-0.0202,-0.0344,0.0187,0.0382,-0.0231,0.0012,0.009,0.0553,0.0095,0.0135,0.0357,0.0169,0.0284,-0.0609,0.0674,-0.0285,0.0439,0.0296,0.2038,-0.0287,0.051,0.0159,-0.0145,0.0394,0.0011,0.0078,-0.0195,-0.0159,0.076,-0.0515,0.0352,0.0504,0.0016,0.0246,0.0363,-0.0264,-0.0091,0.0208,-0.0177,-0.0055,0.0814,0.0167,-0.0855,-0.024,0.0014,0.02,0.0011,-0.0202,-0.0227,-0.0036,0.0345,0.0062,-0.0425,-0.0135,-0.0329,-0.0396,-0.0211,0.0179,-0.0589,-0.0391,0.0151]}
{"key":"[Synthetic Oversampling of Multi-Label Data based on Local Label Distribution] Class-imbalance is an inherent characteristic of multi-label data which affects the prediction accuracy of most multi-label learning methods. One efficient strategy to deal with this problem is to employ resampling techniques before training the classifier. Existing multilabel sampling methods alleviate the (global) imbalance of multi-label datasets. However, performance degradation is mainly due to rare subconcepts and overlapping of classes that could be analysed by looking at the local characteristics of the minority examples, rather than the imbalance of the whole dataset. We propose a new method for synthetic oversampling of multi-label data that focuses on local label distribution to generate more diverse and better labeled instances. Experimental results on 13 multi-label datasets demonstrate the effectiveness of the proposed approach in a variety of evaluation measures, particularly in the case of an ensemble of classifiers trained on repeated samples of the original data.","layer":8,"vector":[-0.0017,-0.0228,0.0234,-0.0327,0.0567,-0.0034,0.0069,0.0373,0.0047,-0.0537,-0.0058,-0.0675,0.0091,0.0564,0.0345,0.0019,0.0277,0.0417,-0.0497,-0.0223,-0.0011,-0.0099,-0.0216,-0.0127,0.0637,0.0428,-0.0093,-0.075,-0.0592,-0.2567,0.0435,-0.0135,0.0618,0.0209,0.0101,-0.0545,-0.0174,0.0414,-0.0496,0.1096,0.0339,0.011,-0.0343,-0.0956,-0.0361,-0.0364,-0.0722,-0.0391,0.014,0.0162,0.0572,-0.0412,-0.0016,-0.0038,-0.0,0.0348,0.0354,0.0226,0.0796,0.0526,0.0414,0.0379,-0.1613,0.0287,0.058,0.0451,-0.0523,-0.021,-0.006,0.0624,-0.0153,0.0325,0.045,0.0477,0.0217,-0.0147,0.046,-0.0345,-0.0114,0.0028,0.0442,-0.007,-0.0174,-0.0542,-0.0082,-0.0657,0.0303,-0.0267,0.073,0.0466,-0.0414,-0.0356,-0.0319,0.0563,-0.0576,-0.0302,0.0102,0.0117,-0.0513,0.1727,-0.0571,0.0444,0.0235,-0.029,-0.0034,-0.0712,-0.0185,-0.0337,-0.0396,-0.0516,0.0024,-0.0171,-0.0,-0.0179,0.0192,0.014,0.0996,0.0263,0.01,-0.0268,0.021,0.0004,0.0706,-0.0262,0.0366,-0.0357,0.0351,0.1154,0.0195,-0.0118,0.0029,0.0011,-0.0631,-0.0357,0.0149,-0.0007,-0.0012,0.0149,0.0921,0.0183,-0.0032,-0.0303,-0.0003,-0.0876,-0.0433,0.1355,-0.0437,0.0532,-0.0476,-0.0282,-0.0322,0.0231,-0.0608,-0.0389,0.0404,0.0342,0.0363,0.0634,-0.0209,0.0521,-0.0023,-0.0711,0.0071,0.1,-0.0056,-0.0512,-0.0371,0.0113,-0.0144,0.012,0.027,0.0101,-0.0462,0.0591,0.0619,0.0018,-0.067,-0.0311,0.0061,-0.0106,0.0297,-0.0331,-0.0695,0.0572,0.0409,-0.0405,-0.0151,-0.0284,0.0108,0.0383,-0.0234,-0.0193,-0.0123,-0.0149,-0.033,-0.0397,0.0045,-0.0063,0.0512,-0.0663,-0.0063,0.0408,-0.0343,-0.0008,0.0016,0.0362,0.0176,-0.0045,0.0461,-0.0147,-0.0344,0.0336,0.054,0.0206,-0.0363,-0.0314,0.0449,0.0882,0.0244,0.0322,0.0268,-0.0218,-0.0448,-0.2169,0.0146,0.0533,-0.0019,0.0378,-0.0524,0.0437,0.0194,0.0726,0.0903,0.0509,0.0121,-0.0284,0.0221,-0.0277,0.0718,0.0099,0.0014,0.0089,0.0004,0.0125,0.0346,0.0164,-0.0764,0.0956,-0.0749,0.2109,0.0171,0.0097,0.0017,0.0441,0.04,0.0003,-0.0616,0.0979,0.0382,0.021,-0.0178,-0.0518,-0.0225,-0.0176,-0.0018,0.045,-0.1379,-0.0254,-0.0373,-0.0544,0.0022,-0.0605,-0.0112,0.0358,-0.025,0.0295,-0.0014,0.0324,-0.0282,-0.1266,0.0278,-0.0553,-0.0127,0.0336,-0.0515,0.0402,-0.0415,0.0281,0.0298,-0.0467,-0.0406,0.0044,-0.0193,-0.0304,0.1167,0.0384,-0.0301,0.0259,-0.0105,0.0052,-0.0588,-0.0218,-0.0441,0.0277,-0.0016,0.0074,0.0103,0.0275,-0.0016,0.0729,0.023,0.0446,-0.0344,0.0489,0.0369,-0.0276,0.006,0.0221,-0.0085,-0.2754,0.0381,-0.0462,0.0559,-0.0016,0.0145,0.0296,0.0223,-0.0469,0.0202,0.0162,-0.002,0.0361,-0.0474,-0.0111,0.0123,0.0642,-0.0349,0.0623,-0.0719,0.012,0.0252,0.1959,0.001,0.0106,-0.0005,-0.0384,-0.0087,-0.0124,-0.0355,-0.0042,-0.0088,0.0926,-0.0437,0.074,0.0625,-0.0551,-0.0292,0.0207,-0.0384,0.0402,-0.0174,-0.0829,-0.0603,0.0966,0.0082,-0.0504,-0.0548,0.0284,0.0238,-0.0392,-0.0018,-0.0449,-0.0119,0.0367,0.0211,-0.0307,-0.0506,-0.0598,-0.0327,0.0312,-0.057,-0.0268,-0.035,-0.0317]}
{"key":"[Local Score Dependent Model Explanation for Time Dependent Covariates] The use of deep neural networks to make high risk decisions creates a need for global and local explanations so that users and experts have confidence in the modeling algorithms. We introduce a novel technique to find global and local explanations for time series data used in binary classification machine learning systems. We identify the most salient of the original features used by a black box model to distinguish between classes. The explanation can be made on categorical, continuous, and time series data and can be generalized to any binary classification model. The analysis is conducted on time series data to train a long short-term memory deep neural network and uses the time dependent structure of the underlying features in the explanation. The proposed technique attributes weights to features to explain an observations risk of belonging to a class as a multiplicative factor of a base hazard rate. We use a variation of the Cox Proportional Hazards regression, a Generalized Additive Model, to explain the effect of variables upon the probability of an in-class response for a score output from the black box model. The covariates incorporate time dependence structure in the features so the explanation is inclusive of the underlying time series data structure.","layer":2,"vector":[-0.0341,-0.0018,-0.0009,-0.0059,0.0828,-0.0097,0.0703,0.0263,0.0616,-0.0414,-0.0036,-0.006,-0.0064,0.0618,0.0022,0.0125,-0.0152,0.0121,-0.0216,0.0214,0.0171,-0.0327,0.0004,-0.0026,0.0507,-0.0041,-0.0135,-0.0096,-0.0356,-0.2254,0.0188,-0.0346,0.0604,-0.0512,-0.004,-0.0198,-0.0055,0.0556,-0.028,0.0281,0.0235,0.0221,-0.0235,-0.0235,-0.0084,-0.044,-0.0034,-0.0444,-0.0021,-0.0198,0.0394,-0.0558,0.0445,0.0657,0.0368,0.0226,0.0771,0.0432,0.0294,0.0194,0.0203,0.0665,-0.198,0.0317,0.0518,0.0585,-0.0153,0.0069,0.0358,0.0526,-0.0342,0.0256,0.0455,0.0641,-0.0161,0.0305,0.0088,-0.0399,-0.0046,0.0249,0.0421,-0.0019,-0.0488,-0.0416,-0.0056,-0.0842,0.0361,-0.0777,0.0017,-0.0165,-0.0431,-0.0126,-0.0567,0.0323,-0.0663,-0.0368,0.0456,0.0365,-0.071,0.1832,-0.0945,0.0054,-0.0042,-0.0037,0.0257,-0.0383,-0.0229,-0.0579,0.0038,-0.0049,0.0248,-0.0048,-0.0005,-0.0304,-0.0067,-0.0002,0.0167,0.0236,0.033,0.0156,-0.0125,-0.0022,0.0772,-0.0091,0.0064,-0.0492,0.0301,0.1447,0.0188,-0.0121,0.0283,-0.0262,-0.0907,0.0163,-0.001,0.0141,0.0087,-0.0093,0.0183,-0.0531,-0.0299,-0.0261,0.0001,-0.0756,-0.0564,0.1242,-0.0413,0.0043,-0.0414,-0.0061,-0.0521,0.0009,-0.0571,-0.0378,0.0218,0.0478,0.0235,0.0091,-0.0385,-0.0011,-0.0208,-0.0603,0.0073,0.0876,0.0236,-0.0552,-0.0564,0.0183,0.0275,-0.0066,0.0488,0.0274,-0.0363,-0.0015,0.0727,0.0383,-0.0529,-0.0109,0.0405,0.0169,0.0192,-0.0445,-0.0539,0.0617,0.0579,-0.0157,0.004,-0.0832,0.0085,0.029,-0.0228,-0.004,-0.059,-0.0038,-0.0122,-0.0266,-0.0185,0.0056,0.0457,-0.0541,-0.002,0.0137,0.0041,0.0129,-0.0366,0.005,0.0004,0.0239,0.0298,0.0059,-0.01,0.0123,0.058,-0.042,0.0149,0.009,-0.0185,0.0248,0.0159,0.0469,0.0414,-0.0458,-0.0328,-0.2636,-0.0234,0.0135,-0.0267,0.0525,-0.0795,0.0041,-0.0233,0.0174,0.072,0.0782,-0.0127,-0.02,-0.0099,-0.0352,0.052,0.028,0.0064,-0.0517,0.0059,-0.0312,-0.0218,0.0157,-0.1008,0.0506,0.0217,0.1863,0.0164,0.0393,-0.0153,0.0086,0.0185,-0.0228,-0.0938,0.0889,0.0051,0.0442,-0.0093,-0.0459,-0.0328,-0.0805,0.0257,-0.0061,-0.0457,-0.0568,-0.0219,0.0613,0.0429,-0.0786,0.0262,0.0276,-0.0286,0.0589,0.0203,0.0332,-0.0562,-0.104,0.0331,-0.0352,0.0219,-0.0214,-0.0526,0.0365,-0.0847,0.055,-0.0238,-0.0471,-0.0681,0.0034,-0.0325,-0.0355,0.103,-0.045,-0.0347,0.0685,-0.0074,0.058,-0.0172,-0.0589,-0.0105,0.0773,-0.024,0.0175,0.0411,0.0248,-0.0083,0.101,0.0005,0.0314,0.0223,-0.0105,0.0036,-0.0508,-0.0328,0.0068,0.0025,-0.2695,0.0473,-0.0118,0.0353,0.0005,-0.048,0.0117,0.0226,-0.0017,0.0174,-0.0148,0.0455,0.1002,-0.0219,0.0008,0.0407,0.0524,-0.0936,0.0788,-0.0079,0.0171,0.0664,0.1953,-0.0468,0.0606,0.0079,-0.0245,-0.0208,0.0368,0.0163,0.0637,0.0319,0.1062,-0.0472,0.0132,0.08,-0.0284,0.0549,0.071,-0.0103,0.0324,0.0098,-0.0591,-0.0294,0.1208,-0.0208,-0.0441,-0.0931,0.0044,0.0621,-0.0013,-0.0037,0.0043,-0.0061,0.0136,0.0286,-0.0145,-0.0368,-0.0326,-0.0372,0.0132,-0.0955,-0.0133,-0.0054,-0.0419]}
{"key":"[Learning to synthesise the ageing brain without longitudinal data] How will my face look when I get older? Or, for a more challenging question: How will my brain look when I get older? To answer this question one must devise (and learn from data) a multivariate auto-regressive function which given an image and a desired target age generates an output image. While collecting data for faces may be easier, collecting longitudinal brain data is not trivial. We propose a deep learning-based method that learns to simulate subject-specific brain ageing trajectories without relying on longitudinal data. Our method synthesises images conditioned on two factors: age (a continuous variable), and status of Alzheimer's Disease (AD, an ordinal variable). With an adversarial formulation we learn the joint distribution of brain appearance, age and AD status, and define reconstruction losses to address the challenging problem of preserving subject identity. We compare with several benchmarks using two widely used datasets. We evaluate the quality and realism of synthesised images using ground-truth longitudinal data and a pre-trained age predictor. We show that, despite the use of cross-sectional data, our model learns patterns of gray matter atrophy in the middle temporal gyrus in patients with AD. To demonstrate generalisation ability, we train on one dataset and evaluate predictions on the other. In conclusion, our model shows an ability to separate age, disease influence and anatomy using only 2D cross-sectional data that should be useful in large studies into neurodegenerative disease, that aim to combine several data sources. To facilitate such future studies by the community at large our code is made available at https://github.com/xiat0616/BrainAgeing.","layer":3,"vector":[-0.0218,-0.0264,0.0075,-0.0137,0.0196,0.0807,0.0079,-0.029,-0.0276,-0.0078,0.0249,-0.0407,0.0359,0.0445,-0.0042,0.0285,-0.0313,0.0594,-0.0432,0.0461,0.0019,-0.0107,0.0125,-0.0285,0.0168,0.0411,-0.0037,-0.0405,-0.0307,-0.2243,0.0278,-0.034,0.0303,-0.0596,0.0006,-0.0169,-0.0335,0.0776,-0.0381,0.0421,0.0073,0.0095,-0.057,-0.0585,-0.0132,-0.0126,-0.0512,-0.0061,-0.003,-0.0085,0.015,-0.0136,0.0081,0.041,0.0218,0.0564,0.0362,0.0587,0.1009,0.0424,-0.0027,0.0481,-0.1285,0.0455,0.0149,0.0127,-0.0151,-0.026,-0.0278,0.0358,-0.0129,0.014,0.0151,0.0317,0.0077,-0.0057,-0.0223,-0.0244,-0.0064,-0.0046,0.0652,0.0617,-0.033,0.0091,-0.0234,-0.0375,-0.0037,-0.0658,-0.0077,-0.0038,-0.0264,0.0167,-0.068,0.0393,-0.0279,-0.0516,0.0449,0.0387,-0.0342,0.2128,-0.0435,0.0476,0.0702,-0.0157,0.0213,-0.0342,-0.0291,-0.0192,-0.0347,0.0231,-0.0608,-0.0003,0.0212,-0.0244,0.0289,-0.0036,0.0156,0.0741,-0.0168,-0.0258,0.0053,0.0013,0.0273,-0.0295,0.038,-0.0507,0.0086,0.1479,0.048,-0.0217,0.0375,-0.0152,-0.0443,0.0105,0.0063,0.0147,-0.0104,0.0051,0.0146,-0.0127,-0.0394,-0.0206,0.0144,-0.0298,-0.0329,0.1275,-0.01,0.0325,-0.0728,0.0025,-0.0389,0.0312,-0.0071,-0.0016,0.0194,0.0078,0.0018,0.0272,-0.0707,0.0138,-0.0324,-0.0934,-0.028,0.0952,-0.004,-0.0742,-0.0352,0.0114,0.0418,0.0393,0.076,0.0306,0.0034,0.0484,0.0915,0.0479,-0.0736,0.0007,-0.0061,0.0479,0.0078,-0.0436,-0.0633,0.0554,0.0307,-0.0228,0.0291,-0.03,0.0099,0.0125,-0.0137,0.0423,-0.0763,-0.0177,-0.0445,-0.0638,-0.0667,-0.0007,0.016,-0.0332,-0.0059,-0.0179,-0.0487,0.025,0.0285,0.0339,-0.0092,-0.0072,0.1196,0.0684,-0.0665,-0.052,0.0893,-0.0304,-0.0088,0.0349,-0.0144,-0.0217,-0.0009,0.0557,0.0532,-0.0544,-0.0656,-0.2258,-0.0124,0.0019,-0.0492,0.0564,-0.0802,0.0177,0.0039,0.0777,0.0965,0.0478,-0.004,-0.0144,0.0172,-0.036,0.0598,0.0372,0.0205,-0.0378,-0.0266,-0.0178,0.0245,0.0271,-0.084,0.0688,0.0146,0.2363,0.0269,0.0379,-0.0308,-0.0176,0.0001,-0.0248,-0.0824,0.0804,-0.0321,0.047,-0.0105,-0.0767,-0.0364,-0.0447,0.0112,0.0329,-0.0691,-0.0591,-0.0138,-0.0385,0.0083,-0.0409,-0.0028,0.0643,-0.0271,0.0545,-0.0191,-0.024,-0.0771,-0.1242,0.0246,-0.0784,0.0385,-0.0064,-0.0262,-0.0011,-0.0886,0.0383,0.0051,-0.0439,-0.0933,0.0256,-0.019,0.0411,0.0807,-0.0243,-0.001,0.0334,0.013,0.0095,0.0078,-0.0299,0.0168,0.0167,-0.0686,0.0242,0.0536,0.0556,-0.0154,0.0485,-0.0371,0.0266,-0.0286,-0.0139,0.0367,-0.0959,-0.0599,0.0454,0.0009,-0.2857,0.0471,-0.0035,0.0762,-0.0193,0.0251,0.0331,0.0676,-0.0477,0.0091,-0.0301,-0.001,0.0735,-0.0087,0.0107,0.0055,0.0757,-0.0554,0.0463,-0.0582,-0.0122,0.01,0.1619,-0.0336,0.0337,0.0149,0.0133,0.0289,0.0604,-0.0476,0.0077,0.0023,0.0976,-0.0353,0.0179,0.1155,-0.0847,0.0347,-0.0304,-0.0143,-0.0017,0.0226,-0.0259,0.01,0.0927,-0.0105,-0.015,0.0026,-0.0259,-0.0144,-0.0159,0.0193,0.0218,-0.0021,0.0682,0.0398,0.0024,-0.0536,-0.0114,-0.0335,-0.0212,-0.0184,-0.0131,0.0236,-0.0549]}
{"key":"[Multi-Dialect Arabic BERT for Country-Level Dialect Identification] Arabic dialect identification is a complex problem for a number of inherent properties of the language itself. In this paper, we present the experiments conducted, and the models developed by our competing team, Mawdoo3 AI, along the way to achieving our winning solution to subtask 1 of the Nuanced Arabic Dialect Identification (NADI) shared task. The dialect identification subtask provides 21,000 country-level labeled tweets covering all 21 Arab countries. An unlabeled corpus of 10M tweets from the same domain is also presented by the competition organizers for optional use. Our winning solution itself came in the form of an ensemble of different training iterations of our pre-trained BERT model, which achieved a micro-averaged F1-score of 26.78% on the subtask at hand. We publicly release the pre-trained language model component of our winning solution under the name of Multi-dialect-Arabic-BERT model, for any interested researcher out there.","layer":0,"vector":[-0.0754,0.0025,0.0168,0.0029,0.0115,0.0078,0.0512,0.0168,0.0148,-0.0063,0.0278,-0.0748,0.051,0.0105,0.0453,0.0122,0.0379,0.0058,-0.0417,-0.0543,0.0246,0.0053,-0.0037,-0.03,0.0427,0.0256,-0.0316,-0.062,-0.045,-0.2167,0.0217,-0.0034,0.0363,-0.0469,-0.0043,0.0186,-0.0526,0.0432,-0.0358,0.0077,0.0423,-0.009,0.0054,-0.0914,0.0001,-0.0341,-0.0564,0.0176,-0.0484,-0.0119,-0.0294,-0.0568,0.0764,-0.0359,0.0251,0.063,0.0342,0.0532,0.0322,0.0236,0.0541,0.0283,-0.1825,0.1028,0.0123,0.0351,-0.0075,-0.0229,0.0206,0.042,-0.0269,0.0323,0.0335,0.0306,-0.0085,0.0142,0.0189,-0.0445,0.0034,-0.0145,0.015,-0.0095,-0.0405,-0.0144,-0.0294,-0.0613,0.0202,-0.0078,0.0159,-0.0283,-0.0482,-0.0013,0.006,0.0461,-0.0215,-0.0317,-0.0038,0.0294,-0.0545,0.2198,-0.0517,-0.0309,0.0501,-0.0588,0.029,0.0019,-0.0409,-0.0157,-0.0146,-0.0186,-0.0587,-0.0374,0.0402,-0.0348,0.0473,0.0077,0.0841,-0.003,-0.0311,0.0001,-0.0388,0.0262,0.0775,-0.021,-0.0113,-0.0488,0.0532,0.1356,0.0619,0.0451,0.05,-0.0102,-0.0183,-0.002,0.0226,0.0302,-0.0096,-0.0231,0.0125,-0.0202,-0.0422,-0.0854,0.0203,-0.0724,-0.0887,0.1483,-0.0561,-0.0161,-0.0668,-0.0327,-0.0299,0.0063,-0.0397,-0.0029,0.0575,0.0591,0.0621,0.0442,-0.0154,0.0086,-0.0039,-0.0935,-0.0488,0.0982,-0.0369,-0.0988,-0.0722,-0.0042,0.0077,-0.0416,0.0925,0.04,-0.0354,0.0134,0.0319,0.0189,-0.0325,0.0526,0.0182,0.0099,0.0404,-0.0341,-0.0188,0.035,0.0079,-0.0508,-0.0293,-0.0452,0.0596,0.0166,-0.0164,0.0394,-0.0148,-0.0083,0.0109,-0.0247,-0.0031,-0.0121,0.0594,-0.0409,0.0069,0.0653,-0.0406,0.0158,0.0029,0.0342,0.0153,-0.0026,0.0448,0.0565,0.0066,-0.016,0.0379,0.0117,-0.0349,-0.0137,0.0312,0.0434,0.019,0.0457,0.0094,-0.0452,-0.0161,-0.2266,-0.017,0.0489,-0.0242,0.0693,-0.0666,0.0067,0.0183,0.0983,0.0653,0.0442,-0.0329,-0.0182,0.0476,-0.0307,0.0692,-0.0164,0.0326,-0.0023,0.0269,0.0148,-0.0013,-0.0508,-0.0948,0.0451,-0.0007,0.1846,0.0287,0.0382,-0.0436,0.0493,0.0339,-0.0398,-0.1563,0.068,0.0136,0.0961,-0.0443,-0.0153,-0.0479,-0.0244,0.0207,0.0556,-0.0609,-0.0386,-0.0591,-0.0256,-0.0196,-0.0409,0.0015,0.0168,-0.0075,0.0607,0.017,-0.0565,-0.0175,-0.1104,-0.0046,-0.0257,0.012,0.0159,-0.048,0.036,-0.0574,0.0493,0.0099,-0.0871,-0.0216,-0.0283,0.0055,-0.0342,0.0832,0.0174,0.0008,-0.0031,0.0241,0.0476,-0.0536,-0.0226,-0.0251,0.0609,-0.0221,0.0668,0.0071,0.057,0.0515,0.0771,0.0029,0.0729,-0.0007,0.0382,-0.0041,0.0087,-0.0054,0.0243,-0.0067,-0.2931,0.0768,0.0098,0.0227,-0.0108,0.0183,0.055,-0.0075,-0.0466,-0.0176,0.0068,0.0307,0.0497,-0.036,-0.0003,0.0142,0.0641,-0.0234,0.0182,-0.0009,0.0214,0.0126,0.1943,-0.0211,0.0658,0.0169,-0.0278,-0.018,-0.003,-0.0032,0.0134,0.0086,0.0704,-0.053,0.0197,0.0152,-0.0802,0.0024,0.0182,-0.0151,-0.0876,0.0443,-0.0373,-0.0261,0.0436,0.0147,-0.0115,-0.035,-0.0486,0.0133,-0.0052,0.0156,-0.0322,-0.0073,0.0554,0.0009,-0.0499,-0.0445,-0.0447,-0.0387,0.0195,-0.081,-0.0002,0.0108,0.0002]}
{"key":"[Action Recognition for American Sign Language] In this research, we present our findings to recognize American Sign Language from series of hand gestures. While most researches in literature focus only on static handshapes, our work target dynamic hand gestures. Since dynamic signs dataset are very few, we collect an initial dataset of 150 videos for 10 signs and an extension of 225 videos for 15 signs. We apply transfer learning models in combination with deep neural networks and background subtraction for videos in different temporal settings. Our primarily results show that we can get an accuracy of $0.86$ and $0.71$ using DenseNet201, LSTM with video sequence of 12 frames accordingly.","layer":5,"vector":[-0.0558,-0.0292,0.0178,-0.0264,-0.0219,0.0723,0.0653,-0.0161,0.0123,-0.0209,-0.0045,-0.0495,-0.0081,0.0077,0.0226,-0.0583,0.0187,0.0381,-0.0455,0.0142,0.0216,0.0101,0.0072,-0.0227,-0.01,0.0203,-0.0167,-0.026,-0.0293,-0.2116,-0.0213,-0.0153,0.0362,-0.011,0.0244,-0.0419,-0.0669,0.0625,-0.0568,0.0473,0.0025,0.0361,0.0083,-0.0496,0.0107,-0.0632,0.0003,-0.0441,-0.0102,-0.0401,0.024,-0.0441,0.0343,0.062,0.0001,0.0254,0.0554,0.0165,0.0427,0.0276,0.0299,0.026,-0.2012,0.0902,-0.0068,0.0219,0.0044,-0.0167,0.0287,0.0153,-0.0438,-0.0059,0.0203,0.0088,-0.0004,0.0361,-0.0347,-0.0845,-0.0114,-0.0542,-0.0022,-0.0352,-0.0431,-0.0248,-0.0429,-0.0354,0.0021,-0.0732,-0.0005,0.0027,-0.0788,-0.0125,-0.0341,0.0256,-0.0593,-0.0454,0.0577,0.022,-0.0698,0.2128,-0.0274,0.024,0.0334,-0.0509,0.0453,-0.0098,-0.0143,-0.0342,-0.0489,0.0153,-0.0303,0.0089,0.0413,-0.0285,0.0467,-0.0347,0.0788,0.0661,-0.0071,-0.043,0.053,-0.012,0.0494,-0.094,-0.0036,-0.093,0.0651,0.1508,0.0619,0.0384,0.0319,-0.0519,-0.0308,-0.0536,0.0484,0.0184,0.0134,0.0263,0.0252,-0.0192,-0.0012,-0.0265,0.0182,-0.0432,-0.0254,0.1048,-0.04,0.0151,-0.0181,0.0364,0.0081,0.0544,-0.0305,0.016,0.0001,0.0257,0.1159,0.0053,-0.0596,0.0053,0.0031,-0.0878,-0.0084,0.058,0.0058,-0.1254,-0.0678,-0.0472,0.0027,-0.0367,0.0478,0.064,0.0078,0.0323,0.0872,0.0771,-0.047,-0.0238,-0.0249,0.009,0.0262,-0.0585,-0.0184,0.0684,0.0002,-0.0103,0.0077,-0.0592,0.0408,0.0545,-0.0135,0.0227,-0.0573,0.01,-0.0192,-0.0001,-0.0153,0.0106,0.0404,-0.0207,0.0182,0.0101,-0.0226,-0.0018,0.0355,0.0285,-0.0207,0.0194,0.0288,0.0473,-0.01,0.0599,0.0569,-0.0355,-0.0435,-0.0151,0.038,0.0168,-0.021,0.0384,0.0174,-0.049,-0.0507,-0.2635,-0.0064,0.0538,0.0062,0.0675,-0.0536,0.0013,-0.0067,0.092,0.0846,0.0321,-0.0112,-0.0367,-0.0016,0.0306,0.0567,0.009,0.0952,-0.0086,-0.0113,-0.0365,-0.0356,-0.0237,-0.0532,0.0244,-0.0243,0.1764,0.0171,0.0217,-0.057,0.0114,0.0307,-0.0284,-0.0622,0.0618,0.017,0.0608,0.0053,-0.0039,-0.0363,-0.043,-0.0167,0.0031,-0.0742,-0.0477,-0.0283,-0.0218,-0.0101,-0.0487,0.0013,0.0868,-0.0336,0.0398,0.0145,-0.0537,-0.0162,-0.0666,0.0299,-0.0308,0.0728,-0.0347,-0.0516,0.0084,-0.0547,0.0806,0.0489,-0.0489,-0.0378,-0.0144,0.0081,-0.0225,0.0613,0.0044,-0.013,0.0811,0.0132,0.0447,-0.0249,-0.0741,-0.0446,0.0435,-0.0169,0.0431,-0.0183,0.0113,0.0435,0.0827,-0.0224,0.0058,-0.0028,0.0566,0.0381,-0.0375,-0.0085,0.0256,0.0129,-0.3,0.0542,-0.0012,0.0298,-0.0288,0.0001,0.0344,-0.0106,-0.0245,0.0129,-0.0095,0.0192,0.0727,-0.018,-0.0094,0.0335,0.0814,-0.0331,0.0307,-0.0971,0.0115,0.035,0.1888,-0.0238,0.0151,-0.0086,-0.0257,-0.0148,0.0757,-0.0614,0.0221,0.0048,0.065,-0.0307,0.0365,0.0497,-0.0373,0.0063,0.018,0.0181,0.0305,0.023,-0.0069,-0.0704,0.0905,-0.0272,0.0245,-0.0461,0.0026,0.074,0.0093,-0.02,-0.0077,0.0285,0.0134,0.0214,-0.0244,-0.0305,-0.0343,-0.0563,0.0124,-0.054,0.0013,-0.0056,0.0224]}
{"key":"[Hierarchies of Relaxations for Online Prediction Problems with Evolving Constraints] We study online prediction where regret of the algorithm is measured against a benchmark defined via evolving constraints. This framework captures online prediction on graphs, as well as other prediction problems with combinatorial structure. A key aspect here is that finding the optimal benchmark predictor (even in hindsight, given all the data) might be computationally hard due to the combinatorial nature of the constraints. Despite this, we provide polynomial-time \\emph{prediction} algorithms that achieve low regret against combinatorial benchmark sets. We do so by building improper learning algorithms based on two ideas that work together. The first is to alleviate part of the computational burden through random playout, and the second is to employ Lasserre semidefinite hierarchies to approximate the resulting integer program. Interestingly, for our prediction algorithms, we only need to compute the values of the semidefinite programs and not the rounded solutions. However, the integrality gap for Lasserre hierarchy \\emph{does} enter the generic regret bound in terms of Rademacher complexity of the benchmark set. This establishes a trade-off between the computation time and the regret bound of the algorithm.","layer":2,"vector":[-0.0701,-0.0326,0.0547,0.0098,0.0243,0.0332,-0.0154,0.0344,0.0541,-0.0098,0.0216,-0.0178,0.0206,0.039,0.0061,0.0062,-0.0037,0.0236,-0.0472,0.0329,0.0343,-0.0149,-0.0228,-0.1051,0.0546,0.0255,-0.0232,-0.0379,-0.0338,-0.2101,0.0023,-0.0692,0.0484,-0.0088,0.0075,-0.0341,0.0179,0.0668,-0.0396,0.0989,0.0097,0.0516,-0.0317,-0.06,-0.0284,-0.055,0.011,-0.014,-0.038,-0.024,-0.0253,-0.038,0.0495,0.0234,0.0426,0.061,0.0052,0.0424,-0.0054,0.0461,0.0317,0.0367,-0.1276,0.0177,0.0304,0.0377,-0.0881,-0.0049,0.0345,0.112,0.0548,0.0362,0.0099,0.0498,0.021,0.0166,0.0182,-0.0,-0.0139,0.0131,0.014,-0.054,-0.0649,0.0138,-0.0201,-0.0953,0.0245,-0.0269,0.0542,0.0238,-0.0453,-0.0041,0.0151,-0.0102,-0.0625,0.0192,0.0432,0.0381,-0.0631,0.2174,-0.0566,0.0925,-0.0099,-0.0384,0.0044,-0.0627,-0.0404,-0.0305,-0.0069,-0.0588,-0.0307,-0.0372,0.0724,-0.0479,0.0053,0.0128,0.0729,0.0711,-0.0145,-0.0052,-0.0516,0.0133,0.0865,-0.0304,0.0148,-0.0471,-0.0119,0.1298,0.0192,0.0263,0.0204,-0.0348,-0.0099,-0.0478,0.03,-0.006,0.047,0.0083,0.03,-0.0513,-0.0622,-0.0356,0.0164,-0.0774,-0.0451,0.1677,-0.0206,0.0405,-0.0138,-0.0396,-0.0326,-0.0055,-0.0217,-0.0397,-0.0151,0.0096,0.035,0.0405,-0.045,0.0087,-0.0272,-0.0108,-0.0429,0.0687,0.0027,-0.0525,-0.028,0.0057,0.0001,0.0056,0.0531,0.001,-0.0247,0.0145,0.063,0.0256,-0.0923,-0.0167,0.0272,-0.0047,0.0074,-0.0033,-0.0423,0.0333,0.029,-0.0263,0.0212,-0.0183,0.0323,0.0259,-0.0269,0.014,-0.0257,-0.0037,0.0186,-0.0141,0.0011,-0.0266,0.0327,-0.0216,0.0138,-0.0111,-0.0101,0.0246,0.0132,0.0171,-0.0016,0.016,0.044,0.0138,-0.0552,-0.0293,0.049,-0.0156,-0.0548,0.0128,0.0757,0.0194,-0.0208,0.0555,0.0339,-0.0335,-0.0256,-0.2257,-0.0161,-0.0348,-0.0142,0.0427,-0.0634,0.071,-0.0299,0.0384,0.0783,0.0231,-0.0429,-0.0245,0.021,0.0105,0.0101,0.0352,0.0025,-0.0356,0.017,-0.0432,0.005,-0.0158,-0.0779,0.0286,0.0337,0.231,0.0176,0.0365,-0.052,0.039,0.0159,-0.025,-0.0668,0.0544,0.0213,0.0503,-0.0404,-0.045,-0.0374,-0.0056,0.0143,-0.0014,-0.1022,-0.0448,0.0024,-0.031,0.0066,-0.0562,0.0319,0.0359,-0.0269,0.0884,-0.0035,-0.0147,-0.0353,-0.1009,0.0283,-0.0203,0.0657,0.0486,-0.0447,-0.0316,-0.0458,0.0571,-0.0071,0.0218,-0.0565,0.0075,-0.0328,-0.024,0.0454,-0.0015,0.0018,0.045,0.017,-0.0046,-0.0341,-0.0211,-0.0026,0.0579,-0.0888,0.0281,0.0224,-0.0162,-0.0171,0.0888,-0.0091,0.001,0.0146,0.0218,-0.0052,-0.0637,0.0046,0.0579,0.0148,-0.3267,0.028,-0.0052,0.0125,-0.0108,0.0104,0.0592,0.0235,-0.029,0.004,0.0336,0.0372,0.0238,0.0111,0.005,0.0558,0.0686,-0.0463,0.012,-0.0606,0.0188,0.0328,0.2496,-0.0729,0.0529,0.0258,-0.0385,-0.0074,0.0455,-0.0344,0.0051,0.009,0.0752,-0.0852,0.0459,0.0626,-0.0342,0.0263,0.0208,-0.0058,-0.0176,0.0069,-0.0359,0.005,0.0851,0.0144,-0.0675,-0.0279,-0.0041,0.0006,-0.0429,0.0273,-0.0243,-0.0173,-0.0304,-0.016,-0.0212,-0.0624,-0.057,-0.0335,0.0574,-0.0101,-0.0005,-0.005,-0.016]}
{"key":"[Likelihood-ratio calibration using prior-weighted proper scoring rules] Prior-weighted logistic regression has become a standard tool for calibration in speaker recognition. Logistic regression is the optimization of the expected value of the logarithmic scoring rule. We generalize this via a parametric family of proper scoring rules. Our theoretical analysis shows how different members of this family induce different relative weightings over a spectrum of applications of which the decision thresholds range from low to high. Special attention is given to the interaction between prior weighting and proper scoring rule parameters. Experiments on NIST SRE'12 suggest that for applications with low false-alarm rate requirements, scoring rules tailored to emphasize higher score thresholds may give better accuracy than logistic regression.","layer":3,"vector":[-0.0195,0.0021,0.0347,-0.102,0.0491,0.0259,0.0557,0.0323,0.0379,-0.0186,-0.0117,-0.0371,0.0003,0.0514,0.0438,0.0088,0.0404,0.053,-0.0253,0.0224,0.0394,0.01,-0.0157,-0.0121,0.058,0.0368,-0.066,-0.074,-0.0287,-0.2487,0.0042,-0.0593,0.0652,-0.0769,-0.0222,-0.0149,-0.0617,0.0587,-0.0347,0.0401,0.0219,0.0224,0.0039,-0.0676,-0.0252,-0.0925,-0.0398,0.0104,-0.0407,-0.0187,0.0435,-0.0601,-0.0074,0.0266,-0.0217,0.0468,0.0615,0.0507,0.069,0.0458,0.022,0.0379,-0.1959,0.0514,0.0184,0.0081,-0.0581,-0.06,0.0042,0.0452,-0.0242,0.0525,0.0255,0.0358,0.0005,-0.0175,0.0132,-0.0521,-0.0007,0.0416,0.0234,-0.0078,-0.0248,-0.0076,-0.0052,-0.0168,0.0262,-0.0115,0.0094,-0.0013,-0.0628,0.0002,-0.0721,0.039,-0.0958,-0.0495,0.0228,0.0134,-0.0484,0.1975,-0.007,0.026,0.0092,-0.0347,0.0403,-0.0476,-0.0629,-0.0431,0.0002,0.0011,0.0094,-0.0137,0.0411,-0.041,0.0312,0.0567,0.0563,0.073,0.0318,-0.0295,0.0156,-0.002,0.0394,-0.0341,0.0583,-0.0636,0.0242,0.1091,0.0218,0.0351,0.0454,-0.0975,-0.0644,-0.0099,-0.0066,0.0279,0.0424,0.0113,0.0332,-0.0259,-0.0047,-0.0837,-0.0069,-0.0391,-0.0845,0.1401,-0.108,0.0314,-0.0584,-0.0241,0.024,0.0272,0.0053,-0.0327,0.0296,0.0318,0.0367,0.0174,-0.0378,-0.0113,0.0158,-0.0836,0.0042,0.0515,-0.0044,-0.092,-0.0112,-0.0,0.019,-0.0067,0.0466,0.0116,-0.0314,0.0264,0.0228,-0.0015,-0.0635,0.0301,-0.0348,0.0133,0.0286,-0.0441,-0.0799,0.026,0.0611,-0.0477,-0.0503,-0.0827,0.0507,0.0826,-0.0258,-0.0127,-0.0623,-0.0052,-0.0049,-0.0223,-0.0083,-0.0137,0.0076,-0.0402,0.003,0.0428,-0.0218,-0.0145,0.0167,0.0315,-0.0117,-0.0106,0.041,0.0292,-0.0254,-0.0218,0.0823,-0.0111,-0.0115,-0.0395,0.0376,0.0456,0.0106,0.0381,-0.0037,-0.0366,-0.0551,-0.2301,0.0084,0.03,-0.0157,0.0686,-0.0566,0.0406,0.0095,0.0078,0.0757,0.0285,-0.0147,0.0187,0.0259,-0.0433,0.0404,-0.0126,0.011,-0.0147,0.0304,-0.0008,0.0218,-0.047,-0.0586,0.0518,0.0009,0.1705,0.037,0.0249,-0.0152,0.0137,-0.0229,-0.0119,-0.0704,0.0938,0.0347,0.0441,-0.0123,-0.0398,-0.0135,-0.0242,-0.0127,0.0085,-0.0845,-0.0731,-0.0224,-0.0726,0.0379,-0.0665,0.0209,0.0694,0.0168,0.0315,0.0185,-0.0137,-0.0414,-0.0885,0.0328,-0.0466,0.0207,0.0152,-0.0618,0.0235,-0.0472,0.0276,-0.0482,-0.047,-0.0396,0.0196,-0.0202,0.0045,0.0746,-0.0056,0.0103,0.0776,0.0286,0.0264,-0.0735,-0.0248,-0.0586,0.0417,-0.002,0.0462,-0.0063,0.0246,0.0104,0.0961,0.0266,0.0233,0.0448,-0.0032,0.0308,-0.011,0.0156,0.0429,0.0274,-0.3038,0.0032,0.0146,0.0005,-0.0106,0.0038,0.0709,-0.0269,-0.06,0.0169,0.0247,0.0585,0.0329,0.0005,-0.0028,0.0333,0.0252,-0.0457,0.0843,-0.0509,-0.0027,0.0186,0.1902,-0.039,0.0168,0.0219,0.0006,0.0211,0.0676,-0.0354,0.0494,-0.0044,0.1128,-0.0362,0.0322,0.0728,-0.0326,-0.0207,0.0127,-0.0366,0.0046,0.0079,-0.0436,-0.0534,0.1225,0.0006,-0.0165,-0.0251,0.0427,0.0389,-0.0045,0.0034,-0.0049,-0.0048,0.0307,0.0172,-0.026,-0.0078,-0.0219,-0.0537,-0.0084,-0.0196,-0.0135,-0.0028,0.0092]}
{"key":"[Bag Graph: Multiple Instance Learning using Bayesian Graph Neural Networks] Multiple Instance Learning (MIL) is a weakly supervised learning problem where the aim is to assign labels to sets or bags of instances, as opposed to traditional supervised learning where each instance is assumed to be independent and identically distributed (IID) and is to be labeled individually. Recent work has shown promising results for neural network models in the MIL setting. Instead of focusing on each instance, these models are trained in an end-to-end fashion to learn effective bag-level representations by suitably combining permutation invariant pooling techniques with neural architectures. In this paper, we consider modelling the interactions between bags using a graph and employ Graph Neural Networks (GNNs) to facilitate end-to-end learning. Since a meaningful graph representing dependencies between bags is rarely available, we propose to use a Bayesian GNN framework that can generate a likely graph structure for scenarios where there is uncertainty in the graph or when no graph is available. Empirical results demonstrate the efficacy of the proposed technique for several MIL benchmark tasks and a distribution regression task.","layer":2,"vector":[0.0005,-0.0175,-0.0157,-0.0224,0.0497,0.0465,0.0505,0.0065,0.0161,-0.0237,-0.0061,-0.0661,0.018,0.0897,0.0448,0.0385,-0.0057,0.0381,-0.0493,-0.0601,0.0132,-0.0326,-0.0313,-0.0367,0.0298,0.0488,-0.0212,-0.0348,-0.0502,-0.2412,0.0215,-0.0467,0.0359,-0.0038,-0.0163,-0.0177,0.0303,0.0471,-0.0326,0.0601,0.0215,0.0222,-0.0219,-0.0552,0.0084,-0.023,-0.0098,-0.0204,-0.017,-0.0397,0.0915,-0.0301,0.0273,0.0318,0.0031,0.0357,0.0383,0.0601,0.0059,0.068,0.0249,0.0286,-0.1489,0.0457,0.0634,0.055,-0.0801,-0.0053,-0.0063,0.0732,-0.0021,0.0353,0.0322,0.0522,0.0176,0.0231,0.0458,-0.0069,-0.0201,-0.0108,-0.0325,-0.01,-0.0245,-0.0147,-0.0347,-0.026,-0.0198,-0.0288,0.0441,0.0179,-0.07,0.0155,-0.015,0.027,-0.0834,-0.043,0.0341,0.0126,-0.0496,0.2113,-0.0169,-0.0004,0.0669,0.0181,0.0128,-0.0292,-0.0308,-0.0676,-0.0382,-0.0139,-0.022,-0.0304,0.0072,-0.0792,-0.0016,0.0519,0.078,0.0381,-0.0322,0.0132,-0.0459,0.001,0.0706,-0.0104,0.0055,-0.0674,0.0002,0.1353,0.034,-0.0157,0.0685,-0.0105,-0.0366,-0.0305,0.0396,-0.0065,0.0578,-0.0159,0.0384,-0.0079,-0.0412,-0.0295,0.0106,-0.0509,-0.0795,0.0906,-0.015,0.0202,-0.047,-0.0509,-0.0157,0.0242,0.0032,-0.0195,-0.0149,0.0491,0.0609,0.0371,-0.0166,0.011,-0.0347,-0.0434,-0.0585,0.0984,0.0407,-0.0938,-0.0423,-0.01,0.0047,-0.0233,0.0316,0.0346,0.0083,0.0095,0.0782,0.0343,-0.071,0.0004,0.0124,0.0059,0.0076,-0.0176,-0.0534,0.0134,0.0251,-0.0396,0.0024,-0.0577,-0.0082,0.0364,-0.0319,0.04,0.0113,-0.001,-0.0525,0.0004,-0.0179,-0.0322,0.0111,-0.0122,0.0201,-0.041,-0.0503,0.026,-0.0324,0.0239,0.0047,0.005,0.0225,-0.0136,-0.0053,0.0167,0.0172,-0.0465,0.0116,-0.0328,0.0456,0.0731,0.0431,0.0351,0.0353,-0.0706,-0.0228,-0.2162,0.0124,0.0275,-0.0332,0.08,-0.0418,0.0363,0.0335,0.0225,0.0815,0.0697,-0.0238,-0.0474,0.0286,0.0157,0.0431,0.0216,0.0409,-0.0274,-0.0361,-0.0362,0.0172,-0.0015,-0.0868,0.0369,0.0052,0.2468,0.0325,0.021,-0.0281,0.0006,0.0532,-0.0648,-0.0518,0.0607,0.0366,0.0623,-0.0095,-0.0068,-0.0218,-0.0515,-0.0058,0.0045,-0.1036,-0.0758,-0.025,-0.0177,0.0235,-0.0802,0.022,0.005,-0.0226,0.0429,-0.0403,-0.0192,-0.0417,-0.0652,0.0427,-0.0372,0.0265,0.025,-0.0798,0.05,-0.0732,0.0617,-0.0056,-0.0284,-0.0404,0.0043,-0.0159,-0.0433,0.0857,0.0223,-0.0142,0.0646,0.0066,-0.0142,-0.0005,-0.0731,-0.0284,0.0488,-0.0465,0.0294,0.0315,0.0267,0.006,0.0435,0.0246,0.0643,-0.0074,0.0299,0.0216,-0.0196,0.015,0.0442,-0.0023,-0.285,0.0343,0.0073,0.0535,-0.0504,0.0714,0.0674,0.0288,-0.0285,-0.0356,0.053,0.0424,0.0345,-0.0147,-0.0417,0.0345,0.0706,-0.0478,0.0313,-0.0993,0.0057,0.0405,0.202,-0.018,0.0536,0.0371,-0.0445,-0.0175,0.0193,-0.0048,0.0261,-0.003,0.0834,-0.0733,0.0322,0.0853,-0.047,0.0281,-0.0002,-0.0087,0.0152,-0.015,-0.0888,-0.0825,0.0917,0.0155,-0.01,-0.0458,-0.0282,-0.0055,0.0209,0.0273,-0.0274,0.0092,0.0057,0.011,0.0012,-0.051,-0.0693,-0.0756,0.0246,-0.086,-0.0113,-0.0428,-0.0149]}
{"key":"[Hierarchical Expert Networks for Meta-Learning] The goal of meta-learning is to train a model on a variety of learning tasks, such that it can adapt to new problems within only a few iterations. Here we propose a principled information-theoretic model that optimally partitions the underlying problem space such that specialized expert decision-makers solve the resulting sub-problems. To drive this specialization we impose the same kind of information processing constraints both on the partitioning and the expert decision-makers. We argue that this specialization leads to efficient adaptation to new tasks. To demonstrate the generality of our approach we evaluate three meta-learning domains: image classification, regression, and reinforcement learning.","layer":5,"vector":[-0.0314,-0.0128,0.0264,0.0023,0.0368,0.007,0.0316,0.0387,0.0192,0.0001,0.0028,-0.0636,0.0087,0.0694,0.0444,0.0017,0.0323,0.0433,-0.021,-0.0117,0.0339,-0.0415,-0.0227,-0.0701,-0.0101,0.0018,-0.0338,-0.0332,-0.0757,-0.1947,0.0345,-0.0396,0.0406,0.0206,0.0234,0.0017,-0.0343,0.0469,-0.0162,0.0558,0.0315,0.032,-0.0248,-0.084,-0.0044,-0.0215,-0.0247,-0.0245,-0.0321,-0.0176,-0.0086,-0.0164,-0.0138,0.0011,0.0182,0.0263,0.0597,0.0786,0.0553,0.0713,-0.0035,0.0492,-0.142,0.0927,0.0108,0.0642,-0.0363,-0.0073,0.0222,0.0374,0.0012,0.0247,-0.0388,0.0454,0.0051,0.0188,-0.0251,-0.0237,0.0134,-0.0139,0.0314,-0.0598,-0.0469,-0.0262,0.0049,-0.055,-0.0013,-0.0419,0.0576,0.0018,-0.0224,-0.0033,-0.0328,-0.0192,-0.0556,-0.0051,0.0498,0.0103,-0.0486,0.1996,-0.0422,0.0271,0.0511,-0.0241,0.0481,-0.0744,-0.0611,0.0063,-0.0478,-0.0266,-0.0173,0.0145,0.0107,-0.0196,-0.0069,0.0165,0.0331,0.0331,-0.0114,-0.0233,-0.0083,-0.0077,0.0693,-0.0341,0.0426,-0.0769,-0.0182,0.1483,0.0266,0.0197,0.0624,-0.0276,-0.0497,-0.0231,0.0289,0.0373,0.0366,-0.0393,0.0038,0.044,-0.0696,-0.0425,0.0064,-0.0703,-0.1052,0.1197,-0.0494,0.0209,-0.0197,-0.0443,-0.0297,0.0022,-0.0541,-0.0425,-0.0162,0.0174,0.053,0.041,-0.0731,0.0168,-0.0175,-0.0425,-0.0245,0.1205,-0.021,-0.0506,-0.0618,-0.0483,0.0195,-0.0163,0.0416,0.051,0.0366,0.0209,0.0903,0.0168,-0.0726,-0.0117,0.015,0.0007,0.0405,-0.0703,-0.0047,0.0397,0.0463,0.0082,0.0358,-0.0664,0.0331,0.0477,-0.0329,0.0881,0.0115,0.0148,-0.0064,-0.0427,0.0058,0.004,-0.0387,-0.0318,-0.0316,0.0139,-0.0405,0.0314,-0.0471,0.0414,-0.0446,-0.0087,0.0548,-0.025,-0.023,0.0015,0.0394,-0.0126,-0.04,0.01,0.0644,0.0453,-0.0002,0.0562,0.0606,-0.0264,-0.047,-0.2286,-0.0283,0.0246,-0.0036,0.0222,-0.0335,0.0251,-0.0258,-0.0061,0.0844,0.0571,-0.0291,-0.0236,0.0206,0.0169,0.0303,0.0548,0.0119,-0.0067,-0.0132,-0.0418,0.0365,0.0128,-0.1251,0.0447,0.0204,0.206,0.041,0.0481,-0.0007,0.0578,0.0219,-0.0577,-0.0882,0.0422,0.0346,0.0799,-0.0596,-0.0335,-0.0699,-0.0167,0.0195,0.0049,-0.1058,-0.0355,-0.0167,-0.0093,0.0367,-0.0426,-0.0021,0.0596,-0.0117,0.0148,-0.0365,-0.0466,-0.0141,-0.0631,0.0424,-0.0297,0.0363,0.0576,-0.0289,0.0245,-0.0638,0.0623,0.0005,-0.0099,-0.0548,0.0347,-0.0405,-0.0331,0.0896,-0.0142,0.0032,0.0236,0.0321,0.003,-0.0281,-0.04,-0.0305,0.0533,-0.0372,0.0414,0.0281,0.0337,-0.0142,0.084,-0.0617,0.0092,-0.0157,0.0173,0.0085,-0.0946,-0.0091,0.0511,0.0086,-0.3176,0.1058,0.0256,0.0026,-0.01,0.0457,0.0392,-0.0032,-0.0225,-0.0086,0.0379,0.0255,0.0204,0.022,0.0256,0.0102,0.0723,-0.0162,0.0357,-0.063,0.0132,0.0578,0.2156,-0.0571,0.0421,0.0011,-0.0155,-0.0288,-0.0131,-0.0173,0.0177,0.0166,0.0554,-0.0309,0.0901,0.0851,-0.076,0.026,-0.0093,-0.0063,-0.0314,-0.0309,-0.032,-0.0344,0.1035,-0.004,-0.0113,-0.0218,-0.0411,0.0127,0.0008,0.0216,-0.0122,-0.0371,0.0044,-0.0117,0.0001,-0.0538,-0.0733,-0.0399,0.0581,-0.0511,0.0218,0.0063,0.0075]}
{"key":"[Mix and Match: Learning-free Controllable Text Generation using Energy Language Models] Recent work on controlled text generation has either required attribute-based fine-tuning of the base language model (LM), or has restricted the parameterization of the attribute discriminator to be compatible with the base autoregressive LM. In this work, we propose Mix and Match LM, a global score-based alternative for controllable text generation that combines arbitrary pre-trained black-box models for achieving the desired attributes in the generated text without involving any fine-tuning or structural assumptions about the black-box models. We interpret the task of controllable generation as drawing samples from an energy-based model whose energy values are a linear combination of scores from black-box models that are separately responsible for fluency, the control attribute, and faithfulness to any conditioning context. We use a Metropolis-Hastings sampling scheme to sample from this energy-based model using bidirectional context and global attribute features. We validate the effectiveness of our approach on various controlled generation and style-based text revision tasks by outperforming recently proposed methods that involve extra training, fine-tuning, or restrictive assumptions over the form of models.","layer":1,"vector":[-0.0439,0.0373,-0.0076,0.0272,0.0074,0.019,-0.0042,0.0224,-0.0191,-0.0112,0.0154,-0.0161,0.0232,0.0493,0.0228,0.0295,0.013,0.0052,-0.0409,0.0009,0.0364,-0.0071,0.0115,-0.0081,0.0289,0.0222,-0.0007,-0.0207,-0.0327,-0.2603,-0.0043,-0.0077,0.0389,-0.001,-0.0291,0.0034,-0.0847,0.0162,-0.0192,0.023,0.0222,-0.0232,-0.0005,-0.037,0.0024,-0.0552,-0.0328,0.0047,-0.0391,-0.0224,0.0195,-0.0466,0.011,0.0447,0.058,0.0639,0.0554,0.0147,0.0133,0.0487,-0.0085,0.0819,-0.2029,0.0714,0.0368,0.0351,-0.0514,0.0231,-0.0156,0.0822,-0.0388,0.0291,0.0097,0.0658,0.0177,0.0149,-0.0491,-0.0402,-0.0044,0.0409,-0.004,-0.0441,-0.0536,-0.0067,-0.0197,-0.0682,0.0212,-0.0187,0.0444,0.0135,-0.0449,-0.0382,0.0002,0.0378,-0.0286,-0.0461,0.0375,0.0353,-0.0422,0.1997,-0.0378,0.0244,0.0322,-0.0445,0.0183,-0.0389,0.0058,-0.0046,-0.0406,-0.0566,-0.0159,-0.0102,-0.0011,-0.0355,0.0717,0.0025,0.0994,0.0194,-0.0001,0.0063,-0.0241,0.0263,0.0296,-0.0485,0.0414,-0.0583,0.0583,0.1405,0.0447,-0.0068,0.0169,0.0031,-0.069,-0.0589,0.0035,-0.0251,-0.0212,0.004,0.0283,-0.0141,-0.0269,-0.054,-0.0198,-0.0903,-0.0586,0.1559,-0.0215,0.0514,-0.0732,0.0076,0.0087,0.0019,0.0175,-0.0283,0.0537,0.0555,0.0473,0.0365,-0.0278,0.0017,0.0399,0.0163,-0.052,0.0724,0.0192,-0.0913,-0.0257,-0.0047,-0.0128,-0.0118,0.0679,0.0204,-0.0556,0.0207,0.0745,0.0544,-0.0735,0.0192,0.0158,0.0339,0.0426,-0.0454,-0.0772,0.0965,0.004,-0.104,-0.0027,-0.0452,0.0414,0.0402,-0.0203,0.0197,0.0205,-0.0138,-0.0225,-0.0133,-0.0208,-0.0254,0.005,-0.0819,-0.0218,0.0372,-0.0419,-0.0596,0.0326,-0.0115,-0.0085,0.0226,0.0697,0.0381,-0.0317,-0.0242,0.0528,0.0015,-0.0418,-0.0131,-0.0013,-0.0202,0.018,0.0598,0.0207,-0.0154,-0.0152,-0.2274,0.0124,0.0369,-0.0223,0.0802,-0.0481,0.0166,-0.0033,0.0413,0.068,0.0027,-0.0277,-0.009,0.0147,-0.0267,0.025,0.018,0.0083,0.0099,0.0074,0.0032,-0.02,-0.0425,-0.0769,0.0715,-0.0234,0.2,0.0399,0.0493,-0.0286,0.0425,0.0456,-0.0205,-0.113,0.0724,0.0373,0.0653,0.0189,-0.061,-0.0253,0.0065,0.0125,-0.0062,-0.1215,-0.0278,-0.0558,-0.0649,-0.0415,-0.074,0.0493,0.0169,-0.0128,0.0621,-0.0059,-0.0475,-0.025,-0.0877,0.0239,-0.037,-0.0157,-0.0138,-0.0287,0.0131,-0.0575,0.0178,0.0074,-0.0206,-0.0627,0.0153,-0.012,-0.0114,0.0649,-0.0113,0.0218,0.0412,0.0183,-0.0202,-0.0405,-0.0594,-0.0182,0.0717,-0.0023,0.0457,0.0287,-0.011,-0.0095,0.0546,-0.0295,0.0695,-0.0022,-0.0281,0.0269,-0.0289,-0.0093,0.0341,0.0024,-0.2921,0.0461,-0.0021,0.0314,-0.0239,0.0097,0.0752,0.0258,-0.0614,0.0286,0.0003,0.0052,-0.002,-0.0513,0.0167,0.0262,0.0792,-0.0719,0.0639,-0.0339,0.0303,0.0376,0.2058,-0.0239,0.0242,-0.0066,0.0034,0.0153,0.0718,0.0189,0.0108,0.0196,0.1182,0.0284,0.0348,0.0735,-0.0289,0.0195,-0.0001,-0.0448,-0.0266,0.0371,-0.0498,-0.0486,0.0355,-0.0005,-0.004,-0.0316,-0.007,-0.02,-0.0033,0.042,-0.0051,-0.001,0.012,0.0188,-0.0868,-0.0381,-0.0283,-0.0374,-0.015,-0.0332,-0.0061,0.0722,-0.0359]}
{"key":"[Riemannian Metric Learning via Optimal Transport] We introduce an optimal transport-based model for learning a metric tensor from cross-sectional samples of evolving probability measures on a common Riemannian manifold. We neurally parametrize the metric as a spatially-varying matrix field and efficiently optimize our model's objective using backpropagation. Using this learned metric, we can nonlinearly interpolate between probability measures and compute geodesics on the manifold. We show that metrics learned using our method improve the quality of trajectory inference on scRNA and bird migration data at the cost of little additional cross-sectional data.","layer":3,"vector":[-0.0177,-0.0785,0.0484,-0.0064,-0.0036,0.0379,0.0132,0.0455,0.0127,-0.0177,0.009,-0.0863,0.0059,0.0694,-0.0098,0.0281,-0.0097,0.0905,-0.05,0.0225,0.0416,-0.0562,-0.0101,-0.0422,0.0354,0.0249,-0.0251,-0.0182,-0.074,-0.2393,0.0245,-0.0565,0.0068,-0.0474,-0.0118,-0.0095,-0.0096,0.0467,-0.0139,0.0516,0.0388,0.0435,-0.0188,-0.0268,-0.0082,-0.0542,0.0033,-0.0006,-0.0203,-0.0152,-0.0117,-0.0425,-0.0172,0.0394,0.0446,0.0574,0.0656,0.0023,0.0773,0.033,-0.0047,0.0075,-0.1652,0.0634,0.0243,0.0145,-0.0594,-0.0144,0.0179,0.0438,-0.0443,0.018,-0.034,0.0394,0.0441,-0.0074,-0.0093,0.0027,-0.0315,0.0084,0.0133,0.0072,-0.0553,-0.0065,0.0042,-0.0323,0.0204,-0.05,0.032,0.0018,-0.0287,-0.0233,-0.04,0.043,-0.0666,-0.0173,0.0414,0.0524,0.0188,0.1991,-0.0282,0.0536,0.0134,0.0331,0.0182,-0.046,-0.0062,-0.0351,-0.0032,-0.0059,0.0239,-0.0088,-0.0062,0.0059,-0.0043,0.0212,0.0395,0.0456,-0.0338,0.0278,-0.0922,-0.0047,0.089,-0.0521,-0.0026,-0.0767,-0.0042,0.1352,0.009,0.0408,0.064,0.0107,-0.0361,-0.0028,0.0177,0.0283,0.0209,-0.0113,-0.0144,0.0254,-0.0512,-0.0547,0.0347,-0.0961,-0.0604,0.173,-0.0262,-0.0161,-0.0302,-0.0293,-0.0108,0.0028,-0.0503,-0.042,-0.0106,0.0272,-0.0053,0.029,-0.0804,0.0245,-0.0585,-0.0836,-0.0056,0.1254,0.0076,-0.038,-0.0184,0.0117,0.0397,-0.0191,0.0369,0.0275,-0.0183,0.0275,0.0829,0.0355,-0.1025,-0.0047,-0.005,-0.0101,0.029,-0.0688,-0.0495,0.0022,0.0486,-0.0119,-0.0187,-0.0323,0.0184,0.0481,0.0294,0.0341,-0.0024,-0.0467,-0.0451,-0.033,-0.0303,0.0119,0.0224,-0.0288,0.0037,-0.038,-0.0195,0.0116,0.0056,0.0135,0.0004,-0.0014,-0.0002,0.0602,-0.0086,-0.0584,0.0612,-0.0315,-0.0297,-0.0026,-0.0016,-0.0047,0.0126,0.031,0.0011,-0.0451,-0.0402,-0.2178,0.0008,0.0167,-0.0019,0.0862,-0.0691,0.0691,-0.009,0.0655,0.0794,0.0668,-0.0271,0.0107,0.0431,0.0066,0.044,-0.0096,0.0483,-0.0386,-0.019,0.0147,0.0185,-0.0381,-0.0918,0.0569,-0.0302,0.2317,0.0608,0.0586,-0.0504,0.0044,0.0136,0.022,-0.0771,0.0707,0.0146,0.0772,-0.0273,-0.0573,-0.0391,-0.0369,0.0361,0.0178,-0.0736,-0.0337,-0.045,-0.0493,-0.0016,-0.0316,0.0093,0.0521,-0.0211,0.0556,-0.0507,-0.0252,-0.0451,-0.0314,-0.0091,-0.0382,0.06,-0.0149,-0.0467,0.0038,-0.0473,0.068,-0.0098,-0.0242,-0.052,0.019,-0.0065,-0.0077,0.0753,-0.0169,-0.0138,0.0866,0.019,0.0266,0.0161,-0.0151,-0.0354,0.0775,-0.0454,0.0211,0.0447,0.0536,0.0011,0.0841,-0.0337,0.0099,-0.0355,0.0156,-0.0048,-0.0677,-0.0173,0.0298,0.0034,-0.2826,0.0203,0.0152,0.0279,-0.0323,-0.0061,0.0534,-0.0069,-0.0146,-0.0625,0.0403,0.0159,0.0278,0.0106,0.0305,0.0402,0.0704,-0.045,0.0254,-0.0805,-0.004,0.0271,0.219,0.0111,0.0414,0.0161,-0.0161,0.0165,0.0668,-0.0448,0.0088,0.0211,0.0741,-0.0735,0.0558,0.0832,-0.0264,0.0335,-0.0029,-0.0275,0.0516,-0.0079,0.0112,0.0177,0.0794,0.0053,-0.0381,-0.0521,-0.013,0.0203,0.0119,0.0291,-0.0176,0.0083,-0.0113,0.0156,-0.0114,-0.0671,-0.0217,-0.0505,0.0073,-0.1105,-0.0143,-0.0547,0.0077]}
{"key":"[Explaining Latent Factor Models for Recommendation with Influence Functions] Latent factor models (LFMs) such as matrix factorization achieve the state-of-the-art performance among various Collaborative Filtering (CF) approaches for recommendation. Despite the high recommendation accuracy of LFMs, a critical issue to be resolved is the lack of explainability. Extensive efforts have been made in the literature to incorporate explainability into LFMs. However, they either rely on auxiliary information which may not be available in practice, or fail to provide easy-to-understand explanations. In this paper, we propose a fast influence analysis method named FIA, which successfully enforces explicit neighbor-style explanations to LFMs with the technique of influence functions stemmed from robust statistics. We first describe how to employ influence functions to LFMs to deliver neighbor-style explanations. Then we develop a novel influence computation algorithm for matrix factorization with high efficiency. We further extend it to the more general neural collaborative filtering and introduce an approximation algorithm to accelerate influence analysis over neural network models. Experimental results on real datasets demonstrate the correctness, efficiency and usefulness of our proposed method.","layer":0,"vector":[-0.0149,-0.003,-0.0231,-0.0213,0.0316,0.0298,0.0646,0.0355,0.0132,-0.0413,0.013,-0.046,0.0308,0.0468,0.0282,0.0084,0.014,0.0314,-0.0517,-0.0365,0.0228,-0.0095,-0.0057,-0.0482,0.0069,0.0362,-0.0497,-0.0319,-0.0647,-0.2126,-0.0092,-0.0464,0.0991,-0.0244,0.0035,-0.0078,-0.0013,-0.0078,-0.036,0.0119,-0.0119,0.0464,-0.0,-0.0045,-0.0499,-0.0257,0.0221,-0.0295,-0.0437,-0.0166,0.0483,-0.0356,0.0272,0.0156,0.0756,0.0498,0.0641,0.0583,0.0067,0.0758,0.0642,0.0538,-0.1648,0.0554,0.0326,0.0086,-0.0215,0.0002,-0.0212,0.0225,-0.0259,0.079,-0.0191,0.0383,-0.02,0.0105,-0.005,-0.0103,-0.0239,0.0275,0.0193,-0.0127,-0.0876,-0.0675,0.033,-0.0132,0.0295,-0.0568,-0.0043,-0.0186,-0.0282,-0.0312,-0.0432,0.0111,-0.0647,-0.0571,0.0594,-0.0151,-0.079,0.2115,-0.0528,0.0516,0.0387,-0.0049,0.003,-0.0239,0.0027,-0.0095,0.0195,0.028,-0.0197,0.0152,0.0411,-0.0718,0.0675,0.0131,0.0833,0.0427,0.0203,-0.0237,-0.0474,0.0069,0.0569,0.012,0.0587,-0.0735,-0.0222,0.1219,0.0404,0.0089,0.0475,-0.0188,-0.0924,0.0179,0.0285,0.0339,-0.039,-0.0048,0.0132,0.0307,-0.0106,-0.047,-0.0043,-0.0555,-0.0463,0.0869,-0.039,-0.0499,-0.032,-0.0337,-0.0265,-0.017,-0.0475,-0.0239,0.0456,0.0302,0.0633,0.0193,-0.0677,0.0166,-0.0168,-0.0656,-0.004,0.0665,0.0161,-0.0882,-0.0033,0.0028,0.0058,-0.0123,0.0392,0.0237,-0.0599,0.0262,0.0784,0.0414,-0.0474,0.0051,0.0352,-0.0035,0.0117,-0.0552,-0.0573,0.0381,0.0077,-0.0215,-0.0135,-0.0341,-0.0044,0.0124,-0.0502,0.0042,-0.0051,0.0103,0.0084,-0.0129,-0.0214,-0.0093,0.0005,-0.0508,0.0021,0.0159,-0.0358,0.0345,-0.0557,0.0795,-0.0264,-0.0227,0.05,0.0239,-0.0289,0.0078,0.0438,-0.0114,-0.0161,0.0173,0.0304,0.0289,0.0301,0.0678,0.0669,-0.0348,-0.0687,-0.2416,0.0118,-0.004,-0.0004,0.0568,-0.0947,0.0408,-0.0431,0.0166,0.1545,0.04,0.007,-0.0203,0.0259,-0.0006,0.0764,0.034,0.0267,-0.0214,-0.0163,-0.0293,0.0438,-0.0023,-0.0643,0.0179,0.0337,0.2147,0.0171,0.0037,-0.0284,0.054,0.0729,-0.0139,-0.0817,0.0517,0.0055,0.0979,-0.0653,-0.0537,-0.0561,0.0026,0.0356,0.0042,-0.0801,-0.022,-0.0137,-0.0069,0.04,-0.0733,0.0239,0.0495,-0.025,0.0671,-0.016,0.0018,-0.0823,-0.062,0.0231,-0.0243,0.0156,-0.0517,-0.0617,0.0258,-0.1029,0.0722,-0.0161,-0.048,-0.0105,0.0069,0.002,-0.0306,0.0602,-0.0287,0.0155,0.0865,-0.0011,0.0545,-0.0054,-0.0814,-0.0124,0.0955,-0.0486,0.014,-0.0237,0.017,-0.047,0.0553,0.0058,0.0306,-0.0247,-0.0114,-0.0042,-0.039,0.0012,0.0669,-0.0617,-0.2885,0.0307,0.0349,0.0524,-0.0225,-0.0065,0.0253,0.002,0.0114,-0.0114,0.005,0.0365,0.0604,-0.0171,-0.0079,0.0079,0.0138,-0.0293,0.0353,-0.0531,0.0375,0.0247,0.228,0.0033,0.0352,0.0026,0.008,-0.0364,0.0397,-0.0221,0.0251,-0.009,0.091,-0.0822,0.018,0.0284,-0.0443,0.0375,0.0227,-0.0532,0.004,-0.0011,-0.0538,-0.0211,0.0928,-0.0198,-0.0135,-0.0332,-0.0212,0.034,-0.0461,0.0012,-0.0352,-0.0315,0.0154,0.0203,-0.0627,-0.0294,-0.0257,-0.0172,-0.0023,-0.046,0.0238,0.0411,0.054]}
{"key":"[Fingerprinting-Based Positioning in Distributed Massive MIMO Systems] Location awareness in wireless networks may enable many applications such as emergency services, autonomous driving and geographic routing. Although there are many available positioning techniques, none of them is adapted to work with massive multiple-in-multiple-out (MIMO) systems, which represent a leading 5G technology candidate. In this paper, we discuss possible solutions for positioning of mobile stations using a vector of signals at the base station, equipped with many antennas distributed over deployment area. Our main proposal is to use fingerprinting techniques based on a vector of received signal strengths. This kind of methods are able to work in highly-cluttered multipath environments, and require just one base station, in contrast to standard range-based and angle-based techniques. We also provide a solution for fingerprinting-based positioning based on Gaussian process regression, and discuss main applications and challenges.","layer":2,"vector":[-0.0545,-0.0676,0.0549,-0.016,0.0364,0.0113,0.0373,0.0583,0.0434,-0.055,0.0599,0.0061,-0.019,0.0419,0.0105,0.0018,-0.0099,0.0373,-0.0245,-0.0238,0.0334,-0.0487,-0.0223,-0.0352,0.0558,0.055,-0.0341,-0.0522,-0.054,-0.2208,0.0328,-0.0332,0.0938,0.0142,-0.0301,-0.0386,-0.0327,0.0519,0.0061,0.0355,0.0415,0.035,-0.0516,-0.0586,-0.0414,-0.0699,-0.0177,0.0081,-0.0439,-0.0733,0.0277,0.0264,-0.0448,0.0208,0.0652,0.0462,0.0409,0.0256,0.0681,0.0573,0.0595,0.0175,-0.2035,0.0775,0.0734,0.0296,-0.0366,-0.0569,0.0061,-0.0024,-0.0204,0.0387,0.0044,0.0637,0.0366,-0.0195,-0.0015,-0.0111,0.0034,0.0476,0.0255,-0.0581,0.0034,0.0453,-0.0262,-0.0401,-0.005,-0.0626,0.0354,0.0022,-0.0463,0.0181,-0.0386,0.0747,-0.0774,-0.0114,0.0136,0.0257,-0.0372,0.1842,-0.0217,0.0215,0.019,0.0031,0.0194,-0.0014,-0.0222,-0.0362,-0.0046,0.0396,0.012,-0.027,-0.0146,-0.0363,0.0056,0.003,0.0581,0.063,-0.0085,-0.008,0.0002,-0.0166,0.0553,-0.026,0.0568,-0.0501,0.0095,0.1225,0.0111,0.0476,0.0098,-0.0194,-0.0722,0.0046,-0.0016,0.0164,0.0286,0.0106,0.0036,0.0198,-0.0142,-0.067,0.0745,-0.081,-0.0258,0.1154,-0.0399,0.0137,-0.0212,-0.0474,-0.0494,0.0226,0.0045,0.0247,-0.021,0.0207,0.0086,0.0453,-0.0635,0.0352,-0.0502,0.0035,-0.0252,0.1352,0.008,-0.1187,-0.0195,0.0095,0.043,-0.0182,-0.0011,0.0323,0.0057,0.0252,0.0273,0.0208,-0.0279,0.0292,0.0043,-0.0202,0.0095,-0.0311,-0.0135,0.0378,0.0411,-0.0463,-0.017,-0.0448,0.0378,0.0145,-0.0356,0.0206,0.0022,-0.0444,-0.0054,-0.0576,-0.0321,0.0047,0.0567,-0.0277,0.046,0.01,-0.0659,-0.0117,0.0059,0.0177,-0.0093,-0.049,0.006,0.0344,0.0141,-0.0105,0.1389,-0.0371,-0.0116,-0.0329,0.0124,0.116,0.0118,0.039,0.0042,-0.0589,-0.0974,-0.227,-0.0344,-0.0129,0.0287,-0.0125,-0.0675,-0.0001,-0.0034,0.0423,0.0482,0.1179,-0.0632,-0.0346,0.0497,-0.018,0.0421,0.0127,0.0229,0.0218,-0.0094,0.0197,0.0037,-0.0751,-0.0344,0.0672,-0.0279,0.1949,0.0207,0.0006,-0.0231,0.0415,-0.0178,-0.0191,-0.0783,0.007,0.0383,0.0616,0.0059,0.0056,-0.0233,-0.0295,0.0088,0.0243,-0.0476,-0.0227,-0.0367,-0.0449,0.013,-0.0586,0.0093,0.0881,-0.0204,0.066,-0.0519,0.0337,-0.0214,-0.0711,0.0198,-0.0546,0.0246,-0.0144,-0.0258,0.0182,-0.0504,0.0604,0.0069,-0.0172,-0.0114,0.0189,-0.0075,0.0285,0.0805,0.0027,-0.0037,0.0494,-0.0131,0.0381,-0.0606,-0.0365,-0.0662,0.062,-0.0356,0.0473,-0.0212,0.0391,-0.0165,0.0845,0.0217,0.0153,-0.0318,-0.0101,0.0198,-0.0168,-0.0239,0.0162,0.0079,-0.2858,0.0218,0.0232,0.0196,-0.0965,-0.0139,0.0305,0.0356,-0.1028,0.0289,-0.03,0.0142,0.0398,-0.0215,0.0379,0.0291,-0.0066,-0.0432,-0.0001,-0.0918,0.0525,0.0069,0.2121,-0.0355,0.0239,0.015,-0.0534,0.0091,-0.001,-0.0102,0.0638,0.0188,0.0812,-0.0465,0.0245,0.0324,0.0017,0.0514,0.011,-0.0009,-0.017,-0.017,-0.0174,-0.0164,0.1029,-0.0268,-0.0743,-0.0429,0.0339,0.0258,-0.0404,0.0004,-0.008,0.0047,0.0223,0.0312,-0.0582,-0.0548,-0.0721,-0.0458,0.038,-0.0783,0.0039,-0.016,0.0195]}
{"key":"[Graph Representation Learning in Biomedicine] Biomedical networks (or graphs) are universal descriptors for systems of interacting elements, from molecular interactions and disease co-morbidity to healthcare systems and scientific knowledge. Advances in artificial intelligence, specifically deep learning, have enabled us to model, analyze, and learn with such networked data. In this review, we put forward an observation that long-standing principles of systems biology and medicine -- while often unspoken in machine learning research -- provide the conceptual grounding for representation learning on graphs, explain its current successes and limitations, and even inform future advancements. We synthesize a spectrum of algorithmic approaches that, at their core, leverage graph topology to embed networks into compact vector spaces. We also capture the breadth of ways in which representation learning has dramatically improved the state-of-the-art in biomedical machine learning. Exemplary domains covered include identifying variants underlying complex traits, disentangling behaviors of single cells and their effects on health, assisting in diagnosis and treatment of patients, and developing safe and effective medicines.","layer":0,"vector":[-0.0327,0.002,-0.0094,-0.0031,0.0333,0.0211,0.0552,0.0752,0.0396,-0.0226,0.0185,-0.0579,0.049,0.0595,0.0038,0.0265,0.0093,0.0815,-0.0639,0.0183,-0.0103,-0.0559,-0.0041,-0.0471,0.0337,0.0031,-0.0152,0.0066,-0.0599,-0.2251,0.0301,-0.0509,0.0494,-0.0483,-0.0092,-0.0462,0.0307,0.0601,-0.0488,0.0444,0.0082,0.0086,-0.0232,-0.0055,-0.0131,-0.0406,-0.044,-0.0387,0.0139,-0.0646,0.0276,-0.0381,0.0228,0.0452,0.0493,0.0376,0.0813,0.0179,0.0232,0.0806,0.054,0.0512,-0.1345,0.0594,0.0612,0.0113,-0.0593,0.0112,0.0477,0.0704,0.0351,0.0035,-0.0029,0.0263,-0.0086,0.051,0.032,-0.0103,0.0245,-0.0088,-0.0083,-0.016,-0.0591,-0.0112,0.0183,-0.0551,0.0255,-0.0857,0.0054,0.0355,-0.0563,-0.0184,-0.0129,0.0212,-0.0895,-0.0209,0.0635,-0.0167,-0.0485,0.1927,-0.0538,-0.0132,0.0546,-0.0019,0.0411,-0.0588,-0.0182,-0.037,-0.0484,0.0076,-0.0177,-0.013,0.0108,-0.0575,-0.0046,0.0138,0.0669,0.0475,-0.0046,-0.0241,-0.0346,0.0215,0.0453,-0.0364,0.0247,-0.0461,-0.0112,0.1332,0.0625,0.0215,0.053,0.0587,-0.002,0.0116,-0.0154,0.0334,0.007,-0.0237,-0.0468,0.0158,-0.0077,-0.0171,-0.0147,-0.0836,-0.1078,0.1307,-0.0852,-0.0518,-0.0398,-0.0406,-0.0393,0.0115,-0.0424,0.0085,-0.0128,0.0147,0.056,0.0321,-0.0544,0.0148,-0.0193,-0.0459,-0.0626,0.115,0.0517,-0.0654,-0.0118,-0.0179,-0.0163,-0.0159,0.0606,0.0322,0.0415,0.0618,0.0442,0.0385,-0.0577,-0.0492,0.0204,-0.0396,0.0231,-0.0226,-0.021,0.0425,0.0252,-0.0375,-0.0182,-0.0209,0.0178,0.0368,0.0029,0.0594,-0.0225,-0.0049,-0.033,-0.0223,-0.035,-0.0445,-0.0183,-0.0162,0.0226,-0.0264,-0.0072,0.0172,-0.0383,0.0092,-0.0518,0.0252,0.0372,0.0139,-0.0691,0.0256,0.0223,-0.0269,-0.0508,0.0099,0.0027,0.018,0.0455,0.0404,0.033,-0.0404,-0.0481,-0.1935,-0.0286,0.0125,-0.0417,0.0206,-0.0433,-0.0074,-0.0163,0.0528,0.0923,0.0847,0.0411,-0.0249,-0.0317,-0.0117,0.022,0.0626,0.0299,-0.0375,-0.0103,0.0054,-0.0021,0.0104,-0.0769,0.0139,0.0223,0.2454,0.0182,0.0311,-0.0011,-0.0029,0.0109,-0.0656,-0.1017,0.0828,0.0237,0.0156,-0.0031,-0.0323,-0.012,-0.0625,0.0026,-0.0196,-0.0916,-0.0199,0.0011,-0.0244,0.0169,-0.0652,0.0046,0.0405,-0.0537,0.0724,-0.0066,-0.0059,-0.0184,-0.066,0.065,-0.0692,0.0123,-0.002,-0.0758,0.0095,-0.0588,0.0543,-0.0225,-0.0464,-0.0377,0.0225,-0.0644,-0.0089,0.0716,0.0042,-0.0078,0.0869,0.0163,0.0221,-0.0355,-0.049,0.0156,0.0632,-0.0507,0.0413,0.0057,0.0187,0.0114,0.0867,0.0063,0.0593,-0.0515,-0.0227,-0.0164,-0.0559,-0.0425,0.0193,-0.0245,-0.2885,0.0681,0.0493,0.0756,-0.0195,-0.022,0.0414,0.0072,-0.0456,-0.0091,0.0575,0.0216,0.0609,0.0131,-0.0306,0.0346,0.095,-0.0355,0.0714,-0.0624,0.051,0.0112,0.1977,-0.0335,0.0546,0.0649,0.0013,0.0237,0.002,-0.0305,-0.0002,-0.0023,0.0732,-0.0634,0.0585,0.0749,-0.0644,0.012,0.0114,0.0212,0.0522,-0.0379,-0.029,-0.0181,0.0729,-0.017,-0.0341,-0.0108,0.0296,0.0149,-0.0346,0.0132,-0.0124,0.0009,0.0544,0.0022,0.0015,-0.0392,-0.0145,-0.0616,-0.0025,-0.0653,-0.0418,0.0034,-0.047]}
{"key":"[Learning Soft Constraints From Constrained Expert Demonstrations] Inverse reinforcement learning (IRL) methods assume that the expert data is generated by an agent optimizing some reward function. However, in many settings, the agent may optimize a reward function subject to some constraints, where the constraints induce behaviors that may be otherwise difficult to express with just a reward function. We consider the setting where the reward function is given, and the constraints are unknown, and propose a method that is able to recover these constraints satisfactorily from the expert data. While previous work has focused on recovering hard constraints, our method can recover cumulative soft constraints that the agent satisfies on average per episode. In IRL fashion, our method solves this problem by adjusting the constraint function iteratively through a constrained optimization procedure, until the agent behavior matches the expert behavior. Despite the simplicity of the formulation, our method is able to obtain good results. We demonstrate our approach on synthetic environments and real world highway driving data.","layer":7,"vector":[-0.0714,0.0127,0.0625,-0.0234,0.0331,0.0563,0.045,0.0166,0.0093,-0.0112,0.026,-0.043,0.0147,0.0338,0.0216,0.0168,0.0083,0.0793,-0.0427,-0.0058,0.0372,-0.0619,-0.0204,-0.0783,0.0122,0.0103,-0.0367,-0.036,-0.015,-0.21,-0.0081,-0.0381,0.0302,-0.0205,0.0111,0.0166,-0.0954,0.092,-0.0059,0.0367,0.057,0.0247,0.0183,-0.0559,-0.0381,-0.0381,-0.0289,-0.0216,0.0118,-0.0125,0.015,0.0139,0.0014,-0.0263,0.0422,0.0241,0.0626,0.0699,0.0643,0.042,-0.0169,0.0472,-0.1523,0.0566,0.0738,0.0683,-0.0499,-0.0062,0.0499,0.0496,0.0092,0.06,0.0092,0.0207,0.0506,0.0081,-0.0018,-0.0224,0.0061,-0.0371,0.0138,-0.053,-0.0628,0.0154,0.0015,-0.1051,0.049,-0.0663,0.0569,0.0245,-0.0511,-0.0269,-0.0172,0.0096,-0.0484,-0.0179,-0.0098,0.0259,-0.0407,0.2043,-0.0206,0.0453,0.0094,0.0007,0.0244,-0.0629,-0.0295,0.0191,-0.0131,-0.0019,-0.0514,-0.0043,0.0331,0.005,-0.0016,0.0418,0.0461,0.0688,-0.0152,-0.0455,-0.0008,0.0174,0.0671,-0.0131,0.0208,-0.0723,0.0027,0.1348,0.0168,0.0264,0.0096,-0.0499,-0.0564,-0.0163,0.0112,-0.0355,0.0292,-0.0118,-0.0057,0.0158,-0.0274,-0.0346,0.0124,-0.0916,-0.0931,0.1015,-0.0036,0.0294,-0.0249,-0.024,-0.0031,-0.008,-0.026,-0.0231,0.0235,0.0117,0.0282,0.0507,-0.0463,0.0268,-0.0122,-0.033,-0.0228,0.1202,-0.0031,-0.0703,-0.0297,0.0223,0.0117,0.0006,-0.0024,0.0469,-0.0724,0.0438,0.1044,0.0352,-0.0654,-0.0118,-0.0221,0.0174,0.0025,-0.0846,-0.0178,0.0216,0.037,-0.0071,-0.0068,-0.0132,-0.0053,0.0375,-0.0243,0.0038,-0.0334,-0.0241,-0.0313,-0.016,0.0097,-0.0032,0.0121,-0.0336,-0.0051,-0.0081,-0.0347,-0.0017,-0.045,0.0063,-0.007,-0.0079,0.0625,0.0192,-0.0252,0.0187,0.0435,-0.0338,-0.055,-0.0035,0.0446,0.0098,-0.0378,0.03,0.014,0.0111,-0.0136,-0.2263,0.0148,-0.0452,0.0054,0.0405,-0.0428,0.0419,-0.017,0.0385,0.0505,0.0487,-0.0351,-0.0604,0.0212,-0.0257,0.0489,0.0323,0.0055,-0.0252,0.0207,0.0114,0.0003,-0.0165,-0.0781,0.0606,0.012,0.2311,0.0187,0.05,-0.0074,0.0221,0.0215,-0.0239,-0.1,0.0328,0.01,0.0761,-0.0115,-0.0136,-0.0284,-0.0024,-0.0146,-0.0279,-0.0624,-0.03,-0.0096,-0.0551,0.0693,-0.0393,0.0246,0.0228,-0.0107,0.0343,-0.0017,-0.0267,-0.0428,-0.0815,0.0316,0.0012,-0.0047,0.0318,-0.0296,-0.0142,-0.0783,0.0587,0.0184,-0.0012,-0.0603,0.0477,0.0159,-0.0426,0.0547,-0.0062,0.009,0.0097,0.027,0.0167,-0.0157,-0.0446,-0.0895,0.0463,-0.0558,0.0451,0.0171,0.0045,0.0322,0.0611,-0.0499,0.0023,0.0063,0.0515,0.0432,-0.0342,-0.0229,0.0799,-0.0162,-0.2981,0.0252,0.0075,0.0079,-0.0249,0.039,0.0362,0.0152,-0.0493,-0.065,0.0147,0.0364,0.0235,0.083,0.0518,-0.0105,0.0426,-0.0459,0.0336,-0.1139,0.0099,0.0714,0.2468,-0.0719,0.0571,0.0523,-0.0412,-0.0746,0.051,-0.0399,0.0428,-0.0199,0.0367,-0.0623,0.0606,0.098,-0.0771,0.0607,0.0427,0.0262,-0.045,0.0126,-0.0241,-0.0457,0.0856,0.0172,-0.0391,-0.06,0.0041,0.0436,-0.0294,0.0223,-0.0108,-0.0166,0.0275,0.003,-0.0203,-0.0545,-0.0311,-0.0435,0.015,-0.0657,0.0278,-0.008,-0.0306]}
{"key":"[Identifying Users From Their Rating Patterns] This paper reports on our analysis of the 2011 CAMRa Challenge dataset (Track 2) for context-aware movie recommendation systems. The train dataset comprises 4,536,891 ratings provided by 171,670 users on 23,974$ movies, as well as the household groupings of a subset of the users. The test dataset comprises 5,450 ratings for which the user label is missing, but the household label is provided. The challenge required to identify the user labels for the ratings in the test set. Our main finding is that temporal information (time labels of the ratings) is significantly more useful for achieving this objective than the user preferences (the actual ratings). Using a model that leverages on this fact, we are able to identify users within a known household with an accuracy of approximately 96% (i.e. misclassification rate around 4%).","layer":4,"vector":[-0.061,-0.0023,0.0079,-0.0269,0.03,0.0215,0.0708,0.0057,0.0158,-0.0225,-0.0087,-0.0213,0.0222,0.0571,0.0397,-0.0265,0.0456,0.0063,-0.0293,-0.0243,0.056,-0.0442,0.0018,-0.0805,0.026,0.0212,-0.057,-0.0406,-0.0773,-0.1976,-0.0096,-0.0327,0.0594,0.0061,-0.0092,-0.0708,-0.011,0.0372,0.0009,0.011,-0.03,0.0337,0.0098,-0.0712,-0.0803,0.0033,-0.0194,-0.0076,-0.0595,-0.0086,-0.0027,-0.04,0.0732,0.0303,-0.0227,0.0366,0.0393,0.0396,0.0453,0.0388,0.0271,0.0098,-0.1672,0.0468,0.0462,0.0193,-0.0242,0.0216,-0.0461,0.0425,-0.0458,0.0571,0.0243,0.043,0.0072,-0.005,0.0082,-0.0193,-0.0104,0.018,-0.0006,-0.0173,-0.0518,-0.0397,0.0346,-0.0357,0.0601,-0.0235,0.0196,0.0331,-0.0457,0.0207,-0.0306,0.0361,-0.0778,-0.0551,0.018,0.0202,-0.0599,0.2142,-0.0331,0.0375,0.025,-0.0423,0.0322,-0.0314,-0.0062,0.0153,0.0165,0.0079,-0.0389,-0.0017,0.0136,-0.0356,0.0665,0.0235,0.0691,0.0471,0.0807,-0.03,-0.0114,0.0316,0.0601,-0.0688,0.0462,-0.0526,0.0412,0.0873,0.0168,0.026,0.0522,-0.0243,-0.1029,0.0185,0.0026,0.0562,0.0052,0.0164,0.0425,-0.0535,-0.0175,-0.0494,0.0134,-0.0689,-0.0574,0.1067,-0.0151,0.0602,-0.0651,-0.0053,0.0107,0.0028,-0.0678,-0.0569,0.0104,0.0077,0.0554,0.0269,-0.0486,0.0707,-0.0124,-0.0104,-0.0602,0.0771,-0.0021,-0.0792,-0.0377,0.0044,0.0088,0.022,0.028,0.0281,-0.0379,0.0306,0.0812,0.0324,-0.0821,0.0209,0.0066,-0.0034,0.0082,-0.0768,-0.0606,-0.0064,0.0093,-0.0278,-0.0147,-0.0712,0.04,0.0479,-0.0313,-0.0523,0.0037,-0.0118,0.0138,-0.0002,0.0006,-0.0083,0.0008,-0.0395,0.0193,0.0068,-0.0102,0.0223,-0.0072,0.0576,-0.0706,-0.0265,0.0411,-0.0036,-0.0003,-0.0114,0.0567,-0.0153,-0.0463,-0.0322,0.029,0.0277,0.0419,0.0725,0.0725,0.0149,-0.0307,-0.2467,0.0176,0.0205,-0.0098,0.032,-0.0925,0.0163,-0.0138,0.055,0.0879,0.0345,-0.0391,-0.0363,0.0241,0.0311,0.0548,0.0135,0.0068,-0.016,-0.002,0.002,0.0414,0.0059,-0.0765,0.0393,0.02,0.2671,0.0528,0.0049,-0.0238,0.0408,0.0412,-0.0687,-0.0962,0.0229,0.0037,0.0784,0.0172,-0.0011,-0.0538,0.0058,0.0066,0.0123,-0.0592,-0.0457,-0.0779,-0.0025,0.0393,-0.0549,0.0515,0.0339,-0.0357,0.0149,0.0012,-0.0196,-0.0253,-0.0607,0.0289,-0.0159,0.057,-0.0264,-0.0289,0.0351,-0.0889,0.0148,-0.0353,-0.0429,0.0147,-0.0039,-0.0278,-0.0097,0.0479,-0.0362,-0.0165,0.0607,-0.0084,0.0273,-0.0402,-0.0481,-0.0584,0.0597,-0.0226,0.0366,-0.0157,0.0406,0.0188,0.0189,0.0067,0.0189,-0.0045,0.0406,-0.0132,-0.0464,-0.0197,0.037,-0.0362,-0.3041,0.027,-0.003,0.0559,-0.0004,-0.0174,0.0721,0.0226,0.0057,-0.0026,0.056,0.0346,0.0126,-0.0108,0.0014,0.0244,0.06,-0.038,0.0566,-0.0324,0.0662,-0.0029,0.2258,-0.0151,0.0352,0.0211,-0.0185,-0.021,0.0652,0.0039,0.0078,0.0012,0.1164,-0.0682,0.0218,0.018,-0.0287,-0.0136,0.0446,-0.0222,-0.0425,-0.0264,-0.0553,-0.0363,0.091,-0.0044,0.0099,-0.0467,0.0087,-0.0006,-0.0284,-0.0318,-0.0221,0.0491,0.0427,0.0456,-0.0501,-0.0313,-0.0207,-0.0739,-0.0124,-0.073,0.0134,-0.003,-0.0181]}
{"key":"[Explaining Reject Options of Learning Vector Quantization Classifiers] While machine learning models are usually assumed to always output a prediction, there also exist extensions in the form of reject options which allow the model to reject inputs where only a prediction with an unacceptably low certainty would be possible. With the ongoing rise of eXplainable AI, a lot of methods for explaining model predictions have been developed. However, understanding why a given input was rejected, instead of being classified by the model, is also of interest. Surprisingly, explanations of rejects have not been considered so far. We propose to use counterfactual explanations for explaining rejects and investigate how to efficiently compute counterfactual explanations of different reject options for an important class of models, namely prototype-based classifiers such as learning vector quantization models.","layer":1,"vector":[-0.036,-0.0172,0.0118,0.0428,0.0479,0.0041,0.0542,0.034,-0.0171,-0.0104,0.0545,-0.0799,0.0036,0.0384,0.044,0.0071,-0.0406,0.0383,-0.0407,0.015,0.0555,-0.0018,-0.0438,-0.0488,0.0169,0.0291,-0.0148,-0.073,-0.0132,-0.2244,0.0268,-0.0342,0.0554,-0.0406,0.0202,-0.0069,-0.0248,0.0429,-0.0649,0.034,0.0424,-0.0157,-0.0321,-0.0712,-0.0304,-0.0519,-0.001,-0.015,-0.0396,-0.0196,0.0329,-0.017,0.0212,0.0088,0.0234,0.0453,0.0594,0.0711,0.0269,0.0725,0.0515,0.0349,-0.1236,0.0804,0.0362,0.0551,-0.0289,-0.0381,0.0284,0.0899,-0.023,0.0538,0.0156,0.0482,0.0099,-0.0104,0.0076,-0.0276,0.0096,0.0117,0.0364,-0.0345,-0.0193,0.0301,-0.0402,-0.0312,0.0394,-0.0068,0.0621,-0.024,-0.0319,-0.0283,-0.0418,0.0116,-0.0498,0.0154,0.028,0.0123,-0.0983,0.1952,-0.0377,-0.0036,0.0088,-0.0136,0.0219,-0.0272,-0.0856,-0.0604,-0.0203,-0.0142,-0.0121,-0.0126,0.0291,-0.035,0.0028,0.0228,0.0643,0.0236,-0.0125,-0.02,-0.045,-0.0106,0.0386,-0.0362,-0.018,-0.0543,0.0374,0.1417,0.0012,-0.0391,0.0226,-0.0796,-0.0581,-0.0447,0.0543,0.0067,0.0443,0.0523,0.0181,0.0201,0.0015,-0.0495,-0.0209,-0.0553,-0.0689,0.0842,-0.0478,0.0214,-0.0086,0.0028,-0.0086,0.0274,-0.0612,-0.0344,0.0185,0.0344,-0.0072,0.0417,-0.0652,0.014,0.0139,-0.0652,-0.0532,0.1001,0.0096,-0.0721,-0.028,-0.0068,0.0189,-0.0104,0.0427,0.0369,-0.0804,0.0164,0.0643,0.0252,-0.0817,-0.0439,0.0109,-0.0065,-0.0011,-0.0629,-0.0547,0.0906,0.0407,-0.0535,0.0144,-0.0604,0.0168,0.0323,-0.0001,0.0036,-0.0246,-0.0234,-0.0385,-0.0368,-0.0242,0.0003,0.0176,-0.0637,-0.0059,0.0328,-0.039,0.0199,-0.0231,0.029,0.0081,0.0008,0.0723,0.0352,-0.0164,-0.0047,0.0169,-0.0536,-0.0082,-0.005,0.0209,0.0407,-0.0101,0.0106,0.0049,-0.0107,-0.0516,-0.2237,-0.0108,-0.0029,-0.0223,0.0542,-0.0848,0.0487,-0.0024,0.0028,0.0945,0.0487,-0.0213,-0.0507,-0.0171,-0.0099,0.0703,0.0343,0.0423,-0.0257,0.0146,-0.0019,0.0547,-0.0136,-0.0939,0.0288,0.0311,0.2083,0.0489,0.0616,-0.0065,0.0544,0.0342,-0.0099,-0.0844,0.0774,0.0211,0.0295,-0.0479,-0.0087,-0.0333,-0.0282,0.067,-0.0108,-0.1119,-0.0473,-0.0033,-0.0328,0.0393,-0.0503,0.0333,0.0203,0.024,0.0684,0.0405,0.0301,-0.0452,-0.1026,0.0127,-0.0172,0.0287,0.0076,-0.0386,0.0219,-0.0472,0.0274,-0.0005,-0.0112,-0.0347,0.0468,0.0105,-0.0065,0.146,-0.0006,-0.0478,0.0636,0.0321,0.0203,-0.0692,-0.0539,-0.0271,0.0616,-0.0094,0.0074,-0.0062,0.0239,0.0058,0.0706,-0.0084,0.0127,0.0193,0.0105,0.0529,-0.0353,0.0119,0.0592,-0.0271,-0.3169,0.0182,0.0332,0.0321,-0.0204,0.0087,0.0525,0.0273,-0.0465,-0.0113,-0.0319,0.0051,0.031,-0.0164,0.0064,0.006,0.059,-0.0317,0.0674,-0.0152,0.0341,0.013,0.2285,-0.0715,0.006,0.0134,-0.0653,-0.0446,0.0373,-0.0243,-0.0095,-0.0107,0.1146,-0.0321,0.0396,0.0287,-0.0148,0.0147,0.0327,-0.0608,-0.008,0.0099,-0.0601,-0.0313,0.1021,-0.0106,-0.0022,-0.0219,-0.0237,0.0414,0.0214,0.0073,-0.0516,-0.0004,-0.0001,0.0107,-0.0459,-0.0786,0.0367,-0.0248,0.048,-0.0574,-0.0019,0.0242,-0.0281]}
{"key":"[GAN-Leaks: A Taxonomy of Membership Inference Attacks against Generative Models] Deep learning has achieved overwhelming success, spanning from discriminative models to generative models. In particular, deep generative models have facilitated a new level of performance in a myriad of areas, ranging from media manipulation to sanitized dataset generation. Despite the great success, the potential risks of privacy breach caused by generative models have not been analyzed systematically. In this paper, we focus on membership inference attack against deep generative models that reveals information about the training data used for victim models. Specifically, we present the first taxonomy of membership inference attacks, encompassing not only existing attacks but also our novel ones. In addition, we propose the first generic attack model that can be instantiated in a large range of settings and is applicable to various kinds of deep generative models. Moreover, we provide a theoretically grounded attack calibration technique, which consistently boosts the attack performance in all cases, across different attack settings, data modalities, and training configurations. We complement the systematic analysis of attack performance by a comprehensive experimental study, that investigates the effectiveness of various attacks w.r.t. model type and training configurations, over three diverse application scenarios (i.e., images, medical data, and location data).","layer":1,"vector":[-0.004,-0.0529,-0.0064,-0.0088,0.031,0.0364,0.0779,-0.0193,0.0362,-0.0089,0.0271,-0.0647,0.0497,0.0335,0.0007,-0.0039,-0.0064,0.0191,-0.0233,0.0097,0.0313,-0.0413,0.0407,-0.0271,-0.0045,0.0025,-0.0358,-0.0327,-0.0526,-0.2424,0.0296,-0.0501,0.0402,0.0022,-0.0045,-0.042,-0.0504,0.0426,-0.0279,0.0545,0.0265,0.0551,-0.0357,-0.0701,-0.0042,-0.0101,-0.0413,0.0133,-0.0299,-0.0492,0.035,-0.0285,0.014,0.0811,0.0376,-0.0334,0.1021,0.0295,0.0405,0.0613,0.0404,0.0498,-0.1412,0.0218,0.0173,0.0467,-0.0321,-0.0188,0.0107,0.0054,-0.0192,0.0575,-0.0188,0.034,0.025,-0.0134,-0.0291,-0.0136,-0.0335,0.0121,0.0199,-0.0186,-0.0187,0.018,0.0032,-0.039,0.0235,-0.0367,0.0095,0.0296,-0.0202,-0.0317,-0.0052,0.0475,-0.0255,0.0049,-0.0045,0.0327,-0.0902,0.221,-0.0757,0.0057,0.0433,-0.0081,0.0088,0.0075,-0.043,-0.0685,-0.0316,-0.0203,0.0304,-0.0005,0.0024,-0.006,0.0324,-0.0351,0.116,0.043,-0.035,-0.0207,-0.017,0.0325,0.0328,0.0133,0.0258,-0.0716,0.0474,0.1704,0.0524,0.0344,0.0113,-0.0235,-0.0244,0.0077,0.0204,0.0366,0.0199,0.0174,-0.0025,-0.0272,-0.0142,-0.0149,0.0301,0.0045,-0.0259,0.1365,-0.0259,0.034,-0.0429,-0.0095,-0.0071,0.0285,-0.0389,-0.034,0.0207,0.0268,0.0067,0.0537,-0.0459,-0.0015,0.004,-0.0419,-0.0281,0.1109,0.0434,-0.0611,-0.0054,0.0247,0.0314,-0.0223,0.0033,0.0133,-0.0125,0.0481,0.0087,0.0156,-0.0402,-0.0147,-0.0365,0.0403,-0.0175,-0.068,-0.0356,0.0005,0.0198,-0.017,-0.0056,-0.0318,0.0295,0.0664,-0.0315,0.0203,-0.083,-0.0103,-0.0149,-0.0577,-0.0334,-0.0248,0.0071,-0.0486,0.0037,0.0027,-0.0664,-0.029,-0.0443,0.0287,0.0098,-0.0076,0.023,0.0308,-0.0388,0.0014,0.0358,-0.0455,-0.0114,-0.0087,-0.0082,0.0442,-0.02,0.027,0.0399,-0.0423,-0.0262,-0.2198,0.0006,-0.0172,-0.0443,0.0278,-0.0992,0.053,-0.0426,0.0079,0.0539,0.0518,-0.0122,-0.011,0.0167,-0.0005,0.0575,0.0217,0.0462,-0.0167,-0.0112,-0.0335,0.0249,-0.0227,-0.1218,0.0052,0.0215,0.2271,0.0908,0.0357,-0.0169,0.0177,0.0411,-0.0398,-0.098,0.0473,0.0098,0.0539,0.0008,-0.0291,-0.0065,-0.0529,0.0371,0.0133,-0.1075,0.0047,-0.0538,-0.0518,0.0611,-0.0676,0.0513,0.0608,-0.0209,0.0791,0.0199,0.0367,-0.0812,-0.1098,0.0468,-0.0775,0.06,0.0175,-0.0321,0.0333,-0.1329,0.0602,-0.0552,-0.0431,-0.0756,0.0516,-0.0378,0.0102,0.0841,0.0077,0.0198,0.0268,0.0123,0.0105,-0.0751,-0.0876,-0.0608,0.0504,0.0094,0.0327,0.0091,0.0109,0.0019,0.0936,0.0353,0.0588,-0.0669,0.0113,0.0019,-0.0794,-0.0303,0.0422,-0.0071,-0.2924,-0.0015,-0.0176,0.0842,-0.04,0.017,0.0522,0.0205,-0.0415,-0.0112,0.0342,0.0538,0.0444,-0.0206,-0.0167,0.0269,0.0537,-0.069,0.0432,-0.0115,0.0126,0.0317,0.205,-0.0215,-0.0085,-0.001,-0.0085,0.0352,0.0158,0.0028,-0.0301,0.0103,0.0634,0.0014,0.0391,0.0568,-0.0388,0.0061,0.0347,-0.0215,-0.0069,-0.0166,-0.07,0.0268,0.052,-0.0116,0.0029,-0.0019,0.0305,0.0057,0.001,-0.0063,-0.0394,0.0193,0.0478,0.026,-0.044,-0.0427,0.0165,-0.0187,-0.0055,-0.0303,-0.0365,0.0342,-0.0278]}
{"key":"[Sliced Wasserstein Kernels for Probability Distributions] Optimal transport distances, otherwise known as Wasserstein distances, have recently drawn ample attention in computer vision and machine learning as a powerful discrepancy measure for probability distributions. The recent developments on alternative formulations of the optimal transport have allowed for faster solutions to the problem and has revamped its practical applications in machine learning. In this paper, we exploit the widely used kernel methods and provide a family of provably positive definite kernels based on the Sliced Wasserstein distance and demonstrate the benefits of these kernels in a variety of learning tasks. Our work provides a new perspective on the application of optimal transport flavored distances through kernel methods in machine learning tasks.","layer":1,"vector":[-0.0048,-0.0505,0.0423,0.0206,0.0279,0.0191,0.0536,0.0262,0.0104,-0.0205,0.014,-0.0974,-0.0252,0.0288,0.0024,0.056,0.0253,0.042,-0.0928,0.0297,0.016,-0.1035,0.0009,-0.0888,0.0593,0.0211,0.0015,-0.0338,-0.0365,-0.2589,0.0104,-0.0604,0.0301,-0.0724,-0.0107,-0.0439,-0.0109,0.0352,-0.0103,0.0356,0.0603,0.0132,-0.0163,-0.0465,-0.0415,-0.0663,-0.0426,-0.0345,-0.0084,-0.0451,0.0094,-0.0164,0.023,-0.0139,0.0006,0.0082,0.0755,0.0479,0.0464,0.0412,-0.0264,0.0392,-0.1526,0.0212,0.032,-0.011,-0.0465,-0.0123,0.0032,0.0541,-0.0315,0.0286,-0.0101,0.0365,-0.039,-0.0102,0.0294,-0.0211,-0.0567,0.0203,0.0106,-0.0157,-0.0257,0.0061,-0.0382,-0.0138,-0.0062,-0.0639,0.0364,0.0091,-0.02,-0.0425,-0.0253,0.0243,-0.0797,-0.0493,0.0283,-0.0153,-0.0064,0.2177,-0.0347,0.0603,0.0489,-0.0269,0.0325,-0.0283,-0.0245,-0.0146,-0.0123,-0.0264,-0.0225,0.0001,0.0218,-0.0455,0.007,0.0269,0.0683,0.0747,-0.0258,0.0111,-0.0329,0.0342,0.0449,-0.0116,0.0192,-0.0386,0.0213,0.1351,0.0063,0.0835,0.0446,-0.0013,-0.0551,-0.0277,0.0086,0.009,0.0124,0.0226,0.0259,0.0081,0.0057,-0.0631,0.0101,-0.1054,-0.0101,0.1193,-0.0512,0.0505,-0.0271,-0.0019,0.0022,-0.0312,-0.0406,-0.0435,0.0346,0.014,-0.014,0.028,-0.0285,0.0511,-0.0148,-0.1065,-0.025,0.097,0.0059,-0.078,-0.0218,0.0052,0.0181,-0.0384,0.0193,0.0224,0.0375,0.0472,0.0628,0.028,-0.1053,0.0049,0.0048,-0.0179,0.0475,-0.0679,-0.0526,0.0128,0.0883,0.0005,0.0054,-0.0449,-0.0059,0.1024,0.0134,-0.0161,-0.0146,-0.0047,-0.0252,-0.032,-0.0211,0.0076,0.0172,-0.0091,0.0082,0.0126,-0.0673,-0.0013,-0.0399,0.0213,0.0046,-0.0105,0.0365,0.0072,-0.0334,-0.0302,0.0333,-0.0356,-0.0256,0.0081,0.0242,0.0508,0.0427,0.0246,0.0546,-0.0466,-0.0448,-0.2347,-0.0129,0.0209,0.0025,0.0382,-0.0574,0.0606,0.0145,0.0822,0.0515,0.0213,-0.0435,-0.0199,-0.0062,-0.0025,0.0416,0.0407,0.025,-0.0536,-0.0127,0.0084,0.0414,-0.0076,-0.0408,0.0536,-0.0358,0.2226,0.0124,0.0331,-0.0463,0.0033,0.0519,-0.0024,0.0009,0.0524,-0.0239,0.0538,-0.0031,-0.0596,-0.0159,0.0203,0.0033,0.0177,-0.0869,-0.0153,-0.06,-0.0358,0.0297,0.0028,0.0353,0.0329,-0.0158,0.0825,-0.0391,0.0405,-0.0268,-0.0802,0.0012,-0.056,0.0886,-0.0055,-0.0802,0.0005,-0.0537,0.0461,0.0242,-0.0331,-0.0337,0.0197,-0.0041,-0.0151,0.0807,-0.0297,-0.0132,0.0773,0.0215,0.0361,0.0041,-0.0186,-0.0393,0.0825,-0.0013,0.0392,-0.0181,0.0387,0.0226,0.0455,-0.0025,0.023,0.0019,0.0171,-0.0382,-0.0732,-0.0105,0.0707,-0.0044,-0.2922,0.042,0.0024,0.0275,-0.0242,-0.0195,0.0189,0.0073,-0.0422,-0.0317,0.0358,0.0266,0.0441,-0.0233,0.0079,0.0185,0.0778,-0.0237,0.0349,-0.0717,0.0136,0.0482,0.2254,-0.0521,0.0294,0.0132,0.0085,0.0148,0.0141,-0.0407,-0.0022,-0.0057,0.0473,-0.0609,0.0268,0.081,-0.0193,0.0343,0.0002,-0.0332,0.028,-0.0064,-0.0354,-0.023,0.1231,0.0035,-0.0034,-0.0492,0.0396,0.0387,-0.0149,0.0131,0.0182,0.0081,0.0409,0.0327,-0.0132,-0.0702,-0.079,-0.0628,0.0501,-0.0578,0.0146,-0.0232,0.0104]}
{"key":"[Flow-based SVDD for anomaly detection] We propose FlowSVDD -- a flow-based one-class classifier for anomaly/outliers detection that realizes a well-known SVDD principle using deep learning tools. Contrary to other approaches to deep SVDD, the proposed model is instantiated using flow-based models, which naturally prevents from collapsing of bounding hypersphere into a single point. Experiments show that FlowSVDD achieves comparable results to the current state-of-the-art methods and significantly outperforms related deep SVDD methods on benchmark datasets.","layer":0,"vector":[-0.0255,-0.0263,0.0478,-0.0413,0.068,-0.0052,0.0562,0.0265,0.0205,-0.0173,0.0236,0.0049,0.0073,0.0212,0.0124,0.0106,-0.008,0.0348,-0.0042,0.0062,0.0337,-0.0259,0.0079,-0.0616,0.0379,0.0437,0.0008,-0.0763,-0.0813,-0.2305,0.015,-0.0106,0.0162,-0.0505,0.0421,-0.0756,-0.0294,0.0512,-0.0425,0.0209,-0.0041,0.0148,-0.015,-0.0607,-0.0557,-0.0689,-0.0281,0.0098,-0.0073,-0.0195,0.025,-0.0412,0.04,0.0349,0.0414,0.0153,0.0612,0.0103,0.0609,0.0755,0.0341,0.0439,-0.1309,0.0202,0.0605,0.0003,-0.0253,-0.009,0.025,0.0098,0.0014,0.0105,0.0026,0.0465,-0.0106,0.0122,-0.0011,-0.045,-0.0091,0.0374,-0.0133,-0.0236,-0.0318,-0.0261,-0.0403,-0.0758,0.001,-0.0246,0.0715,-0.0258,-0.0547,-0.0181,0.0034,0.0075,-0.0703,0.0058,0.038,-0.0119,0.0007,0.2162,-0.0418,0.0316,-0.0093,-0.002,0.0403,0.0263,-0.0251,-0.0749,0.0244,-0.0469,0.0018,-0.0492,0.0608,-0.0581,0.0104,-0.0064,0.053,0.0251,-0.068,0.0341,-0.0437,0.0201,0.0797,-0.0453,0.0207,-0.0487,0.0447,0.1366,0.0197,0.0444,0.0173,0.0116,-0.0834,-0.0399,0.0308,0.0571,-0.0181,0.0542,-0.0035,-0.0432,-0.0948,0.0003,0.0208,-0.0755,-0.0515,0.0851,-0.0645,0.0229,0.0053,-0.0577,-0.0133,-0.0073,-0.0291,-0.0338,0.018,0.0197,0.0506,0.0583,-0.0647,0.0328,0.0209,-0.0932,-0.0311,0.0948,0.0132,-0.0796,-0.0375,-0.0223,0.024,0.0046,0.0099,0.032,-0.0332,-0.007,0.0461,-0.0201,-0.0595,0.0103,0.0041,0.0066,0.0208,-0.0561,-0.0031,0.0178,0.061,-0.0598,0.0013,-0.0212,0.0073,0.059,-0.0693,0.0031,-0.0301,-0.0392,0.0198,-0.0009,-0.0387,0.0055,0.0357,-0.0573,0.0128,0.026,-0.012,0.0278,-0.0472,0.0591,-0.0034,-0.0191,-0.0134,0.0391,-0.0251,0.0065,0.0761,-0.0271,-0.001,-0.0092,0.0053,0.0546,-0.0277,0.0315,0.0655,-0.0465,-0.0431,-0.2526,-0.0634,0.0082,-0.0432,0.0075,-0.026,0.0332,-0.0124,0.0374,0.0879,0.0661,-0.0021,-0.0187,-0.0088,0.015,0.1023,0.0356,0.0326,-0.0857,0.0159,-0.0221,0.0407,-0.0153,-0.0926,0.0292,0.0071,0.182,0.0298,0.0476,-0.0487,-0.0084,0.0156,-0.0046,-0.0989,0.0444,0.0169,0.0406,-0.0024,-0.0336,-0.0221,-0.038,0.0258,0.0129,-0.079,-0.0168,-0.0142,-0.0333,-0.0165,-0.0455,0.0703,0.0589,-0.0165,0.0749,0.0115,0.0437,-0.0281,-0.0673,0.0416,-0.0373,0.01,-0.0143,-0.0661,0.0361,-0.0623,0.0733,-0.0025,-0.0478,-0.0688,0.0771,-0.0332,-0.0415,0.1079,0.0016,-0.0003,0.0604,0.0106,0.0097,-0.0119,-0.0329,-0.0225,0.0798,0.0026,0.0399,0.0126,0.008,0.0219,0.0349,0.013,0.0236,-0.0207,0.014,0.0177,-0.0453,-0.0311,-0.004,0.0386,-0.2981,0.0081,-0.0337,0.0338,-0.0638,0.0413,0.0543,-0.0002,-0.0058,-0.0372,-0.0164,0.0139,0.0287,-0.0288,0.0266,0.0257,0.0735,-0.0615,0.0649,-0.031,0.0156,0.0892,0.254,-0.0261,-0.0004,0.0087,0.0168,0.0067,0.0402,-0.0353,0.0408,0.0067,0.1162,-0.0371,0.0243,0.0736,-0.038,0.0282,0.0059,-0.0398,0.0077,-0.0081,-0.0208,-0.0182,0.0676,-0.0156,0.0016,-0.027,0.0364,0.037,-0.0217,-0.0162,-0.0022,0.0142,0.028,0.0145,-0.0528,-0.0469,-0.0412,-0.0335,0.0379,-0.0727,0.01,0.0301,-0.0086]}
{"key":"[Learning Algebraic Structures: Preliminary Investigations] We employ techniques of machine-learning, exemplified by support vector machines and neural classifiers, to initiate the study of whether AI can \"learn\" algebraic structures. Using finite groups and finite rings as a concrete playground, we find that questions such as identification of simple groups by \"looking\" at the Cayley table or correctly matching addition and multiplication tables for finite rings can, at least for structures of small size, be performed by the AI, even after having been trained only on small number of cases. These results are in tandem with recent investigations on whether AI can solve certain classes of problems in algebraic geometry.","layer":3,"vector":[-0.0424,-0.0071,-0.0078,0.0237,-0.0017,0.0126,-0.0117,0.0333,0.0346,-0.0451,0.0046,-0.0375,0.045,0.0425,0.0113,-0.0041,-0.0419,0.0758,-0.0605,0.0128,0.0263,-0.0204,-0.0395,-0.0155,0.0246,0.045,-0.0407,-0.0414,-0.0175,-0.2207,-0.0068,-0.0112,0.0674,-0.0247,0.003,-0.0088,-0.0288,0.0616,-0.0479,0.0406,0.0406,-0.0068,-0.0269,-0.0462,0.0058,-0.028,0.0055,-0.0186,-0.0412,-0.0454,0.0042,-0.0361,0.0285,0.0412,0.0204,0.0558,0.05,0.0242,0.0373,0.0308,0.0253,0.0476,-0.1569,0.1024,0.011,0.0705,-0.0269,-0.0332,0.0366,0.076,-0.0012,0.0276,0.031,0.0029,0.0212,0.008,0.0007,-0.0614,-0.0107,0.0189,-0.0276,-0.0449,-0.0408,0.0106,-0.0098,-0.0151,-0.0166,-0.0171,0.0602,-0.0031,-0.0204,0.0255,-0.0464,0.0572,-0.0468,-0.0273,0.035,-0.0004,-0.0555,0.2236,-0.0537,-0.006,0.0657,-0.0516,-0.0278,-0.0095,-0.0396,-0.0414,-0.0333,-0.0756,-0.0115,-0.0419,0.009,-0.0198,-0.0192,0.001,0.0305,0.0483,-0.0485,-0.002,0.015,0.0191,0.0603,0.0261,0.0305,-0.0511,-0.0238,0.1416,0.0481,0.0784,0.0406,-0.0213,-0.0223,-0.0461,-0.0091,0.0334,0.0662,-0.0073,-0.0077,-0.0021,-0.0745,-0.056,0.0381,-0.0603,-0.0486,0.0972,-0.0389,0.0214,-0.0513,-0.0004,-0.0463,0.0232,-0.0805,-0.0317,0.0065,-0.0148,0.0108,0.0198,-0.055,-0.012,-0.0332,-0.0327,-0.0622,0.1579,0.0267,-0.0858,-0.0222,-0.0082,0.0296,-0.0077,0.0592,0.0774,-0.0532,0.0581,0.0782,0.0382,-0.0622,-0.0103,0.0198,0.0358,0.0419,-0.0438,-0.0062,0.0694,0.0609,-0.0274,0.0046,-0.0633,0.0405,0.0272,-0.0145,0.0621,-0.0874,-0.02,-0.0641,-0.0334,0.0185,0.014,-0.0048,0.0087,-0.0112,0.0253,-0.0177,0.0635,-0.0126,0.0081,0.0298,0.0012,0.0059,0.0442,0.0014,-0.0223,0.0418,-0.0401,-0.0411,0.018,0.0018,0.0104,-0.0317,0.0357,0.0421,-0.066,-0.0477,-0.2258,-0.0393,-0.0208,-0.0272,0.0532,-0.0783,0.0521,-0.0565,-0.0225,0.0377,0.0524,0.0116,-0.0317,-0.0093,-0.0226,0.0517,0.0147,0.0112,-0.0195,0.043,-0.0085,0.0206,-0.0022,-0.0628,0.011,-0.0091,0.2192,0.0028,0.0304,-0.0052,0.0025,0.007,-0.057,-0.0782,0.0695,-0.0143,0.0526,-0.0209,-0.0022,-0.03,-0.0105,0.0252,0.0138,-0.0606,-0.0306,-0.025,-0.0032,0.0175,-0.0207,0.0268,0.02,-0.0109,0.0657,0.0217,-0.0189,-0.032,-0.0766,0.0134,0.0036,0.0659,0.055,-0.0465,0.0111,-0.0437,0.0629,0.0169,-0.0372,-0.0276,0.0439,-0.053,-0.0059,0.0615,0.004,-0.0338,0.0523,-0.0005,0.0423,-0.021,-0.0163,-0.0178,0.049,-0.0136,0.0485,-0.02,0.0155,0.0044,0.0272,0.0025,0.0376,-0.0203,0.0033,0.024,-0.028,0.0371,0.0344,-0.025,-0.3003,0.075,0.0303,0.0329,-0.0551,0.0381,0.035,0.001,-0.0383,-0.0588,0.0274,0.0274,0.06,-0.0393,0.0277,0.0385,0.0568,-0.0831,0.0265,-0.0333,-0.0058,0.0624,0.2608,-0.0743,0.0308,-0.0356,-0.016,-0.0328,0.0136,0.0179,0.0288,0.0179,0.0968,-0.0588,0.0418,0.0534,-0.0392,-0.0283,0.0345,0.0001,-0.0343,-0.0179,-0.1137,0.009,0.1096,0.0235,-0.008,-0.0308,-0.007,0.0204,0.0139,0.0024,-0.0449,-0.0121,0.0317,0.0015,-0.0546,-0.0412,-0.0235,-0.043,-0.0072,-0.0666,0.04,0.0232,-0.0132]}
{"key":"[A Note on Rough Set Algebra and Core Regular Double Stone Algebras] Rough Set Theory (RST), first introduced by Pawlak in 1982, is an approach for dealing with information systems where knowledge is uncertain or incomplete.\\cite{Pawlak} It is of fundamental importance in many subfields of artificial intelligence and cognitive science.\\cite{RSTppf} Given a universe $U$ with an equivalence relation $\\theta$, the pair $\\langle U,\\theta\\rangle$ is referred to as an information system and we denote its collection of rough sets $R_\\theta$. In our main Theorem we show $R_\\theta$ with $|\\theta_u| > 1\\ \\forall\\ u \\in U$ to be isomorphic to core regular double Stone algebras, CRDSA, that are complete and atomic, and that the crisp, or definable, sets form a complete atomistic Boolean algebra. These guarantees of infimum/supremeum for arbitrary subsets and formulations in terms of fundamental elements are likely useful if dealing with equivalence relations with an infinite number of partitions, such as projective Hilbert spaces. We further derive that every CRDSA is isomorphic to a subalgebra of a principal rough set algebra, $R_\\theta$, for some approximation space $\\langle U,\\theta \\rangle$. In our main Corollary we show explicitly how to embed $R_\\theta$ into the CRDSA and first demonstrate by extending the culminating finite example of \\cite{RCRDSA}. As our capstone, we consider the projective Hilbert space of complex numbers, $\\mathbb{C}$ and show, among other things, the power set of the set of pure states is a complete, atomistic Boolean algebra. In closing, we suggest other Quantum relevant applications that may be useful, such as Hilbert spaces of operators","layer":1,"vector":[-0.087,-0.044,-0.0163,-0.011,-0.0538,0.0411,0.0834,0.0204,0.0336,-0.0454,0.0011,-0.0841,0.0809,0.0519,0.0449,0.032,0.0046,0.0506,-0.0665,0.0134,0.0993,-0.0397,-0.013,-0.0258,0.0473,0.0281,-0.0195,-0.0309,-0.0386,-0.1749,-0.0275,0.0027,0.0671,-0.027,0.0361,-0.0763,-0.0398,0.0811,-0.0402,0.0374,0.042,-0.0162,0.0328,-0.0158,-0.0552,-0.0322,-0.0192,-0.0185,-0.0346,-0.0806,0.0348,-0.0064,0.0046,-0.0114,0.0406,0.0081,0.0422,0.043,0.0501,0.0264,-0.0108,0.0339,-0.167,0.0778,0.0725,0.0121,-0.0286,-0.0393,0.0816,0.0209,-0.0736,0.0567,0.0099,0.0645,0.0574,-0.0455,-0.0118,-0.0227,-0.0043,0.0702,0.0086,-0.0445,-0.0201,0.0225,-0.0696,-0.0528,0.019,-0.0593,0.0402,-0.0077,0.015,-0.0219,-0.0186,0.0222,-0.0114,-0.0022,0.036,0.0417,-0.0264,0.227,-0.0353,-0.0007,0.0715,-0.069,0.0284,-0.0373,-0.0017,-0.0858,-0.0537,-0.0026,-0.0019,-0.0093,0.0242,-0.0527,0.0386,-0.0365,0.0841,0.0404,-0.0101,-0.0158,-0.0395,0.0319,0.0552,0.0022,-0.0037,-0.0426,-0.0029,0.1345,0.0106,0.0534,0.0336,-0.0411,-0.0608,-0.02,-0.0325,0.0567,0.0024,0.0333,0.0385,-0.009,-0.0097,-0.0918,0.0286,-0.0573,-0.0234,0.0528,-0.0446,0.0011,-0.0174,0.0222,0.0033,-0.0089,-0.0032,-0.0256,0.0138,0.0059,-0.0001,-0.0279,-0.0431,0.0123,-0.0209,-0.0664,-0.0113,0.1318,-0.0056,-0.0662,-0.0415,-0.02,0.0137,-0.0019,0.0353,0.0601,-0.0178,0.0253,0.0503,-0.0055,-0.0334,0.0203,-0.0424,0.0332,0.0515,-0.028,-0.0413,0.0524,-0.0202,-0.0256,-0.0067,-0.0118,0.0328,-0.0192,-0.0175,0.0042,-0.0801,-0.0076,-0.0288,-0.0361,-0.0031,-0.0015,0.008,0.018,0.0356,0.0162,-0.0466,0.0113,-0.0202,0.0065,-0.0006,-0.0015,-0.0048,0.038,0.0136,-0.012,-0.0024,0.0016,-0.0226,-0.009,0.0177,0.0026,0.0091,0.002,0.0342,-0.0599,-0.0856,-0.2416,-0.016,-0.0173,-0.0025,0.0752,-0.0202,0.0536,-0.0236,-0.01,0.0695,0.0693,0.0275,-0.0002,-0.005,-0.0444,0.0682,0.0363,0.0323,-0.048,0.0043,0.0122,0.0519,0.0004,-0.0808,0.067,0.0288,0.2342,0.032,0.0254,0.0111,0.0241,0.0504,-0.0411,-0.0875,0.0141,0.0305,-0.0009,0.0079,0.0055,-0.0866,0.0032,-0.0024,-0.0387,-0.0774,0.0464,-0.0368,-0.0178,0.0103,0.0043,0.0303,-0.0034,-0.0563,-0.0142,-0.0033,-0.0269,-0.0082,-0.0598,0.0142,0.0091,0.0585,-0.0049,-0.0473,0.0129,-0.0178,0.0158,0.0167,-0.0163,-0.0262,-0.007,-0.062,-0.0396,0.0402,-0.0302,-0.031,0.0239,0.0319,0.0161,0.0087,-0.0071,-0.0195,0.0458,-0.0157,0.0584,0.0479,-0.0023,-0.0048,0.0824,-0.0008,-0.0008,-0.0019,0.0199,0.0279,-0.0349,0.0425,0.0252,-0.0017,-0.3176,0.0528,-0.0246,-0.0279,-0.086,0.0193,0.0088,0.0236,-0.0312,-0.0277,0.0413,0.0491,0.0292,-0.0189,-0.0353,0.0278,0.0758,-0.0755,0.0712,-0.0134,0.0576,0.0277,0.2581,0.009,0.0375,0.0222,-0.007,-0.0197,-0.0057,0.0113,0.0016,0.0231,0.0795,-0.0782,0.044,0.0417,-0.0278,0.0698,0.0108,-0.0194,-0.0509,0.0021,-0.0563,0.0117,0.1012,-0.0296,-0.0789,-0.0165,0.0055,0.0147,0.0056,-0.0063,-0.0259,-0.0421,0.0188,0.0326,-0.0761,-0.015,-0.0474,-0.0305,0.0105,-0.0425,-0.0081,0.0429,0.005]}
{"key":"[Generative Adversarial Networks for Bitcoin Data Augmentation] In Bitcoin entity classification, results are strongly conditioned by the ground-truth dataset, especially when applying supervised machine learning approaches. However, these ground-truth datasets are frequently affected by significant class imbalance as generally they contain much more information regarding legal services (Exchange, Gambling), than regarding services that may be related to illicit activities (Mixer, Service). Class imbalance increases the complexity of applying machine learning techniques and reduces the quality of classification results, especially for underrepresented, but critical classes. In this paper, we propose to address this problem by using Generative Adversarial Networks (GANs) for Bitcoin data augmentation as GANs recently have shown promising results in the domain of image classification. However, there is no \"one-fits-all\" GAN solution that works for every scenario. In fact, setting GAN training parameters is non-trivial and heavily affects the quality of the generated synthetic data. We therefore evaluate how GAN parameters such as the optimization function, the size of the dataset and the chosen batch size affect GAN implementation for one underrepresented entity class (Mining Pool) and demonstrate how a \"good\" GAN configuration can be obtained that achieves high similarity between synthetically generated and real Bitcoin address data. To the best of our knowledge, this is the first study presenting GANs as a valid tool for generating synthetic address data for data augmentation in Bitcoin entity classification.","layer":1,"vector":[-0.0489,-0.0569,0.0092,-0.0362,0.0514,0.0268,0.0343,-0.0213,0.0003,0.0118,0.0133,-0.0375,0.0128,0.0309,0.0425,0.0126,0.0383,0.0067,-0.0127,0.0204,0.0659,-0.0375,0.0162,-0.024,0.0271,0.0153,-0.0052,-0.0495,-0.0492,-0.2396,0.066,-0.0145,0.0289,-0.0209,0.0294,-0.0265,-0.0542,0.0799,-0.0358,0.0456,0.0036,0.0051,-0.0368,-0.0755,0.0046,-0.0337,-0.0229,-0.0064,-0.0345,-0.0271,0.0568,-0.0393,0.0035,0.0412,0.0102,-0.0048,0.0456,0.0166,0.051,0.0525,0.0681,0.0778,-0.1297,0.0425,0.0485,0.0311,-0.0425,-0.0381,-0.0122,0.0426,0.0251,0.0277,0.0144,0.0429,-0.0097,0.0014,-0.0022,-0.0267,-0.0262,0.002,-0.0157,-0.0127,-0.029,0.005,-0.0187,-0.0122,0.0054,-0.0384,0.0501,-0.0211,-0.0455,0.0213,-0.0176,0.0285,-0.0135,0.0159,0.0026,0.034,-0.0955,0.2064,-0.0473,0.0339,0.0323,-0.0311,0.0272,0.0056,-0.0459,-0.0298,-0.0399,-0.0296,0.0004,-0.0015,0.0281,-0.0129,-0.0329,-0.0422,0.0442,-0.0213,-0.0145,0.0136,-0.0329,0.0254,0.0389,-0.0158,0.0024,-0.0665,0.0371,0.1298,0.0191,0.0384,-0.0067,0.009,-0.0642,-0.0138,-0.0218,-0.0031,-0.0168,0.0158,0.0057,-0.0093,-0.026,-0.0153,-0.003,-0.0495,-0.0544,0.1066,-0.0366,0.0512,-0.015,-0.034,-0.0162,0.0115,-0.0669,-0.0309,-0.0059,0.086,0.0233,0.0422,-0.0189,0.0087,-0.0185,-0.0377,-0.0339,0.123,0.0273,-0.1085,-0.0333,0.0133,-0.0047,-0.0307,0.0205,0.0235,-0.0612,0.0708,0.0831,0.0216,-0.0504,-0.0489,-0.0604,-0.0065,-0.0133,-0.0664,-0.0404,0.0351,0.0373,-0.0266,0.0038,-0.0222,0.0153,0.0568,-0.0545,0.0263,-0.0578,-0.0128,0.0067,-0.0503,-0.0177,-0.0251,-0.0077,0.0134,0.0401,0.0775,-0.0373,0.0161,-0.04,0.011,0.0196,0.0018,0.0484,0.0203,-0.029,-0.0048,0.0391,-0.0218,-0.0153,-0.0315,0.0217,0.0688,-0.0132,0.0261,0.0092,-0.0521,-0.0139,-0.2422,-0.0204,-0.0257,-0.0333,0.0683,-0.0931,0.0493,0.0127,0.0469,0.0656,0.0465,-0.0111,-0.0084,0.0252,-0.0222,0.0657,0.0241,0.0242,-0.0129,-0.011,-0.058,0.0079,-0.0065,-0.1006,0.0425,0.0105,0.2204,0.0471,-0.0155,-0.0007,0.019,0.0316,-0.0214,-0.1439,0.05,0.0216,0.1141,0.0486,-0.0365,-0.0068,-0.0573,0.0494,0.0208,-0.1122,0.0061,-0.0096,-0.0331,0.0304,-0.0485,0.0394,0.0259,-0.0208,0.047,0.0331,0.034,-0.037,-0.0812,0.0299,-0.0343,0.0146,0.0067,-0.0937,0.0241,-0.0749,0.048,0.0063,-0.0683,-0.045,0.0262,0.019,-0.0495,0.0982,-0.0201,-0.0061,0.0701,0.0035,0.0223,-0.0652,-0.0549,0.001,-0.0006,0.0086,0.0569,0.0385,0.027,0.0408,0.0731,0.0182,0.0396,-0.0026,-0.0084,0.019,-0.0921,-0.0264,0.0128,0.0091,-0.2768,0.0214,-0.0173,0.067,0.0227,0.0276,0.0769,0.0404,-0.047,-0.0032,0.0267,-0.0007,0.0853,-0.0464,-0.0255,0.0084,0.026,-0.0703,0.0056,-0.0024,0.0438,0.0345,0.2315,-0.0628,0.003,0.0197,-0.0295,0.0444,0.0121,-0.0203,0.0236,0.035,0.0919,-0.0253,0.0417,0.1026,-0.059,0.0312,0.0191,-0.0203,-0.0189,0.0225,-0.0797,0.0017,0.0657,0.0066,-0.0376,-0.0372,0.0369,0.0237,-0.0808,0.0218,-0.0299,0.0401,0.0314,0.0312,-0.0556,-0.0253,-0.0018,-0.0038,0.0029,-0.0423,-0.0366,-0.0137,-0.0731]}
{"key":"[ErfAct and Pserf: Non-monotonic Smooth Trainable Activation Functions] An activation function is a crucial component of a neural network that introduces non-linearity in the network. The state-of-the-art performance of a neural network depends also on the perfect choice of an activation function. We propose two novel non-monotonic smooth trainable activation functions, called ErfAct and Pserf. Experiments suggest that the proposed functions improve the network performance significantly compared to the widely used activations like ReLU, Swish, and Mish. Replacing ReLU by ErfAct and Pserf, we have 5.68% and 5.42% improvement for top-1 accuracy on Shufflenet V2 (2.0x) network in CIFAR100 dataset, 2.11% and 1.96% improvement for top-1 accuracy on Shufflenet V2 (2.0x) network in CIFAR10 dataset, 1.0%, and 1.0% improvement on mean average precision (mAP) on SSD300 model in Pascal VOC dataset.","layer":0,"vector":[-0.0621,-0.0374,0.0438,-0.0291,0.0126,0.0621,0.0081,0.0478,0.0049,-0.0241,-0.0023,-0.0779,0.0326,0.0581,-0.0026,0.0144,-0.0055,0.0572,-0.0192,0.0363,0.0245,-0.0414,-0.0155,-0.0133,0.0057,-0.0071,-0.0395,-0.0349,-0.0611,-0.2637,-0.0042,-0.0226,0.0788,-0.035,-0.0247,-0.0088,-0.0264,0.0367,-0.0631,0.0284,0.0191,0.0159,-0.003,-0.0595,-0.0162,-0.0442,-0.0311,0.0243,0.0134,0.0155,0.0205,-0.03,0.0237,-0.0144,0.0275,0.0114,0.0221,0.0521,0.0712,0.072,0.0406,0.0556,-0.1679,0.0421,0.0279,-0.0073,-0.0193,-0.063,-0.002,0.0667,-0.0104,0.0418,0.0344,0.0269,0.047,0.0125,0.0019,-0.0151,-0.018,0.0519,0.0288,-0.0428,-0.0501,-0.0476,0.0101,0.013,0.0019,-0.0428,-0.0169,-0.0065,0.013,0.0295,-0.015,0.0399,-0.0379,0.0138,0.0619,0.0297,-0.0606,0.1995,-0.0696,0.0235,-0.0046,0.0189,0.0345,-0.0266,-0.0325,-0.0157,-0.0379,-0.0079,-0.0199,-0.0523,0.0626,-0.0576,0.0206,0.0478,0.0541,0.0159,0.0111,-0.0293,-0.0073,0.0359,0.0272,-0.0217,0.0163,-0.0421,0.026,0.1265,0.0485,0.0427,0.027,-0.0455,-0.0318,-0.0141,0.0387,0.0177,0.0501,0.0266,-0.0233,0.0164,-0.0114,-0.0436,0.0249,-0.1259,-0.0622,0.1204,-0.0332,0.0317,-0.0587,0.0055,-0.0466,0.0476,-0.0191,-0.035,0.0062,0.0323,0.0405,0.0241,-0.0673,-0.0134,-0.0224,-0.0623,-0.066,0.1074,0.0147,-0.0929,-0.0409,0.0171,0.0102,-0.0479,0.0238,0.0256,-0.0428,0.0396,0.0733,0.0312,-0.0178,-0.0226,-0.0,0.0214,0.0289,-0.0587,-0.0141,0.0556,0.0158,-0.0575,-0.0049,-0.0803,0.0333,0.0275,-0.0708,0.0034,-0.0034,0.0092,-0.013,-0.0232,-0.0057,0.0032,-0.0182,-0.0422,0.0196,-0.0195,-0.0085,0.0095,0.0202,0.0307,-0.0382,-0.0103,0.03,0.0226,-0.0025,-0.0097,0.0617,-0.0341,-0.0513,-0.0087,0.0029,0.0521,0.0124,0.0335,0.0185,-0.0418,-0.0687,-0.2246,-0.002,0.0411,-0.0134,0.0995,-0.0681,0.0375,0.0381,0.0612,0.0594,0.0614,0.0214,-0.0122,0.0281,-0.0142,0.0991,0.0126,-0.0153,-0.0053,-0.0197,-0.0074,0.0904,-0.0046,-0.0439,0.075,0.0222,0.1684,-0.035,0.0703,-0.0635,0.044,0.0327,-0.0594,-0.0852,0.0564,0.0272,0.0801,-0.0442,-0.0708,-0.0493,-0.0219,0.0037,0.0189,-0.1041,-0.0507,-0.0221,-0.0299,0.0365,-0.0743,0.0081,0.0247,-0.0463,0.0752,0.0106,0.0155,-0.0367,-0.0797,0.0505,-0.0654,0.0285,0.0121,-0.0455,-0.0185,-0.0557,0.0352,0.012,-0.0549,-0.0607,0.0763,-0.0018,-0.035,0.102,0.0084,0.0317,0.1032,0.0145,0.0121,-0.0022,-0.013,-0.0249,0.0583,-0.0194,0.0229,-0.0133,0.0072,-0.0372,0.0994,-0.0366,0.0287,0.0114,-0.0173,0.0049,-0.0366,-0.0212,0.0632,-0.0116,-0.3044,0.0071,0.0106,-0.0,-0.0086,0.0215,0.0512,-0.0192,-0.0595,-0.0127,-0.0105,0.0279,0.0445,-0.0028,-0.0223,0.021,0.0606,-0.0488,0.0585,-0.0532,0.0336,0.0495,0.2256,-0.0558,0.0121,0.0001,-0.0141,-0.006,0.029,-0.0443,0.0239,-0.0219,0.0123,-0.0329,0.0201,0.0517,-0.0242,0.053,-0.0236,-0.0341,-0.0045,0.0357,-0.0485,-0.042,0.0607,-0.0297,0.0117,-0.0255,-0.003,0.0159,-0.0303,0.0065,0.02,0.0223,-0.0079,0.0417,-0.0574,-0.057,-0.0784,0.0127,0.0297,-0.0589,0.0142,0.0333,-0.0157]}
{"key":"[Robust Subspace Clustering via Thresholding] The problem of clustering noisy and incompletely observed high-dimensional data points into a union of low-dimensional subspaces and a set of outliers is considered. The number of subspaces, their dimensions, and their orientations are assumed unknown. We propose a simple low-complexity subspace clustering algorithm, which applies spectral clustering to an adjacency matrix obtained by thresholding the correlations between data points. In other words, the adjacency matrix is constructed from the nearest neighbors of each data point in spherical distance. A statistical performance analysis shows that the algorithm exhibits robustness to additive noise and succeeds even when the subspaces intersect. Specifically, our results reveal an explicit tradeoff between the affinity of the subspaces and the tolerable noise level. We furthermore prove that the algorithm succeeds even when the data points are incompletely observed with the number of missing entries allowed to be (up to a log-factor) linear in the ambient dimension. We also propose a simple scheme that provably detects outliers, and we present numerical results on real and synthetic data.","layer":6,"vector":[-0.0073,-0.0387,0.012,0.0023,0.0407,0.0045,0.0605,0.0148,0.0609,0.0012,0.0594,-0.0785,-0.0033,0.0582,-0.0093,0.0294,0.0329,0.0776,-0.0356,0.0123,-0.0256,-0.0187,-0.0221,-0.035,0.0266,0.0282,-0.0203,-0.0381,-0.0886,-0.2638,-0.0232,-0.0247,0.0958,-0.0231,0.0445,-0.0565,-0.0177,0.0526,-0.0359,0.0232,0.006,0.0139,-0.0115,-0.0488,-0.1081,-0.047,-0.0025,0.0248,-0.0051,-0.0433,-0.0092,-0.033,-0.0092,0.0496,0.0701,0.0027,0.0529,0.018,0.0311,0.058,0.0703,0.0222,-0.1463,0.0418,0.0746,-0.0447,-0.0225,-0.0228,0.0072,0.007,0.0085,0.0727,0.0107,0.0226,0.0413,-0.0046,-0.0043,-0.0223,-0.0054,0.0275,0.0103,-0.0512,-0.0271,0.0093,-0.0726,-0.0517,0.0074,-0.0646,0.0337,0.0093,-0.0591,0.0288,-0.0125,0.0469,-0.0998,-0.0445,0.0595,0.0042,0.0232,0.2107,-0.063,0.0344,0.0785,-0.0276,-0.0038,-0.0808,-0.0259,-0.0518,-0.0096,-0.0405,0.0138,0.0004,-0.0086,-0.0482,-0.0277,-0.0232,0.0915,0.0321,-0.0178,-0.0206,-0.0302,-0.0271,0.0965,-0.0146,0.088,-0.0468,0.0555,0.1217,0.0708,0.0269,0.0313,0.0347,-0.0243,-0.0071,-0.0002,0.0224,-0.0011,0.0091,0.0279,0.0008,-0.0104,-0.0832,0.0506,-0.0507,-0.026,0.1156,-0.0577,0.025,-0.0148,-0.0134,-0.0027,0.0185,-0.0399,0.0046,0.0104,-0.0186,0.0223,0.0047,-0.0186,0.0099,-0.0611,-0.0716,0.0398,0.1275,-0.0049,-0.0821,-0.0434,0.0123,0.0406,0.0047,0.033,0.0519,0.0056,0.0636,0.0824,-0.0026,-0.0676,0.0042,-0.0021,0.0189,0.0185,-0.0453,-0.0276,0.0224,0.0776,-0.0184,-0.0228,0.0066,0.0092,0.0173,-0.0704,-0.0337,-0.0522,-0.0132,-0.0515,-0.0558,-0.0037,-0.0211,0.0108,-0.0294,0.0394,0.0193,-0.0471,0.0581,0.0263,0.0178,0.039,-0.0351,0.022,0.0608,0.0084,-0.0111,0.0385,-0.0424,-0.0141,-0.026,0.0088,0.046,-0.0313,0.063,0.0483,-0.0289,-0.1272,-0.2275,-0.0297,-0.0345,-0.0028,0.0063,-0.0443,0.0351,-0.0123,0.0817,0.0529,0.0313,0.01,-0.0065,0.0268,-0.0106,0.0775,0.0165,0.0215,-0.0361,0.0035,-0.0424,0.0284,-0.0433,-0.0205,0.0541,-0.0129,0.1921,0.0258,0.0174,-0.0269,0.0351,0.0062,-0.0397,-0.0455,0.0511,0.0402,0.0752,0.0321,-0.0286,-0.0315,-0.0345,0.0016,0.022,-0.0672,-0.0133,-0.0505,0.001,-0.0024,-0.0445,0.0152,0.067,-0.0261,0.0544,-0.007,0.0205,-0.0235,-0.0624,0.029,-0.0281,0.0216,0.007,-0.0811,0.0035,-0.0859,0.0741,-0.0013,-0.0205,-0.0526,0.0346,-0.0429,-0.032,0.0858,-0.0117,0.0081,0.0493,-0.0258,0.0206,-0.0076,-0.0656,0.0046,0.0674,-0.0213,0.0159,0.0224,0.0393,0.0263,0.0641,-0.0046,0.0116,-0.0169,0.012,0.0117,-0.0414,0.0065,0.0359,0.0191,-0.27,-0.0181,0.0002,-0.0012,-0.0291,-0.0509,-0.0069,0.0125,-0.0253,-0.0043,-0.0012,0.0391,0.0174,-0.0507,0.0119,0.058,0.0555,-0.0406,0.0659,-0.0648,0.0185,-0.0013,0.2184,-0.0692,0.0254,0.0206,-0.0087,0.0111,-0.0015,-0.0497,-0.0112,0.0176,0.1097,-0.026,0.052,0.0694,-0.0203,0.0115,0.0278,0.0022,-0.0125,0.0054,-0.0125,-0.0515,0.1065,-0.0182,0.0001,-0.0407,0.046,0.0301,-0.0402,-0.0171,-0.0126,-0.0096,-0.0075,0.0567,-0.0629,-0.0531,-0.0065,-0.0435,-0.0345,-0.0696,-0.0557,-0.0048,0.0035]}
{"key":"[Peer Offloading with Delayed Feedback in Fog Networks] Comparing to cloud computing, fog computing performs computation and services at the edge of networks, thus relieving the computation burden of the data center and reducing the task latency of end devices. Computation latency is a crucial performance metric in fog computing, especially for real-time applications. In this paper, we study a peer computation offloading problem for a fog network with unknown dynamics. In this scenario, each fog node (FN) can offload their computation tasks to neighboring FNs in a time slot manner. The offloading latency, however, could not be fed back to the task dispatcher instantaneously due to the uncertainty of the processing time in peer FNs. Besides, peer competition occurs when different FNs offload tasks to one FN at the same time. To tackle the above difficulties, we model the computation offloading problem as a sequential FN selection problem with delayed information feedback. Using adversarial multi-arm bandit framework, we construct an online learning policy to deal with delayed information feedback. Different contention resolution approaches are considered to resolve peer competition. Performance analysis shows that the regret of the proposed algorithm, or the performance loss with suboptimal FN selections, achieves a sub-linear order, suggesting an optimal FN selection policy. In addition, we prove that the proposed strategy can result in a Nash equilibrium (NE) with all FNs playing the same policy. Simulation results validate the effectiveness of the proposed policy.","layer":0,"vector":[-0.0764,-0.021,-0.0097,-0.0069,0.0183,0.0483,0.0547,0.0341,0.0269,-0.0099,0.0096,0.0255,0.0365,0.068,-0.0009,0.0339,-0.0169,-0.0137,-0.0269,-0.0295,0.0267,-0.0978,-0.0308,-0.0676,0.013,0.0121,-0.063,-0.0855,-0.0514,-0.214,-0.0006,-0.084,0.0323,-0.0204,0.0158,-0.0732,-0.0038,0.027,-0.029,0.0375,0.024,0.0334,-0.0484,-0.0728,-0.008,-0.0184,-0.0059,-0.0276,-0.032,-0.0519,0.0417,-0.0513,0.0112,0.01,0.0552,0.0175,0.0023,0.059,0.0276,0.0245,0.0425,0.0454,-0.1613,0.0588,0.0495,0.0284,-0.0286,-0.0347,0.022,0.0445,-0.0107,0.0672,0.0248,0.0436,0.0097,0.0341,-0.0167,-0.0004,0.0114,0.0089,0.0192,-0.0017,-0.0373,-0.0298,-0.0281,-0.0253,0.0141,-0.0144,0.0484,-0.0117,-0.0169,0.0201,-0.0177,0.0604,-0.007,-0.0169,0.0152,0.0049,-0.0766,0.2313,-0.0267,0.0366,0.0125,-0.0357,0.0317,-0.0654,-0.0222,-0.06,0.0002,0.0501,-0.0096,-0.0137,0.0675,-0.0452,0.0318,0.0423,0.007,0.0674,-0.0254,-0.0122,-0.0232,0.0455,0.0608,-0.025,0.0314,-0.0696,0.0074,0.1477,-0.0048,-0.0018,0.0483,-0.0465,-0.0174,-0.0368,0.0584,0.0229,-0.0146,0.0031,-0.0261,0.0047,0.0132,-0.0215,0.0468,-0.1007,0.0056,0.1288,0.04,0.0714,-0.0143,-0.062,-0.0399,-0.001,-0.0332,-0.0359,-0.0135,0.0554,0.0375,0.0719,-0.0135,-0.0089,-0.0462,-0.0337,-0.0014,0.0748,-0.0296,-0.0937,0.0042,0.002,-0.0158,-0.0054,0.0306,-0.0005,-0.0445,0.0202,0.0974,0.0147,-0.0818,-0.0456,-0.0118,0.0025,0.0197,-0.0249,0.0157,0.0141,0.0338,-0.0719,-0.018,-0.0519,0.0112,0.0422,-0.0705,-0.0088,-0.0476,0.0085,-0.0303,-0.0654,0.0138,0.0177,0.0377,-0.0124,0.0324,0.0044,-0.0746,-0.0145,0.0011,0.0051,0.0247,-0.0277,0.0306,0.0197,-0.0237,-0.012,-0.0065,-0.0236,-0.0245,0.0243,0.031,0.0573,-0.0167,0.0298,0.0263,-0.0003,-0.0479,-0.1634,0.0046,-0.0628,-0.0011,0.0618,-0.0141,0.0623,-0.035,0.0175,0.0503,0.0922,-0.0188,-0.0199,0.0376,-0.0435,0.0903,0.0233,0.0356,0.0065,0.0027,0.0053,0.034,-0.0303,-0.0618,0.0549,0.039,0.2581,-0.0107,0.0567,-0.0347,0.0359,0.0708,-0.0172,-0.0975,0.0238,0.0383,0.1116,-0.0243,-0.0121,0.0089,-0.0169,0.0321,-0.0079,-0.0823,-0.0099,-0.0071,-0.0286,0.0182,-0.0767,-0.0387,0.0462,-0.0313,0.0625,-0.0036,0.0025,-0.0143,-0.0852,0.0486,-0.0517,0.0191,0.0043,-0.0559,-0.051,-0.0714,0.0637,0.0132,-0.0054,-0.0656,0.0039,-0.0106,0.0103,0.0348,-0.041,0.0276,0.0096,-0.0045,0.028,-0.0374,-0.032,-0.0235,0.0683,-0.0919,0.0324,0.0674,-0.0268,-0.0149,0.0648,0.0421,-0.0292,-0.0428,-0.0164,0.0356,-0.0557,-0.0432,0.0559,0.0007,-0.3004,0.0482,0.0434,0.0542,-0.0193,0.0098,0.0423,0.032,-0.0332,0.0254,0.0102,0.0637,-0.0183,0.0045,0.0072,0.0518,0.0759,-0.0401,0.0018,-0.0441,-0.0051,0.0239,0.2099,-0.0668,0.0644,0.0072,-0.0412,0.0596,-0.0163,-0.0083,0.0069,0.0151,0.0768,-0.0842,0.0048,0.0594,-0.026,0.0489,0.0454,-0.0024,-0.0196,0.0061,-0.0203,0.0223,0.0788,0.0221,-0.0719,-0.0455,0.0124,0.0314,-0.0641,-0.0191,-0.015,0.0052,0.0024,0.0087,-0.0588,-0.0634,-0.0326,-0.0163,0.0244,-0.0543,0.0137,-0.0366,0.0214]}
{"key":"[Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training] Large-scale distributed training requires significant communication bandwidth for gradient exchange that limits the scalability of multi-node training, and requires expensive high-bandwidth network infrastructure. The situation gets even worse with distributed training on mobile devices (federated learning), which suffers from higher latency, lower throughput, and intermittent poor connections. In this paper, we find 99.9% of the gradient exchange in distributed SGD is redundant, and propose Deep Gradient Compression (DGC) to greatly reduce the communication bandwidth. To preserve accuracy during compression, DGC employs four methods: momentum correction, local gradient clipping, momentum factor masking, and warm-up training. We have applied Deep Gradient Compression to image classification, speech recognition, and language modeling with multiple datasets including Cifar10, ImageNet, Penn Treebank, and Librispeech Corpus. On these scenarios, Deep Gradient Compression achieves a gradient compression ratio from 270x to 600x without losing accuracy, cutting the gradient size of ResNet-50 from 97MB to 0.35MB, and for DeepSpeech from 488MB to 0.74MB. Deep gradient compression enables large-scale distributed training on inexpensive commodity 1Gbps Ethernet and facilitates distributed training on mobile. Code is available at: https://github.com/synxlin/deep-gradient-compression.","layer":1,"vector":[-0.023,-0.0035,0.0197,-0.0067,0.0333,0.0364,-0.0294,-0.0104,0.0395,-0.0253,-0.0007,-0.0528,0.0637,0.0453,0.0477,0.0104,0.0469,-0.0028,-0.0315,-0.0233,0.0368,-0.0198,-0.0138,-0.0721,0.0216,0.0131,-0.0855,-0.052,-0.0502,-0.2159,0.059,-0.0359,0.0443,0.0103,0.0517,-0.022,-0.0257,0.0151,-0.0645,0.0418,-0.0109,0.0466,-0.0744,-0.0028,0.0026,-0.0385,-0.0279,-0.0579,-0.0245,-0.0436,0.0584,-0.0571,-0.0025,0.0163,-0.0085,0.0382,0.0537,0.0703,0.0602,0.0173,0.0187,0.0569,-0.1969,0.0912,0.0297,0.0121,-0.032,0.0205,0.0206,0.0361,-0.0148,0.0274,0.0683,0.0322,-0.0031,-0.0354,0.0076,-0.005,0.0133,-0.0059,0.0319,-0.0253,-0.0539,-0.0396,-0.0155,-0.0397,0.0498,-0.0396,-0.0049,-0.0296,-0.0699,-0.0042,-0.0195,0.0338,-0.0483,0.0112,-0.0161,0.0181,-0.0703,0.2,-0.0297,0.0324,0.0019,-0.0558,0.0172,-0.0171,-0.0264,-0.0014,-0.0245,0.045,-0.0226,-0.0237,0.0092,-0.0472,0.0448,0.0113,0.057,0.0585,-0.0205,0.0584,-0.0274,0.0141,0.0219,-0.0487,0.0494,-0.0663,0.0066,0.1349,0.0436,0.0559,0.0313,-0.0009,-0.0181,-0.0124,0.0366,0.0375,0.0596,-0.0116,0.0167,-0.0169,-0.0345,-0.0475,-0.0353,-0.0392,-0.056,0.1401,-0.0449,0.0763,-0.0822,-0.0681,-0.0088,0.0006,0.0021,-0.0093,0.03,0.0028,0.0435,0.0793,-0.0653,0.0207,-0.0057,-0.0491,-0.0246,0.0914,0.0279,-0.1089,-0.0501,-0.0018,0.043,-0.0485,0.0414,0.027,-0.0228,0.0221,0.0635,0.0172,-0.0781,-0.0127,-0.0246,0.0107,0.0227,-0.0258,-0.0165,0.0145,0.0146,-0.0332,0.0094,-0.0654,0.0269,0.0384,-0.0246,0.0354,-0.0235,-0.0253,-0.0279,-0.041,-0.0013,0.0003,-0.0151,-0.0308,0.0024,0.0224,-0.0071,0.0496,-0.0072,0.024,-0.0165,-0.0038,-0.008,0.0089,-0.0521,-0.0154,0.0645,-0.0591,0.0011,-0.0116,0.0242,0.0455,-0.0038,0.0571,0.0544,-0.0321,-0.0639,-0.2133,-0.0292,0.0637,-0.0695,0.0629,-0.0547,0.0433,0.0093,0.033,0.051,0.0519,-0.0194,-0.0324,0.0161,-0.0245,0.0522,0.0458,0.05,0.0138,0.018,0.0398,0.024,0.0191,-0.06,0.0848,0.0084,0.2197,-0.0112,0.0551,-0.067,-0.0097,0.039,0.0005,-0.1447,0.0608,-0.0058,0.0607,-0.027,-0.0365,0.0259,-0.0522,0.0477,0.0423,-0.1228,-0.0442,-0.0191,-0.054,0.0212,-0.0801,0.0231,0.0488,-0.0241,0.0715,0.0347,0.0048,-0.0373,-0.0778,0.0388,-0.0365,0.0124,-0.0352,-0.0274,0.015,-0.0442,0.065,-0.0075,-0.0501,-0.0278,0.0122,-0.0227,-0.0128,0.0448,0.0037,0.014,0.0325,0.0165,0.0119,-0.0202,-0.0553,-0.0448,0.1091,-0.0041,0.0842,0.0031,-0.0014,0.0412,0.0947,0.0552,0.0487,-0.0452,-0.0041,0.0259,-0.0402,-0.0228,0.0126,-0.0294,-0.2951,0.036,0.0194,0.032,-0.0377,0.0542,0.0918,-0.0128,-0.0452,0.0414,-0.0044,0.0457,0.0591,-0.0343,0.0279,0.0278,0.0755,-0.0341,0.0495,-0.052,0.0166,0.0337,0.1407,-0.0319,0.0351,-0.0063,-0.0439,0.0069,0.039,-0.0407,-0.0329,-0.0238,0.0711,-0.0455,0.0025,0.0518,-0.0479,0.027,0.0525,-0.0178,0.0074,-0.0049,0.0146,-0.0039,0.0315,0.0105,-0.0291,-0.0546,-0.0416,0.0203,-0.0006,0.0077,0.0062,-0.0339,0.0054,0.0492,-0.0367,-0.045,-0.0542,-0.0496,0.0192,-0.0871,-0.0843,-0.0155,-0.0157]}
{"key":"[Mitigating Annotation Artifacts in Natural Language Inference Datasets to Improve Cross-dataset Generalization Ability] Natural language inference (NLI) aims at predicting the relationship between a given pair of premise and hypothesis. However, several works have found that there widely exists a bias pattern called annotation artifacts in NLI datasets, making it possible to identify the label only by looking at the hypothesis. This irregularity makes the evaluation results over-estimated and affects models' generalization ability. In this paper, we consider a more trust-worthy setting, i.e., cross-dataset evaluation. We explore the impacts of annotation artifacts in cross-dataset testing. Furthermore, we propose a training framework to mitigate the impacts of the bias pattern. Experimental results demonstrate that our methods can alleviate the negative effect of the artifacts and improve the generalization ability of models.","layer":6,"vector":[-0.006,-0.0109,0.013,-0.0252,0.0288,0.0044,0.0278,0.0268,0.0016,-0.0389,-0.0019,-0.0401,0.0624,0.0793,0.0359,0.013,-0.011,0.0487,-0.0578,-0.0016,0.0407,-0.018,0.0109,-0.0509,-0.0251,0.0252,-0.0196,-0.0601,-0.0541,-0.2411,0.0106,-0.0412,0.0517,-0.0152,0.0163,0.0048,-0.0175,0.0596,-0.0423,0.0059,0.0358,-0.0015,0.0165,-0.0335,-0.0148,-0.0525,-0.0552,0.0008,-0.0816,-0.0023,0.0094,-0.0133,0.0105,0.0144,0.0148,0.0861,0.0431,0.0236,0.0639,0.049,0.0015,0.024,-0.1886,0.1091,0.0197,0.0253,-0.0321,0.0063,-0.0157,0.0369,-0.0025,0.0299,0.0448,0.0999,0.0405,0.0125,0.0215,-0.0264,0.0293,0.0122,-0.0114,-0.0108,-0.0167,-0.0032,0.0163,-0.0616,-0.0093,-0.0189,0.019,-0.0141,-0.0052,-0.0056,0.0225,0.0549,-0.0501,-0.0118,0.0317,0.0398,-0.0481,0.213,-0.0717,0.0005,0.0018,-0.0235,0.0177,-0.0315,-0.0251,-0.0538,-0.0152,-0.0112,-0.0317,0.0168,0.0313,-0.0703,0.0689,0.0225,0.1218,-0.0047,-0.0339,0.0239,-0.0376,-0.0174,0.0133,-0.0098,-0.0105,-0.0545,0.0197,0.111,0.0401,-0.0349,0.0295,-0.0591,-0.0633,-0.0508,0.0439,0.0445,0.0196,-0.0003,0.0393,0.0324,-0.0403,-0.0672,-0.0064,-0.0594,-0.0587,0.1115,-0.0215,0.0184,-0.0303,-0.0102,0.0269,0.0355,-0.0059,-0.0321,0.0348,0.026,0.0204,0.0438,-0.0389,0.0324,0.0238,0.0012,-0.0565,0.0649,-0.0072,-0.083,-0.0399,-0.016,0.0345,-0.0445,0.0506,0.0353,-0.0377,0.0217,0.0023,0.0191,-0.0373,0.006,0.0069,0.0489,0.0456,-0.0348,-0.0534,0.1034,-0.0035,-0.0493,-0.0443,-0.0581,0.0414,0.0409,-0.0117,0.0271,-0.0317,0.0322,-0.0119,-0.0306,0.0014,0.0074,0.0196,-0.0267,-0.0058,0.027,-0.0508,-0.0232,-0.0398,0.0016,0.0048,0.014,0.0679,0.0198,-0.0328,-0.0362,0.0557,-0.0161,-0.0237,0.0103,0.0187,-0.001,0.0512,0.0778,-0.0043,-0.0243,-0.0024,-0.2488,-0.0229,0.0329,0.0109,0.0563,-0.045,0.0517,0.0106,0.0349,0.0866,0.0009,-0.0367,-0.0469,0.0197,0.0157,0.056,0.0217,0.02,-0.0218,0.0217,0.0109,-0.0082,-0.0326,-0.1053,0.0525,-0.0414,0.225,0.0162,0.0324,-0.0596,0.0335,0.0452,-0.0333,-0.1346,0.1068,0.0133,0.0447,-0.0255,-0.0482,-0.0302,-0.0243,0.0152,-0.0039,-0.1195,-0.0271,-0.0418,-0.0451,-0.0058,-0.0195,0.0324,0.0252,-0.0094,0.0487,0.0194,-0.0045,-0.0244,-0.1266,0.0204,-0.0415,0.0183,0.0587,0.0175,0.0224,-0.0711,0.0229,-0.0061,-0.0153,-0.0354,0.0232,0.0171,-0.0305,0.0964,-0.0201,-0.0092,0.0272,0.0703,-0.0016,-0.0804,-0.0558,-0.0481,0.0747,-0.042,0.0266,0.0072,0.027,0.0386,0.0794,-0.0303,0.0941,-0.0477,0.0361,-0.0046,-0.0858,-0.0737,0.0642,-0.0111,-0.293,0.0203,-0.0108,0.0307,-0.0314,-0.0041,0.0257,-0.0042,-0.0492,-0.0257,-0.0114,0.0604,0.0548,-0.0104,0.0011,0.0549,0.0624,-0.056,0.0305,-0.026,0.0076,0.0507,0.1812,-0.0123,-0.0095,0.0132,-0.0115,-0.0311,0.0362,0.0324,0.0021,0.0031,0.0826,-0.0149,0.0311,0.0889,-0.048,-0.0224,0.0475,-0.0064,-0.0054,-0.03,-0.0398,-0.0026,0.0428,0.0078,0.02,-0.0417,-0.0251,-0.0064,-0.0272,-0.0166,-0.0102,-0.0092,0.0393,0.0197,-0.0205,-0.0504,-0.0283,-0.0455,-0.0015,-0.0639,-0.0414,0.0022,-0.0069]}
{"key":"[Gasper: GrAph Signal ProcEssing in R] We present a short tutorial on to the use of the \\proglang{R} \\pkg{gasper} package. Gasper is a package dedicated to signal processing on graphs. It also provides an interface to the SuiteSparse Matrix Collection.","layer":0,"vector":[-0.0383,0.0012,0.0341,-0.0359,0.0374,0.0214,0.0225,0.0201,0.0216,-0.0173,0.0477,-0.0882,0.0485,0.0487,0.0279,0.0566,-0.0183,0.0686,-0.0366,-0.0284,0.0177,-0.0073,-0.007,-0.114,0.0372,0.0377,-0.0576,-0.0378,-0.0454,-0.2201,0.0179,-0.0353,0.0862,-0.0023,-0.0278,-0.0222,0.0064,-0.0163,-0.032,0.0429,0.0326,0.0147,-0.0373,-0.0074,-0.0395,-0.0285,-0.0433,-0.0005,0.009,-0.0607,0.0153,-0.0585,0.0322,0.0147,0.0531,0.0281,0.0978,0.0229,0.0551,0.0298,0.0281,0.0315,-0.2062,0.0754,0.0427,0.0351,-0.0498,-0.029,0.0555,0.0338,-0.0167,0.0478,-0.0044,-0.01,-0.0014,-0.0065,-0.017,-0.0292,-0.0047,-0.0154,0.0266,-0.0261,-0.021,0.0144,-0.0207,-0.0227,-0.027,-0.0664,0.0348,-0.0076,-0.0377,0.0049,0.0122,0.0218,-0.0781,0.0063,0.0521,0.0259,-0.0006,0.1912,-0.0548,-0.0091,0.0471,0.0152,0.0279,-0.0864,0.0058,-0.0232,-0.0392,0.0163,0.027,-0.0368,0.0494,-0.0669,0.0495,-0.0399,0.0218,0.0483,-0.014,-0.0203,-0.0076,0.0371,0.024,-0.0001,0.0348,-0.043,0.0513,0.1053,0.0456,0.0328,0.0694,0.05,-0.0294,0.0049,-0.0252,-0.0465,0.0559,0.0348,0.0197,-0.0037,-0.0561,-0.0264,-0.017,-0.0666,-0.0403,0.1347,-0.0639,0.0278,-0.0371,0.0149,-0.0271,0.0247,-0.0057,-0.023,-0.0107,0.0283,0.0402,0.0438,-0.0617,0.0227,-0.0307,-0.0386,-0.0155,0.1032,0.0174,-0.0813,-0.0001,-0.0398,0.0263,-0.0346,0.0457,0.039,-0.0235,0.0178,0.0251,0.0659,-0.0339,-0.0183,0.0245,-0.0007,0.0331,-0.0267,-0.0023,0.0112,0.0074,-0.0637,-0.0389,-0.023,0.0096,0.0232,-0.0273,0.0113,0.0089,-0.0131,-0.056,-0.0116,0.0121,-0.0264,-0.0164,-0.0501,0.0763,0.0151,-0.004,0.043,0.0107,0.0457,-0.0121,-0.0167,0.0364,0.0308,-0.0316,-0.0138,0.0715,-0.0073,-0.0662,-0.0267,0.0413,0.0217,0.0091,0.0408,0.031,-0.0701,-0.0965,-0.2501,-0.0429,-0.0106,-0.0271,0.0819,-0.0589,0.0317,0.0019,0.047,0.084,0.0489,-0.0124,-0.0488,0.0498,0.0069,0.0584,0.0278,0.0292,-0.0319,-0.0233,-0.0251,0.0015,-0.042,-0.0261,0.0201,-0.0235,0.2014,0.0576,0.0345,-0.0137,0.0205,-0.0019,-0.0379,-0.0759,0.0427,0.0721,0.0524,-0.0107,0.0004,-0.0037,-0.0402,-0.0295,-0.0025,-0.0486,-0.0262,-0.0035,0.002,0.0295,-0.049,0.004,0.0392,-0.0339,0.0577,0.0117,0.0332,-0.0835,-0.0523,0.025,-0.0362,0.026,-0.0076,-0.0921,0.0251,-0.0386,0.0594,0.033,0.0016,-0.0032,0.012,0.014,-0.002,0.0753,0.0067,-0.0005,0.0443,0.0031,0.0483,-0.0561,-0.0445,-0.0248,0.0119,-0.0434,0.0181,0.0335,0.0305,0.0235,0.0525,-0.0169,0.0271,-0.0175,0.0053,0.0078,-0.0341,-0.0183,0.0337,-0.0086,-0.3343,-0.0066,0.0195,-0.0073,-0.0408,0.0093,0.0113,0.0106,-0.0565,-0.0375,0.0309,0.0712,0.0188,-0.0262,-0.0201,0.0528,0.0745,-0.0714,0.0203,-0.0473,0.0004,-0.0004,0.2382,-0.0453,0.0466,0.0019,-0.0013,-0.0138,0.0301,-0.0517,-0.0004,0.0175,0.074,-0.0449,0.0541,0.087,-0.0462,0.0859,0.0333,-0.0001,0.0057,-0.0001,0.004,-0.0214,0.0923,-0.0677,-0.0502,-0.0834,-0.016,0.0257,0.0013,0.0119,-0.0288,-0.0135,-0.0156,0.0385,-0.0529,-0.0309,-0.0156,-0.0622,0.0094,-0.0723,-0.0181,-0.0072,0.0191]}
{"key":"[Dual Distribution Alignment Network for Generalizable Person Re-Identification] Domain generalization (DG) serves as a promising solution to handle person Re-Identification (Re-ID), which trains the model using labels from the source domain alone, and then directly adopts the trained model to the target domain without model updating. However, existing DG approaches are usually disturbed by serious domain variations due to significant dataset variations. Subsequently, DG highly relies on designing domain-invariant features, which is however not well exploited, since most existing approaches directly mix multiple datasets to train DG based models without considering the local dataset similarities, i.e., examples that are very similar but from different domains. In this paper, we present a Dual Distribution Alignment Network (DDAN), which handles this challenge by mapping images into a domain-invariant feature space by selectively aligning distributions of multiple source domains. Such an alignment is conducted by dual-level constraints, i.e., the domain-wise adversarial feature learning and the identity-wise similarity enhancement. We evaluate our DDAN on a large-scale Domain Generalization Re-ID (DG Re-ID) benchmark. Quantitative results demonstrate that the proposed DDAN can well align the distributions of various source domains, and significantly outperforms all existing domain generalization approaches.","layer":3,"vector":[0.0002,-0.0266,0.0337,-0.0702,0.0315,0.0237,0.0303,-0.0444,-0.0074,-0.0303,-0.0106,-0.0333,0.0054,0.0844,-0.0176,0.0416,0.0513,0.0644,-0.0614,0.0215,-0.0046,-0.0203,0.0028,-0.0104,0.0327,0.0013,-0.0071,-0.0044,-0.0188,-0.2367,0.0516,-0.0093,0.0049,-0.0305,-0.014,-0.0515,-0.0508,0.0724,-0.0133,0.0082,0.0113,-0.0124,-0.0257,-0.0669,-0.0177,-0.0032,-0.0407,0.0002,-0.0243,-0.0357,0.0522,-0.0189,0.0268,0.0853,0.008,0.0419,0.0779,0.0118,0.0535,0.052,0.0087,0.0618,-0.1579,0.0381,0.0733,0.0337,-0.033,-0.0364,0.0004,0.0232,-0.0089,0.0518,0.042,0.0121,-0.0369,-0.0102,0.0013,-0.0329,-0.0269,0.0269,0.062,-0.0041,0.0081,0.0071,-0.0112,-0.0377,0.0006,-0.068,0.038,0.0017,-0.0556,-0.0195,-0.0067,0.0617,-0.0826,-0.0387,0.0157,0.0468,-0.0812,0.2176,-0.0365,0.0359,0.0643,-0.0013,0.0496,0.0007,-0.0971,-0.01,0.0342,-0.0003,-0.0085,-0.0023,-0.0211,-0.0374,0.0426,0.0044,0.0437,0.0224,0.0091,-0.0244,0.009,-0.0056,0.0004,-0.025,0.0178,-0.0241,0.031,0.1202,0.0299,0.0528,0.057,-0.0065,-0.0563,-0.0549,-0.0004,0.0523,-0.0002,0.0245,0.0373,-0.0019,-0.0256,-0.0497,-0.0032,-0.0185,-0.0424,0.1338,-0.0683,0.0465,-0.023,0.0034,-0.0337,0.0161,-0.0485,-0.0108,0.0183,0.0405,0.0363,0.015,-0.0243,0.0047,0.0239,-0.0399,-0.0463,0.0984,0.0389,-0.1503,-0.0104,-0.0049,0.0025,-0.0057,0.0363,0.0234,0.0005,0.0394,0.0568,0.0412,-0.0229,-0.05,-0.0025,0.0327,0.021,-0.0581,-0.042,0.0105,0.0354,-0.0499,0.0142,-0.0061,0.0086,0.0453,-0.0374,0.0382,-0.0437,-0.0513,-0.0309,-0.0291,-0.0418,-0.0181,0.0405,-0.006,0.0251,0.0305,-0.0202,-0.0278,-0.0183,0.0158,-0.0277,-0.0273,0.0043,0.018,-0.0026,-0.0085,0.0513,-0.0426,-0.0334,0.0126,0.0229,0.0622,-0.0103,0.0511,0.0656,-0.0108,-0.0215,-0.2513,-0.0075,-0.0312,-0.0186,0.0232,-0.079,0.0517,0.0367,0.0623,0.068,0.0237,-0.048,-0.0228,0.0906,-0.0071,0.0834,0.0227,0.0034,-0.005,-0.0403,-0.0174,0.0375,0.0075,-0.0927,0.0892,-0.0236,0.2257,0.0532,0.0302,-0.0239,-0.0022,0.0341,-0.0295,-0.0944,0.0525,0.0226,0.0583,-0.021,-0.0289,-0.0129,-0.0229,-0.0055,0.0779,-0.1042,-0.0223,-0.0548,-0.0621,0.0218,-0.0434,0.0436,0.0584,-0.0303,0.0345,-0.0138,-0.0289,-0.0201,-0.0927,-0.0166,-0.0574,0.0045,-0.0128,-0.0593,0.0357,-0.0499,0.0648,0.017,-0.0462,-0.0509,0.0472,-0.0556,-0.0278,0.0484,0.0195,0.0119,0.0313,-0.0193,0.0028,-0.0347,-0.037,-0.0406,0.0486,-0.0017,0.0365,0.0155,0.0956,0.0036,0.0743,0.0127,0.0142,-0.0174,-0.0129,0.0103,-0.0794,-0.0476,-0.0011,0.018,-0.3068,0.0374,-0.025,0.0766,-0.0136,0.0406,0.0233,0.0465,-0.0161,-0.0416,0.0196,0.026,0.063,-0.0182,0.0034,0.0246,0.0211,-0.0592,-0.0062,-0.0591,0.0079,0.0328,0.1734,-0.0553,-0.0138,0.0059,-0.0101,-0.0202,0.0124,-0.0325,-0.0414,0.018,0.0628,-0.0556,0.0345,0.1007,-0.0477,0.0176,0.0358,0.0019,-0.0266,-0.028,-0.0441,0.0186,0.0906,0.0296,0.0006,-0.0003,0.0358,0.0247,-0.0294,-0.0034,-0.0231,0.0238,0.0648,-0.0003,-0.0691,-0.0483,-0.0413,-0.0684,0.0292,-0.0074,-0.0597,-0.0076,-0.0443]}
{"key":"[Deep Transformer Networks for Time Series Classification: The NPP Safety Case] A challenging part of dynamic probabilistic risk assessment for nuclear power plants is the need for large amounts of temporal simulations given various initiating events and branching conditions from which representative feature extraction becomes complicated for subsequent applications. Artificial Intelligence techniques have been shown to be powerful tools in time-dependent sequential data processing to automatically extract and yield complex features from large data. An advanced temporal neural network referred to as the Transformer is used within a supervised learning fashion to model the time-dependent NPP simulation data and to infer whether a given sequence of events leads to core damage or not. The training and testing datasets for the Transformer are obtained by running 10,000 RELAP5-3D NPP blackout simulations with the list of variables obtained from the RAVEN software. Each simulation is classified as \"OK\" or \"CORE DAMAGE\" based on the consequence. The results show that the Transformer can learn the characteristics of the sequential data and yield promising performance with approximately 99% classification accuracy on the testing dataset.","layer":3,"vector":[-0.0661,0.021,0.0211,-0.0452,0.0456,0.0473,0.0204,0.0126,0.0227,-0.0161,-0.0149,-0.0616,0.0357,0.0354,-0.0384,0.0376,-0.0331,0.0284,0.0016,-0.0028,0.0535,-0.0253,-0.0119,-0.0119,0.0182,0.0157,0.0266,0.0223,-0.0607,-0.2281,-0.0069,-0.0453,0.0077,-0.0601,-0.0162,-0.008,-0.0481,0.0441,-0.0031,0.0388,0.0189,-0.0097,-0.0228,-0.0728,-0.007,-0.0604,0.0311,-0.0917,0.0061,-0.0497,0.0757,-0.0359,0.046,0.0556,0.0228,-0.0112,0.1235,0.0356,0.0809,0.045,0.0245,0.0331,-0.2169,0.0392,0.0578,0.0208,-0.0529,0.01,0.0413,0.011,-0.0201,0.0405,0.0163,0.011,-0.0075,0.0366,0.0114,-0.0512,-0.0264,0.0135,0.0342,-0.0534,-0.0665,-0.034,-0.0083,-0.0484,0.0098,-0.0465,0.0126,0.0184,-0.0396,0.0139,0.0015,0.0464,-0.0699,-0.0321,0.0767,0.0143,-0.0459,0.1766,-0.0726,0.0362,-0.0038,-0.0134,0.0342,-0.0328,-0.0261,-0.0723,-0.0388,-0.0011,0.004,-0.0186,0.0008,-0.0653,0.0282,-0.0005,0.0501,0.0077,-0.0429,-0.0098,-0.013,0.0188,0.0751,-0.0012,0.0342,-0.0465,0.0659,0.1316,0.0054,0.0469,0.0494,0.0033,-0.0662,0.0265,0.0287,0.0552,0.0041,-0.0175,0.0078,-0.0149,-0.0295,-0.0089,-0.0293,-0.0642,-0.0249,0.0823,-0.0664,0.0351,-0.029,-0.0046,-0.0643,0.0095,-0.0406,-0.0586,0.0514,0.0374,0.0118,0.014,-0.0276,-0.0157,-0.0224,-0.0762,0.0049,0.1239,0.0012,-0.0828,-0.0363,-0.0024,0.0136,-0.0327,0.0173,0.0518,-0.0005,-0.0291,0.0724,0.0719,0.0047,-0.0223,0.0117,0.0266,0.0171,-0.0213,0.0202,0.0319,0.0318,-0.0151,0.0072,-0.0481,0.0041,0.0362,-0.0629,-0.0043,-0.0478,0.0273,-0.032,-0.0045,-0.0158,-0.0028,0.0068,-0.0406,-0.0092,-0.0444,0.0077,0.0197,-0.0342,0.0554,-0.0264,0.0326,0.0119,0.0138,-0.0011,0.0009,0.0851,-0.014,-0.0201,0.022,0.0132,0.0322,-0.0115,0.0845,0.0513,-0.0199,-0.0348,-0.2257,-0.0009,0.0083,-0.0011,0.0836,-0.0544,0.0007,-0.0674,0.0139,0.0209,0.0849,0.0063,-0.0252,-0.0352,-0.0095,0.0379,0.0004,0.0088,-0.0695,0.0249,-0.0316,-0.0015,0.0178,-0.1053,0.0212,-0.0078,0.1994,-0.0206,0.0704,-0.0356,0.0423,0.0115,-0.0252,-0.0589,0.0835,0.0076,0.0477,0.0275,-0.0441,-0.0444,-0.0588,-0.0066,0.0107,-0.0749,-0.0095,-0.0556,-0.0066,0.079,-0.0778,0.0118,0.0372,-0.0436,0.0237,0.004,0.0378,-0.0598,-0.1275,0.0493,-0.0185,-0.0094,0.0055,-0.0261,-0.0115,-0.057,0.0335,0.0069,-0.0106,0.0057,-0.0028,-0.0313,-0.0283,0.1354,-0.0043,-0.0103,0.0483,0.0203,0.0033,-0.0377,-0.0106,-0.0115,0.0393,-0.0595,0.0496,0.0323,0.0303,-0.0114,0.0535,0.0384,0.0475,0.0501,-0.0141,0.0002,-0.0341,-0.0071,0.0208,-0.0024,-0.2951,0.0629,0.002,0.0143,-0.0108,-0.0617,-0.0001,0.0314,-0.002,-0.037,-0.0171,0.0437,0.0794,-0.0172,-0.029,0.0295,0.0631,-0.0374,0.0372,-0.0213,0.0414,0.055,0.2282,-0.0576,0.0221,-0.0028,-0.0197,-0.0093,0.037,-0.0169,0.0471,0.0006,0.074,-0.0737,0.0149,0.1027,0.0111,0.0347,0.026,-0.0155,0.0134,0.0199,-0.0207,-0.052,0.102,-0.033,-0.0316,-0.0584,-0.0094,0.0447,-0.0166,0.0213,-0.0064,-0.0118,0.0739,0.0596,-0.0064,-0.0519,-0.0428,-0.0525,0.0022,-0.0582,-0.0046,-0.0286,0.0036]}
{"key":"[Complex-Valued Kernel Methods for Regression] Usually, complex-valued RKHS are presented as an straightforward application of the real-valued case. In this paper we prove that this procedure yields a limited solution for regression. We show that another kernel, here denoted as pseudo kernel, is needed to learn any function in complex-valued fields. Accordingly, we derive a novel RKHS to include it, the widely RKHS (WRKHS). When the pseudo-kernel cancels, WRKHS reduces to complex-valued RKHS of previous approaches. We address the kernel and pseudo-kernel design, paying attention to the kernel and the pseudo-kernel being complex-valued. In the experiments included we report remarkable improvements in simple scenarios where real a imaginary parts have different similitude relations for given inputs or cases where real and imaginary parts are correlated. In the context of these novel results we revisit the problem of non-linear channel equalization, to show that the WRKHS helps to design more efficient solutions.","layer":12,"vector":[-0.0402,-0.0145,0.0256,-0.0285,0.0352,-0.0106,0.0193,0.071,0.0056,-0.0365,0.0204,-0.0759,0.0123,0.0309,0.0521,0.0005,0.0403,0.052,-0.0756,0.0528,0.0237,-0.0337,0.0104,-0.0588,0.0143,-0.0075,-0.0131,-0.0616,-0.0126,-0.2606,0.0126,-0.0469,0.0566,-0.0072,0.0143,-0.0506,-0.026,0.0236,-0.0332,0.0981,0.0211,-0.018,0.0021,-0.0206,-0.0314,-0.0861,-0.0264,-0.0174,-0.0292,-0.0183,0.0334,0.0136,0.0161,0.0384,0.0099,0.0043,0.0709,0.0502,0.0993,0.0371,0.0222,0.0489,-0.1707,0.0522,0.0516,0.0183,0.0112,-0.0143,0.0395,0.0548,0.0117,0.0528,0.0309,0.0064,0.0214,-0.0015,0.0349,-0.0192,-0.0577,0.0466,0.0412,-0.0422,-0.0279,-0.0017,-0.0196,-0.0382,-0.0105,-0.051,0.024,0.0363,-0.0633,-0.0325,-0.0175,0.0142,-0.0498,0.02,-0.0036,0.0187,-0.0183,0.1911,-0.0097,0.057,0.0572,-0.0573,0.0362,-0.0538,-0.0535,-0.0486,-0.0037,0.0039,-0.0368,-0.059,0.0041,-0.0286,0.0158,-0.0372,0.0757,0.0587,0.0316,-0.0307,0.0027,0.0112,0.0559,0.0296,0.0563,-0.0799,0.0116,0.1584,0.0101,0.0151,0.0013,-0.0274,-0.0721,-0.0279,-0.0076,0.0497,0.0214,0.0538,-0.0074,0.0466,-0.0313,-0.0837,0.0352,-0.09,-0.0592,0.1191,-0.0728,0.002,-0.0397,-0.0175,0.0175,0.0237,-0.0084,-0.038,0.022,0.017,0.0315,0.0121,-0.0277,0.0191,-0.0416,-0.0494,-0.0457,0.1049,-0.0268,-0.0709,-0.0057,-0.0149,0.0423,-0.0039,0.0597,-0.0189,-0.0242,0.0031,0.0842,0.0203,-0.0661,0.0279,0.0005,-0.0155,0.0259,-0.0237,-0.0461,0.0704,0.0461,-0.0218,-0.0037,-0.0642,0.0171,0.0487,-0.0186,0.0014,-0.0622,-0.0134,-0.0286,-0.0255,0.0026,-0.0289,0.0341,-0.0006,-0.0021,-0.0078,-0.0534,-0.0033,-0.0376,0.0045,0.0119,-0.0282,0.0447,0.0294,-0.0153,-0.0156,0.0719,-0.0529,-0.0362,-0.0139,0.032,0.0646,-0.0058,0.0683,0.0288,-0.0341,-0.0706,-0.2374,-0.0282,0.0126,-0.0275,0.0615,-0.0611,0.0585,-0.0279,0.0451,0.1008,0.041,-0.0041,-0.045,0.0184,0.0042,0.0451,0.0557,-0.0032,-0.0178,-0.0295,-0.0271,-0.0167,0.0189,-0.0502,0.0817,-0.0426,0.1752,0.0012,0.0316,-0.0073,0.0385,0.014,-0.021,-0.0489,0.0292,0.0612,0.0921,0.0045,-0.0336,-0.0147,-0.0151,0.0084,0.0304,-0.0726,-0.0615,0.0007,-0.0455,0.0063,-0.0572,0.0322,0.03,-0.0059,0.0507,0.0087,0.0413,-0.0079,-0.1172,0.033,-0.0198,0.0481,0.0409,-0.088,-0.0058,-0.0397,0.0614,0.0013,-0.0106,-0.0242,0.002,0.0055,-0.0352,0.0944,-0.0204,0.0142,0.064,0.0127,-0.0174,-0.0249,-0.0216,-0.0266,0.0828,-0.0257,0.0477,0.0062,0.0114,0.0086,0.0837,-0.0247,0.032,-0.0672,-0.0424,-0.0047,-0.0583,0.0009,0.0303,-0.051,-0.261,0.0209,-0.0409,-0.0168,-0.0277,0.0269,0.034,-0.0175,-0.0926,0.0267,-0.021,0.0655,0.0399,-0.0363,0.0509,0.0358,0.058,-0.0632,0.0263,-0.0492,0.0076,0.0377,0.2109,-0.0398,0.0372,-0.0338,-0.0267,-0.0221,0.0304,-0.0038,0.0136,0.0183,0.0785,-0.026,0.0339,0.0756,-0.0257,0.0528,-0.0146,-0.0096,0.0116,-0.0078,-0.0572,-0.0389,0.095,0.0407,-0.0534,-0.0499,0.0146,0.0182,-0.0481,0.0349,0.0348,0.0183,0.0222,0.0266,-0.0693,-0.052,-0.0515,-0.0504,0.0353,-0.0579,-0.0254,-0.016,-0.0236]}
{"key":"[Sketching for Kronecker Product Regression and P-splines] TensorSketch is an oblivious linear sketch introduced in Pagh'13 and later used in Pham, Pagh'13 in the context of SVMs for polynomial kernels. It was shown in Avron, Nguyen, Woodruff'14 that TensorSketch provides a subspace embedding, and therefore can be used for canonical correlation analysis, low rank approximation, and principal component regression for the polynomial kernel. We take TensorSketch outside of the context of polynomials kernels, and show its utility in applications in which the underlying design matrix is a Kronecker product of smaller matrices. This allows us to solve Kronecker product regression and non-negative Kronecker product regression, as well as regularized spline regression. Our main technical result is then in extending TensorSketch to other norms. That is, TensorSketch only provides input sparsity time for Kronecker product regression with respect to the $2$-norm. We show how to solve Kronecker product regression with respect to the $1$-norm in time sublinear in the time required for computing the Kronecker product, as well as for more general $p$-norms.","layer":1,"vector":[-0.0329,-0.0035,0.0292,0.0273,0.0059,0.0226,0.0304,0.0583,0.0035,-0.0026,0.0476,-0.0621,0.0041,0.0356,0.0216,-0.0209,0.0151,0.0931,-0.04,0.0223,0.0259,-0.0444,-0.0179,-0.0576,0.0302,-0.009,0.0051,-0.0443,-0.0419,-0.2494,0.0184,-0.02,0.0827,-0.0062,0.0055,-0.0018,-0.0028,0.0867,-0.0427,0.0233,-0.0131,0.0116,-0.0185,0.0001,-0.0311,-0.0478,-0.0251,-0.0141,-0.0473,-0.0036,-0.0142,-0.0212,0.0219,0.0201,0.0713,-0.0054,0.0,0.0277,0.0363,0.0592,0.0052,-0.0121,-0.1907,0.0301,0.049,0.0129,-0.0406,-0.0766,0.0271,0.0914,-0.0218,0.0353,-0.0032,0.0175,0.0143,-0.0089,0.0309,-0.0175,-0.0449,0.0422,0.0163,-0.0162,-0.0348,-0.005,-0.0491,-0.0068,0.0111,-0.044,0.0498,0.0146,-0.0418,-0.0457,-0.0283,-0.0046,-0.0784,-0.0431,0.0619,0.0404,-0.0367,0.2212,-0.0592,0.0527,0.0196,0.0003,0.0023,-0.0436,-0.0053,0.0022,-0.0073,0.0076,0.0076,-0.0023,0.0046,-0.0819,0.0519,-0.0471,0.0446,0.065,-0.0004,0.0137,-0.0079,0.0365,0.0494,-0.0166,0.0146,-0.0877,0.0123,0.1244,0.0467,0.0805,0.0366,-0.0115,-0.0212,-0.0155,-0.0161,-0.0053,0.0037,0.0717,0.0072,-0.0018,-0.0583,-0.022,0.0674,-0.0506,-0.021,0.137,-0.0499,-0.0003,-0.0057,0.0066,-0.0108,0.0396,-0.0202,0.0171,0.0255,0.0324,0.0027,0.0235,-0.0698,0.0658,-0.0579,-0.0755,-0.0161,0.0618,0.0017,-0.0966,0.0084,0.0284,0.0449,-0.0095,0.0518,0.0068,-0.0238,0.0228,0.0621,-0.0089,-0.0777,0.0251,0.012,-0.0103,0.0424,-0.0129,-0.0487,0.0556,0.0147,-0.0382,0.0114,-0.0109,0.0115,0.0006,-0.0384,-0.0056,-0.0716,-0.0191,-0.0763,-0.0281,0.0162,-0.002,0.0416,-0.0611,0.0284,0.017,-0.0318,0.014,-0.013,0.0555,0.0206,-0.0024,0.0133,0.0461,-0.0431,-0.0459,0.0369,-0.0554,-0.022,0.0258,0.0242,0.0965,-0.0192,0.0542,0.0583,-0.0565,-0.0825,-0.2337,-0.0402,-0.0025,-0.0218,0.0522,-0.0643,0.0577,-0.0314,0.0313,0.0903,0.0456,0.0317,-0.0361,0.006,-0.0379,0.0546,0.0481,0.0036,-0.0003,-0.0124,-0.0576,0.0128,0.001,-0.0346,0.0433,-0.0095,0.2024,0.0639,0.0187,-0.0242,-0.0146,0.0468,-0.0476,-0.0955,0.0751,0.04,0.0791,0.0182,-0.0236,-0.0284,-0.0164,-0.001,0.0133,-0.0537,-0.0529,-0.0211,-0.0228,-0.0136,-0.0408,0.0326,0.0363,-0.0307,0.075,-0.0297,0.0095,-0.0483,-0.0837,0.0045,-0.0238,0.0653,0.0143,-0.0645,0.0308,-0.0687,0.0688,0.0191,-0.0066,-0.0529,-0.0067,0.0216,-0.0226,0.0564,-0.0126,0.0012,0.0498,0.0186,0.0585,0.028,-0.0441,0.0096,0.0809,-0.0314,0.0305,0.0074,0.0213,-0.0027,0.0542,-0.005,0.0211,-0.0429,-0.039,-0.0082,-0.083,-0.0055,0.0535,-0.0091,-0.2967,0.0068,0.0481,-0.0075,-0.0748,0.008,0.0163,-0.0123,-0.0697,-0.0324,0.0231,0.0573,0.057,-0.0161,0.007,0.0378,0.0888,-0.0665,0.0781,-0.0703,0.0078,0.0087,0.1825,-0.0535,0.0255,-0.0509,-0.0123,-0.0083,0.0744,0.0125,0.032,0.0125,0.0967,-0.0323,0.0463,0.0493,-0.0492,-0.001,-0.0101,-0.0257,-0.0097,-0.0075,-0.0664,-0.0403,0.0672,-0.0442,-0.0357,-0.0643,0.0118,0.0093,0.0187,0.0208,-0.0049,0.0316,0.0164,0.0089,-0.0257,-0.0484,-0.0525,-0.0052,-0.0056,-0.0504,-0.0409,-0.0327,0.0004]}
{"key":"[Causal Distillation for Language Models] Distillation efforts have led to language models that are more compact and efficient without serious drops in performance. The standard approach to distillation trains a student model against two objectives: a task-specific objective (e.g., language modeling) and an imitation objective that encourages the hidden states of the student model to be similar to those of the larger teacher model. In this paper, we show that it is beneficial to augment distillation with a third objective that encourages the student to imitate the causal computation process of the teacher through interchange intervention training(IIT). IIT pushes the student model to become a causal abstraction of the teacher model - a simpler model with the same causal structure. IIT is fully differentiable, easily implemented, and combines flexibly with other objectives. Compared with standard distillation of BERT, distillation via IIT results in lower perplexity on Wikipedia (masked language modeling) and marked improvements on the GLUE benchmark (natural language understanding), SQuAD (question answering), and CoNLL-2003 (named entity recognition).","layer":0,"vector":[-0.0556,-0.021,-0.0006,-0.0238,-0.0042,-0.0396,0.0539,0.0221,-0.0,-0.0233,0.0705,-0.1058,0.0554,0.059,0.0501,0.0165,-0.0034,0.0258,-0.0359,0.0175,0.0185,-0.0151,-0.0045,-0.0171,-0.0042,0.0444,-0.039,-0.0136,0.0059,-0.234,0.025,-0.0608,-0.0321,0.0304,-0.0123,-0.0017,-0.0672,0.0438,-0.0174,0.021,0.0255,0.0138,-0.0438,-0.078,-0.0059,-0.0655,-0.0588,-0.0011,-0.0541,-0.0473,0.0074,-0.0793,0.0251,0.0079,0.0212,0.0658,0.0628,0.0659,0.0557,0.042,0.0094,0.0317,-0.1885,0.1142,0.0181,0.0347,-0.06,-0.0143,0.0397,0.0715,-0.0226,0.0524,0.0302,0.0674,0.0119,0.0024,0.0169,-0.0085,0.0079,-0.0189,0.0235,-0.0122,0.0013,-0.0426,-0.017,-0.0559,0.039,-0.0316,0.0215,-0.0017,-0.0739,-0.0321,-0.0061,0.0307,-0.0082,-0.04,0.0345,-0.0095,-0.0228,0.2191,-0.0327,-0.0205,0.0023,-0.0836,0.0482,-0.0006,-0.0054,-0.0292,-0.0332,-0.0025,-0.0465,-0.0236,0.0293,-0.0489,0.0391,0.0064,0.1025,-0.0122,-0.0177,0.0099,0.0188,0.0022,0.0163,-0.0393,-0.0001,-0.0581,0.0197,0.1231,0.0327,0.0356,0.0525,-0.0149,-0.0376,-0.0082,-0.0096,-0.0076,0.0319,-0.0197,0.0097,-0.0338,-0.0489,-0.0257,0.0221,-0.0978,-0.0688,0.1469,-0.0243,0.0145,-0.052,-0.0036,0.0039,0.0495,-0.0285,-0.0326,0.0587,0.0247,0.0607,0.0395,-0.019,-0.0051,0.0095,-0.0426,-0.0651,0.0466,0.0334,-0.0707,-0.0383,0.0168,0.0024,-0.037,0.1031,0.0512,-0.0297,0.0452,0.052,0.0195,-0.0488,-0.0187,0.0203,0.0008,0.0335,-0.0701,-0.0322,0.0794,-0.0014,-0.0264,0.0082,-0.0492,0.0357,0.0227,-0.0408,0.0499,-0.0469,0.0245,-0.0328,-0.0061,0.0219,-0.0356,0.0182,-0.0468,-0.0076,0.0275,-0.0565,-0.0377,-0.0213,0.0376,0.0011,0.0049,0.0792,0.0278,-0.0212,0.0312,0.0118,-0.0264,-0.0182,0.0167,0.0314,0.0291,0.0112,0.0262,0.0377,-0.0159,0.0145,-0.2333,-0.0291,0.0209,0.0221,0.0337,-0.0582,0.0379,0.0006,0.0233,0.0799,0.0093,-0.0457,-0.0332,0.0065,0.0005,0.0238,0.0222,0.0292,0.0071,-0.0131,0.0197,-0.0336,-0.0108,-0.089,0.0268,0.0014,0.2123,0.08,0.0756,-0.012,0.0309,0.0313,-0.0125,-0.1221,0.05,-0.0105,0.0582,-0.0111,-0.0041,-0.0599,-0.0151,0.0193,-0.0234,-0.1071,-0.05,-0.024,-0.0297,-0.0507,-0.0593,0.0696,0.0135,-0.0303,0.0266,0.0238,-0.0418,-0.0401,-0.0906,0.0143,-0.0439,0.0043,-0.0118,0.0184,-0.0046,-0.0455,0.0133,0.0161,-0.0252,-0.0526,0.024,0.0394,-0.0279,0.0703,0.0103,0.0075,0.0328,0.0754,-0.001,-0.0432,-0.0669,-0.0314,0.0741,-0.0243,0.0262,-0.0106,0.049,0.0218,0.1023,0.0211,0.0604,0.0149,0.0058,0.0083,-0.0284,0.0023,0.0248,-0.0422,-0.2841,0.052,0.0208,0.0295,-0.023,0.0194,0.0098,0.0177,-0.0278,-0.027,-0.0137,0.018,0.0535,0.0073,-0.0177,0.0853,0.0684,-0.0556,0.0376,-0.0525,-0.0043,0.0844,0.2012,0.0115,0.0262,0.0171,-0.0056,-0.019,0.0618,-0.0266,0.009,0.0104,0.1082,-0.0149,0.044,0.012,-0.058,0.0357,0.0429,-0.0554,-0.0262,0.0009,-0.0456,-0.0612,0.0716,0.0338,-0.0158,-0.0743,-0.0252,-0.0253,0.0175,0.0046,-0.0333,0.0038,0.0187,0.0492,-0.0605,-0.0427,-0.03,-0.0507,0.0056,-0.0339,0.0048,0.0279,-0.025]}
{"key":"[Non-linear aggregation of filters to improve image denoising] We introduce a novel aggregation method to efficiently perform image denoising. Preliminary filters are aggregated in a non-linear fashion, using a new metric of pixel proximity based on how the pool of filters reaches a consensus. We provide a theoretical bound to support our aggregation scheme, its numerical performance is illustrated and we show that the aggregate significantly outperforms each of the preliminary filters.","layer":5,"vector":[-0.0051,-0.0493,0.0075,0.0111,0.0594,0.022,0.017,0.0081,0.0368,-0.0175,0.0868,-0.1088,0.0841,0.0257,-0.0182,-0.02,0.0459,0.0338,-0.0366,0.0032,0.023,-0.0234,0.0019,-0.0257,-0.0006,0.0146,-0.0175,-0.0456,-0.0594,-0.2429,-0.005,-0.0295,0.0771,0.0105,-0.0001,-0.0055,-0.0217,0.0658,-0.0624,0.0466,0.0094,0.025,-0.0218,-0.0975,-0.0616,-0.0714,0.0105,-0.0814,-0.023,-0.002,0.0615,-0.0401,0.0205,0.0064,0.016,0.0345,0.017,0.0283,0.068,0.0314,0.0076,0.0349,-0.1599,0.0871,0.0571,0.0015,0.0143,-0.034,0.0195,0.0622,0.0176,0.0562,0.0227,0.02,0.0306,-0.037,0.0019,-0.057,-0.0517,0.0057,-0.0001,-0.0182,0.0001,0.0006,0.0,-0.0052,0.0308,-0.0881,0.0192,0.0114,-0.0228,-0.019,-0.0103,0.0276,-0.0546,-0.0171,0.0072,0.051,0.0142,0.2264,-0.0516,0.0317,0.0807,-0.0855,0.0405,-0.05,-0.0338,-0.0333,-0.0086,0.0041,-0.0195,-0.0372,0.0521,-0.0483,0.0238,-0.0313,0.0357,0.003,-0.0154,-0.0296,-0.0183,0.024,0.0451,0.0054,0.0695,0.0036,0.0168,0.1504,0.0468,0.0412,0.0484,0.0221,-0.0124,-0.0168,0.0529,0.0192,-0.0218,0.0022,0.008,0.035,-0.0691,-0.1043,-0.0063,-0.0712,-0.053,0.1203,-0.0205,0.0296,-0.0722,-0.0741,-0.0226,0.0266,-0.0131,0.0177,0.0139,0.0445,0.0344,0.0596,-0.0541,0.0365,-0.0287,-0.084,0.0268,0.106,0.0192,-0.0634,-0.0335,-0.0547,0.002,0.0112,0.0506,-0.0007,-0.0141,0.0302,0.0579,0.0053,-0.0177,0.0349,-0.0157,-0.0237,0.0076,-0.0433,-0.0534,0.0177,0.0468,-0.0491,-0.0194,-0.0244,0.0565,0.0523,-0.0745,0.0198,-0.0154,0.0171,-0.0057,-0.0403,-0.0175,-0.0564,-0.0409,-0.0457,0.0209,-0.0403,-0.0192,0.0426,0.0153,0.0342,-0.0417,-0.0262,0.0084,0.0344,-0.024,-0.0508,0.0912,-0.0157,-0.0193,0.0143,0.0468,0.0389,0.0158,0.0426,0.0754,-0.0251,-0.083,-0.2113,-0.0037,0.0126,0.0204,0.0504,-0.054,0.0191,-0.0162,0.0442,0.0676,0.0274,-0.0101,0.0172,0.0306,-0.0123,0.107,0.0043,0.0327,-0.0174,-0.0325,-0.0135,0.0795,-0.007,-0.0618,0.0722,0.0068,0.1894,-0.0124,0.0154,-0.003,0.0112,0.0416,-0.0347,-0.0631,0.0262,0.0603,0.0489,-0.0186,-0.0182,-0.0492,0.0039,-0.008,-0.0104,-0.0817,-0.0212,-0.0468,-0.0493,0.0417,-0.0592,-0.0352,0.0418,-0.0743,0.0444,-0.0281,0.0327,-0.0381,-0.076,0.0247,-0.0082,0.0691,0.0049,-0.0577,0.009,-0.0694,0.0974,-0.0123,0.0027,-0.0395,0.0185,-0.0099,-0.0157,0.0908,-0.0118,-0.0023,0.0089,0.041,0.0263,-0.0283,-0.0448,-0.0783,0.059,-0.0236,0.0693,0.0636,0.0517,-0.0145,0.0741,-0.0168,-0.0122,-0.0492,0.0138,0.005,-0.0713,-0.0229,-0.0022,-0.007,-0.2735,0.0017,-0.0,0.0354,0.0065,0.013,0.0423,0.043,-0.0598,-0.0002,-0.0648,0.0596,0.0443,-0.0323,0.0135,0.0422,0.0064,-0.0831,0.0752,-0.0591,0.0081,-0.0119,0.2006,-0.031,-0.0235,0.008,-0.0012,-0.0109,-0.034,-0.002,0.0007,0.0105,0.089,-0.0705,0.0261,0.0659,-0.0531,0.0382,-0.0216,-0.0469,0.0096,0.0066,0.0268,0.0151,0.0655,0.0083,-0.0371,-0.0051,0.0182,0.0149,-0.0366,0.0326,0.0172,0.0095,0.0184,0.0121,-0.0647,-0.0351,-0.0308,-0.0159,-0.0248,-0.0506,-0.0323,0.0233,-0.0159]}
{"key":"[Uniform-in-Time Weak Error Analysis for Stochastic Gradient Descent Algorithms via Diffusion Approximation] Diffusion approximation provides weak approximation for stochastic gradient descent algorithms in a finite time horizon. In this paper, we introduce new tools motivated by the backward error analysis of numerical stochastic differential equations into the theoretical framework of diffusion approximation, extending the validity of the weak approximation from finite to infinite time horizon. The new techniques developed in this paper enable us to characterize the asymptotic behavior of constant-step-size SGD algorithms for strongly convex objective functions, a goal previously unreachable within the diffusion approximation framework. Our analysis builds upon a truncated formal power expansion of the solution of a stochastic modified equation arising from diffusion approximation, where the main technical ingredient is a uniform-in-time weak error bound controlling the long-term behavior of the expansion coefficient functions near the global minimum. We expect these new techniques to greatly expand the range of applicability of diffusion approximation to cover wider and deeper aspects of stochastic optimization algorithms in data science.","layer":1,"vector":[-0.0487,-0.0053,0.0197,-0.0355,0.0096,0.0358,-0.0138,0.0166,0.0812,0.0051,0.0491,-0.0236,0.0821,0.0758,0.0223,0.0714,-0.036,-0.011,-0.0697,0.0055,0.0216,-0.0009,0.0189,-0.0648,0.0441,-0.0153,-0.0432,-0.0219,-0.0181,-0.2664,0.003,-0.0759,0.0101,-0.0625,0.0138,-0.0093,-0.0032,0.0656,-0.0508,0.0623,-0.0146,0.0367,-0.0528,-0.0299,-0.0047,-0.0692,-0.0395,-0.0242,-0.0347,-0.0212,0.0109,-0.0026,0.0444,0.0069,-0.0021,0.038,0.0493,0.0119,0.0479,0.0469,0.0262,0.0448,-0.175,0.0296,0.0749,0.0003,-0.0222,-0.0487,-0.0128,0.06,-0.0162,0.0365,0.0048,0.0764,0.0287,0.0087,0.0293,-0.0391,-0.0063,0.0538,0.0521,-0.0403,-0.0221,0.0101,-0.0316,-0.0558,0.0267,-0.0429,0.0656,0.0132,0.0143,-0.0571,-0.0175,0.04,-0.0833,-0.0052,0.0366,0.0382,-0.0328,0.1878,-0.0463,0.0569,0.0362,0.0285,0.0127,-0.0306,-0.0173,-0.0386,-0.0142,0.0212,-0.0409,-0.0329,0.0536,-0.0104,-0.0129,-0.0055,0.0204,0.0022,-0.0313,-0.0146,-0.029,0.0027,0.0575,0.0057,-0.0006,-0.0604,-0.0018,0.1304,0.0122,0.0369,0.0712,-0.0202,-0.0684,-0.0395,0.0192,0.0232,-0.0447,0.0081,0.0277,-0.023,-0.0533,-0.0569,0.0091,-0.0714,-0.0395,0.1615,-0.0268,0.0403,-0.0901,-0.0807,0.0188,-0.0026,0.0025,-0.0451,0.0277,0.0104,0.0264,0.0479,-0.0563,0.0285,-0.0275,-0.0626,-0.0203,0.1563,-0.018,-0.0274,-0.0218,0.0066,0.0106,-0.007,0.0246,0.0468,-0.0403,0.0125,0.0699,0.0352,-0.071,-0.0177,0.0064,-0.0314,0.0053,-0.0131,-0.054,0.0334,0.0243,-0.0401,0.0043,-0.026,0.0053,0.0386,-0.0559,-0.0251,-0.032,-0.0445,-0.0259,-0.0497,-0.0046,-0.0308,0.0336,-0.0188,-0.0029,-0.0005,-0.0613,0.0623,-0.001,-0.0069,-0.0184,-0.019,0.0021,0.0621,-0.0211,0.0081,0.0775,-0.007,0.0049,0.033,0.0321,0.02,0.0003,0.0125,0.0498,0.0106,-0.0464,-0.1946,-0.048,0.0122,0.0298,0.0153,-0.0696,-0.0004,-0.046,0.0629,0.0798,0.0441,-0.0202,0.0159,-0.0019,0.0157,0.02,0.0576,-0.0143,-0.0051,-0.0066,0.0055,0.0146,-0.0171,-0.0933,0.0772,0.0017,0.1668,-0.0178,0.0815,-0.0344,0.0139,0.0333,0.0206,-0.0377,0.0347,0.0213,0.1086,-0.0561,-0.0165,-0.0542,-0.0239,0.0568,0.0285,-0.0905,-0.0333,0.0153,0.0098,-0.0121,-0.0507,0.0122,0.0571,-0.0308,0.0743,-0.0336,0.0253,-0.0317,-0.0722,0.0401,0.0111,0.0261,-0.0552,-0.0759,0.0018,-0.028,0.0335,-0.0005,-0.0044,-0.0404,-0.0246,-0.0143,0.0246,0.0642,-0.0523,0.0003,0.0502,0.0092,0.0241,0.0085,-0.0355,-0.0166,0.0697,-0.0524,0.0739,0.0552,0.0363,0.0244,0.0686,-0.0072,-0.0121,-0.018,-0.0238,0.0066,-0.0775,0.0431,0.0436,-0.0174,-0.2873,0.0494,0.016,-0.011,-0.0377,0.0033,0.0456,-0.011,-0.0708,-0.0018,-0.0178,0.0812,0.0289,0.0342,0.0354,0.0201,0.0221,-0.0228,0.0582,-0.0713,0.0128,0.049,0.2298,-0.0598,-0.0122,0.0254,-0.0113,0.0036,0.0155,-0.0407,0.0021,0.0262,0.0735,-0.0521,0.0798,0.0803,-0.042,0.0485,0.0425,-0.0554,0.0137,-0.009,-0.0133,0.017,0.0786,-0.0302,-0.0283,-0.0507,-0.0044,0.0428,-0.0649,0.0467,-0.0024,-0.0521,0.0068,0.0177,-0.0641,-0.0435,-0.0282,-0.0579,0.0087,-0.0722,-0.0538,0.0089,0.0039]}
{"key":"[Input-Cell Attention Reduces Vanishing Saliency of Recurrent Neural Networks] Recent efforts to improve the interpretability of deep neural networks use saliency to characterize the importance of input features to predictions made by models. Work on interpretability using saliency-based methods on Recurrent Neural Networks (RNNs) has mostly targeted language tasks, and their applicability to time series data is less understood. In this work we analyze saliency-based methods for RNNs, both classical and gated cell architectures. We show that RNN saliency vanishes over time, biasing detection of salient features only to later time steps and are, therefore, incapable of reliably detecting important features at arbitrary time intervals. To address this vanishing saliency problem, we propose a novel RNN cell structure (input-cell attention), which can extend any RNN cell architecture. At each time step, instead of only looking at the current input vector, input-cell attention uses a fixed-size matrix embedding, each row of the matrix attending to different inputs from current or previous time steps. Using synthetic data, we show that the saliency map produced by the input-cell attention RNN is able to faithfully detect important features regardless of their occurrence in time. We also apply the input-cell attention RNN on a neuroscience task analyzing functional Magnetic Resonance Imaging (fMRI) data for human subjects performing a variety of tasks. In this case, we use saliency to characterize brain regions (input features) for which activity is important to distinguish between tasks. We show that standard RNN architectures are only capable of detecting important brain regions in the last few time steps of the fMRI data, while the input-cell attention model is able to detect important brain region activity across time without latter time step biases.","layer":0,"vector":[-0.0524,0.0162,0.0169,-0.0568,0.0288,0.082,0.0648,0.0103,0.0671,-0.0485,0.0058,-0.0355,0.0366,0.0621,-0.0187,0.0012,-0.0105,0.0412,-0.0356,-0.0197,-0.0003,-0.0175,0.0189,-0.0613,-0.0171,-0.0293,-0.031,-0.0012,-0.0661,-0.2421,0.027,-0.0125,0.0168,-0.0274,-0.0142,-0.0237,-0.0545,0.0292,-0.0452,0.0167,0.0088,0.0158,-0.0176,-0.044,-0.0418,-0.0447,-0.0096,-0.0395,-0.0011,-0.0161,0.0146,-0.0205,0.025,0.0161,0.0556,0.0612,0.036,0.0519,0.0789,0.0181,0.0145,0.0587,-0.129,-0.0046,0.054,0.0163,-0.0377,-0.0567,-0.0318,0.0335,-0.0264,0.0123,-0.0366,0.0272,0.0368,-0.012,-0.0236,-0.0327,0.027,0.004,0.0604,-0.0063,-0.0454,-0.0287,0.0036,-0.0335,-0.0036,-0.0224,0.0301,-0.0301,-0.058,0.0161,-0.0392,0.0303,-0.0297,0.0074,0.0524,-0.0102,-0.0477,0.1891,-0.0757,0.0665,0.0507,-0.0287,0.0477,-0.0511,-0.0071,0.0089,-0.0389,0.0124,-0.0562,-0.0448,0.0188,-0.0368,0.0147,0.0187,0.0924,0.0141,0.008,0.0131,-0.0297,-0.0267,0.0335,-0.0486,0.0182,-0.0712,0.0412,0.1294,0.0547,0.0071,0.0558,-0.0047,-0.0388,-0.0259,0.018,0.0122,-0.0029,-0.0265,0.0074,-0.041,-0.0574,-0.0231,0.0329,-0.0796,-0.0457,0.1238,-0.0433,-0.0133,-0.0438,-0.01,-0.0578,0.0029,0.0058,-0.03,0.0374,0.0061,0.0149,0.0061,-0.0678,0.012,-0.0196,-0.0448,-0.0431,0.0918,0.0339,-0.0605,-0.0185,-0.0282,0.0208,-0.0308,0.0265,0.0016,-0.0088,0.0046,0.1016,0.0573,-0.0714,0.0449,0.0062,-0.0241,0.0221,-0.0938,0.0049,0.0472,0.035,-0.0327,0.0302,-0.0692,0.0216,0.0923,-0.0481,0.0085,-0.0105,0.0298,-0.0742,-0.0616,-0.0063,-0.0179,0.0105,-0.0513,-0.0142,-0.0146,-0.0231,0.0122,0.04,0.037,-0.0283,0.0391,0.0463,0.0197,-0.0448,0.0259,0.0512,-0.0357,-0.0417,-0.0111,0.0103,-0.0258,0.0096,0.0417,0.057,-0.0419,-0.0901,-0.2308,-0.0051,0.0141,-0.0672,0.0416,-0.1053,0.0296,-0.0259,0.1126,0.0718,0.0295,0.0058,-0.0132,0.018,0.0336,0.0728,0.0052,0.002,-0.0463,-0.0259,0.0244,0.0173,0.0096,-0.045,0.0405,-0.0082,0.2435,0.0129,0.0572,-0.0203,0.0361,0.0127,-0.0315,-0.1158,0.0307,0.0174,0.0716,0.0311,-0.0428,0.011,-0.0221,0.0083,0.0085,-0.0846,-0.0148,-0.0056,-0.028,0.0022,-0.0358,0.0046,0.0425,-0.0651,0.0309,0.0492,0.0042,-0.0132,-0.0623,-0.0086,-0.1186,0.0236,-0.0059,-0.0096,0.0275,-0.0605,0.0589,0.0326,-0.069,-0.0081,0.03,0.0214,-0.032,0.1008,0.0188,-0.0093,0.0652,-0.0141,0.0575,0.0127,-0.0707,0.0107,0.0499,-0.026,0.0335,-0.0019,0.0631,0.0106,0.0728,-0.0301,0.0103,-0.0071,-0.006,0.0268,-0.0309,-0.0674,0.0284,-0.0275,-0.2819,0.0558,-0.0089,0.0044,0.0296,0.0089,0.0359,0.033,-0.021,-0.0057,0.0005,0.0736,0.0488,-0.016,0.0068,0.0537,0.0684,-0.0308,0.0494,-0.0501,0.0395,0.0405,0.2148,-0.0265,0.0391,-0.0463,-0.0057,-0.0457,0.0505,-0.0113,0.0489,0.0281,0.0612,-0.0248,0.0103,0.0838,-0.0427,0.0516,0.0064,0.0144,0.014,0.0201,-0.0633,-0.0604,0.1105,-0.0117,-0.0346,-0.0351,0.0123,0.0463,-0.0382,-0.0041,-0.0032,0.0383,0.0391,0.0407,-0.0316,-0.0267,-0.0043,-0.0114,-0.0096,-0.0937,0.0196,0.0256,-0.0125]}
{"key":"[A Review of Relational Machine Learning for Knowledge Graphs] Relational machine learning studies methods for the statistical analysis of relational, or graph-structured, data. In this paper, we provide a review of how such statistical models can be \"trained\" on large knowledge graphs, and then used to predict new facts about the world (which is equivalent to predicting new edges in the graph). In particular, we discuss two fundamentally different kinds of statistical relational models, both of which can scale to massive datasets. The first is based on latent feature models such as tensor factorization and multiway neural networks. The second is based on mining observable patterns in the graph. We also show how to combine these latent and observable models to get improved modeling power at decreased computational cost. Finally, we discuss how such statistical models of graphs can be combined with text-based information extraction methods for automatically constructing knowledge graphs from the Web. To this end, we also discuss Google's Knowledge Vault project as an example of such combination.","layer":0,"vector":[-0.023,-0.0151,0.0163,0.0247,0.0618,-0.0034,0.0219,0.0507,0.0163,-0.028,0.0216,-0.038,0.0527,0.0446,0.0457,0.029,0.0098,0.0536,-0.0515,-0.019,0.0312,-0.0572,-0.0252,-0.0558,0.0325,0.0501,-0.038,-0.0619,-0.0167,-0.2033,-0.0219,-0.0586,0.0297,0.0039,0.0078,-0.0446,0.0146,0.0262,-0.0284,0.0251,0.0461,-0.0027,0.0255,-0.0083,0.0059,-0.0259,0.0082,0.0188,-0.0535,-0.0607,0.0032,-0.0687,0.016,0.0009,0.0649,0.0061,0.0406,0.0427,0.0104,0.1105,0.0581,0.0465,-0.1689,0.0536,0.0325,0.0345,-0.0421,-0.0192,0.0138,0.0777,0.0196,0.0398,0.0014,0.027,0.0209,0.0261,-0.001,-0.007,-0.0259,0.0246,-0.0049,-0.0201,-0.0522,-0.0158,-0.0092,-0.0445,-0.0053,-0.0395,0.0252,-0.002,-0.036,-0.0419,-0.009,0.0014,-0.0526,-0.0055,0.0522,0.0008,-0.0297,0.1862,-0.0532,0.0308,0.0469,-0.0095,0.0131,-0.038,-0.0152,-0.049,-0.026,0.0073,-0.0122,-0.0121,0.004,-0.0764,0.0795,0.0106,0.083,0.0478,-0.0332,-0.0173,-0.0362,0.0191,0.0279,-0.0178,0.0145,-0.0382,-0.0345,0.1072,0.0744,0.0498,0.0505,0.0052,-0.0683,-0.0173,0.0058,0.0042,0.0344,0.0178,0.0153,-0.0094,-0.0152,-0.0379,0.0408,-0.0958,-0.0904,0.1497,-0.0309,-0.0219,-0.0242,-0.0333,-0.0281,0.0481,0.003,-0.0603,-0.0029,0.0512,0.0063,0.0292,-0.0558,0.0164,-0.0152,-0.0534,-0.0398,0.0774,0.0396,-0.115,-0.0473,-0.0198,0.0299,-0.0284,0.0948,0.0704,-0.0363,0.0262,0.079,-0.0156,-0.0733,-0.0114,0.0321,-0.0318,0.044,-0.0302,-0.0477,0.0555,0.0018,-0.0519,-0.0089,-0.0024,0.0381,0.01,0.0261,0.0403,-0.0182,-0.0065,-0.0299,0.0005,-0.0226,-0.006,0.0133,-0.0693,-0.0099,0.0095,-0.0722,0.0302,-0.062,0.0223,-0.0211,-0.0053,0.0122,-0.0138,0.0027,-0.0343,0.0146,-0.0074,-0.0228,-0.0132,0.0253,0.0179,-0.0215,0.0661,0.054,-0.0708,-0.0598,-0.2546,0.0037,-0.0285,-0.0042,0.0782,-0.0437,0.0318,0.0047,0.0265,0.0944,0.0605,0.0048,-0.0163,0.0127,-0.0271,0.0312,0.0524,0.0386,-0.0197,0.0093,-0.023,0.0274,-0.0192,-0.0846,0.0185,0.0341,0.2261,0.0426,-0.0038,-0.0563,0.0416,0.0474,-0.0354,-0.1158,0.0972,0.0012,0.0439,0.0189,-0.0325,-0.0293,-0.0467,0.0026,0.0151,-0.0694,-0.024,-0.0224,-0.0168,0.0156,-0.0428,0.0201,0.0144,-0.01,0.0372,0.0344,-0.0542,-0.0596,-0.0701,0.0511,-0.0339,0.0001,0.0533,-0.0658,0.0073,-0.039,0.066,-0.0258,-0.033,-0.0004,0.0343,-0.0648,-0.0342,0.1246,0.0231,-0.0056,0.061,0.0084,0.0501,-0.0318,-0.0414,0.018,0.0603,-0.09,0.0474,0.0069,0.0366,0.006,0.0489,-0.0239,0.0453,-0.0104,0.0027,-0.0148,-0.0189,-0.0124,0.0605,0.0184,-0.3117,0.057,0.0018,0.0253,-0.0285,0.0076,0.0312,0.0522,-0.0189,-0.0263,0.013,0.0501,0.018,-0.0479,-0.0403,0.0173,0.0456,-0.0298,0.0179,-0.0054,0.0304,0.0149,0.2147,0.0172,0.0558,0.01,-0.0345,0.0137,0.0345,0.0021,0.0152,0.0154,0.1156,-0.0279,0.0213,0.0292,-0.0472,0.0104,0.0039,-0.0384,-0.0111,-0.0106,-0.0468,-0.0383,0.0978,0.0035,-0.0152,-0.0756,0.027,0.0196,-0.0019,-0.0195,-0.0454,0.0299,0.0175,0.0113,0.0115,-0.0199,-0.0293,-0.0783,-0.0481,-0.0739,-0.017,-0.0075,-0.0333]}
{"key":"[Latent Space Arc Therapy Optimization] Volumetric modulated arc therapy planning is a challenging problem in high-dimensional, non-convex optimization. Traditionally, heuristics such as fluence-map-optimization-informed segment initialization use locally optimal solutions to begin the search of the full arc therapy plan space from a reasonable starting point. These routines facilitate arc therapy optimization such that clinically satisfactory radiation treatment plans can be created in about 10 minutes. However, current optimization algorithms favor solutions near their initialization point and are slower than necessary due to plan overparameterization. In this work, arc therapy overparameterization is addressed by reducing the effective dimension of treatment plans with unsupervised deep learning. An optimization engine is then built based on low-dimensional arc representations which facilitates faster planning times.","layer":3,"vector":[-0.0463,-0.042,0.0564,0.0057,-0.0032,-0.0018,0.01,0.0347,0.0042,-0.0399,0.058,-0.065,0.0351,0.0694,-0.0135,0.0072,0.0078,0.0472,-0.0613,0.0205,0.0538,-0.0492,-0.0111,-0.0336,0.0372,-0.0263,-0.0359,-0.0292,-0.0398,-0.251,-0.0001,-0.0127,0.0171,-0.0432,-0.0159,-0.0288,0.0006,0.0788,-0.0534,0.0287,0.0096,0.0053,-0.0464,-0.0414,-0.0261,-0.0723,-0.001,-0.058,0.0253,-0.0125,0.0733,-0.0511,0.0085,0.0237,0.074,0.0256,0.0473,0.0518,0.027,0.0293,-0.01,0.0378,-0.1601,0.0659,0.0281,0.0293,-0.0237,-0.0333,0.0341,0.0385,-0.0199,0.0173,0.0283,0.0315,0.0202,-0.0332,0.0115,0.003,0.0297,0.0057,0.003,-0.0041,-0.0522,0.0037,-0.0336,-0.0631,0.007,-0.0502,0.0396,0.0295,-0.0375,-0.0075,-0.0292,0.0147,-0.0458,-0.0232,0.0438,0.0359,-0.0519,0.2047,-0.033,0.0059,0.0157,-0.0131,0.0066,-0.0499,-0.0643,-0.0031,-0.0055,0.0114,-0.043,-0.0214,0.0565,-0.0221,0.0239,0.0504,0.0486,0.0543,-0.0224,0.0079,-0.038,0.0081,0.0364,-0.0175,0.0018,-0.0779,0.0022,0.077,0.026,-0.0009,0.0555,0.0222,-0.0359,-0.0142,-0.0049,0.0277,0.0198,-0.0159,-0.0156,0.0303,-0.0247,-0.0352,-0.0047,-0.1113,-0.0059,0.1567,-0.0866,0.0126,-0.0111,-0.0637,-0.0298,-0.0034,-0.0374,-0.0377,0.004,0.0078,-0.0374,0.0605,-0.0489,0.0737,0.0205,-0.0364,-0.002,0.1195,-0.0241,-0.0791,-0.0526,0.0058,0.0243,-0.0291,0.0352,0.0333,-0.0076,0.0395,0.1009,0.0037,-0.0564,-0.0199,-0.0199,0.0044,0.0169,-0.0491,-0.0056,0.0373,0.0006,-0.0521,0.0139,-0.0203,0.0148,0.0296,-0.0268,0.0436,-0.0584,0.0218,-0.0145,-0.0447,-0.0374,-0.0205,-0.0184,0.0302,0.0258,0.0292,-0.0373,0.0415,0.0205,0.0201,-0.0137,-0.0015,0.0504,0.0624,-0.0416,0.0224,0.0904,-0.0336,-0.0379,0.0533,-0.0346,0.0022,0.0241,0.0696,0.0438,0.0082,-0.021,-0.226,0.0147,-0.0451,-0.0164,0.0242,-0.0498,0.0117,-0.0378,0.0276,0.044,0.0686,-0.0115,0.0045,-0.0291,-0.0242,0.0133,0.0083,0.0344,-0.0269,-0.0111,0.0182,0.0367,-0.0111,-0.0726,0.0355,0.0111,0.234,-0.0176,0.0136,-0.0139,0.0166,-0.0055,0.0077,-0.097,0.0265,0.0018,0.1087,0.0118,-0.0321,-0.0485,-0.045,0.0186,-0.0229,-0.0951,-0.0418,-0.0284,-0.0388,0.0483,-0.0087,-0.0273,0.0676,-0.0558,0.019,0.0306,0.0272,-0.0419,-0.1224,0.0276,-0.0561,-0.0082,0.0226,-0.0795,0.003,-0.0437,0.0243,-0.0066,-0.0028,0.0018,-0.006,-0.0598,-0.0311,0.0697,-0.0306,0.0209,0.0595,-0.016,0.0678,0.024,-0.0841,-0.0087,0.0431,-0.073,0.0517,0.0142,0.0169,0.0271,0.0588,-0.0281,0.0186,-0.0308,0.0337,-0.0006,-0.0577,-0.0078,0.0342,0.0171,-0.3037,0.076,0.0196,-0.0049,0.0,-0.0185,0.0525,0.0201,0.0084,-0.0001,-0.0436,0.0341,0.0468,-0.0098,0.0463,-0.029,0.0793,-0.0185,0.0705,-0.0487,0.0068,0.0136,0.2325,-0.0687,0.0012,0.0407,-0.0197,0.0139,0.036,0.0011,0.0102,0.0331,0.0385,-0.0516,0.0647,0.1028,0.0058,0.0421,0.0018,0.0421,0.0135,-0.0063,0.0358,-0.0182,0.0993,-0.0277,-0.0287,-0.0304,0.0198,-0.0096,-0.0553,0.0449,-0.0021,-0.0106,0.0454,-0.0216,-0.0117,-0.116,-0.0161,-0.0373,0.0338,-0.0272,0.0036,0.0165,0.05]}
{"key":"[Diverse Knowledge Distillation (DKD): A Solution for Improving The Robustness of Ensemble Models Against Adversarial Attacks] This paper proposes an ensemble learning model that is resistant to adversarial attacks. To build resilience, we introduced a training process where each member learns a radically distinct latent space. Member models are added one at a time to the ensemble. Simultaneously, the loss function is regulated by a reverse knowledge distillation, forcing the new member to learn different features and map to a latent space safely distanced from those of existing members. We assessed the security and performance of the proposed solution on image classification tasks using CIFAR10 and MNIST datasets and showed security and performance improvement compared to the state of the art defense methods.","layer":3,"vector":[-0.0134,-0.0428,-0.0134,-0.0047,0.051,0.026,0.05,0.0048,-0.0374,-0.0026,0.0027,-0.066,0.0377,0.0775,0.0032,-0.0187,0.0136,0.0451,-0.0411,0.0235,-0.015,-0.0198,-0.0042,-0.0267,-0.0088,0.023,0.0013,-0.0431,-0.0613,-0.2573,0.0094,-0.0457,0.0019,-0.0141,0.0098,-0.012,-0.0948,0.0324,-0.0283,0.0431,0.0197,0.0184,0.0062,-0.079,0.0108,-0.0566,-0.0375,-0.0012,-0.0149,-0.0659,0.0896,-0.0244,0.0094,0.0569,0.047,0.0268,0.0471,0.0665,0.0618,0.0569,0.0295,0.0248,-0.147,0.041,0.0323,0.031,-0.0258,0.0047,0.0415,0.0139,0.0163,0.0421,0.0415,0.0298,0.0164,0.007,-0.0382,-0.0002,-0.0282,-0.004,0.0575,-0.0274,-0.0422,-0.032,0.0111,-0.0501,0.0176,-0.0152,0.0851,-0.0047,-0.0398,0.0111,-0.0017,0.0457,-0.0414,-0.0548,0.0236,0.0129,-0.0573,0.2121,-0.0159,-0.0229,0.0217,-0.0205,0.0006,-0.0518,-0.0359,-0.0293,-0.0575,-0.0129,-0.0226,0.018,0.0146,-0.0348,0.0113,-0.0102,0.0493,0.0622,-0.0404,-0.0254,-0.0054,0.0192,0.0528,-0.0343,0.0323,-0.0502,-0.0189,0.1539,0.0106,0.0233,0.0242,-0.0097,-0.0243,-0.0027,0.0317,0.0538,0.0033,-0.0037,0.0358,-0.0823,-0.0464,-0.0191,0.0336,-0.087,0.0115,0.1091,-0.0256,0.0378,-0.0022,-0.0202,-0.0092,0.006,-0.0432,-0.0047,0.0216,0.0032,0.0089,0.0528,-0.0344,-0.0219,-0.0096,-0.0652,-0.0305,0.1337,0.0045,-0.0531,-0.0213,0.0109,0.0011,-0.01,0.0079,0.0158,0.0015,0.0257,0.029,-0.0199,-0.0841,-0.0545,0.0416,-0.0269,0.0144,-0.0793,-0.061,0.042,0.0398,-0.0205,-0.0138,-0.0529,0.0236,0.056,-0.0752,0.0587,-0.0064,-0.0181,-0.0244,0.0296,-0.0023,-0.007,0.021,-0.0688,-0.0177,0.0375,-0.0303,0.012,0.0052,0.0135,0.0185,-0.0096,0.0263,0.0199,-0.0313,-0.0214,0.0332,-0.037,0.0018,-0.0025,0.0295,0.0695,0.0113,0.0346,0.0777,-0.0341,-0.0508,-0.2566,0.0047,0.0091,0.0072,0.0524,-0.0528,0.0159,0.001,0.0391,0.0116,0.0699,0.024,-0.0525,0.0249,-0.0313,0.06,0.0534,0.0529,-0.0223,-0.005,-0.0151,0.0645,0.0001,-0.0948,0.0548,0.0466,0.1995,0.0565,0.0283,-0.0355,0.0135,0.0803,-0.0218,-0.1125,0.0707,0.0076,0.0395,-0.0095,-0.0455,-0.0151,-0.0096,0.0333,0.0036,-0.1185,-0.0671,-0.051,-0.0173,0.0389,-0.0515,0.0469,0.069,-0.0496,0.0265,0.024,0.0098,0.0026,-0.1025,0.0519,-0.0472,0.0304,0.0111,-0.0573,-0.0079,-0.0813,0.068,-0.0251,-0.0611,-0.0708,0.0585,-0.048,-0.0445,0.058,0.0319,0.0138,0.0778,-0.0011,0.0383,-0.0251,-0.0731,-0.0458,0.0568,-0.0159,0.0045,0.002,0.0841,0.0052,0.087,0.0209,0.02,-0.019,0.0388,-0.0211,-0.0653,-0.0002,0.0352,-0.0163,-0.2725,0.0295,-0.0143,0.0688,-0.018,0.015,0.0557,-0.0315,-0.0516,-0.0189,-0.0134,0.0513,0.0253,-0.025,0.0126,0.0197,0.0638,-0.0619,0.0356,-0.0292,0.0155,0.0835,0.2331,0.0204,-0.0176,-0.0004,-0.0176,0.0163,0.0102,-0.0298,0.0191,-0.0438,0.0771,-0.045,0.0516,0.0355,-0.0167,-0.0192,0.0267,-0.0428,-0.0141,-0.0217,-0.0276,-0.0033,0.0722,0.0137,-0.026,-0.0554,-0.011,-0.0162,0.017,0.0058,-0.0398,0.0124,0.016,0.0548,-0.0382,-0.0379,-0.0141,-0.0355,0.001,-0.0223,-0.0054,0.0223,-0.042]}
{"key":"[Subspace Robust Wasserstein Distances] Making sense of Wasserstein distances between discrete measures in high-dimensional settings remains a challenge. Recent work has advocated a two-step approach to improve robustness and facilitate the computation of optimal transport, using for instance projections on random real lines, or a preliminary quantization of the measures to reduce the size of their support. We propose in this work a \"max-min\" robust variant of the Wasserstein distance by considering the maximal possible distance that can be realized between two measures, assuming they can be projected orthogonally on a lower $k$-dimensional subspace. Alternatively, we show that the corresponding \"min-max\" OT problem has a tight convex relaxation which can be cast as that of finding an optimal transport plan with a low transportation cost, where the cost is alternatively defined as the sum of the $k$ largest eigenvalues of the second order moment matrix of the displacements (or matchings) corresponding to that plan (the usual OT definition only considers the trace of that matrix). We show that both quantities inherit several favorable properties from the OT geometry. We propose two algorithms to compute the latter formulation using entropic regularization, and illustrate the interest of this approach empirically.","layer":4,"vector":[-0.0355,-0.0468,0.0339,0.0067,-0.0183,0.0197,0.0311,0.0077,0.0449,-0.0114,0.031,-0.0675,0.0054,0.0643,0.015,0.055,0.0326,0.0768,-0.0616,0.0701,0.0185,-0.0702,-0.0115,-0.079,0.0545,-0.0005,-0.013,-0.0402,-0.0279,-0.2783,0.0114,-0.043,0.004,-0.0352,0.0276,-0.0301,-0.0382,0.0294,0.0008,0.0193,0.0254,0.0436,-0.0125,-0.0452,-0.0604,-0.0471,-0.0484,0.0026,-0.0597,-0.0317,-0.007,-0.0096,0.0132,0.0092,0.0745,0.0287,0.0367,-0.0072,0.0197,0.0733,0.0224,-0.0,-0.1362,0.0209,0.0384,-0.0115,-0.0287,-0.0228,-0.0028,0.0568,-0.0097,0.018,0.0243,0.0329,0.0284,-0.0081,-0.0012,0.0042,-0.0289,0.0356,0.0292,-0.0417,-0.0505,0.0162,-0.0456,-0.0331,0.0107,-0.0602,0.0299,0.015,-0.0304,-0.0421,0.0046,0.0308,-0.0552,-0.0652,0.0589,0.0248,0.0093,0.2138,-0.0254,0.0456,0.0692,-0.0211,0.032,-0.0356,-0.0386,-0.014,-0.0142,-0.0119,-0.0238,0.0077,0.0171,-0.0225,0.0007,0.0092,0.0667,0.0431,-0.0228,-0.0142,-0.0671,-0.0029,0.0304,-0.0405,0.036,-0.0504,0.0234,0.1403,0.0264,0.0791,0.0362,0.0026,-0.0294,-0.0343,0.0118,-0.0203,0.0087,-0.0052,0.0297,0.0142,-0.0081,-0.077,0.0297,-0.1213,-0.0184,0.1451,-0.0368,0.0275,-0.0213,-0.0652,0.0302,-0.0386,-0.0361,-0.0274,0.0453,0.0053,-0.0025,0.0144,-0.0593,0.0307,-0.0219,-0.0617,0.008,0.1202,-0.0012,-0.0672,-0.0182,-0.0117,0.0407,-0.0389,0.025,0.0064,0.0089,0.0756,0.0701,0.0097,-0.0954,0.0245,-0.0101,0.0151,0.0174,-0.0745,-0.0424,-0.0107,0.0685,0.005,0.0231,-0.0119,0.0009,0.069,-0.0232,-0.0342,-0.0253,0.0011,-0.0244,-0.0337,-0.021,0.0143,-0.0012,-0.0108,0.0413,0.0249,-0.081,0.034,0.0114,-0.0118,0.022,-0.0364,0.0299,0.0383,-0.0301,-0.0171,0.0472,-0.0759,0.0013,0.0063,0.0068,0.0474,0.0216,0.0521,0.0398,-0.028,-0.0945,-0.2318,-0.0231,-0.015,0.0152,0.043,-0.0779,0.0331,-0.0158,0.0483,0.0838,0.0364,-0.0344,-0.0021,0.0095,-0.0143,0.0372,0.0364,0.0384,-0.0407,-0.002,-0.0214,0.0622,-0.0585,-0.0067,0.0656,-0.0376,0.2221,0.0295,0.0467,-0.0253,-0.0015,0.0041,-0.0207,-0.0173,0.0204,-0.0128,0.0477,0.0128,-0.0368,-0.0438,-0.0022,-0.0208,0.0278,-0.0652,-0.0184,-0.0517,-0.0283,-0.0012,-0.0519,0.003,0.0462,-0.053,0.0541,-0.0398,0.0387,0.0083,-0.0781,-0.013,-0.0462,0.0508,-0.0259,-0.0689,0.0041,-0.0504,0.041,0.0254,0.0477,-0.016,0.0146,-0.003,-0.0053,0.0272,-0.0457,-0.0178,0.0735,-0.0038,0.0323,0.0187,-0.0498,-0.0351,0.0706,-0.0263,0.0339,-0.0071,0.0359,0.0551,0.0909,-0.0067,-0.0187,-0.0018,0.04,-0.011,-0.0629,-0.0135,0.0605,-0.0112,-0.2947,-0.0163,0.0084,0.0223,-0.0739,-0.0332,0.0306,0.0269,-0.0483,-0.0504,0.0124,0.0482,0.0292,-0.0121,0.0222,0.0139,0.1041,-0.0521,0.0412,-0.0839,0.0171,-0.0089,0.2231,-0.0326,0.016,0.0042,0.0055,0.0121,0.0019,-0.048,-0.0113,0.0138,0.0718,-0.0515,0.0662,0.0748,-0.0246,0.0658,-0.0028,0.0194,0.0057,0.0071,-0.0005,-0.0029,0.1234,0.0294,-0.0117,-0.0276,0.0466,0.025,-0.0146,0.0166,0.0187,0.02,0.0262,0.0361,-0.0262,-0.0557,-0.06,-0.0399,0.0112,-0.0287,-0.0566,-0.0201,0.0177]}
{"key":"[DeFlow: Learning Complex Image Degradations from Unpaired Data with Conditional Flows] The difficulty of obtaining paired data remains a major bottleneck for learning image restoration and enhancement models for real-world applications. Current strategies aim to synthesize realistic training data by modeling noise and degradations that appear in real-world settings. We propose DeFlow, a method for learning stochastic image degradations from unpaired data. Our approach is based on a novel unpaired learning formulation for conditional normalizing flows. We model the degradation process in the latent space of a shared flow encoder-decoder network. This allows us to learn the conditional distribution of a noisy image given the clean input by solely minimizing the negative log-likelihood of the marginal distributions. We validate our DeFlow formulation on the task of joint image restoration and super-resolution. The models trained with the synthetic data generated by DeFlow outperform previous learnable approaches on three recent datasets. Code and trained models are available at: https://github.com/volflow/DeFlow","layer":2,"vector":[-0.0179,-0.0173,0.0324,-0.0093,0.0426,0.0201,-0.0084,0.0216,0.0068,0.0026,0.0366,-0.0572,0.0648,0.0644,0.0011,0.0135,-0.0254,0.0807,-0.0551,-0.0145,0.0115,-0.0442,0.0043,-0.0393,-0.0055,0.0055,-0.0071,-0.0816,-0.0288,-0.2511,0.0039,-0.0092,0.0269,0.0026,0.0522,-0.0657,-0.0437,0.0438,-0.0559,0.0354,0.0171,0.0387,-0.0166,-0.0543,-0.044,-0.061,-0.0341,-0.0383,-0.0044,-0.0354,0.0731,-0.0224,0.0167,0.0421,0.0271,0.0202,0.0696,0.0546,0.0378,0.0606,-0.0341,0.0523,-0.1588,0.0519,0.0469,-0.0018,-0.0573,-0.0096,0.0121,-0.016,-0.0216,0.0359,0.0098,0.0438,-0.0053,-0.0369,0.0075,-0.0611,-0.0476,-0.0049,0.046,-0.0232,-0.0048,-0.0191,-0.0063,-0.0543,0.0189,-0.0541,0.0216,-0.0368,-0.0651,-0.0053,-0.0017,0.0402,-0.0362,0.0183,-0.019,0.0328,-0.0028,0.2204,-0.0992,0.0323,0.0454,-0.0319,0.0234,-0.0358,-0.0102,0.0221,-0.0067,-0.0142,-0.018,-0.0407,0.0652,-0.0424,0.05,0.0183,0.0386,0.03,-0.021,0.0067,-0.0324,0.004,0.0652,-0.0214,0.0272,-0.0799,0.009,0.1508,0.0293,0.0349,0.0286,-0.0109,-0.0297,0.0096,0.0135,0.0167,0.0161,0.0132,0.0041,-0.0271,-0.0365,-0.0522,-0.0216,-0.0701,-0.0525,0.0928,-0.0411,0.0436,-0.0501,-0.054,0.0049,0.0149,-0.0135,-0.0251,0.0187,0.0328,0.0261,-0.0021,-0.0499,0.0414,0.0138,-0.1017,-0.0633,0.0657,0.008,-0.0219,-0.0097,0.0002,0.0107,0.0462,0.0139,-0.0138,-0.0064,0.0245,0.0342,0.0436,-0.0454,-0.0003,0.0295,0.0068,-0.0148,-0.07,-0.0143,0.0578,0.0453,-0.0276,0.006,0.0132,0.0145,0.042,-0.0437,-0.0143,-0.0244,-0.0358,-0.0006,-0.0207,-0.011,-0.0143,-0.045,-0.0083,0.0289,-0.0,-0.0435,0.0273,-0.0229,0.043,-0.0026,0.0294,0.0182,0.0341,-0.0004,-0.0272,0.0696,0.0005,0.0127,-0.0056,0.0465,0.0118,-0.0257,0.0452,0.0506,-0.0334,-0.0569,-0.2349,0.0281,0.0208,0.0034,0.0878,-0.0751,0.0103,-0.0097,0.0697,0.0616,0.0585,-0.0321,0.0218,0.0232,0.0278,0.0465,0.0538,0.0305,-0.0544,-0.0441,-0.0388,0.0282,0.0154,-0.0711,0.0868,-0.005,0.2527,0.0171,0.0409,-0.0209,-0.0077,0.0434,-0.0573,-0.0728,0.0382,0.0426,0.051,-0.0145,-0.0412,0.0022,-0.0313,0.0144,0.0048,-0.1137,-0.0622,-0.0359,-0.0688,0.0142,-0.0749,0.0039,0.022,-0.0152,0.0319,-0.0022,0.023,-0.018,-0.0962,0.0628,-0.0621,0.0217,0.0249,-0.0703,0.001,-0.078,0.0847,-0.0221,-0.0469,-0.065,0.0105,-0.042,-0.027,0.0971,0.0002,0.0188,0.0348,0.067,0.0364,-0.0296,-0.0599,-0.0565,0.0856,-0.0452,0.0616,0.0337,0.0503,0.0558,0.0758,-0.0381,0.0013,-0.0325,-0.002,-0.0141,-0.0503,-0.0436,0.015,0.0031,-0.2956,-0.038,0.0289,0.0002,-0.0456,0.0432,0.054,0.0586,-0.0598,-0.0306,-0.0414,0.0576,0.0387,-0.0248,0.0098,0.0394,0.0745,-0.0662,0.0683,-0.0558,0.0109,0.0466,0.1645,-0.0331,0.0293,0.0011,-0.0362,0.0368,0.0357,-0.0383,0.033,0.0408,0.0612,-0.0241,0.0265,0.0788,-0.0338,0.0398,0.0115,0.009,-0.0208,-0.0034,0.003,-0.0394,0.0949,-0.0213,0.0173,0.0175,-0.0236,0.0021,-0.0089,0.0296,0.0022,0.0031,0.0332,0.0191,-0.0467,-0.0496,0.0018,0.0245,0.0094,-0.0625,-0.002,0.005,-0.0544]}
{"key":"[Modeling Atmospheric Data and Identifying Dynamics: Temporal Data-Driven Modeling of Air Pollutants] Atmospheric modeling has recently experienced a surge with the advent of deep learning. Most of these models, however, predict concentrations of pollutants following a data-driven approach in which the physical laws that govern their behaviors and relationships remain hidden. With the aid of real-world air quality data collected hourly in different stations throughout Madrid, we present an empirical approach using data-driven techniques with the following goals: (1) Find parsimonious systems of ordinary differential equations via sparse identification of nonlinear dynamics (SINDy) that model the concentration of pollutants and their changes over time; (2) assess the performance and limitations of our models using stability analysis; (3) reconstruct the time series of chemical pollutants not measured in certain stations using delay coordinate embedding results. Our results show that Akaike's Information Criterion can work well in conjunction with best subset regression as to find an equilibrium between sparsity and goodness of fit. We also find that, due to the complexity of the chemical system under study, identifying the dynamics of this system over longer periods of time require higher levels of data filtering and smoothing. Stability analysis for the reconstructed ordinary differential equations (ODEs) reveals that more than half of the physically relevant critical points are saddle points, suggesting that the system is unstable even under the idealized assumption that all environmental conditions are constant over time.","layer":1,"vector":[-0.0405,-0.009,0.0308,-0.0245,0.0322,0.0317,-0.0134,-0.0339,0.0454,-0.0073,-0.0594,-0.0344,0.0565,0.04,0.0099,0.0344,-0.007,0.0187,-0.0299,0.005,0.0301,-0.0075,-0.0502,-0.0406,0.0067,0.0515,-0.0119,-0.0108,-0.0349,-0.2649,0.0157,-0.0542,0.0437,-0.0478,0.0513,-0.0204,-0.0393,0.0445,-0.0305,0.0642,0.0045,-0.0102,-0.0214,-0.0901,-0.0168,-0.0636,-0.0085,-0.0184,-0.0388,-0.0888,-0.001,0.0041,-0.02,0.06,0.0159,0.0489,0.0438,0.0324,0.055,0.0185,0.0125,0.0211,-0.1905,0.0264,0.0526,0.063,-0.0127,-0.0008,0.0382,0.0223,-0.0137,0.0297,-0.0095,0.0449,-0.0013,0.0243,-0.0554,-0.0219,-0.0072,0.0175,0.0652,-0.0242,-0.0228,-0.0312,0.0259,-0.0408,0.0226,-0.0393,0.018,0.042,-0.0687,-0.0474,-0.0129,0.0215,-0.0673,0.0111,0.0424,0.0513,-0.0157,0.1849,-0.0771,0.024,0.0192,-0.0201,-0.0166,-0.0468,-0.0346,-0.0495,-0.0035,-0.0095,-0.0526,-0.0136,0.0264,-0.0843,0.0096,-0.0052,0.0725,0.0181,-0.0199,0.0201,0.0187,0.0411,0.0704,0.0412,0.0735,-0.0444,0.0212,0.1536,0.0218,0.0692,0.0642,-0.0194,-0.0646,-0.025,-0.0349,0.0193,0.0094,0.0082,-0.0211,0.0011,-0.0521,-0.0375,0.0242,-0.1105,-0.085,0.1057,-0.0162,0.0199,-0.0446,-0.0449,-0.0254,0.0261,-0.0434,-0.022,0.0299,0.0546,0.0046,0.0286,-0.0346,0.0471,-0.0304,-0.048,-0.0291,0.114,-0.0484,-0.0203,-0.0244,0.0483,0.0182,0.006,0.0615,0.0186,-0.0193,0.0215,0.0679,0.0187,-0.036,0.0368,0.0074,0.0262,0.0258,-0.0237,-0.0158,0.0687,0.0586,-0.0557,-0.0256,-0.0493,0.0064,0.0688,-0.0338,-0.0304,-0.0006,0.0044,-0.0225,-0.0379,0.0006,-0.0177,0.0286,-0.0631,0.0108,0.0093,-0.0503,0.0223,-0.0159,0.0328,-0.0387,0.0251,-0.002,0.0075,-0.0208,-0.0073,0.0395,-0.0681,-0.0155,0.0054,-0.0136,-0.0136,-0.0137,0.0397,0.078,-0.0341,-0.0506,-0.2317,-0.0234,0.0293,-0.0078,0.0799,-0.068,0.0462,-0.0337,0.0313,0.0553,0.0491,-0.0113,0.0149,0.001,-0.019,0.0639,0.0248,0.0604,-0.0258,-0.0299,-0.0375,-0.0311,-0.0323,-0.0909,0.0468,-0.0199,0.1873,-0.0076,0.0507,-0.0251,0.0223,0.0029,-0.0177,-0.0782,0.0277,-0.0128,0.0916,-0.0009,-0.0598,-0.0456,0.0128,-0.0172,-0.0089,-0.0582,-0.0563,0.0066,-0.0079,0.0334,-0.0784,-0.004,0.0294,-0.0037,0.0542,-0.0289,0.0389,-0.0008,-0.0555,0.0798,-0.0239,-0.001,-0.0203,-0.0503,-0.0048,-0.019,0.0441,0.0302,0.0018,-0.0974,0.0284,-0.023,-0.0043,0.1365,-0.0424,0.0108,0.0405,0.0214,-0.0107,0.0033,-0.0321,0.0022,0.0842,-0.0432,0.0659,0.039,0.0344,0.0068,0.0731,-0.032,0.0022,-0.023,-0.0014,0.0138,-0.0496,-0.0491,0.0623,-0.0074,-0.2859,-0.0036,-0.004,0.0535,0.0039,-0.0231,0.033,0.0104,-0.0417,0.0004,0.0003,0.063,0.0609,0.0139,0.0364,0.0448,0.0353,-0.0782,0.041,-0.0582,0.0363,0.0497,0.2002,-0.0478,0.025,0.0416,-0.0291,-0.0005,0.0016,-0.0226,0.0462,0.0278,0.116,-0.043,-0.0014,0.0938,-0.0429,0.0539,0.0423,-0.0071,-0.0058,0.0551,0.0014,-0.0327,0.0644,-0.0228,-0.0266,-0.0459,0.027,0.0375,-0.0425,0.0016,-0.0141,0.025,0.0546,0.0318,-0.0791,-0.0211,-0.032,-0.0275,-0.0044,-0.0745,-0.0231,-0.029,-0.0091]}
{"key":"[Neural Policy Gradient Methods: Global Optimality and Rates of Convergence] Policy gradient methods with actor-critic schemes demonstrate tremendous empirical successes, especially when the actors and critics are parameterized by neural networks. However, it remains less clear whether such \"neural\" policy gradient methods converge to globally optimal policies and whether they even converge at all. We answer both the questions affirmatively in the overparameterized regime. In detail, we prove that neural natural policy gradient converges to a globally optimal policy at a sublinear rate. Also, we show that neural vanilla policy gradient converges sublinearly to a stationary point. Meanwhile, by relating the suboptimality of the stationary points to the representation power of neural actor and critic classes, we prove the global optimality of all stationary points under mild regularity conditions. Particularly, we show that a key to the global optimality and convergence is the \"compatibility\" between the actor and critic, which is ensured by sharing neural architectures and random initializations across the actor and critic. To the best of our knowledge, our analysis establishes the first global optimality and convergence guarantees for neural policy gradient methods.","layer":1,"vector":[-0.0495,-0.0111,0.012,-0.0268,-0.0122,0.0313,0.0359,0.0304,0.0748,0.0075,0.0077,-0.0223,0.0552,0.1142,-0.0224,-0.0089,-0.0123,0.0329,-0.0222,0.0214,0.0606,-0.0519,-0.002,-0.0695,0.0124,-0.0124,-0.056,-0.0395,-0.0348,-0.2467,0.0455,-0.0624,0.0096,-0.0259,0.0006,0.0039,0.0006,0.025,-0.0445,0.0056,0.0082,0.0477,-0.0425,-0.0646,-0.0154,-0.0606,-0.0163,0.0001,-0.0657,-0.0148,0.0395,-0.0122,0.0373,0.0031,0.0217,0.0131,0.0431,0.0973,0.062,0.0397,-0.0117,0.0255,-0.1774,0.0465,0.0175,0.0921,0.0061,-0.0036,-0.011,0.0687,-0.0038,0.0165,0.0374,0.0241,0.0029,-0.001,-0.0317,-0.0307,0.0067,0.0306,0.0169,-0.0366,-0.0609,-0.0239,0.0305,-0.0347,0.0305,-0.055,0.0216,0.0243,-0.0348,0.008,0.043,0.0302,-0.0188,-0.0026,0.0089,0.0331,-0.0809,0.1899,-0.0465,0.0383,0.049,0.0056,0.0391,-0.0256,-0.0361,0.0007,-0.0016,-0.0056,-0.0457,-0.0311,0.0157,-0.0345,0.0194,0.0214,0.0429,0.0422,-0.0275,-0.0204,-0.0096,0.0161,0.0176,-0.0054,0.0256,-0.0684,-0.0175,0.1502,0.0242,0.0276,0.0528,-0.0443,-0.0316,-0.0585,0.0035,0.0354,0.0198,-0.0083,0.0242,-0.0344,-0.0485,-0.0344,-0.0145,-0.0932,-0.0611,0.1255,-0.0228,0.0244,-0.0327,-0.0035,-0.0179,0.0029,-0.0215,-0.0334,0.0255,0.0406,0.0048,0.0298,-0.0855,0.0105,-0.0342,-0.0466,-0.0216,0.1128,0.0011,-0.0372,-0.0236,-0.0294,0.0035,0.0112,0.0428,0.0469,-0.0185,0.0095,0.056,0.0391,-0.0928,-0.0029,-0.0203,-0.0189,0.0461,-0.0629,-0.0671,0.0222,0.0279,-0.0017,0.0218,-0.0482,-0.0009,0.0154,-0.0384,0.0241,-0.052,0.0013,-0.019,-0.0372,0.0162,-0.0085,0.0285,-0.0597,-0.0159,-0.0021,-0.071,0.0324,0.0172,0.0052,-0.0415,-0.0263,0.0374,0.008,-0.0599,0.0043,0.0552,-0.0145,-0.0337,0.0442,-0.006,-0.0177,-0.0105,0.0469,0.0241,0.0039,-0.0339,-0.2132,-0.0291,-0.0287,-0.0211,0.0961,-0.0866,0.0416,-0.0128,0.0494,0.071,0.0529,-0.0102,0.0133,0.0519,-0.0053,0.0416,0.0529,0.0421,-0.0128,0.0187,0.0201,0.0169,-0.0117,-0.096,0.0567,-0.0217,0.1975,0.0192,0.0805,-0.0291,0.0308,0.0252,-0.0352,-0.1122,0.0699,0.0084,0.1097,-0.0611,-0.0259,-0.0303,0.0225,0.0516,-0.0088,-0.0898,-0.049,-0.0454,-0.0596,0.0472,-0.1039,-0.0552,0.0457,-0.0314,0.0377,-0.0126,-0.0204,-0.0126,-0.0874,-0.0039,-0.032,0.0761,0.0177,-0.0632,0.0148,-0.0207,0.0779,0.0152,0.0312,-0.0208,0.0305,0.0442,-0.018,0.0447,-0.0116,-0.0169,0.0558,0.0157,0.026,0.0374,-0.0694,0.0117,0.0671,-0.0299,0.0454,0.0044,-0.0128,-0.0107,0.0864,-0.0474,0.0369,-0.0048,0.0057,0.0135,-0.0971,0.0032,0.0312,-0.0156,-0.2946,0.0517,0.0263,0.0458,-0.0123,-0.0002,0.0326,-0.0073,-0.0768,0.0046,0.0096,0.0746,0.042,0.0479,0.0199,0.038,0.0607,-0.06,0.0732,-0.0489,0.0131,0.0508,0.1932,-0.0219,0.0414,-0.0046,-0.004,-0.0015,0.033,-0.0292,0.0168,-0.0036,0.0562,-0.0692,0.0853,0.1105,-0.0399,0.0293,0.0313,-0.0244,-0.0104,0.0207,-0.0161,-0.0303,0.053,-0.0155,-0.0603,-0.0111,-0.0251,0.0317,-0.0283,0.0155,-0.0025,-0.0428,0.0294,-0.0062,-0.0701,-0.0442,-0.0616,-0.0148,-0.001,-0.0717,0.0079,0.0032,-0.0074]}
{"key":"[Detecting Novel Processes with CANDIES -- An Holistic Novelty Detection Technique based on Probabilistic Models] In this article, we propose CANDIES (Combined Approach for Novelty Detection in Intelligent Embedded Systems), a new approach to novelty detection in technical systems. We assume that in a technical system several processes interact. If we observe these processes with sensors, we are able to model the observations (samples) with a probabilistic model, where, in an ideal case, the components of the parametric mixture density model we use, correspond to the processes in the real world. Eventually, at run-time, novel processes emerge in the technical systems such as in the case of an unpredictable failure. As a consequence, new kinds of samples are observed that require an adaptation of the model. CANDIES relies on mixtures of Gaussians which can be used for classification purposes, too. New processes may emerge in regions of the models' input spaces where few samples were observed before (low-density regions) or in regions where already many samples were available (high-density regions). The latter case is more difficult, but most existing solutions focus on the former. Novelty detection in low- and high-density regions requires different detection strategies. With CANDIES, we introduce a new technique to detect novel processes in high-density regions by means of a fast online goodness-of-fit test. For detection in low-density regions we combine this approach with a 2SND (Two-Stage-Novelty-Detector) which we presented in preliminary work. The properties of CANDIES are evaluated using artificial data and benchmark data from the field of intrusion detection in computer networks, where the task is to detect new kinds of attacks.","layer":10,"vector":[-0.1122,0.0231,0.0381,-0.017,0.0364,0.0015,0.1009,0.0043,0.0658,-0.0466,0.0295,-0.0346,0.0081,0.0317,-0.0228,0.0002,-0.0049,0.0307,-0.0232,-0.0167,0.0314,-0.0295,-0.0351,-0.0376,-0.0117,0.0821,-0.0128,-0.0386,-0.0563,-0.2182,0.0036,-0.068,0.0663,-0.0246,0.0404,-0.0362,-0.0689,0.0362,-0.0046,0.0449,0.0117,0.0233,-0.0489,-0.0447,-0.0114,-0.0573,-0.0302,0.0039,0.0003,-0.0589,0.0106,-0.0158,0.0714,0.003,0.0452,0.0272,0.0509,0.0347,0.0576,0.0362,0.0331,0.035,-0.0963,0.0168,0.0714,0.0257,-0.0753,-0.0265,0.0444,0.0174,-0.0207,0.0808,-0.0206,0.0583,0.0034,0.0051,0.009,-0.0044,-0.0085,0.066,-0.0162,-0.0405,-0.0225,0.0058,-0.1003,-0.0454,0.0314,0.0106,0.0944,-0.0268,-0.0522,0.0284,-0.0355,0.0381,-0.0461,-0.033,-0.0038,-0.0094,-0.0142,0.2089,-0.0487,-0.036,0.0441,-0.0002,0.0468,-0.0159,-0.0346,-0.0483,0.0084,-0.0034,0.0412,-0.0312,0.0206,-0.061,0.031,-0.0312,0.0657,0.0299,-0.0159,0.0003,-0.0481,-0.011,0.0787,-0.0677,0.0384,-0.0612,0.0167,0.1571,0.0116,0.0314,0.0155,0.0131,-0.045,0.0017,0.0228,0.0474,0.0036,-0.008,0.0309,-0.0219,-0.0441,-0.0249,0.0539,-0.1084,-0.0124,0.1226,-0.0637,0.0339,-0.0201,-0.0364,-0.0293,0.0043,-0.0379,-0.0591,0.0288,-0.0068,0.0355,0.0377,-0.059,0.0357,-0.033,-0.0415,-0.0227,0.1466,0.0169,-0.0524,-0.0229,-0.0287,0.0,0.003,0.0014,0.0637,-0.0066,0.0234,0.0262,0.0156,-0.0689,-0.0068,-0.008,0.0253,0.0353,-0.0063,-0.0225,0.056,0.042,-0.0808,0.0237,-0.0122,0.031,0.0471,-0.0285,0.0067,-0.0024,-0.014,0.0308,-0.0105,-0.0184,-0.0051,0.0358,-0.0561,0.0153,0.0162,-0.078,0.0008,-0.0638,0.0269,-0.0457,-0.0085,0.0526,0.0066,-0.0235,-0.0098,0.0385,0.0111,0.0071,-0.0145,0.0012,0.0496,0.0163,0.0066,0.0134,-0.0219,-0.0466,-0.2587,-0.0052,0.0169,-0.022,0.041,-0.0341,-0.0043,0.0025,0.064,0.0301,0.0656,0.0001,-0.0298,0.0162,-0.0153,0.0832,0.0138,0.0067,-0.032,0.0127,-0.0187,0.0281,-0.021,-0.0687,0.0219,-0.037,0.1862,0.0323,0.0504,-0.0595,0.0195,0.0238,0.0165,-0.0579,0.0717,0.058,0.0916,0.0264,-0.0194,-0.0233,-0.0795,-0.0157,-0.018,-0.0814,-0.0317,-0.0448,-0.0022,0.0131,-0.0719,0.0155,0.0484,0.0072,0.0662,-0.0035,0.0307,-0.0795,-0.0228,0.0397,-0.0253,0.012,0.0085,-0.0552,-0.0105,-0.0724,0.0904,-0.0226,-0.0323,-0.0653,0.0136,-0.0026,-0.035,0.1338,0.0324,-0.0443,0.0345,0.0345,0.0323,-0.0563,-0.0701,-0.0484,0.0412,-0.0468,0.0265,0.0197,-0.0008,-0.0137,0.0793,0.0214,0.0548,-0.0497,0.0177,-0.0071,0.0059,0.0174,0.0517,-0.0007,-0.2763,0.0395,0.003,0.0228,-0.0009,-0.0078,0.0345,0.0065,-0.0376,0.0096,0.0012,-0.0026,0.0159,-0.009,-0.0122,0.031,0.0365,-0.0538,0.0104,-0.0697,-0.0022,0.057,0.2477,-0.0128,0.0056,0.0042,0.0146,0.0724,0.0138,-0.0212,0.04,-0.0006,0.0752,-0.0757,0.0274,0.0532,-0.0126,0.0293,-0.0005,-0.0117,-0.0252,-0.0207,-0.085,-0.0488,0.0917,-0.0443,-0.0455,-0.0573,0.0194,0.0756,-0.042,-0.0324,0.0019,0.0163,0.0397,0.0243,-0.0361,-0.0311,-0.049,-0.0208,0.012,-0.0451,0.0325,-0.0011,-0.0175]}
{"key":"[Deep neural heart rate variability analysis] Despite of the pain and limited accuracy of blood tests for early recognition of cardiovascular disease, they dominate risk screening and triage. On the other hand, heart rate variability is non-invasive and cheap, but not considered accurate enough for clinical practice. Here, we tackle heart beat interval based classification with deep learning. We introduce an end to end differentiable hybrid architecture, consisting of a layer of biological neuron models of cardiac dynamics (modified FitzHugh Nagumo neurons) and several layers of a standard feed-forward neural network. The proposed model is evaluated on ECGs from 474 stable at-risk (coronary artery disease) patients, and 1172 chest pain patients of an emergency department. We show that it can significantly outperform models based on traditional heart rate variability predictors, as well as approaching or in some cases outperforming clinical blood tests, based only on 60 seconds of inter-beat intervals.","layer":4,"vector":[-0.0396,0.0053,0.0343,0.0238,0.0253,0.0181,0.0461,-0.0219,0.0402,-0.0507,-0.033,-0.0422,0.01,0.0717,-0.0019,-0.0063,0.031,0.0159,-0.0181,0.068,-0.003,0.0113,0.0162,-0.0246,-0.0057,-0.0141,-0.0082,-0.0261,-0.0713,-0.223,0.0256,-0.0085,0.0436,-0.0333,0.0228,-0.0511,-0.0392,0.0516,-0.0462,0.0463,0.0269,-0.0179,-0.0024,-0.0579,0.0267,-0.0725,-0.0431,-0.0159,0.0095,-0.0214,0.0111,-0.0187,0.0383,0.0229,0.0218,0.0155,0.0677,0.0161,0.0486,0.0347,0.0509,0.0396,-0.1602,0.0595,0.0289,0.0209,-0.0382,-0.0181,0.0234,0.0367,-0.0311,0.0362,0.0148,0.0257,-0.0128,0.0214,-0.0053,-0.0435,0.0052,0.0314,0.0119,-0.0128,-0.0254,-0.053,0.0295,-0.0376,0.0355,-0.0668,-0.0266,0.0201,-0.0421,-0.0182,-0.0391,0.0397,-0.0299,-0.0161,0.0388,0.0386,-0.0978,0.2082,-0.0441,0.005,0.0155,-0.0312,0.0626,-0.0273,-0.0353,-0.0907,-0.0395,-0.0012,-0.0019,0.0105,0.049,-0.0589,0.046,0.0337,0.0345,-0.0093,0.0628,-0.0174,-0.0261,0.0017,0.0417,-0.0645,0.0433,-0.0335,0.0108,0.1362,0.0446,0.0251,0.0489,0.0021,-0.0362,-0.0294,0.0033,0.0065,0.0048,0.016,-0.0086,-0.0115,-0.0497,-0.0388,0.043,-0.1035,-0.0937,0.0803,-0.0198,0.0298,-0.0679,-0.0397,-0.0516,0.0497,-0.0522,-0.0283,0.0417,0.0166,0.0462,0.0523,-0.0414,-0.0523,-0.0523,-0.0447,-0.0239,0.1094,0.0248,-0.0462,-0.0085,-0.0129,0.0062,-0.021,0.0702,0.0217,0.0017,0.0306,0.1037,0.0118,-0.0348,0.0015,0.0186,0.0193,0.0404,-0.0357,-0.0317,0.0278,0.0194,-0.0338,-0.0016,-0.0824,0.0195,0.0454,-0.0182,-0.0318,-0.0217,0.0755,-0.0195,-0.0395,-0.0342,-0.0077,0.0133,-0.0148,-0.0025,-0.0106,0.0057,0.0358,0.0202,0.0094,-0.0147,0.009,0.0149,0.0439,-0.0084,0.0372,0.0732,-0.0189,-0.0508,0.033,-0.022,0.0345,-0.0173,0.0596,0.0316,-0.0265,-0.0272,-0.2271,0.0119,0.0049,-0.032,0.052,-0.0779,0.0259,-0.0055,0.0311,0.0738,0.0475,0.0184,-0.0307,0.0007,0.0327,0.1131,0.0609,-0.0067,-0.0219,0.013,0.016,0.0264,-0.0233,-0.0998,0.0227,0.0097,0.2288,-0.0148,0.0467,-0.0295,-0.0214,0.0286,-0.002,-0.1282,0.0644,-0.0138,0.0826,0.022,-0.0605,-0.0373,-0.0739,0.0633,0.0213,-0.1257,-0.0633,-0.0501,-0.0004,0.0294,-0.0588,-0.0013,0.0271,-0.1148,0.0177,-0.0061,0.0408,-0.0305,-0.1142,0.0314,-0.042,-0.0173,-0.0331,-0.0362,0.0605,-0.0103,0.0064,0.0217,-0.0096,0.0024,0.0305,-0.0567,-0.0165,0.0762,-0.0307,-0.0084,0.0661,-0.0283,0.0364,-0.0192,-0.0224,0.0215,0.0485,-0.0182,0.0435,0.0337,0.0375,-0.0087,0.1031,0.0228,0.0379,-0.0112,0.0056,0.001,-0.0311,-0.0464,0.0119,-0.0333,-0.2644,0.0076,-0.0197,-0.0127,-0.0059,-0.0437,0.0056,-0.0229,-0.0562,0.0136,-0.0394,0.01,0.0701,0.0051,0.0021,0.0554,0.0756,-0.0477,0.0695,-0.052,0.033,0.0494,0.2037,-0.0225,0.04,0.0573,-0.0055,0.0247,0.0428,-0.0239,-0.0065,0.0024,0.0745,-0.0727,0.0395,0.0647,-0.0288,0.0633,-0.0132,-0.015,0.0636,0.0154,-0.0519,-0.0428,0.1007,-0.0375,-0.0894,-0.0015,0.0092,0.049,-0.0029,-0.0299,0.0058,0.0023,0.0493,0.0258,-0.0506,-0.0666,-0.0107,-0.0532,0.0102,-0.0619,0.0027,0.0128,-0.036]}
{"key":"[Taming the Beast: Learning to Control Neural Conversational Models] This thesis investigates the controllability of deep learning-based, end-to-end, generative dialogue systems in both task-oriented and chit-chat scenarios. In particular, we study the different aspects of controlling generative dialogue systems, including controlling styles and topics and continuously adding and combining dialogue skills. In the three decades since the first dialogue system was commercialized, the basic architecture of such systems has remained substantially unchanged, consisting of four pipelined basic components, namely, natural language understanding (NLU), dialogue state tracking (DST), a dialogue manager (DM) and natural language generation (NLG). The dialogue manager, which is the critical component of the modularized system, controls the response content and style. This module is usually programmed by rules and is designed to be highly controllable and easily extendable. With the emergence of powerful \"deep learning\" architectures, end-to-end generative dialogue systems have been proposed to optimize overall system performance and simplify training. However, these systems cannot be easily controlled and extended as the modularized dialogue manager can. This is because a single neural system is used, which is usually a large pre-trained language model (e.g., GPT-2), and thus it is hard to surgically change desirable attributes (e.g., style, topics, etc.). More importantly, uncontrollable dialogue systems can generate offensive and even toxic responses. Therefore, in this thesis, we study controllable methods for end-to-end generative dialogue systems in task-oriented and chit-chat scenarios. Throughout the chapters, we describe 1) how to control the style and topics of chit-chat models, 2) how to continuously control and extend task-oriented dialogue systems, and 3) how to compose and control multi-skill dialogue models.","layer":0,"vector":[-0.0382,0.0147,0.0294,-0.0134,-0.0015,0.015,0.0263,0.0221,0.0078,0.0,-0.0122,-0.0275,0.0605,0.0314,0.0641,0.0124,0.0283,0.0237,-0.0467,0.022,0.0336,-0.0168,-0.0025,-0.0506,-0.031,-0.0155,-0.0547,-0.0717,-0.0341,-0.2353,0.0197,0.0092,0.0501,-0.0431,-0.0305,0.0112,-0.0474,-0.0053,-0.042,0.0114,0.04,0.0386,0.0028,-0.1098,0.0009,-0.0416,-0.019,-0.0148,-0.0447,-0.0607,0.0048,-0.0234,0.0197,0.0295,0.0248,0.0489,0.0731,0.0424,0.0219,0.0425,0.0102,0.0097,-0.1966,0.0921,0.005,0.0348,-0.0418,0.0101,0.0147,0.0339,-0.0201,0.048,0.0153,0.0571,0.0209,0.0012,0.0107,-0.0173,0.0278,0.047,-0.0232,-0.0008,-0.0293,-0.048,-0.0047,-0.0772,0.0108,-0.0125,0.0109,0.0466,-0.0704,-0.0237,-0.0015,0.055,-0.0214,-0.0325,-0.0029,0.0112,-0.0634,0.2075,-0.0419,0.0218,0.0193,-0.0583,0.0019,0.0186,-0.0263,-0.0226,-0.0405,-0.0036,-0.0362,-0.0114,0.0301,-0.0529,0.0519,0.0268,0.0975,0.0149,-0.0134,-0.014,-0.0038,0.0219,0.0119,-0.0379,0.0868,-0.0856,0.0583,0.1162,0.0496,0.0055,0.0673,0.0031,-0.0596,0.0016,0.0387,0.0001,0.016,-0.0198,-0.0001,-0.0366,-0.027,-0.057,-0.0201,-0.0688,0.0006,0.1006,-0.0061,-0.001,-0.0791,-0.0272,-0.0284,0.0201,0.0225,-0.0315,0.0074,0.0407,0.0696,0.0355,-0.0479,0.0248,-0.0065,-0.0432,-0.045,0.0657,0.0614,-0.1056,-0.0343,0.0144,0.0268,-0.0086,0.0239,0.0209,-0.011,-0.0103,0.0498,0.0539,-0.0777,-0.0043,-0.0389,0.0473,0.0611,-0.0847,-0.022,0.0394,-0.0437,-0.0887,0.0104,-0.0519,0.0224,0.0198,0.0131,0.0358,0.0157,-0.009,-0.0231,-0.0017,-0.0269,-0.0406,0.0028,-0.0334,-0.0032,0.0036,-0.0763,0.0043,-0.0287,0.008,-0.0101,0.0348,0.1159,0.0389,-0.0459,0.0263,0.0612,0.0085,-0.0532,-0.028,0.0031,0.0014,0.0144,0.0123,0.041,0.0445,-0.017,-0.2789,0.0277,0.0186,-0.0544,0.0552,-0.0504,0.0469,-0.0184,0.0329,0.0543,0.0549,-0.0001,-0.0114,0.0104,-0.0002,0.0387,0.0062,0.0453,-0.0093,-0.009,0.0245,-0.0082,-0.0059,-0.1173,-0.0047,-0.0328,0.2353,0.0409,0.0662,0.0027,0.0473,0.0621,-0.0222,-0.1216,0.0823,0.0074,0.097,-0.0177,-0.0033,-0.0115,-0.0262,0.0244,-0.0095,-0.1086,-0.0288,-0.0283,-0.0568,0.0135,-0.0313,0.0037,-0.0078,-0.0211,0.056,-0.0124,-0.023,-0.0546,-0.1171,0.0098,-0.0314,0.0166,-0.0235,0.0087,0.0175,-0.0478,0.0046,0.0706,-0.01,-0.0597,0.0353,-0.026,-0.0377,0.0984,-0.003,0.0523,0.0179,0.0378,-0.0339,-0.0285,-0.0833,-0.0195,0.0546,0.0208,0.049,0.0076,0.0021,-0.0099,0.0364,-0.0021,0.0927,-0.0362,-0.0179,0.0287,-0.0293,-0.0295,0.0409,-0.0091,-0.2946,0.0385,-0.0254,0.0247,-0.0376,0.0446,0.0101,0.026,-0.0625,0.0209,0.0069,0.0634,0.0156,-0.0095,0.002,0.014,0.0736,-0.0379,0.056,-0.0028,0.0378,0.0082,0.1658,0.0057,0.0687,-0.0467,-0.0372,0.007,0.0575,-0.0068,0.0116,-0.0017,0.1165,-0.0347,0.0256,0.0341,-0.0136,0.0385,0.0162,0.0091,-0.0497,0.019,-0.0069,-0.0117,0.0456,0.0102,-0.0088,-0.0122,-0.0034,0.0011,-0.0044,0.0328,-0.0137,-0.0181,0.0243,-0.0255,-0.017,-0.0688,-0.0192,-0.0198,-0.0022,-0.0394,-0.0028,0.0098,-0.0194]}
{"key":"[Primal-dual Learning for the Model-free Risk-constrained Linear Quadratic Regulator] Risk-aware control, though with promise to tackle unexpected events, requires a known exact dynamical model. In this work, we propose a model-free framework to learn a risk-aware controller with a focus on the linear system. We formulate it as a discrete-time infinite-horizon LQR problem with a state predictive variance constraint. To solve it, we parameterize the policy with a feedback gain pair and leverage primal-dual methods to optimize it by solely using data. We first study the optimization landscape of the Lagrangian function and establish the strong duality in spite of its non-convex nature. Alongside, we find that the Lagrangian function enjoys an important local gradient dominance property, which is then exploited to develop a convergent random search algorithm to learn the dual function. Furthermore, we propose a primal-dual algorithm with global convergence to learn the optimal policy-multiplier pair. Finally, we validate our results via simulations.","layer":2,"vector":[-0.0442,-0.0123,0.0184,0.0024,-0.0283,0.0298,0.033,0.0472,0.0513,-0.0014,0.0191,-0.0218,0.0198,0.0823,0.0373,0.0507,-0.0111,0.0261,-0.0322,0.0228,0.0808,-0.047,0.0005,-0.0619,0.0142,0.0017,-0.0408,-0.0326,0.0042,-0.257,0.0355,-0.0264,0.0132,-0.0352,-0.0082,-0.0685,-0.0412,0.0761,-0.0524,0.0077,0.0319,0.0327,-0.0245,-0.0416,-0.0144,-0.0804,0.0072,-0.0211,-0.0327,-0.0309,0.0092,-0.0436,0.0466,0.003,0.0522,0.0166,0.0252,0.0597,0.0392,0.0949,0.0004,-0.0099,-0.1626,0.0463,0.0499,0.0738,-0.0742,0.0121,0.0249,0.0486,-0.0421,0.0424,0.0296,0.0186,0.0081,-0.0022,0.0162,-0.0454,-0.0056,0.0441,0.0537,-0.0512,-0.0735,-0.0392,-0.0434,-0.0701,0.0179,-0.0429,0.0749,-0.0074,-0.0248,-0.0273,0.0066,-0.006,-0.068,-0.0246,0.0463,0.0294,-0.0704,0.201,-0.0646,0.0536,-0.0004,0.0002,0.0208,-0.0503,0.0204,-0.0123,-0.004,-0.0721,-0.0012,0.012,0.0459,-0.0542,0.0102,0.0162,0.0266,0.0441,0.0272,-0.012,-0.0246,0.0188,0.0536,0.0026,0.0551,-0.0767,0.0173,0.1489,0.0133,0.0442,0.0552,-0.0612,-0.0212,-0.0506,0.0123,-0.004,-0.0116,0.0073,0.0729,-0.0157,-0.0409,-0.0471,0.0031,-0.1152,-0.0721,0.0762,0.0116,0.002,0.0106,-0.0304,-0.0184,0.0352,-0.0266,0.0082,0.0546,0.0163,0.0235,0.0129,-0.0145,0.0119,-0.0877,-0.0509,-0.0469,0.0951,-0.0297,-0.0569,-0.0131,0.0123,-0.0167,-0.0244,0.0607,0.0324,-0.0405,-0.0346,0.078,0.0653,-0.0931,-0.0088,-0.0076,-0.0018,-0.0014,-0.0571,-0.0212,0.052,0.0254,0.0014,0.0345,-0.0386,-0.0125,0.0296,-0.0577,-0.0088,-0.0262,0.0029,-0.0216,-0.0341,-0.0101,-0.0194,0.0396,-0.0146,0.0159,0.0128,-0.0445,0.0491,-0.0146,-0.025,-0.012,-0.0082,-0.0081,0.0325,-0.0265,0.0094,0.0618,-0.0495,-0.0664,0.0125,0.0374,0.0285,-0.0234,0.0123,0.0618,0.0448,-0.0461,-0.2137,-0.0137,-0.0519,-0.0212,0.0366,-0.0393,0.0041,-0.0691,0.0261,0.0777,0.0577,-0.008,-0.0172,0.0491,-0.0122,0.0797,0.0198,-0.0123,-0.0364,-0.0,-0.017,0.0127,-0.0311,-0.0543,0.0698,0.0046,0.2249,0.0171,0.0696,0.0193,0.0172,0.0089,0.0195,-0.036,0.0791,0.0159,0.0392,-0.0312,-0.0352,-0.0281,0.0232,-0.0129,-0.0334,-0.0669,-0.0512,-0.037,-0.0336,0.0975,-0.0992,0.0017,0.0429,-0.0167,0.0608,-0.0445,-0.0227,-0.0457,-0.0934,0.0276,-0.0169,0.0573,0.014,-0.0399,-0.0192,-0.0536,0.0583,0.0157,0.0137,-0.0375,0.0469,-0.0257,0.0009,0.05,0.0148,0.0098,0.0301,0.0058,0.0215,-0.0159,-0.0731,-0.0382,0.0703,-0.0566,0.0512,0.04,-0.0094,-0.0267,0.0873,0.0119,0.0058,-0.0064,-0.0601,0.0031,-0.0602,-0.0093,0.0435,0.0124,-0.2782,0.0274,0.034,0.0095,-0.057,-0.0209,0.0936,-0.0052,-0.0705,0.041,0.0046,0.0552,0.0273,0.0511,0.0129,0.0195,0.0329,-0.005,0.0451,-0.0967,0.0196,0.0495,0.1704,-0.0298,0.0044,-0.0044,-0.0624,0.002,0.0429,-0.0049,0.029,0.0129,0.0852,-0.0989,0.0611,0.0992,-0.0172,0.0446,0.0094,0.0143,-0.0302,0.0236,0.0098,0.0041,0.0885,-0.0197,-0.0264,-0.0125,-0.0303,0.0299,0.0301,0.0172,0.0077,0.0242,0.0224,0.0311,-0.0542,-0.0834,-0.0063,-0.0414,0.0258,-0.0475,-0.0241,-0.0115,-0.0265]}
{"key":"[Customer Sentiment Analysis using Weak Supervision for Customer-Agent Chat] Prior work on sentiment analysis using weak supervision primarily focuses on different reviews such as movies (IMDB), restaurants (Yelp), products (Amazon).~One under-explored field in this regard is customer chat data for a customer-agent chat in customer support due to the lack of availability of free public data. Here, we perform sentiment analysis on customer chat using weak supervision on our in-house dataset. We fine-tune the pre-trained language model (LM) RoBERTa as a sentiment classifier using weak supervision. Our contribution is as follows:1) We show that by using weak sentiment classifiers along with domain-specific lexicon-based rules as Labeling Functions (LF), we can train a fairly accurate customer chat sentiment classifier using weak supervision. 2) We compare the performance of our custom-trained model with off-the-shelf google cloud NLP API for sentiment analysis. We show that by injecting domain-specific knowledge using LFs, even with weak supervision, we can train a model to handle some domain-specific use cases better than off-the-shelf google cloud NLP API. 3) We also present an analysis of how customer sentiment in a chat relates to problem resolution.","layer":14,"vector":[-0.0029,0.0142,0.0009,-0.0045,0.0028,-0.0137,0.0599,0.0346,0.008,-0.0349,-0.0002,-0.0088,0.065,0.0639,0.0415,0.0346,0.0768,0.0282,-0.0697,0.0148,0.0511,-0.0444,-0.0459,-0.0228,0.0225,0.0079,-0.076,-0.0485,-0.0328,-0.1939,-0.0058,-0.0602,0.0756,0.0097,0.0063,-0.0272,-0.0437,-0.0163,-0.0214,0.0158,0.0271,-0.0461,-0.0167,-0.0436,0.0274,-0.0774,-0.0135,-0.0179,-0.0519,-0.0518,0.0214,-0.0333,0.0301,-0.0138,0.0018,0.0477,0.0393,0.0248,0.0644,0.0417,0.0319,0.0371,-0.2068,0.0696,0.041,0.0584,-0.084,-0.0091,-0.0367,0.079,-0.0215,0.023,0.0375,0.0604,-0.0091,0.0539,0.0109,-0.0074,0.0201,0.032,0.0276,-0.0243,-0.0709,-0.0012,-0.0274,-0.0556,0.0256,-0.0042,0.0412,0.0576,-0.0044,-0.0023,-0.007,0.0443,-0.0652,-0.0464,-0.0093,0.0098,-0.0798,0.2321,-0.0347,0.0543,0.0081,-0.0235,0.0297,-0.0319,-0.0241,-0.0321,-0.0269,-0.0209,-0.0479,-0.0076,0.0106,-0.0607,0.052,-0.0052,0.061,0.0232,0.005,-0.0124,-0.0004,-0.0227,0.045,-0.0208,0.0483,-0.0423,0.0294,0.1316,0.0378,0.0221,0.0718,-0.0298,-0.07,-0.0099,0.0294,0.0554,-0.0143,0.0148,0.0474,-0.0235,-0.0617,-0.0782,-0.0419,-0.0552,-0.0447,0.1455,-0.0031,0.0341,-0.0519,-0.0125,0.0052,0.0431,-0.0584,-0.0525,0.0331,0.0261,0.0589,0.0487,-0.0447,0.0342,0.0271,-0.0766,-0.0428,0.0536,-0.008,-0.1085,0.0018,-0.0022,0.0219,-0.0158,0.0505,0.0548,-0.0079,0.0234,0.0193,0.0205,-0.0802,-0.0125,-0.0289,0.0124,0.0324,-0.0234,-0.0301,0.0449,0.0083,-0.0777,0.0044,-0.0376,0.0201,-0.0067,-0.0569,-0.0397,-0.0153,0.0262,-0.0361,-0.0232,-0.022,-0.0186,0.0077,-0.0125,0.0381,0.0119,-0.0647,0.0371,-0.0101,0.004,0.0517,0.0052,0.0295,0.0123,-0.0229,0.0112,0.0351,-0.0008,-0.0335,-0.0055,0.0399,0.0465,0.0106,-0.0038,0.04,-0.0006,-0.0689,-0.2512,0.0223,0.0202,-0.0177,0.0412,-0.0577,0.043,-0.0017,0.0513,0.1092,0.0579,-0.046,-0.0135,0.0022,-0.0231,0.035,0.0526,0.0394,-0.0153,0.0177,0.0073,-0.0013,0.0053,-0.1225,0.0474,0.0221,0.2222,0.0404,0.0193,-0.0382,0.0406,-0.0174,-0.0259,-0.1062,0.1183,-0.0093,0.0623,-0.017,-0.0167,-0.0213,-0.0467,0.0485,-0.0117,-0.1026,-0.0083,-0.015,0.0032,-0.043,-0.0905,0.0446,-0.0063,-0.027,0.0398,0.0227,-0.0145,-0.0207,-0.0713,0.0201,-0.029,0.042,-0.0024,-0.0344,0.0478,-0.0347,0.029,-0.0158,-0.0361,0.0045,-0.0068,-0.008,-0.0361,0.1139,-0.0437,-0.0206,0.0255,0.0193,0.0157,-0.0387,-0.0754,-0.0289,0.065,-0.0112,0.0209,0.016,0.0314,0.0059,0.058,-0.0164,0.0432,-0.0149,-0.0023,0.035,-0.1218,0.0037,0.0488,-0.0082,-0.2811,0.0446,0.0025,0.0193,-0.0109,0.0569,0.0487,0.0128,-0.0777,-0.0145,0.0352,0.0096,-0.0041,-0.0312,0.0038,0.0099,0.0713,-0.0274,0.0416,-0.0229,0.0192,0.0441,0.1869,0.0045,0.0275,0.0133,-0.0,-0.0132,0.0149,-0.0076,-0.0007,0.0175,0.1234,-0.0648,0.032,0.0085,-0.066,-0.0029,0.0435,-0.0082,0.0109,0.0144,-0.0232,-0.038,0.0476,0.036,-0.0006,-0.0648,-0.0143,0.0063,-0.019,-0.0123,0.0099,0.0258,-0.0017,-0.018,-0.0389,-0.0281,-0.0147,-0.0316,-0.0193,-0.0485,0.014,0.0424,0.0003]}
{"key":"[A Convergence Result for Regularized Actor-Critic Methods] In this paper, we present a probability one convergence proof, under suitable conditions, of a certain class of actor-critic algorithms for finding approximate solutions to entropy-regularized MDPs using the machinery of stochastic approximation. To obtain this overall result, we prove the convergence of policy evaluation with general regularizers when using linear approximation architectures and show convergence of entropy-regularized policy improvement.","layer":5,"vector":[-0.0753,-0.0034,0.0542,-0.0317,-0.0248,0.0262,0.0324,0.0265,0.0713,-0.0007,0.0163,-0.0263,0.0491,0.0631,0.0315,0.0318,-0.0017,0.0267,-0.0102,0.0038,0.0391,-0.0317,-0.0042,-0.0661,0.014,0.0145,-0.0462,-0.0665,-0.0141,-0.2307,0.0524,-0.0706,0.0309,-0.0399,-0.0101,-0.0276,-0.0205,0.0586,-0.0489,0.0287,0.0332,0.0413,-0.0443,-0.0797,-0.0374,-0.0495,0.0079,-0.0052,-0.0422,-0.0276,0.0243,-0.0131,0.0425,-0.0047,0.0405,0.0045,0.0199,0.0884,0.0424,0.0427,0.0004,0.0474,-0.1938,0.0711,0.0196,0.085,-0.0228,-0.027,0.0244,0.0764,-0.0343,0.0367,0.0292,0.0502,0.0134,-0.0203,-0.0267,-0.0469,0.0078,0.037,0.0044,-0.0582,-0.0591,0.0065,0.0065,-0.0472,0.0379,-0.051,0.0362,0.0082,-0.0211,0.0028,0.049,0.0261,-0.0366,-0.0149,0.0143,0.0252,-0.0453,0.1909,-0.0376,0.063,0.0596,0.0062,0.0511,-0.0277,-0.025,-0.028,-0.0005,0.0136,-0.0164,-0.0052,0.0469,-0.051,0.0206,0.0138,0.0504,0.0719,-0.0467,-0.0371,-0.002,0.0161,0.0082,0.0265,0.0246,-0.0616,-0.0221,0.1605,0.0475,0.0633,0.066,-0.0358,-0.0344,-0.0529,0.0002,-0.0078,-0.0068,-0.0241,0.0474,-0.004,-0.0216,-0.0518,-0.0288,-0.1007,-0.0443,0.0974,-0.0038,0.0444,-0.0405,0.0003,0.0068,-0.0274,-0.0218,-0.0214,0.0267,0.0563,0.0319,0.0437,-0.0525,0.0231,-0.0149,-0.0543,-0.0145,0.0753,-0.0031,-0.0359,-0.0186,-0.0318,0.0256,-0.0162,0.0319,0.0274,-0.0273,0.0025,0.0241,0.0456,-0.0743,-0.0097,0.0156,-0.0188,0.0366,-0.0441,-0.0413,0.0076,-0.0017,-0.0093,0.0281,-0.0303,0.0058,0.0138,-0.0434,-0.0102,-0.0328,-0.0123,-0.0017,-0.0675,-0.0122,-0.0223,0.0243,-0.0602,0.0314,-0.0077,-0.0825,0.027,-0.0232,0.0326,-0.0794,-0.0489,0.0338,0.0062,-0.0541,0.0165,0.034,0.0367,-0.0459,0.0766,0.0206,-0.0007,-0.0116,0.0167,0.0413,0.0476,-0.0418,-0.2167,-0.0448,-0.0573,-0.001,0.0662,-0.0688,0.035,-0.0597,0.0219,0.0813,0.0372,-0.0336,-0.0039,0.061,-0.0174,0.0315,0.0485,0.0229,-0.0462,0.0332,0.0313,0.0582,-0.0214,-0.0995,0.0398,-0.023,0.1952,0.0385,0.0862,0.0039,0.0171,0.0497,-0.0424,-0.0742,0.0436,0.0284,0.0915,-0.0374,0.0264,-0.0599,0.0471,0.0642,-0.054,-0.0936,-0.0444,-0.0501,-0.0443,0.0332,-0.1001,-0.0288,0.0325,-0.0053,0.0267,-0.0237,-0.0296,-0.023,-0.0713,-0.0079,-0.0147,0.0612,-0.0022,-0.058,0.0044,-0.0003,0.0547,-0.0043,0.0582,-0.0541,-0.003,0.0135,-0.027,0.0541,-0.0187,-0.0162,0.0305,0.0254,0.0194,0.0082,-0.0401,-0.0354,0.0465,-0.012,0.0347,0.0099,-0.0028,0.0021,0.0767,-0.0132,-0.0045,-0.0411,0.026,-0.0035,-0.0954,0.0514,0.0356,-0.0007,-0.3224,0.0589,0.0125,0.0495,-0.0645,-0.0029,0.0257,-0.0326,-0.051,0.0124,0.025,0.0735,0.0183,0.0292,0.0059,0.015,0.0457,-0.0526,0.0678,-0.0675,0.0233,-0.0009,0.2066,-0.0296,0.0327,0.0049,0.0057,-0.0158,-0.0028,-0.0242,0.0272,0.0031,0.0716,-0.066,0.0886,0.0956,0.003,0.0273,0.0037,-0.011,-0.017,-0.03,0.0056,0.0052,0.0895,0.0085,-0.0206,-0.0226,-0.0173,0.0141,-0.0302,0.0389,-0.012,-0.0304,0.0168,0.0124,-0.0547,-0.039,-0.0411,-0.0234,-0.0259,-0.0761,0.0028,0.0062,0.0098]}
{"key":"[Joint Abductive and Inductive Neural Logical Reasoning] Neural logical reasoning (NLR) is a fundamental task in knowledge discovery and artificial intelligence. NLR aims at answering multi-hop queries with logical operations on structured knowledge bases based on distributed representations of queries and answers. While previous neural logical reasoners can give specific entity-level answers, i.e., perform inductive reasoning from the perspective of logic theory, they are not able to provide descriptive concept-level answers, i.e., perform abductive reasoning, where each concept is a summary of a set of entities. In particular, the abductive reasoning task attempts to infer the explanations of each query with descriptive concepts, which make answers comprehensible to users and is of great usefulness in the field of applied ontology. In this work, we formulate the problem of the joint abductive and inductive neural logical reasoning (AI-NLR), solving which needs to address challenges in incorporating, representing, and operating on concepts. We propose an original solution named ABIN for AI-NLR. Firstly, we incorporate description logic-based ontological axioms to provide the source of concepts. Then, we represent concepts and queries as fuzzy sets, i.e., sets whose elements have degrees of membership, to bridge concepts and queries with entities. Moreover, we design operators involving concepts on top of the fuzzy set representation of concepts and queries for optimization and inference. Extensive experimental results on two real-world datasets demonstrate the effectiveness of ABIN for AI-NLR.","layer":1,"vector":[-0.0508,0.0057,0.0273,0.0188,0.0112,0.0229,0.0565,0.0137,0.0469,-0.0398,-0.0173,-0.0943,0.0415,0.0568,0.009,0.0108,-0.0031,0.0677,-0.04,0.0147,0.0358,-0.0593,-0.0482,0.0108,-0.0175,0.0105,-0.0278,-0.0086,-0.0444,-0.1966,0.0067,-0.0705,0.0284,0.0278,-0.0269,0.0241,-0.0263,0.0728,-0.0447,0.0277,0.0545,-0.0201,0.0066,-0.035,0.0067,-0.0199,-0.007,-0.0336,-0.0796,-0.0299,0.0176,-0.044,-0.0143,-0.0072,0.0039,0.0961,0.0409,0.0646,0.0235,0.0306,0.0159,0.0287,-0.1391,0.0789,0.0416,-0.0201,-0.0411,-0.0368,0.0214,0.0543,-0.0081,0.0494,0.0246,0.1017,-0.0042,0.0017,0.0044,-0.0078,-0.0226,0.0032,-0.014,-0.0077,-0.0297,0.0025,-0.0084,-0.0095,0.019,-0.0514,0.0108,-0.0046,-0.0253,-0.0137,-0.0165,0.0334,-0.034,-0.0187,0.0336,0.0134,-0.0579,0.17,-0.0618,0.0119,0.0011,-0.0987,-0.0016,-0.0204,-0.0361,-0.0663,-0.0074,-0.0222,-0.0204,-0.0259,0.0369,0.0253,0.031,0.0251,0.1031,0.0296,-0.016,-0.0384,-0.0152,0.0144,0.0489,0.0169,0.0179,-0.0602,0.0141,0.0964,-0.0229,0.0214,0.0517,-0.0182,-0.0187,-0.0155,0.0605,0.0079,0.0767,0.033,-0.0105,0.0071,-0.0434,-0.0691,0.0613,-0.078,-0.0941,0.0983,-0.0438,-0.0233,-0.0513,-0.0412,-0.0792,0.0331,-0.0275,-0.0746,-0.0332,0.1106,0.031,0.0211,-0.0775,-0.0164,0.0119,-0.0268,-0.0194,0.1137,0.0037,-0.0938,-0.0286,0.023,0.0223,-0.037,0.0611,0.0056,-0.0125,0.0406,0.0792,0.0301,-0.0437,0.0259,0.0009,0.0056,0.0361,-0.0775,-0.0028,0.0366,-0.0053,-0.0736,0.0045,-0.0522,0.0042,0.001,-0.0431,0.0191,-0.038,0.019,-0.0098,-0.047,0.0092,-0.0048,-0.0063,-0.0186,0.0354,-0.0163,-0.0419,0.0313,-0.0528,0.0059,0.0068,0.0193,0.0488,0.0427,0.016,0.0161,0.0101,-0.032,-0.0007,-0.0374,-0.0062,0.001,0.0292,0.0489,0.0311,-0.0571,-0.0195,-0.2303,0.024,0.0079,-0.057,0.0311,-0.0388,-0.0052,-0.0071,-0.0025,0.0889,0.0699,0.0018,-0.0167,0.0134,-0.0308,0.0353,0.048,0.0312,-0.0482,0.0232,-0.0127,0.0325,0.0041,-0.1239,0.041,0.0003,0.245,0.0623,0.0863,-0.0055,0.0554,0.0121,-0.0297,-0.1149,0.0681,-0.0134,0.0207,-0.0011,-0.0092,-0.0251,-0.0194,0.0513,-0.0385,-0.0714,-0.0095,-0.0485,-0.0234,0.0283,-0.0429,0.0274,-0.0275,-0.0337,-0.0061,0.0276,-0.0422,-0.0251,-0.0826,0.0119,-0.0545,0.0372,0.0157,-0.0532,0.0031,-0.0345,0.0463,0.003,-0.006,0.0084,0.0106,-0.0448,-0.023,0.1,-0.0032,0.0276,0.0192,0.0543,0.073,-0.052,-0.0151,0.0121,0.0607,-0.0686,0.0684,-0.007,0.0318,0.0076,0.0617,-0.054,0.0733,-0.0047,-0.0049,-0.0004,-0.0417,-0.0206,0.0422,-0.0214,-0.2858,0.0308,0.0134,-0.0111,-0.024,0.0486,0.0246,0.0439,-0.0351,-0.0289,-0.002,0.0481,0.0006,-0.0102,-0.0197,0.0238,0.0573,-0.0637,0.059,-0.0309,0.0359,0.0639,0.2305,-0.004,0.0528,0.0618,0.0024,-0.0453,0.004,0.0307,0.015,-0.002,0.0748,-0.0576,0.0099,0.1237,-0.0698,0.0381,0.0764,-0.042,-0.0338,0.024,-0.0457,-0.0013,0.079,0.0275,-0.0433,-0.0509,-0.022,0.0444,-0.0382,-0.0142,-0.0224,-0.0004,0.0954,-0.0085,-0.0126,-0.0373,-0.0902,-0.0024,-0.0117,-0.0583,-0.014,0.0074,0.0091]}
{"key":"[Physics Guided Machine Learning for Variational Multiscale Reduced Order Modeling] We propose a new physics guided machine learning (PGML) paradigm that leverages the variational multiscale (VMS) framework and available data to dramatically increase the accuracy of reduced order models (ROMs) at a modest computational cost. The hierarchical structure of the ROM basis and the VMS framework enable a natural separation of the resolved and unresolved ROM spatial scales. Modern PGML algorithms are used to construct novel models for the interaction among the resolved and unresolved ROM scales. Specifically, the new framework builds ROM operators that are closest to the true interaction terms in the VMS framework. Finally, machine learning is used to reduce the projection error and further increase the ROM accuracy. Our numerical experiments for a two-dimensional vorticity transport problem show that the novel PGML-VMS-ROM paradigm maintains the low computational cost of current ROMs, while significantly increasing the ROM accuracy.","layer":1,"vector":[-0.036,-0.0314,0.0681,-0.0205,-0.014,0.0147,-0.0426,0.0205,0.0395,-0.0272,0.0042,-0.0744,0.0458,0.0396,0.0357,-0.0123,0.0387,0.0579,-0.028,0.044,-0.0113,-0.014,-0.0289,-0.0498,0.0418,-0.0013,-0.0449,-0.0274,-0.0226,-0.2646,0.0392,-0.0483,-0.0262,-0.0143,0.0131,-0.012,-0.0277,0.0302,-0.0193,0.032,0.0329,-0.0245,0.0311,-0.0111,-0.0149,-0.0259,-0.0094,0.0034,-0.0145,-0.0443,0.004,-0.0578,-0.0033,0.034,0.0374,0.0588,0.0465,0.0248,0.0451,0.0154,0.0349,0.0413,-0.1925,0.0705,0.0825,0.0172,-0.0108,0.0252,0.0363,0.0537,-0.0335,0.0165,0.0097,0.044,0.0096,-0.0259,-0.0061,-0.0136,-0.0014,-0.0267,0.0168,-0.0309,-0.0531,-0.045,-0.008,0.0158,0.0524,-0.0297,0.0093,-0.026,-0.0461,-0.0652,-0.0117,0.032,-0.0432,0.0389,0.0139,-0.0091,-0.0542,0.2088,-0.0258,0.0037,-0.0081,0.0158,0.0533,-0.0705,-0.0411,-0.0531,-0.0066,-0.0103,-0.0073,-0.0236,0.0434,-0.0465,0.0262,0.0089,0.0417,0.0175,-0.0268,0.004,-0.0521,-0.0114,0.021,0.0036,0.0448,-0.079,-0.0209,0.1271,0.0236,0.0281,0.022,0.0124,-0.0674,-0.0591,0.0278,0.0142,0.0001,-0.0105,0.0266,0.0252,-0.0727,-0.027,-0.003,-0.1261,-0.0444,0.1177,-0.0552,0.0046,-0.081,-0.0073,-0.0116,0.0316,-0.1009,-0.026,0.0284,0.0335,-0.0449,0.055,-0.0431,0.0475,-0.0101,-0.0696,-0.0624,0.0716,-0.0347,-0.0912,-0.0339,0.047,0.0285,-0.0128,0.0059,0.0669,-0.0106,0.0047,0.0767,0.0349,-0.0672,0.0388,0.045,0.0198,0.031,-0.0327,0.0017,0.0633,0.0545,-0.0137,0.0065,-0.0625,-0.0114,0.0,-0.0222,-0.0099,0.021,0.0056,0.0011,-0.0125,0.0061,-0.0216,0.0292,0.0006,0.027,0.012,-0.0543,0.1059,-0.083,0.0363,0.0325,0.001,0.0177,0.0437,-0.0479,-0.0607,0.0701,-0.0126,-0.011,0.0152,0.0123,0.0465,0.0086,0.0157,0.0773,-0.0696,-0.0747,-0.2219,-0.0249,-0.0015,-0.014,0.0534,-0.0331,0.0573,-0.0283,0.0664,0.0341,0.0657,-0.0121,-0.046,-0.0193,-0.0128,0.0338,0.0079,-0.0116,-0.0331,-0.0246,-0.0167,0.0273,-0.0807,-0.086,0.046,0.0177,0.1696,0.0397,0.0015,-0.0298,0.0447,0.0104,0.0156,-0.038,0.0723,0.0209,0.099,-0.0175,-0.0041,-0.0256,-0.0153,0.043,-0.0009,-0.0724,-0.0432,-0.0292,-0.0182,0.0599,-0.036,0.0168,0.0542,-0.0179,0.0161,-0.0441,-0.0186,-0.0258,-0.0835,0.08,-0.0179,0.0105,-0.0359,-0.0537,0.0123,-0.0303,0.0294,0.0303,-0.0383,-0.0409,0.0273,-0.0607,-0.0364,0.0713,0.0105,0.0067,0.0524,0.0115,0.0275,0.0388,-0.0352,-0.0308,0.0755,0.0213,0.0419,0.0334,0.0364,0.0467,0.0598,-0.0195,-0.0324,-0.0336,-0.0542,0.0156,-0.0655,0.0265,0.0404,-0.0293,-0.2933,0.0477,0.0211,0.0544,-0.0499,-0.0055,0.0557,-0.0013,-0.0331,0.0113,-0.0248,0.0537,0.0456,0.039,0.0283,0.0419,0.0868,-0.0721,0.0188,-0.0583,0.0177,0.0756,0.2296,-0.0278,0.0203,0.048,-0.0301,0.0035,0.0105,-0.0226,-0.0127,-0.0158,0.0742,-0.0631,0.0445,0.041,-0.0417,0.0542,0.0576,-0.0354,0.019,0.0308,0.0034,-0.0335,0.069,0.0013,-0.015,-0.0226,0.0047,0.0365,-0.0015,0.0136,0.0211,0.0171,0.0436,0.0435,-0.033,-0.0768,-0.016,-0.0264,0.0103,-0.0581,-0.0227,-0.0601,-0.0302]}
{"key":"[Differentially Private Distributed Online Learning] Online learning has been in the spotlight from the machine learning society for a long time. To handle massive data in Big Data era, one single learner could never efficiently finish this heavy task. Hence, in this paper, we propose a novel distributed online learning algorithm to solve the problem. Comparing to typical centralized online learner, the distributed learners optimize their own learning parameters based on local data sources and timely communicate with neighbors. However, communication may lead to a privacy breach. Thus, we use differential privacy to preserve the privacy of learners, and study the influence of guaranteeing differential privacy on the utility of the distributed online learning algorithm. Furthermore, by using the results from Kakade and Tewari (2009), we use the regret bounds of online learning to achieve fast convergence rates for offline learning algorithms in distributed scenarios, which provides tighter utility performance than the existing state-of-the-art results. In simulation, we demonstrate that the differentially private offline learning algorithm has high variance, but we can use mini-batch to improve the performance. Finally, the simulations show that the analytical results of our proposed theorems are right and our private distributed online learning algorithm is a general framework.","layer":3,"vector":[-0.0255,-0.024,0.0084,-0.0209,0.0217,0.0163,0.0348,0.0193,0.0523,-0.0327,0.0479,-0.0053,0.0265,0.0542,-0.0018,0.0434,-0.0048,-0.0027,-0.0463,0.0335,0.0009,-0.0619,-0.0364,-0.0807,0.0058,-0.0071,-0.0821,-0.059,-0.0619,-0.2167,0.0298,-0.0844,0.0555,-0.0019,0.0139,0.0059,-0.0236,0.0627,-0.0671,0.0377,0.0097,0.0695,-0.0399,-0.0426,-0.0092,-0.0305,0.0003,-0.0113,0.0113,-0.0257,0.0037,-0.029,-0.0063,0.0561,0.027,0.0572,0.0434,0.0749,0.0013,0.0689,0.0363,0.0518,-0.1561,0.0314,0.012,0.0538,-0.0235,0.0157,0.0389,0.0223,0.0044,0.0503,0.0242,0.0342,0.026,0.0161,-0.0248,-0.0087,-0.0081,0.0214,0.0291,-0.0168,0.0048,-0.0191,-0.0451,-0.026,0.0218,-0.066,0.0612,-0.0045,-0.0153,0.0078,0.0037,0.0115,-0.0486,-0.0091,-0.0056,0.0358,-0.0427,0.1529,-0.0635,0.0381,0.0101,-0.0368,0.0196,-0.0258,-0.0188,-0.0064,0.0065,-0.0315,0.0105,-0.0243,0.0204,-0.0296,-0.0151,0.0185,0.059,0.0135,-0.0149,-0.0031,-0.0244,0.0515,0.0567,0.0158,0.0238,-0.0487,0.0002,0.1396,0.0102,0.0317,0.0309,-0.0609,-0.024,-0.0369,0.0467,0.0244,0.0319,0.0198,0.0606,-0.0332,-0.0645,-0.0428,0.0026,-0.0908,-0.0178,0.1418,0.0275,0.0848,-0.0098,-0.0455,0.025,0.0052,-0.0291,0.0106,0.039,-0.0293,0.0373,0.0599,-0.0727,0.0077,-0.0379,-0.0335,0.0149,0.1288,0.0095,-0.0884,-0.0098,0.0087,0.0305,-0.0264,0.0473,0.0215,-0.0439,0.0368,0.0695,0.0206,-0.0413,-0.0377,-0.0328,-0.0015,-0.0274,-0.0157,-0.0064,0.0273,0.0306,-0.037,0.0106,-0.0557,0.0198,0.045,-0.0342,0.0076,-0.0416,-0.0114,-0.0386,-0.0662,0.0149,-0.008,0.0393,-0.0002,-0.0087,-0.0295,-0.0656,0.002,-0.0441,0.0371,0.0302,-0.0073,0.0418,-0.0116,-0.0439,-0.014,0.02,-0.045,-0.0624,0.0304,0.0647,0.0525,-0.0278,0.0547,0.0256,-0.0291,-0.0453,-0.1829,-0.0375,0.0051,-0.0246,0.0499,-0.0582,0.0904,-0.0094,0.0643,0.077,0.1006,-0.026,-0.0188,0.027,-0.0322,0.0723,0.062,0.0477,-0.0063,0.0031,-0.0206,0.0285,-0.0088,-0.0996,0.0561,0.0122,0.2145,-0.0199,0.0527,-0.0588,0.0794,0.034,0.0225,-0.1523,0.0075,-0.0065,0.0093,-0.0275,-0.0522,-0.0362,0.0001,-0.0018,-0.0005,-0.1512,-0.0374,-0.043,-0.0351,0.025,-0.0962,0.0071,0.052,0.0029,0.0794,-0.041,0.0117,-0.0748,-0.0782,0.0265,-0.0578,0.0304,0.0034,-0.0131,-0.0286,-0.1087,0.064,-0.0299,-0.0277,-0.0496,0.051,-0.0466,0.0006,0.0748,0.0241,-0.0092,0.0256,0.0073,0.051,-0.0202,-0.0581,-0.0291,0.1011,-0.0325,0.0265,0.0196,0.0329,0.0081,0.1029,0.0262,-0.0093,-0.003,0.0143,0.0265,-0.0814,-0.043,0.0332,0.0403,-0.2933,0.0343,-0.0436,0.0334,-0.0283,0.0047,0.0476,-0.0085,-0.056,0.0018,0.0206,0.0675,0.0049,0.0063,0.0349,0.0302,0.075,-0.0183,0.0155,-0.0363,0.05,0.0251,0.2083,-0.0533,0.0487,-0.015,-0.0242,0.0212,0.0321,-0.049,-0.0258,-0.0019,0.0382,-0.0721,0.0177,0.0633,-0.0601,0.0358,0.0668,-0.025,-0.007,-0.0064,-0.0254,0.0136,0.0975,0.0364,-0.0416,-0.0593,-0.0324,0.0133,0.0064,-0.0023,-0.0295,-0.0013,-0.0059,0.0346,-0.0281,-0.0374,-0.051,-0.0448,0.0083,-0.0642,-0.0296,-0.02,0.0128]}
{"key":"[Style is a Distribution of Features] Neural style transfer (NST) is a powerful image generation technique that uses a convolutional neural network (CNN) to merge the content of one image with the style of another. Contemporary methods of NST use first or second order statistics of the CNN's features to achieve transfers with relatively little computational cost. However, these methods cannot fully extract the style from the CNN's features. We present a new algorithm for style transfer that fully extracts the style from the features by redefining the style loss as the Wasserstein distance between the distribution of features. Thus, we set a new standard in style transfer quality. In addition, we state two important interpretations of NST. The first is a re-emphasis from Li et al., which states that style is simply the distribution of features. The second states that NST is a type of generative adversarial network (GAN) problem.","layer":0,"vector":[0.0084,0.0081,0.0045,-0.025,0.0244,0.0682,0.0236,-0.0557,0.0039,-0.0245,0.0001,-0.0255,0.0535,0.0117,-0.0104,0.0107,0.0375,0.0406,-0.085,0.0157,0.0331,0.017,0.005,-0.0441,-0.009,-0.0157,-0.0061,-0.0565,-0.0643,-0.2505,0.0046,-0.0539,0.0251,-0.0569,-0.0135,-0.013,-0.0673,0.0841,-0.0528,0.0433,0.0227,0.0226,-0.0412,-0.0706,0.0015,-0.0047,-0.0313,0.0166,-0.0122,-0.0413,0.0182,-0.0433,-0.0078,0.0152,0.0477,0.0551,0.0761,0.0481,0.0326,0.07,-0.0002,0.0749,-0.1651,0.0532,0.0176,0.0082,-0.06,-0.0121,-0.0336,0.0393,-0.022,0.0274,-0.0031,0.0495,-0.006,-0.0097,0.0319,-0.0448,-0.063,0.0345,0.0358,0.0054,-0.0346,0.0284,0.0256,0.0136,0.0043,-0.0565,0.0126,-0.012,-0.0374,-0.019,-0.0399,0.0081,-0.0341,-0.0328,0.0262,0.0021,-0.0097,0.2099,-0.0484,0.0099,0.0724,-0.0179,0.0151,-0.0031,-0.0134,-0.0379,-0.047,0.0143,-0.0617,-0.0029,0.0236,-0.0236,0.034,-0.0082,0.0418,0.0501,0.0067,-0.0375,-0.0389,0.0001,0.0134,-0.022,0.0091,-0.0375,0.0141,0.106,0.0579,0.0331,0.017,0.005,-0.061,-0.0342,-0.0188,0.0193,0.0068,-0.0247,-0.0254,-0.0303,-0.0175,-0.0503,0.0063,-0.0431,-0.0033,0.103,-0.0181,0.0469,-0.0347,-0.0004,-0.0412,0.0242,-0.0209,-0.0274,0.0204,0.0629,0.029,0.0532,-0.0427,0.0471,0.0106,-0.0511,-0.0351,0.0635,0.0227,-0.1088,-0.0392,-0.0108,0.0237,-0.0254,0.0156,0.008,-0.034,0.0604,0.1028,0.0235,-0.0869,-0.0378,0.0165,0.0363,0.0745,-0.0766,-0.0202,0.0482,0.052,-0.0332,0.0247,-0.0553,-0.0016,0.0866,-0.0074,0.0306,-0.0311,0.0045,-0.0013,0.0122,-0.0474,0.0185,0.0103,-0.0053,-0.0142,0.0013,-0.064,-0.0148,-0.0352,0.0284,-0.0117,-0.0109,0.022,0.0571,-0.0301,0.0169,0.0443,0.0024,-0.0205,-0.044,0.0089,0.0481,0.0339,0.0109,0.0195,-0.047,-0.0143,-0.2665,0.0151,0.0507,-0.0576,0.0522,-0.0786,0.0597,0.0025,0.0535,0.0441,0.0593,-0.0029,-0.0066,0.0142,-0.0316,0.0617,0.0321,0.0521,-0.0463,-0.006,0.0001,0.034,-0.0063,-0.1129,0.0394,-0.0022,0.2227,0.041,0.0542,0.0066,0.0163,0.0362,-0.0127,-0.0895,0.0563,0.0111,0.0854,-0.0049,-0.0428,-0.0298,0.0026,0.0128,0.0346,-0.0949,0.0209,-0.0427,-0.0609,-0.0056,-0.0786,0.0176,0.0022,-0.0184,0.0446,0.0124,0.0072,-0.0203,-0.1345,0.0379,-0.021,-0.0076,-0.0239,-0.0488,0.0678,-0.0905,0.0243,0.0444,-0.0284,-0.0202,0.0434,-0.0072,0.0121,0.0681,0.027,0.0239,0.048,0.0054,0.0278,0.024,-0.0675,-0.0367,0.0294,-0.0192,0.0384,0.0361,0.0311,0.0198,0.055,-0.0668,0.0081,-0.0454,-0.0328,0.0068,-0.0649,-0.0218,0.0223,0.0002,-0.2887,0.044,0.0214,0.0774,-0.012,0.0401,0.0632,0.028,-0.0285,-0.0154,0.0086,0.0126,0.0392,-0.0603,0.0307,-0.0022,0.0631,-0.082,0.0548,-0.0472,0.0006,0.0175,0.2179,-0.0443,-0.0377,-0.0414,0.0008,0.0162,0.0437,-0.017,0.0487,0.0599,0.0997,-0.0256,-0.0176,0.0745,-0.0222,0.0249,-0.0266,-0.0439,0.0007,0.0454,-0.0423,0.038,0.086,-0.0197,-0.0154,-0.0028,-0.0112,0.023,-0.0343,0.0039,0.0076,-0.0182,0.0132,-0.0051,-0.076,-0.0244,-0.0152,-0.0013,0.0494,-0.066,-0.0362,0.0019,-0.0093]}
{"key":"[Generalized Federated Learning via Sharpness Aware Minimization] Federated Learning (FL) is a promising framework for performing privacy-preserving, distributed learning with a set of clients. However, the data distribution among clients often exhibits non-IID, i.e., distribution shift, which makes efficient optimization difficult. To tackle this problem, many FL algorithms focus on mitigating the effects of data heterogeneity across clients by increasing the performance of the global model. However, almost all algorithms leverage Empirical Risk Minimization (ERM) to be the local optimizer, which is easy to make the global model fall into a sharp valley and increase a large deviation of parts of local clients. Therefore, in this paper, we revisit the solutions to the distribution shift problem in FL with a focus on local learning generality. To this end, we propose a general, effective algorithm, \\texttt{FedSAM}, based on Sharpness Aware Minimization (SAM) local optimizer, and develop a momentum FL algorithm to bridge local and global models, \\texttt{MoFedSAM}. Theoretically, we show the convergence analysis of these two algorithms and demonstrate the generalization bound of \\texttt{FedSAM}. Empirically, our proposed algorithms substantially outperform existing FL studies and significantly decrease the learning deviation.","layer":0,"vector":[0.0186,-0.0572,0.0375,-0.0239,0.0392,0.0216,0.0139,0.035,0.0479,-0.0598,-0.0007,-0.0416,0.0081,0.0727,-0.0064,0.0443,-0.0058,0.0295,-0.049,-0.0273,0.0143,-0.0701,-0.0009,-0.0537,-0.0084,0.0492,-0.0173,-0.0434,-0.0754,-0.2242,0.0395,-0.033,0.0231,-0.0075,0.0367,-0.0475,-0.0302,0.0513,-0.059,0.0613,-0.0132,0.0229,-0.0565,-0.0269,-0.0452,-0.0476,0.0145,-0.0221,-0.0507,-0.0162,0.021,-0.0381,0.0021,0.0544,-0.0022,0.0446,0.0465,0.0602,0.0159,0.0554,-0.0182,0.054,-0.1344,0.0496,0.0368,0.0532,-0.0392,-0.007,0.0014,0.0114,0.004,0.0686,-0.0073,0.0323,-0.0112,-0.009,0.0069,0.0013,-0.0092,0.0318,0.0032,-0.0525,-0.0031,-0.0212,-0.025,-0.0456,0.024,-0.0994,0.0294,-0.0111,-0.0561,-0.003,0.0106,-0.0067,-0.0521,-0.0028,0.0185,0.0477,-0.0605,0.1929,-0.049,0.052,0.021,-0.0689,0.0472,-0.0557,-0.0324,-0.0293,0.0159,-0.0165,-0.0204,-0.0071,0.0129,-0.0652,0.0023,0.0219,0.0756,0.0674,-0.0139,-0.0055,-0.025,0.0105,0.0677,-0.0031,0.0462,-0.0486,-0.0048,0.1138,0.0442,0.0314,0.0082,-0.0437,-0.0209,-0.0285,0.0357,0.0725,-0.0022,-0.0236,0.0431,0.0257,-0.0071,-0.0424,-0.0249,-0.0781,-0.0281,0.1517,0.0059,0.0772,-0.0476,-0.0531,-0.0256,-0.0033,-0.0266,-0.0059,0.0406,0.0095,0.0373,0.037,-0.0411,-0.0206,-0.032,-0.029,0.0004,0.1552,0.0057,-0.1111,-0.0227,0.0136,0.025,-0.0353,0.0705,0.06,0.0023,0.0439,0.0549,0.0233,-0.043,-0.0207,-0.0076,-0.0082,0.0393,-0.0178,-0.0097,0.0234,0.0051,-0.0352,0.0546,-0.0721,-0.0142,0.0118,-0.0559,0.017,-0.0383,-0.0381,-0.0276,-0.0269,0.0025,-0.0412,0.0083,-0.0012,0.0019,0.0143,-0.0314,0.0121,-0.0422,0.0059,0.0073,0.0056,0.0172,0.0236,-0.0478,-0.0027,0.0415,-0.0383,-0.0341,0.0179,0.0046,0.0207,-0.0026,0.0462,0.0547,0.012,-0.0386,-0.1987,-0.028,0.0303,-0.046,0.0753,-0.0306,0.0632,-0.0021,0.0517,0.1038,0.0589,-0.0554,-0.0589,0.0512,-0.0122,0.0303,0.04,0.0505,-0.0086,0.009,0.0174,0.025,-0.0079,-0.0726,0.0458,0.0345,0.2111,0.0016,0.0047,-0.0614,-0.0126,0.0332,-0.0226,-0.118,0.0528,0.0234,0.0275,-0.0187,-0.0626,-0.0527,-0.0081,0.01,0.0287,-0.131,-0.0205,-0.0636,-0.0435,0.0428,-0.0808,0.0359,0.0352,-0.0434,0.0608,0.0221,0.0124,-0.0709,-0.0436,0.0444,-0.0226,0.0641,-0.0111,-0.0276,0.0064,-0.0916,0.0644,-0.0117,-0.0338,-0.0131,0.0336,-0.0776,-0.0128,0.0648,0.0135,0.0063,0.0396,0.0161,0.0378,-0.0277,-0.0834,-0.0587,0.1234,0.0085,0.066,-0.0102,-0.0151,0.0044,0.0978,0.0423,0.0233,-0.0213,0.0319,-0.0083,-0.0531,-0.0159,0.0593,-0.0005,-0.2922,0.0218,-0.0118,0.0442,-0.0394,0.0033,0.0908,-0.0025,-0.0228,0.0294,0.0484,0.0552,0.0395,0.0037,0.0341,0.021,0.0201,-0.0213,0.0303,-0.0865,0.0297,0.0391,0.2272,-0.0411,0.0156,0.0284,-0.0189,0.0188,0.0227,-0.0031,-0.0226,0.0023,0.0484,-0.0512,0.0567,0.0792,-0.001,0.0008,0.044,-0.0078,-0.004,-0.0219,-0.0102,-0.0106,0.0979,-0.0157,-0.0597,-0.0343,-0.0172,0.034,0.0064,-0.0401,0.0092,-0.0038,0.0101,-0.0191,-0.0344,-0.0315,-0.0314,-0.0451,-0.0121,-0.0477,-0.0733,-0.042,0.004]}
{"key":"[Image coding for machines: an end-to-end learned approach] Over recent years, deep learning-based computer vision systems have been applied to images at an ever-increasing pace, oftentimes representing the only type of consumption for those images. Given the dramatic explosion in the number of images generated per day, a question arises: how much better would an image codec targeting machine-consumption perform against state-of-the-art codecs targeting human-consumption? In this paper, we propose an image codec for machines which is neural network (NN) based and end-to-end learned. In particular, we propose a set of training strategies that address the delicate problem of balancing competing loss functions, such as computer vision task losses, image distortion losses, and rate loss. Our experimental results show that our NN-based codec outperforms the state-of-the-art Versa-tile Video Coding (VVC) standard on the object detection and instance segmentation tasks, achieving -37.87% and -32.90% of BD-rate gain, respectively, while being fast thanks to its compact size. To the best of our knowledge, this is the first end-to-end learned machine-targeted image codec.","layer":3,"vector":[-0.0282,-0.0187,0.0085,-0.0033,0.0481,0.0765,0.0259,0.0233,0.0032,0.0015,0.0183,-0.0662,0.0184,0.0623,0.0221,0.0127,0.0109,0.0543,-0.0099,-0.0112,0.0462,-0.067,-0.0336,-0.0388,-0.0194,-0.0014,-0.0107,-0.0183,-0.0379,-0.2003,0.0024,-0.0363,0.0818,-0.0238,0.0177,-0.0472,-0.0482,0.0213,-0.0944,0.0145,0.03,0.0128,-0.0559,-0.0535,-0.002,-0.0015,-0.0053,-0.062,0.0045,-0.0264,0.0631,-0.0101,0.0271,0.0445,-0.0405,0.0115,0.0128,0.1008,0.0595,0.0242,0.0402,0.0598,-0.1847,0.0333,0.0537,0.0467,-0.0266,0.0124,0.0087,0.0437,-0.0439,0.0356,0.0143,0.0418,-0.0221,0.0201,0.0262,-0.043,-0.0087,-0.0323,-0.0217,0.001,-0.0328,-0.0439,0.0093,-0.0375,-0.0137,-0.0171,0.0529,-0.0301,-0.0357,0.0222,-0.0736,0.0177,-0.0549,-0.0071,0.0158,0.0148,-0.042,0.1893,-0.0755,0.0337,0.0494,-0.0868,0.0545,-0.0439,-0.0218,0.0161,-0.0299,-0.0012,-0.0118,-0.0139,0.0151,-0.0212,0.0485,0.0108,0.0519,0.0548,0.0336,0.0417,-0.021,-0.0104,0.027,-0.0707,0.0,-0.0559,0.0129,0.1757,0.0632,-0.0096,0.0675,-0.0089,-0.0083,-0.0198,0.0341,0.0552,0.0617,0.0194,-0.0219,-0.0137,-0.038,-0.0503,0.0436,-0.049,-0.0433,0.0703,-0.0376,0.0143,-0.0305,-0.0681,0.0264,0.0302,-0.0538,-0.0303,-0.0029,0.0452,0.0653,0.0581,-0.0599,-0.0146,0.0055,-0.0597,-0.0621,0.085,0.0076,-0.0633,-0.0281,-0.0345,0.0358,-0.0247,0.0173,0.0023,-0.011,-0.0292,0.0875,0.0334,-0.0972,0.0032,0.0145,0.0284,0.0003,-0.0717,0.0006,0.0207,0.0573,-0.0274,0.0397,-0.0616,0.0107,0.049,-0.0272,0.0505,-0.0239,0.0065,0.0092,-0.0199,-0.0248,-0.0084,-0.0318,-0.0305,-0.0163,-0.0173,-0.007,0.0644,-0.009,0.0108,-0.0404,0.0182,-0.0089,0.0453,-0.0485,-0.006,0.0889,-0.0199,-0.0308,-0.0074,0.0372,0.0501,0.0351,0.0181,0.0424,-0.0822,-0.0518,-0.2071,0.0166,0.03,-0.044,-0.0044,-0.0495,0.0211,-0.0007,0.0274,0.008,0.0744,-0.037,-0.0018,0.0119,-0.0001,0.0527,0.0458,0.0514,-0.018,-0.0196,-0.02,0.0337,0.0169,-0.12,0.0249,-0.0079,0.2293,0.0004,0.065,-0.018,0.0086,0.0454,-0.026,-0.1038,0.0535,0.0087,0.0661,0.0023,-0.0407,-0.0184,-0.03,-0.0099,-0.004,-0.1055,-0.041,-0.0291,-0.0469,0.0372,-0.0449,-0.0216,0.0243,-0.0842,0.0401,0.0176,-0.0143,-0.0382,-0.0372,-0.0083,-0.0327,0.0563,0.0037,-0.0786,0.0341,-0.0717,0.0797,-0.0024,0.0071,-0.021,0.0222,0.0026,-0.0147,0.1094,0.015,-0.0313,0.0802,0.0039,0.0218,-0.0159,-0.0396,-0.0434,0.0755,-0.0213,0.0337,0.0193,0.0739,0.0183,0.0221,0.0042,0.0327,-0.0166,0.0035,-0.0165,-0.0572,-0.0126,0.0377,-0.0234,-0.3143,0.0453,0.0065,0.0368,-0.0239,0.0236,0.0732,-0.0041,-0.0192,0.0043,-0.015,0.0105,0.0426,-0.0329,0.0294,0.0473,0.0687,-0.013,0.0912,-0.0338,0.0206,0.0153,0.1834,-0.052,0.0036,-0.0121,-0.032,0.0062,0.0541,-0.0261,0.0235,0.0413,0.0997,-0.07,0.0049,0.1007,-0.0438,0.0232,-0.0072,-0.0101,0.0186,0.0189,-0.0234,-0.0212,0.0667,0.0428,-0.0481,-0.0232,-0.0147,-0.011,-0.026,-0.0051,-0.0249,-0.0074,0.0333,0.0064,-0.111,-0.0392,-0.0406,-0.0103,0.0828,-0.0834,-0.025,-0.0048,-0.0347]}
{"key":"[SWIPENET: Object detection in noisy underwater images] In recent years, deep learning based object detection methods have achieved promising performance in controlled environments. However, these methods lack sufficient capabilities to handle underwater object detection due to these challenges: (1) images in the underwater datasets and real applications are blurry whilst accompanying severe noise that confuses the detectors and (2) objects in real applications are usually small. In this paper, we propose a novel Sample-WeIghted hyPEr Network (SWIPENET), and a robust training paradigm named Curriculum Multi-Class Adaboost (CMA), to address these two problems at the same time. Firstly, the backbone of SWIPENET produces multiple high resolution and semantic-rich Hyper Feature Maps, which significantly improve small object detection. Secondly, a novel sample-weighted detection loss function is designed for SWIPENET, which focuses on learning high weight samples and ignore learning low weight samples. Moreover, inspired by the human education process that drives the learning from easy to hard concepts, we here propose the CMA training paradigm that first trains a clean detector which is free from the influence of noisy data. Then, based on the clean detector, multiple detectors focusing on learning diverse noisy data are trained and incorporated into a unified deep ensemble of strong noise immunity. Experiments on two underwater robot picking contest datasets (URPC2017 and URPC2018) show that the proposed SWIPENET+CMA framework achieves better accuracy in object detection against several state-of-the-art approaches.","layer":0,"vector":[-0.0285,-0.0096,0.0238,-0.0253,0.0262,0.0097,0.0499,0.0101,0.0216,-0.029,0.0393,-0.0881,0.0258,0.0886,-0.005,0.0273,0.0553,0.0434,-0.0268,0.0269,0.05,-0.004,-0.0173,-0.0798,0.0035,0.0155,-0.0113,-0.0296,-0.0574,-0.2385,-0.0001,-0.0544,0.0696,-0.0232,0.0444,-0.0096,-0.0208,-0.003,-0.0337,0.0223,-0.0055,0.039,-0.0024,-0.0772,0.009,-0.0701,0.0003,-0.0397,0.0074,-0.0371,0.0323,-0.0652,0.0011,0.0327,0.0419,0.0116,0.0401,0.0344,0.0555,0.0347,0.0551,0.0742,-0.1746,0.0793,0.027,0.0243,-0.0669,-0.0421,0.0481,-0.0164,-0.0179,0.0513,0.0346,0.0565,0.0208,-0.0131,0.0172,-0.0284,0.0219,0.0049,-0.0049,-0.0394,-0.0098,-0.0166,0.0252,-0.0565,0.0028,-0.0803,0.0422,0.0014,-0.0548,-0.0245,-0.0128,0.0263,-0.0642,-0.043,0.0449,0.0458,-0.0683,0.2186,-0.0342,0.03,0.0093,-0.0251,0.0564,-0.0096,-0.0465,-0.0516,0.0032,0.0282,-0.0021,-0.0474,0.0104,-0.0267,-0.0276,0.0218,0.0656,0.0191,-0.0368,-0.0087,-0.035,-0.0723,0.0683,-0.0314,0.006,-0.0661,0.0535,0.1365,0.043,0.0034,0.0411,-0.0394,-0.0985,-0.0209,0.051,0.0066,0.0149,-0.014,0.008,0.0219,-0.0582,-0.0366,0.0543,-0.0853,-0.0333,0.071,-0.0502,0.0317,-0.0448,-0.0417,-0.0241,0.058,-0.0115,-0.0249,-0.0022,0.0373,0.0511,0.0509,-0.0431,0.0267,-0.0339,-0.0585,-0.0361,0.0491,0.0297,-0.0946,-0.0272,-0.0172,-0.0117,0.0017,0.003,0.0406,0.0077,0.0208,0.0882,-0.0031,-0.0741,0.0257,0.0036,-0.0021,-0.0442,-0.0391,-0.0522,0.0362,0.0833,-0.0295,0.0106,-0.0499,0.0319,0.0614,-0.0381,-0.0156,-0.0285,-0.0261,0.0087,-0.0252,-0.0006,-0.012,-0.0021,-0.0216,-0.0133,-0.0303,-0.0148,0.011,-0.0027,0.018,0.0252,0.0021,0.007,0.0368,-0.0096,0.0112,0.0397,-0.0354,-0.003,-0.0838,0.044,0.0548,-0.0285,-0.0127,0.0454,-0.0312,-0.0391,-0.2583,0.0066,0.0215,-0.0072,0.0427,-0.0468,0.0366,0.0094,0.0611,0.0648,0.0539,-0.0267,-0.025,0.0355,0.0107,0.0913,0.0235,0.0494,-0.0043,-0.0006,-0.0188,0.0326,-0.0087,-0.0436,0.0616,0.0117,0.1979,0.0568,0.0278,-0.0133,0.0138,0.038,0.0005,-0.0863,0.0134,0.0687,0.0599,-0.0076,-0.0622,-0.0433,-0.0509,0.0394,-0.0227,-0.1272,-0.0297,-0.054,-0.0463,0.0356,-0.0334,0.0033,0.0618,-0.0348,0.0125,-0.0052,-0.0296,0.0229,-0.0706,0.0127,-0.0894,0.0312,0.0187,-0.0372,0.0417,-0.0608,0.0592,0.0236,-0.0552,-0.0508,0.0319,-0.0428,-0.027,0.0669,0.0277,0.0292,0.0652,-0.0027,0.0512,-0.0521,-0.0156,-0.0578,0.0807,-0.0109,-0.0364,0.0192,0.0649,-0.0096,0.102,0.005,-0.0056,0.0165,0.045,0.0168,-0.0339,0.0108,0.0323,0.0188,-0.2812,-0.0185,0.0223,0.0855,-0.0256,0.0423,0.0556,0.0091,-0.0143,-0.0147,-0.0332,0.0491,0.0218,0.0136,-0.0054,0.041,0.0414,-0.0411,0.0468,-0.0095,-0.0455,0.0502,0.1812,-0.0142,-0.005,0.003,-0.0271,-0.0276,0.0102,-0.0307,0.0114,0.0654,0.044,-0.0725,-0.0083,0.1313,-0.0197,0.0123,0.011,-0.0676,-0.0051,-0.009,-0.0394,-0.0355,0.089,-0.0326,0.004,-0.0355,0.0349,0.0366,-0.0267,-0.0242,-0.0207,-0.0482,0.0481,0.0259,-0.0328,-0.0514,-0.0422,-0.0209,0.0381,0.0019,-0.0301,0.0132,0.0013]}
{"key":"[Local Canonical Correlation Analysis for Nonlinear Common Variables Discovery] In this paper, we address the problem of hidden common variables discovery from multimodal data sets of nonlinear high-dimensional observations. We present a metric based on local applications of canonical correlation analysis (CCA) and incorporate it in a kernel-based manifold learning technique.We show that this metric discovers the hidden common variables underlying the multimodal observations by estimating the Euclidean distance between them. Our approach can be viewed both as an extension of CCA to a nonlinear setting as well as an extension of manifold learning to multiple data sets. Experimental results show that our method indeed discovers the common variables underlying high-dimensional nonlinear observations without assuming prior rigid model assumptions.","layer":2,"vector":[-0.0323,-0.0435,0.0269,0.0228,0.0407,0.0148,0.0434,0.0409,0.0155,-0.0407,0.0231,-0.0763,0.0017,0.0554,0.0156,-0.0073,0.0265,0.0837,-0.0769,0.0212,0.055,-0.0131,0.007,-0.0326,0.002,-0.006,0.0014,-0.0454,-0.0066,-0.2473,0.0143,-0.0272,0.0708,-0.011,0.0273,-0.0164,-0.0323,0.0774,-0.0253,0.0481,-0.0122,0.0254,-0.0013,-0.0332,-0.064,-0.0838,0.0143,-0.0046,0.004,-0.0052,0.0168,-0.0329,0.0047,0.03,0.0338,0.0485,0.056,0.0208,0.0767,0.0454,0.0324,0.0473,-0.1733,0.0657,0.0496,0.0027,-0.0064,-0.026,0.049,0.0299,0.0074,0.0414,0.0178,0.031,0.0042,-0.0162,0.021,-0.0178,-0.0596,0.0405,0.0587,0.0256,-0.0366,-0.0238,-0.0173,-0.0381,0.0153,-0.0415,0.0273,0.0005,-0.0946,-0.0477,-0.0355,0.0043,-0.088,-0.0022,0.0539,0.0277,-0.0133,0.1889,-0.0754,0.0376,0.0202,-0.0288,0.0106,-0.0661,-0.0217,-0.0433,0.0399,0.0446,0.0232,-0.0359,-0.0251,-0.0467,0.0434,-0.0258,0.0573,0.0734,-0.0181,-0.0326,-0.0072,0.0203,0.0568,-0.0091,0.0137,-0.0602,0.0377,0.1155,0.049,-0.0218,0.0401,0.04,-0.0093,-0.0234,0.0216,0.0312,0.0504,0.0486,0.0359,0.037,-0.0367,-0.058,0.0353,-0.0547,-0.0637,0.1529,-0.0408,0.0007,-0.0882,-0.0197,-0.0098,0.0288,-0.0202,-0.0345,0.045,0.0279,0.0373,0.0326,-0.0364,0.0519,-0.061,-0.0664,-0.0193,0.094,0.0043,-0.0856,-0.0244,0.0061,0.0679,-0.0001,0.0384,0.0362,-0.0994,0.0017,0.0836,0.0182,-0.087,0.0299,0.0282,0.0068,0.0018,-0.0684,-0.0422,0.0591,0.042,-0.0336,0.0063,-0.0434,0.0148,-0.0088,-0.0021,-0.0261,-0.0131,-0.0033,-0.0523,-0.0154,0.0113,-0.0302,0.0032,-0.0119,0.0103,-0.0334,-0.0342,0.0387,-0.0365,0.0149,0.0084,-0.0123,0.0323,0.0186,-0.0094,-0.0254,0.0429,-0.0347,-0.033,-0.0407,-0.0233,0.0594,-0.0071,0.0385,0.0382,-0.0405,-0.0555,-0.252,-0.007,-0.0062,0.0079,0.0628,-0.0389,0.0406,-0.0065,0.0478,0.0723,0.0323,0.0073,-0.0602,0.035,0.0008,0.0742,0.0234,0.0535,-0.0567,0.0214,-0.0212,0.0093,-0.0256,-0.094,0.055,-0.0263,0.1946,0.0164,-0.0036,-0.0753,0.0269,0.0121,-0.0315,-0.0556,0.0498,0.0278,0.0501,0.0042,-0.053,-0.0327,-0.037,-0.0093,0.0297,-0.0498,-0.0066,-0.0279,-0.0152,-0.0018,-0.0297,0.0059,0.0499,0.011,0.0514,-0.0175,-0.022,-0.0485,-0.0881,-0.0306,-0.0288,0.0472,-0.0108,-0.0477,0.0238,-0.0802,0.0777,-0.0504,-0.0517,-0.0528,0.0362,-0.0189,-0.0694,0.0771,0.0178,-0.0154,0.0501,0.003,0.0538,0.0118,-0.0399,-0.0101,0.0908,-0.0178,0.0211,0.0025,0.0066,-0.0295,0.1067,-0.0239,0.0245,-0.0686,0.0062,-0.0048,-0.0202,-0.0422,0.0415,0.0472,-0.3147,0.0077,0.008,0.0153,0.0088,0.0102,0.031,-0.0073,-0.002,-0.0092,0.0447,0.0217,0.0255,-0.0196,0.0246,0.0402,0.0456,-0.0505,0.0521,-0.0644,0.0019,0.0402,0.2016,-0.0207,0.0181,-0.0084,-0.0083,-0.0138,0.0254,-0.0061,-0.0012,0.0061,0.0558,-0.0489,0.0395,0.0756,-0.0579,0.0217,0.01,-0.0364,0.0464,0.0274,-0.033,-0.0662,0.0937,-0.0102,-0.0178,-0.0558,0.0253,0.019,0.019,-0.0019,-0.0019,0.0243,0.0047,0.0227,-0.0188,-0.044,-0.0234,-0.0509,-0.0421,-0.0468,-0.0524,-0.025,-0.0105]}
{"key":"[Persistent Monitoring of Stochastic Spatio-temporal Phenomena with a Small Team of Robots] This paper presents a solution for persistent monitoring of real-world stochastic phenomena, where the underlying covariance structure changes sharply across time, using a small number of mobile robot sensors. We propose an adaptive solution for the problem where stochastic real-world dynamics are modeled as a Gaussian Process (GP). The belief on the underlying covariance structure is learned from recently observed dynamics as a Gaussian Mixture (GM) in the low-dimensional hyper-parameters space of the GP and adapted across time using Sequential Monte Carlo methods. Each robot samples a belief point from the GM and locally optimizes a set of informative regions by greedy maximization of the submodular entropy function. The key contributions of this paper are threefold: adapting the belief on the covariance using Markov Chain Monte Carlo (MCMC) sampling such that particles survive even under sharp covariance changes across time; exploiting the belief to transform the problem of entropy maximization into a decentralized one; and developing an approximation algorithm to maximize entropy on a set of informative regions in the continuous space. We illustrate the application of the proposed solution through extensive simulations using an artificial dataset and multiple real datasets from fixed sensor deployments, and compare it to three competing state-of-the-art approaches.","layer":3,"vector":[-0.0451,-0.0313,0.0543,-0.0169,0.0397,0.0117,0.0596,0.0247,0.0426,-0.0152,0.0471,-0.021,0.045,0.0925,-0.0088,0.0162,-0.0366,0.0467,-0.0176,-0.0581,0.0266,-0.0589,0.0066,-0.019,0.0009,0.0608,-0.0429,-0.0378,-0.057,-0.2361,0.0122,-0.085,0.0378,-0.0177,0.0201,-0.0076,-0.0183,0.0744,0.0099,0.0581,-0.0264,0.0191,-0.0293,-0.0599,-0.0259,-0.0474,0.0098,-0.001,-0.0539,-0.0411,0.0054,-0.0341,0.0063,-0.006,0.048,0.0701,0.0539,0.0228,0.0681,-0.0166,0.0178,0.0637,-0.1548,0.074,0.0409,0.0514,-0.0566,-0.0221,0.0225,-0.0025,-0.033,0.0805,-0.0046,0.0761,0.0157,-0.0358,-0.0076,-0.0316,-0.016,0.0513,-0.003,-0.0063,-0.0172,-0.0008,-0.0676,-0.0628,0.0524,-0.0249,0.0707,-0.0029,-0.0603,0.0021,-0.0101,0.0159,-0.0685,-0.0022,0.0421,0.0469,0.0174,0.1865,-0.0292,0.0319,0.0606,0.0132,0.0342,-0.047,-0.0473,-0.0596,-0.0089,0.0424,0.0172,-0.0151,0.0321,-0.0351,0.0006,-0.0058,0.0364,0.0381,-0.0019,-0.0181,0.0276,0.0143,0.0831,-0.0211,0.0323,-0.0715,0.0067,0.154,0.0318,0.0209,0.0515,-0.0056,-0.0736,-0.0298,0.051,0.0082,0.0179,-0.0004,0.0457,0.0321,-0.0233,-0.0346,0.0155,-0.109,-0.0373,0.1116,-0.0243,0.0365,-0.0595,-0.035,-0.0187,0.0023,0.0249,-0.0348,0.0165,0.0569,0.0116,0.033,-0.0656,0.0356,-0.0581,-0.0215,0.0151,0.0922,0.0072,-0.0798,0.0044,0.0152,0.023,-0.0202,0.0341,0.0187,-0.0244,-0.0121,0.0483,0.0339,-0.0936,0.0533,0.0227,0.0018,0.0124,-0.0402,-0.0165,0.0467,0.0254,-0.0364,0.0049,-0.0251,0.0202,0.0413,-0.0061,-0.0192,0.0342,-0.0175,-0.0328,0.0113,-0.0203,-0.0123,0.0292,-0.0408,-0.0017,-0.0344,-0.0882,0.0165,0.0162,0.0213,-0.0596,-0.0041,0.0297,0.035,0.0202,0.0233,0.0428,-0.0372,-0.0066,-0.0012,-0.0462,0.0281,-0.0139,-0.0185,0.0356,-0.0048,-0.059,-0.2531,-0.0047,0.0032,0.0309,0.0398,-0.0544,0.0209,-0.0291,0.0703,0.0228,0.0527,-0.0249,-0.0206,-0.0073,-0.0227,0.0429,0.0313,0.0299,-0.0442,0.0263,0.014,0.0147,-0.039,-0.0946,0.0584,-0.0485,0.2173,0.0054,0.0373,-0.0217,-0.025,0.0,-0.0033,-0.0862,0.0349,0.0344,0.0558,0.0011,-0.0135,-0.0429,-0.0292,0.0244,-0.0193,-0.0836,-0.0819,-0.0733,-0.0021,0.0473,-0.0781,-0.0509,0.0236,-0.017,0.0694,-0.0657,-0.0333,-0.0298,-0.0325,0.0043,-0.0275,0.0272,0.008,-0.0227,-0.0136,-0.0091,0.0891,-0.0576,0.0284,-0.0719,-0.0027,0.0023,-0.0033,0.0964,0.0414,-0.0181,0.0788,-0.0036,0.0414,-0.0321,-0.0773,-0.0074,0.0851,-0.0362,0.0185,0.0819,0.0431,-0.0202,0.0631,-0.022,0.0068,0.0152,0.0417,-0.0004,-0.0242,-0.0011,0.0132,0.0063,-0.2945,0.0435,0.0023,0.0121,-0.0575,-0.0054,0.011,0.0343,-0.0403,0.0159,-0.0139,0.0818,0.0486,0.0222,0.0218,0.0455,-0.0008,-0.0457,0.0208,-0.0897,0.0316,0.0446,0.2125,-0.0152,0.0271,-0.0,-0.013,0.0399,0.0252,-0.0599,-0.0058,0.0084,0.0324,-0.0599,0.0362,0.0634,-0.0219,0.0553,0.0085,-0.0194,0.0033,0.0147,0.0057,-0.0258,0.1167,-0.0287,-0.0667,-0.0667,-0.0049,0.0327,-0.0293,-0.0207,-0.0339,-0.0226,0.0001,0.0342,-0.0182,-0.0445,-0.0348,-0.0548,0.0039,-0.0599,0.0043,-0.0412,-0.0286]}
{"key":"[Diversity Regularized Interests Modeling for Recommender Systems] With the rapid development of E-commerce and the increase in the quantity of items, users are presented with more items hence their interests broaden. It is increasingly difficult to model user intentions with traditional methods, which model the user's preference for an item by combining a single user vector and an item vector. Recently, some methods are proposed to generate multiple user interest vectors and achieve better performance compared to traditional methods. However, empirical studies demonstrate that vectors generated from these multi-interests methods are sometimes homogeneous, which may lead to sub-optimal performance. In this paper, we propose a novel method of Diversity Regularized Interests Modeling (DRIM) for Recommender Systems. We apply a capsule network in a multi-interest extractor to generate multiple user interest vectors. Each interest of the user should have a certain degree of distinction, thus we introduce three strategies as the diversity regularized separator to separate multiple user interest vectors. Experimental results on public and industrial data sets demonstrate the ability of the model to capture different interests of a user and the superior performance of the proposed approach.","layer":0,"vector":[-0.0353,0.0296,0.0082,-0.0545,0.0022,0.0013,0.0609,0.0319,0.0146,-0.0282,0.0012,-0.0352,0.0094,0.0334,0.0595,-0.0166,0.0289,-0.0102,-0.0773,-0.0015,0.0099,-0.0535,-0.0088,-0.1003,0.0221,0.0136,-0.0262,-0.0671,-0.0511,-0.2163,-0.016,-0.0513,0.1277,-0.0046,0.0265,-0.0225,0.0036,0.0221,-0.0457,0.0428,0.0081,-0.0109,0.0032,-0.0383,-0.0427,0.0079,-0.0254,0.0071,-0.0283,-0.0454,0.0488,-0.0361,0.0259,0.0191,0.0183,0.0307,0.0333,0.0362,0.0197,0.0381,0.0616,0.0164,-0.1676,0.0247,0.0303,0.0199,-0.0324,0.0122,-0.0102,0.0204,0.0039,0.0393,-0.0158,0.0472,-0.0189,0.0051,0.0366,0.0029,0.0023,0.0682,-0.0419,-0.041,-0.0292,-0.0391,0.064,-0.0282,-0.0136,-0.033,-0.0037,-0.0217,-0.0417,-0.0211,0.0262,0.0421,-0.0652,-0.0206,0.0096,0.0068,-0.0632,0.2247,-0.0408,0.0267,0.0693,-0.0418,0.0041,0.0038,-0.03,-0.0315,0.0215,0.0039,-0.0058,0.0195,0.0406,-0.0334,-0.0049,0.033,0.0633,0.0479,0.0381,-0.0665,-0.0483,0.0305,0.0473,0.0046,0.0374,-0.0217,0.0185,0.1407,0.0054,0.048,0.0487,-0.0046,-0.0945,-0.0437,0.0452,0.0092,0.0188,-0.0337,0.0105,0.0175,-0.0216,-0.0253,0.0047,-0.1006,-0.0271,0.1373,-0.0225,-0.0161,-0.0728,-0.0131,-0.0083,-0.0151,-0.0105,0.013,0.0423,0.0466,0.087,0.0413,-0.0714,0.01,-0.0093,-0.0608,-0.0352,0.1111,-0.0292,-0.0767,-0.0433,0.016,-0.0059,-0.0276,0.0286,0.0245,0.0197,0.0094,0.0911,0.0431,-0.0593,-0.0181,0.0544,-0.0257,0.0363,-0.0326,-0.0265,0.0523,-0.0002,-0.0351,-0.008,-0.012,0.0098,0.0409,-0.0631,0.0131,0.0003,0.0338,0.0189,-0.0145,-0.0129,-0.0058,0.0088,-0.0725,0.0357,0.0262,-0.0755,0.0282,-0.0251,0.0651,-0.0008,-0.0585,0.0572,-0.0131,-0.0736,0.0192,0.0515,-0.0262,-0.0231,0.0308,0.0531,0.0348,0.0582,0.0318,0.0532,-0.0444,-0.0076,-0.2343,-0.0048,-0.0192,0.017,0.0519,-0.0541,0.035,-0.0012,0.0665,0.0796,0.0578,-0.0138,-0.018,0.0559,0.0402,0.0915,0.0263,0.0557,-0.0138,-0.0201,0.0091,0.0322,0.0064,-0.0747,0.047,-0.018,0.2194,0.058,0.0069,-0.0349,0.0688,0.0392,-0.0492,-0.0896,0.0273,0.0323,0.0548,-0.0206,-0.0171,-0.0331,-0.025,0.0256,0.0029,-0.0701,-0.0566,-0.0476,-0.0143,0.0407,-0.0376,0.0582,0.0384,-0.041,0.044,-0.0127,-0.03,-0.0152,-0.0844,0.0065,-0.042,0.0215,-0.0233,-0.0546,0.0176,-0.0711,0.0429,0.0237,0.0018,0.0084,-0.008,-0.0427,-0.0283,0.0567,-0.02,0.0203,0.0287,-0.008,0.021,0.0136,-0.0345,-0.0443,0.0363,-0.0333,0.0147,0.0088,0.0056,-0.0484,0.0587,0.0022,0.0163,-0.0377,-0.0031,-0.0289,-0.0769,-0.0089,0.0441,-0.0379,-0.3078,0.0304,-0.0085,0.0473,-0.0061,0.0032,0.0578,-0.0086,-0.0129,-0.0076,0.0085,0.0253,0.042,-0.0617,-0.0016,0.026,0.0507,0.0018,0.0073,-0.0273,0.0405,0.0038,0.2324,-0.0124,0.0163,0.0065,0.0228,-0.0378,0.0071,0.0034,0.0165,-0.0193,0.1247,-0.0463,0.0161,0.0928,-0.0152,0.0135,0.021,-0.0519,-0.0425,-0.053,-0.0566,-0.0178,0.0766,-0.0029,-0.0043,-0.0361,-0.0159,0.0505,-0.035,-0.0124,-0.0553,-0.0078,0.0261,0.0566,-0.0612,-0.0347,-0.0363,-0.0589,-0.0283,-0.0603,-0.0077,-0.0225,0.0359]}
{"key":"[Quilting Stochastic Kronecker Product Graphs to Generate Multiplicative Attribute Graphs] We describe the first sub-quadratic sampling algorithm for the Multiplicative Attribute Graph Model (MAGM) of Kim and Leskovec (2010). We exploit the close connection between MAGM and the Kronecker Product Graph Model (KPGM) of Leskovec et al. (2010), and show that to sample a graph from a MAGM it suffices to sample small number of KPGM graphs and \\emph{quilt} them together. Under a restricted set of technical conditions our algorithm runs in $O((\\log_2(n))^3 |E|)$ time, where $n$ is the number of nodes and $|E|$ is the number of edges in the sampled graph. We demonstrate the scalability of our algorithm via extensive empirical evaluation; we can sample a MAGM graph with 8 million nodes and 20 billion edges in under 6 hours.","layer":1,"vector":[-0.0523,-0.0199,0.0148,-0.0444,0.0147,0.0333,0.0091,0.0536,0.0172,0.0005,0.0262,-0.047,0.0458,0.0432,0.0314,0.0327,0.0095,-0.0045,-0.0211,-0.0073,0.0587,-0.0749,0.011,-0.0588,0.0797,0.033,-0.0279,-0.0847,-0.0307,-0.2657,0.0068,0.017,0.0913,-0.0367,0.0039,-0.0453,-0.0248,0.0721,-0.0345,0.0252,0.0306,0.0642,-0.0351,-0.0229,-0.0124,-0.0465,-0.0227,-0.0361,-0.0258,-0.0336,0.0202,-0.0694,0.031,0.0013,0.0387,0.0357,0.0213,0.0206,0.017,0.0657,-0.0048,0.052,-0.1459,0.0496,0.036,0.0331,-0.0731,-0.0235,0.0028,0.0687,-0.0027,0.0415,0.0157,0.0557,0.0602,0.0007,-0.0128,-0.0376,-0.0381,0.0371,-0.0167,-0.0338,-0.0507,0.0064,-0.0059,0.003,-0.0017,-0.0453,0.0641,0.0147,-0.0152,0.0053,-0.0153,0.002,-0.0678,-0.0351,0.0302,0.0127,-0.0117,0.2261,-0.0415,0.0243,0.0408,-0.0231,-0.0384,-0.049,-0.0183,-0.0365,-0.0432,0.0377,0.0236,-0.0216,0.0481,-0.086,0.0293,-0.0157,0.0473,0.0424,0.008,-0.0105,-0.0312,0.0386,0.0469,-0.0217,0.0265,-0.0555,0.0063,0.1296,0.0518,0.0395,-0.0039,0.0121,-0.0238,-0.0263,-0.0142,-0.0171,-0.0167,0.0283,0.0158,-0.004,-0.0383,-0.0376,0.0237,-0.0843,-0.0574,0.122,-0.0107,0.0222,-0.0326,-0.0364,-0.0045,0.0326,0.0279,-0.0485,0.026,0.0296,0.0297,0.0373,-0.0401,0.0633,-0.037,-0.0389,-0.0009,0.1049,0.0116,-0.093,-0.018,0.0291,-0.0148,-0.0158,0.0697,0.0533,-0.0748,0.0157,0.0772,0.0353,-0.0906,-0.0271,0.0303,-0.0106,0.0459,-0.0018,-0.0683,0.045,-0.0011,-0.0713,0.0055,-0.0183,-0.0172,0.0184,-0.0474,0.0184,-0.0139,0.001,-0.0607,-0.0111,-0.0235,-0.0279,0.0117,-0.0245,0.0061,-0.0393,-0.0528,0.0157,0.0004,0.0268,0.0065,-0.0006,0.0138,0.0302,-0.0219,0.0006,0.0247,0.0164,-0.0014,0.0351,0.0574,0.0454,0.0276,0.0566,0.032,-0.061,-0.0461,-0.2087,-0.0555,0.002,0.0329,0.0788,-0.036,0.0,-0.0136,0.057,0.1011,0.0509,-0.0228,-0.0521,0.0046,-0.0261,0.0546,0.0175,0.0329,-0.0023,0.0166,-0.0253,0.0199,-0.0084,-0.0491,0.0301,-0.006,0.2504,0.0079,-0.0374,0.0073,0.0004,0.04,-0.0478,-0.0368,0.0943,0.0797,0.0312,0.0059,-0.0147,-0.0277,-0.0594,0.0024,-0.0179,-0.0936,-0.0332,-0.0198,-0.0475,0.0244,-0.0553,0.0168,0.0169,0.0165,0.0861,-0.0176,0.0127,-0.0786,-0.0426,0.0332,-0.0132,0.0279,0.0179,-0.0871,0.0001,-0.0239,0.0663,-0.0153,-0.0171,-0.0548,-0.0181,-0.0301,-0.0,0.0505,0.043,-0.0216,0.0364,0.0126,0.0259,-0.0027,-0.0437,-0.0369,0.0469,-0.0765,-0.014,0.0162,-0.014,0.0074,0.0178,0.0112,0.0144,-0.0185,0.0015,0.0031,-0.0444,-0.0145,0.0546,-0.0024,-0.2915,0.0594,0.0424,0.0617,-0.0883,0.0097,0.0174,0.0172,-0.0238,-0.0165,0.0365,0.0907,0.0204,-0.0356,-0.0168,0.0483,0.0714,-0.0374,0.0593,-0.0563,0.0375,-0.0165,0.2059,-0.0043,0.03,0.0125,-0.0153,0.025,0.0484,0.0094,0.0303,-0.0028,0.1305,-0.0605,0.024,0.0382,-0.0708,0.0174,0.005,-0.037,-0.0323,-0.0584,-0.0505,-0.0178,0.0787,-0.0168,-0.0729,-0.0692,0.046,0.015,-0.0179,0.0264,-0.0451,-0.012,0.0153,0.0327,-0.0081,-0.0194,-0.0327,-0.0483,-0.0247,-0.047,-0.0138,0.0182,-0.0206]}
{"key":"[Subspace Inference for Bayesian Deep Learning] Bayesian inference was once a gold standard for learning with neural networks, providing accurate full predictive distributions and well calibrated uncertainty. However, scaling Bayesian inference techniques to deep neural networks is challenging due to the high dimensionality of the parameter space. In this paper, we construct low-dimensional subspaces of parameter space, such as the first principal components of the stochastic gradient descent (SGD) trajectory, which contain diverse sets of high performing models. In these subspaces, we are able to apply elliptical slice sampling and variational inference, which struggle in the full parameter space. We show that Bayesian model averaging over the induced posterior in these subspaces produces accurate predictions and well calibrated predictive uncertainty for both regression and image classification.","layer":2,"vector":[-0.017,-0.0304,0.0246,-0.0224,0.0182,0.048,0.0126,0.0336,0.0803,0.0295,0.0396,-0.0506,-0.0035,0.0686,0.0398,0.0067,-0.0029,0.0534,-0.0525,-0.0372,0.0362,-0.0128,-0.0001,-0.032,0.0455,-0.0019,-0.0087,-0.0447,-0.0583,-0.2546,0.0164,-0.0381,0.0808,-0.0303,0.017,-0.0538,-0.0164,0.034,-0.0064,0.044,0.0036,0.0294,-0.0244,-0.0226,-0.0136,-0.0579,0.0067,-0.0314,-0.0374,-0.0052,0.011,-0.0098,-0.0053,0.0624,0.0366,0.0094,0.0306,0.0473,0.0683,0.0649,0.0146,-0.0017,-0.1861,0.0617,0.0409,-0.0083,-0.0318,-0.0299,0.0167,0.0338,-0.0369,0.0312,0.0044,0.0733,0.007,-0.0222,0.0059,0.0103,-0.0055,0.0183,0.0222,0.0084,-0.035,-0.0125,-0.0519,-0.0546,0.0258,-0.0328,0.0429,0.0169,-0.0456,-0.019,-0.0588,0.006,-0.0757,0.0068,0.0535,0.0473,-0.0085,0.2028,-0.0503,0.046,0.0247,0.0169,0.0194,-0.0478,-0.0514,-0.0083,-0.0303,-0.018,-0.0145,-0.0355,0.0228,-0.0576,-0.0119,-0.0095,0.0705,0.0372,-0.0413,-0.0364,-0.031,-0.0163,0.0619,-0.0165,0.0193,-0.0834,0.0359,0.1706,0.0475,0.0199,0.0409,-0.0357,-0.0271,-0.0335,0.0326,0.0303,0.0329,-0.0278,0.0078,0.0048,-0.079,-0.0168,0.0234,-0.0535,-0.0432,0.1072,-0.0564,0.0034,-0.0462,-0.0098,0.0452,0.0687,0.0093,-0.0096,-0.0001,-0.0037,-0.0221,0.0423,-0.0834,0.0004,-0.0418,-0.0417,-0.0143,0.1172,0.0245,-0.0726,-0.0784,-0.0042,0.0325,0.0141,0.0905,0.0416,-0.0117,0.0079,0.0683,0.0388,-0.0761,0.0429,0.0026,0.0147,0.0038,-0.0449,-0.0359,0.0154,0.0693,-0.0402,0.0001,-0.0327,-0.0112,0.0378,-0.0007,-0.0134,-0.0258,-0.0435,-0.0405,-0.0271,-0.0309,-0.0073,0.0332,-0.0213,0.0242,0.001,-0.0587,0.0519,-0.001,0.0319,0.0117,0.0127,0.0358,0.0463,-0.0298,-0.0238,0.0594,-0.051,0.0307,0.0125,-0.0037,0.024,-0.0218,0.056,0.052,-0.0376,-0.0323,-0.2172,0.0053,0.0042,-0.0048,0.0581,-0.0655,0.0363,-0.0126,0.0647,0.0559,0.0243,0.0025,-0.0004,-0.0037,0.0228,0.0688,0.0182,-0.0088,-0.029,0.0098,-0.0389,0.0203,-0.0567,-0.0477,0.0459,0.0062,0.197,0.0514,0.0692,-0.0283,0.0183,0.0631,-0.0074,-0.0649,0.0759,0.0391,0.0807,-0.0272,-0.0629,-0.0386,-0.0446,-0.002,-0.0065,-0.1187,-0.0715,-0.0502,-0.0449,0.0459,-0.0547,-0.0154,0.0549,-0.0447,0.0465,-0.0383,0.017,-0.018,-0.0772,0.0588,-0.0252,0.0203,0.0108,-0.0566,0.0363,-0.0572,0.0177,-0.0262,-0.0453,-0.0593,0.0306,-0.0479,-0.028,0.055,-0.0067,0.0431,0.0531,-0.0204,0.0285,-0.0296,-0.0689,-0.0217,0.0596,-0.0103,0.0153,0.0517,0.0274,0.0134,0.0695,-0.0136,0.0475,-0.0364,-0.0289,-0.0052,-0.0942,0.0147,0.0102,0.0168,-0.2586,0.0141,0.0306,-0.0064,-0.0403,0.0023,0.0837,-0.0334,-0.0548,-0.0244,0.0,0.0598,0.0866,0.0073,-0.038,0.0133,0.0718,-0.0495,0.0846,-0.0697,0.0028,0.0114,0.2123,-0.0432,0.0274,0.0079,0.0114,-0.0158,0.0149,-0.0278,0.0034,-0.018,0.0711,-0.0387,0.0837,0.1007,-0.0416,0.0578,0.0174,-0.0257,0.017,-0.0155,-0.0328,-0.0693,0.1053,-0.036,0.0173,-0.0078,-0.0111,0.0247,0.0058,0.0401,0.0154,-0.0552,0.0281,-0.0012,-0.0423,-0.028,-0.0169,-0.0401,0.0029,-0.0804,-0.0406,-0.0331,-0.02]}
{"key":"[HAR-GCNN: Deep Graph CNNs for Human Activity Recognition From Highly Unlabeled Mobile Sensor Data] The problem of human activity recognition from mobile sensor data applies to multiple domains, such as health monitoring, personal fitness, daily life logging, and senior care. A critical challenge for training human activity recognition models is data quality. Acquiring balanced datasets containing accurate activity labels requires humans to correctly annotate and potentially interfere with the subjects' normal activities in real-time. Despite the likelihood of incorrect annotation or lack thereof, there is often an inherent chronology to human behavior. For example, we take a shower after we exercise. This implicit chronology can be used to learn unknown labels and classify future activities. In this work, we propose HAR-GCCN, a deep graph CNN model that leverages the correlation between chronologically adjacent sensor measurements to predict the correct labels for unclassified activities that have at least one activity label. We propose a new training strategy enforcing that the model predicts the missing activity labels by leveraging the known ones. HAR-GCCN shows superior performance relative to previously used baseline methods, improving classification accuracy by about 25% and up to 68% on different datasets. Code is available at \\url{https://github.com/abduallahmohamed/HAR-GCNN}.","layer":1,"vector":[-0.0116,0.0139,0.0287,-0.0354,0.0378,0.037,0.0289,-0.0079,0.0275,-0.0003,0.023,-0.0563,0.043,0.0707,0.0152,-0.0063,0.0016,0.0895,-0.0397,-0.0124,0.0018,-0.0403,-0.0007,-0.0142,0.0297,0.0059,-0.0587,-0.0376,-0.0607,-0.2193,0.0246,-0.0408,0.0752,-0.0009,0.0104,-0.0695,-0.0135,0.0486,-0.0547,0.0665,-0.0176,-0.0097,-0.027,-0.0438,-0.0235,-0.0365,-0.0463,-0.0319,-0.0401,-0.0502,0.0223,-0.0227,0.0271,0.025,0.0585,0.0277,0.0706,0.003,0.0246,0.0539,0.0458,0.0439,-0.1795,0.0317,0.0119,0.0251,-0.05,-0.0278,0.0283,0.0546,0.0077,0.0424,0.0233,0.0212,-0.0067,-0.0042,0.0134,-0.036,-0.0234,-0.0346,0.0046,-0.0047,-0.0198,-0.0002,0.0469,-0.0613,-0.0006,-0.0647,0.0064,0.0418,-0.0847,-0.0214,-0.0244,0.0546,-0.0825,0.0043,0.0369,0.0388,-0.0699,0.2189,-0.0835,0.0473,0.0109,-0.0143,0.021,-0.0149,0.0105,-0.0502,-0.0527,-0.02,0.0108,-0.0209,0.0389,-0.0529,0.0521,0.0445,0.1025,0.0488,0.0031,0.017,-0.0217,-0.021,0.0569,-0.0527,-0.0022,-0.0498,0.0265,0.1236,0.0195,0.0238,0.0385,-0.0039,-0.0132,0.0086,0.0161,0.0405,0.0433,-0.007,0.0545,-0.0149,0.0029,0.0045,0.0412,-0.0863,-0.0585,0.0947,-0.0254,-0.0126,-0.0326,-0.0337,-0.0547,0.0277,-0.0258,-0.0186,0.0551,0.0459,0.0463,0.0229,-0.0605,0.0034,-0.0174,-0.0665,-0.0394,0.0785,0.0236,-0.1161,0.0185,-0.0168,-0.0147,-0.0249,0.0714,0.0581,-0.0094,0.037,0.1191,0.0731,-0.0719,0.0284,0.0022,-0.0005,0.008,-0.0291,-0.039,0.0147,0.0562,-0.0244,-0.01,-0.0452,0.0308,0.0715,-0.0487,0.0024,-0.0316,-0.0091,-0.0179,-0.0238,-0.0367,0.0062,-0.0135,-0.0383,0.0229,-0.043,0.0214,0.0279,0.0058,0.0086,-0.0667,0.0252,0.0262,0.0044,0.0134,0.0226,0.0552,-0.0345,-0.0023,-0.0275,0.0197,0.0099,-0.039,0.0187,0.0285,-0.0246,-0.0157,-0.2224,-0.0049,0.0354,-0.0393,0.0305,-0.0362,-0.002,0.0027,0.0839,0.0577,0.0587,-0.023,-0.0207,-0.0232,-0.0302,0.0935,0.0436,0.0447,0.0043,-0.0094,0.0023,-0.0107,0.0137,-0.0919,0.0469,0.0083,0.2502,0.0414,0.0546,-0.0056,-0.0093,0.0152,-0.0416,-0.1223,0.0617,0.0067,0.0755,0.0034,-0.0598,-0.0741,-0.0886,0.013,0.0215,-0.0881,-0.0492,-0.0123,-0.028,-0.0126,-0.0574,-0.0384,0.0318,-0.0053,0.0723,0.0125,-0.0314,-0.0308,-0.094,0.0296,-0.0954,0.002,-0.0403,-0.0351,-0.0083,-0.0361,0.0552,0.0192,-0.0301,-0.0493,0.0007,-0.0107,-0.0126,0.0927,0.0196,-0.0096,0.0441,-0.0078,-0.0015,-0.0147,-0.0167,-0.0026,0.0666,-0.0299,0.0033,0.0083,0.0812,0.0073,0.0642,-0.0453,0.0401,-0.0495,0.0444,0.0071,-0.0258,-0.0636,0.0574,-0.0388,-0.2696,0.0004,0.0067,0.0464,-0.0409,-0.0291,0.0595,0.0552,-0.051,0.0552,0.0206,0.0538,0.0317,-0.0281,-0.0116,0.0213,0.0666,-0.0273,0.0309,-0.0665,0.0654,0.0467,0.143,-0.0415,0.0477,0.0599,0.0042,-0.0206,0.0595,-0.0339,-0.0208,0.0092,0.0944,-0.0222,0.0125,0.0729,0.0074,0.0142,0.0226,0.0158,0.0052,-0.024,-0.0115,-0.045,0.0926,-0.0022,-0.0525,-0.0381,0.0017,0.0273,0.0023,-0.0559,-0.0562,0.0272,0.0116,0.0259,-0.0146,-0.0578,-0.0561,-0.0638,0.0184,-0.0186,0.0071,-0.0352,-0.045]}
{"key":"[Learning Temporal Dependence from Time-Series Data with Latent Variables] We consider the setting where a collection of time series, modeled as random processes, evolve in a causal manner, and one is interested in learning the graph governing the relationships of these processes. A special case of wide interest and applicability is the setting where the noise is Gaussian and relationships are Markov and linear. We study this setting with two additional features: firstly, each random process has a hidden (latent) state, which we use to model the internal memory possessed by the variables (similar to hidden Markov models). Secondly, each variable can depend on its latent memory state through a random lag (rather than a fixed lag), thus modeling memory recall with differing lags at distinct times. Under this setting, we develop an estimator and prove that under a genericity assumption, the parameters of the model can be learned consistently. We also propose a practical adaption of this estimator, which demonstrates significant performance gains in both synthetic and real-world datasets.","layer":0,"vector":[-0.0089,0.0189,0.0201,-0.0078,0.0371,0.0294,0.0638,0.0312,0.0629,-0.0254,0.0353,-0.0081,0.0541,0.0703,0.0055,0.0516,-0.0139,0.0514,-0.0299,-0.014,0.0005,-0.0678,-0.0016,-0.0232,0.0199,0.0231,-0.0217,-0.0217,-0.0476,-0.2234,0.011,-0.0637,0.0325,0.0063,0.0258,-0.041,-0.0482,0.0423,0.005,0.0583,0.0277,0.0239,-0.0392,-0.0812,-0.0446,-0.0568,0.012,-0.0242,-0.0244,-0.0415,-0.0442,0.002,0.0523,0.0279,0.0572,0.0561,0.0686,0.0373,0.052,0.0272,0.0272,0.0587,-0.1725,0.0503,0.053,0.0449,-0.0431,-0.0077,0.0192,0.0158,-0.0415,0.0261,-0.0091,0.0253,0.0335,0.0537,0.0108,-0.0335,-0.0151,-0.0159,0.0373,-0.0122,-0.0384,-0.0581,-0.0314,-0.0745,0.028,-0.0714,0.0316,0.0212,-0.0289,0.0093,-0.0034,0.0297,-0.0684,-0.0256,0.0519,0.0544,-0.0015,0.1863,-0.0464,0.0265,0.0292,0.003,0.0244,-0.065,-0.0168,-0.0461,0.0097,-0.0056,0.0115,-0.0284,0.0335,-0.0782,0.0363,-0.0261,0.031,0.0428,0.0056,-0.0254,-0.025,0.0243,0.0324,-0.0305,-0.0002,-0.0995,-0.0024,0.1556,0.0289,-0.0151,0.0501,-0.0165,-0.0283,0.0209,0.0118,0.0013,-0.0049,-0.0111,0.0465,-0.0435,-0.0451,-0.0112,0.0497,-0.0731,-0.1081,0.1484,0.0107,0.0234,-0.0718,0.0038,-0.0512,0.0115,-0.0011,-0.0385,0.0552,0.023,0.0179,0.0062,-0.0324,0.0494,-0.0919,-0.027,-0.0121,0.0905,-0.0091,-0.0367,-0.0436,0.011,0.0466,0.0071,0.0817,0.0424,-0.0504,0.0041,0.0716,0.0262,-0.0389,0.0169,0.0424,-0.0134,-0.003,-0.0529,-0.018,0.0753,0.0521,-0.0462,0.0171,0.0028,0.008,0.0264,0.0151,-0.0093,-0.0024,-0.0081,-0.0285,-0.0302,-0.0325,-0.0265,0.0393,-0.045,-0.032,-0.0332,-0.0331,-0.0097,-0.0171,0.0254,-0.0298,0.0311,0.0392,-0.0057,-0.0113,0.0067,0.0498,-0.0165,-0.0348,0.005,-0.0061,0.0563,0.0004,0.0079,0.0443,-0.0672,-0.0663,-0.2265,-0.0244,-0.007,0.0222,0.0852,-0.0618,0.0153,-0.0105,0.0657,0.0649,0.0187,-0.0454,-0.0519,-0.015,0.0159,0.0554,0.0321,0.0607,-0.0083,0.013,-0.0448,-0.0202,-0.0192,-0.0915,0.0571,0.011,0.2194,-0.0035,0.0156,-0.0702,0.0281,0.0176,-0.0122,-0.078,0.0375,0.0277,0.061,-0.0012,-0.042,-0.0441,-0.073,-0.0012,0.0011,-0.0439,-0.0754,-0.0291,-0.0138,0.0025,-0.0624,0.0284,0.0246,-0.0275,0.0878,-0.0201,-0.0141,-0.0825,-0.1046,0.0143,-0.0477,0.0413,0.0037,-0.0215,0.0231,-0.0651,0.0447,-0.0155,-0.0125,-0.0657,-0.0155,0.0077,0.0075,0.0761,-0.0123,-0.0267,0.0395,-0.007,0.0127,-0.0389,-0.0733,-0.0081,0.0538,-0.0711,0.0389,0.0273,0.0396,-0.0221,0.0996,0.0042,0.0359,0.0111,0.0244,-0.0039,-0.0431,0.0068,0.0316,-0.0183,-0.2947,0.0398,0.0069,0.0332,0.0011,-0.0271,-0.0035,0.0282,-0.0502,0.0172,-0.0228,0.0379,0.0752,-0.0077,0.002,0.0601,0.0732,-0.0558,0.0441,-0.0577,0.0091,0.0361,0.2107,-0.0089,0.0667,-0.0009,-0.0344,0.0051,0.0533,-0.023,0.0393,0.036,0.0729,-0.0184,0.0364,0.061,-0.0439,0.0425,0.0044,-0.0078,0.0024,-0.001,-0.0593,-0.0209,0.121,0.0035,-0.0079,-0.0706,-0.0457,0.0608,-0.0211,-0.0246,0.0025,0.0297,0.0116,0.0492,-0.0045,-0.0508,-0.0098,-0.0484,-0.0461,-0.0439,-0.0113,-0.0449,-0.0161]}
{"key":"[Multi-Scale High-Resolution Vision Transformer for Semantic Segmentation] Vision Transformers (ViTs) have emerged with superior performance on computer vision tasks compared to convolutional neural network (CNN)-based models. However, ViTs are mainly designed for image classification that generate single-scale low-resolution representations, which makes dense prediction tasks such as semantic segmentation challenging for ViTs. Therefore, we propose HRViT, which enhances ViTs to learn semantically-rich and spatially-precise multi-scale representations by integrating high-resolution multi-branch architectures with ViTs. We balance the model performance and efficiency of HRViT by various branch-block co-optimization techniques. Specifically, we explore heterogeneous branch designs, reduce the redundancy in linear layers, and augment the attention block with enhanced expressiveness. Those approaches enabled HRViT to push the Pareto frontier of performance and efficiency on semantic segmentation to a new level, as our evaluation results on ADE20K and Cityscapes show. HRViT achieves 50.20% mIoU on ADE20K and 83.16% mIoU on Cityscapes, surpassing state-of-the-art MiT and CSWin backbones with an average of +1.78 mIoU improvement, 28% parameter saving, and 21% FLOPs reduction, demonstrating the potential of HRViT as a strong vision backbone for semantic segmentation.","layer":1,"vector":[-0.0343,-0.0394,0.0481,-0.0321,0.0105,-0.0083,0.0056,0.0122,0.011,0.0161,0.0113,-0.0882,0.0371,0.0921,0.0173,0.0415,0.013,0.013,-0.0307,0.005,0.0415,-0.0397,-0.0047,-0.0509,0.0008,0.0145,0.0144,-0.0303,-0.0292,-0.2401,0.0036,-0.0289,0.0205,0.0069,-0.0013,-0.0636,-0.0283,0.0668,0.0041,0.045,0.0162,0.0101,0.0009,-0.0254,-0.0363,-0.06,0.0016,-0.0565,-0.0558,-0.0298,0.05,-0.0282,0.0564,0.0815,-0.0253,0.0371,0.0596,0.056,0.0742,0.0165,0.0454,0.0737,-0.1801,0.0572,0.0131,-0.0163,-0.0536,-0.0067,-0.0379,-0.0026,0.0024,-0.003,-0.0015,0.0107,0.0368,-0.0437,-0.0202,-0.0743,-0.0039,-0.02,-0.0064,-0.0337,-0.0506,-0.0197,-0.0072,-0.0439,0.0155,-0.0427,0.0121,-0.0213,-0.0314,-0.0297,-0.0422,0.0309,-0.0739,-0.0205,0.0203,-0.0117,-0.0469,0.1989,-0.0845,0.0648,0.0368,-0.0755,0.0507,-0.0207,-0.0462,-0.0036,-0.0616,0.0113,-0.0339,-0.0328,0.0189,0.0165,0.022,-0.0149,0.0574,0.0599,-0.0279,0.006,-0.0225,0.0077,0.0388,-0.0015,0.0115,-0.0457,0.0577,0.1513,0.0566,0.0424,0.0227,0.027,-0.0125,0.0017,0.0137,0.0613,-0.005,-0.0422,-0.018,-0.029,-0.0315,-0.0143,0.0069,-0.085,0.007,0.0876,-0.0832,0.055,-0.0373,-0.0216,0.006,0.0188,-0.0706,-0.0244,0.0393,0.0437,0.011,0.0563,-0.0168,0.0088,0.0107,-0.0494,-0.0889,0.0921,0.0197,-0.0811,-0.0352,0.0096,0.017,-0.0138,0.0249,0.014,-0.0107,0.0362,0.0683,0.0213,-0.1052,0.0172,-0.0015,0.0365,0.0512,-0.0487,0.0024,-0.0154,0.056,-0.0522,-0.0058,-0.058,-0.0063,0.0288,-0.04,0.0594,-0.0414,-0.0386,0.0107,-0.01,-0.0213,0.0006,0.0073,-0.0062,0.0076,-0.01,-0.006,0.0288,-0.0145,-0.0076,-0.0387,0.0092,-0.0112,0.0261,-0.0119,-0.0053,0.0944,0.0652,-0.0075,-0.0197,0.053,0.0223,0.0045,0.0529,0.0282,-0.0478,-0.0517,-0.2048,-0.0144,0.0228,-0.0358,0.0576,-0.0335,-0.0083,-0.0213,0.0316,0.0653,0.0757,-0.0377,-0.0305,0.0429,-0.031,0.0541,0.0169,0.059,-0.0629,-0.0238,0.022,0.0159,0.0139,-0.0832,0.0638,-0.0345,0.2424,-0.0005,0.0486,0.0102,0.0461,0.0421,-0.01,-0.0776,0.0611,0.0336,0.0451,0.0017,-0.0548,-0.0328,-0.0593,-0.0022,-0.0124,-0.1024,-0.0387,-0.0507,0.0002,0.0873,-0.0118,-0.0172,-0.0074,-0.0381,0.0374,-0.0115,0.0028,-0.0358,-0.091,0.0252,-0.0402,-0.0168,0.0162,-0.0392,0.0086,-0.0682,0.0505,0.0099,-0.0298,-0.0195,0.0029,-0.0363,-0.0505,0.0261,0.0268,-0.0126,0.0373,0.0077,0.0719,0.0198,-0.0413,-0.0308,0.04,-0.0646,0.0132,-0.0239,0.0637,0.0294,0.0805,-0.0321,0.0205,0.0252,0.0531,0.0069,-0.0383,-0.0215,0.0375,-0.0234,-0.2963,0.0404,0.0671,0.002,-0.0162,-0.0051,0.0526,0.0125,0.0061,0.0057,-0.0542,0.0342,0.0855,0.0162,0.0059,0.0274,0.0905,-0.0296,0.0816,-0.027,0.0005,0.0297,0.2326,-0.0639,-0.0337,0.0167,-0.0197,0.0043,0.0316,-0.0121,-0.0141,0.0171,0.079,-0.0544,0.017,0.0889,-0.0444,0.0413,0.0288,0.0168,0.0145,0.0049,-0.0305,-0.0421,0.0726,0.0083,-0.0192,-0.0256,0.0178,0.0001,0.001,0.0145,0.0005,-0.0027,0.0559,0.0399,-0.056,-0.0256,-0.0953,0.0201,0.0207,-0.0448,-0.0408,0.0171,-0.0089]}
{"key":"[XGBoost: A Scalable Tree Boosting System] Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems.","layer":1,"vector":[-0.0236,0.0178,0.0282,-0.0108,0.014,0.0372,-0.032,-0.0127,0.0419,0.0186,0.0117,-0.0452,0.0751,0.0444,0.0055,0.0234,0.0067,0.0359,-0.0642,-0.0072,0.0486,-0.0174,-0.0225,-0.0881,0.0471,0.0037,0.0135,-0.0611,-0.0701,-0.2649,-0.0101,-0.0545,0.0612,0.0052,0.027,0.0193,-0.0237,0.0514,-0.0326,0.0272,0.0108,0.0477,-0.0252,-0.018,-0.0333,-0.0674,-0.014,-0.0055,-0.0242,-0.0248,0.0057,-0.0324,0.064,-0.009,0.0139,0.0225,0.0145,0.0242,0.0161,0.0257,0.01,0.0421,-0.1167,0.0489,0.0629,0.0394,-0.0486,-0.0059,0.0025,0.0436,-0.0175,0.0295,0.0158,0.0532,0.0036,0.0116,0.0054,-0.05,0.0117,0.0007,0.0061,-0.0422,-0.0772,0.0029,-0.015,-0.0465,-0.0073,-0.0408,0.0497,0.0161,-0.0109,0.0038,-0.0316,0.0101,-0.0951,-0.0359,0.0211,0.0393,-0.0684,0.2111,-0.0288,0.075,0.0134,-0.041,0.0137,-0.0551,0.0183,-0.0273,-0.0401,-0.0585,-0.0222,-0.0515,0.018,0.0026,0.0382,-0.0101,0.0396,0.0441,-0.0308,0.0204,-0.0074,0.0366,0.0498,-0.0271,0.0488,-0.0242,-0.0179,0.1089,0.0593,0.0362,0.0459,-0.0359,-0.0199,-0.0065,0.0141,0.0127,0.047,0.0195,0.0148,0.0149,-0.0074,-0.0434,0.0313,-0.0619,-0.0608,0.0988,-0.0072,0.0333,-0.0631,-0.0589,-0.0403,0.0035,-0.0133,-0.0465,0.0309,0.0361,0.0277,0.0544,0.0013,-0.0047,0.0146,-0.0539,-0.0209,0.0593,0.0373,-0.0949,-0.0302,-0.0016,-0.0091,0.0071,0.0654,0.034,-0.0367,0.0467,0.0878,-0.0214,-0.0646,-0.0592,0.0317,0.006,0.0596,-0.031,-0.06,0.0942,0.1013,-0.0259,-0.0052,-0.0186,0.0364,0.0266,-0.0263,0.0089,-0.0415,-0.0325,0.0104,-0.0385,-0.0058,0.0167,0.0013,-0.0403,0.0215,0.0037,-0.0349,-0.0003,-0.0006,-0.0153,-0.0258,0.0162,0.0242,0.0083,-0.0088,-0.0212,0.0805,-0.0163,-0.037,0.0236,0.0303,0.0362,-0.0176,0.028,0.0179,-0.0596,-0.0551,-0.2453,-0.0507,0.0319,-0.0226,0.0238,-0.0822,0.0391,0.0109,0.0162,0.0451,0.0377,-0.021,-0.0134,0.041,-0.0156,0.0501,0.0436,0.0307,-0.0386,-0.0038,-0.0109,0.0331,0.007,-0.0881,0.0341,0.003,0.2221,0.0394,0.0128,-0.0343,0.0397,0.0086,-0.034,-0.1391,0.0443,0.0487,0.0337,0.0277,-0.0301,0.035,-0.0418,0.008,-0.0087,-0.1137,-0.0519,-0.0086,-0.0354,0.0241,-0.0567,0.0023,0.0429,-0.0141,0.0339,0.0114,0.0106,-0.0775,-0.0888,0.0586,-0.0139,0.0226,0.0175,-0.0648,0.0021,-0.0456,0.048,-0.0259,-0.0123,-0.0032,-0.0009,-0.0718,-0.0343,0.042,-0.0125,-0.0202,0.062,0.0311,0.0265,-0.0117,-0.05,-0.0181,0.067,-0.0402,0.0547,-0.0326,0.0123,0.0517,0.0656,0.0171,0.0704,-0.034,-0.0111,-0.0076,-0.0527,-0.0152,0.0466,0.0234,-0.3308,0.0345,-0.017,0.0139,-0.0464,0.0013,0.0705,0.0172,0.0005,0.0003,-0.0074,0.053,0.0296,-0.0299,-0.0253,0.0778,0.0905,-0.0037,0.0392,-0.0712,0.0333,0.0042,0.2276,-0.0118,0.0502,0.008,-0.067,0.0127,0.0536,-0.0644,-0.0046,0.0155,0.0715,-0.0398,0.0375,0.0867,-0.0076,0.0551,0.0158,-0.009,0.0084,0.0247,-0.0635,-0.0181,0.0702,-0.0659,-0.0041,-0.0406,-0.0041,0.0399,0.0069,0.0112,-0.0232,0.0129,-0.0149,0.01,-0.0455,-0.0521,-0.0329,-0.0325,-0.0058,-0.0362,-0.0239,0.0102,0.0172]}
{"key":"[Fast Dimension Independent Private AdaGrad on Publicly Estimated Subspaces] We revisit the problem of empirical risk minimziation (ERM) with differential privacy. We show that noisy AdaGrad, given appropriate knowledge and conditions on the subspace from which gradients can be drawn, achieves a regret comparable to traditional AdaGrad plus a well-controlled term due to noise. We show a convergence rate of $O(\\text{Tr}(G_T)/T)$, where $G_T$ captures the geometry of the gradient subspace. Since $\\text{Tr}(G_T)=O(\\sqrt{T})$ we can obtain faster rates for convex and Lipschitz functions, compared to the $O(1/\\sqrt{T})$ rate achieved by known versions of noisy (stochastic) gradient descent with comparable noise variance. In particular, we show that if the gradients lie in a known constant rank subspace, and assuming algorithmic access to an envelope which bounds decaying sensitivity, one can achieve faster convergence to an excess empirical risk of $\\tilde O(1/\\epsilon n)$, where $\\epsilon$ is the privacy budget and $n$ the number of samples. Letting $p$ be the problem dimension, this result implies that, by running noisy Adagrad, we can bypass the DP-SGD bound $\\tilde O(\\sqrt{p}/\\epsilon n)$ in $T=(\\epsilon n)^{2/(1+2\\alpha)}$ iterations, where $\\alpha \\geq 0$ is a parameter controlling gradient norm decay, instead of the rate achieved by SGD of $T=\\epsilon^2n^2$. Our results operate with general convex functions in both constrained and unconstrained minimization. Along the way, we do a perturbation analysis of noisy AdaGrad of independent interest. Our utility guarantee for the private ERM problem follows as a corollary to the regret guarantee of noisy AdaGrad.","layer":4,"vector":[-0.0173,-0.0367,0.0232,-0.0002,-0.0386,0.0142,0.054,0.0423,0.0806,0.0056,0.0278,-0.0311,0.0258,0.0491,0.0357,0.0488,-0.0001,0.0323,-0.0497,0.0692,0.0288,-0.0466,0.0009,-0.0946,0.0152,-0.0164,-0.0613,-0.0435,-0.0398,-0.249,0.022,-0.0758,0.0344,-0.0529,0.0264,-0.0287,-0.0434,0.0601,-0.0609,0.0413,-0.0149,0.0589,-0.0671,-0.0367,-0.0146,-0.0704,-0.0214,-0.0454,-0.057,0.0022,-0.002,0.0045,0.0228,0.063,0.044,0.0204,0.0558,0.0223,0.0105,0.0755,0.0078,0.0217,-0.1582,0.0178,0.0282,0.0234,-0.0236,-0.0432,0.0074,0.057,0.0046,0.0713,-0.0071,0.025,-0.0096,-0.0029,0.0065,-0.0268,-0.0196,0.0227,0.0061,-0.0269,-0.0452,-0.0052,-0.0484,-0.0743,0.0794,-0.0291,0.0534,0.0045,-0.022,-0.0243,-0.0032,0.004,-0.032,-0.0221,0.0507,0.0273,-0.0153,0.2153,-0.0643,0.0705,0.0208,-0.0179,0.0443,-0.0321,-0.0352,0.0023,-0.0142,-0.0225,-0.0383,-0.0262,0.0347,-0.0213,-0.0093,-0.0007,0.0576,0.0522,-0.0328,-0.0213,-0.0254,0.0084,0.0541,0.027,0.0337,-0.086,-0.0052,0.1462,0.0496,0.0533,0.0274,-0.0581,-0.0054,-0.0264,0.0228,0.0218,-0.0161,0.0239,0.0543,-0.0387,-0.0712,-0.0315,0.0162,-0.075,-0.0293,0.1556,0.0017,0.0606,-0.0098,-0.0672,0.0264,0.0183,-0.0011,0.0097,0.0251,-0.0301,0.0071,0.0591,-0.0678,0.0385,-0.0391,-0.0106,0.0275,0.1522,-0.031,-0.0736,-0.0296,0.0139,0.0341,-0.0178,0.0412,0.0546,-0.0092,0.0237,0.0556,0.0367,-0.0812,-0.0006,-0.018,-0.0081,-0.0246,-0.0397,-0.028,0.0115,0.0206,-0.0267,0.0261,-0.0296,0.0166,0.0469,-0.0547,-0.0003,-0.0505,-0.0071,-0.0496,-0.062,-0.0131,-0.0066,0.0134,-0.0002,0.006,-0.0069,-0.0345,0.0521,-0.012,0.0155,0.029,-0.0119,0.0194,0.032,-0.0522,-0.0254,0.0352,-0.0501,-0.0313,0.029,0.041,0.0438,-0.0124,0.0625,0.0462,-0.0093,-0.0707,-0.2178,-0.0351,-0.0131,-0.0122,0.0231,-0.0535,0.0346,-0.0378,0.0764,0.0558,0.042,-0.029,-0.0347,0.0603,-0.0008,0.0481,0.0074,0.0088,-0.003,-0.0253,-0.0202,0.0019,-0.0079,-0.0582,0.081,-0.0047,0.2205,-0.0007,0.0672,-0.0315,0.02,-0.0061,0.0146,-0.0992,0.0416,0.0104,0.0258,-0.0041,-0.0543,-0.072,-0.0217,-0.0292,0.0062,-0.0798,-0.0404,-0.0685,-0.0254,0.0227,-0.081,0.0076,0.061,-0.0131,0.0736,-0.045,0.0325,-0.0501,-0.0463,0.0142,-0.0444,0.0421,0.0103,-0.0651,0.0274,-0.0828,0.0462,-0.0215,-0.002,-0.0583,0.0387,-0.0354,-0.0116,0.0598,0.0082,-0.0053,0.0262,-0.0119,0.0424,-0.0129,-0.0726,-0.0396,0.0746,-0.008,0.0256,0.0214,0.0013,0.0345,0.1066,0.0248,-0.0186,-0.0082,-0.0149,0.0187,-0.0967,0.0105,0.0515,0.0113,-0.2614,-0.0219,0.0021,-0.0055,-0.04,-0.0104,0.0517,-0.0205,-0.0803,0.0065,0.0035,0.0866,0.018,0.0089,0.0723,0.0374,0.0724,-0.0168,0.0424,-0.0623,0.0464,0.0146,0.2109,-0.0275,-0.0291,0.0214,-0.0145,0.0101,0.0155,-0.0469,0.0193,0.0189,0.0589,-0.0654,0.0592,0.076,-0.0237,0.0292,0.0379,-0.0056,-0.0106,-0.0294,-0.0052,0.0116,0.0896,-0.0057,0.0014,0.0044,-0.0044,0.0081,0.0095,0.0204,0.0043,0.0077,0.0439,0.0359,-0.0651,-0.0186,0.0026,-0.0472,-0.0071,-0.0518,-0.0712,-0.0016,0.0058]}
{"key":"[Algorithms and SQ Lower Bounds for PAC Learning One-Hidden-Layer ReLU Networks] We study the problem of PAC learning one-hidden-layer ReLU networks with $k$ hidden units on $\\mathbb{R}^d$ under Gaussian marginals in the presence of additive label noise. For the case of positive coefficients, we give the first polynomial-time algorithm for this learning problem for $k$ up to $\\tilde{O}(\\sqrt{\\log d})$. Previously, no polynomial time algorithm was known, even for $k=3$. This answers an open question posed by~\\cite{Kliv17}. Importantly, our algorithm does not require any assumptions about the rank of the weight matrix and its complexity is independent of its condition number. On the negative side, for the more general task of PAC learning one-hidden-layer ReLU networks with arbitrary real coefficients, we prove a Statistical Query lower bound of $d^{\\Omega(k)}$. Thus, we provide a separation between the two classes in terms of efficient learnability. Our upper and lower bounds are general, extending to broader families of activation functions.","layer":0,"vector":[-0.0565,-0.0062,0.0259,-0.0054,0.0059,0.0377,0.0476,0.0198,0.0207,-0.0213,0.0307,-0.0456,0.0472,0.0594,0.0334,0.0406,0.0226,0.0648,-0.0529,0.0135,0.0525,-0.0454,0.0079,-0.0493,0.0292,-0.0321,-0.0269,-0.1083,-0.042,-0.247,-0.0108,-0.0203,0.0578,-0.0199,0.021,-0.0046,-0.0255,0.0353,-0.0226,0.0243,0.0418,0.0309,0.0113,-0.0859,-0.0386,-0.0145,-0.0227,-0.0535,-0.0127,-0.0379,0.0378,0.0297,0.036,0.0414,0.0275,0.0034,0.0416,0.0549,-0.0089,0.0329,-0.008,0.0487,-0.1682,0.044,0.0547,-0.0132,-0.0677,-0.0495,0.0261,0.0635,-0.0079,0.0293,-0.0057,0.0347,0.0245,0.0144,0.0301,-0.0468,0.0106,0.0098,0.0026,-0.0376,-0.0542,-0.038,-0.0187,-0.0596,0.0048,-0.0392,-0.0077,0.0138,-0.032,0.0122,-0.005,0.0396,-0.0734,-0.0249,0.0766,0.0363,-0.0371,0.174,-0.0557,0.0031,0.0343,-0.0228,0.027,-0.0435,-0.0095,-0.0147,-0.0259,0.0087,-0.0137,-0.0289,0.0339,-0.0749,0.0317,0.0562,0.0283,0.0412,-0.0272,-0.0151,-0.0316,-0.0271,0.0227,-0.0341,0.0585,-0.0516,0.0137,0.1457,0.0259,0.063,0.057,-0.0452,-0.0129,-0.0098,0.0362,0.0194,0.019,0.0052,0.0128,-0.0285,-0.0532,-0.0259,0.0427,-0.0979,-0.0676,0.0918,-0.0471,0.0621,-0.0672,-0.0284,-0.0179,0.0353,-0.0131,-0.0432,0.0317,-0.0039,0.0431,0.0273,-0.0503,0.0145,-0.0175,-0.0925,-0.0083,0.1151,0.0235,-0.0742,-0.0361,-0.0083,0.0369,-0.0311,0.0621,0.0497,-0.02,0.0202,0.0966,0.0127,-0.0501,0.0055,-0.0161,0.0153,-0.0464,-0.0352,-0.0036,0.0348,0.0153,-0.0515,0.0128,-0.0044,0.0138,0.0578,-0.0505,-0.0233,-0.0834,0.0259,-0.0874,-0.0332,-0.027,-0.0358,0.0168,-0.0113,-0.0082,-0.0456,-0.0197,-0.0122,0.0151,0.0424,0.0027,0.0293,0.0283,0.0268,-0.0205,0.0153,0.0327,-0.07,-0.0438,-0.023,0.0138,0.0334,-0.0216,0.0061,0.0266,-0.0479,-0.0758,-0.2387,0.0116,-0.007,-0.0366,0.0429,-0.0758,0.0556,0.0062,0.0507,0.0578,0.0648,0.0054,-0.0809,0.0262,0.0194,0.0456,0.049,0.0158,0.0401,-0.0252,-0.0183,0.0366,0.005,-0.0273,0.0826,0.0303,0.2284,-0.0018,0.0372,-0.0273,0.0052,0.0119,-0.0313,-0.0064,0.0392,0.0416,0.0612,0.0131,-0.0322,-0.0122,0.0161,-0.0105,0.0011,-0.1056,-0.0435,-0.013,-0.0266,0.0096,-0.0744,0.0453,0.02,-0.0393,0.0544,-0.0044,-0.0259,-0.051,-0.0822,0.0055,-0.0614,0.0279,-0.0068,-0.0299,-0.001,-0.0812,0.0395,0.0098,-0.045,-0.0176,0.0257,-0.0273,0.0095,0.075,0.0158,0.0115,0.0531,-0.0206,0.0585,-0.0504,-0.0431,-0.0352,0.089,-0.0477,0.0608,-0.0425,0.0394,0.0233,0.1562,-0.0102,0.017,-0.0083,-0.0255,-0.0044,-0.0455,-0.0044,0.0331,0.0152,-0.2887,0.0251,0.0193,0.047,-0.0362,0.0136,0.0675,0.0198,-0.0618,-0.0187,0.0782,0.0461,0.0455,-0.0146,-0.0043,0.0275,0.0519,-0.0214,0.0558,-0.0231,0.0134,0.0187,0.1786,-0.0476,0.0281,-0.0256,-0.0622,-0.0468,0.0087,0.0133,0.0366,0.0314,0.0643,-0.0326,0.0425,0.0865,-0.0243,0.0593,0.0251,0.0019,-0.0148,-0.0522,-0.0798,0.0065,0.0781,-0.0314,0.0148,-0.0377,-0.0073,0.0416,0.0067,0.0088,0.0227,0.0041,0.0467,0.0611,-0.0434,-0.0345,-0.0319,-0.06,0.0251,-0.0669,-0.0027,0.034,-0.0444]}
{"key":"[Intrinsic Non-stationary Covariance Function for Climate Modeling] Designing a covariance function that represents the underlying correlation is a crucial step in modeling complex natural systems, such as climate models. Geospatial datasets at a global scale usually suffer from non-stationarity and non-uniformly smooth spatial boundaries. A Gaussian process regression using a non-stationary covariance function has shown promise for this task, as this covariance function adapts to the variable correlation structure of the underlying distribution. In this paper, we generalize the non-stationary covariance function to address the aforementioned global scale geospatial issues. We define this generalized covariance function as an intrinsic non-stationary covariance function, because it uses intrinsic statistics of the symmetric positive definite matrices to represent the characteristic length scale and, thereby, models the local stochastic process. Experiments on a synthetic and real dataset of relative sea level changes across the world demonstrate improvements in the error metrics for the regression estimates using our newly proposed approach.","layer":0,"vector":[-0.0695,-0.0346,0.0609,0.0002,0.0477,-0.0146,0.0336,-0.0233,0.0322,-0.0143,0.0201,-0.0537,0.0204,0.07,-0.0378,0.0057,-0.0149,0.0841,-0.0387,0.0217,0.0387,-0.0195,-0.0096,-0.0224,0.0362,0.0507,-0.033,-0.0451,-0.0356,-0.2497,0.008,-0.0285,0.0315,0.0103,-0.0114,-0.0091,-0.0225,0.0339,-0.0097,0.0622,-0.0104,0.0347,-0.0425,-0.0178,-0.0443,-0.0597,-0.0774,-0.0049,-0.0904,-0.0112,0.0201,-0.0015,-0.0123,0.0374,0.0132,0.0752,0.0374,0.0191,0.0894,0.0272,0.0165,0.041,-0.2277,0.0739,0.063,0.0523,-0.0345,-0.0216,-0.0075,-0.0101,-0.0376,0.059,0.0175,0.0331,0.0483,-0.024,-0.037,0.029,-0.0429,-0.0091,0.0377,0.03,-0.0392,-0.023,-0.0201,-0.0519,0.0156,-0.0513,0.0367,-0.0081,-0.0158,-0.0401,-0.0114,0.0313,-0.0698,0.0099,0.0352,0.0267,0.0047,0.1763,-0.0736,0.039,0.0568,0.038,0.0478,-0.0172,-0.0253,-0.0415,0.0223,0.0024,-0.0048,-0.0223,-0.0145,-0.0625,-0.0014,-0.0591,0.0561,0.0342,-0.0159,-0.0156,-0.0153,0.0224,0.048,0.0035,0.036,-0.0487,0.0338,0.1352,0.0659,-0.0208,0.0478,-0.0139,-0.0578,0.0106,0.0112,-0.0121,-0.0067,-0.0017,0.0041,0.0057,-0.0384,-0.0134,-0.0008,-0.0814,-0.0433,0.1351,-0.045,0.0501,-0.0792,0.0067,-0.0425,0.0487,-0.0184,-0.0235,0.0458,0.0401,-0.0578,0.0024,-0.067,0.0397,-0.0584,-0.0384,-0.0074,0.0846,-0.0021,-0.0612,-0.0401,0.0649,0.048,0.0273,0.0473,0.0237,-0.0169,-0.009,0.1253,0.0292,-0.0523,0.0204,0.0215,0.0036,0.0051,-0.0367,-0.0244,0.042,0.053,-0.0368,-0.0016,-0.0593,0.0329,0.001,-0.0195,-0.028,-0.015,0.0071,-0.0183,-0.0235,-0.005,-0.0012,0.0642,-0.0207,0.0514,0.0087,-0.0263,-0.0215,-0.0058,0.0201,0.003,-0.0254,0.0108,0.0439,-0.0084,-0.0174,0.0353,-0.0012,-0.0291,0.0246,0.0034,0.0616,0.0011,0.0308,0.0301,-0.0627,-0.0933,-0.2334,-0.0093,0.0154,-0.0116,0.0715,-0.0628,0.0447,0.0083,0.0859,0.0967,0.0819,-0.0054,-0.0098,0.0257,-0.0123,0.0443,0.0398,0.0434,-0.0229,-0.0091,-0.0634,-0.0004,-0.0516,-0.0518,0.0717,-0.0195,0.1456,-0.012,0.0211,-0.0545,-0.0027,-0.0262,-0.0142,-0.0932,0.0608,0.0623,0.0739,0.0286,-0.0607,-0.0288,0.0229,0.0435,0.0258,-0.0116,-0.0526,0.0013,-0.0058,0.0531,-0.0604,-0.0143,0.0431,-0.0065,0.0978,-0.0342,0.0111,-0.0192,-0.071,0.0106,-0.0221,0.0221,0.0083,-0.0505,0.0292,-0.0607,0.0622,0.0038,-0.0386,-0.0636,0.044,-0.0113,-0.0271,0.0925,-0.0371,-0.0106,0.0527,-0.0404,0.0283,-0.0193,-0.0423,-0.0468,0.0662,-0.0689,0.0211,0.0156,0.034,-0.0008,0.0633,-0.0415,0.041,-0.0378,-0.0057,-0.0036,-0.0089,-0.0504,0.0588,0.0281,-0.277,0.0093,-0.0175,-0.0018,-0.0302,-0.0672,0.0134,0.0546,-0.088,0.0277,-0.0279,0.0209,0.0434,0.0096,0.0089,0.0317,-0.0097,-0.0528,0.0745,-0.0424,0.0117,0.0293,0.1794,0.0027,0.0036,0.0237,-0.0138,0.008,0.0323,-0.0154,0.0519,0.0203,0.0549,-0.0265,0.0731,0.0298,-0.0247,0.0799,0.0195,-0.0548,-0.0023,0.0237,0.0069,-0.0441,0.1193,-0.0375,-0.0318,-0.0877,0.0064,0.0711,-0.0322,0.0077,0.0095,0.0196,-0.017,0.0429,-0.0568,-0.0388,-0.0445,-0.0697,-0.0009,-0.0898,-0.0461,-0.0372,-0.0247]}
{"key":"[Missing Data Imputation and Acquisition with Deep Hierarchical Models and Hamiltonian Monte Carlo] Variational Autoencoders (VAEs) have recently been highly successful at imputing and acquiring heterogeneous missing data. However, within this specific application domain, existing VAE methods are restricted by using only one layer of latent variables and strictly Gaussian posterior approximations. To address these limitations, we present HH-VAEM, a Hierarchical VAE model for mixed-type incomplete data that uses Hamiltonian Monte Carlo with automatic hyper-parameter tuning for improved approximate inference. Our experiments show that HH-VAEM outperforms existing baselines in the tasks of missing data imputation and supervised learning with missing features. Finally, we also present a sampling-based approach for efficiently computing the information gain when missing features are to be acquired with HH-VAEM. Our experiments show that this sampling-based approach is superior to alternatives based on Gaussian approximations.","layer":1,"vector":[-0.0249,0.029,0.051,-0.0168,0.0103,0.0307,0.0084,0.0169,0.0187,0.031,0.0293,-0.0763,0.0766,0.0563,0.0285,0.0045,0.0086,0.0441,-0.0712,0.0031,-0.0283,-0.035,-0.0209,-0.0256,0.0584,-0.0278,-0.0188,-0.0412,-0.0414,-0.2863,0.0225,-0.05,0.066,-0.0044,0.0218,-0.0393,-0.0565,0.0316,-0.0304,0.0485,0.0003,0.0098,0.0008,-0.0065,-0.0179,-0.0543,-0.0253,-0.0199,0.014,-0.04,0.0131,-0.0402,-0.0083,0.0252,0.0583,0.0449,0.0465,0.0235,0.0463,0.0751,0.0544,0.0666,-0.1527,0.0495,0.0566,0.0391,-0.0578,-0.0171,0.0432,0.04,-0.0452,0.0375,0.0096,0.0658,-0.0093,-0.0419,0.0182,-0.0739,-0.0372,-0.0136,0.0394,-0.0535,-0.0166,0.0208,-0.0222,-0.0772,0.0199,-0.0565,0.0339,-0.0154,-0.0113,-0.0651,0.0021,0.0453,-0.0512,0.0004,0.0583,0.0402,-0.0753,0.2351,-0.0182,0.0176,0.0449,-0.0228,0.0408,-0.0579,-0.0301,-0.029,-0.0185,0.0247,0.0181,-0.0368,0.0481,-0.0563,0.0249,0.0089,0.0686,0.0288,0.0087,-0.0009,0.0163,-0.0238,0.0794,-0.0109,0.0261,-0.0423,0.0082,0.1404,0.0356,0.0288,0.0405,-0.0279,-0.0427,-0.0612,0.011,-0.0257,0.0088,-0.0136,-0.0199,-0.0299,-0.0283,-0.0551,-0.0139,-0.0668,-0.0696,0.0908,-0.0403,0.0565,-0.0823,-0.036,-0.0274,0.0255,-0.0402,-0.0056,0.028,0.051,0.0139,0.0116,-0.0488,0.0359,-0.006,-0.0734,-0.0477,0.1031,0.0021,-0.0507,-0.0582,0.0474,0.0017,0.018,0.014,0.0293,-0.0343,0.0226,0.1129,0.0061,-0.0751,0.0557,0.0253,0.0395,-0.0154,-0.025,-0.013,0.0775,0.0518,-0.0503,0.0432,-0.0362,0.0016,0.0193,-0.0422,-0.0063,-0.0175,-0.0596,0.0288,-0.0162,-0.0315,-0.0242,0.0268,-0.019,0.0125,-0.0026,-0.0265,0.0391,-0.0048,0.0542,-0.0175,-0.002,0.0857,0.0271,-0.0119,-0.0154,0.0891,-0.0267,-0.0171,-0.0033,0.0146,-0.0015,0.0061,0.013,0.0242,-0.0483,-0.0505,-0.2263,-0.0355,0.0169,-0.0373,0.0164,-0.0613,0.0537,0.0047,0.0647,0.0848,0.0001,-0.0091,-0.0273,0.0532,-0.0275,0.056,0.0236,0.0316,-0.0503,0.0571,-0.0125,0.022,-0.0493,-0.0529,0.0708,0.0077,0.196,0.0226,-0.0016,-0.0414,-0.0306,0.0288,-0.0109,-0.0569,0.0381,-0.0306,0.0397,0.0027,-0.048,-0.0596,-0.0338,0.0297,-0.0182,-0.1127,0.0067,-0.0415,-0.0598,0.0183,-0.0604,0.0277,0.0798,-0.0366,0.0356,-0.0348,-0.0289,-0.0153,-0.1191,0.0207,-0.0326,0.0242,0.0415,-0.0138,-0.01,-0.0512,0.0317,-0.0049,-0.0297,-0.0458,0.0141,-0.0453,-0.013,0.0768,-0.0247,0.058,0.0434,-0.0056,0.0291,-0.023,-0.0333,0.0034,0.0477,-0.0141,0.0248,0.0696,0.058,0.0695,0.0918,-0.0117,0.0433,-0.0085,-0.0091,0.0184,-0.0463,-0.0165,0.0149,0.0001,-0.275,0.0074,0.0417,0.0149,-0.0517,0.008,0.0146,-0.0211,0.0084,-0.0485,-0.001,0.0279,0.0416,-0.0118,0.0076,0.0186,0.078,-0.0115,0.0365,-0.0462,0.0109,0.0464,0.2041,-0.0511,0.0183,0.0231,-0.0066,-0.0027,0.0531,-0.0589,-0.0506,0.0244,0.0978,-0.0206,0.0254,0.0639,-0.0452,0.0827,0.0037,-0.0627,-0.0215,0.0045,-0.0241,-0.0673,0.0571,-0.0159,-0.0031,-0.0191,0.0207,0.0432,-0.0245,-0.0024,-0.0197,0.0108,0.012,0.0272,-0.0265,-0.0493,-0.0142,-0.0132,-0.0206,0.0059,-0.0161,-0.025,-0.0384]}
{"key":"[Just Go with the Flow: Self-Supervised Scene Flow Estimation] When interacting with highly dynamic environments, scene flow allows autonomous systems to reason about the non-rigid motion of multiple independent objects. This is of particular interest in the field of autonomous driving, in which many cars, people, bicycles, and other objects need to be accurately tracked. Current state-of-the-art methods require annotated scene flow data from autonomous driving scenes to train scene flow networks with supervised learning. As an alternative, we present a method of training scene flow that uses two self-supervised losses, based on nearest neighbors and cycle consistency. These self-supervised losses allow us to train our method on large unlabeled autonomous driving datasets; the resulting method matches current state-of-the-art supervised performance using no real world annotations and exceeds state-of-the-art performance when combining our self-supervised approach with supervised learning on a smaller labeled dataset.","layer":2,"vector":[-0.0104,-0.0282,0.0169,-0.0357,0.0143,0.053,0.0406,0.014,0.0062,-0.0069,0.0323,-0.0625,0.0514,0.0476,-0.0029,0.0092,0.0211,0.0789,-0.0117,-0.0321,0.0254,-0.059,0.0007,-0.0415,0.0157,0.0229,-0.0111,-0.0423,-0.0327,-0.2295,-0.0008,-0.033,-0.0022,0.0042,-0.0089,-0.0547,-0.0346,0.0564,-0.0314,-0.0072,0.0278,0.0384,-0.0285,-0.0476,-0.0004,-0.0099,-0.0032,-0.0312,-0.0027,-0.0365,0.0208,-0.0386,0.0423,0.0211,-0.0067,0.0368,0.0325,0.0485,0.0418,0.0121,0.042,0.0434,-0.1722,0.0574,0.0628,0.0424,-0.0515,0.0125,-0.0064,0.0287,-0.016,0.0174,0.0115,0.0145,0.0269,-0.0461,0.0428,-0.0226,-0.0107,-0.0154,-0.007,-0.0259,-0.0684,-0.0256,0.019,-0.0477,0.025,-0.0486,0.0274,-0.002,-0.088,-0.0066,-0.0203,0.0053,-0.045,-0.0037,0.0172,-0.0119,-0.0003,0.1982,-0.046,0.0498,0.0273,-0.0033,0.0091,-0.0443,-0.0489,-0.0315,-0.0242,0.0123,0.0188,-0.0056,0.0451,-0.0616,0.0777,0.0104,0.0687,0.0829,-0.0489,0.049,-0.0205,-0.0299,0.0411,-0.027,0.0346,-0.0736,0.0484,0.1165,0.0526,0.0223,0.0506,0.012,-0.0779,-0.0055,0.0197,0.0157,0.0453,-0.0028,0.0107,-0.0347,-0.0356,-0.041,0.006,-0.0823,-0.0416,0.1057,-0.0044,0.0801,-0.0125,-0.0495,-0.0405,-0.006,-0.0548,-0.0362,-0.0029,0.0476,0.0622,0.0544,-0.08,0.0252,-0.0242,-0.0405,-0.0318,0.0644,0.0181,-0.1077,0.0183,-0.0011,0.0263,0.0032,-0.0051,0.0318,-0.0246,0.0033,0.0858,0.0353,-0.0853,0.0522,-0.0073,0.0514,0.0072,-0.0421,-0.0346,0.0092,0.0396,-0.0142,-0.0211,-0.0525,-0.0043,0.0893,-0.0165,0.0067,0.0225,0.0101,-0.0282,-0.0061,0.0019,-0.0146,-0.0198,0.0013,0.0113,0.0208,0.0001,0.0503,-0.0206,-0.002,-0.0558,-0.0036,0.0132,0.0408,-0.0525,-0.0174,0.0367,0.0049,-0.0102,-0.0071,-0.0097,0.0582,-0.0124,0.0378,0.0266,-0.0266,-0.0383,-0.2497,-0.0225,0.0265,-0.0369,0.0527,-0.0606,0.0273,0.0096,0.0502,0.113,0.0709,-0.0333,-0.0109,0.0317,0.0066,0.0819,0.0206,0.0565,-0.0299,0.0243,0.0087,0.0422,-0.0311,-0.0925,0.0397,-0.0155,0.2384,0.0099,0.043,-0.0234,0.0141,0.0418,-0.0382,-0.0821,0.0327,-0.0012,0.0593,-0.0338,-0.0319,-0.0523,-0.0673,-0.0217,0.0268,-0.078,-0.0642,-0.0435,-0.0578,0.0368,-0.0419,0.0224,0.0427,-0.0494,0.0529,-0.0521,-0.0028,-0.0021,-0.0314,0.0324,-0.0242,0.0274,-0.0386,-0.0328,0.0273,-0.0824,0.1173,-0.0012,-0.0493,-0.0711,-0.0059,-0.0079,-0.0435,0.0822,-0.0073,-0.0286,0.0725,-0.0041,0.0264,0.0146,-0.0593,-0.0513,0.0705,-0.0303,0.0687,0.052,0.0561,0.0525,0.064,-0.0526,0.0094,-0.0573,0.0406,0.0192,-0.0557,-0.0297,0.0503,-0.0235,-0.2846,0.0107,-0.0023,0.0682,-0.0703,0.0335,0.0577,0.0186,-0.0188,-0.0019,-0.0204,0.0554,0.04,-0.004,0.0116,0.0453,0.0943,-0.0137,0.04,-0.0382,0.0253,0.0602,0.1885,-0.0437,0.0268,0.019,-0.0935,-0.0009,0.0608,-0.0377,0.0185,0.0176,0.074,-0.0364,0.0276,0.0199,-0.0145,-0.0111,-0.0011,0.0023,0.0016,0.0031,-0.0159,-0.0762,0.0681,-0.0103,-0.0113,-0.0324,-0.0179,-0.0008,-0.0104,-0.0048,-0.0039,-0.0161,0.0527,0.0275,-0.0597,-0.0635,-0.0604,-0.0501,0.0388,-0.092,0.027,-0.0041,-0.012]}
{"key":"[How COVID-19 Has Changed Crowdfunding: Evidence From GoFundMe] While the long-term effects of COVID-19 are yet to be determined, its immediate impact on crowdfunding is nonetheless significant. This study takes a computational approach to more deeply comprehend this change. Using a unique data set of all the campaigns published over the past two years on GoFundMe, we explore the factors that have led to the successful funding of a crowdfunding project. In particular, we study a corpus of crowdfunded projects, analyzing cover images and other variables commonly present on crowdfunding sites. Furthermore, we construct a classifier and a regression model to assess the significance of features based on XGBoost. In addition, we employ counterfactual analysis to investigate the causality between features and the success of crowdfunding. More importantly, sentiment analysis and the paired sample t-test are performed to examine the differences in crowdfunding campaigns before and after the COVID-19 outbreak that started in March 2020. First, we note that there is significant racial disparity in crowdfunding success. Second, we find that sad emotion expressed through the campaign's description became significant after the COVID-19 outbreak. Considering all these factors, our findings shed light on the impact of COVID-19 on crowdfunding campaigns.","layer":2,"vector":[0.0092,-0.0252,0.0421,0.0449,0.0434,0.0474,0.0361,-0.0129,-0.012,-0.0459,0.0368,-0.0186,0.0137,0.0313,0.0182,0.0078,0.0025,-0.0441,-0.0698,0.063,-0.013,-0.0409,0.0114,-0.0754,0.0238,-0.0092,-0.0308,-0.0162,-0.0932,-0.2023,0.0318,-0.0423,0.0715,-0.0094,0.0579,-0.0011,-0.0374,0.0426,-0.0516,0.0408,-0.0132,-0.0065,-0.0109,-0.0473,-0.022,-0.0877,-0.022,-0.0158,-0.0139,-0.0404,0.0308,-0.0761,-0.0068,0.0267,0.0339,0.0103,0.0436,0.0368,0.0406,0.0225,0.0608,0.0349,-0.2012,0.0569,0.0425,-0.0012,-0.0233,-0.0138,0.0053,0.0554,0.0309,0.0478,0.0119,0.0203,0.0494,0.0234,0.0083,-0.0064,0.0329,-0.0082,0.0625,0.0279,-0.002,-0.0057,-0.0426,-0.028,0.034,-0.025,0.0389,-0.0085,-0.0124,-0.0038,0.0075,0.0125,-0.1035,-0.0179,0.0237,0.0088,0.0038,0.2088,-0.0312,0.0169,0.0228,-0.0314,0.0564,-0.0697,-0.0146,-0.0294,-0.05,0.002,-0.0242,-0.02,0.0769,-0.0202,-0.0007,0.005,0.0102,0.0292,-0.0058,0.0117,-0.0055,0.0218,0.0283,-0.024,0.0004,-0.0543,0.0223,0.1429,0.0557,-0.0297,0.0376,0.0077,-0.1027,-0.0171,0.0197,0.0029,-0.0196,0.034,0.0186,-0.01,0.0009,-0.0265,-0.0151,-0.1136,-0.0899,0.1135,-0.0188,0.0405,-0.0284,0.0127,-0.0226,0.0501,-0.0592,-0.0269,0.01,0.0218,0.0502,0.0514,-0.0251,0.0284,0.0151,-0.0522,-0.0289,0.0704,0.0008,-0.11,-0.0063,0.0101,0.0182,-0.0169,0.0129,0.0169,-0.0164,-0.0112,0.0814,0.009,-0.0763,-0.024,-0.0065,0.0275,0.0474,-0.0061,-0.04,0.0626,-0.0038,-0.0772,-0.0085,-0.0394,0.0223,0.036,0.0035,-0.0126,-0.0616,0.0131,-0.0074,0.0083,-0.0251,-0.0081,0.0406,-0.025,-0.0128,0.0074,-0.0489,0.0116,-0.022,-0.0029,0.0358,0.0051,0.0751,-0.0032,-0.0299,0.0314,-0.012,-0.0046,-0.012,0.0249,0.0176,0.0293,-0.0123,0.0582,0.0487,-0.0269,-0.0671,-0.2381,-0.0367,-0.014,0.005,0.0272,-0.0411,0.0135,-0.0216,0.0282,0.1115,0.0863,-0.0637,-0.017,0.0523,-0.0032,0.0386,-0.0345,0.0043,-0.0167,0.0177,-0.0276,0.0064,-0.0243,-0.0704,0.0198,-0.0022,0.2221,0.0655,-0.0438,-0.0245,0.0259,0.0292,-0.0326,-0.1455,0.1003,0.007,0.0755,0.0094,-0.0976,-0.0152,-0.0255,0.0298,0.0221,-0.0237,-0.016,0.028,-0.0021,-0.0035,-0.0637,0.0577,0.0435,-0.0536,0.0572,0.0375,0.0517,-0.055,-0.0894,0.0493,0.0092,0.0114,0.0316,-0.0515,0.0324,-0.0456,0.0539,-0.0251,-0.0366,-0.0342,0.0075,-0.0145,-0.0341,0.1098,-0.0367,-0.0267,0.0753,-0.0314,0.0391,-0.0403,-0.0365,-0.0102,0.0626,-0.0421,-0.0179,0.0197,0.0421,0.0063,0.059,0.0109,0.0345,-0.03,0.0303,0.0396,-0.0978,-0.0283,0.0072,-0.0056,-0.2889,0.0324,0.0118,0.0186,-0.0041,0.0295,0.0244,0.0251,-0.0462,0.0188,0.0519,0.069,0.049,-0.0677,-0.0234,0.0317,0.0148,-0.0801,0.0632,-0.0734,0.0176,0.0132,0.2213,-0.0558,0.0023,0.0096,-0.0335,0.0152,0.0264,-0.0117,0.0451,0.0071,0.0777,-0.0541,0.0348,-0.0152,-0.0201,0.0184,0.0154,-0.012,-0.0168,0.051,0.0148,-0.0208,0.0596,0.0036,0.0128,-0.0834,0.0019,0.0413,-0.0335,-0.0071,-0.0438,0.0047,-0.0261,0.034,-0.0814,-0.0505,0.0263,-0.0262,-0.0023,-0.0293,-0.0401,0.0345,0.0277]}
{"key":"[Avoiding Jammers: A Reinforcement Learning Approach] This paper investigates the anti-jamming performance of a cognitive radar under a partially observable Markov decision process (POMDP) model. First, we obtain an explicit expression for uncertainty of jammer dynamics, which paves the way for illuminating the performance metric of probability of being jammed for the radar beyond a conventional signal-to-noise ratio ($\\mathsf{SNR}$) based analysis. Considering two frequency hopping strategies developed in the framework of reinforcement learning (RL), this performance metric is analyzed with deep Q-network (DQN) and long short term memory (LSTM) networks under various uncertainty values. Finally, the requirement of the target network in the RL algorithm for both network architectures is replaced with a softmax operator. Simulation results show that this operator improves upon the performance of the traditional target network.","layer":0,"vector":[-0.0786,-0.0469,0.0245,-0.0076,0.0374,0.0527,0.0545,0.0316,0.0593,-0.0088,0.0562,0.0077,0.0404,0.0551,0.0519,0.0585,-0.0138,0.0436,0.0117,-0.0223,0.012,-0.0334,-0.0173,-0.0278,-0.0015,-0.0022,-0.0532,-0.0335,-0.0516,-0.2296,0.032,-0.0383,0.0364,-0.0485,-0.0119,-0.0305,0.0031,0.0784,0.0422,0.0502,0.0619,0.0575,0.0005,-0.0701,-0.0358,-0.0635,-0.0311,-0.0489,0.0013,-0.0839,-0.0212,-0.0231,-0.0147,0.0177,0.0364,0.0012,0.0297,0.0457,0.0613,0.0281,0.0106,0.033,-0.1689,0.0434,0.0099,0.0116,-0.0648,-0.0074,0.0487,0.0385,-0.0228,0.0496,0.0731,0.0286,0.0445,0.0353,-0.0205,-0.0422,-0.0109,-0.0263,0.0066,-0.0391,-0.0479,-0.0468,-0.0223,-0.0449,-0.0143,-0.0577,0.0565,-0.0313,-0.0091,0.0095,-0.0095,0.0064,-0.0415,-0.0075,0.0023,0.0045,-0.0607,0.1819,-0.0011,-0.0086,0.0583,-0.0229,0.0515,-0.0449,-0.043,-0.0029,-0.0656,0.0359,0.0026,-0.0343,0.0556,-0.0095,0.0473,0.0334,0.034,0.0548,0.0233,-0.011,-0.0658,-0.0374,0.0627,0.004,0.034,-0.0601,-0.003,0.1422,-0.0044,0.0312,0.0594,-0.0835,-0.0006,-0.018,0.0356,0.0306,0.0171,0.0383,0.0329,0.0102,-0.0489,-0.0067,0.0259,-0.1434,-0.0511,0.0655,-0.0456,0.022,-0.0067,-0.0179,-0.0391,0.0029,0.0162,-0.0445,0.0066,0.0569,0.068,0.0728,-0.0666,-0.024,-0.0356,-0.0691,-0.026,0.1228,-0.0164,-0.0468,-0.0288,-0.0241,0.0046,-0.0267,0.0168,0.0233,0.0033,-0.0095,0.0743,0.003,-0.0671,-0.0144,-0.0121,0.002,-0.0142,-0.0376,-0.0084,-0.0057,0.0639,-0.0266,0.0124,-0.0444,0.0616,-0.0153,-0.0143,-0.0076,0.0129,-0.005,-0.0452,-0.0666,-0.025,0.0052,-0.0171,-0.0262,-0.0095,-0.0224,-0.0438,0.0173,0.0146,0.036,-0.0014,-0.0235,0.0118,-0.0129,-0.0275,-0.0055,0.0534,0.0001,-0.0187,0.0069,-0.0043,0.0549,0.0017,0.0233,0.0087,0.0021,-0.0815,-0.2325,0.0019,-0.0054,0.0304,0.0684,-0.0479,0.0259,-0.001,0.0557,0.0833,0.0611,-0.0512,-0.0085,0.0058,0.0031,0.0471,-0.0165,0.0298,-0.0173,0.0053,-0.0405,0.0182,-0.0607,-0.0628,0.0836,-0.0393,0.1868,0.0391,0.0584,-0.0003,0.0041,0.0497,-0.0203,-0.0683,0.0329,0.0557,0.0643,-0.0134,-0.0161,-0.0542,-0.0476,-0.0068,-0.0284,-0.0955,-0.0323,-0.0355,-0.0383,0.0569,-0.0292,-0.0119,0.0445,-0.0426,0.0604,0.0026,-0.0322,-0.0809,-0.0739,0.0424,-0.0623,0.0493,0.0128,-0.0287,0.0019,-0.0325,0.0406,-0.0044,0.02,-0.0511,0.0081,-0.0106,0.004,0.0984,0.018,0.0013,0.0557,-0.0224,0.0164,-0.0596,-0.0095,-0.0451,0.0551,-0.0025,0.008,0.0154,0.0022,-0.0141,0.1006,-0.0202,-0.0032,0.0132,0.0004,0.0261,-0.0038,-0.0303,0.0305,-0.0128,-0.3207,0.0599,0.0112,0.0278,-0.024,0.0055,0.0627,0.0497,-0.0874,-0.0069,-0.0262,0.0616,0.0409,-0.0282,0.0319,0.0113,0.0025,-0.0466,0.0387,-0.0428,0.0132,0.0334,0.217,-0.0147,0.0798,0.0044,0.0028,0.0372,-0.0251,-0.034,0.0022,0.0143,0.1007,-0.0478,0.0269,0.0881,-0.0347,0.0691,0.0212,-0.0037,0.0098,0.013,0.0112,-0.0088,0.1317,-0.0226,0.007,-0.0582,-0.0437,0.0343,-0.0492,-0.0057,-0.0331,-0.0213,0.0086,0.0174,-0.0588,-0.0679,-0.0264,0.006,0.0527,-0.066,0.0317,0.0103,0.0156]}
{"key":"[A Deep Reinforcement Learning Approach for Fair Traffic Signal Control] Traffic signal control is one of the most effective methods of traffic management in urban areas. In recent years, traffic control methods based on deep reinforcement learning (DRL) have gained attention due to their ability to exploit real-time traffic data, which is often poorly used by the traditional hand-crafted methods. While most recent DRL-based methods have focused on maximizing the throughput or minimizing the average travel time of the vehicles, the fairness of the traffic signal controllers has often been neglected. This is particularly important as neglecting fairness can lead to situations where some vehicles experience extreme waiting times, or where the throughput of a particular traffic flow is highly impacted by the fluctuations of another conflicting flow at the intersection. In order to address these issues, we introduce two notions of fairness: delay-based and throughput-based fairness, which correspond to the two issues mentioned above. Furthermore, we propose two DRL-based traffic signal control methods for implementing these fairness notions, that can achieve a high throughput as well. We evaluate the performance of our proposed methods using three traffic arrival distributions, and find that our methods outperform the baselines in the tested scenarios.","layer":0,"vector":[-0.0681,-0.0353,0.0273,-0.0095,-0.0049,0.0649,0.0279,-0.0029,0.071,-0.0009,-0.0081,-0.0099,0.0092,0.0344,0.0113,0.0173,-0.0091,0.0162,-0.024,-0.0065,0.0177,-0.0442,-0.0532,-0.0679,-0.009,0.0163,-0.0279,-0.026,-0.0703,-0.2201,0.0077,-0.0534,0.0426,-0.0338,-0.0401,-0.0534,-0.051,0.0603,-0.0157,0.0219,0.0692,0.0037,-0.0113,-0.0488,-0.0125,-0.0437,0.0016,-0.0196,-0.0117,-0.0672,0.0564,-0.0328,-0.0173,0.0282,0.0158,-0.0098,0.0757,0.0488,0.0724,0.037,0.003,0.0158,-0.1907,0.066,0.0381,0.0216,0.0124,-0.0423,0.0041,0.0441,-0.0422,0.0398,0.0217,0.0194,0.0188,-0.0123,0.0149,-0.0468,-0.0082,-0.012,0.0108,-0.0395,-0.0217,-0.011,-0.0099,-0.0656,-0.0161,-0.0553,0.0251,-0.0154,-0.0162,0.0014,-0.0086,0.0284,-0.0681,0.0051,-0.002,0.0036,-0.005,0.1992,-0.0217,0.0373,0.0097,0.0111,0.0371,-0.0063,0.0068,-0.0353,-0.0708,0.0465,-0.0014,-0.0417,0.0639,0.0101,0.0046,0.0363,0.0526,0.0532,-0.0054,0.0133,-0.0097,-0.0054,0.0447,-0.0188,0.0393,-0.0914,0.0183,0.1669,-0.01,0.0324,0.0181,-0.0403,-0.0615,-0.0239,0.016,0.0139,-0.0028,0.0126,-0.0267,-0.0003,-0.0628,-0.011,-0.0215,-0.0854,-0.0171,0.0883,0.0114,0.0289,-0.0366,-0.0522,-0.0073,-0.0091,-0.0234,-0.033,0.0058,0.0523,0.0735,0.0874,-0.0181,0.0338,-0.0171,-0.0207,-0.0378,0.126,-0.0439,-0.0968,-0.0244,0.0078,0.0195,-0.0274,-0.0128,-0.0092,-0.0429,0.0271,0.0971,0.0188,-0.0842,0.0605,-0.0425,-0.0044,-0.0008,-0.0621,-0.0325,0.0176,0.0526,-0.0369,-0.0106,-0.0463,-0.0045,0.0408,-0.0343,-0.0034,-0.0254,0.0116,-0.0241,-0.0744,-0.0119,-0.02,0.0011,0.0005,-0.03,-0.0118,-0.0119,-0.0041,-0.0347,0.0342,-0.0439,-0.0155,0.0449,0.0084,-0.0543,0.0175,0.0625,-0.0043,-0.0182,0.0363,0.0215,0.0284,0.0077,0.0526,0.0443,0.0157,-0.05,-0.1812,-0.0248,-0.0065,-0.0034,0.0807,-0.0419,0.0368,0.0125,0.0171,0.0935,0.085,-0.0151,-0.0114,0.0461,0.0333,0.0661,0.001,0.0806,-0.0075,-0.0293,-0.0227,0.0175,-0.0256,-0.0792,0.0666,0.0097,0.2129,-0.0147,0.0366,-0.0454,0.0343,0.0536,-0.0228,-0.0806,0.0294,-0.0194,0.0765,0.0015,-0.0256,-0.0717,0.0027,0.0049,-0.0468,-0.0764,-0.008,-0.015,-0.0344,0.0709,-0.0156,-0.0599,0.0152,-0.0768,0.0156,0.0121,0.0382,-0.0168,-0.0755,0.0565,-0.0367,-0.0024,0.0228,-0.0513,0.0303,-0.0306,0.0573,0.0442,-0.0009,-0.0988,0.0144,0.0306,-0.0096,0.0973,-0.0214,0.0124,0.05,0.0431,-0.0068,-0.0096,-0.0268,-0.0262,0.0975,-0.0021,0.0264,0.0519,0.0038,-0.0074,0.0504,0.0243,0.0174,0.0128,0.0214,0.013,-0.0249,-0.0541,0.0386,-0.0176,-0.2954,0.0509,0.0021,0.007,-0.0422,0.0081,0.0564,-0.0047,-0.0786,-0.0276,0.0015,0.0808,0.0322,0.0006,0.0326,0.0115,0.0885,-0.0212,0.0209,-0.0656,0.0141,0.0498,0.2299,-0.0635,0.0383,0.0433,-0.0538,0.0056,0.0561,-0.0375,-0.003,0.028,0.0863,-0.064,0.0213,0.0768,-0.0497,0.0564,0.0224,0.0436,-0.0174,0.0538,-0.0014,0.0253,0.0646,0.0098,-0.0707,-0.0398,0.0168,0.0534,-0.0169,-0.0064,-0.0652,-0.0079,0.0511,0.0311,-0.08,-0.0957,-0.0346,-0.0355,0.0127,-0.0914,0.017,0.0038,-0.0195]}
{"key":"[Learning Speaker Embedding from Text-to-Speech] Zero-shot multi-speaker Text-to-Speech (TTS) generates target speaker voices given an input text and the corresponding speaker embedding. In this work, we investigate the effectiveness of the TTS reconstruction objective to improve representation learning for speaker verification. We jointly trained end-to-end Tacotron 2 TTS and speaker embedding networks in a self-supervised fashion. We hypothesize that the embeddings will contain minimal phonetic information since the TTS decoder will obtain that information from the textual input. TTS reconstruction can also be combined with speaker classification to enhance these embeddings further. Once trained, the speaker encoder computes representations for the speaker verification task, while the rest of the TTS blocks are discarded. We investigated training TTS from either manual or ASR-generated transcripts. The latter allows us to train embeddings on datasets without manual transcripts. We compared ASR transcripts and Kaldi phone alignments as TTS inputs, showing that the latter performed better due to their finer resolution. Unsupervised TTS embeddings improved EER by 2.06\\% absolute with regard to i-vectors for the LibriTTS dataset. TTS with speaker classification loss improved EER by 0.28\\% and 0.73\\% absolutely from a model using only speaker classification loss in LibriTTS and Voxceleb1 respectively.","layer":0,"vector":[-0.0719,0.0079,0.0014,-0.025,-0.0363,0.0168,-0.0144,0.0438,-0.0397,-0.0103,-0.0102,-0.0625,0.035,0.0575,0.0512,0.0209,0.026,0.0447,-0.0592,0.0324,0.038,-0.0327,0.0308,-0.039,0.0435,0.0366,-0.0483,-0.0494,-0.0022,-0.2497,0.0399,-0.029,0.0341,-0.0351,-0.0221,-0.0462,-0.0454,0.028,-0.0285,0.024,-0.0161,0.0177,0.0058,-0.0992,-0.0039,-0.0768,-0.054,-0.0109,-0.0111,-0.0382,0.0262,-0.0149,0.0488,0.0399,-0.0249,0.0349,0.0456,0.0952,0.0243,0.0256,0.0193,0.037,-0.2059,0.0866,0.0426,0.0587,-0.0293,0.0092,-0.009,0.0346,0.0056,0.012,0.0331,0.0283,-0.0087,0.0065,0.0757,-0.0165,0.0131,0.03,0.0225,-0.021,0.007,0.0098,-0.0031,-0.0713,0.0441,-0.0288,-0.0274,0.0138,-0.0786,-0.0218,-0.0109,0.032,-0.0647,-0.0417,0.0448,0.0426,-0.0122,0.1797,-0.0264,0.0039,0.0167,-0.0378,0.0147,-0.0669,-0.0382,-0.0083,-0.0332,-0.016,-0.0126,-0.0232,0.0104,-0.017,0.0713,0.0482,0.0919,0.0327,-0.0169,-0.0033,-0.0007,0.0579,0.0064,-0.0214,0.0697,-0.0467,0.0307,0.1327,0.0604,0.0421,0.0272,0.0029,-0.0257,0.0016,0.0212,0.0607,0.0033,-0.0103,0.008,-0.0152,-0.0025,-0.0936,-0.0071,-0.0379,-0.0695,0.1018,-0.0744,-0.009,-0.0557,-0.0192,-0.0077,0.0484,0.0225,-0.0549,0.0049,-0.0155,0.0619,0.039,-0.0447,0.0091,0.0011,-0.0383,-0.0408,0.0671,0.0565,-0.1142,-0.0474,-0.0117,0.012,-0.0547,0.041,-0.0146,-0.033,0.0181,0.0711,0.0161,-0.0898,-0.0361,-0.0125,0.0327,-0.0013,-0.0462,-0.0313,0.046,0.0275,-0.025,-0.034,-0.0629,0.0071,0.052,-0.0049,0.0246,-0.0418,-0.0142,-0.0425,-0.0298,0.0193,0.0128,0.0089,-0.0357,0.0069,0.0269,-0.0368,0.0462,0.0056,0.0189,-0.0085,0.0409,0.0661,0.0537,-0.0391,-0.0233,0.095,-0.006,-0.0333,-0.0313,0.018,0.0144,0.0331,0.0453,-0.0141,-0.0637,-0.0229,-0.2071,-0.0299,0.0118,-0.0135,0.0072,-0.0582,0.0148,0.0048,0.0611,0.0674,0.0284,-0.0158,0.0292,0.0584,-0.0158,0.059,0.0059,0.0303,0.0071,0.0054,0.0035,0.0478,-0.0408,-0.0674,0.0245,-0.0133,0.2382,0.0384,0.0509,-0.0191,0.013,0.04,-0.0286,-0.1154,0.0882,-0.0002,0.039,-0.029,-0.0136,-0.0261,-0.0589,0.0085,0.0262,-0.1039,-0.0567,-0.0285,-0.0926,0.0096,-0.0733,0.0158,0.0689,0.0183,0.0703,0.0153,-0.033,-0.0055,-0.0626,0.036,-0.0392,0.0079,-0.0215,-0.0166,0.0306,-0.0683,0.0194,-0.0158,-0.0417,-0.0387,0.0471,0.0027,-0.0218,0.0496,-0.057,0.0377,0.0407,0.019,0.025,-0.084,-0.0509,-0.0381,0.0461,-0.0025,0.0911,-0.0176,0.0421,0.0634,0.081,0.0352,0.0193,-0.0316,-0.0192,-0.005,0.0026,0.0177,0.0635,-0.0117,-0.288,-0.0178,0.0169,0.0233,-0.0543,0.0064,0.0106,-0.0098,-0.0769,0.0239,-0.006,0.0455,0.0561,-0.0022,-0.0032,0.059,0.1187,-0.0532,0.0228,-0.0265,0.014,0.0121,0.1724,-0.0205,0.013,-0.0182,-0.0236,0.0079,0.0289,-0.0305,0.0263,0.0123,0.0963,-0.0453,-0.0606,0.0649,-0.0487,0.0144,0.0011,0.0091,-0.0048,-0.049,-0.0512,-0.0436,0.0568,-0.0068,0.0353,-0.0008,-0.0037,0.0494,-0.0196,0.0464,-0.0005,-0.0249,0.0313,0.089,-0.0511,-0.0337,-0.0505,-0.0003,-0.0307,-0.03,-0.0329,-0.0044,0.0176]}
{"key":"[Real-Time Illegal Parking Detection System Based on Deep Learning] The increasing illegal parking has become more and more serious. Nowadays the methods of detecting illegally parked vehicles are based on background segmentation. However, this method is weakly robust and sensitive to environment. Benefitting from deep learning, this paper proposes a novel illegal vehicle parking detection system. Illegal vehicles captured by camera are firstly located and classified by the famous Single Shot MultiBox Detector (SSD) algorithm. To improve the performance, we propose to optimize SSD by adjusting the aspect ratio of default box to accommodate with our dataset better. After that, a tracking and analysis of movement is adopted to judge the illegal vehicles in the region of interest (ROI). Experiments show that the system can achieve a 99% accuracy and real-time (25FPS) detection with strong robustness in complex environments.","layer":5,"vector":[-0.0307,-0.0291,0.0361,-0.0415,0.0871,0.0539,0.0601,-0.0256,0.0179,0.0062,0.0283,-0.0229,0.019,0.0832,0.0015,-0.0405,0.0503,0.0242,-0.0069,-0.0187,0.0272,-0.0167,0.0159,-0.0895,-0.0296,0.03,0.0192,-0.0301,-0.076,-0.2135,-0.0143,-0.0218,0.0707,-0.0218,0.0158,-0.0605,-0.0603,0.0513,-0.0316,0.0153,0.0351,0.0229,-0.0424,-0.0602,-0.034,-0.059,-0.0189,-0.0188,0.0048,-0.055,0.0314,-0.0327,0.0471,0.0485,-0.0278,0.0127,0.0565,0.0213,0.0229,0.0074,0.0485,0.0395,-0.2122,0.0131,0.0569,0.003,-0.0106,-0.0124,0.0417,0.0291,-0.0227,0.0363,0.0094,0.0326,-0.0248,-0.0314,-0.0285,0.0005,-0.0045,-0.0217,-0.0021,-0.0365,-0.0048,0.0114,-0.0193,-0.0211,-0.0207,-0.0635,0.0462,0.0027,-0.0349,-0.0061,-0.0066,0.0152,-0.0446,-0.0169,0.0051,-0.0199,-0.0297,0.1956,-0.0361,0.0741,0.0226,-0.0448,-0.0043,-0.0149,-0.0288,-0.0497,-0.0383,0.0176,0.0078,-0.0407,0.0177,0.0028,0.0231,0.0288,0.0694,0.0438,0.0208,0.0065,-0.0369,-0.0172,0.053,-0.0575,0.0151,-0.0733,0.0334,0.1257,0.0028,0.0527,-0.0052,-0.0345,-0.0624,-0.0316,0.0426,0.0863,0.0057,0.0447,0.0307,-0.0395,-0.0673,-0.056,0.0545,-0.106,-0.0144,0.0563,-0.0562,0.021,-0.037,-0.0517,-0.0176,0.006,-0.0288,-0.0207,0.0583,0.0094,0.08,0.0792,-0.0355,-0.0009,-0.0275,-0.0251,-0.0613,0.09,0.019,-0.1206,-0.0101,-0.0035,0.0084,-0.0508,0.0279,0.0267,-0.0395,0.0472,0.0821,0.0183,-0.0855,0.0474,-0.014,0.0043,0.0203,-0.0699,-0.0671,0.0362,0.0702,-0.0362,-0.0247,-0.0196,0.0151,0.0968,-0.0273,0.0332,-0.056,-0.0181,-0.0114,-0.0075,-0.0147,-0.0308,0.0037,0.0027,0.0398,-0.0308,-0.0359,-0.0062,0.0111,0.0034,-0.0396,-0.0368,-0.0137,0.0467,0.0025,-0.002,0.037,-0.0187,0.0116,-0.0258,0.0181,0.0402,-0.0022,0.0468,0.0075,-0.0123,-0.0698,-0.2204,0.0079,-0.0482,0.0084,0.0189,-0.0494,0.0403,-0.026,0.0605,0.0603,0.0954,-0.0671,0.0184,0.0563,0.0431,0.109,0.0048,0.061,-0.0241,-0.032,-0.0054,0.0315,-0.0371,-0.0716,0.1152,0.0059,0.2113,0.0255,0.0566,-0.0284,0.0335,-0.0138,-0.0067,-0.1022,0.0961,-0.0197,0.03,-0.005,-0.0469,-0.0494,-0.0512,0.017,-0.0003,-0.07,-0.0249,-0.0247,-0.0311,0.0842,-0.0209,0.0128,0.0369,-0.0409,0.0241,0.0426,0.0075,0.0109,-0.062,0.0454,0.0004,0.0253,-0.0261,-0.07,0.0596,-0.0337,0.0789,-0.0016,-0.0613,-0.0631,-0.044,-0.0051,-0.0075,0.1007,0.0084,-0.0203,0.0841,0.0015,0.0444,0.0099,-0.0262,-0.0749,0.0416,0.0218,-0.034,0.0303,0.0392,0.0215,0.0687,-0.0046,-0.0091,-0.0208,0.0657,0.0037,-0.0643,-0.0239,-0.0025,0.0098,-0.2979,0.0192,0.0127,0.0372,-0.0077,0.0166,0.0351,0.0422,-0.0142,-0.0191,-0.0496,0.0396,0.025,-0.0221,-0.0184,-0.0156,0.0529,-0.0153,0.0656,-0.0808,0.0289,0.0707,0.1935,-0.0412,0.0082,0.0197,0.0138,0.0109,0.0106,-0.0643,-0.0365,-0.0025,0.0525,-0.0349,-0.0074,0.0728,-0.005,0.0632,0.013,0.0243,-0.0258,0.0424,-0.0082,-0.0321,0.0651,-0.0493,-0.0002,-0.0266,0.039,0.0337,-0.0163,0.0035,-0.0305,-0.0024,0.0471,0.0286,-0.0395,-0.0511,-0.0384,-0.0165,0.0245,-0.0415,-0.0018,-0.0097,0.013]}
{"key":"[AttentionHTR: Handwritten Text Recognition Based on Attention Encoder-Decoder Networks] This work proposes an attention-based sequence-to-sequence model for handwritten word recognition and explores transfer learning for data-efficient training of HTR systems. To overcome training data scarcity, this work leverages models pre-trained on scene text images as a starting point towards tailoring the handwriting recognition models. ResNet feature extraction and bidirectional LSTM-based sequence modeling stages together form an encoder. The prediction stage consists of a decoder and a content-based attention mechanism. The effectiveness of the proposed end-to-end HTR system has been empirically evaluated on a novel multi-writer dataset Imgur5K and the IAM dataset. The experimental results evaluate the performance of the HTR framework, further supported by an in-depth analysis of the error cases. Source code and pre-trained models are available at https://github.com/dmitrijsk/AttentionHTR.","layer":3,"vector":[-0.0554,0.013,0.0092,-0.028,0.0139,0.0401,0.0209,0.0378,0.006,-0.0394,-0.0064,-0.0201,0.0329,0.0866,-0.042,-0.0301,0.0114,0.0225,-0.0339,-0.0128,-0.006,-0.0209,-0.0065,-0.0006,-0.036,0.039,-0.0249,-0.032,-0.0843,-0.244,0.0077,-0.0338,0.0447,-0.0318,0.0111,-0.0084,-0.0728,0.0204,-0.0285,0.0146,0.0099,0.01,-0.0102,-0.0356,-0.0463,-0.0691,0.0008,-0.0822,-0.0163,-0.0644,0.018,-0.0212,0.0188,0.0201,0.0336,0.0126,0.0394,0.0313,0.0247,0.0443,0.0493,0.0627,-0.196,0.0622,0.0172,0.029,-0.0686,-0.0121,0.0103,0.0341,-0.0119,-0.0027,0.0051,0.0259,0.0056,-0.0147,-0.0005,-0.0585,0.0081,-0.0435,0.041,0.0015,-0.0259,-0.0088,-0.014,-0.0205,-0.0192,-0.0459,0.0004,-0.0297,-0.0403,-0.0327,-0.0028,-0.0005,-0.0563,-0.038,0.0273,0.0374,-0.058,0.1611,-0.0845,-0.0041,0.0257,-0.0782,0.0521,-0.0126,0.0101,-0.0144,-0.0586,-0.0118,-0.0208,-0.0253,0.0324,0.0027,0.0634,-0.005,0.0592,0.0613,-0.0256,-0.0045,-0.0084,0.045,-0.0257,-0.0276,-0.0058,-0.0587,0.038,0.1227,0.056,0.016,0.0525,0.0061,-0.0339,-0.0567,0.0023,0.0323,0.0372,-0.012,0.052,-0.0675,-0.0031,-0.0456,0.0144,-0.044,-0.0481,0.1249,-0.0343,0.0188,-0.0416,-0.0148,0.0068,0.0088,-0.0196,-0.0411,0.0257,0.0357,0.0772,0.0041,-0.0649,0.0091,0.0188,-0.0179,-0.05,0.0568,-0.0135,-0.1121,-0.0181,-0.0049,0.0035,-0.0196,0.037,0.0249,-0.0266,0.0637,0.0965,0.0775,-0.0483,0.0241,0.0086,0.0253,0.0478,-0.0527,-0.027,0.0314,0.0412,-0.0376,0.0268,-0.0613,0.0374,0.0404,0.019,0.0517,-0.007,-0.0109,-0.0437,-0.034,-0.0378,0.0119,-0.0005,-0.073,0.0226,0.0735,0.0013,-0.0017,0.007,0.0194,-0.0032,0.0155,0.0192,-0.0049,-0.0675,0.0529,0.0674,-0.0369,-0.0369,-0.0251,0.01,0.0429,0.0387,0.0539,0.0106,-0.0576,-0.0497,-0.2604,0.0013,0.0631,-0.0526,0.0666,-0.071,0.015,0.0018,0.0816,0.0716,0.0674,-0.0385,-0.0039,0.0089,-0.0129,0.0669,0.0229,0.0037,0.0034,-0.0165,-0.0191,0.0041,0.0029,-0.0464,0.0614,0.0004,0.195,0.0017,0.0461,-0.0006,-0.0026,0.0401,-0.0319,-0.089,0.0793,0.0052,0.0453,0.0016,-0.0206,-0.0393,-0.0373,0.0103,0.0097,-0.0852,-0.0496,0.0125,-0.0978,0.01,-0.0253,0.0602,0.1071,-0.0081,0.0356,0.0436,-0.0364,0.0134,-0.0886,0.0025,-0.0555,0.0231,0.0218,-0.0305,0.0155,-0.0717,0.0288,0.0391,-0.0033,-0.0186,0.019,-0.0104,-0.0439,0.08,0.0592,0.0344,0.0538,0.0131,0.0697,-0.0402,-0.0293,-0.0523,0.0646,-0.0168,0.0583,0.0011,0.0462,0.0518,0.0883,-0.0211,-0.0186,0.0218,-0.0043,0.0292,-0.0253,0.0057,0.0121,-0.0212,-0.3278,0.0224,-0.0057,0.0498,0.017,0.0046,0.0171,0.0059,-0.0358,0.0168,-0.0109,0.0387,0.0347,-0.0668,-0.0381,0.0244,0.0393,-0.0331,-0.0052,-0.0636,0.007,0.0308,0.1805,-0.0349,0.0264,-0.042,-0.0308,-0.0015,0.0196,-0.0222,0.0571,0.0022,0.0794,-0.0225,0.0411,0.094,-0.0163,0.0568,0.0297,0.0136,0.0176,0.0266,-0.065,-0.0948,0.1022,0.0405,0.0056,-0.0448,-0.0311,0.0666,-0.0298,0.0024,-0.0075,0.041,0.0404,0.0361,-0.0339,-0.0499,-0.0216,-0.0152,0.0193,-0.0544,-0.0213,0.0229,-0.0275]}
{"key":"[An Effective and Efficient Approach for Clusterability Evaluation] Clustering is an essential data mining tool that aims to discover inherent cluster structure in data. As such, the study of clusterability, which evaluates whether data possesses such structure, is an integral part of cluster analysis. Yet, despite their central role in the theory and application of clustering, current notions of clusterability fall short in two crucial aspects that render them impractical; most are computationally infeasible and others fail to classify the structure of real datasets. In this paper, we propose a novel approach to clusterability evaluation that is both computationally efficient and successfully captures the structure of real data. Our method applies multimodality tests to the (one-dimensional) set of pairwise distances based on the original, potentially high-dimensional data. We present extensive analyses of our approach for both the Dip and Silverman multimodality tests on real data as well as 17,000 simulations, demonstrating the success of our approach as the first practical notion of clusterability.","layer":0,"vector":[0.0084,-0.027,0.0085,0.0018,0.0439,0.0385,0.0416,0.0167,-0.025,-0.0362,0.0017,-0.0867,0.0125,0.0697,0.018,0.0184,0.0201,0.0313,-0.0644,-0.0005,0.0392,-0.0201,-0.0249,-0.0394,0.0593,0.085,-0.0039,-0.0134,-0.089,-0.2675,-0.0267,-0.0129,0.0811,0.0019,-0.0129,0.0219,-0.0153,0.069,-0.026,-0.002,0.0289,-0.0057,-0.0346,-0.0405,-0.0771,-0.0345,-0.0391,0.0161,-0.0305,-0.0321,0.0025,-0.0482,0.0067,0.041,0.0222,0.036,0.0368,0.0301,0.0139,0.0095,0.0561,0.0479,-0.1275,0.0329,0.0794,0.0217,0.0003,-0.0214,0.0033,0.0223,0.0172,0.0547,-0.0248,0.0375,0.0449,0.0138,-0.0127,0.0139,-0.0254,0.0107,-0.0271,-0.048,-0.054,0.0142,0.0048,-0.0308,-0.0132,-0.0464,0.0231,-0.0014,-0.0057,0.0355,-0.0302,0.051,-0.0796,-0.0436,0.0412,-0.0079,-0.0204,0.1796,-0.091,0.0524,0.0552,-0.0468,0.0087,-0.0398,-0.0006,-0.0879,-0.0027,-0.016,-0.0133,-0.0163,0.0132,-0.0574,0.0494,0.0045,0.0541,0.0283,0.0086,-0.0565,-0.0134,0.0117,0.0679,0.0061,0.0588,-0.0514,0.0392,0.1169,0.0424,-0.0066,0.0402,0.0388,-0.0574,-0.0089,0.0102,0.0495,0.0022,-0.0239,0.0206,0.017,-0.0265,-0.0934,0.0419,-0.0722,-0.0576,0.1276,-0.0524,0.0078,-0.0575,-0.0139,-0.0315,-0.0034,-0.0755,-0.0665,0.0067,0.0155,0.0474,0.0464,-0.0298,0.0372,-0.013,-0.0506,-0.0152,0.1068,0.0213,-0.0799,-0.0201,0.0048,0.0299,0.0,0.0574,0.0547,-0.0286,0.0492,0.0708,0.0047,-0.0629,-0.0058,-0.0115,0.0206,0.0214,0.0253,-0.0323,0.061,0.0486,-0.0635,-0.0186,-0.0047,0.0538,0.0686,0.0032,-0.0133,-0.0187,-0.0087,-0.0344,-0.0452,0.0237,-0.0418,-0.0066,-0.0287,0.011,0.0128,-0.0468,0.0331,0.0005,0.0265,-0.0003,-0.0084,0.0734,0.0266,-0.0326,0.0015,0.0279,-0.0395,-0.0155,-0.0116,0.05,0.0468,-0.0019,0.0786,0.0639,-0.024,-0.0801,-0.2228,-0.0084,0.0294,0.0076,0.0312,-0.0469,0.0186,0.0015,0.0585,0.0728,0.0356,-0.0147,-0.0652,0.0667,-0.0134,0.0308,0.0062,0.0241,-0.0698,0.024,0.0001,0.0286,-0.0158,-0.0675,0.0131,-0.0036,0.2082,-0.0379,-0.0028,0.0026,0.0382,0.0509,-0.0338,-0.1284,0.0559,0.0415,0.0377,-0.019,-0.024,-0.0364,-0.0395,0.0326,-0.0228,-0.102,-0.0159,-0.0016,-0.0242,0.0192,-0.0588,-0.0167,0.0231,-0.0132,0.0339,-0.0089,0.0099,-0.0777,-0.0764,0.0095,-0.0267,0.0348,0.0166,-0.057,0.0289,-0.0902,0.0672,-0.0424,-0.0567,-0.0136,-0.0205,-0.0663,-0.0415,0.0933,-0.0014,-0.0404,0.0607,-0.0186,0.0193,0.0006,0.0008,-0.0102,0.0697,0.01,0.0014,0.0305,-0.0244,-0.0065,0.0939,-0.0031,0.0563,-0.036,0.0295,-0.0216,-0.0863,-0.0385,0.0215,0.0188,-0.2766,0.0485,0.0043,0.0174,-0.0232,-0.0372,0.0104,0.0435,-0.0105,0.0101,0.059,0.0381,0.0443,-0.1005,0.0003,0.0359,0.0419,-0.0332,0.0333,-0.0728,0.0205,0.0517,0.2262,-0.0395,-0.0075,0.0541,0.0235,-0.0043,0.0151,-0.0055,-0.0055,-0.0284,0.0772,-0.0456,0.0178,0.059,-0.027,0.0238,0.0347,-0.0442,0.0444,0.0439,-0.0406,-0.0473,0.1167,0.0271,0.0082,-0.0677,0.0117,0.0279,-0.0703,-0.0026,-0.0391,-0.0062,0.0357,0.0218,0.0074,-0.0088,-0.029,-0.0455,-0.0203,-0.059,-0.0174,0.0108,0.0332]}
{"key":"[Riemannian Tensor Completion with Side Information] By restricting the iterate on a nonlinear manifold, the recently proposed Riemannian optimization methods prove to be both efficient and effective in low rank tensor completion problems. However, existing methods fail to exploit the easily accessible side information, due to their format mismatch. Consequently, there is still room for improvement in such methods. To fill the gap, in this paper, a novel Riemannian model is proposed to organically integrate the original model and the side information by overcoming their inconsistency. For this particular model, an efficient Riemannian conjugate gradient descent solver is devised based on a new metric that captures the curvature of the objective.Numerical experiments suggest that our solver is more accurate than the state-of-the-art without compromising the efficiency.","layer":0,"vector":[-0.0402,-0.0437,0.0333,0.0128,-0.038,0.0253,-0.0332,0.0678,-0.0042,-0.0365,0.0168,-0.0776,0.031,0.0148,0.0459,-0.0013,0.013,0.1111,-0.0461,0.0337,0.0331,-0.0608,-0.0641,-0.0586,0.0274,-0.0143,-0.0043,-0.004,-0.0496,-0.2485,0.0167,-0.0476,0.0522,-0.0342,-0.0232,0.0012,-0.0482,0.0573,-0.0568,0.0635,0.0023,0.031,-0.0022,-0.0107,-0.0078,-0.0443,-0.0042,-0.0123,0.0065,-0.0281,0.004,-0.0626,-0.032,0.0183,0.0251,0.055,0.0453,0.0066,0.026,0.0514,0.0437,0.0153,-0.1624,0.0514,0.0344,0.0334,-0.0148,-0.0636,0.0213,0.1129,-0.047,0.0405,0.0176,0.011,0.0393,-0.0026,0.0443,-0.0192,-0.0144,0.0286,0.0021,-0.0155,-0.0556,0.0068,-0.0206,-0.0135,0.0251,-0.0632,0.0466,0.04,-0.0154,-0.0499,-0.0612,0.0669,-0.0425,-0.003,0.0531,0.0963,-0.0355,0.1783,-0.0533,0.0403,0.0571,0.0026,0.024,-0.0238,0.0043,0.0061,-0.0053,0.0077,0.0073,-0.0072,0.0453,0.0136,0.0026,0.0397,0.0354,0.0702,-0.0175,0.0371,-0.0398,0.0109,0.0754,-0.0427,-0.0127,-0.0424,-0.0285,0.0968,0.0259,0.0659,0.0765,-0.004,0.0094,-0.0478,-0.0153,-0.0143,-0.0284,0.0089,-0.0205,0.0116,-0.0761,-0.0846,0.0011,-0.1269,-0.0204,0.1508,-0.0686,0.0141,-0.0541,-0.0409,-0.004,0.0421,-0.06,0.0114,0.0126,-0.0134,0.0011,0.0276,-0.0971,0.0097,-0.0223,-0.0645,-0.029,0.1264,0.0117,-0.0913,-0.0121,0.0348,0.0276,-0.0105,0.0228,0.0177,-0.0133,0.0042,0.1132,0.0144,-0.079,0.008,-0.013,-0.0037,0.0255,-0.0356,-0.0363,-0.027,0.0036,-0.0584,-0.0097,-0.0183,0.044,0.0045,-0.0368,0.0312,-0.0166,-0.0417,-0.0405,-0.0377,-0.0295,-0.0508,0.0252,0.0063,0.0127,-0.0312,-0.0084,0.0388,0.0329,0.0597,0.0071,-0.0243,-0.0062,0.0735,-0.0175,-0.0746,0.0439,-0.0316,-0.0242,-0.0141,0.0315,-0.0025,-0.0206,0.0475,0.0275,-0.0221,-0.0464,-0.2169,-0.0134,0.0042,-0.0114,0.0423,-0.1004,0.064,0.0379,0.0147,0.0255,0.0795,0.0142,-0.0496,0.0312,-0.0058,0.0382,0.0227,0.0211,-0.0152,-0.0246,0.013,0.023,-0.0003,-0.0482,0.0327,-0.0424,0.2228,0.0799,0.0215,-0.0445,-0.0086,0.0098,-0.0337,-0.0868,0.0477,0.0202,0.0655,-0.0435,-0.0249,-0.0364,-0.0537,0.0695,0.0037,-0.0421,-0.0031,-0.0496,-0.0526,0.0001,-0.0286,0.0437,0.0771,-0.0561,0.0225,-0.021,-0.0159,-0.0332,-0.0899,-0.024,-0.0161,0.0689,-0.0251,-0.0455,0.0209,-0.0473,0.0676,0.0228,-0.0117,-0.013,0.0507,-0.0529,-0.0025,0.0784,0.0087,0.0187,0.0684,0.0254,0.0547,0.0452,0.0139,-0.0197,0.0346,-0.0467,0.0342,0.0439,0.0496,-0.0057,0.0843,-0.0291,-0.0086,-0.0781,-0.0083,-0.0159,-0.0209,0.0202,0.0429,0.0146,-0.2727,0.015,0.0099,-0.0063,-0.0238,-0.0213,0.0642,0.0119,-0.044,-0.0379,0.0029,0.0348,-0.0034,-0.0315,0.0487,0.0281,0.0494,-0.0692,0.0438,-0.0616,-0.0021,0.0427,0.2245,-0.0308,0.0243,0.0455,-0.036,0.0205,0.0608,-0.0128,-0.0126,0.0552,0.0713,-0.0547,0.0311,0.0566,-0.0296,0.0338,0.03,-0.0091,0.0629,-0.011,-0.0153,-0.0178,0.0424,-0.0049,-0.0385,-0.0197,0.0118,0.0039,0.0196,0.0342,-0.0001,0.0091,-0.004,0.0184,-0.0591,-0.0465,-0.0214,0.0,-0.028,-0.0945,-0.0406,-0.0172,-0.0108]}
{"key":"[Local Rademacher Complexity-based Learning Guarantees for Multi-Task Learning] We show a Talagrand-type concentration inequality for Multi-Task Learning (MTL), using which we establish sharp excess risk bounds for MTL in terms of distribution- and data-dependent versions of the Local Rademacher Complexity (LRC). We also give a new bound on the LRC for norm regularized as well as strongly convex hypothesis classes, which applies not only to MTL but also to the standard i.i.d. setting. Combining both results, one can now easily derive fast-rate bounds on the excess risk for many prominent MTL methods, including---as we demonstrate---Schatten-norm, group-norm, and graph-regularized MTL. The derived bounds reflect a relationship akeen to a conservation law of asymptotic convergence rates. This very relationship allows for trading off slower rates w.r.t. the number of tasks for faster rates with respect to the number of available samples per task, when compared to the rates obtained via a traditional, global Rademacher analysis.","layer":2,"vector":[-0.0211,-0.0152,0.0079,-0.0036,-0.0288,0.0589,0.0433,0.0301,0.0304,-0.0394,-0.0184,-0.0448,0.0375,0.1102,0.0658,0.0672,-0.0015,0.0103,-0.0596,-0.0012,0.0348,-0.0702,0.0136,-0.0627,0.0253,-0.0129,-0.0315,-0.0781,-0.0079,-0.2866,0.0516,-0.0162,0.0337,0.0027,0.0449,-0.0311,-0.031,0.0233,-0.0681,0.0756,-0.0024,0.0817,-0.0439,-0.0855,-0.0335,-0.0499,-0.0457,-0.0184,-0.007,-0.0065,0.0188,-0.0693,0.0446,0.0526,0.0341,0.0112,-0.0057,0.0365,0.0417,0.08,-0.0046,0.0454,-0.192,0.0314,0.0337,0.0303,-0.0595,-0.0105,0.007,0.049,-0.0073,0.0222,0.0046,0.0413,0.01,0.0034,0.0088,-0.0382,-0.026,0.0249,0.0081,-0.0723,-0.0428,-0.0096,-0.0586,-0.0674,0.02,-0.0285,0.0291,0.0164,-0.0167,-0.0551,-0.0009,0.0009,-0.0642,-0.012,0.063,0.0403,-0.0265,0.1489,-0.0662,0.0308,0.0093,-0.025,0.014,-0.0121,0.0061,-0.0272,-0.0324,-0.0105,-0.024,-0.0259,0.0451,-0.0489,0.0413,0.0464,0.0704,0.0196,-0.0001,-0.0325,-0.0296,0.0473,0.0627,-0.0105,0.0293,-0.0453,0.0009,0.1081,0.0715,0.0181,0.0146,-0.0272,-0.014,-0.0171,0.0294,0.006,0.0017,0.0007,0.017,-0.0065,-0.0253,-0.0317,0.0278,-0.0625,-0.0266,0.1139,-0.0211,0.0357,-0.0325,-0.0542,-0.0483,0.0117,-0.0335,-0.0412,-0.0042,0.0055,0.0695,0.0209,-0.071,0.0061,-0.0327,-0.0663,0.0318,0.1297,-0.0321,-0.0317,-0.0266,-0.0486,0.0125,0.0149,0.045,0.0244,-0.0307,0.0487,0.0723,0.0248,-0.1378,-0.0021,0.0471,-0.0171,0.0318,-0.0416,-0.0159,0.0345,0.0314,-0.0096,0.0479,-0.0256,0.0045,0.0132,-0.0253,0.024,-0.0029,-0.0179,-0.0074,-0.0165,0.0065,-0.0263,0.0278,-0.0124,-0.0183,-0.0293,-0.0489,0.0594,0.0174,0.0191,-0.0032,0.0403,0.0491,0.0415,-0.042,-0.0338,0.022,-0.0144,-0.0277,0.034,0.0689,0.0247,-0.0151,0.0311,0.0281,-0.0163,-0.0757,-0.2115,-0.023,0.0452,0.0159,0.0394,-0.0681,0.0362,-0.02,0.0463,0.0686,0.0554,-0.0552,-0.0282,0.0492,0.0291,0.055,0.0363,0.0221,-0.0323,-0.0105,-0.0273,0.0247,-0.0065,-0.0401,0.0674,-0.036,0.2067,0.0006,-0.0057,-0.0608,0.0131,0.0278,-0.0099,-0.0515,0.0425,0.0142,0.0418,-0.0307,0.0082,-0.0069,-0.0142,0.0431,-0.0208,-0.1312,-0.1062,-0.0134,-0.0153,0.0229,-0.0587,-0.0069,0.0514,-0.0312,0.0606,-0.0182,-0.0206,-0.0047,-0.1162,0.0343,-0.0837,0.0393,0.0183,-0.0346,0.0069,-0.0323,0.0799,0.0046,-0.0199,-0.0402,0.0237,-0.0206,-0.0144,0.0526,0.0103,-0.0014,0.0039,-0.0061,0.0203,-0.0079,-0.001,-0.0287,0.1122,-0.052,0.0431,0.0072,0.0051,-0.0089,0.1028,0.0089,0.0188,-0.019,0.0183,-0.0197,-0.0431,0.0145,0.0505,-0.0013,-0.2974,0.0352,-0.0076,0.0229,-0.0376,-0.0224,0.0576,-0.014,-0.0603,0.0167,0.0092,0.0758,0.0225,0.0052,-0.0039,0.0614,0.085,-0.0113,0.0182,-0.0289,0.0132,0.0912,0.1931,-0.0396,0.0299,0.0243,-0.0236,-0.0162,0.0171,-0.0348,0.0266,-0.0137,0.0739,-0.0608,0.037,0.0796,-0.031,0.0563,0.0286,0.0034,-0.0072,-0.0217,-0.0304,-0.0085,0.0942,-0.0279,0.0036,-0.0779,0.0017,0.0465,-0.011,0.0036,-0.0148,-0.0366,0.0353,-0.007,-0.0074,-0.0864,-0.0601,-0.0517,0.0184,0.007,-0.0396,-0.0036,-0.0252]}
{"key":"[Robust Anytime Learning of Markov Decision Processes] Markov decision processes (MDPs) are formal models commonly used in sequential decision-making. MDPs capture the stochasticity that may arise, for instance, from imprecise actuators via probabilities in the transition function. However, in data-driven applications, deriving precise probabilities from (limited) data introduces statistical errors that may lead to unexpected or undesirable outcomes. Uncertain MDPs (uMDPs) do not require precise probabilities but instead use so-called uncertainty sets in the transitions, accounting for such limited data. Tools from the formal verification community efficiently compute robust policies that provably adhere to formal specifications, like safety constraints, under the worst-case instance in the uncertainty set. We continuously learn the transition probabilities of an MDP in a robust anytime-learning approach that combines a dedicated Bayesian inference scheme with the computation of robust policies. In particular, our method (1) approximates probabilities as intervals, (2) adapts to new data that may be inconsistent with an intermediate model, and (3) may be stopped at any time to compute a robust policy on the uMDP that faithfully captures the data so far. We show the effectiveness of our approach and compare it to robust policies computed on uMDPs learned by the UCRL2 reinforcement learning algorithm in an experimental evaluation on several benchmarks.","layer":2,"vector":[-0.0871,-0.0303,-0.0013,-0.0631,0.0082,0.0714,0.0653,0.0337,0.047,-0.012,0.0228,-0.0478,0.0268,0.0662,-0.022,0.0474,-0.04,0.0676,-0.0073,-0.0039,0.0427,-0.0597,-0.0232,-0.0538,0.0299,0.0287,-0.0137,-0.042,-0.0348,-0.2351,0.0193,-0.0654,0.0029,-0.0489,0.0408,-0.0238,-0.0748,0.059,0.0299,0.0158,0.0278,0.0543,-0.0145,-0.0965,-0.0398,-0.0817,0.0423,-0.0326,-0.0324,-0.043,-0.0028,-0.0157,0.0412,0.0079,0.0734,0.0308,0.0735,0.0718,0.0439,0.0387,-0.0303,0.0283,-0.1418,0.0889,0.0109,0.026,-0.079,-0.0108,0.0206,0.0346,-0.0338,0.0254,-0.004,0.0715,0.0133,-0.0345,0.0355,-0.0506,-0.0106,0.0012,-0.012,-0.0492,-0.0444,0.0101,-0.0337,-0.0392,-0.0176,-0.0557,0.0444,0.0147,-0.018,-0.0163,-0.0336,0.0046,-0.0496,0.0257,0.0788,0.0138,-0.0073,0.183,-0.0002,0.0492,-0.0078,-0.0082,0.0442,-0.0232,-0.0367,-0.0264,-0.0001,-0.0013,0.0038,0.0023,0.0412,-0.0316,-0.0265,0.0523,0.0582,0.0021,-0.009,0.0061,-0.0423,0.0334,0.0821,-0.0223,0.0217,-0.0783,0.0211,0.1453,0.0013,-0.0043,0.0713,-0.0774,-0.0343,-0.0155,0.0375,0.0002,0.0154,-0.0138,0.037,-0.0381,-0.0373,-0.0093,0.0016,-0.1136,-0.0624,0.1304,-0.0244,0.0465,-0.0363,-0.0174,0.0129,-0.0127,0.0158,-0.0307,0.0368,0.0273,0.0228,0.0373,-0.0247,-0.038,-0.0098,-0.0729,-0.0153,0.104,-0.0336,-0.0449,-0.029,-0.0232,0.0129,0.0107,0.0409,0.06,-0.0173,0.0083,0.0519,-0.0123,-0.055,0.006,0.0366,-0.0007,-0.0064,-0.0201,0.0126,-0.0232,0.0401,0.0056,-0.0077,-0.0334,0.0314,0.021,-0.0108,-0.0078,0.0014,-0.0294,-0.0367,-0.0536,-0.0143,-0.0098,0.028,-0.0392,-0.0018,-0.0412,-0.0778,0.0281,-0.0101,0.0273,-0.0588,0.0292,0.0359,0.0246,-0.0299,0.019,0.0387,-0.0013,0.0086,0.015,0.0403,0.0521,-0.0181,-0.0104,0.0325,0.0072,-0.0177,-0.2029,0.0117,-0.0093,-0.0009,0.0602,-0.0315,0.0054,-0.0799,0.0105,0.0606,0.0579,-0.0313,-0.0602,0.0164,-0.0439,0.0388,0.0261,0.0089,-0.0485,0.041,-0.0482,0.0124,-0.0723,-0.1089,0.0412,-0.0076,0.2304,0.0051,0.07,-0.0262,0.0263,0.0383,-0.0176,-0.0772,0.069,0.0298,0.0574,-0.0141,0.0139,-0.0622,0.0047,0.0224,-0.0449,-0.1001,-0.0412,-0.0475,-0.0228,0.0714,-0.0394,0.0081,0.0158,0.0045,0.038,-0.0368,-0.0314,-0.0149,-0.0647,0.0472,-0.0059,0.012,0.0504,-0.0206,-0.0014,-0.0313,0.038,-0.0326,0.0145,-0.0618,0.05,-0.0353,-0.0145,0.0873,0.0002,-0.0118,0.0286,0.011,-0.0281,-0.0533,-0.0807,-0.0296,0.0631,-0.0514,0.0365,0.0484,0.0444,-0.0272,0.0609,0.0138,0.0271,-0.0294,0.0156,-0.0167,-0.0352,0.0264,0.0639,0.0189,-0.3002,0.0353,-0.0019,0.0242,-0.035,-0.0015,0.0574,0.0011,-0.0582,-0.0258,0.0075,0.0616,0.0646,0.0224,-0.0052,0.0366,0.0748,-0.0286,0.0621,-0.0724,0.0374,0.027,0.2385,-0.0422,0.0701,0.0252,-0.0031,0.0387,0.0626,-0.0001,0.019,0.0129,0.041,-0.0469,0.0493,0.079,-0.0405,0.0575,-0.0011,0.006,-0.0409,0.0122,0.0067,-0.0244,0.1035,-0.0282,-0.0181,-0.0431,-0.0406,0.0207,-0.0261,-0.0022,-0.0341,-0.0116,-0.0037,0.0336,-0.0285,-0.0763,-0.0407,-0.0495,0.0237,-0.0323,0.0392,0.0008,0.0094]}
{"key":"[Machine learning transfer efficiencies for noisy quantum walks] Quantum effects are known to provide an advantage in particle transfer across networks. In order to achieve this advantage, requirements on both a graph type and a quantum system coherence must be found. Here we show that the process of finding these requirements can be automated by learning from simulated examples. The automation is done by using a convolutional neural network of a particular type that learns to understand with which network and under which coherence requirements quantum advantage is possible. Our machine learning approach is applied to study noisy quantum walks on cycle graphs of different sizes. We found that it is possible to predict the existence of quantum advantage for the entire decoherence parameter range, even for graphs outside of the training set. Our results are of importance for demonstration of advantage in quantum experiments and pave the way towards automating scientific research and discoveries.","layer":1,"vector":[-0.0787,-0.0424,-0.037,0.027,-0.0069,0.0318,0.0654,0.0027,0.0148,-0.0119,0.0292,-0.0583,0.061,0.0666,0.0212,0.0188,-0.0262,0.0201,-0.0718,0.0078,0.0102,-0.0309,0.0068,-0.0718,0.0182,0.011,-0.0027,-0.0607,-0.0807,-0.2184,-0.0029,-0.0572,-0.0075,-0.0247,0.015,-0.0247,0.001,0.0115,-0.0642,0.0321,0.0454,0.0268,0.001,-0.0117,-0.0194,-0.0368,-0.0315,-0.0466,-0.0457,-0.0503,0.0356,0.0044,0.0227,0.0287,0.0514,0.0452,0.0784,0.062,0.0273,0.0443,0.0071,0.0417,-0.1687,0.061,0.0521,-0.0062,-0.0435,-0.0223,0.0373,0.0752,-0.0243,0.0422,0.0008,0.0269,0.0257,0.0018,0.0288,-0.0346,-0.0369,-0.0035,-0.0119,-0.0507,-0.0455,0.0056,-0.032,-0.0375,-0.0029,-0.0341,0.0131,0.0198,0.0031,-0.0202,-0.0558,0.0025,-0.039,0.0427,0.0467,0.0206,0.0038,0.1705,-0.0505,0.0166,0.0023,-0.0218,0.043,-0.0446,-0.0332,-0.0176,-0.0158,-0.0314,0.0217,-0.052,0.0364,-0.0254,-0.0091,0.0099,0.0662,0.0437,-0.0318,-0.0075,-0.0675,0.002,0.042,-0.0312,-0.0317,-0.044,-0.0151,0.1379,0.0735,0.0554,0.0639,-0.0208,0.0177,-0.0208,0.0264,-0.005,0.0308,-0.0248,0.0237,0.0029,0.0139,-0.029,0.004,-0.114,-0.0618,0.0962,-0.0204,0.005,0.0312,-0.0349,0.0151,0.0333,-0.0268,-0.0403,0.058,0.0637,0.0112,0.0435,-0.0248,0.0182,-0.0704,-0.0328,-0.0253,0.0994,0.0135,-0.0723,-0.0477,-0.0465,0.0194,-0.0371,0.0261,0.0404,-0.066,0.0548,0.053,-0.0153,-0.0845,0.0205,-0.0268,0.0097,-0.0195,-0.0555,-0.0207,0.027,0.0763,-0.018,0.0074,-0.038,-0.0067,0.0194,0.0108,0.0375,-0.0566,-0.0007,-0.0607,0.0034,-0.042,0.0042,-0.0278,-0.0121,0.0332,-0.0268,-0.0012,0.0108,-0.0427,-0.028,-0.0061,0.0035,0.0347,0.0097,-0.0196,-0.0293,0.0436,-0.0362,-0.0532,-0.0189,0.0011,0.0102,0.0188,0.0514,0.0194,-0.0612,-0.0916,-0.263,-0.0257,0.0291,-0.0162,0.1041,-0.0906,0.0334,-0.0135,0.058,0.0666,0.04,-0.0086,-0.0054,-0.0084,-0.0016,0.0721,0.0568,0.0858,-0.0397,0.019,-0.0066,0.0156,-0.0268,-0.1041,0.0361,0.0104,0.2259,0.0316,0.0342,0.034,0.0101,0.0528,-0.0381,-0.0798,0.0143,0.0174,0.0549,0.0203,0.0075,-0.0235,-0.0384,0.0238,0.0141,-0.0697,-0.0092,0.02,-0.0163,0.0116,-0.0644,0.0196,0.0241,-0.0095,0.0543,-0.0092,-0.0024,-0.0438,-0.0945,0.0308,-0.0155,0.0231,-0.0264,-0.0571,0.0114,-0.0186,0.0576,0.0046,-0.0254,-0.027,0.0608,-0.0095,-0.0115,0.091,0.0042,0.0023,0.0634,-0.0211,0.0398,-0.0312,-0.035,0.0284,0.0897,-0.0297,0.0288,0.0084,-0.0069,0.0265,0.0704,-0.019,0.029,-0.0246,0.0155,-0.0029,-0.0559,-0.016,0.0081,0.0091,-0.2974,0.057,0.0412,0.0619,-0.0489,-0.0131,0.0422,0.0475,-0.0455,-0.0125,-0.0153,0.0219,0.0114,-0.0216,0.0227,0.0965,0.0406,-0.0483,0.0028,-0.0324,0.0392,0.0365,0.2457,-0.0384,0.0551,0.022,-0.0446,0.0082,-0.0115,-0.0522,0.004,0.017,0.0623,-0.0481,0.0926,0.0599,-0.0574,0.0298,-0.0037,0.003,0.0124,0.0058,-0.0048,-0.01,0.1357,-0.0485,-0.0271,-0.0525,-0.0086,0.033,-0.0273,0.0245,-0.0204,0.0233,-0.0002,0.0512,-0.0211,-0.0446,-0.0295,-0.0288,0.0521,-0.0628,0.0167,-0.0087,-0.0192]}
{"key":"[Theoretical Linear Convergence of Unfolded ISTA and its Practical Weights and Thresholds] In recent years, unfolding iterative algorithms as neural networks has become an empirical success in solving sparse recovery problems. However, its theoretical understanding is still immature, which prevents us from fully utilizing the power of neural networks. In this work, we study unfolded ISTA (Iterative Shrinkage Thresholding Algorithm) for sparse signal recovery. We introduce a weight structure that is necessary for asymptotic convergence to the true sparse signal. With this structure, unfolded ISTA can attain a linear convergence, which is better than the sublinear convergence of ISTA/FISTA in general cases. Furthermore, we propose to incorporate thresholding in the network to perform support selection, which is easy to implement and able to boost the convergence rate both theoretically and empirically. Extensive simulations, including sparse vector recovery and a compressive sensing experiment on real image data, corroborate our theoretical results and demonstrate their practical usefulness. We have made our codes publicly available: https://github.com/xchen-tamu/linear-lista-cpss.","layer":0,"vector":[-0.0475,0.0062,0.025,0.0071,0.0118,0.0396,-0.014,0.0198,0.0469,-0.0234,0.0351,-0.0418,0.0605,0.0667,0.0154,0.0379,0.0329,0.0733,-0.0196,0.007,0.0448,-0.0594,0.0252,-0.0306,0.011,-0.0196,-0.0015,-0.0439,-0.0592,-0.2855,0.0035,-0.0661,0.1053,-0.0167,0.063,0.0061,-0.0346,0.013,-0.0663,0.0808,0.0418,0.0187,-0.0451,-0.0388,-0.0178,-0.0743,-0.0188,-0.0205,0.0014,-0.0586,0.0301,0.0019,0.0303,0.0072,-0.0051,0.0074,0.0453,0.0799,0.0024,0.0327,0.0579,0.0236,-0.2013,0.0458,0.0703,0.0004,-0.0433,-0.036,0.0216,0.0651,0.0033,0.0371,0.0527,0.0107,0.0143,0.0192,0.0117,-0.0644,-0.0114,0.0043,0.0269,-0.0034,-0.0289,0.0127,-0.0287,-0.0263,0.0056,-0.0686,0.0076,-0.0362,-0.0495,0.0088,-0.0039,0.0382,-0.089,-0.0558,-0.0011,0.0748,-0.0292,0.2154,-0.0331,0.0554,0.0497,-0.0014,0.006,-0.0326,-0.0204,-0.0012,-0.0341,-0.0513,-0.0331,-0.0275,0.0619,-0.054,0.0235,-0.0268,0.0277,0.0004,-0.0325,0.012,-0.0038,0.0016,0.0614,-0.021,0.0102,-0.0598,0.0011,0.0982,0.0474,0.0214,0.0396,-0.0354,-0.0145,-0.0414,0.003,0.0348,0.0054,0.0107,-0.0091,-0.005,-0.0236,-0.0804,0.0601,-0.0742,-0.085,0.1188,-0.0556,0.0111,-0.0351,-0.0721,-0.0123,-0.0016,-0.0223,-0.042,0.0313,0.0213,0.039,0.0121,-0.0329,0.0329,-0.0168,-0.0567,-0.0403,0.1025,0.037,-0.0688,-0.0398,-0.0022,-0.0027,-0.0215,-0.0085,0.0264,-0.0154,0.0151,0.0465,0.0348,-0.0561,-0.0468,0.0212,0.0353,0.0061,-0.0626,-0.0154,0.0729,0.0401,-0.0284,0.0094,-0.0279,0.0187,-0.0264,-0.066,-0.0067,-0.0377,-0.0009,-0.0172,0.0028,0.0017,-0.0037,0.0015,-0.0124,0.0515,-0.0159,-0.0472,0.035,0.0232,0.0253,-0.0031,-0.0098,0.0474,0.0639,0.0068,0.006,0.0899,-0.0345,-0.0288,0.004,0.026,0.0298,-0.0335,0.0409,0.0423,-0.0897,-0.0744,-0.2005,-0.0442,0.0033,-0.0115,0.036,-0.0714,0.0254,0.0015,0.0794,0.0451,0.0248,0.0261,-0.0353,0.0348,-0.0258,0.0468,0.0427,0.0328,-0.005,-0.0305,-0.0093,0.0272,-0.0001,-0.0604,0.0473,0.0222,0.1829,0.0257,0.0359,-0.0185,0.0146,0.0412,-0.0501,-0.0892,0.0368,0.0306,0.0752,0.0107,-0.0532,-0.0259,-0.0334,0.0049,-0.0122,-0.0706,-0.0473,-0.0444,-0.0322,0.025,-0.0354,0.0221,0.0601,-0.0313,0.0216,-0.0423,0.0402,-0.0416,-0.0652,0.0026,-0.0298,0.0291,-0.0078,-0.0706,0.0076,-0.0502,0.0866,0.0339,0.0119,0.0093,-0.018,-0.0507,0.0076,0.0712,-0.0115,0.0247,0.0498,0.0209,0.0419,-0.0381,-0.0422,-0.0051,0.0512,-0.0254,0.0378,0.0298,0.0456,0.0577,0.1092,0.0042,-0.0065,0.0122,0.0057,0.0138,-0.0364,-0.0357,0.0081,0.0115,-0.3107,0.0046,0.0236,0.0175,-0.0178,0.0235,0.0469,0.032,-0.0638,0.0207,-0.0401,0.0123,0.0022,-0.0066,-0.0129,0.0413,0.0339,-0.058,0.0605,-0.1211,-0.0069,0.0595,0.2087,-0.0574,0.0015,0.0313,-0.0126,0.0047,-0.0124,-0.052,0.03,0.019,0.0503,-0.0652,0.0421,0.0683,-0.0319,0.0828,0.0338,-0.0298,0.0539,-0.0226,-0.0501,-0.0256,0.0677,-0.0373,-0.0288,-0.0121,-0.0039,0.0089,0.0096,0.0045,0.0211,-0.0458,0.0355,0.0573,-0.0685,0.0217,-0.0481,-0.0225,0.006,-0.029,-0.0468,0.021,0.0105]}
{"key":"[A Cluster-Based Trip Prediction Graph Neural Network Model for Bike Sharing Systems] Bike Sharing Systems (BSSs) are emerging as an innovative transportation service. Ensuring the proper functioning of a BSS is crucial given that these systems are committed to eradicating many of the current global concerns, by promoting environmental and economic sustainability and contributing to improving the life quality of the population. Good knowledge of users' transition patterns is a decisive contribution to the quality and operability of the service. The analogous and unbalanced users' transition patterns cause these systems to suffer from bicycle imbalance, leading to a drastic customer loss in the long term. Strategies for bicycle rebalancing become important to tackle this problem and for this, bicycle traffic prediction is essential, as it allows to operate more efficiently and to react in advance. In this work, we propose a bicycle trips predictor based on Graph Neural Network embeddings, taking into consideration station groupings, meteorology conditions, geographical distances, and trip patterns. We evaluated our approach in the New York City BSS (CitiBike) data and compared it with four baselines, including the non-clustered approach. To address our problem's specificities, we developed the Adaptive Transition Constraint Clustering Plus (AdaTC+) algorithm, eliminating shortcomings of previous work. Our experiments evidence the clustering pertinence (88% accuracy compared with 83% without clustering) and which clustering technique best suits this problem. Accuracy on the Link Prediction task is always higher for AdaTC+ than benchmark clustering methods when the stations are the same, while not degrading performance when the network is upgraded, in a mismatch with the trained model.","layer":3,"vector":[-0.0461,-0.0669,0.0433,0.0014,0.0179,0.0604,0.0372,0.0167,0.0242,0.0213,-0.0128,-0.048,0.0189,0.0461,0.0016,0.0286,0.0332,0.0627,-0.0007,-0.0009,0.0012,-0.0193,-0.0201,-0.0681,0.0564,0.0419,-0.0129,-0.0078,-0.0421,-0.224,-0.014,-0.0522,0.033,-0.0291,-0.0042,-0.0114,0.0004,0.0593,-0.0251,0.0651,0.0315,0.0606,-0.0163,-0.0689,-0.0284,-0.0095,-0.0236,0.0109,-0.0293,-0.0482,0.054,-0.0401,0.0003,-0.0083,0.0075,0.075,0.0067,0.0189,0.0068,0.0482,0.0165,0.0509,-0.1803,0.0372,0.058,0.0613,-0.0394,0.0058,-0.0174,0.0438,0.0152,0.0686,0.0126,0.0347,0.0297,0.0533,0.0162,-0.0323,-0.0336,-0.0307,-0.0159,-0.0425,-0.0125,-0.0086,0.0279,-0.0545,-0.0001,-0.033,0.0038,0.0159,-0.0458,-0.0233,-0.0004,0.0019,-0.102,-0.0166,0.0708,0.0005,-0.0536,0.1735,-0.0492,0.0952,0.0512,-0.0274,0.0207,-0.0681,0.0079,-0.0369,-0.0424,-0.0023,-0.0314,-0.039,0.0401,-0.0231,0.0303,0.0277,0.0779,0.041,-0.0071,0.007,-0.0466,0.0144,0.0267,-0.0018,0.0113,-0.0327,0.0302,0.1394,0.006,0.0111,0.0493,0.0194,-0.0492,0.0213,-0.0064,0.0001,0.0207,-0.0393,0.0005,0.0088,0.0076,-0.0433,0.0366,-0.0936,-0.0657,0.1104,0.0056,-0.0233,-0.0225,-0.0177,-0.0177,-0.0092,-0.0381,-0.0535,-0.001,0.0695,0.0515,0.0296,-0.0661,0.0438,-0.0152,-0.0075,-0.0843,0.0895,0.0084,-0.0982,0.0035,-0.005,0.017,-0.0652,0.0148,0.0316,-0.0338,0.0656,0.1234,0.0377,-0.0546,-0.0096,-0.0211,-0.0162,-0.003,-0.033,-0.0576,0.0499,0.064,-0.0306,-0.0588,-0.0564,-0.0013,0.0667,-0.0374,0.0027,-0.0407,0.0399,-0.0449,0.0004,-0.0305,0.0038,0.0115,-0.0281,0.0296,0.0243,-0.0261,0.024,-0.0453,0.023,-0.0009,-0.0314,0.027,0.0273,0.0034,-0.0213,0.0836,-0.0106,-0.0203,0.0047,0.0382,0.0331,0.0318,0.0623,0.0497,0.0266,-0.0478,-0.1982,0.0083,0.0443,0.0161,0.0793,-0.0466,0.0212,0.0299,0.0453,0.0685,0.0974,-0.0378,-0.0418,0.0191,-0.001,0.0356,0.0375,0.0493,-0.0328,-0.0129,-0.0199,0.0094,-0.0095,-0.1051,0.0137,0.0232,0.2103,-0.0099,0.02,-0.0404,0.0106,0.021,-0.0634,-0.0873,0.0621,-0.0054,0.0793,-0.0352,-0.0563,-0.0351,-0.0266,0.0175,-0.0362,-0.0452,-0.0407,-0.0098,0.0,0.0352,-0.0669,-0.0091,0.0159,-0.0136,0.0393,-0.0117,0.0006,-0.0436,-0.0642,0.0279,-0.0212,-0.0053,0.0079,-0.0463,-0.0193,-0.0658,0.0619,0.0138,-0.0553,-0.03,0.0163,-0.0117,-0.0439,0.1078,0.0199,-0.0128,0.0533,-0.0201,0.0008,-0.0175,-0.0308,-0.0187,0.0629,-0.0715,0.0476,0.0761,0.005,0.0316,0.0406,-0.0067,0.0427,-0.0265,0.0317,-0.0243,-0.0376,-0.0272,0.0693,-0.0289,-0.304,0.0217,0.0033,0.0212,-0.013,0.016,0.0707,0.0493,-0.014,0.001,0.0462,0.0151,0.0753,-0.0276,0.0337,0.0256,0.0643,-0.0353,0.0015,-0.0486,-0.0163,0.0283,0.2202,-0.0356,0.0648,0.0447,-0.0779,-0.0186,0.0305,-0.0134,-0.0388,-0.0203,0.0906,-0.0585,0.0284,0.0288,-0.0246,0.0374,0.0655,0.0126,-0.0222,0.0034,-0.0386,-0.0384,0.1087,-0.0034,-0.0492,-0.0518,0.0366,0.0151,-0.0525,-0.0024,-0.04,-0.0051,0.0129,0.0045,-0.0847,-0.0427,-0.0799,-0.0563,0.0444,-0.0919,-0.0317,-0.0229,-0.0126]}
{"key":"[An Ensemble Approach for Automatic Structuring of Radiology Reports] Automatic structuring of electronic medical records is of high demand for clinical workflow solutions to facilitate extraction, storage, and querying of patient care information. However, developing a scalable solution is extremely challenging, specifically for radiology reports, as most healthcare institutes use either no template or department/institute specific templates. Moreover, radiologists' reporting style varies from one to another as sentences are telegraphic and do not follow general English grammar rules. We present an ensemble method that consolidates the predictions of three models, capturing various attributes of textual information for automatic labeling of sentences with section labels. These three models are: 1) Focus Sentence model, capturing context of the target sentence; 2) Surrounding Context model, capturing the neighboring context of the target sentence; and finally, 3) Formatting/Layout model, aimed at learning report formatting cues. We utilize Bi-directional LSTMs, followed by sentence encoders, to acquire the context. Furthermore, we define several features that incorporate the structure of reports. We compare our proposed approach against multiple baselines and state-of-the-art approaches on a proprietary dataset as well as 100 manually annotated radiology notes from the MIMIC-III dataset, which we are making publicly available. Our proposed approach significantly outperforms other approaches by achieving 97.1% accuracy.","layer":1,"vector":[-0.0057,0.0009,-0.0023,0.0064,0.0483,-0.0351,0.0229,0.0422,-0.0049,0.0166,-0.011,-0.0182,0.0368,0.0406,0.0134,-0.0109,-0.0461,0.0379,-0.0056,0.0458,0.0137,-0.0109,0.0132,-0.018,0.0305,0.0868,-0.0562,-0.0458,-0.063,-0.2034,-0.0224,-0.0516,0.0715,-0.0202,0.0002,-0.0231,-0.0431,0.0392,-0.0256,0.0383,-0.0163,-0.0027,-0.0092,-0.0763,-0.0127,-0.0422,-0.0255,-0.0026,-0.0112,-0.0135,-0.0074,-0.0169,0.03,0.0329,0.0297,0.0316,0.04,0.0119,0.0315,0.0188,0.0394,0.0446,-0.1685,0.0902,0.0336,0.0307,-0.0556,-0.0119,-0.019,0.0769,-0.0084,-0.0083,-0.009,0.0556,0.0162,-0.0215,-0.0033,-0.0113,0.0119,0.008,-0.0156,0.0001,-0.0736,0.0236,-0.0113,-0.0652,-0.0378,-0.0552,0.0424,-0.0262,-0.0305,-0.0324,-0.0474,0.033,-0.0716,-0.0743,0.0192,0.0293,-0.0346,0.1738,-0.0243,0.021,0.0838,-0.0373,0.0193,-0.0434,-0.001,-0.0161,-0.0352,-0.0082,-0.0082,-0.0129,0.0183,-0.0635,0.0511,0.0076,0.0969,0.0416,-0.0145,-0.0361,-0.0145,0.0079,0.0105,-0.0216,0.0141,-0.0373,0.0696,0.1172,0.0492,-0.0026,0.0651,0.019,-0.064,-0.0163,-0.0095,-0.007,0.0353,-0.0284,0.0056,-0.0026,-0.0363,-0.0777,-0.0207,-0.0716,-0.072,0.1685,-0.0806,-0.0039,-0.0575,-0.0445,-0.0331,-0.0008,-0.009,-0.0297,0.0178,0.013,0.0344,0.0451,-0.0337,-0.011,0.0415,-0.0649,-0.0268,0.1109,0.0039,-0.0813,-0.043,0.0101,0.0505,-0.0218,0.0803,0.0445,-0.0085,0.0307,0.0552,0.063,0.007,-0.0216,0.0467,-0.0114,0.0408,-0.0517,-0.045,0.0388,0.0172,-0.0556,-0.0074,-0.0436,0.0666,0.0152,-0.0198,0.0732,-0.0157,0.0129,-0.0206,-0.0372,-0.0006,0.0041,-0.0036,-0.0329,0.0262,0.0341,-0.0352,0.0233,0.0082,-0.0295,-0.0253,0.0143,0.0452,-0.0037,-0.0802,0.0082,0.1127,-0.0024,-0.0334,-0.0183,0.0227,0.0169,-0.0121,0.0665,0.0152,-0.0019,-0.0245,-0.2419,0.0081,0.0513,-0.0025,0.0215,-0.0603,0.0378,0.0317,0.0368,0.0639,0.0547,-0.0135,-0.0406,-0.0326,-0.0376,0.0322,0.0516,0.0297,-0.0178,-0.0116,0.039,0.0132,-0.0243,-0.079,0.0316,-0.0022,0.2305,0.0435,0.006,0.0003,0.0484,-0.0063,-0.0244,-0.0813,0.0301,0.0703,0.0534,0.0017,-0.0821,-0.018,-0.0294,0.0529,-0.0344,-0.1036,-0.0488,-0.0496,-0.0156,0.0071,-0.0626,0.0458,0.0395,-0.0804,0.031,0.0203,-0.0304,-0.0253,-0.1236,-0.023,-0.0097,-0.0259,-0.0116,-0.0126,0.0329,-0.0743,-0.0076,0.0146,-0.0351,0.0238,0.0192,-0.0376,-0.035,0.0583,-0.0437,-0.0207,0.0438,0.036,0.0226,-0.0257,-0.0093,-0.0491,0.0657,-0.0191,0.0608,0.0431,0.0263,0.0616,0.0602,0.0023,0.0505,-0.034,0.0264,0.0139,-0.0665,0.0037,0.0235,-0.0244,-0.2976,0.0691,0.0022,0.0115,-0.0133,0.0725,0.0082,0.0083,-0.0049,0.0148,-0.0204,0.0107,0.0251,-0.061,-0.0519,0.0563,0.1094,-0.025,0.0407,-0.0441,0.0076,0.0393,0.2154,-0.0176,0.0418,0.0331,-0.0332,-0.0038,0.0409,0.0383,0.0368,-0.0446,0.1044,0.0188,0.0308,0.0687,0.0013,0.0584,-0.0121,-0.0086,-0.0248,0.0135,-0.0321,-0.0234,0.0687,-0.007,0.0226,-0.0885,0.0107,-0.0111,-0.0073,0.018,-0.0644,0.0053,0.0128,0.0058,-0.0003,-0.0465,-0.0088,-0.0554,-0.0265,-0.0738,-0.007,0.0793,-0.0169]}
{"key":"[Energy Disaggregation using Variational Autoencoders] Non-intrusive load monitoring (NILM) is a technique that uses a single sensor to measure the total power consumption of a building. Using an energy disaggregation method, the consumption of individual appliances can be estimated from the aggregate measurement. Recent disaggregation algorithms have significantly improved the performance of NILM systems. However, the generalization capability of these methods to different houses as well as the disaggregation of multi-state appliances are still major challenges. In this paper we address these issues and propose an energy disaggregation approach based on the variational autoencoders framework. The probabilistic encoder makes this approach an efficient model for encoding information relevant to the reconstruction of the target appliance consumption. In particular, the proposed model accurately generates more complex load profiles, thus improving the power signal reconstruction of multi-state appliances. Moreover, its regularized latent space improves the generalization capabilities of the model across different houses. The proposed model is compared to state-of-the-art NILM approaches on the UK-DALE and REFIT datasets, and yields competitive results. The mean absolute error reduces by 18% on average across all appliances compared to the state-of-the-art. The F1-Score increases by more than 11%, showing improvements for the detection of the target appliance in the aggregate measurement.","layer":3,"vector":[-0.0273,-0.014,0.053,-0.026,0.0279,0.0365,0.0143,-0.0272,0.0336,-0.0092,0.0181,-0.0642,0.0139,0.0584,0.0235,-0.0116,0.0108,0.0605,-0.0053,-0.0019,0.0399,-0.0428,-0.0045,-0.0242,0.0362,0.0108,0.0007,-0.0118,-0.0432,-0.2397,0.0329,-0.0584,0.0839,-0.0127,0.0297,-0.0458,-0.0595,0.0326,-0.0166,0.0054,0.0241,-0.0164,0.0108,-0.0708,-0.0347,-0.0419,0.0007,-0.046,-0.0394,-0.0097,0.0724,-0.0276,0.0197,0.0356,0.0657,0.034,0.0519,0.034,0.1205,0.0295,0.0313,0.0753,-0.196,0.0649,0.0695,0.006,-0.0405,-0.0157,0.0344,0.0332,-0.0314,0.0128,0.0003,0.026,0.0316,0.0308,0.0101,-0.0244,-0.0315,-0.0045,0.0307,-0.0448,-0.0441,0.0025,-0.054,-0.066,-0.0166,-0.0515,0.0599,0.0016,-0.0673,-0.0321,-0.0127,0.0662,-0.0478,-0.0327,0.0067,0.0268,-0.0346,0.1942,-0.0545,0.024,0.0517,-0.0318,0.0309,-0.0811,-0.0225,-0.0113,-0.0432,-0.0062,-0.006,-0.0055,0.0552,-0.0795,0.0667,-0.0426,0.0592,0.0489,0.047,0.0013,-0.0099,0.0129,0.0637,-0.0357,0.0352,-0.0411,0.0248,0.1524,0.0129,0.0521,0.0191,0.0029,-0.0267,0.0002,0.0658,0.0469,0.0772,-0.0072,0.0684,0.0232,-0.0264,-0.0527,0.0272,-0.1225,-0.043,0.1077,-0.0587,0.03,-0.0776,-0.0647,-0.0014,0.0097,-0.024,-0.0362,0.0436,0.0644,0.0404,0.0194,-0.0509,0.0049,0.011,-0.0167,-0.0712,0.0494,0.0158,-0.0962,-0.0405,-0.0035,-0.0221,-0.02,0.0789,0.0182,-0.0731,0.0229,0.1013,0.0291,-0.0497,-0.0043,0.0074,0.0174,-0.0004,-0.0429,-0.0234,0.0247,0.0366,-0.0435,0.0151,-0.0226,-0.0095,0.0333,-0.0301,-0.0351,-0.017,-0.0163,0.0129,-0.001,-0.0414,-0.0134,0.0281,-0.0666,0.0322,-0.004,-0.0308,0.0726,0.0097,0.0476,-0.0419,0.0031,0.0745,-0.0002,0.0523,-0.0115,0.0834,-0.022,-0.0438,0.0107,0.0238,0.0347,0.0294,0.0376,0.0405,-0.0342,-0.0787,-0.2125,0.0183,-0.0055,-0.0004,0.0227,-0.0438,0.0458,-0.0315,0.0375,0.0193,0.0294,-0.0064,-0.0365,0.0714,-0.0092,0.0926,0.0356,0.0214,-0.1118,0.0126,0.0109,-0.0102,-0.0018,-0.117,0.0611,-0.0068,0.1869,-0.0284,0.0724,-0.0129,0.0117,0.0314,-0.0472,-0.064,0.0601,0.0499,0.065,0.0119,-0.0423,-0.0287,-0.0061,0.0112,0.0021,-0.0843,-0.0367,-0.1002,-0.0274,-0.0156,-0.1053,0.0024,0.0474,-0.0254,0.0446,-0.0383,0.0043,-0.0307,-0.0778,0.0321,-0.0409,0.0277,0.0159,-0.0476,0.0187,-0.0803,0.0578,-0.0081,-0.0377,-0.0376,-0.0241,0.0137,-0.0173,0.0868,-0.0115,-0.0207,0.0281,0.0245,0.0283,-0.0206,-0.038,-0.0395,0.0299,-0.0117,0.0183,0.0286,0.0435,-0.0113,0.0342,0.0066,0.0222,0.0022,-0.0214,0.0452,-0.0254,-0.0199,0.0219,-0.01,-0.2817,0.0201,0.0182,-0.0389,-0.0075,-0.0325,-0.018,0.0245,-0.0263,-0.0214,-0.0513,0.0029,0.0001,-0.0132,0.0278,-0.0149,0.0847,-0.0425,0.0475,-0.0458,0.0343,0.0479,0.2058,-0.0209,0.0552,0.009,0.0016,0.0151,-0.002,-0.0271,-0.0334,-0.0167,0.086,-0.0599,0.0503,0.0522,-0.0134,0.0546,0.016,-0.0481,0.0011,0.0024,-0.0127,-0.0115,0.0924,0.021,-0.0322,-0.0245,-0.0063,0.0128,-0.0057,-0.0216,-0.0153,0.0158,0.0582,0.0248,-0.061,-0.0409,-0.016,-0.0399,0.0077,0.0042,-0.0333,-0.0249,-0.015]}
{"key":"[Discovering Object Masks with Transformers for Unsupervised Semantic Segmentation] The task of unsupervised semantic segmentation aims to cluster pixels into semantically meaningful groups. Specifically, pixels assigned to the same cluster should share high-level semantic properties like their object or part category. This paper presents MaskDistill: a novel framework for unsupervised semantic segmentation based on three key ideas. First, we advocate a data-driven strategy to generate object masks that serve as a pixel grouping prior for semantic segmentation. This approach omits handcrafted priors, which are often designed for specific scene compositions and limit the applicability of competing frameworks. Second, MaskDistill clusters the object masks to obtain pseudo-ground-truth for training an initial object segmentation model. Third, we leverage this model to filter out low-quality object masks. This strategy mitigates the noise in our pixel grouping prior and results in a clean collection of masks which we use to train a final segmentation model. By combining these components, we can considerably outperform previous works for unsupervised semantic segmentation on PASCAL (+11% mIoU) and COCO (+4% mask AP50). Interestingly, as opposed to existing approaches, our framework does not latch onto low-level image cues and is not limited to object-centric datasets. The code and models will be made available.","layer":0,"vector":[-0.0293,-0.0164,0.0363,-0.0127,0.0283,0.014,0.044,-0.0356,-0.0169,0.0112,0.0271,-0.1215,0.0088,0.0592,0.0643,0.0106,0.0137,0.0527,-0.017,-0.0206,0.0475,-0.0777,-0.0039,-0.0519,0.0134,0.0614,0.0085,0.0261,-0.0428,-0.2206,-0.0588,-0.0716,0.0468,-0.0008,0.0054,-0.0112,-0.0705,0.05,-0.0345,0.0071,0.0252,-0.0045,-0.076,-0.0416,-0.023,-0.0289,-0.0267,-0.0446,-0.0061,-0.0238,0.0595,-0.0336,0.0439,0.0367,-0.0272,0.0569,0.0148,0.0242,0.0653,0.021,0.0454,0.0474,-0.137,0.0589,0.0503,-0.0066,-0.0437,-0.0317,0.0115,0.0349,-0.0378,0.0136,-0.0199,0.0138,0.0223,-0.0193,-0.0457,-0.1053,-0.0291,0.0031,-0.0208,0.027,-0.0482,-0.0132,0.0139,-0.0521,0.0173,-0.0597,0.0006,0.0062,-0.0439,-0.0165,-0.0014,0.0519,-0.0783,-0.0287,0.0304,0.0317,-0.0304,0.2209,-0.079,0.0228,0.0274,-0.0598,-0.0338,0.0019,-0.0152,0.0103,0.0039,0.0012,-0.0223,-0.0028,0.0089,-0.0285,0.0338,0.0128,0.1094,0.0427,-0.0204,-0.0017,-0.0307,-0.0414,0.0422,-0.0235,0.0191,-0.0562,0.0443,0.1348,0.0678,0.017,0.0253,0.0108,-0.0441,-0.015,0.0103,0.0573,0.0119,-0.0184,0.0004,-0.0273,-0.0492,-0.0288,0.0315,-0.0709,-0.0172,0.1117,-0.0961,0.0646,-0.0649,-0.036,-0.0101,-0.0101,-0.0491,-0.0091,0.0009,-0.0068,0.0216,0.0591,-0.0119,0.0196,0.0192,-0.0422,-0.0184,0.0955,0.0055,-0.0784,-0.0916,0.0137,0.0077,-0.0273,0.0133,0.0306,-0.0102,0.0254,0.1044,-0.0172,-0.1003,0.0184,-0.0041,0.0322,0.0321,-0.0577,-0.052,0.0392,0.0247,-0.0453,-0.017,-0.0287,0.0517,0.0557,-0.0315,0.0612,-0.0426,-0.0097,0.0018,-0.021,-0.0126,-0.0218,-0.0374,-0.0283,0.0111,-0.026,-0.0367,0.044,-0.0227,0.0425,-0.0336,-0.0062,0.0663,0.0196,-0.0308,-0.0215,0.0445,-0.0034,0.0054,0.0112,0.0477,0.0138,0.0022,0.0406,0.0465,-0.0535,-0.0686,-0.1733,0.0554,0.0315,-0.0487,0.0078,-0.01,-0.0028,0.0179,0.0542,0.077,0.0654,-0.0563,-0.035,0.0519,-0.0058,0.0346,0.0239,0.0411,-0.0432,0.0221,0.0029,0.0337,-0.0145,-0.0566,0.0413,0.0196,0.255,0.0466,0.0585,-0.0497,0.0252,0.011,-0.0341,-0.0841,0.0642,0.0099,0.0195,-0.0126,-0.0337,-0.0471,-0.0462,-0.0108,0.0165,-0.0896,-0.0171,-0.0455,-0.053,0.0657,-0.0105,-0.0137,0.0114,-0.0496,0.0524,0.043,-0.0141,-0.0721,-0.0501,-0.015,-0.0063,0.0306,0.0295,-0.0372,0.0176,-0.0861,0.0635,0.0163,-0.0514,-0.0443,0.0327,-0.0294,-0.0559,0.0936,0.0208,-0.0299,0.0654,0.0028,0.0549,-0.021,-0.0612,-0.0487,0.1342,-0.0464,0.0051,-0.0344,0.0411,-0.0032,0.0832,-0.0236,0.0219,-0.0164,0.0517,-0.0175,-0.0742,0.0084,0.0288,-0.0385,-0.2561,0.0239,0.0331,0.026,-0.0039,0.0051,0.0832,0.0256,-0.0141,-0.0064,-0.0261,0.024,0.0929,0.0117,-0.0252,0.0594,0.0548,-0.0272,0.0958,-0.0334,0.0088,0.0383,0.2111,-0.0435,-0.0117,-0.0134,0.0031,0.0007,0.0663,-0.0065,0.0272,0.0197,0.1086,-0.057,-0.0065,0.0695,-0.0198,0.0257,0.0169,-0.0364,-0.0126,0.0213,-0.0012,-0.0099,0.0813,-0.0099,0.0172,-0.0257,0.0425,0.0254,-0.0174,0.0037,-0.0282,0.0052,0.049,-0.0002,-0.0342,-0.0278,-0.0878,0.0097,-0.0116,-0.0505,-0.0423,0.0326,-0.0147]}
{"key":"[Activation Density based Mixed-Precision Quantization for Energy Efficient Neural Networks] As neural networks gain widespread adoption in embedded devices, there is a need for model compression techniques to facilitate deployment in resource-constrained environments. Quantization is one of the go-to methods yielding state-of-the-art model compression. Most approaches take a fully trained model, apply different heuristics to determine the optimal bit-precision for different layers of the network, and retrain the network to regain any drop in accuracy. Based on Activation Density (AD)-the proportion of non-zero activations in a layer-we propose an in-training quantization method. Our method calculates bit-width for each layer during training yielding a mixed precision model with competitive accuracy. Since we train lower precision models during training, our approach yields the final quantized model at lower training complexity and also eliminates the need for re-training. We run experiments on benchmark datasets like CIFAR-10, CIFAR-100, TinyImagenet on VGG19/ResNet18 architectures and report the accuracy and energy estimates for the same. We achieve ~4.5x benefit in terms of estimated multiply-and-accumulate (MAC) reduction while reducing the training complexity by 50% in our experiments. To further evaluate the energy benefits of our proposed method, we develop a mixed-precision scalable Process In Memory (PIM) hardware accelerator platform. The hardware platform incorporates shift-add functionality for handling multi-bit precision neural network models. Evaluating the quantized models obtained with our proposed method on the PIM platform yields ~5x energy reduction compared to 16-bit models. Additionally, we find that integrating AD based quantization with AD based pruning (both conducted during training) yields up to ~198x and ~44x energy reductions for VGG19 and ResNet18 architectures respectively on PIM platform compared to baseline 16-bit precision, unpruned models.","layer":0,"vector":[-0.0664,-0.0177,0.0537,-0.0099,-0.0147,0.0221,-0.0051,0.0085,0.0085,-0.0197,0.0588,-0.072,0.035,0.064,0.0554,0.0181,0.0054,0.01,0.0042,0.0044,0.0397,-0.0296,-0.0078,-0.0101,0.0197,-0.0059,-0.0298,-0.0359,-0.0369,-0.2583,0.0412,-0.0239,0.0813,-0.0329,-0.0113,-0.0019,-0.041,0.0192,-0.0276,0.0204,0.0435,0.019,-0.0091,-0.0419,0.0077,-0.0169,-0.0307,0.018,0.0008,0.0039,0.034,0.0069,-0.0016,0.0487,0.0581,0.0083,0.0254,0.0685,0.0367,0.0384,0.0477,0.058,-0.1471,0.064,0.02,-0.0003,-0.0205,-0.0341,-0.0188,0.0174,-0.0576,0.0166,0.0461,0.0314,0.0319,0.0003,-0.006,-0.0334,0.0155,-0.0107,0.0049,-0.0349,-0.069,-0.0058,-0.039,-0.0361,0.0399,0.0078,0.0141,-0.0468,-0.0198,-0.0102,-0.0863,0.0082,0.0012,0.0047,0.0464,0.0234,-0.0869,0.2246,-0.0207,0.0582,-0.0318,-0.0111,0.0102,-0.0366,-0.0567,0.0073,-0.0702,-0.0112,-0.0012,-0.0299,0.0361,-0.0515,0.066,0.0135,0.0316,0.0225,0.0397,0.0142,-0.045,-0.02,0.0052,-0.0352,0.0255,-0.066,0.0018,0.1358,0.0328,0.0456,0.0377,-0.0067,-0.0028,-0.0321,0.074,0.0253,0.02,-0.0335,0.0099,0.0278,-0.0297,-0.0303,0.0482,-0.1027,-0.0566,0.1312,-0.0565,0.0599,-0.0223,-0.0435,0.0116,0.049,-0.0226,-0.0077,0.0336,0.0386,-0.0035,0.0387,-0.0803,-0.034,-0.0136,-0.0525,-0.0505,0.1179,0.0313,-0.0494,0.0058,-0.007,0.0114,-0.0798,0.0435,0.0112,-0.0337,0.0204,0.0634,0.0323,-0.0411,-0.0234,-0.0148,0.0065,0.0252,-0.0248,-0.0043,0.0351,0.0544,-0.0461,0.0262,-0.0576,-0.0245,0.0126,-0.0477,0.0494,-0.0306,-0.0125,0.0208,-0.0075,-0.031,0.019,0.0049,-0.0367,0.0227,-0.0347,-0.0344,-0.0266,0.0009,0.0206,-0.0486,-0.0037,0.0482,0.0535,-0.0406,-0.0114,0.0838,-0.0476,-0.0368,0.0076,0.0066,0.0577,0.0183,0.0545,0.0201,-0.0635,-0.0922,-0.2571,-0.0066,0.0287,-0.0252,0.0942,-0.0759,0.0125,0.0303,0.0024,0.0366,0.0124,-0.0143,-0.0299,0.0362,-0.0343,0.0632,0.063,0.0225,-0.0426,-0.0208,-0.0204,0.0421,-0.0445,-0.0596,0.0786,0.0072,0.1755,0.0375,0.0901,-0.0178,-0.0103,0.0113,-0.0323,-0.0802,0.0882,0.0447,0.063,0.0463,0.0082,0.0028,-0.0846,0.0298,-0.0168,-0.1313,-0.0066,-0.0187,-0.0386,-0.006,-0.0631,-0.0018,0.0228,-0.0458,0.0601,-0.0135,-0.0095,-0.0451,-0.0747,0.0731,-0.0074,-0.002,-0.0017,-0.0933,-0.0101,-0.0103,0.0164,-0.0175,-0.0113,-0.0317,0.0371,-0.0256,-0.0319,0.066,0.0258,-0.0088,0.0956,0.01,0.0326,-0.0309,-0.0068,-0.0197,0.0486,-0.0053,0.0296,0.0288,0.017,0.0004,0.0791,-0.0175,0.0003,0.0062,-0.048,0.0138,-0.0338,0.0366,0.0427,-0.0412,-0.2947,0.055,0.0133,0.0237,-0.0152,0.0345,0.0525,0.016,-0.0455,0.0179,-0.063,0.0247,0.0371,-0.0068,0.0043,0.0132,0.0378,-0.0559,0.0428,-0.0629,0.0028,0.0196,0.1675,-0.0488,0.0256,0.0286,-0.0272,0.0612,0.0707,-0.0332,-0.0271,0.0099,0.0965,-0.0135,0.0306,0.0854,-0.0679,0.0809,0.0446,0.0149,-0.0052,-0.0062,-0.0672,-0.039,0.0545,-0.0384,-0.0102,-0.0443,-0.042,0.042,0.0,0.0148,-0.0156,-0.0004,0.0135,0.0556,-0.0449,-0.0571,-0.0265,-0.0423,0.0371,-0.0521,-0.0591,0.0251,-0.0694]}
{"key":"[Configuration Learning in Underwater Optical Links] A new research problem named configuration learning is described in this work. A novel algorithm is proposed to address the configuration learning problem. The configuration learning problem is defined to be the optimization of the Machine Learning (ML) classifier to maximize the ML performance metric optimizing the transmitter configuration in the signal processing/communication systems. Specifically, this configuration learning problem is investigated in an underwater optical communication system with signal processing performance metric of the physical-layer communication throughput. A novel algorithm is proposed to perform the configuration learning by alternating optimization of key design parameters and switching between several Recurrent Neural Network (RNN) classifiers dependant on the learning objective. The proposed ML algorithm is validated with the datasets of an underwater optical communication system and is compared with competing ML algorithms. Performance results indicate that the proposal outperforms the competing algorithms for binary and multi-class configuration learning in underwater optical communication datasets. The proposed configuration learning framework can be further investigated and applied to a broad range of topics in signal processing and communications.","layer":1,"vector":[-0.056,-0.0272,0.0594,-0.0427,0.0133,0.0263,0.0363,0.0438,0.0055,-0.0129,0.0342,-0.0577,0.0153,0.0433,0.0568,0.0242,0.0402,0.0557,-0.0082,0.0145,0.1003,-0.0069,-0.0448,-0.0703,0.0019,-0.0119,-0.0337,-0.0378,-0.044,-0.2174,0.0122,-0.0331,0.0284,0.0115,0.0198,-0.0145,-0.0553,-0.0049,-0.068,0.0563,-0.0014,0.0193,-0.0144,-0.0372,0.0089,-0.0366,0.0056,-0.054,-0.0012,-0.0398,0.0147,-0.0517,-0.0191,0.0131,0.0313,0.0563,0.0403,0.0499,0.0116,0.0257,0.0752,0.0807,-0.2128,0.0434,0.0038,0.0351,-0.0261,0.0008,0.0273,0.0348,-0.0501,0.0251,0.0377,0.0352,0.0083,0.0485,-0.0011,-0.0196,0.0107,0.0204,0.04,-0.0453,-0.0269,-0.0205,0.0127,-0.0563,0.0354,-0.077,-0.0228,0.0195,-0.058,-0.0026,-0.0175,0.0114,-0.0725,0.0007,0.0174,-0.0038,-0.0478,0.1878,-0.0723,-0.0036,0.0298,-0.0445,0.0575,-0.0342,-0.0374,-0.0041,-0.0278,0.0057,-0.0225,-0.0257,0.0062,-0.0287,-0.0166,0.0542,-0.0091,0.0709,-0.0028,-0.0011,-0.0495,-0.0512,0.0674,0.0188,0.0285,-0.0707,0.0608,0.1085,0.013,0.0188,0.0035,-0.0404,-0.0362,-0.0197,0.0377,0.0323,0.03,-0.0283,0.0052,0.0211,-0.048,-0.0465,0.0488,-0.0552,-0.0391,0.1616,-0.0679,0.0323,-0.0361,-0.0286,-0.0192,-0.0043,0.0214,-0.0269,0.0118,0.0144,0.0118,0.047,-0.0705,0.036,-0.0382,-0.0627,-0.018,0.0983,0.0136,-0.1152,-0.0489,-0.0449,-0.0086,-0.0176,-0.0032,0.0538,-0.0126,0.0391,0.0708,0.0186,-0.0776,-0.0027,-0.0341,0.0127,-0.0034,-0.031,-0.0378,0.0238,0.0617,-0.0445,0.0088,-0.0397,0.0135,0.0307,-0.0291,-0.0122,-0.011,0.0179,-0.0289,-0.0335,-0.0142,0.0099,-0.0165,-0.0169,0.0056,-0.0348,-0.0298,0.027,0.0118,0.0044,0.0257,0.0024,0.0222,0.0554,-0.0348,-0.0059,0.0442,-0.0437,-0.0245,-0.0345,0.0085,0.05,0.0126,0.0541,0.043,-0.0602,-0.0687,-0.2189,-0.002,0.0128,-0.0176,0.0805,-0.0847,0.0532,0.0468,0.057,0.044,0.0401,0.0025,-0.0374,0.0181,-0.0038,0.0263,0.0621,0.0198,-0.0237,-0.0105,-0.0318,0.0217,0.0046,-0.0799,0.0436,0.0148,0.1836,0.001,0.0334,-0.0345,0.0206,0.0337,-0.0025,-0.0846,0.0297,0.0234,0.0904,0.0032,-0.0069,0.0052,-0.0024,0.0356,-0.0335,-0.0993,-0.0297,-0.0516,-0.067,0.0256,-0.0363,-0.0029,0.0224,-0.0272,0.0361,-0.0167,-0.0058,0.01,-0.0813,-0.0058,-0.0486,0.028,0.0188,-0.0063,0.0241,-0.0887,0.0702,0.0019,-0.0091,-0.0254,0.0358,-0.002,-0.0056,0.1077,0.0146,0.0278,0.0789,-0.0186,0.0086,-0.0734,-0.0135,-0.0229,0.0919,-0.0528,0.0318,0.0241,0.0481,0.0026,0.1139,-0.0041,0.0181,0.0418,-0.0337,-0.0007,-0.0117,-0.0048,0.048,0.0002,-0.3008,0.0302,0.0312,0.0324,-0.0534,0.0125,0.0583,0.0171,-0.0303,0.0305,-0.0442,0.0119,0.0014,0.0056,0.0394,0.0745,0.0434,-0.0559,0.0135,-0.039,0.0483,0.0522,0.2052,-0.0453,0.0379,0.0268,-0.0293,0.034,0.0045,-0.0026,0.0762,0.0459,0.0964,-0.0733,0.0368,0.0461,-0.0039,-0.0127,0.0252,-0.0609,0.0084,-0.008,-0.067,0.0003,0.1302,-0.0237,-0.0115,-0.0426,-0.0396,0.0178,-0.0564,-0.0003,0.0094,0.014,0.0012,0.0286,-0.0795,-0.0835,-0.0316,-0.027,0.0429,-0.0922,-0.0061,-0.0303,0.0307]}
{"key":"[Using Distance Estimation and Deep Learning to Simplify Calibration in Food Calorie Measurement] High calorie intake in the human body on the one hand, has proved harmful in numerous occasions leading to several diseases and on the other hand, a standard amount of calorie intake has been deemed essential by dieticians to maintain the right balance of calorie content in human body. As such, researchers have proposed a variety of automatic tools and systems to assist users measure their calorie in-take. In this paper, we consider the category of those tools that use image processing to recognize the food, and we propose a method for fully automatic and user-friendly calibration of the dimension of the food portion sizes, which is needed in order to measure food portion weight and its ensuing amount of calories. Experimental results show that our method, which uses deep learning, mobile cloud computing, distance estimation and size calibration inside a mobile device, leads to an accuracy improvement to 95% on average compared to previous work","layer":1,"vector":[0.0076,-0.0279,0.0462,-0.0344,0.0306,0.0216,0.0365,0.0266,0.0141,-0.0114,0.0129,-0.105,0.0467,0.0545,0.0505,-0.0659,0.0382,0.0171,-0.0542,0.0322,0.0556,-0.0465,-0.025,-0.0372,0.0219,0.0508,-0.0265,-0.0364,-0.0242,-0.2238,0.0611,-0.0365,0.0805,-0.0849,-0.0096,-0.0388,-0.0211,0.0354,-0.0587,0.0223,0.057,0.0457,-0.0232,-0.0492,-0.0253,-0.0604,-0.0822,-0.0021,0.0013,0.0012,0.0363,-0.0105,0.0011,0.0443,0.0374,0.0611,0.0536,0.0177,0.0731,0.0267,0.036,0.0539,-0.1638,0.0677,0.0469,-0.0201,-0.0103,-0.0236,-0.0065,0.0215,-0.0369,0.0414,0.0363,0.0508,-0.0102,-0.0299,0.0099,-0.0226,-0.0394,0.024,0.025,0.021,-0.0143,-0.003,-0.0214,-0.0323,0.0349,-0.0242,0.0321,-0.0083,-0.0484,-0.0416,-0.0469,0.03,-0.0617,-0.0467,0.0267,-0.0193,-0.0387,0.2115,-0.0267,0.0344,0.0436,-0.0285,0.0509,-0.0076,-0.0737,-0.0098,-0.0073,0.0608,-0.0473,0.0118,-0.004,0.0086,0.0278,0.0038,0.0516,0.0206,0.0499,-0.0182,-0.0332,0.0101,0.0508,-0.0161,0.022,-0.0562,0.0173,0.1286,-0.0089,0.053,0.0504,-0.0522,-0.077,-0.0076,-0.0029,0.0325,0.0466,-0.0349,0.0325,0.0044,-0.049,-0.0477,0.0557,-0.1171,-0.0106,0.137,-0.0133,0.05,-0.0756,-0.1048,0.0023,0.0075,-0.0669,-0.0,0.0479,0.0225,0.0158,0.0313,-0.048,0.0014,-0.0222,-0.0712,-0.0092,0.061,0.0153,-0.0898,-0.0061,0.0051,0.0153,-0.0115,0.0549,0.0176,-0.0863,-0.0028,0.0825,0.0178,-0.0546,0.0216,-0.006,-0.0007,0.0498,-0.0505,-0.0162,-0.003,0.0916,-0.061,0.0152,-0.0901,0.0276,0.0622,-0.0114,0.0447,-0.0518,-0.0424,-0.0109,-0.0648,-0.0355,0.0019,-0.007,-0.0573,0.0377,0.0083,-0.0008,0.0099,0.0505,-0.0059,-0.0124,0.0019,0.0655,0.0748,-0.0318,0.019,0.027,-0.0258,-0.0579,0.0046,0.093,0.0402,-0.0212,0.0288,0.0781,-0.0515,-0.0687,-0.1966,0.0089,0.0208,-0.0228,0.0207,-0.0475,0.0392,-0.0117,0.0268,0.0545,0.063,-0.0068,-0.0049,0.0148,-0.0141,0.0333,0.0832,0.0257,-0.0525,0.0082,-0.018,0.0164,-0.0316,-0.0806,0.036,0.0172,0.2156,0.0292,-0.0016,-0.0259,0.0115,0.0182,-0.0245,-0.1209,0.0327,0.0101,0.0113,-0.033,-0.0763,-0.0505,-0.0622,0.0104,-0.011,-0.0864,0.0105,-0.0383,-0.0252,0.048,-0.0347,-0.0085,0.0207,0.002,0.0002,-0.0092,-0.0119,-0.0272,-0.0477,0.0233,-0.0401,0.003,-0.0533,-0.08,-0.0135,-0.0501,0.0599,-0.0072,-0.013,-0.0266,0.0013,0.0016,0.0036,0.0865,0.0173,0.0143,0.0614,0.0061,0.0517,-0.0192,-0.0279,-0.0318,0.0345,0.0203,0.0469,0.0485,0.02,-0.0217,0.0615,-0.0166,0.0281,-0.0459,0.0225,0.0115,-0.0835,-0.051,0.0265,0.0355,-0.267,0.08,-0.0062,0.0355,-0.0025,0.0128,0.0309,0.045,-0.0333,0.0002,-0.0065,-0.0389,0.0524,-0.0499,0.018,0.0127,0.0101,-0.0454,0.068,-0.0705,0.0221,0.0161,0.2087,-0.0712,-0.006,0.0631,0.0368,0.0101,0.0564,-0.0071,-0.0307,0.0197,0.0696,-0.0222,0.0127,0.0987,-0.0179,0.0225,0.0277,0.0114,0.0508,0.0153,-0.0093,-0.0419,0.0879,-0.0017,-0.0109,-0.037,0.0144,0.0322,0.0036,-0.0592,0.0096,-0.0005,0.0247,0.0236,-0.0928,-0.0397,-0.0503,-0.0629,0.0137,-0.0462,-0.0342,0.0187,-0.0117]}
{"key":"[Low Precision Neural Networks using Subband Decomposition] Large-scale deep neural networks (DNN) have been successfully used in a number of tasks from image recognition to natural language processing. They are trained using large training sets on large models, making them computationally and memory intensive. As such, there is much interest in research development for faster training and test time. In this paper, we present a unique approach using lower precision weights for more efficient and faster training phase. We separate imagery into different frequency bands (e.g. with different information content) such that the neural net can better learn using less bits. We present this approach as a complement existing methods such as pruning network connections and encoding learning weights. We show results where this approach supports more stable learning with 2-4X reduction in precision with 17X reduction in DNN parameters.","layer":0,"vector":[-0.0383,-0.0198,0.0176,-0.0183,0.0415,0.0555,-0.0166,0.0081,0.0411,-0.0215,0.0079,-0.0771,0.1004,0.0387,0.0107,0.0171,0.0395,0.0286,0.0025,0.0231,0.0307,-0.0187,-0.0148,0.0071,0.035,-0.0375,-0.0465,-0.0466,-0.0408,-0.2359,0.0629,-0.0148,0.0609,-0.0437,-0.0037,-0.0445,-0.0493,0.0158,-0.0496,0.0179,0.0423,0.0053,-0.0107,-0.0377,0.0013,-0.0203,-0.0112,-0.0471,0.0039,-0.0392,0.0361,-0.0374,0.0181,0.0529,-0.0146,0.0212,0.0525,0.0254,0.0514,0.0455,0.0093,0.0416,-0.1627,0.0668,-0.0049,0.0033,-0.0403,-0.019,-0.0185,0.0572,-0.0424,0.0418,0.0745,0.0374,0.0379,0.0166,-0.009,-0.0398,-0.0221,-0.0141,0.0232,-0.0135,-0.0338,-0.0399,-0.0255,-0.0108,0.0096,-0.0293,-0.0252,-0.0331,-0.0376,-0.0238,-0.0335,0.0129,-0.0464,0.0125,0.0077,0.0471,-0.0674,0.2054,-0.0501,0.0091,0.0298,-0.0307,0.033,-0.0357,-0.0486,0.0034,-0.0489,0.0027,0.0249,-0.0333,0.0018,-0.0387,0.0206,0.012,0.0363,0.0449,-0.0209,0.0329,-0.0386,0.0015,0.0533,0.0071,0.0477,-0.045,-0.0229,0.1284,0.036,0.0678,0.0081,-0.0135,-0.0461,-0.0282,0.0225,0.044,0.0379,-0.0072,-0.0074,-0.0089,-0.0869,-0.0613,0.0329,-0.0575,-0.0415,0.0903,-0.0221,0.0479,-0.0272,-0.04,-0.0201,0.0276,-0.0258,-0.0635,0.0424,0.0673,0.0373,0.0026,-0.0668,0.0134,0.0301,-0.0579,-0.0168,0.0906,0.0197,-0.0827,-0.0273,-0.0346,-0.0296,-0.026,0.0123,0.0061,0.0186,0.026,0.0726,0.0306,-0.0723,0.0035,-0.0054,0.0068,0.0025,-0.061,-0.0142,0.0377,0.0505,-0.0173,0.025,-0.0895,0.0351,0.0079,-0.0534,0.063,-0.0237,-0.0487,-0.0238,-0.045,-0.0389,-0.0133,-0.0129,-0.0627,0.016,-0.027,-0.0332,0.0282,-0.0326,0.0563,0.0386,-0.0183,0.0163,0.0471,-0.008,0.0044,0.0823,-0.0417,-0.0138,-0.0222,0.0101,0.0229,-0.0077,0.0676,0.053,-0.0793,-0.0959,-0.2255,-0.0012,0.0505,-0.0616,0.0772,-0.0635,0.0331,0.0059,0.0634,0.0461,0.0223,-0.0024,-0.026,-0.0003,-0.0307,0.0544,0.0419,0.0129,0.0127,-0.0486,-0.0164,0.0281,-0.0023,-0.0821,0.0992,-0.0098,0.2136,-0.0297,0.053,-0.0288,0.0003,0.0082,0.011,-0.0744,0.064,-0.005,0.0765,0.0426,-0.0333,0.0036,-0.0657,0.0277,0.0177,-0.0949,-0.037,-0.0019,-0.0346,0.0071,-0.0442,0.0042,0.0319,-0.005,0.0149,0.004,-0.0126,-0.0026,-0.092,0.0495,-0.0724,-0.0034,0.0196,-0.0789,-0.0139,-0.032,0.0557,0.0389,-0.0578,-0.008,0.0321,-0.0104,-0.0215,0.0713,-0.0048,-0.0057,0.0652,0.0064,0.0097,-0.0188,-0.0411,0.0067,0.1009,0.0096,0.0451,0.0001,0.033,0.0468,0.089,-0.0396,0.0302,-0.0113,0.0369,0.0382,-0.0365,0.0163,0.0156,-0.0307,-0.3029,0.0581,0.0237,0.024,-0.0387,0.0518,0.0375,0.0158,-0.0411,-0.0011,-0.0164,0.0236,0.0518,-0.009,-0.006,0.0577,0.0649,-0.0569,0.0703,-0.0294,0.01,0.0349,0.2257,-0.0409,0.0422,0.0322,-0.0068,-0.0052,0.0303,0.0092,0.0483,0.0037,0.0979,-0.0281,0.0121,0.0933,-0.0905,-0.0002,0.0169,-0.0141,0.0077,-0.0323,-0.0165,-0.0271,0.1163,0.0003,-0.0187,-0.0688,-0.0151,0.0311,-0.0134,-0.0381,-0.0259,-0.0129,0.0554,0.0232,-0.0272,-0.0148,-0.0213,-0.0202,0.0192,-0.0954,-0.0334,-0.0393,-0.0545]}
{"key":"[Shape-CD: Change-Point Detection in Time-Series Data with Shapes and Neurons] Change-point detection in a time series aims to discover the time points at which some unknown underlying physical process that generates the time-series data has changed. We found that existing approaches become less accurate when the underlying process is complex and generates large varieties of patterns in the time series. To address this shortcoming, we propose Shape-CD, a simple, fast, and accurate change point detection method. Shape-CD uses shape-based features to model the patterns and a conditional neural field to model the temporal correlations among the time regions. We evaluated the performance of Shape-CD using four highly dynamic time-series datasets, including the ExtraSensory dataset with up to 2000 classes. Shape-CD demonstrated improved accuracy (7-60% higher in AUC) and faster computational speed compared to existing approaches. Furthermore, the Shape-CD model consists of only hundreds of parameters and require less data to train than other deep supervised learning models.","layer":1,"vector":[-0.0543,0.0131,0.0862,0.0197,0.0467,0.0572,0.0386,-0.0072,0.0309,-0.0331,0.0423,-0.0526,-0.0031,0.0281,-0.014,-0.024,-0.0387,0.0528,-0.0389,0.0109,0.0042,-0.0026,-0.0138,-0.0481,0.0266,0.0217,-0.023,-0.0261,-0.0796,-0.2461,0.0448,-0.0079,0.0118,0.0027,0.0161,-0.031,-0.044,0.033,-0.0197,0.0326,0.005,0.0297,0.0022,-0.0751,-0.0425,-0.106,0.0248,0.0197,0.0227,-0.0316,0.0073,-0.0494,0.0768,0.0441,0.0372,0.0343,0.0494,0.0428,0.0776,0.0401,0.0443,0.0427,-0.2055,0.0254,0.0339,0.0071,0.0123,-0.0295,-0.003,0.0233,-0.0444,0.0494,-0.0066,-0.0105,0.0192,0.0085,-0.0298,-0.0151,-0.0135,0.0297,0.0324,0.006,-0.0293,-0.0558,-0.0158,-0.0669,0.0047,-0.0735,0.0459,-0.0303,-0.0695,0.0072,-0.0328,-0.0018,-0.0747,-0.0309,0.0405,0.0211,-0.0308,0.1868,-0.1014,0.0531,0.0349,-0.0194,0.0046,-0.0551,0.009,-0.0414,-0.0103,0.0393,-0.0109,-0.0178,0.013,-0.0527,0.0258,-0.0125,0.0429,0.0565,-0.015,0.0097,-0.019,0.0265,0.0634,-0.062,0.0343,-0.0596,0.0752,0.1138,0.032,0.0123,0.0398,0.0117,-0.0914,-0.009,0.0166,0.045,0.0098,0.0108,0.021,0.0032,-0.0356,-0.0227,0.0369,-0.0747,-0.038,0.1214,-0.0369,0.0071,-0.0652,0.0005,-0.0274,0.0345,-0.046,-0.0495,-0.0026,0.0226,0.044,0.0134,-0.045,-0.0068,-0.044,-0.039,-0.005,0.092,0.0517,-0.0818,-0.0341,-0.0363,0.0252,0.0051,0.0391,0.0544,-0.0285,0.0157,0.1209,0.0336,-0.0426,0.0048,0.0343,0.0065,0.0679,-0.046,-0.0267,0.0645,0.0486,-0.046,0.0128,-0.0381,0.0204,0.0399,-0.0095,-0.0071,-0.0262,0.0368,0.0067,-0.0309,-0.0323,-0.0002,0.0059,-0.0652,-0.0325,-0.0345,-0.0193,0.0108,0.0157,0.0148,-0.0396,0.0043,0.0342,0.0369,0.0028,-0.0023,0.0584,-0.0466,-0.0192,-0.0135,-0.0292,0.0314,-0.0408,0.0426,0.0186,-0.0491,-0.0486,-0.2347,-0.0103,0.0164,-0.0091,0.0548,-0.0479,0.0105,-0.0205,0.0882,0.0381,0.068,-0.0044,-0.0086,-0.0155,-0.0101,0.0646,0.0697,0.0316,-0.0208,-0.0198,-0.0379,-0.0054,-0.0194,-0.0856,0.0281,0.0253,0.2044,0.0218,0.0626,-0.0146,-0.0274,-0.0015,0.0097,-0.0752,0.0734,0.0423,0.0511,-0.0019,-0.078,-0.0586,-0.0321,0.0153,0.0266,-0.0518,-0.0419,-0.0153,-0.0027,0.0289,-0.0598,-0.0286,0.0244,-0.0342,0.0503,0.0062,-0.0063,-0.0451,-0.0256,0.0358,-0.0226,-0.0045,0.0092,-0.0039,0.0181,-0.0593,0.0187,0.0005,-0.07,-0.0535,-0.0048,-0.0357,-0.0219,0.1224,0.0196,-0.0048,0.0811,-0.0288,0.0077,0.0204,-0.0775,-0.0106,0.0682,-0.0435,0.0405,0.0168,-0.0025,0.0019,0.0655,-0.0226,0.0622,-0.0115,0.0539,-0.0077,-0.0373,-0.0193,0.0536,0.0211,-0.3079,0.0248,-0.0188,0.0287,-0.0176,0.0128,-0.0255,0.0551,-0.0284,-0.0004,-0.0385,0.0102,0.0568,-0.0327,0.0123,0.042,0.0605,-0.0497,0.0601,-0.042,0.0172,0.0453,0.2275,-0.0133,0.032,-0.0111,-0.0223,-0.0199,0.0772,0.0103,0.033,0.0164,0.065,-0.027,-0.0016,0.098,-0.06,0.0305,0.0026,-0.0307,0.035,0.0164,-0.0467,-0.0287,0.0824,0.0064,-0.032,-0.027,0.0168,0.027,-0.0449,-0.022,-0.0377,0.029,-0.0025,0.0511,-0.0596,-0.0395,0.0191,-0.0577,-0.0028,-0.0477,-0.0129,-0.0063,0.0038]}
{"key":"[Monitoring Spatial Sustainable Development: semi-automated analysis of Satellite and Aerial Images for Energy Transition and Sustainability Indicators] This report presents the results of the DeepSolaris project that was carried out under the ESS action 'Merging Geostatistics and Geospatial Information in Member States'. During the project several deep learning algorithms were evaluated to detect solar panels in remote sensing data. The aim of the project was to evaluate whether deep learning models could be developed, that worked across different member states in the European Union. Two remote sensing data sources were considered: aerial images on the one hand, and satellite images on the other. Two flavours of deep learning models were evaluated: classification models and object detection models. For the evaluation of the deep learning models we used a cross-site evaluation approach: the deep learning models where trained in one geographical area and then evaluated on a different geographical area, previously unseen by the algorithm. The cross-site evaluation was furthermore carried out twice: deep learning models trained on he Netherlands were evaluated on Germany and vice versa. While the deep learning models were able to detect solar panels successfully, false detection remained a problem. Moreover, model performance decreased dramatically when evaluated in a cross-border fashion. Hence, training a model that performs reliably across different countries in the European Union is a challenging task. That being said, the models detected quite a share of solar panels not present in current solar panel registers and therefore can already be used as-is to help reduced manual labor in checking these registers.","layer":1,"vector":[-0.0319,-0.0125,0.0474,0.0238,0.0892,0.0235,0.0447,-0.0254,0.0352,0.006,-0.0097,-0.0847,0.0298,0.0859,0.02,0.0065,-0.0331,0.036,-0.0186,-0.0133,0.05,0.007,-0.0211,-0.0609,0.0413,0.0311,-0.0073,-0.0487,-0.0727,-0.2396,0.0209,-0.065,0.0239,-0.0077,0.0048,-0.0642,-0.0325,0.0128,-0.0461,0.0071,0.0158,-0.0299,-0.008,-0.0416,-0.0428,-0.0381,-0.0101,-0.0225,-0.0724,-0.0356,0.0728,-0.0234,0.0062,0.0215,0.0171,0.0329,0.0527,0.0648,0.0621,0.0205,0.0484,0.0437,-0.2408,0.048,0.0318,0.0419,-0.0166,-0.0224,-0.0075,0.0041,0.0127,0.0605,0.0068,0.0382,0.012,0.0338,-0.0475,-0.0075,-0.005,-0.0542,0.0319,0.0414,-0.018,-0.0112,0.0376,-0.0434,-0.0004,0.0064,0.0895,-0.0111,-0.0309,-0.0022,-0.0223,0.0374,-0.0535,-0.0122,0.0592,0.0012,-0.0329,0.1679,-0.0632,0.0362,0.0683,-0.0156,0.0058,-0.0263,-0.0367,-0.02,-0.0255,-0.012,-0.0352,-0.0408,0.0022,-0.0149,0.0182,-0.0232,0.044,0.0521,-0.0282,0.0017,-0.0405,-0.0124,0.0737,0.006,0.0401,-0.0159,0.0338,0.1065,0.0279,0.0602,0.0324,-0.0252,-0.059,0.0095,0.0156,0.0348,0.0439,0.023,-0.0014,0.0068,-0.0127,-0.0298,0.051,-0.1003,-0.004,0.0817,-0.0376,0.0344,-0.0649,-0.0027,0.0209,0.0283,-0.0088,-0.033,0.051,0.0407,0.0005,0.0317,-0.0247,0.0249,0.0101,-0.0216,-0.0508,0.1464,-0.0105,-0.0757,-0.0491,-0.0267,0.0239,0.0266,0.0525,0.0888,-0.0131,-0.0221,0.0811,0.01,-0.0619,-0.0067,-0.0177,0.0065,-0.0004,-0.0402,-0.0345,0.0362,0.051,-0.0197,-0.0399,-0.078,-0.0016,0.0306,-0.0568,0.0176,-0.0419,0.0365,0.0015,-0.0044,-0.037,-0.0327,0.0391,-0.0165,0.0244,0.0262,-0.0105,0.0047,-0.0028,0.0018,-0.011,0.0221,0.0532,0.016,-0.0021,0.004,0.0763,-0.064,-0.0198,-0.0073,0.0629,0.022,-0.0174,0.0498,0.0087,-0.0597,-0.0532,-0.2531,-0.0241,0.0258,0.0145,0.0375,-0.0697,0.0517,0.0099,0.0118,0.0426,0.0892,-0.0219,-0.0165,0.027,-0.0094,0.0341,0.0492,0.018,-0.0655,-0.026,-0.0105,-0.0071,-0.0006,-0.1002,0.0579,-0.0417,0.1968,0.014,0.0335,-0.0124,-0.0095,0.009,-0.0276,-0.1363,0.0551,0.0055,0.0851,0.0074,-0.0877,-0.0417,0.0006,0.01,-0.0226,-0.0632,-0.0273,-0.0232,-0.0064,0.004,-0.0116,0.0018,0.0151,-0.0874,0.0326,-0.0021,-0.0054,0.005,-0.1251,0.0394,-0.0192,0.0148,0.0086,-0.0251,-0.0136,-0.0381,0.0813,-0.0114,-0.0594,-0.0211,0.0044,0.0151,-0.0244,0.1598,-0.0127,-0.0444,0.0484,-0.0069,0.0385,0.0125,-0.0057,-0.0215,0.0805,-0.0115,0.0058,0.0068,0.0478,0.0019,0.0502,-0.0529,-0.0115,-0.0318,0.0493,-0.0075,-0.0554,-0.0098,0.0656,-0.0079,-0.2665,0.0632,0.005,0.0319,-0.0283,-0.0121,0.0298,0.0319,-0.0099,-0.0424,0.0124,0.0012,0.0593,-0.02,-0.0159,0.0326,0.0393,-0.0489,0.067,-0.0424,0.0642,0.0275,0.2096,-0.0157,0.0021,0.0355,-0.0402,-0.0018,0.0154,-0.0114,-0.0166,0.0148,0.0838,-0.0345,0.0096,0.1302,-0.0237,0.0236,0.0176,-0.0418,-0.0138,0.0082,0.0132,-0.0146,0.0761,-0.0319,-0.0073,-0.0464,-0.0041,0.03,-0.0089,-0.0277,-0.068,-0.0265,-0.0036,-0.0089,-0.0543,-0.0515,-0.0478,-0.0446,0.0429,-0.0412,-0.0207,-0.0457,0.0061]}
{"key":"[A First-Order Algorithmic Framework for Wasserstein Distributionally Robust Logistic Regression] Wasserstein distance-based distributionally robust optimization (DRO) has received much attention lately due to its ability to provide a robustness interpretation of various learning models. Moreover, many of the DRO problems that arise in the learning context admits exact convex reformulations and hence can be tackled by off-the-shelf solvers. Nevertheless, the use of such solvers severely limits the applicability of DRO in large-scale learning problems, as they often rely on general purpose interior-point algorithms. On the other hand, there are very few works that attempt to develop fast iterative methods to solve these DRO problems, which typically possess complicated structures. In this paper, we take a first step towards resolving the above difficulty by developing a first-order algorithmic framework for tackling a class of Wasserstein distance-based distributionally robust logistic regression (DRLR) problem. Specifically, we propose a novel linearized proximal ADMM to solve the DRLR problem, whose objective is convex but consists of a smooth term plus two non-separable non-smooth terms. We prove that our method enjoys a sublinear convergence rate. Furthermore, we conduct three different experiments to show its superb performance on both synthetic and real-world datasets. In particular, our method can achieve the same accuracy up to 800+ times faster than the standard off-the-shelf solver.","layer":1,"vector":[0.0269,0.0028,0.0029,-0.0159,0.0355,0.0372,0.0051,-0.0031,0.0179,-0.0097,0.0141,-0.0446,-0.0023,0.064,0.032,0.022,0.0163,0.0241,-0.0632,0.0461,-0.009,-0.0813,-0.0117,-0.0783,0.0802,0.0129,-0.0323,-0.041,-0.0325,-0.3055,0.0111,-0.0581,0.0191,-0.0542,0.0334,-0.0011,-0.0142,0.0573,-0.0393,0.0483,0.024,0.0331,-0.0223,-0.0674,-0.0451,-0.0491,0.0046,-0.0067,-0.0562,-0.0839,0.0082,-0.0512,0.0118,0.0154,0.0183,-0.008,0.0545,0.0234,0.0246,0.0581,0.0052,0.0489,-0.1414,0.0179,0.0355,0.0344,-0.0502,-0.0253,-0.0241,0.0476,-0.0208,0.0122,0.0244,0.0549,-0.0002,0.0106,0.0457,-0.0178,-0.0293,0.0271,0.0595,-0.05,-0.0181,0.0009,-0.0392,-0.0557,0.04,-0.0762,0.0417,0.0068,-0.0123,-0.0271,-0.0243,0.0394,-0.0783,-0.0866,0.0574,-0.0047,-0.0254,0.2019,-0.0317,0.0604,0.0505,-0.0564,0.0097,-0.0314,-0.0211,-0.0194,0.0036,-0.0165,-0.0343,0.0126,0.0365,-0.0241,0.0091,0.0417,0.0706,0.032,-0.0013,0.0024,-0.03,-0.0016,0.0295,-0.009,0.0314,-0.0616,-0.0021,0.1428,0.0211,0.0434,0.0265,-0.0033,-0.0686,-0.0329,0.0006,-0.0176,-0.0289,0.007,0.0204,0.0122,-0.011,-0.0389,0.0256,-0.0807,-0.0356,0.1073,-0.0176,0.0338,-0.0661,-0.0518,0.0033,-0.021,-0.017,-0.004,0.0463,0.0298,0.0109,0.0262,-0.0316,0.0094,-0.0337,-0.1335,0.017,0.0693,-0.0016,-0.081,0.0061,0.0408,0.0252,-0.0231,0.0317,0.026,0.0229,0.0692,0.053,0.002,-0.0373,-0.0078,-0.0256,0.0113,0.0201,-0.0753,-0.0205,0.0207,0.0442,-0.038,0.0049,-0.0275,0.0384,0.0501,-0.0323,-0.0255,-0.0146,-0.0103,0.005,-0.0139,-0.0305,-0.0036,0.0446,0.0216,0.0075,0.0227,-0.0737,0.028,0.0123,0.0387,0.0451,-0.0067,0.0417,-0.0025,-0.0468,-0.0109,0.0729,-0.0452,-0.0051,0.0116,0.037,0.0566,0.0248,0.0794,0.0398,0.0066,-0.0602,-0.1979,-0.0557,0.0214,-0.0109,0.0245,-0.0766,0.0358,-0.019,0.0495,0.1009,0.0424,0.0026,0.0056,0.0345,-0.0314,0.047,0.0623,0.0053,-0.0437,-0.013,-0.0054,0.0288,-0.0254,-0.0243,0.0743,0.0016,0.1791,-0.0035,0.0462,-0.0186,0.0122,0.0274,0.0068,-0.0473,0.015,-0.0059,0.0497,-0.0586,-0.0758,-0.0556,0.0077,0.0207,0.0135,-0.0926,-0.0432,-0.0448,-0.0398,0.0519,-0.0579,0.0475,0.0676,-0.0236,0.0865,-0.0347,0.0217,0.0026,-0.1192,0.0505,-0.0524,0.0197,0.0069,-0.0668,0.0205,-0.0334,0.0331,-0.0014,0.0091,-0.0588,0.0154,-0.0434,0.0216,0.0694,-0.0217,0.0139,0.0684,0.0016,0.0363,0.0003,-0.0434,-0.0312,0.076,-0.0112,0.0558,-0.0278,0.0294,0.0224,0.033,0.0148,0.0328,0.0143,0.002,-0.0325,-0.0514,-0.0263,0.0494,0.0386,-0.2973,0.0087,-0.0278,0.028,-0.0453,-0.0003,0.0382,0.0234,-0.0493,-0.0252,0.0147,0.0469,0.0265,-0.0084,0.0098,-0.0458,0.0599,-0.0474,0.078,-0.103,0.0168,0.0358,0.2062,-0.0358,0.0173,0.0034,-0.0181,-0.0034,0.0176,-0.0483,-0.0186,0.0081,0.052,-0.0511,0.0438,0.0805,-0.0086,0.0719,0.0106,-0.0193,-0.0064,0.0453,-0.024,-0.0277,0.122,-0.0266,-0.0086,-0.0697,0.0286,0.0707,-0.0455,0.0015,0.0128,0.0468,0.0065,-0.0004,-0.0088,-0.0638,-0.0848,-0.0588,0.0568,-0.0233,-0.0634,-0.0085,0.0128]}
{"key":"[TransforMAP: Transformer for Memory Access Prediction] Data Prefetching is a technique that can hide memory latency by fetching data before it is needed by a program. Prefetching relies on accurate memory access prediction, to which task machine learning based methods are increasingly applied. Unlike previous approaches that learn from deltas or offsets and perform one access prediction, we develop TransforMAP, based on the powerful Transformer model, that can learn from the whole address space and perform multiple cache line predictions. We propose to use the binary of memory addresses as model input, which avoids information loss and saves a token table in hardware. We design a block index bitmap to collect unordered future page offsets under the current page address as learning labels. As a result, our model can learn temporal patterns as well as spatial patterns within a page. In a practical implementation, this approach has the potential to hide prediction latency because it prefetches multiple cache lines likely to be used in a long horizon. We show that our approach achieves 35.67% MPKI improvement and 20.55% IPC improvement in simulation, higher than state-of-the-art Best-Offset prefetcher and ISB prefetcher.","layer":0,"vector":[-0.0716,-0.0112,0.0308,0.0001,0.0053,0.0419,0.0088,0.0128,0.036,-0.0263,0.0115,-0.0221,0.0315,0.0121,-0.0092,0.0133,-0.0081,0.0297,0.0023,-0.0289,0.0973,-0.042,-0.0268,-0.035,0.0131,0.031,-0.0104,-0.0474,-0.0452,-0.2273,-0.0291,-0.0766,0.0449,-0.0226,0.0132,-0.0333,-0.0396,0.095,-0.0385,0.0563,0.0031,0.0244,-0.0538,-0.0172,-0.0325,-0.0527,0.0225,-0.0717,0.0115,-0.0294,-0.0295,-0.0093,0.0216,0.0312,0.0214,0.015,0.039,0.0512,0.0411,0.0186,0.0382,0.041,-0.1509,0.0757,0.0528,0.0192,-0.0591,-0.0414,0.0303,0.0321,-0.0735,0.0462,0.0248,0.067,0.0325,0.0077,-0.0161,0.0068,0.0086,0.007,-0.0299,-0.0374,-0.0084,-0.0438,-0.0441,-0.0433,0.0109,-0.0114,0.0334,-0.0326,-0.0534,-0.0297,-0.0201,0.0647,-0.0784,0.0105,0.0612,0.062,-0.0626,0.1836,-0.0669,0.0479,0.0211,-0.0375,0.0376,-0.0364,-0.0004,0.0061,-0.0251,-0.0314,0.0027,-0.0538,0.0005,-0.0513,0.0601,-0.0039,0.0934,0.0552,-0.0227,-0.0023,-0.0491,0.0239,0.0364,-0.0089,0.014,-0.0745,0.0279,0.1245,0.0436,0.0133,0.0487,-0.0188,-0.0347,0.0064,0.0072,0.038,0.0156,-0.003,0.0045,-0.0496,-0.0563,0.0062,0.0045,-0.0573,-0.0412,0.128,0.0358,0.0335,-0.0455,-0.044,-0.0203,0.0507,0.0008,-0.0381,0.0344,0.0703,-0.0183,0.0617,-0.0319,0.0355,-0.0176,-0.0108,-0.037,0.0608,-0.0074,-0.0588,-0.0206,0.0031,0.0025,-0.035,0.0401,0.0092,-0.0377,-0.0059,0.051,0.0501,-0.0567,-0.0148,0.0044,-0.0078,0.0443,-0.012,-0.0508,0.024,0.0871,-0.0656,-0.0009,0.0135,0.0365,0.0346,-0.0304,0.0184,-0.0592,-0.0073,-0.0067,-0.0197,-0.0191,-0.0025,0.0152,-0.0687,0.0164,0.0138,-0.0668,0.0509,-0.0393,-0.032,-0.0369,-0.0013,0.0525,0.0183,-0.0017,-0.0483,0.0595,-0.0182,-0.0059,-0.0359,0.0213,0.0571,0.0273,0.0637,0.0266,-0.0688,-0.0415,-0.2205,0.0064,-0.0054,-0.0178,0.0356,-0.0613,0.0375,0.0079,0.0274,0.0342,0.0315,-0.0627,-0.0478,0.0676,-0.0565,0.0481,0.048,0.0365,-0.0423,-0.0054,0.0042,-0.002,-0.0047,-0.0963,0.0743,0.0044,0.2184,0.0131,0.0239,-0.0768,0.0406,0.0325,-0.0688,-0.0736,0.075,0.0293,0.0663,0.0058,0.0426,-0.0195,-0.0475,0.0105,0.0033,-0.1376,-0.0545,-0.0343,-0.0266,0.0081,-0.0188,0.0187,0.0333,-0.0692,0.0338,0.0399,0.0004,-0.0589,-0.0427,-0.0203,-0.0429,0.0237,0.0168,-0.0631,-0.0074,-0.0658,0.0038,0.0245,-0.0408,-0.0579,-0.0097,-0.0436,-0.0504,0.0528,-0.0242,-0.0294,0.0355,-0.0015,0.0497,-0.0091,-0.04,-0.0118,0.0773,-0.0345,0.0371,0.0075,0.0169,0.0366,0.0752,0.0126,0.0463,0.0025,-0.0364,0.014,-0.0377,-0.004,0.0306,-0.024,-0.3079,0.0373,-0.013,0.0265,0.0332,0.0091,0.0577,0.017,-0.0365,0.0463,-0.0327,0.0581,0.0548,0.0012,-0.0133,0.0565,0.1439,-0.0237,-0.0111,-0.054,0.0138,-0.0083,0.24,-0.0067,0.0311,0.0105,-0.0223,0.0005,0.0637,-0.003,0.0413,-0.0275,0.0675,-0.03,0.0499,0.0902,-0.0344,0.0325,0.0316,0.0027,0.0081,0.0266,-0.0551,-0.0021,0.0741,-0.0198,-0.017,-0.0707,0.0208,0.0044,0.0047,0.0048,-0.0141,0.0039,0.0649,0.0288,-0.0463,-0.0438,-0.0518,-0.0208,0.0576,-0.031,-0.0179,-0.0383,0.0089]}
{"key":"[Adversarial Likelihood-Free Inference on Black-Box Generator] Generative Adversarial Network (GAN) can be viewed as an implicit estimator of a data distribution, and this perspective motivates using the adversarial concept in the true input parameter estimation of black-box generators. While previous works on likelihood-free inference introduces an implicit proposal distribution on the generator input, this paper analyzes theoretic limitations of the proposal distribution approach. On top of that, we introduce a new algorithm, Adversarial Likelihood-Free Inference (ALFI), to mitigate the analyzed limitations, so ALFI is able to find the posterior distribution on the input parameter for black-box generative models. We experimented ALFI with diverse simulation models as well as pre-trained statistical models, and we identified that ALFI achieves the best parameter estimation accuracy with a limited simulation budget.","layer":4,"vector":[-0.0294,0.0067,0.0121,-0.0252,0.0428,0.018,0.0081,-0.021,0.0358,-0.0046,0.0028,-0.043,0.0244,0.084,0.0106,-0.0015,0.0199,0.0079,-0.0366,-0.0001,0.0695,-0.0456,0.016,-0.0351,0.0073,0.02,-0.0121,-0.0476,-0.0497,-0.2412,0.0013,-0.048,0.068,0.0056,-0.0212,-0.0146,-0.0664,0.0526,0.002,0.0341,0.027,0.0248,-0.0089,-0.0335,-0.0171,0.0139,0.0137,-0.0163,-0.034,-0.0361,0.0564,-0.0322,0.0284,0.0425,0.0501,0.0059,0.067,0.0136,0.0288,0.0894,-0.0223,0.0769,-0.1719,0.0649,0.0496,0.0296,-0.0524,-0.0489,-0.0001,0.0241,-0.0287,0.0366,0.0201,0.0495,0.0119,-0.0328,-0.0057,-0.0553,0.024,-0.0036,0.0068,-0.016,-0.0229,0.0377,0.0007,-0.0307,-0.0131,-0.0148,0.0554,-0.0183,-0.0441,0.0098,-0.0551,0.0411,-0.0711,-0.0154,0.009,-0.0063,-0.0713,0.2098,-0.0425,0.0033,0.0348,0.0006,-0.0025,-0.0377,-0.0464,-0.0476,-0.045,0.0116,-0.0164,-0.0119,0.0049,-0.0308,0.0441,0.0144,0.0405,0.0222,-0.0254,-0.054,-0.0624,0.0136,0.0347,0.0041,0.017,-0.0589,0.0204,0.1611,0.0599,0.0151,0.0346,-0.0525,-0.0962,-0.0324,0.0022,-0.0308,0.0418,0.0212,-0.0018,-0.0039,-0.06,-0.057,-0.0214,-0.0353,-0.0033,0.0758,-0.0165,0.03,-0.0501,-0.013,-0.0091,0.0334,-0.0095,-0.005,0.037,0.0405,-0.0093,0.0558,-0.0379,0.023,-0.0569,-0.0598,-0.0325,0.0769,-0.0026,-0.0828,-0.0624,0.0206,0.0301,0.0367,0.001,0.0116,-0.0377,0.0357,0.0815,0.0305,-0.0972,-0.0329,-0.01,0.048,-0.0127,-0.0484,0.0078,0.0549,0.0265,-0.055,0.0099,-0.0418,0.0159,0.0569,-0.0421,-0.0005,-0.0107,-0.0154,-0.0361,-0.0559,-0.0225,-0.0194,0.0075,-0.0449,-0.0358,0.0078,-0.0794,-0.0365,-0.0372,0.0405,-0.007,-0.0061,0.0706,0.0211,0.002,0.0115,0.0931,-0.0129,-0.02,0.0311,-0.025,0.0246,-0.0211,0.0591,0.0241,-0.0674,-0.0084,-0.2386,0.0142,0.0191,-0.0291,0.0448,-0.0663,0.0417,0.0014,0.0615,0.0682,0.0427,0.0139,0.0377,0.0385,-0.0276,0.041,-0.0432,0.0028,-0.0127,0.0226,-0.0139,0.0194,-0.0343,-0.117,0.0379,0.0202,0.2112,0.0246,0.0491,-0.0189,0.0206,0.0373,0.0043,-0.1018,0.0399,0.0674,0.058,-0.017,-0.0081,0.0394,-0.0331,0.0586,-0.0197,-0.1153,-0.0263,-0.0343,-0.0185,0.0533,-0.0706,0.054,0.0351,0.0013,0.0674,-0.0244,-0.0041,-0.0415,-0.1443,0.0145,-0.0239,0.0294,0.0059,-0.0138,0.03,-0.0609,0.0532,-0.012,-0.0463,-0.0826,0.0482,-0.0236,0.0164,0.0683,0.0214,0.0506,0.0488,0.0047,0.048,-0.0486,-0.0729,-0.0355,0.0187,-0.013,0.0041,0.0325,0.0034,0.0235,0.065,0.0083,0.0103,-0.0158,0.0081,0.0262,-0.0442,-0.0297,0.0329,0.0238,-0.3239,0.0475,0.0239,0.0782,-0.0375,0.016,0.0546,0.0081,-0.064,-0.0212,-0.0196,0.0277,0.0437,-0.0112,0.0117,0.0046,0.0537,-0.0658,0.0478,-0.0426,0.033,0.0106,0.2144,-0.0605,0.0014,-0.0038,-0.0108,0.0266,0.0432,-0.0361,-0.0063,0.0088,0.0755,-0.0183,0.034,0.0784,-0.0365,0.0567,0.0207,-0.0426,-0.0154,0.006,-0.0334,0.014,0.0554,0.0088,-0.0109,0.0035,0.0095,0.0327,-0.0388,0.0418,0.0162,0.0318,0.0469,0.0076,-0.0279,-0.0568,-0.0067,-0.0051,-0.0046,-0.0425,-0.0073,0.0008,-0.0316]}
{"key":"[An Efficient Indoor Navigation Technique To Find Optimal Route For Blinds Using QR Codes] Blind navigation is an accessibility application that enables blind to use an android Smartphone in an easy way for indoor navigation with instructions in audio form. We have proposed a prototype which is an indoor navigation application for blinds that uses QR codes. It is developed for android Smart phones and does not require any additional hardware for navigation. It provides automatic navigational assistance on pre-defined paths for blind. QR codes are placed on the floor sections after specific distance that acts as an input for current location detection and navigation. Whenever a QR code is scanned it provides the user with the information of the current location and asks the user to select the destination and then offers optimal and shortest path using path finding algorithms. During navigation whenever the deviation from the proposed path is detected it prompts the user and guides back to the right path by comparing the current path with the generated path. All of the instructions throughout the application are provided in audio form to the user. The interface of the application is well built for blinds which makes the smart phones user-friendly and useable for blind people. The user interacts with the application through a specific set of user-friendly gestures for specific inputs and operations. At the end, we have performed comparison between different state of art approaches and concluded that our approach is more user friendly, cost effective and produced more accurate results.","layer":1,"vector":[-0.0302,-0.0219,0.0394,-0.0148,-0.0051,0.0137,0.0413,0.0337,0.0055,0.0085,-0.0055,-0.0252,0.0369,0.0218,0.0354,-0.0342,0.007,0.0372,0.0128,0.0169,0.0473,-0.0122,0.0023,-0.0821,0.025,0.0423,-0.0201,-0.0428,-0.0154,-0.1689,-0.0087,-0.0506,0.0561,-0.0249,-0.0454,-0.0191,-0.0683,0.0578,0.0149,0.0375,0.0535,0.0163,-0.0033,-0.0676,-0.0498,-0.0328,0.0003,-0.0156,-0.0222,-0.0798,-0.0396,-0.0003,0.0165,0.0098,0.0003,0.044,0.0304,0.0443,0.0357,0.0458,0.037,0.0193,-0.2,0.0942,0.0286,-0.0136,0.0033,-0.0622,0.0375,-0.0045,-0.0295,0.0593,0.0369,0.0134,0.058,-0.057,0.0065,-0.0415,-0.0069,0.0088,0.0207,-0.0259,-0.0571,0.0494,-0.0246,-0.0204,-0.0179,-0.0577,0.0184,0.0005,-0.0474,-0.0004,-0.0531,0.0327,-0.0528,-0.0457,-0.0018,-0.0065,-0.0509,0.2213,0.0092,0.053,0.0088,-0.0198,0.0611,-0.0379,0.0208,-0.0625,-0.0064,0.0465,0.0083,-0.0799,0.0384,0.0023,-0.0162,0.0454,0.0213,0.0412,0.0171,-0.0039,-0.0186,0.0187,0.0489,-0.0182,0.0218,-0.0758,0.0245,0.0919,-0.0047,0.0627,0.1016,-0.0707,-0.0051,0.0092,-0.0061,0.0254,0.0125,-0.0027,0.0456,-0.0412,-0.0163,-0.0767,0.0255,-0.1005,-0.0199,0.1162,-0.0651,-0.0073,-0.0498,-0.0464,0.0063,0.035,-0.0003,-0.0083,0.0296,-0.0022,0.0511,0.0478,-0.0385,0.0309,-0.0195,-0.0639,-0.054,0.0665,-0.0265,-0.1136,-0.0209,0.0135,-0.0263,-0.0358,0.0335,0.0572,-0.0579,0.0788,0.0654,0.0255,-0.0833,0.0156,-0.0496,-0.0045,0.001,-0.0688,-0.0525,-0.0207,0.0542,-0.0048,0.009,0.0023,0.0498,0.0668,0.0088,0.0186,-0.0449,-0.0228,-0.074,-0.0132,-0.0132,-0.0235,0.0414,-0.0381,0.0461,-0.0422,-0.0579,0.0475,0.0122,0.0103,0.0079,-0.0729,0.0334,0.0235,-0.068,0.0173,0.0983,-0.0587,0.0093,-0.0467,0.0082,0.0359,-0.0184,0.0261,0.0278,-0.0688,-0.0642,-0.1986,0.0279,0.0433,-0.0035,-0.021,-0.0355,-0.0178,-0.0344,0.0485,0.0163,0.1022,-0.0756,0.0218,0.0568,-0.0344,0.0565,0.0018,0.0517,-0.0192,-0.017,0.0142,-0.0025,-0.0146,-0.0558,0.0348,0.0212,0.2288,0.0154,0.0334,-0.0158,0.0622,0.0312,-0.0331,-0.1238,0.0105,0.0596,0.0472,-0.0217,-0.0063,-0.052,-0.0246,0.0264,-0.007,-0.0345,-0.0164,0.0013,-0.0356,-0.016,-0.0428,0.011,0.0249,-0.0097,0.0328,0.0353,-0.0327,-0.0505,-0.0238,0.0555,-0.0811,0.0736,-0.0355,-0.0453,0.0442,-0.0427,0.0822,0.0289,-0.0098,-0.0032,0.045,0.0021,-0.0302,0.0746,0.0077,0.0169,0.0278,0.0528,0.0738,-0.0554,0.0166,-0.0669,0.065,-0.0225,0.0457,0.0254,0.0009,-0.0007,0.0809,-0.0279,0.0229,-0.0312,-0.0017,-0.0138,-0.0122,-0.0548,0.0349,-0.032,-0.2681,-0.0152,-0.0123,-0.0038,-0.0731,-0.0044,0.0462,0.0041,-0.0092,0.0358,-0.0284,0.0136,0.0253,-0.0336,0.0229,0.0015,0.0822,0.0104,0.0732,-0.076,0.048,0.0319,0.24,-0.0283,0.0331,0.0156,-0.0007,-0.0301,0.0263,-0.0076,-0.0281,0.0464,0.1068,-0.0425,0.0638,0.042,-0.002,0.0527,-0.0036,0.0119,-0.0032,0.0301,-0.0549,-0.0172,0.1144,-0.0186,-0.0731,0.0087,0.0166,0.0193,-0.0517,-0.0264,-0.0388,0.0248,-0.0057,0.0306,-0.0587,-0.0458,-0.0631,-0.0386,0.0366,-0.0524,0.0264,-0.0063,0.0401]}
{"key":"[Event2Graph: Event-driven Bipartite Graph for Multivariate Time-series Anomaly Detection] Modeling inter-dependencies between time-series is the key to achieve high performance in anomaly detection for multivariate time-series data. The de-facto solution to model the dependencies is to feed the data into a recurrent neural network (RNN). However, the fully connected network structure underneath the RNN (either GRU or LSTM) assumes a static and complete dependency graph between time-series, which may not hold in many real-world applications. To alleviate this assumption, we propose a dynamic bipartite graph structure to encode the inter-dependencies between time-series. More concretely, we model time series as one type of nodes, and the time series segments (regarded as event) as another type of nodes, where the edge between two types of nodes describe a temporal pattern occurred on a specific time series at a certain time. Based on this design, relations between time series can be explicitly modelled via dynamic connections to event nodes, and the multivariate time-series anomaly detection problem can be formulated as a self-supervised, edge stream prediction problem in dynamic graphs. We conducted extensive experiments to demonstrate the effectiveness of the design.","layer":2,"vector":[-0.013,-0.0189,0.013,-0.0329,0.042,0.0001,0.0333,0.0147,0.0393,-0.043,0.0051,-0.0296,0.0127,0.0822,0.0056,0.0036,-0.013,0.0292,-0.0084,-0.0132,0.0277,-0.0238,0.0138,-0.0521,0.0571,0.0334,0.0026,-0.0228,-0.0859,-0.2134,0.0111,-0.0682,0.022,-0.0211,0.0199,-0.0423,-0.0131,0.0465,-0.0058,0.0553,0.0048,0.0158,0.0122,-0.0623,-0.0228,-0.0411,0.0225,-0.0113,-0.0362,-0.032,-0.0026,-0.0321,0.0494,0.0209,0.0247,0.0324,0.0475,0.0058,0.0576,0.0479,0.0291,0.0401,-0.1621,0.02,0.0592,0.0311,-0.035,0.0267,0.036,0.0333,-0.0116,0.0298,0.0044,0.0053,-0.0075,0.0404,-0.0168,-0.0139,-0.0099,-0.0017,-0.0268,-0.0103,-0.0326,-0.0426,-0.0012,-0.0581,0.0054,-0.0476,0.0429,0.0063,-0.0484,0.001,0.0152,0.03,-0.0556,0.0024,0.045,0.0573,-0.024,0.1947,-0.0724,0.0316,0.0213,0.0125,0.0452,-0.0294,-0.0332,-0.0858,-0.0416,-0.0126,-0.0082,-0.0276,0.0522,-0.0842,0.0369,-0.0099,0.0514,0.0598,-0.0029,0.0039,0.0002,0.0152,0.0718,-0.056,0.021,-0.057,0.0476,0.1274,0.0127,-0.0061,-0.0046,0.0299,-0.0665,0.0061,0.0155,-0.0103,-0.0105,-0.0255,0.0091,-0.0171,-0.0419,-0.0132,0.0408,-0.0515,-0.0796,0.119,-0.0376,0.0083,-0.0344,-0.0218,-0.0938,0.0038,-0.0359,-0.049,0.009,0.0312,0.0544,0.0181,-0.0513,0.0422,-0.0394,-0.0306,-0.0279,0.0831,0.0246,-0.1063,-0.0014,-0.0076,0.0188,-0.0068,0.0781,0.0292,-0.0179,0.0122,0.0865,0.0505,-0.0489,-0.0095,-0.0007,0.0026,0.0446,-0.0516,-0.0337,0.0753,0.0117,-0.0563,0.0044,-0.0451,0.0191,0.0533,-0.0636,-0.0029,-0.019,0.0367,-0.0033,-0.026,-0.037,-0.0287,0.0102,-0.0727,0.0027,-0.0502,-0.0093,0.006,-0.0264,0.0089,-0.0512,-0.002,-0.0322,0.0089,0.0175,0.014,0.0733,-0.069,-0.0175,-0.0069,-0.0041,0.0692,0.0089,0.0405,0.0339,-0.024,-0.0522,-0.2838,-0.0164,0.0312,0.0003,0.0284,-0.0407,-0.0101,-0.0019,0.0602,0.0941,0.0702,0.0017,-0.0117,-0.0208,0.0022,0.0665,0.0391,0.0452,-0.0384,0.0082,-0.0443,-0.0022,0.0055,-0.0638,0.0325,0.0493,0.1824,0.0432,0.0337,-0.0671,-0.0039,-0.0152,-0.0289,-0.0702,0.0647,0.0517,0.0609,0.0013,-0.0736,-0.0677,-0.0752,0.0159,-0.0051,-0.0449,-0.0288,-0.0034,-0.0009,-0.0078,-0.0671,0.0135,0.0781,-0.0351,0.0837,0.0241,0.0006,-0.0524,-0.0746,0.0124,-0.0257,-0.0065,0.0046,-0.0188,0.0375,-0.077,0.0661,0.033,-0.0248,-0.0334,-0.0074,0.0033,-0.026,0.1342,-0.0017,-0.0065,0.056,-0.0079,0.0171,-0.0438,-0.0317,-0.0091,0.0655,-0.0762,0.07,0.0327,0.0358,0.004,0.055,0.0102,0.0601,-0.0128,0.0073,-0.0352,-0.0388,-0.0286,0.0336,-0.0081,-0.3114,0.06,-0.0194,0.0384,-0.0121,-0.0009,-0.0351,0.0619,-0.029,-0.0175,-0.0209,0.0284,0.0628,-0.0156,-0.0134,0.0691,0.066,-0.0573,-0.0023,-0.0297,0.0509,0.0686,0.2046,0.0235,0.0411,0.0115,-0.0147,-0.0412,0.0371,-0.0036,0.0227,-0.001,0.0911,-0.0346,0.0233,0.0582,-0.0276,0.0894,0.0116,-0.0174,0.0073,0.0097,-0.0336,-0.061,0.0956,-0.0303,-0.0051,-0.0537,0.0125,0.1113,-0.0204,-0.03,-0.0079,0.0353,0.0022,0.0418,-0.0191,-0.0293,-0.0347,-0.0581,0.012,-0.0579,-0.0114,-0.0198,-0.0478]}
{"key":"[Self-Supervised Representation Learning from Flow Equivariance] Self-supervised representation learning is able to learn semantically meaningful features; however, much of its recent success relies on multiple crops of an image with very few objects. Instead of learning view-invariant representation from simple images, humans learn representations in a complex world with changing scenes by observing object movement, deformation, pose variation, and ego motion. Motivated by this ability, we present a new self-supervised learning representation framework that can be directly deployed on a video stream of complex scenes with many moving objects. Our framework features a simple flow equivariance objective that encourages the network to predict the features of another frame by applying a flow transformation to the features of the current frame. Our representations, learned from high-resolution raw video, can be readily used for downstream tasks on static images. Readout experiments on challenging semantic segmentation, instance segmentation, and object detection benchmarks show that we are able to outperform representations obtained from previous state-of-the-art methods including SimCLR and BYOL.","layer":2,"vector":[-0.0394,-0.0347,0.0021,-0.0435,0.0214,0.0361,0.0666,-0.0033,-0.0091,-0.0009,-0.0013,-0.0728,0.0128,0.0515,0.0129,0.0178,0.0166,0.1011,-0.0671,0.0121,0.0004,-0.0646,0.0011,-0.0101,0.021,-0.0089,-0.0068,-0.0335,-0.0112,-0.2251,0.0167,-0.0498,0.0026,0.034,-0.0029,-0.1004,-0.027,0.0787,-0.0537,0.0476,0.0159,0.0289,0.0004,-0.0317,-0.0382,-0.0261,-0.027,-0.0427,-0.0097,0.0078,0.0273,-0.0543,0.0381,0.025,0.0003,0.0354,0.0496,0.0399,0.0528,0.026,0.0536,0.0165,-0.1761,0.0649,0.0236,0.02,-0.0387,0.0076,0.0166,0.008,-0.0116,0.0656,-0.0053,0.0153,0.0121,-0.0288,0.0369,-0.0013,-0.0088,-0.004,0.0104,0.0117,-0.063,-0.0417,-0.0195,-0.0325,0.0385,-0.0359,0.0426,0.0232,-0.112,-0.0302,-0.0045,0.036,-0.0207,-0.0026,-0.0033,-0.0148,-0.0199,0.2142,-0.0385,0.0571,0.0719,-0.0237,0.0444,-0.0384,-0.0451,-0.0257,-0.0135,0.0003,-0.0197,0.0003,0.02,-0.0366,0.0142,-0.0208,0.0513,0.0488,-0.0544,0.0431,-0.0092,0.0114,0.0235,-0.0414,0.0008,-0.0851,0.0738,0.1373,0.0367,0.0107,0.0289,0.0286,-0.062,-0.006,-0.0035,0.0704,0.02,0.0027,-0.0146,-0.0269,-0.0411,0.0047,0.0233,-0.0854,-0.0517,0.1214,-0.0553,0.0466,-0.0195,-0.0235,-0.0341,0.0324,-0.0416,-0.001,-0.0008,0.0195,0.0308,0.0196,-0.0723,0.0129,-0.0277,-0.0444,-0.0164,0.0526,0.0181,-0.0889,-0.0012,0.0089,0.0104,-0.0205,0.0176,0.0084,-0.0281,0.0306,0.1271,0.0291,-0.0931,0.0067,0.006,0.0603,0.0279,-0.0796,-0.0096,0.0421,0.0516,0.0014,0.0042,-0.0381,0.0147,0.0483,-0.0132,0.0207,-0.013,-0.008,-0.0162,0.0359,0.0334,0.0033,-0.015,-0.0028,-0.0193,0.0322,-0.0147,0.0227,-0.0414,0.0057,-0.0524,0.0118,0.0172,0.0302,-0.0257,0.0265,0.0259,-0.0219,0.0004,-0.0369,-0.0137,0.0524,0.0276,0.012,0.0197,-0.0862,-0.0477,-0.2031,0.0008,0.0319,-0.0307,0.0151,-0.0469,0.0012,-0.0137,0.09,0.0833,0.0866,-0.0242,-0.0132,-0.0021,0.0305,0.0487,0.0277,0.0792,-0.0481,0.0153,-0.0389,0.0527,-0.0079,-0.0788,0.0236,-0.0202,0.2469,0.0345,0.033,-0.017,0.0139,0.0055,-0.062,-0.1007,0.0405,0.0132,0.051,0.0065,-0.0372,-0.0425,-0.0442,-0.0163,0.0235,-0.1184,-0.0147,-0.009,-0.0608,0.0629,-0.0545,0.0046,0.0315,-0.0743,0.0435,-0.0301,-0.0094,-0.0087,-0.0478,0.0272,-0.0657,0.0175,-0.0364,-0.0452,0.0507,-0.0963,0.1067,-0.0199,-0.016,-0.0493,0.022,-0.0208,-0.016,0.0639,-0.019,-0.0127,0.0714,0.0107,0.0493,0.0071,-0.0576,-0.0232,0.0596,-0.0325,0.0367,0.0108,0.0435,0.0444,0.0383,-0.0389,0.0123,-0.0208,0.0155,-0.0185,-0.0574,-0.002,0.0418,0.0026,-0.2886,0.0509,0.012,0.0453,-0.0053,0.0336,0.0617,0.0297,-0.0329,-0.0063,-0.026,0.0029,0.0594,0.0286,-0.015,0.0639,0.0669,-0.0438,0.0495,-0.0517,0.0142,0.0439,0.2125,-0.0334,0.0151,-0.0095,-0.0508,-0.0098,0.0483,-0.0665,0.0237,-0.0256,0.1002,-0.0467,0.005,0.0535,-0.0271,-0.0093,0.0215,0.0012,0.005,-0.0103,-0.02,-0.0972,0.0786,0.0205,0.0074,-0.0073,-0.0124,0.0061,-0.0137,0.0222,0.0068,0.0109,0.0381,0.0253,-0.0633,-0.0387,-0.0774,-0.0347,0.047,-0.0977,0.0088,0.0019,-0.0143]}
{"key":"[Domain Generalization via Domain-based Covariance Minimization] Researchers have been facing a difficult problem that data generation mechanisms could be influenced by internal or external factors leading to the training and test data with quite different distributions, consequently traditional classification or regression from the training set is unable to achieve satisfying results on test data. In this paper, we address this nontrivial domain generalization problem by finding a central subspace in which domain-based covariance is minimized while the functional relationship is simultaneously maximally preserved. We propose a novel variance measurement for multiple domains so as to minimize the difference between conditional distributions across domains with solid theoretical demonstration and supports, meanwhile, the algorithm preserves the functional relationship via maximizing the variance of conditional expectations given output. Furthermore, we also provide a fast implementation that requires much less computation and smaller memory for large-scale matrix operations, suitable for not only domain generalization but also other kernel-based eigenvalue decompositions. To show the practicality of the proposed method, we compare our methods against some well-known dimension reduction and domain generalization techniques on both synthetic data and real-world applications. We show that for small-scale datasets, we are able to achieve better quantitative results indicating better generalization performance over unseen test datasets. For large-scale problems, the proposed fast implementation maintains the quantitative performance but at a substantially lower computational cost.","layer":1,"vector":[0.0077,0.0099,0.0539,-0.0423,0.0307,0.0023,0.0291,0.0303,0.044,-0.0276,0.0167,-0.027,0.0327,0.0847,0.0117,0.0248,0.052,0.0737,-0.0579,0.0217,0.004,-0.017,-0.0101,-0.0214,0.0092,0.0046,-0.003,-0.0403,-0.0277,-0.282,0.0432,-0.0402,0.0706,-0.0197,0.0338,-0.0488,-0.0611,0.043,-0.0171,0.0262,-0.0116,0.0091,-0.019,-0.0089,-0.0369,-0.0619,-0.044,0.0012,-0.0603,-0.0119,0.0128,-0.0415,0.0186,0.0341,0.038,0.0537,0.0489,0.016,0.0637,0.0569,0.0115,0.0332,-0.1566,0.051,0.0443,0.0305,-0.0134,-0.0095,-0.0225,0.0713,-0.0066,0.0525,0.0296,0.0518,0.009,0.0086,-0.011,0.0126,-0.014,0.0189,0.0299,0.0013,-0.0279,-0.0022,-0.027,-0.0527,-0.0039,-0.0323,0.03,0.0057,-0.0585,-0.0572,-0.0055,-0.0032,-0.0873,-0.0194,0.0319,0.0302,-0.0317,0.2033,-0.0353,0.0156,0.0125,-0.0324,0.0171,-0.0308,-0.0487,-0.0026,0.0153,-0.0098,0.014,-0.0063,-0.0263,-0.0581,-0.003,-0.0561,0.0748,0.0244,-0.0018,-0.0107,-0.0052,-0.0144,0.0317,-0.0247,0.0471,-0.0571,0.0417,0.1451,0.058,0.0363,0.0241,-0.0149,-0.0796,-0.0362,0.029,0.035,0.0141,0.0288,0.023,0.0446,-0.0306,-0.0189,0.0329,-0.0592,-0.0533,0.1514,-0.0479,0.0175,-0.0124,0.0007,0.0121,0.0788,-0.0448,-0.0315,-0.0003,0.0453,0.0063,-0.0076,-0.0112,0.0104,-0.041,0.0029,-0.0079,0.1134,-0.012,-0.0983,-0.0203,0.0058,0.0514,-0.0338,0.0431,0.0229,-0.0503,0.0256,0.0758,0.0045,-0.0441,0.0388,-0.0038,0.0304,0.0586,-0.0498,-0.0165,0.0769,0.045,-0.0293,0.0178,-0.0298,-0.0059,-0.029,-0.0472,-0.0329,-0.0753,-0.0084,-0.0333,-0.0113,-0.0173,0.0026,0.0254,0.0093,0.0544,0.016,-0.0316,0.0362,-0.047,0.0121,0.0116,-0.0331,0.0284,0.0349,-0.0211,0.0001,0.0615,-0.0509,-0.0225,0.009,0.0385,0.0694,-0.0159,0.0649,0.054,-0.0243,-0.0925,-0.231,-0.012,-0.0364,-0.004,0.0695,-0.0868,0.0661,0.0327,0.0769,0.0757,0.0119,0.0056,-0.0402,0.0286,-0.01,0.0265,0.0273,0.0101,-0.0355,-0.0227,-0.0135,-0.0095,0.0048,-0.0853,0.0907,-0.0435,0.1944,0.016,0.0297,-0.0251,0.014,0.0146,0.0128,-0.0645,0.0325,0.0077,0.0522,0.0029,-0.0384,-0.0295,-0.0071,0.0121,0.0498,-0.1249,-0.0062,-0.0618,-0.0041,0.0314,-0.0757,0.0392,0.061,-0.0424,0.0703,-0.0504,0.0242,-0.032,-0.0895,0.0108,-0.0453,0.0039,0.0418,-0.057,0.0293,-0.0678,0.035,-0.0397,-0.0386,-0.0412,0.0578,-0.0439,-0.0387,0.0635,-0.0106,-0.0175,0.0521,-0.0439,-0.0144,-0.0292,-0.0362,-0.0,0.0666,-0.0204,0.0073,0.0123,0.0679,0.0151,0.0943,-0.0021,0.0424,-0.0197,-0.0164,0.0297,-0.0483,-0.05,0.0269,0.0117,-0.277,0.0068,0.0034,0.008,-0.0306,-0.0164,0.0115,0.014,-0.0693,0.0014,0.0013,0.0347,0.0643,-0.0054,0.0239,0.0513,0.0366,-0.0808,0.0154,-0.0776,0.0013,0.0175,0.2008,-0.0873,-0.0122,-0.0089,-0.0269,-0.0233,0.0135,-0.029,0.0036,0.0063,0.0988,-0.0233,0.0405,0.0961,-0.0606,0.0352,0.0207,-0.0007,0.0146,-0.0232,-0.0187,-0.009,0.1086,0.0039,-0.0031,-0.0542,-0.0014,0.0502,-0.0161,-0.0139,0.0079,0.0004,0.0252,0.0035,-0.0335,-0.0485,-0.0434,-0.0582,-0.0015,-0.039,-0.0752,-0.0346,-0.0208]}
{"key":"[K-12BERT: BERT for K-12 education] Online education platforms are powered by various NLP pipelines, which utilize models like BERT to aid in content curation. Since the inception of the pre-trained language models like BERT, there have also been many efforts toward adapting these pre-trained models to specific domains. However, there has not been a model specifically adapted for the education domain (particularly K-12) across subjects to the best of our knowledge. In this work, we propose to train a language model on a corpus of data curated by us across multiple subjects from various sources for K-12 education. We also evaluate our model, K12-BERT, on downstream tasks like hierarchical taxonomy tagging.","layer":1,"vector":[-0.0364,-0.0275,0.0248,-0.0279,0.0389,-0.0438,-0.0015,0.0134,-0.0023,0.0025,0.0165,-0.0571,0.0413,0.0291,0.0327,-0.0029,0.0347,0.0358,-0.046,-0.0187,0.0646,-0.0041,-0.0157,-0.0421,0.0264,0.0431,-0.0171,-0.0793,-0.0371,-0.1958,0.0101,-0.0679,0.0529,0.0366,-0.0192,0.0129,-0.0406,0.0359,0.0007,-0.0028,-0.0042,-0.0188,-0.0243,-0.0362,-0.0042,-0.0343,-0.0507,-0.0417,-0.0526,-0.0385,0.0014,-0.0433,-0.0106,0.0442,0.0047,0.0717,0.0333,-0.017,0.061,0.0142,0.0067,0.0485,-0.1855,0.1282,0.0032,0.0318,-0.0647,0.0029,0.0079,0.0652,-0.0039,0.0541,0.0331,0.0856,0.0247,0.0258,-0.0206,-0.0284,0.0152,0.0108,-0.0049,0.0005,-0.0521,-0.0306,0.0223,-0.0321,0.0361,-0.0669,0.0388,0.0194,-0.0223,-0.0273,-0.0008,0.0214,-0.0332,-0.0242,0.0165,-0.0118,-0.0857,0.2016,-0.0662,0.0208,0.0624,-0.0697,0.0152,-0.0116,-0.0001,-0.0497,0.0048,-0.016,-0.0128,-0.0137,0.0187,-0.0425,0.0333,-0.0289,0.0769,-0.0291,-0.0245,-0.0205,-0.0114,0.0458,0.0448,-0.0357,0.0468,-0.0628,0.0569,0.1097,0.0753,0.0164,0.0399,-0.0158,-0.078,-0.0115,-0.0034,0.0296,0.0222,-0.0312,0.0229,-0.003,-0.0046,-0.0489,-0.0053,-0.0953,-0.0548,0.1483,-0.0232,0.0086,-0.0805,0.0202,0.0026,0.0285,-0.0255,0.0068,0.031,0.0556,0.0826,0.0629,-0.0139,-0.0192,0.008,-0.0703,-0.0373,0.0605,0.0335,-0.0604,-0.0122,0.028,0.0296,-0.0584,0.0758,0.0529,-0.0312,0.0575,0.0415,0.0052,-0.0391,0.0043,0.0188,-0.0101,0.0397,-0.0491,-0.0316,0.0459,0.0338,-0.0375,-0.0193,-0.0601,0.1022,0.0578,-0.0329,0.0062,0.0049,-0.002,0.0157,-0.0229,-0.0087,-0.0359,0.0249,-0.0384,-0.0108,0.0108,-0.067,0.0205,-0.0124,0.0212,0.032,-0.0258,0.0798,-0.0083,-0.0141,0.0175,0.0128,-0.0658,-0.052,-0.0175,-0.0077,0.0508,0.0392,0.0507,0.0079,-0.0514,-0.0084,-0.2361,-0.0096,0.0108,0.022,0.0393,-0.0772,0.0102,-0.0079,0.0404,0.0994,0.0395,-0.0467,-0.009,0.019,-0.0314,0.029,0.0412,0.0199,0.0351,0.0288,-0.0081,0.0023,-0.0246,-0.0871,0.026,-0.0154,0.2084,0.0962,0.0169,-0.0481,0.074,0.041,-0.0158,-0.1266,0.0545,-0.0186,0.0446,-0.031,-0.0134,-0.076,-0.0334,-0.0105,-0.0007,-0.1188,-0.027,-0.0327,-0.0193,-0.0409,-0.0258,0.0375,0.0392,-0.0048,0.0069,-0.0076,-0.0605,-0.002,-0.0814,0.0315,-0.0308,-0.0,-0.0001,-0.0468,0.0052,-0.069,0.0509,0.0152,-0.0332,-0.0179,-0.0074,-0.0714,-0.0298,0.1024,-0.0138,-0.005,0.0029,0.0241,0.0074,-0.0214,-0.012,-0.033,0.046,-0.0716,0.0333,0.0069,0.0443,0.0223,0.0935,-0.0232,0.0518,0.0069,-0.0325,0.0306,-0.063,-0.0159,0.0169,-0.0336,-0.2779,0.0646,0.0577,0.0452,0.0085,0.0093,0.0625,-0.0168,-0.0111,-0.0044,0.0186,0.0328,0.0169,-0.0203,-0.0321,0.0561,0.0545,-0.0299,0.0191,0.0056,0.0325,0.0467,0.2334,-0.0471,0.0527,-0.0209,-0.0389,0.0382,0.0498,0.0069,-0.0313,0.0015,0.1093,-0.0195,0.0313,0.067,-0.0497,0.0065,0.0234,0.0005,-0.0314,0.0127,-0.0625,-0.0281,0.0435,0.0119,0.0143,-0.0941,0.0078,0.0062,0.0032,-0.0141,-0.0129,0.0479,0.0298,0.0283,-0.0269,-0.03,-0.0417,-0.0735,-0.0053,-0.0594,-0.013,0.0481,0.0218]}
{"key":"[DMCP: Differentiable Markov Channel Pruning for Neural Networks] Recent works imply that the channel pruning can be regarded as searching optimal sub-structure from unpruned networks. However, existing works based on this observation require training and evaluating a large number of structures, which limits their application. In this paper, we propose a novel differentiable method for channel pruning, named Differentiable Markov Channel Pruning (DMCP), to efficiently search the optimal sub-structure. Our method is differentiable and can be directly optimized by gradient descent with respect to standard task loss and budget regularization (e.g. FLOPs constraint). In DMCP, we model the channel pruning as a Markov process, in which each state represents for retaining the corresponding channel during pruning, and transitions between states denote the pruning process. In the end, our method is able to implicitly select the proper number of channels in each layer by the Markov process with optimized transitions. To validate the effectiveness of our method, we perform extensive experiments on Imagenet with ResNet and MobilenetV2. Results show our method can achieve consistent improvement than state-of-the-art pruning methods in various FLOPs settings. The code is available at https://github.com/zx55/dmcp","layer":2,"vector":[-0.0311,0.0011,-0.0076,-0.007,0.0584,0.0171,-0.0013,0.0256,0.0345,-0.0111,0.0301,-0.0669,0.0412,0.0967,0.0479,0.006,-0.005,0.0454,-0.0402,-0.0032,0.0111,-0.0463,0.0082,-0.0364,0.049,-0.0162,-0.0024,-0.0197,-0.0071,-0.2654,0.0379,-0.0363,0.0276,-0.0411,0.0171,-0.0413,-0.0622,0.0458,-0.0636,0.0418,0.0174,0.0049,-0.0267,-0.0858,-0.0005,-0.0346,-0.06,-0.012,0.009,-0.0523,0.063,-0.0433,0.0635,0.0221,0.0303,0.0301,0.0153,0.0304,0.0202,0.0195,-0.0018,0.0462,-0.1691,0.0512,0.0651,0.0194,-0.0317,0.0178,0.021,0.0748,-0.0375,0.0312,0.0121,0.0428,0.0286,-0.0109,0.0036,-0.0314,-0.0145,0.0285,0.0518,-0.0378,-0.0194,-0.0471,0.0171,-0.0025,-0.0372,-0.0242,-0.0321,-0.0214,-0.0332,0.0318,-0.0144,0.0116,-0.0853,0.01,0.0397,0.0217,-0.0291,0.2129,-0.0512,0.0499,0.063,-0.035,0.0517,-0.0271,-0.0134,0.0146,-0.0495,0.0168,0.0171,-0.0427,0.0321,-0.0634,0.0548,-0.009,0.0151,0.0501,-0.0303,-0.0169,-0.0614,-0.033,0.0323,0.0141,0.0315,-0.0645,0.0088,0.1403,0.0043,0.0294,0.0605,-0.0293,-0.0296,-0.0029,0.0501,0.0323,-0.012,-0.0252,0.0162,-0.0493,-0.0328,-0.0372,0.041,-0.1062,-0.0767,0.1016,-0.0618,0.0513,-0.0434,-0.0011,-0.0484,-0.0086,-0.0082,-0.0291,0.0376,0.0103,0.0262,0.0383,-0.044,-0.0281,0.0074,-0.0464,-0.0472,0.1034,0.0313,-0.0763,-0.0129,-0.0358,0.007,-0.0048,0.0244,0.0361,-0.0242,0.0698,0.036,0.0389,-0.0642,0.0061,0.0009,0.0278,0.0054,-0.008,-0.0265,0.0301,0.0198,-0.0457,0.0249,-0.0578,0.0165,0.013,-0.0463,-0.0035,-0.0317,-0.0171,-0.0291,-0.0234,-0.0081,0.0071,0.013,-0.0204,0.0172,-0.0093,-0.0523,-0.0012,-0.0146,0.027,-0.0038,0.0111,0.016,0.0438,-0.0365,-0.0116,0.0718,-0.0337,-0.0088,-0.0269,0.0275,0.0859,-0.0039,0.0329,0.0043,-0.0346,-0.0335,-0.217,0.0265,0.0075,-0.0149,0.0564,-0.0671,-0.0209,-0.0228,0.0711,0.0441,0.0569,-0.0306,-0.0519,0.0126,0.0363,0.0764,0.0223,0.0186,-0.0078,-0.0382,0.0116,0.0052,0.0151,-0.086,0.047,-0.003,0.2214,0.0028,0.038,-0.0315,-0.0004,0.0547,-0.0559,-0.0753,0.0501,0.024,0.0929,0.0321,-0.0505,-0.0469,-0.0319,0.0404,0.0144,-0.1141,-0.0708,-0.0771,-0.0609,0.019,-0.0343,0.0131,0.0057,-0.0269,0.0261,-0.0232,-0.0119,-0.0333,-0.0815,-0.0043,-0.0655,0.0358,0.0262,-0.0455,-0.0172,-0.0711,0.0626,-0.0235,-0.0098,-0.0183,0.0516,0.0152,-0.0198,0.0514,0.0393,0.0105,0.0341,0.0285,0.0332,-0.0186,-0.0359,-0.0464,0.0608,-0.0489,0.032,0.0094,0.0142,-0.009,0.0826,-0.013,0.0388,-0.0008,0.0591,0.0026,-0.0528,0.02,0.0292,0.0077,-0.2751,0.0369,0.0208,0.0149,-0.0443,0.0399,0.0821,0.0616,-0.0358,0.0153,-0.0267,0.0319,0.032,-0.0148,-0.007,0.0755,0.0763,-0.0494,0.06,-0.0649,0.005,-0.0016,0.2192,-0.0543,0.0434,-0.0,-0.0338,0.0317,0.0451,-0.0082,0.0293,0.0312,0.0868,-0.047,0.0874,0.048,-0.0186,0.0339,-0.018,-0.0208,-0.0164,-0.04,-0.0544,0.0056,0.116,0.0223,-0.0166,-0.0287,-0.009,0.0567,-0.053,-0.017,-0.0006,-0.0023,0.0302,0.0252,-0.058,-0.0497,-0.0357,-0.0228,0.0579,-0.1008,-0.0239,-0.0153,-0.0152]}
{"key":"[Disjoint principal component analysis by constrained binary particle swarm optimization] In this paper, we propose an alternative method to the disjoint principal component analysis. The method consists of a principal component analysis with constraints, which allows us to determine disjoint components that are linear combinations of disjoint subsets of the original variables. The proposed method is named constrained binary optimization by particle swarm disjoint principal component analysis, since it is based on the particle swarm optimization. The method uses stochastic optimization to find solutions in cases of high computational complexity. The algorithm associated with the method starts generating randomly a particle population which iteratively evolves until attaining a global optimum which is function of the disjoint components. Numerical results are provided to confirm the quality of the solutions attained by the proposed method. Illustrative examples with real data are conducted to show the potential applications of the method.","layer":10,"vector":[-0.0517,0.0196,0.0512,-0.041,0.0311,0.0391,0.0473,-0.0097,0.0232,0.0058,0.0436,-0.0699,0.0078,0.0097,0.0187,-0.0243,0.0509,0.0404,-0.0016,0.0198,0.0092,-0.0384,-0.0669,-0.0546,0.028,-0.0025,-0.0332,-0.0134,-0.0471,-0.2226,-0.0063,0.0081,0.0489,-0.0503,-0.0069,0.0025,-0.053,0.0825,-0.0348,0.0394,-0.0112,0.0368,0.0205,-0.0668,-0.0091,-0.0229,-0.023,-0.0353,-0.0039,-0.0221,0.0327,-0.029,0.0012,0.0066,0.0235,0.0326,-0.0001,0.0418,0.0436,0.0351,0.0148,0.0352,-0.1864,0.0671,0.0796,0.0439,-0.0095,-0.0214,-0.0213,0.0536,-0.0065,0.0448,0.0182,0.0241,-0.0125,0.0232,-0.0045,-0.0433,-0.0082,0.0366,0.0196,-0.0044,-0.0202,0.0125,-0.0724,-0.0609,0.0154,-0.0299,0.0286,0.0222,-0.0216,-0.0534,0.0022,-0.0229,-0.0667,-0.0106,0.0575,0.0101,-0.0131,0.2096,-0.0801,0.0011,0.0358,-0.0567,0.0217,0.0039,-0.0142,-0.017,-0.0301,0.0242,0.0105,-0.0225,0.0103,-0.0311,0.0071,-0.0135,0.0148,0.0226,-0.0074,-0.0066,0.0114,0.0222,0.062,0.0374,0.0409,-0.061,0.0399,0.1441,-0.0125,0.0747,0.0757,0.0127,-0.0308,-0.0343,0.0183,0.0111,0.017,0.0128,-0.001,0.0282,-0.0593,-0.0619,0.0191,-0.0914,-0.0212,0.1256,-0.0395,0.0347,-0.0281,-0.002,-0.0179,-0.0246,-0.0083,-0.0011,-0.0107,-0.0123,-0.0078,0.0499,-0.0348,-0.0075,-0.0512,-0.0281,0.0157,0.0975,0.0264,-0.1162,-0.0675,-0.0124,0.0232,0.0089,0.0207,0.0718,-0.0672,0.0137,0.1127,0.0118,-0.0143,-0.005,-0.0079,0.0465,0.0481,-0.0522,-0.0649,0.0131,0.0554,-0.0526,0.019,-0.0104,-0.046,0.0162,-0.0465,-0.001,0.0038,-0.0207,-0.0123,-0.0138,0.0019,-0.0453,0.0574,-0.0441,0.0555,0.0266,-0.0695,0.0065,-0.0117,0.0264,-0.0012,-0.0415,0.0334,0.0701,-0.0069,-0.0084,0.0718,-0.0055,-0.0143,-0.0005,0.0358,0.0561,0.0542,0.0504,0.0474,-0.0174,-0.0478,-0.2468,0.0095,-0.0377,0.0131,0.0406,-0.0445,0.0205,-0.0055,0.0746,0.0521,0.1126,0.0057,-0.0685,0.0209,-0.006,0.0501,0.0357,-0.0201,-0.0246,-0.0122,0.0161,-0.0036,0.0065,-0.0214,0.0614,0.0398,0.1613,0.0117,0.0203,0.0108,0.0137,0.0136,-0.0498,-0.0563,0.0827,0.0115,-0.0066,-0.0298,-0.0427,-0.0385,-0.0095,0.0397,-0.016,-0.0474,-0.0527,-0.069,-0.0159,0.029,-0.0719,0.0044,0.0292,-0.0347,0.0372,-0.0398,0.0284,-0.0287,-0.0827,0.0352,-0.0452,0.0094,-0.0305,-0.0648,-0.005,-0.094,0.0268,-0.0099,-0.0254,-0.0213,0.0291,-0.0075,-0.034,0.1455,0.0002,-0.001,0.093,-0.0067,0.0312,-0.0395,-0.0241,-0.0074,0.0674,-0.0319,0.0259,0.0235,-0.0199,0.0189,0.0828,-0.008,-0.0106,-0.0204,-0.0269,0.0127,-0.0602,0.0322,0.003,0.0155,-0.2994,0.0257,-0.0008,-0.0091,0.0111,-0.0141,0.0367,0.0037,-0.0319,-0.0277,-0.0216,-0.0146,0.0023,-0.0372,-0.0116,0.026,0.0144,-0.0085,0.0815,-0.0771,0.0132,0.0348,0.2744,-0.0407,0.0402,-0.0125,-0.0137,0.004,0.0168,-0.0354,0.008,-0.0203,0.0983,-0.1021,0.0262,0.0737,-0.0636,0.072,-0.0156,-0.0083,-0.0124,0.009,-0.0267,-0.0321,0.1162,-0.0252,-0.0367,-0.0285,0.0297,0.0211,-0.0595,0.0355,-0.0273,0.0014,0.0054,0.0382,-0.0417,-0.0227,-0.0197,-0.0365,0.0551,-0.0445,-0.0228,0.0058,0.0029]}
{"key":"[A Structured Span Selector] Many natural language processing tasks, e.g., coreference resolution and semantic role labeling, require selecting text spans and making decisions about them. A typical approach to such tasks is to score all possible spans and greedily select spans for task-specific downstream processing. This approach, however, does not incorporate any inductive bias about what sort of spans ought to be selected, e.g., that selected spans tend to be syntactic constituents. In this paper, we propose a novel grammar-based structured span selection model which learns to make use of the partial span-level annotation provided for such problems. Compared to previous approaches, our approach gets rid of the heuristic greedy span selection scheme, allowing us to model the downstream task on an optimal set of spans. We evaluate our model on two popular span prediction tasks: coreference resolution and semantic role labeling; and show improvements on both.","layer":2,"vector":[-0.0375,-0.0313,0.0369,-0.0113,0.0009,-0.0116,0.0512,0.0671,0.0222,-0.0288,-0.0008,-0.0128,0.0116,0.0379,0.0337,0.037,-0.0186,0.0651,-0.0668,0.053,0.0262,-0.0188,-0.0301,-0.0607,0.0082,0.0347,-0.0157,-0.0363,-0.0295,-0.2198,-0.0262,-0.0361,0.0371,-0.0113,-0.022,-0.0116,-0.0016,0.0323,-0.0024,0.0026,0.0024,-0.0104,-0.0003,-0.0378,0.0055,-0.0136,-0.0223,-0.0312,-0.0731,0.0061,-0.0167,-0.0203,0.0245,0.0362,0.0228,0.0819,0.0523,0.0188,0.0212,0.0153,-0.004,0.0313,-0.1716,0.0835,0.0162,0.0062,-0.0352,0.0172,0.0045,0.0781,-0.0295,0.0531,0.0231,0.0675,0.0207,0.0105,-0.0053,-0.0035,0.004,-0.0135,-0.0164,-0.0195,-0.0544,-0.0203,-0.0178,-0.0189,-0.0086,-0.0479,0.0525,0.0088,-0.0117,-0.07,-0.0013,0.008,-0.1021,-0.0333,0.0272,0.0217,-0.0345,0.213,-0.065,0.0566,0.0131,-0.0543,0.0104,0.0046,-0.0048,-0.0037,-0.0685,0.0015,-0.0343,-0.0111,0.0094,-0.0295,0.0486,-0.0044,0.0932,0.0214,0.0002,-0.022,0.0012,0.0087,0.0455,0.0069,0.0107,-0.029,0.0489,0.1207,0.0647,-0.0093,0.052,-0.0678,-0.0493,-0.0143,0.0159,0.0117,0.032,-0.0053,0.0151,-0.0368,-0.047,-0.0761,0.007,-0.0622,-0.0728,0.1365,-0.0479,0.0204,-0.0709,-0.042,-0.0445,0.0237,0.0038,-0.0422,0.0292,0.0178,0.0126,-0.011,-0.0426,0.0181,0.0124,-0.0399,-0.0541,0.0706,-0.0039,-0.0687,-0.0501,0.0008,-0.0383,-0.014,0.0641,0.0288,-0.082,0.0274,0.0308,0.0821,-0.0858,0.0176,0.0358,0.0431,0.0747,-0.0539,-0.0439,0.0418,0.0492,-0.0631,0.0158,-0.0438,0.0835,-0.0232,-0.0654,0.0418,0.0267,0.0032,-0.0432,-0.0354,0.0054,0.0039,0.0011,-0.0232,0.0086,0.0632,-0.0824,-0.0085,0.0043,-0.0095,-0.0165,-0.0024,0.0694,0.0023,-0.025,0.0406,0.0649,0.0174,-0.0559,-0.0441,0.0061,0.0055,0.042,0.0784,0.0151,-0.0132,-0.0201,-0.2378,0.0459,0.0471,-0.0085,0.0554,-0.067,0.0611,-0.0321,0.0479,0.0339,-0.0049,-0.0507,-0.0471,-0.0129,-0.0113,0.0413,0.0269,0.0161,-0.0134,0.05,0.0141,0.0083,-0.0503,-0.0634,0.0514,0.0279,0.209,0.074,0.0246,-0.0463,0.0515,0.0058,-0.0052,-0.0643,0.0513,0.0044,0.0258,-0.0354,-0.0533,-0.0194,0.0106,0.0514,-0.014,-0.1146,-0.0508,-0.0646,-0.0367,-0.0421,-0.0332,0.021,0.0601,-0.0191,0.0373,-0.0073,-0.0477,-0.0381,-0.1047,0.0239,-0.016,0.0455,0.0423,-0.0489,0.007,-0.0434,0.0263,0.0386,-0.0203,-0.0369,-0.0108,-0.0144,0.0155,0.0111,-0.0076,-0.0147,-0.0078,0.0537,-0.0259,-0.0305,-0.04,-0.0222,0.0498,-0.0361,0.0599,0.0246,-0.0169,0.0185,0.0841,-0.0176,0.0524,-0.0056,0.0224,0.014,-0.057,-0.0367,0.0644,0.0039,-0.3136,0.0609,0.0285,0.0079,0.0059,0.0273,0.057,0.0112,-0.0204,0.0409,0.0071,0.0214,-0.0089,-0.0181,-0.0413,0.0658,0.0941,-0.0416,0.0184,-0.0879,0.0425,0.0308,0.2178,-0.0023,0.0705,-0.0058,-0.023,-0.0558,0.0416,-0.0038,0.0274,0.0052,0.1201,-0.0472,0.0154,0.0362,-0.0141,-0.007,0.0198,-0.0078,-0.0074,0.0411,-0.0492,-0.0368,0.1054,0.0155,-0.0112,-0.0128,0.0268,0.0258,-0.0114,0.0275,-0.0371,-0.0007,0.0473,0.0399,-0.034,-0.0382,-0.0564,-0.0274,-0.0345,-0.0401,0.0029,0.0494,-0.0105]}
{"key":"[Correlating Twitter Language with Community-Level Health Outcomes] We study how language on social media is linked to diseases such as atherosclerotic heart disease (AHD), diabetes and various types of cancer. Our proposed model leverages state-of-the-art sentence embeddings, followed by a regression model and clustering, without the need of additional labelled data. It allows to predict community-level medical outcomes from language, and thereby potentially translate these to the individual level. The method is applicable to a wide range of target variables and allows us to discover known and potentially novel correlations of medical outcomes with life-style aspects and other socioeconomic risk factors.","layer":3,"vector":[-0.0085,0.0149,0.0112,0.0165,-0.0035,-0.0153,0.0553,0.0535,0.0053,-0.0349,0.0066,-0.0555,0.0341,0.0095,0.0096,0.0202,-0.0045,0.0332,-0.0665,0.0167,0.0096,-0.0316,0.0283,-0.062,0.0544,0.0361,-0.0497,-0.056,-0.0515,-0.1674,-0.0106,-0.0491,0.0574,-0.0074,-0.0045,-0.0307,-0.0328,0.0386,-0.024,0.0509,-0.0061,0.0163,-0.0217,-0.0388,-0.0051,-0.0435,-0.0499,-0.0068,-0.0608,-0.0254,-0.0197,-0.0487,0.0183,0.0426,0.0764,0.0445,0.0573,0.0393,-0.0017,0.0168,0.0573,0.0559,-0.1829,0.1099,0.0188,0.001,-0.0276,0.0371,0.033,0.0383,0.0246,0.0365,0.0414,0.0757,0.0078,0.0335,0.0388,-0.0446,-0.021,0.0131,0.0394,-0.0115,-0.0089,-0.037,-0.0377,-0.0911,0.0125,-0.0518,-0.009,0.023,-0.0824,-0.0619,-0.0059,0.0199,-0.0584,-0.0535,0.0326,-0.0091,-0.0348,0.2145,-0.0535,-0.0146,0.0083,-0.0495,0.0377,-0.0526,-0.0012,-0.0484,0.0101,-0.0241,-0.0194,-0.0046,0.0418,-0.0167,0.0554,0.006,0.0902,0.0228,0.0254,0.0223,-0.0483,0.0151,0.0457,-0.0368,0.032,-0.0049,0.0567,0.1598,0.0876,-0.0097,0.0483,0.0464,-0.0265,0.0097,-0.0299,-0.005,-0.0102,0.0061,0.0223,-0.0249,-0.0088,-0.056,-0.0153,-0.0945,-0.0805,0.1514,-0.0557,0.0063,-0.0444,0.0051,0.0164,0.0245,-0.0259,-0.0161,0.0006,0.045,0.0549,-0.0196,-0.0352,-0.014,0.0183,-0.0738,-0.038,0.0724,0.003,-0.075,-0.0448,-0.0053,0.0408,-0.0382,0.0813,0.0266,0.0046,0.0239,0.0603,0.0396,-0.0382,-0.0163,0.0187,0.0055,0.0505,-0.0094,-0.0022,0.068,-0.004,-0.0403,-0.0276,-0.0413,0.0763,0.014,-0.0074,0.027,-0.0672,-0.0443,-0.0236,-0.0544,-0.0329,-0.0464,0.0178,-0.0044,-0.0107,-0.0092,-0.0369,0.031,0.0217,-0.011,-0.0213,0.0078,0.0374,-0.0024,-0.0349,0.011,0.0866,0.0259,-0.0165,-0.0177,0.0186,0.0316,0.0403,0.0678,0.0248,-0.0384,-0.027,-0.232,-0.0659,0.0436,-0.0592,0.0095,-0.0538,-0.0148,-0.0271,0.0591,0.0992,0.0552,-0.0476,-0.0216,0.0317,-0.0034,0.0426,0.0292,0.0078,-0.0034,0.0004,0.0251,-0.013,-0.0096,-0.0879,0.0766,-0.0236,0.2395,0.0622,-0.0039,-0.0334,0.0329,0.0265,-0.0543,-0.132,0.0802,0.0132,0.0278,0.0009,-0.0919,-0.0044,-0.0491,0.0179,-0.007,-0.0699,-0.0449,-0.0515,-0.03,-0.0049,-0.0792,0.0578,0.0453,-0.027,0.0531,0.0234,0.0104,-0.0354,-0.1189,0.0249,-0.057,0.0134,0.0281,-0.016,0.0199,-0.0478,0.0366,-0.0068,-0.0545,-0.0333,0.0003,-0.0117,-0.053,0.0921,-0.0466,-0.0066,0.0359,0.0143,-0.0048,-0.0549,-0.037,-0.0182,0.1063,-0.0515,0.0815,0.0345,0.0111,-0.0089,0.0715,0.0124,0.0152,-0.0152,0.0245,0.0055,-0.0366,-0.0459,-0.0009,-0.0125,-0.2802,0.0137,0.005,0.0236,-0.0158,-0.0199,0.0589,0.0388,-0.031,0.0116,0.0389,0.0402,0.0867,-0.0617,-0.0301,0.0241,0.0677,-0.0704,0.0536,-0.0239,0.0338,0.0395,0.1744,-0.0411,0.0292,-0.0074,-0.0056,-0.0021,0.0286,0.0259,-0.0234,0.0269,0.0939,0.0024,0.0608,0.0091,-0.0419,-0.0168,0.032,0.0099,0.0039,0.0447,-0.0235,-0.0281,0.0547,-0.0041,-0.0108,-0.058,0.03,0.0264,-0.0242,0.0169,-0.0026,0.0401,0.0134,0.0073,-0.0097,-0.0263,-0.0187,-0.037,-0.0454,-0.0459,-0.046,0.0416,-0.0059]}
{"key":"[Utility Fairness for the Differentially Private Federated Learning] Federated learning (FL) allows predictive model training on the sensed data in a wireless Internet of things (IoT) network evading data collection cost in terms of energy, time, and privacy. In this paper, for a FL setting, we model the learning gain achieved by an IoT device against its participation cost as its utility. The local model quality and the associated cost differs from device to device due to the device-heterogeneity which could be time-varying. We identify that this results in utility unfairness because the same global model is shared among the devices. In the vanilla FL setting, the master is unaware of devices' local model computation and transmission costs, thus it is unable to address the utility unfairness problem. In addition, a device may exploit this lack of knowledge at the master to intentionally reduce its expenditure and thereby boost its utility. We propose to control the quality of the global model shared with the devices, in each round, based on their contribution and expenditure. This is achieved by employing differential privacy to curtail global model divulgence based on the learning contribution. Furthermore, we devise adaptive computation and transmission policies for each device to control its expenditure in order to mitigate utility unfairness. Our results show that the proposed scheme reduces the standard deviation of the energy cost of devices by 99% in comparison to the benchmark scheme, while the standard deviation of the training loss of devices varies around 0.103.","layer":0,"vector":[-0.0159,-0.0468,0.0211,-0.0238,0.0309,0.0054,0.0482,0.0399,0.0628,-0.0276,0.042,-0.0212,0.01,0.0688,0.027,0.0015,-0.0001,-0.0058,-0.0604,0.0115,0.0363,-0.0242,-0.0394,-0.0711,-0.0118,0.0228,-0.0413,0.0041,-0.0707,-0.2246,0.0217,-0.0913,0.0206,0.0112,-0.0078,-0.0368,-0.0347,0.0037,-0.0444,0.0657,0.0141,0.0262,-0.0286,-0.0098,-0.0304,-0.0422,-0.0006,0.0035,-0.0315,-0.0618,0.0614,-0.0208,-0.0287,0.0316,0.0299,0.0447,0.0481,0.0483,0.0697,-0.0039,-0.0029,0.0789,-0.1583,0.0811,0.0675,0.0582,-0.0024,-0.0046,0.0199,-0.0085,0.0232,0.04,0.0316,-0.0038,0.0144,0.0044,0.0069,-0.0009,-0.045,0.0387,-0.0033,0.0065,-0.028,0.0022,-0.0208,-0.0302,0.0204,-0.0442,0.0279,-0.0161,-0.0521,0.0006,-0.0043,0.0342,-0.0304,0.0032,0.0196,0.0113,-0.0751,0.1856,-0.0392,0.0706,0.0242,-0.0451,0.0432,-0.0629,-0.0414,-0.0416,-0.0408,0.006,-0.0234,-0.001,0.0149,-0.0523,0.0024,0.0376,0.0502,0.0554,-0.0186,-0.0126,-0.0097,0.0345,0.0689,-0.0273,0.0092,-0.0481,0.0161,0.171,-0.0093,0.0304,0.0327,-0.0774,-0.032,-0.0102,0.0598,0.0603,0.0234,-0.0416,0.0166,0.0516,-0.0084,-0.067,0.0029,-0.0882,-0.0485,0.123,0.0236,0.0977,-0.0392,-0.0458,-0.02,0.0181,0.0147,-0.0145,0.0657,0.0373,0.0258,0.0689,-0.0643,0.0111,-0.045,-0.0132,-0.0418,0.1222,0.0029,-0.0923,-0.0046,0.0344,0.0229,-0.0443,0.0482,0.0123,-0.023,0.0403,0.0857,0.0184,-0.0643,0.0077,-0.0264,-0.0475,-0.0112,-0.0566,-0.0191,0.0069,0.0163,-0.0472,0.0304,-0.0588,-0.0043,0.0752,-0.0446,0.014,-0.056,0.0151,-0.0081,-0.029,-0.0051,-0.002,0.0193,-0.0342,-0.0186,0.0126,-0.0199,0.0102,-0.0263,0.0081,-0.0093,-0.0154,0.0159,-0.0219,-0.0472,0.0005,0.0443,-0.0339,-0.0394,-0.005,0.0398,0.0343,-0.0175,0.0386,-0.0022,0.0016,-0.0627,-0.163,-0.0203,0.0057,-0.0046,0.0524,-0.0399,0.0472,-0.0304,-0.0042,0.0542,0.0886,-0.0148,-0.0472,0.0469,-0.0392,0.0551,0.0523,0.0702,-0.009,0.0127,-0.0323,0.0141,0.0196,-0.0964,0.0552,0.0511,0.225,-0.0178,0.0286,-0.0714,0.0317,0.0399,-0.0288,-0.1163,0.0032,0.0102,-0.0122,-0.018,-0.0246,-0.0701,-0.0177,0.0146,0.0142,-0.1449,-0.0508,-0.0161,-0.0429,0.0325,-0.0855,0.0168,0.0155,-0.0235,0.0402,0.0006,0.0111,-0.051,-0.0754,0.0727,-0.0523,0.064,0.014,-0.0561,0.0039,-0.1003,0.0752,-0.0241,-0.0273,-0.0495,0.0205,-0.023,0.0294,0.0516,0.0546,-0.0184,0.0066,0.0129,0.0134,-0.0201,-0.0598,-0.0173,0.1237,-0.0091,0.0326,0.0422,-0.0339,0.0175,0.049,0.0266,0.0134,-0.0679,-0.0294,0.0402,-0.0676,-0.0,0.0611,-0.0207,-0.292,0.0321,0.0117,0.0392,-0.0399,0.0132,0.0589,0.0191,-0.0604,0.0261,0.0435,0.0818,0.0104,0.029,0.0719,0.0451,0.0435,-0.0457,0.0249,-0.0494,0.0485,0.0546,0.2072,-0.0278,0.0382,0.0232,-0.027,0.0422,0.054,-0.007,-0.0306,-0.0062,0.0425,-0.0131,0.0581,0.0499,0.0024,0.0037,0.026,-0.0239,-0.0048,-0.0189,-0.0288,-0.0033,0.0834,0.0516,-0.05,-0.0407,-0.0305,0.0268,-0.0066,-0.0105,-0.0391,-0.0323,-0.0195,0.0074,-0.052,0.0113,-0.0589,-0.0618,-0.0103,-0.0503,-0.0463,0.0035,-0.0026]}
{"key":"[Interpreting multi-variate models with setPCA] Principal Component Analysis (PCA) and other multi-variate models are often used in the analysis of \"omics\" data. These models contain much information which is currently neither easily accessible nor interpretable. Here we present an algorithmic method which has been developed to integrate this information with existing databases of background knowledge, stored in the form of known sets (for instance genesets or pathways). To make this accessible we have produced a Graphical User Interface (GUI) in Matlab which allows the overlay of known set information onto the loadings plot and thus improves the interpretability of the multi-variate model. For each known set the optimal convex hull, covering a subset of elements from the known set, is found through a search algorithm and displayed. In this paper we discuss two main topics; the details of the search algorithm for the optimal convex hull for this problem and the GUI interface which is freely available for download for academic use.","layer":1,"vector":[-0.0355,-0.0337,0.0384,0.0116,0.0537,0.0484,0.0484,0.013,0.0292,-0.0066,0.0413,-0.1029,0.0264,0.0513,0.0134,0.0173,0.0133,0.0412,-0.0328,0.0345,0.0253,-0.0473,-0.0388,-0.0512,0.0409,0.0317,-0.0512,-0.0281,-0.0134,-0.2487,-0.0004,-0.0187,0.0833,0.0038,-0.017,-0.0121,-0.0255,0.0229,-0.0242,0.0434,0.0673,-0.0304,-0.0195,-0.0268,0.0027,-0.0471,-0.0006,-0.0069,-0.0006,-0.0716,0.0039,-0.0539,-0.0088,0.0573,0.0387,0.0173,0.0268,0.0085,0.0201,0.0314,0.0239,0.0271,-0.1945,0.0626,0.0865,0.0106,-0.0192,0.0021,0.0482,0.0468,-0.0659,0.0505,0.0112,0.0338,0.0271,-0.0171,-0.0003,-0.0282,-0.0065,0.0399,0.0195,0.0379,0.0014,-0.0124,-0.0265,-0.023,-0.0191,-0.0459,0.0265,0.035,0.0061,-0.0269,0.0156,0.0141,-0.0754,-0.0117,0.0359,0.0138,-0.029,0.2039,-0.052,0.0023,0.0279,-0.0367,-0.0062,-0.0226,-0.017,-0.0266,-0.0038,0.0312,0.0153,-0.0478,-0.012,-0.0507,0.0226,-0.0369,0.0626,0.002,-0.0356,-0.0234,0.0103,-0.0335,0.0449,-0.0032,0.0228,-0.0325,0.0413,0.0974,0.0025,-0.013,0.095,0.0438,-0.0638,-0.0295,0.0304,0.0138,0.0242,0.0385,-0.0242,-0.0284,-0.0446,-0.0646,0.0395,-0.1149,-0.0299,0.1893,-0.0307,-0.0138,-0.062,-0.0121,-0.0332,0.0162,-0.0289,0.005,-0.0004,-0.0041,0.023,0.0265,-0.0422,0.0069,-0.0442,-0.0404,-0.0146,0.0799,0.0116,-0.0786,-0.0382,0.0234,0.0387,-0.015,0.0961,0.0434,-0.0499,0.0218,0.0269,0.0089,-0.0804,-0.0062,-0.0054,0.0212,0.0655,-0.0829,-0.0643,0.0256,-0.0305,-0.0416,0.0149,-0.0028,-0.0075,0.0318,-0.0173,-0.0131,-0.0066,-0.0033,-0.0275,-0.0338,-0.0101,0.0018,0.0472,-0.0657,0.0653,-0.0048,-0.0036,0.0374,-0.0667,0.0566,-0.0552,-0.0183,0.0306,0.0229,-0.0301,0.0133,0.0372,-0.0048,-0.0136,-0.0125,0.038,0.06,0.0421,0.0654,0.0679,-0.0591,-0.0681,-0.2341,-0.0213,-0.0027,0.0112,-0.0173,-0.0398,0.0054,0.0108,0.0078,0.0924,0.0646,0.0284,-0.0295,-0.0046,-0.0521,0.0432,0.0274,0.0347,-0.0261,0.0054,0.0358,-0.0015,-0.013,-0.0157,0.0229,0.0327,0.2057,0.0372,0.0311,-0.0338,0.0386,0.0193,-0.0262,-0.0973,0.0784,0.0364,0.0172,-0.0281,0.0149,-0.0463,-0.0117,-0.0178,-0.0015,-0.0894,-0.0482,-0.0592,0.0547,0.0239,-0.0483,0.0444,0.0425,-0.0163,0.0176,0.0166,0.0075,-0.0431,-0.0828,0.0478,-0.0504,0.0438,-0.0059,-0.0508,0.0214,-0.0599,0.0364,-0.044,-0.0344,-0.0293,0.0039,-0.0445,-0.0244,0.0928,-0.0091,0.0171,0.0196,0.0109,0.0197,-0.0642,-0.1091,-0.0109,0.0537,-0.0501,0.0204,-0.0019,0.0535,0.0331,0.0907,0.0041,0.0209,-0.0246,0.0084,0.0229,-0.0276,-0.0343,0.0193,0.007,-0.3218,0.0417,-0.0128,0.0076,-0.0115,-0.0142,0.0085,-0.0182,-0.0456,-0.0133,0.0357,0.0326,0.0335,-0.0084,-0.0132,0.0391,0.101,-0.0068,0.0452,-0.0363,0.01,0.038,0.2044,-0.0512,0.0129,-0.0101,0.0047,-0.0086,0.0343,-0.0063,0.0331,-0.0065,0.0845,-0.0193,0.0819,0.0883,-0.0478,0.0523,-0.0058,-0.0099,-0.0053,0.0218,-0.0913,-0.0254,0.081,0.0055,-0.0426,-0.0196,0.0346,0.0203,-0.0287,-0.001,-0.0066,0.0036,0.0108,0.0098,-0.0037,-0.0474,-0.02,-0.0628,0.0041,-0.0525,-0.0442,0.0162,-0.0092]}
{"key":"[Unifying Gaussian LWF and AMP Chain Graphs to Model Interference] An intervention may have an effect on units other than those to which it was administered. This phenomenon is called interference and it usually goes unmodeled. In this paper, we propose to combine Lauritzen-Wermuth-Frydenberg and Andersson-Madigan-Perlman chain graphs to create a new class of causal models that can represent both interference and non-interference relationships for Gaussian distributions. Specifically, we define the new class of models, introduce global and local and pairwise Markov properties for them, and prove their equivalence. We also propose an algorithm for maximum likelihood parameter estimation for the new models, and report experimental results. Finally, we show how to compute the effects of interventions in the new models.","layer":3,"vector":[-0.0285,0.0007,0.034,-0.014,0.0492,-0.0276,0.0397,0.0272,0.0434,-0.0174,0.1095,-0.0435,0.0369,0.0877,0.039,0.051,-0.0276,0.0371,-0.0844,0.0032,0.0283,-0.0458,-0.0334,-0.0119,0.0278,0.0898,-0.045,-0.0244,-0.0466,-0.2391,0.0214,-0.0513,0.0266,0.0016,-0.0426,-0.038,-0.0222,0.0234,-0.0154,0.028,0.0405,0.0163,-0.051,-0.0427,-0.0021,-0.0739,-0.0504,-0.0184,-0.0429,-0.0621,0.003,-0.0378,0.0411,0.0233,0.0331,0.0099,0.073,0.0433,0.0293,0.0524,0.0096,0.0664,-0.2062,0.0702,0.0824,0.0624,-0.0413,-0.0179,0.0541,0.0463,-0.0573,0.0211,-0.0318,0.0251,0.0348,-0.0099,-0.0064,-0.0187,-0.0116,-0.0003,0.0056,-0.0513,-0.0135,-0.0309,-0.0066,-0.0772,0.0391,-0.0498,0.0219,0.0185,-0.038,0.0293,-0.0075,0.0184,-0.0675,-0.0183,0.0161,-0.0245,-0.0059,0.1959,-0.0403,-0.007,0.0352,-0.0016,0.0635,-0.0119,-0.0023,-0.0476,0.0019,0.0138,0.0244,-0.0388,0.0164,-0.0749,0.0262,0.0198,0.0653,0.0244,0.0067,-0.0145,-0.0514,-0.0208,0.033,-0.0185,0.0087,-0.074,-0.0114,0.1581,0.0273,-0.0097,0.0589,-0.0647,-0.0365,0.0222,-0.0257,-0.0287,0.0219,0.0064,0.0351,0.023,0.0147,-0.0788,-0.0121,-0.1157,-0.0424,0.1278,-0.0269,0.0237,-0.065,-0.0214,-0.0073,-0.0024,0.0167,-0.0072,0.008,0.0456,0.0317,0.0344,-0.0073,0.0656,-0.0662,-0.0613,-0.0533,0.0776,-0.0191,-0.0536,-0.0145,0.0236,-0.027,-0.0007,0.0541,0.0363,-0.0144,0.0243,0.0596,0.0283,-0.0747,0.0364,0.0028,0.009,-0.0003,-0.0406,-0.0353,0.0483,0.0381,-0.0365,0.0088,-0.0007,0.022,0.0048,-0.029,0.0068,-0.0486,0.0335,-0.0409,-0.0103,-0.0632,-0.0258,0.0038,-0.0065,0.0058,-0.0199,-0.0598,0.0149,-0.0566,0.0585,-0.0201,0.0149,0.0406,-0.0112,-0.0084,0.0138,0.0533,-0.0005,-0.0019,0.0445,0.0116,0.0106,0.0252,0.0517,0.0349,-0.0515,-0.0439,-0.2249,-0.0156,0.0168,-0.0014,0.0376,-0.0558,0.0444,0.0024,0.0607,0.1008,0.0626,-0.0061,-0.0794,0.0167,0.0006,0.0214,-0.0174,0.0415,-0.0164,0.0064,-0.0102,0.0032,-0.0195,-0.0768,0.0238,0.0109,0.1681,0.0186,0.0124,-0.0106,0.0103,0.0393,-0.0435,-0.0575,0.0427,0.0681,0.0105,-0.0067,-0.0035,-0.0543,-0.0816,-0.013,-0.0188,-0.0745,-0.0048,-0.021,-0.0105,-0.0198,-0.0563,-0.0091,-0.0063,-0.0552,0.0823,0.0258,0.0095,-0.0791,-0.1138,0.0304,-0.03,0.0342,0.0076,-0.0324,0.0156,-0.0559,0.0521,-0.0385,-0.0161,-0.0647,0.0019,0.0379,0.0122,0.0904,0.0317,0.0098,0.0785,0.0285,0.024,-0.0573,-0.0785,-0.0443,0.0532,-0.0308,0.008,0.0269,0.0027,-0.0335,0.0766,0.0226,0.0475,-0.0159,0.001,-0.0181,-0.0078,0.0314,0.035,-0.024,-0.2912,0.0292,0.044,0.0324,-0.0489,0.0005,0.0158,0.0106,-0.0755,-0.0291,0.002,0.0747,0.0506,0.0376,-0.0143,0.0389,0.0325,-0.0343,0.0247,-0.0541,0.0161,0.0346,0.217,0.0262,0.0418,0.0449,0.012,0.0471,0.0012,-0.0199,0.0259,0.0243,0.0692,-0.042,0.0671,0.0142,-0.0172,0.0767,0.0228,-0.0299,-0.0224,-0.0027,-0.0333,-0.0182,0.1467,0.0229,-0.0397,-0.0767,0.0261,0.0593,-0.0426,-0.0147,0.0207,-0.0006,-0.0105,-0.0189,-0.0203,-0.0274,-0.0324,-0.0579,0.0087,-0.0271,-0.0204,-0.0075,-0.0082]}
{"key":"[A Deep and Wide Neural Network-based Model for Rajasthan Summer Monsoon Rainfall (RSMR) Prediction] Importance of monsoon rainfall cannot be ignored as it affects round the year activities ranging from agriculture to industrial. Accurate rainfall estimation and prediction is very helpful in decision making in the sectors of water resource management and agriculture. Due to dynamic nature of monsoon rainfall, it's accurate prediction becomes very challenging task. In this paper, we analyze and evaluate various deep learning approaches such as one dimensional Convolutional Neutral Network, Multi-layer Perceptron and Wide Deep Neural Networks for the prediction of summer monsoon rainfall in Indian state of Rajasthan.For our analysis purpose we have used two different types of datasets for our experiments. From IMD grided dataset, rainfall data of 484 coordinates are selected which lies within the geographical boundaries of Rajasthan. We have also collected rainfall data of 158 rain gauge station from water resources department. The comparison of various algorithms on both these data sets is presented in this paper and it is found that Deep Wide Neural Network based model outperforms the other two approaches.","layer":3,"vector":[-0.0429,-0.0337,0.0206,-0.0264,0.0257,0.0432,0.0332,0.001,0.0318,-0.0303,-0.0254,-0.0847,0.0258,0.0421,0.0224,0.0001,0.0023,0.0222,0.0042,0.0279,0.0307,0.0393,-0.0298,-0.1012,0.0263,-0.0088,0.0003,-0.0513,-0.0487,-0.2278,0.004,-0.0465,0.0308,-0.0216,-0.0551,-0.0354,0.0102,0.0538,-0.0112,0.0273,-0.0389,-0.0176,-0.0149,-0.0444,-0.0416,-0.0433,-0.02,-0.0172,0.0019,-0.0255,0.0554,-0.0082,-0.001,0.0154,0.0221,0.0138,0.039,0.0633,0.0375,0.0079,0.0137,0.0524,-0.233,0.0626,0.0272,0.0157,-0.0326,-0.0199,0.0179,0.0365,-0.0065,0.0575,0.0353,0.0026,-0.0108,0.0205,-0.0187,0.0562,-0.0023,0.0061,0.0236,-0.0194,-0.0408,-0.0725,0.0279,-0.03,0.0217,-0.0397,0.0634,-0.0407,-0.0384,-0.0168,-0.0452,0.0314,-0.052,0.0422,0.0263,0.0157,-0.0516,0.217,-0.1014,0.0081,0.0246,-0.0114,0.0209,-0.0133,-0.044,-0.0486,-0.0116,0.0193,-0.0146,-0.0842,0.014,-0.0195,0.0274,-0.0368,0.0584,0.0492,-0.0004,-0.0181,-0.0479,0.0507,0.0561,0.0378,0.0165,-0.0707,0.05,0.1327,-0.002,0.0285,0.0455,-0.0473,-0.0748,-0.0367,0.0397,0.0112,0.0361,0.0164,-0.0044,-0.0479,-0.0608,-0.0422,0.0332,-0.0845,-0.0333,0.0866,-0.033,-0.0005,-0.0549,-0.0411,-0.0435,0.0709,-0.0066,-0.023,0.0555,0.0668,0.0002,0.0363,-0.066,0.0166,-0.0275,-0.0497,-0.0588,0.1008,0.0048,-0.1046,-0.0227,-0.0163,-0.0195,0.0232,0.0109,0.0259,0.0065,0.0234,0.0869,0.0199,-0.0704,0.0224,0.0122,0.0242,0.0221,-0.0533,0.0135,0.0583,0.0482,-0.0451,0.019,-0.0394,-0.0338,0.0098,-0.0415,0.0013,-0.0348,-0.0115,-0.0369,-0.0291,-0.0122,0.0027,0.0417,-0.082,-0.022,0.0084,-0.022,0.001,-0.0037,0.0173,0.0334,0.0261,-0.0005,0.0515,-0.0021,-0.012,0.0675,-0.0596,-0.0461,0.0008,0.0403,0.0601,-0.0041,0.0345,0.0661,-0.089,-0.0364,-0.2013,-0.0009,0.0288,-0.0689,0.0559,-0.0691,0.0528,-0.0402,0.0683,0.0469,0.089,0.0124,0.0127,-0.0007,0.0407,0.0562,0.0352,0.0523,-0.0272,-0.0119,-0.0183,0.0246,0.0224,-0.0974,0.0659,0.0037,0.1733,-0.0298,0.0546,-0.0285,-0.0023,0.0477,-0.0097,-0.1188,0.0293,-0.0112,0.0293,-0.0156,-0.0835,-0.0639,0.0156,0.0201,0.0537,-0.0542,-0.0286,-0.015,-0.0085,0.0283,-0.0406,-0.0142,0.0057,-0.0544,0.0621,0.0336,0.0399,-0.0096,-0.1023,0.0635,-0.044,0.013,0.015,-0.049,-0.0004,-0.0525,0.0092,0.0,-0.0343,-0.036,-0.0067,-0.0148,-0.0222,0.0909,-0.0109,0.0198,0.0432,-0.0546,0.0249,-0.0092,0.0087,-0.0108,0.0843,-0.0221,0.0707,0.0704,0.03,0.0517,0.0652,-0.0237,0.0141,-0.0233,0.0129,0.0012,-0.0314,-0.0208,0.0321,-0.0005,-0.2729,0.0359,-0.0347,0.0239,0.0024,-0.0022,0.0276,0.0326,-0.011,0.0217,0.0177,0.0239,0.0705,-0.0395,0.0417,-0.0017,0.0339,-0.0142,0.0485,-0.0414,-0.0216,0.0313,0.221,-0.0414,0.0119,0.0521,-0.0197,-0.0227,0.0146,-0.0149,0.038,0.0227,0.1067,-0.0662,-0.004,0.0983,-0.0066,0.0671,0.0448,0.0046,-0.0174,0.0543,-0.0322,-0.0112,0.0812,0.022,0.0316,-0.0187,-0.0169,0.0357,-0.0426,0.0008,-0.0088,0.0016,0.039,0.0349,-0.059,-0.0247,-0.0675,-0.0355,0.042,-0.0973,-0.023,-0.0391,-0.0379]}
{"key":"[TOP: Backdoor Detection in Neural Networks via Transferability of Perturbation] Deep neural networks (DNNs) are vulnerable to \"backdoor\" poisoning attacks, in which an adversary implants a secret trigger into an otherwise normally functioning model. Detection of backdoors in trained models without access to the training data or example triggers is an important open problem. In this paper, we identify an interesting property of these models: adversarial perturbations transfer from image to image more readily in poisoned models than in clean models. This holds for a variety of model and trigger types, including triggers that are not linearly separable from clean data. We use this feature to detect poisoned models in the TrojAI benchmark, as well as additional models.","layer":2,"vector":[-0.0229,-0.0303,-0.0047,-0.0105,0.0336,0.0063,0.055,-0.0234,0.0106,-0.0307,0.016,-0.0302,0.0247,0.0614,-0.0042,-0.0243,-0.0223,0.0351,-0.0525,0.0267,0.0103,-0.006,0.0048,-0.0448,0.0413,0.0155,-0.0195,-0.0017,-0.0645,-0.2495,0.0186,-0.0782,0.0123,-0.0197,0.0206,0.0017,-0.0348,0.0279,-0.0312,0.0219,0.0212,0.0413,-0.0193,-0.0724,-0.0319,-0.0511,-0.0366,-0.0018,-0.0142,-0.052,0.0204,-0.0157,0.0385,0.0494,0.0437,-0.0296,0.0762,0.0414,0.037,0.0457,0.0352,0.0776,-0.1487,0.031,0.0793,0.0155,-0.0313,-0.0444,-0.01,0.0317,-0.0211,0.0423,0.0058,0.0037,-0.021,-0.0146,-0.0273,-0.0466,-0.0185,0.0498,0.0276,-0.0283,-0.001,0.0022,-0.0297,-0.0049,0.018,-0.0248,0.0613,0.0098,-0.0133,0.0037,0.0025,0.0217,-0.027,-0.0201,0.0395,0.0293,-0.0605,0.2138,-0.0704,-0.0085,0.0117,-0.0204,0.0651,0.015,-0.0475,-0.0555,-0.0082,0.0075,0.0155,-0.0328,0.0445,-0.0382,0.0367,0.0292,0.0571,0.0106,-0.0367,0.0177,-0.0199,-0.0143,0.066,-0.0315,0.041,-0.0425,0.0145,0.1692,0.0515,0.0607,0.0058,-0.042,-0.0377,0.0239,0.0012,0.0336,0.0337,0.0069,0.0181,-0.0228,-0.0879,-0.0604,0.0557,-0.0786,-0.0272,0.0887,-0.0301,0.0108,-0.0488,-0.0443,-0.0367,0.0557,-0.0331,-0.01,0.0193,0.0294,-0.0156,0.0671,-0.0304,0.0012,0.0059,-0.0562,-0.0644,0.1009,0.0222,-0.0574,-0.016,0.0161,-0.0307,-0.0256,0.0061,0.037,-0.0153,-0.0166,0.0046,0.0161,-0.0932,-0.0176,-0.018,0.025,0.0095,-0.0883,-0.053,0.0593,0.0525,-0.0357,0.0262,-0.0417,0.0364,0.0574,-0.0661,0.0223,-0.0603,-0.0142,-0.0321,-0.057,-0.035,0.0019,-0.025,-0.0128,-0.0082,0.0117,-0.052,0.0205,-0.0091,0.0071,0.0055,-0.0253,0.0152,0.0363,-0.0392,0.0163,0.0325,-0.0466,0.0014,-0.0487,0.0013,0.0502,0.0039,0.0255,0.0493,-0.0336,-0.0375,-0.2539,-0.0147,-0.011,-0.0309,0.0413,-0.0962,0.0281,-0.0109,0.0554,0.0541,0.0517,0.0113,-0.0266,0.0695,0.0286,0.0538,0.0109,0.0416,-0.007,-0.0291,-0.0348,0.009,-0.0213,-0.0717,0.0283,0.0139,0.2424,0.032,0.0566,-0.0402,0.0053,0.0232,-0.0274,-0.0791,0.0567,-0.0047,0.0592,0.0116,-0.0157,-0.0366,-0.0648,0.0092,0.0058,-0.1178,-0.022,-0.0282,-0.0169,-0.0051,-0.0531,0.0494,0.0584,0.0086,0.0562,0.0158,0.0145,-0.0453,-0.1152,0.0516,-0.0083,0.0751,-0.0021,-0.0693,-0.0052,-0.1149,0.0755,0.0041,-0.0413,-0.0741,0.059,0.0077,-0.0063,0.0857,0.0185,0.0241,0.0767,0.017,-0.0182,-0.0395,-0.0551,-0.034,0.0537,0.028,-0.0181,0.0557,0.0383,0.0003,0.0478,-0.0352,0.0529,-0.0131,-0.0084,-0.0052,-0.0972,-0.0126,0.0579,0.0267,-0.2863,0.01,0.0084,0.0848,0.0014,0.0473,0.0612,0.0229,-0.0441,-0.0122,-0.0229,0.0316,0.0289,-0.0041,0.001,0.0107,0.067,-0.0671,0.0476,-0.0393,-0.0203,0.0275,0.2105,-0.0134,-0.0218,0.0075,0.0189,0.0418,0.0562,-0.0054,0.0325,0.0042,0.0585,-0.0173,0.0345,0.0452,-0.0334,0.0413,-0.0006,-0.0226,-0.0387,0.0212,-0.0577,0.0062,0.048,-0.0197,-0.0036,-0.0088,0.0337,0.0586,0.0217,-0.003,-0.0031,-0.0032,0.0635,0.0181,-0.0703,-0.0309,-0.0316,0.0037,0.0206,-0.0508,0.0197,0.0129,-0.031]}
{"key":"[Hierarchical clustering that takes advantage of both density-peak and density-connectivity] This paper focuses on density-based clustering, particularly the Density Peak (DP) algorithm and the one based on density-connectivity DBSCAN; and proposes a new method which takes advantage of the individual strengths of these two methods to yield a density-based hierarchical clustering algorithm. Our investigation begins with formally defining the types of clusters DP and DBSCAN are designed to detect; and then identifies the kinds of distributions that DP and DBSCAN individually fail to detect all clusters in a dataset. These identified weaknesses inspire us to formally define a new kind of clusters and propose a new method called DC-HDP to overcome these weaknesses to identify clusters with arbitrary shapes and varied densities. In addition, the new method produces a richer clustering result in terms of hierarchy or dendrogram for better cluster structures understanding. Our empirical evaluation results show that DC-HDP produces the best clustering results on 14 datasets in comparison with 7 state-of-the-art clustering algorithms.","layer":1,"vector":[-0.0516,-0.0318,0.0541,-0.0356,0.0224,0.0051,0.0391,-0.0194,0.0351,-0.0306,0.0431,-0.0809,0.0315,0.0739,-0.0037,0.0346,0.0171,0.0095,-0.0184,-0.0101,0.0017,-0.0272,-0.0326,-0.0356,0.0138,0.0494,-0.0219,-0.0331,-0.091,-0.252,0.0091,-0.0235,0.0804,0.0042,-0.017,-0.0417,0.0064,0.0688,-0.0213,0.0383,0.024,0.0226,-0.0472,-0.0612,-0.0453,-0.0668,-0.0537,0.0011,-0.0025,-0.029,0.0381,-0.0173,0.033,0.0352,0.0446,0.0531,0.0623,-0.0106,0.0348,-0.0046,0.0543,0.0466,-0.1444,0.0413,0.0946,0.0115,-0.0305,-0.0043,0.0262,0.007,0.0119,0.0624,-0.0216,0.0107,0.0477,-0.0001,-0.0042,-0.0537,-0.0246,-0.0049,-0.0204,-0.007,-0.0157,0.0124,-0.033,-0.0373,0.0133,-0.0785,0.0434,-0.0105,-0.0117,0.0125,-0.0553,0.0117,-0.1062,-0.0488,0.0173,-0.0324,0.0145,0.1891,-0.0735,0.0344,0.0585,-0.0618,-0.0438,-0.0579,0.0222,-0.0193,-0.0161,0.0054,0.019,-0.0047,-0.0138,-0.047,0.0141,-0.0202,0.0789,0.0553,-0.0151,-0.0323,-0.0126,-0.0037,0.0692,-0.0069,0.0878,-0.0754,0.0137,0.0997,0.084,0.0213,0.0394,0.0293,-0.0712,-0.0292,0.0001,0.0503,0.0278,-0.0453,-0.0026,0.0058,-0.0276,-0.0594,0.0262,-0.0798,-0.0057,0.1198,-0.0407,0.0616,-0.0215,0.0036,-0.0072,-0.0108,-0.0429,-0.0381,-0.024,0.014,0.0415,0.0482,-0.0242,-0.0022,-0.0311,-0.0528,-0.0619,0.1252,0.0253,-0.0985,-0.0263,-0.0018,0.0252,-0.0052,0.0275,0.0803,0.0022,0.089,0.117,0.0231,-0.0925,-0.0045,0.009,0.0106,0.0604,-0.0131,-0.0211,0.0221,0.0567,-0.0475,-0.032,-0.036,0.0714,0.0443,-0.0608,-0.0111,0.0315,-0.004,0.0104,-0.0617,0.0043,0.0029,0.0242,-0.033,0.0288,0.0511,-0.0276,0.0428,-0.0222,0.0523,0.0044,-0.0205,0.0414,-0.0094,0.0118,0.0187,0.0565,-0.0301,-0.0353,-0.0383,0.0392,0.0631,-0.0168,0.0568,0.0741,-0.0784,-0.076,-0.2195,-0.0464,0.0345,-0.0093,0.0092,0.007,0.0209,0.0087,0.0335,0.0692,0.0538,0.0206,-0.0265,0.0553,-0.0053,0.0432,0.0139,0.0506,-0.0399,0.0007,0.0208,0.0135,-0.0597,-0.0433,0.0463,-0.0028,0.2009,0.0024,0.0254,0.0188,0.0556,0.0055,-0.0385,-0.0862,0.0368,0.0268,0.0492,-0.0564,-0.0478,-0.0597,-0.0522,0.0117,0.0311,-0.0947,-0.036,-0.0543,-0.0036,0.0189,-0.0168,0.0146,0.0187,-0.0199,0.0346,-0.0149,0.003,-0.0659,-0.0635,0.0013,-0.0442,0.0201,0.0289,-0.029,-0.0043,-0.0872,0.0136,-0.0394,-0.0565,0.0179,-0.0238,-0.0479,-0.0775,0.0867,0.0153,-0.0212,0.0565,0.0262,0.0403,-0.0116,-0.0204,-0.0263,0.0485,-0.0334,0.0294,0.0343,-0.0005,0.0202,0.058,-0.0113,0.0271,-0.0007,0.0297,0.0008,-0.0607,-0.0143,0.0315,0.0308,-0.2773,0.0372,-0.018,-0.0114,-0.0204,-0.0223,0.0297,0.0519,0.0148,-0.0273,0.0055,0.0081,0.04,-0.0462,0.0128,0.0544,0.0008,-0.0467,0.0525,-0.0433,-0.0061,0.0286,0.2003,-0.033,0.0277,0.0268,-0.0455,0.0469,0.0119,-0.0428,-0.0421,-0.0026,0.0888,-0.0574,0.0429,0.0768,0.0023,0.0392,0.0571,-0.0358,0.0188,0.0486,-0.051,-0.0323,0.0921,-0.0001,-0.0537,-0.087,0.0548,0.0427,-0.0802,-0.0236,-0.0063,0.0205,0.0122,0.0388,-0.0292,0.0005,-0.0255,-0.0394,-0.0317,-0.0535,-0.0381,0.0192,0.037]}
{"key":"[Game Redesign in No-regret Game Playing] We study the game redesign problem in which an external designer has the ability to change the payoff function in each round, but incurs a design cost for deviating from the original game. The players apply no-regret learning algorithms to repeatedly play the changed games with limited feedback. The goals of the designer are to (i) incentivize all players to take a specific target action profile frequently; and (ii) incur small cumulative design cost. We present game redesign algorithms with the guarantee that the target action profile is played in T-o(T) rounds while incurring only o(T) cumulative design cost. Game redesign describes both positive and negative applications: a benevolent designer who incentivizes players to take a target action profile with better social welfare compared to the solution of the original game, or a malicious attacker whose target action profile benefits themselves but not the players. Simulations on four classic games confirm the effectiveness of our proposed redesign algorithms.","layer":0,"vector":[-0.0816,-0.0067,0.0376,-0.0247,0.0331,0.0203,0.0371,0.0215,0.0083,0.0265,0.0291,-0.0256,0.0279,0.0313,-0.0062,0.0125,-0.0094,0.0484,-0.0218,0.0175,-0.005,-0.0582,0.0352,-0.0851,0.0021,0.0215,-0.0393,-0.0279,-0.0447,-0.2045,0.0338,-0.0173,-0.0114,-0.0168,-0.0471,-0.0372,-0.0324,0.0602,-0.0691,0.0291,0.0138,0.0766,-0.0068,-0.0645,-0.0441,-0.0671,-0.012,-0.0295,0.0051,-0.0564,0.0009,-0.0431,0.0091,0.0339,0.0783,0.0114,0.0559,0.0764,0.0417,0.0387,0.0531,0.0204,-0.1553,0.0422,0.0398,0.0515,-0.0456,-0.0151,0.0268,0.0936,-0.0319,0.0297,0.0196,0.0278,0.0538,0.018,-0.0341,-0.0355,0.0024,0.0398,-0.0161,-0.0452,-0.0485,0.0236,-0.0232,-0.0924,0.0072,-0.0277,0.0751,0.0324,0.0302,0.0342,0.0134,-0.0067,-0.0729,0.0147,-0.0024,0.02,-0.1005,0.2293,-0.0336,0.0028,-0.0033,-0.0228,0.0741,-0.049,-0.0135,-0.0452,-0.0035,-0.0261,-0.0274,-0.0089,0.0721,-0.0129,-0.0043,0.0543,-0.0041,0.0597,0.0031,-0.0207,-0.0278,0.0316,0.0346,-0.0167,0.0149,-0.0462,0.0085,0.1675,0.0178,0.035,-0.0048,-0.0745,-0.0257,-0.0223,0.0299,0.0305,-0.027,0.0181,0.0258,-0.0035,-0.0451,-0.0258,0.0227,-0.105,-0.0509,0.0945,0.0253,0.0392,-0.011,0.0093,0.0055,-0.005,-0.0516,-0.0117,-0.026,-0.0022,0.0527,0.0516,-0.0612,0.0064,-0.0154,-0.0469,0.0012,0.0931,0.0195,-0.0554,-0.0208,-0.013,0.0082,0.0024,-0.0252,0.0368,-0.0486,0.0056,0.0873,0.0229,-0.0684,-0.0284,0.0302,0.037,0.0515,-0.0285,-0.0409,0.0018,0.0129,-0.0337,0.0071,0.0106,0.0376,0.0725,-0.0251,0.0529,-0.0116,0.0022,-0.0431,-0.0373,-0.0069,-0.0248,0.003,-0.009,-0.0219,0.0069,-0.0551,0.0017,0.0316,0.0217,0.0177,-0.0677,0.0787,0.0153,-0.0394,0.0101,0.0111,-0.0141,-0.0433,0.0011,0.0254,0.0177,0.0096,0.0231,0.0113,0.0177,-0.0082,-0.1966,-0.0195,-0.0432,-0.0293,0.0469,-0.0513,0.0443,-0.0765,0.0242,0.0448,0.0375,-0.0588,-0.0315,0.0474,-0.0134,0.0228,0.0139,-0.0084,-0.0268,-0.0008,-0.0322,0.0022,0.0185,-0.102,0.0054,0.0102,0.2669,0.0688,0.0394,-0.0118,0.0391,-0.0053,0.0143,-0.0895,0.0672,0.056,0.045,0.0007,-0.0594,-0.0361,-0.0176,0.0254,-0.0016,-0.0883,-0.0364,-0.0224,-0.0691,0.0219,-0.0602,0.0222,0.0275,-0.0225,0.0638,0.0009,-0.016,-0.0775,-0.0735,0.0273,-0.0013,0.0482,-0.004,-0.0404,-0.0303,-0.0959,0.0786,-0.005,0.0104,-0.0529,0.082,-0.0313,-0.0064,0.01,0.0215,-0.0437,-0.0066,0.0408,-0.0013,-0.0257,-0.0814,-0.007,0.0496,-0.0282,0.0329,0.0099,0.0168,-0.0259,0.0239,-0.013,0.0495,-0.0331,-0.0094,0.0305,-0.0526,0.0116,0.0283,0.0145,-0.3114,0.0103,0.0059,0.0127,-0.028,0.039,0.0398,0.0228,-0.0352,-0.0045,0.0284,0.0561,-0.0056,0.0439,0.0247,-0.0026,0.0688,-0.0472,0.0827,-0.0764,0.0499,0.0382,0.2367,-0.0592,0.0488,0.0095,-0.0123,0.0339,0.0368,-0.0104,0.0056,-0.0035,0.0612,-0.0326,0.0452,0.0312,-0.0363,0.0109,-0.0379,0.0186,-0.0853,0.0007,-0.0381,0.0132,0.1098,0.0153,-0.0454,-0.0282,-0.0098,0.0068,-0.0436,0.0346,0.0188,-0.0205,0.0365,0.0336,-0.0801,-0.0347,-0.0195,-0.0052,-0.0034,0.0038,0.0253,0.0318,0.0153]}
{"key":"[GSN: A Graph-Structured Network for Multi-Party Dialogues] Existing neural models for dialogue response generation assume that utterances are sequentially organized. However, many real-world dialogues involve multiple interlocutors (i.e., multi-party dialogues), where the assumption does not hold as utterances from different interlocutors can occur \"in parallel.\" This paper generalizes existing sequence-based models to a Graph-Structured neural Network (GSN) for dialogue modeling. The core of GSN is a graph-based encoder that can model the information flow along the graph-structured dialogues (two-party sequential dialogues are a special case). Experimental results show that GSN significantly outperforms existing sequence-based models.","layer":2,"vector":[-0.0458,0.0015,0.0074,-0.0582,0.0027,0.0053,0.0136,0.0115,0.0078,-0.0317,-0.0219,-0.004,0.0701,0.0675,0.0469,-0.0006,0.0041,0.0308,-0.0336,0.0097,0.0294,-0.0638,0.0017,-0.0565,0.0058,0.0136,-0.0496,-0.0401,-0.0608,-0.2149,0.0198,0.0041,0.0809,-0.0102,-0.0622,-0.0224,0.0219,0.0213,-0.0321,0.0745,0.0161,0.0182,0.0017,-0.0805,0.0252,-0.009,-0.0286,0.001,-0.0318,-0.0631,0.0137,-0.0201,-0.012,0.0044,0.0246,0.032,0.0096,0.06,0.0192,0.0564,0.0071,0.0126,-0.1861,0.0903,0.0301,0.051,-0.0725,0.0176,0.0269,0.04,-0.0343,0.0219,0.046,0.0342,-0.0061,-0.0035,0.0345,-0.0141,0.0084,0.0304,-0.0106,-0.0173,-0.0543,-0.0281,0.0277,-0.0472,0.0075,-0.0155,-0.0294,0.0203,-0.0718,-0.0067,0.0146,0.061,-0.0591,-0.0438,0.0455,0.0027,-0.0314,0.1943,-0.0315,0.0267,0.0534,-0.0421,0.0314,-0.0264,-0.013,-0.0396,-0.0405,0.0453,-0.0263,-0.0337,0.0537,-0.0406,0.0123,0.0358,0.0849,0.0151,-0.0349,-0.0353,-0.0024,0.0557,0.0061,-0.0746,0.0521,-0.0886,0.0517,0.0765,0.0402,0.0119,0.0761,-0.0027,-0.0252,-0.0221,0.0076,-0.0493,0.0508,-0.0152,0.0018,-0.0071,0.001,-0.0474,-0.0219,-0.0517,-0.0534,0.1369,-0.0341,-0.03,-0.075,-0.0131,-0.0685,0.0524,0.0405,-0.0552,0.0144,0.0397,0.0201,0.0425,-0.0589,0.0015,0.0114,-0.0513,-0.0569,0.0893,0.0546,-0.1067,-0.036,0.006,0.0233,-0.044,0.0381,0.0601,-0.0422,-0.0217,0.0519,0.058,-0.0656,0.0317,0.0064,0.0124,0.026,-0.0153,-0.0488,0.0178,-0.0067,-0.0613,0.02,-0.026,-0.0144,0.0174,-0.0441,0.0523,0.0024,0.0573,-0.0453,-0.0219,-0.014,-0.0506,-0.0125,-0.0385,0.0135,-0.0344,-0.0936,0.0304,-0.064,0.02,-0.0226,0.0368,0.0494,0.0336,-0.0535,-0.0087,0.0481,-0.0115,-0.0234,0.011,0.0231,0.0021,0.024,0.0538,0.0147,-0.0132,-0.0257,-0.2418,0.0305,0.0173,-0.039,0.0914,-0.0296,0.0431,-0.0215,0.0828,0.0791,0.0774,0.0045,-0.0037,0.0356,-0.0017,0.0672,0.0096,0.0246,0.0252,0.012,0.012,0.0079,-0.0024,-0.1007,0.021,0.0095,0.2258,0.0489,0.0137,-0.0263,0.0293,0.0393,-0.0416,-0.0922,0.0915,0.0202,0.0818,-0.0172,-0.0313,-0.0358,-0.0501,-0.002,-0.0166,-0.0724,-0.0443,-0.0435,-0.028,0.0093,-0.0358,0.0148,0.0346,-0.0294,0.0685,0.0157,-0.02,-0.0403,-0.1134,0.014,-0.0222,0.0317,0.0098,-0.0141,0.0208,-0.0235,0.0419,0.06,-0.0078,-0.0399,-0.0028,-0.0123,-0.0152,0.1026,0.0185,0.0249,0.0469,0.0559,0.0055,-0.0429,-0.0588,-0.0353,0.0607,-0.0503,0.0643,0.0004,0.0382,-0.013,0.0352,0.0269,0.0874,-0.038,0.0299,-0.0073,-0.031,-0.0168,0.0361,-0.0371,-0.2978,0.0418,-0.0033,0.0373,-0.0375,0.0657,0.025,0.0181,-0.0829,0.0004,0.0116,0.0714,0.0058,-0.0196,-0.0196,0.0379,0.0907,-0.0058,0.0252,-0.0418,0.0359,0.0248,0.1791,-0.0327,0.0735,0.0029,-0.0284,-0.0474,0.0304,-0.0313,0.0178,-0.0207,0.0964,-0.0626,0.0191,0.0369,-0.0352,0.0482,0.0305,0.0105,-0.0466,0.0075,-0.0234,-0.0365,0.0676,0.0135,0.0072,-0.0518,-0.0046,0.0351,-0.0016,0.0138,0.0185,0.0088,0.0074,0.0035,-0.0214,-0.0346,-0.0829,-0.0231,-0.0276,-0.0541,-0.0009,-0.0192,-0.0733]}
{"key":"[Random Hyperboxes] This paper proposes a simple yet powerful ensemble classifier, called Random Hyperboxes, constructed from individual hyperbox-based classifiers trained on the random subsets of sample and feature spaces of the training set. We also show a generalization error bound of the proposed classifier based on the strength of the individual hyperbox-based classifiers as well as the correlation among them. The effectiveness of the proposed classifier is analyzed using a carefully selected illustrative example and compared empirically with other popular single and ensemble classifiers via 20 datasets using statistical testing methods. The experimental results confirmed that our proposed method outperformed other fuzzy min-max neural networks, popular learning algorithms, and is competitive with other ensemble methods. Finally, we identify the existing issues related to the generalization error bounds of the real datasets and inform the potential research directions.","layer":0,"vector":[-0.0281,-0.0099,0.0367,0.0035,0.0324,0.0048,0.0568,0.0119,0.0199,-0.0442,0.0178,-0.0346,0.0155,0.0682,-0.0028,0.003,0.0315,0.0379,-0.0741,-0.0008,0.0159,-0.0068,-0.0172,-0.0746,0.0135,0.0007,-0.0353,-0.0419,-0.0609,-0.2541,0.0011,-0.0554,0.0538,-0.0396,0.0119,-0.0102,-0.0433,0.0274,-0.045,0.0642,-0.0017,0.0325,-0.0011,-0.0413,0.0209,-0.0705,-0.0122,-0.0249,-0.0315,-0.006,0.0254,-0.0145,0.0062,0.0286,0.0031,0.0585,0.0081,0.0507,0.0483,0.0521,-0.0081,0.0466,-0.1237,0.0168,0.0685,0.0306,-0.0481,0.0036,0.0506,0.0364,0.0026,0.0557,0.0267,0.0538,0.0107,0.0102,0.0139,-0.0039,-0.0035,0.0381,-0.0253,-0.0535,-0.0463,-0.0248,0.0139,-0.0454,0.0368,-0.0303,0.017,0.0215,-0.0596,0.0107,0.0015,0.0126,-0.0612,-0.0171,0.069,0.0131,-0.0513,0.1884,-0.0865,0.0078,0.0347,-0.0335,0.0389,-0.0163,-0.0545,-0.0304,-0.0554,-0.0272,0.0023,-0.0446,-0.0189,-0.0307,-0.0072,0.0325,0.0564,0.0545,-0.0354,-0.0297,-0.0234,-0.0089,0.0725,-0.0214,0.0051,-0.0362,0.0253,0.132,0.0138,0.0191,0.0545,0.0098,-0.0752,-0.0355,0.0409,0.0522,0.0496,0.0295,0.0351,-0.0186,-0.0325,-0.0457,0.0336,-0.092,-0.0704,0.1037,-0.083,0.0336,-0.033,-0.0397,-0.0058,-0.0025,-0.0371,-0.0474,0.0225,0.0225,0.0656,0.0543,-0.0119,-0.0082,-0.0293,-0.0539,0.0109,0.0801,0.0233,-0.0892,-0.0268,-0.0049,0.0169,-0.0232,0.0135,0.032,-0.0503,0.056,0.0697,0.0059,-0.0589,-0.0251,0.0324,0.0144,0.0134,-0.0143,-0.0513,0.0199,0.0454,-0.0475,0.032,-0.0539,0.0263,0.0295,-0.0621,-0.0011,-0.0293,0.0076,0.0082,-0.0339,0.0138,0.0147,0.0507,-0.0736,-0.0093,0.0108,-0.0235,0.0376,-0.0043,0.0046,-0.0004,-0.0022,0.0615,0.0191,-0.0228,-0.0037,0.0609,-0.0161,-0.0594,0.0158,0.0151,0.0709,-0.0085,0.0168,0.0654,-0.0417,-0.093,-0.2327,0.0032,0.0417,-0.016,0.0461,-0.0379,0.0643,0.0508,0.0791,0.0762,0.0553,-0.0022,-0.0249,0.0362,-0.0135,0.055,-0.0147,0.0468,-0.0542,0.0108,-0.006,0.0191,-0.0015,-0.0741,0.054,0.0035,0.1507,-0.0049,0.0278,-0.057,0.038,0.025,-0.0075,-0.0969,0.0888,0.0711,0.0256,-0.0086,-0.0613,-0.0541,-0.0146,0.045,0.0235,-0.1366,-0.0598,-0.0446,-0.054,0.0014,-0.065,0.0224,0.0382,-0.0282,0.0305,-0.0069,0.0251,0.029,-0.0744,0.0126,-0.0294,0.0386,0.0198,-0.0803,0.0428,-0.035,0.0174,-0.0038,-0.0158,-0.0124,0.0575,-0.0208,-0.0515,0.135,0.0,0.0166,0.0894,-0.0074,0.0595,-0.0288,-0.0026,-0.0116,0.0477,0.0219,0.0251,0.0108,0.0154,0.0126,0.0892,-0.0005,0.0234,0.0128,0.0443,0.0519,-0.0413,0.0129,0.0117,-0.0281,-0.2973,0.0086,0.0015,0.0738,-0.0254,0.012,-0.0069,0.016,-0.035,-0.0475,-0.0003,0.036,0.0726,-0.0522,-0.0027,0.0053,0.0656,-0.0411,0.0643,-0.057,0.0347,0.0813,0.2215,-0.0177,0.0094,0.0249,-0.0712,-0.0589,-0.0035,-0.0605,0.0372,0.0177,0.1078,-0.0972,0.0159,0.0306,-0.038,-0.0166,0.0122,-0.0147,-0.0131,-0.0258,-0.0793,-0.0297,0.1139,-0.018,-0.0202,-0.0638,0.0009,0.012,-0.021,0.003,-0.0408,-0.0228,0.0067,0.0303,-0.0007,-0.0457,-0.0484,-0.0366,0.0187,-0.037,-0.0088,-0.0198,0.0091]}
{"key":"[Generating Black-Box Adversarial Examples for Text Classifiers Using a Deep Reinforced Model] Recently, generating adversarial examples has become an important means of measuring robustness of a deep learning model. Adversarial examples help us identify the susceptibilities of the model and further counter those vulnerabilities by applying adversarial training techniques. In natural language domain, small perturbations in the form of misspellings or paraphrases can drastically change the semantics of the text. We propose a reinforcement learning based approach towards generating adversarial examples in black-box settings. We demonstrate that our method is able to fool well-trained models for (a) IMDB sentiment classification task and (b) AG's news corpus news categorization task with significantly high success rates. We find that the adversarial examples generated are semantics-preserving perturbations to the original text.","layer":5,"vector":[-0.0289,0.0064,-0.0267,0.0012,0.0163,-0.011,0.0212,0.0192,-0.027,0.0032,-0.0099,0.0007,0.0109,0.0746,0.0184,0.0212,0.0217,0.0072,-0.0802,-0.0006,0.064,-0.0304,0.0102,-0.0416,0.0281,-0.0059,-0.0104,-0.0261,-0.038,-0.2267,-0.0114,-0.0463,0.0308,0.0056,0.0143,-0.0163,-0.0739,0.0396,-0.0283,0.0391,0.0204,-0.007,-0.0109,-0.0454,0.0044,-0.0313,-0.029,-0.0185,-0.0082,-0.0359,0.0279,-0.0671,0.0324,0.0452,0.037,-0.0049,0.0732,0.0452,0.016,0.0404,-0.0061,0.0787,-0.1796,0.0832,0.0152,0.058,-0.0629,-0.042,0.0023,0.0693,-0.021,0.0362,0.0455,0.065,-0.0235,0.0399,0.0087,-0.0361,0.0194,0.0001,0.0635,-0.0349,-0.031,0.0126,0.0046,-0.0804,0.017,-0.0437,0.0656,0.0086,-0.0095,-0.0285,0.005,0.068,-0.0539,-0.0461,0.0162,-0.0127,-0.084,0.1849,-0.0308,0.0275,-0.011,-0.0493,0.0282,-0.0317,-0.0258,-0.0302,-0.0521,0.0053,-0.0415,-0.0237,0.0331,-0.0205,0.0463,-0.0206,0.0612,0.008,-0.0248,-0.0224,-0.0293,-0.0251,0.0512,-0.0019,0.0563,-0.0742,0.0413,0.1829,0.0286,0.0082,0.0173,-0.0179,-0.0574,-0.0103,-0.0048,0.0307,-0.0259,0.0188,0.0344,-0.0067,-0.0249,-0.0393,-0.027,-0.055,-0.0655,0.0966,-0.0324,0.0378,-0.0294,-0.0149,-0.0116,0.0071,-0.0062,-0.0093,0.0437,0.0373,0.0347,0.0571,-0.0213,-0.0101,0.0247,-0.0642,-0.0555,0.0963,0.0225,-0.0809,-0.0453,-0.0154,0.0013,-0.0159,0.0427,0.0286,-0.0505,0.0419,0.0448,0.0417,-0.0963,-0.0403,0.0113,0.0149,0.0267,-0.0685,-0.0496,0.0414,0.0191,-0.0623,0.0167,-0.0732,0.0584,0.0184,-0.0371,0.0299,-0.03,-0.0232,-0.0584,-0.0334,0.005,-0.0364,-0.0157,-0.089,-0.0159,-0.0162,-0.0271,-0.0374,-0.0071,0.0004,0.0013,-0.0384,0.0396,-0.0262,-0.0078,-0.01,0.0437,-0.0195,-0.0212,-0.029,-0.0247,0.0572,-0.0021,0.0521,0.0735,-0.0058,0.0043,-0.2591,-0.034,-0.0168,-0.0472,0.0562,-0.0825,0.0045,-0.0124,0.0217,0.0713,0.0279,-0.0692,0.0122,0.0009,0.0144,0.0524,0.0055,0.0149,0.0274,0.039,-0.023,0.0281,-0.0293,-0.12,0.0279,0.0043,0.1909,0.0794,0.0378,-0.0181,0.0402,0.0494,0.0025,-0.1276,0.0605,-0.0059,0.0632,0.0176,-0.0497,0.0076,0.0089,0.076,-0.0072,-0.1097,-0.0269,-0.0201,-0.0223,0.0114,-0.0898,0.0759,0.0324,0.0086,0.0554,0.0312,-0.0105,-0.0091,-0.1142,0.0316,-0.0117,0.0029,-0.001,-0.0451,0.0617,-0.0663,0.0295,0.0253,-0.0175,-0.0404,0.0194,0.017,0.0064,0.0545,0.013,0.0118,0.0112,-0.0281,0.0021,-0.0107,-0.0506,-0.0327,0.0522,0.0054,0.0321,0.0257,0.0331,0.0116,0.104,-0.0242,0.0628,0.0472,0.0217,0.0301,-0.0506,-0.0125,0.0448,0.0194,-0.2989,0.0361,0.022,0.0592,-0.0232,0.0109,0.0348,0.0245,-0.0499,-0.0044,-0.026,0.053,0.0438,-0.0314,-0.0199,0.035,0.0744,-0.0511,0.0455,-0.0333,0.0199,0.0305,0.2226,-0.0396,0.0186,-0.0068,-0.0141,0.0323,-0.016,-0.038,0.019,0.0089,0.0928,0.01,0.0211,0.0668,-0.0324,0.014,0.0424,0.0288,-0.0413,0.0271,-0.0498,-0.0038,0.0599,0.0155,0.0336,-0.0334,-0.0193,0.0123,-0.0378,0.0473,0.0198,0.0184,0.0425,0.0437,-0.0478,-0.06,-0.0311,-0.0099,-0.0109,-0.055,0.0105,0.0013,-0.028]}
{"key":"[Position-Aware Convolutional Networks for Traffic Prediction] Forecasting the future traffic flow distribution in an area is an important issue for traffic management in an intelligent transportation system. The key challenge of traffic prediction is to capture spatial and temporal relations between future traffic flows and historical traffic due to highly dynamical patterns of human activities. Most existing methods explore such relations by fusing spatial and temporal features extracted from multi-source data. However, they neglect position information which helps distinguish patterns on different positions. In this paper, we propose a position-aware neural network that integrates data features and position information. Our approach employs the inception backbone network to capture rich features of traffic distribution on the whole area. The novelty lies in that under the backbone network, we apply position embedding technique used in neural language processing to represent position information as embedding vectors which are learned during the training. With these embedding vectors, we design position-aware convolution which allows different kernels to process features of different positions. Extensive experiments on two real-world datasets show that our approach outperforms previous methods even with fewer data sources.","layer":6,"vector":[-0.0262,-0.031,0.0388,-0.0063,0.004,0.0894,0.035,0.0058,0.031,-0.0113,-0.0295,-0.0408,0.012,0.0723,0.0286,0.0001,-0.0001,0.0449,-0.0078,-0.0265,0.056,-0.0464,-0.001,-0.0795,-0.0209,0.061,-0.0052,-0.0218,-0.0641,-0.1959,0.0052,-0.0623,0.0488,-0.0301,-0.0691,-0.0589,-0.0341,0.0865,-0.0135,0.0345,0.0339,0.0051,-0.0542,-0.0672,-0.0156,-0.0453,0.0042,-0.021,-0.0245,-0.0579,0.0485,-0.0414,0.0031,0.0312,0.0396,0.0262,0.0249,0.0438,0.0543,0.0493,0.0342,0.0084,-0.1928,0.0465,-0.003,0.0119,-0.0339,0.0203,-0.0159,0.0254,-0.0017,0.0227,-0.0198,0.0481,0.036,0.005,0.0433,0.0127,-0.0381,-0.0228,0.0306,-0.0314,-0.0231,-0.0254,0.0224,-0.0422,-0.0024,-0.0333,0.0218,0.006,-0.0545,-0.0444,-0.0693,0.0307,-0.0568,0.007,0.0382,0.0158,-0.0165,0.1972,-0.0678,0.0721,0.0366,-0.0294,0.0063,-0.0196,-0.0194,-0.0074,-0.0297,0.0036,-0.0083,-0.0091,0.034,-0.0275,0.0609,-0.005,0.0763,0.0546,-0.0451,0.0174,-0.0304,0.0087,0.0314,-0.0312,0.0358,-0.064,0.0633,0.1273,0.0543,0.0246,0.0591,0.0418,-0.0698,0.0014,-0.0178,0.0213,0.0023,-0.0346,0.013,-0.0396,-0.0403,-0.0578,-0.0032,-0.0664,-0.0445,0.1171,-0.0289,0.0036,0.0162,-0.0237,-0.0223,0.0072,-0.0297,-0.0409,0.0116,0.0151,0.01,0.024,-0.0557,0.0428,-0.0464,-0.0419,-0.0399,0.0988,-0.0037,-0.1089,-0.0338,-0.0285,0.0318,-0.0023,0.0357,0.022,-0.0277,0.0295,0.1073,0.0828,-0.0708,0.0328,-0.0254,0.021,0.0103,-0.0841,-0.0277,0.0339,0.038,-0.0425,0.0076,-0.0118,-0.0091,0.058,-0.0411,0.0403,-0.0105,-0.005,-0.0312,-0.0237,-0.0006,0.0013,0.0152,-0.0367,-0.01,-0.0033,-0.0434,-0.0069,-0.0522,-0.001,-0.0165,0.0296,0.0097,0.022,-0.0388,-0.0152,0.0938,-0.0353,-0.029,-0.0171,-0.0091,0.0069,0.0373,0.0088,-0.0021,-0.0334,-0.0201,-0.2385,-0.0128,0.0156,0.0114,0.0119,-0.0311,0.0103,0.036,0.0645,0.053,0.0748,-0.0381,0.001,0.0184,0.0253,0.0631,0.0236,0.0606,-0.0214,-0.011,0.024,0.0203,-0.0771,-0.0801,0.0853,0.0252,0.2142,0.025,0.0456,-0.0201,0.0466,0.0146,-0.0238,-0.0994,0.0613,-0.0116,0.046,-0.0084,-0.043,-0.0389,-0.0563,0.0145,-0.0026,-0.0529,-0.0448,-0.0296,-0.0291,0.041,-0.0283,-0.0035,0.0476,-0.0787,0.075,-0.0052,0.0258,-0.0073,-0.0491,0.033,-0.0185,0.0004,-0.0038,-0.0391,0.0125,-0.036,0.0496,0.0138,-0.065,-0.0567,-0.0209,0.0423,-0.0566,0.1047,0.0019,0.0069,0.062,-0.0196,0.0274,-0.025,-0.0315,-0.0326,0.0715,-0.0315,0.0863,0.0508,0.0592,0.0221,0.1125,-0.0241,0.0127,-0.0126,-0.0166,0.0215,-0.0285,-0.0448,0.0122,0.0008,-0.2999,0.0553,-0.0013,0.0372,-0.0159,0.0061,0.0342,0.0368,-0.0218,-0.0072,-0.0014,0.0402,0.0668,-0.0498,-0.0072,-0.0106,0.0779,-0.0498,-0.0048,-0.0582,0.0126,0.0789,0.2299,-0.0298,0.0433,0.0178,-0.0461,-0.008,0.0376,-0.0201,0.0218,-0.0059,0.0922,-0.0912,-0.0076,0.0576,-0.0062,0.0284,0.0328,0.0206,-0.002,0.0527,-0.0029,-0.018,0.0691,0.0072,0.0063,-0.021,0.0381,-0.0164,-0.0374,0.0136,-0.0051,0.0192,0.0317,-0.0172,-0.06,-0.0277,-0.0797,-0.0248,0.0373,-0.1153,-0.0085,-0.0111,-0.0292]}
{"key":"[Unsupervised learning of transcriptional regulatory networks via latent tree graphical models] Gene expression is a readily-observed quantification of transcriptional activity and cellular state that enables the recovery of the relationships between regulators and their target genes. Reconstructing transcriptional regulatory networks from gene expression data is a problem that has attracted much attention, but previous work often makes the simplifying (but unrealistic) assumption that regulator activity is represented by mRNA levels. We use a latent tree graphical model to analyze gene expression without relying on transcription factor expression as a proxy for regulator activity. The latent tree model is a type of Markov random field that includes both observed gene variables and latent (hidden) variables, which factorize on a Markov tree. Through efficient unsupervised learning approaches, we determine which groups of genes are co-regulated by hidden regulators and the activity levels of those regulators. Post-processing annotates many of these discovered latent variables as specific transcription factors or groups of transcription factors. Other latent variables do not necessarily represent physical regulators but instead reveal hidden structure in the gene expression such as shared biological function. We apply the latent tree graphical model to a yeast stress response dataset. In addition to novel predictions, such as condition-specific binding of the transcription factor Msn4, our model recovers many known aspects of the yeast regulatory network. These include groups of co-regulated genes, condition-specific regulator activity, and combinatorial regulation among transcription factors. The latent tree graphical model is a general approach for analyzing gene expression data that requires no prior knowledge of which possible regulators exist, regulator activity, or where transcription factors physically bind.","layer":3,"vector":[-0.0152,-0.0027,0.0011,-0.032,0.0548,0.0379,0.0577,0.0244,0.0126,-0.0053,0.0072,-0.0601,0.0542,0.0517,0.0239,0.0177,-0.0024,0.0836,-0.0691,-0.0221,0.065,-0.0504,0.0045,-0.0423,0.0207,0.011,-0.0587,-0.0283,-0.022,-0.2656,-0.007,-0.0452,0.048,-0.0281,0.0246,-0.0261,-0.0633,0.0066,-0.018,0.0375,0.0638,0.0105,-0.0283,-0.0497,-0.0277,-0.0571,0.0252,-0.0153,-0.0028,-0.0392,0.0102,-0.0245,0.0305,0.0176,0.0754,0.0104,0.0822,0.0514,0.0051,0.0372,0.016,0.0229,-0.1489,0.0585,0.0621,0.0272,-0.0684,-0.005,0.0463,0.0654,-0.0527,0.0369,-0.0157,0.0369,0.0326,-0.0087,0.0189,-0.0393,0.0207,0.0022,-0.0002,-0.004,-0.0507,0.0232,0.0051,-0.021,0.0104,-0.0786,0.0472,0.0322,-0.0211,-0.0293,0.005,0.0298,-0.0839,-0.0086,0.0588,0.0049,-0.0728,0.1961,-0.0493,0.0169,0.0228,0.0039,-0.0175,-0.0445,-0.0146,0.0055,-0.0055,0.0225,0.0195,-0.0489,0.0389,-0.0745,0.0611,-0.0003,0.0648,0.0552,-0.0611,0.0197,0.0055,0.0155,0.0071,-0.0171,0.0396,-0.0582,-0.0528,0.1358,0.0597,0.0219,0.0753,-0.0012,-0.0532,0.0137,0.0058,-0.026,0.0231,-0.0171,-0.0243,-0.0109,-0.0272,-0.0507,-0.0053,-0.0659,-0.064,0.1036,-0.0101,0.0113,-0.0491,-0.0165,-0.0727,0.0208,-0.0187,-0.0199,0.0231,0.0264,0.0235,0.0042,-0.0416,0.03,-0.0387,-0.0411,-0.022,0.071,-0.0006,-0.0589,-0.0388,-0.0077,0.0225,-0.0286,0.0735,0.0239,-0.0231,0.0132,0.0013,0.0108,-0.0821,0.0048,0.0041,0.0516,0.0508,-0.0584,-0.0119,0.0424,-0.0201,-0.0291,0.0094,-0.0052,-0.0166,0.0332,-0.0136,0.0391,-0.0332,0.0398,-0.04,-0.053,-0.0091,0.0138,-0.0149,-0.039,-0.0006,0.0188,-0.0165,0.0112,-0.0593,0.0205,-0.0779,0.0047,0.0467,-0.0024,-0.0638,0.0225,0.0452,-0.0173,-0.0119,0.0254,0.0457,0.0424,0.0195,0.0232,0.0567,-0.0574,-0.0743,-0.2459,-0.0125,0.0129,-0.0214,0.0248,-0.0442,0.0042,-0.0197,0.0439,0.0507,0.0364,0.0299,-0.0425,-0.01,-0.0092,0.0515,0.0385,0.0259,-0.0253,0.0084,0.0102,0.0068,0.018,-0.0746,0.0401,0.0137,0.2229,0.0131,0.0391,-0.0036,0.0125,0.0032,-0.0264,-0.0734,0.0671,0.0203,0.0136,-0.0187,-0.0337,0.0039,-0.027,0.0048,-0.0319,-0.0781,-0.0287,-0.0068,-0.0009,0.0536,-0.0651,-0.0375,0.0569,-0.0196,0.0731,-0.0117,0.0125,-0.0555,-0.0662,0.0453,-0.0416,0.0543,0.0167,-0.0538,0.0197,-0.0492,0.0187,-0.0052,-0.0871,-0.0561,0.0424,-0.0425,-0.0427,0.0617,-0.0051,-0.007,0.0602,0.01,0.0234,-0.0679,-0.0755,-0.0323,0.0512,-0.08,0.049,-0.0037,0.0543,-0.0192,0.0958,0.0052,0.0767,-0.0447,-0.0294,-0.0094,0.0194,-0.0069,0.0626,0.0205,-0.2911,0.0525,0.0282,0.0353,-0.0127,0.0013,0.0807,0.0322,-0.0486,-0.0175,0.058,0.0386,0.0683,0.0175,-0.0291,0.0734,0.0719,-0.0356,0.0735,-0.0353,0.0269,0.0624,0.1721,-0.0089,0.1101,0.015,-0.0476,0.0149,0.0417,-0.0159,0.0137,0.0465,0.1191,-0.0299,0.0542,0.0673,-0.0478,0.0284,-0.0182,-0.0203,0.0088,-0.0446,-0.0238,-0.0345,0.045,-0.0407,-0.0121,-0.0395,-0.0348,0.0497,-0.0347,0.0185,-0.0018,0.0227,-0.046,0.0182,-0.0374,-0.0669,-0.0134,-0.0605,-0.0659,-0.0781,-0.0176,0.0203,0.0002]}
{"key":"[Cognitively-Inspired Model for Incremental Learning Using a Few Examples] Incremental learning attempts to develop a classifier which learns continuously from a stream of data segregated into different classes. Deep learning approaches suffer from catastrophic forgetting when learning classes incrementally, while most incremental learning approaches require a large amount of training data per class. We examine the problem of incremental learning using only a few training examples, referred to as Few-Shot Incremental Learning (FSIL). To solve this problem, we propose a novel approach inspired by the concept learning model of the hippocampus and the neocortex that represents each image class as centroids and does not suffer from catastrophic forgetting. We evaluate our approach on three class-incremental learning benchmarks: Caltech-101, CUBS-200-2011 and CIFAR-100 for incremental and few-shot incremental learning and show that our approach achieves state-of-the-art results in terms of classification accuracy over all learned classes.","layer":6,"vector":[-0.0047,-0.0092,0.0358,-0.0242,0.0183,0.0403,0.0103,0.0195,0.0046,-0.0122,0.0578,-0.0369,0.0471,0.0322,0.021,-0.0022,0.0117,0.0415,-0.0249,-0.0033,0.0161,-0.0203,0.0056,-0.0127,-0.0146,0.0049,0.0039,-0.0477,-0.0366,-0.242,0.002,-0.013,0.0412,-0.0649,-0.0038,-0.0383,-0.0253,0.0704,-0.0141,0.0461,0.0125,0.0116,-0.0297,-0.0328,-0.0411,-0.035,-0.0102,-0.0813,0.0208,-0.0334,0.0074,-0.0831,0.0201,0.0349,-0.0068,0.0524,0.0293,0.054,0.0638,0.0246,0.0576,0.0372,-0.1499,0.0421,0.0368,0.0042,-0.0359,-0.0083,0.0197,0.0742,-0.0294,0.0621,0.028,0.0686,0.0209,-0.0115,0.0068,-0.0156,-0.0256,-0.0096,0.0157,0.0002,-0.0413,-0.0167,-0.0194,-0.045,-0.0004,-0.0937,0.0133,0.0185,-0.0418,-0.0111,-0.0544,0.0135,-0.0422,-0.0368,0.0356,0.0294,-0.0835,0.1753,-0.0235,0.0455,0.0473,-0.0478,0.0594,-0.0299,-0.0144,-0.0503,-0.0323,-0.0161,-0.0035,-0.0519,0.0311,-0.0319,0.0447,0.0004,0.074,0.046,-0.0078,0.0384,-0.0186,0.0057,0.0436,-0.054,0.0074,-0.0675,0.031,0.1199,0.0448,0.0057,0.059,-0.0174,-0.009,-0.0403,0.0064,0.071,0.049,-0.0149,-0.0156,-0.0154,-0.0637,-0.0001,0.014,-0.0932,-0.0685,0.1305,-0.0072,0.0267,-0.042,-0.0316,-0.0392,0.0169,-0.041,-0.0483,0.0227,-0.0065,0.0648,0.0466,-0.0287,0.015,-0.0163,-0.0844,-0.0298,0.0972,0.0581,-0.0427,-0.0391,-0.0239,0.0178,-0.0144,0.0571,0.08,-0.0335,0.0136,0.0965,0.0575,-0.0953,-0.0473,0.017,0.0299,0.0244,-0.0575,-0.0179,0.0428,0.0381,-0.008,0.0233,-0.0386,0.04,0.0527,-0.0019,0.0394,-0.013,-0.0285,-0.0701,-0.032,-0.0127,-0.0429,-0.0093,-0.0856,-0.0099,0.003,-0.0383,0.0548,0.0394,-0.0085,0.0113,0.0213,0.0643,0.0028,-0.0148,-0.0146,0.0526,-0.0285,-0.0127,-0.0035,0.01,-0.0109,-0.0141,0.0521,0.0805,-0.0307,-0.0501,-0.227,0.0043,0.0063,-0.037,0.0562,-0.1063,0.0419,0.01,0.0249,0.0335,0.0529,-0.0315,-0.0159,0.0127,-0.0241,0.0495,0.0556,-0.0093,-0.0329,0.0091,-0.0276,0.0359,0.0018,-0.0925,0.0858,-0.0178,0.2117,0.0531,0.0491,-0.039,0.0008,0.0418,-0.0139,-0.0854,0.0532,-0.0116,0.0495,-0.0025,-0.0288,-0.0023,-0.0141,0.032,0.0133,-0.133,-0.0269,-0.0274,-0.0731,0.0148,-0.0333,0.0117,0.0548,-0.0363,0.0137,-0.0273,-0.0101,-0.0216,-0.1136,0.087,-0.0109,0.034,0.0021,-0.0608,-0.0258,-0.0517,0.0541,-0.0167,-0.03,-0.1102,0.0495,-0.0236,-0.0118,0.0909,0.006,-0.0241,0.0182,-0.0196,0.0656,-0.0394,-0.0338,0.0321,0.0199,-0.0449,-0.0119,0.0193,0.0361,0.065,0.0867,-0.0116,0.0498,-0.0087,0.0416,0.0402,-0.0478,0.0286,0.0238,-0.0613,-0.2839,-0.0062,-0.0203,0.0104,-0.0114,0.0404,0.0219,0.0025,-0.0164,0.0207,-0.0003,0.0266,0.0513,0.0212,-0.0033,0.0316,0.0569,-0.0429,0.0946,-0.0374,0.0183,0.0467,0.214,-0.0654,0.0406,-0.0126,-0.011,0.0057,0.0265,-0.0222,-0.0129,-0.0074,0.0836,-0.0233,0.0103,0.0845,-0.0393,0.0452,-0.0053,-0.0573,0.0048,-0.0033,-0.0794,-0.0319,0.0985,-0.0015,0.0223,-0.0332,-0.046,0.027,-0.023,-0.0001,-0.011,-0.0337,0.0301,0.0123,-0.0382,-0.0413,-0.0537,-0.023,0.0453,-0.0577,0.0208,0.0141,0.0031]}
{"key":"[DeepSampling: Selectivity Estimation with Predicted Error and Response Time] The rapid growth of spatial data urges the research community to find efficient processing techniques for interactive queries on large volumes of data. Approximate Query Processing (AQP) is the most prominent technique that can provide real-time answer for ad-hoc queries based on a random sample. Unfortunately, existing AQP methods provide an answer without providing any accuracy metrics due to the complex relationship between the sample size, the query parameters, the data distribution, and the result accuracy. This paper proposes DeepSampling, a deep-learning-based model that predicts the accuracy of a sample-based AQP algorithm, specially selectivity estimation, given the sample size, the input distribution, and query parameters. The model can also be reversed to measure the sample size that would produce a desired accuracy. DeepSampling is the first system that provides a reliable tool for existing spatial databases to control the accuracy of AQP.","layer":1,"vector":[-0.0519,-0.0049,0.0296,-0.0176,0.0101,-0.0118,0.0083,0.0103,0.0253,0.0186,-0.0238,-0.0145,0.0354,0.0664,0.0032,0.0516,0.0019,0.0075,0.0042,0.022,0.019,-0.0106,-0.0566,-0.0735,0.0272,0.0186,-0.013,-0.0838,-0.0601,-0.2528,0.0085,-0.0131,0.0559,-0.0577,-0.0003,-0.0425,0.0088,0.028,-0.0055,0.0272,0.0389,-0.0012,-0.0439,-0.033,-0.0205,-0.0355,-0.0268,-0.0374,-0.0114,-0.035,-0.0059,-0.0321,0.026,0.0338,0.059,0.0481,0.0557,0.0335,0.0278,0.0096,0.0306,0.0458,-0.1357,0.0572,0.0156,0.0043,-0.0229,-0.0436,-0.0043,0.0015,-0.018,0.0476,-0.0157,0.0595,0.049,-0.0145,-0.0184,-0.0079,-0.0011,-0.0073,-0.0017,-0.021,-0.0312,0.0051,-0.016,-0.0641,0.0097,-0.017,0.0742,-0.0146,-0.0205,-0.0043,-0.0718,0.0203,-0.0592,-0.0255,0.0017,-0.0121,-0.0038,0.2101,-0.0551,0.0625,-0.0055,-0.0214,-0.0051,-0.0499,-0.0405,-0.0496,-0.0514,0.073,0.0138,-0.0574,0.0169,-0.0249,0.0068,0.036,0.0613,0.0414,-0.0076,-0.0155,0.0001,0.0177,0.0457,0.01,0.0575,-0.0778,0.0265,0.1134,0.0107,0.0238,0.0597,-0.019,-0.0885,0.0015,-0.0023,0.0302,0.0495,0.0236,0.0114,0.005,-0.0499,-0.0578,0.0242,-0.0739,-0.0631,0.1519,-0.031,0.0151,-0.0734,-0.073,-0.0215,0.0406,-0.0432,-0.0128,-0.0035,0.0282,0.0318,0.077,-0.0444,0.0013,-0.011,-0.0537,0.0062,0.1128,0.0064,-0.0971,-0.0467,0.0405,0.0357,0.0017,0.0457,0.0262,-0.0696,0.0355,0.054,0.0064,-0.0815,0.0216,0.0518,-0.0029,0.0215,-0.0018,0.0133,0.0162,0.0149,-0.0677,-0.0276,-0.0085,0.0317,0.0238,-0.0192,-0.0177,-0.0379,0.0034,-0.0266,-0.0061,-0.0199,-0.0064,0.0532,-0.0296,0.0402,-0.001,-0.0591,0.0231,0.0287,0.0351,-0.0067,0.018,-0.0012,0.0215,-0.014,-0.0349,0.0194,-0.0087,-0.0089,-0.0326,0.0227,-0.003,-0.0048,0.0775,0.0427,-0.0579,-0.0458,-0.2527,-0.0183,0.0126,0.0096,0.0322,-0.0881,0.0428,0.0006,0.0177,0.0868,0.0388,-0.0095,-0.0051,0.0623,-0.0342,0.0509,0.0287,0.0442,-0.0286,-0.0507,0.0111,0.0161,-0.0691,-0.0662,0.0559,0.0077,0.2018,0.0282,-0.0178,-0.0422,0.0447,-0.0042,0.0118,-0.1212,0.0361,0.036,0.0445,0.0203,-0.0348,-0.0069,-0.0281,0.0628,-0.0078,-0.1071,-0.0349,-0.0123,-0.0345,0.0325,-0.041,0.0067,0.0159,-0.029,0.0498,0.0031,0.0162,-0.0067,-0.0797,0.0407,-0.0241,0.0213,0.0107,-0.0554,-0.0198,-0.0149,0.028,0.0212,0.0238,-0.0268,-0.0047,-0.0188,0.0044,0.0927,-0.0286,0.037,0.0286,0.0103,0.026,-0.0265,0.0372,-0.0356,0.0815,-0.034,0.0654,0.0255,0.0195,0.0578,0.0978,-0.0184,0.0314,-0.0319,0.015,0.0101,-0.0635,-0.0695,0.007,-0.0252,-0.3089,0.034,-0.0072,0.0352,-0.0243,-0.0342,0.034,0.0389,0.0029,0.0098,-0.0066,0.0562,0.0187,-0.0588,-0.0077,0.0144,0.0596,-0.0109,0.0842,-0.0899,0.0486,0.0153,0.21,0.0111,0.0546,0.0503,-0.0322,-0.0485,-0.0089,-0.0421,-0.0356,0.0195,0.1248,-0.0633,0.0349,0.1087,-0.0154,0.0639,0.0068,-0.0339,-0.0262,0.0341,0.0023,0.0026,0.0732,-0.0268,-0.0366,-0.0437,0.0129,0.049,-0.0453,-0.0073,-0.0151,-0.0187,0.0102,0.0164,-0.0506,-0.0303,-0.0421,0.0078,0.0305,-0.0687,0.0092,-0.0592,0.0222]}
{"key":"[Multi-Level Generative Models for Partial Label Learning with Non-random Label Noise] Partial label (PL) learning tackles the problem where each training instance is associated with a set of candidate labels that include both the true label and irrelevant noise labels. In this paper, we propose a novel multi-level generative model for partial label learning (MGPLL), which tackles the problem by learning both a label level adversarial generator and a feature level adversarial generator under a bi-directional mapping framework between the label vectors and the data samples. Specifically, MGPLL uses a conditional noise label generation network to model the non-random noise labels and perform label denoising, and uses a multi-class predictor to map the training instances to the denoised label vectors, while a conditional data feature generator is used to form an inverse mapping from the denoised label vectors to data samples. Both the noise label generator and the data feature generator are learned in an adversarial manner to match the observed candidate labels and data features respectively. Extensive experiments are conducted on synthesized and real-world partial label datasets. The proposed approach demonstrates the state-of-the-art performance for partial label learning.","layer":2,"vector":[-0.0125,-0.0703,0.0206,-0.0269,0.0141,0.0152,0.0248,0.0303,-0.0074,-0.0072,0.0124,-0.0577,0.0569,0.065,0.0346,0.0056,0.0353,0.0453,-0.059,-0.0238,0.0171,-0.0015,0.0265,-0.0111,0.0263,0.0401,0.0209,-0.058,-0.0442,-0.2556,0.0239,-0.0389,0.039,-0.022,-0.0137,-0.0294,-0.0463,0.0731,-0.006,0.0698,0.0301,-0.0151,-0.0151,-0.1154,-0.0415,-0.0149,-0.0263,-0.0553,-0.0132,-0.0453,0.0382,-0.0507,-0.0114,0.0004,0.0103,0.0659,0.0503,0.0099,0.0415,0.09,0.0146,0.0478,-0.1768,0.0444,0.0274,0.0315,-0.0595,-0.0323,0.003,0.0557,-0.0189,0.0252,0.034,0.0579,0.0354,0.0011,-0.0026,-0.049,-0.0234,0.0149,0.0385,0.0165,-0.0322,-0.0286,-0.0362,-0.0434,0.0087,-0.0569,0.0461,0.0257,-0.0131,-0.0172,-0.0178,0.0704,-0.042,-0.0133,0.016,0.0214,-0.0667,0.1897,-0.0719,0.044,0.0365,-0.0402,0.0122,-0.0299,-0.0313,-0.023,-0.031,0.0042,-0.0442,-0.0002,0.0292,-0.0525,0.0596,-0.0454,0.1019,0.0427,-0.0209,-0.032,-0.0061,-0.0341,0.0481,-0.0072,0.0422,-0.0389,0.022,0.1346,0.0605,0.0145,0.0272,-0.0329,-0.028,0.0149,0.0238,-0.0272,0.0095,0.0221,0.0427,-0.003,-0.0225,-0.0107,-0.0103,-0.0696,-0.0623,0.0855,-0.0255,0.0608,-0.0619,-0.0253,-0.0224,0.0363,0.0097,-0.0014,0.0444,0.0459,0.0444,0.0458,-0.0302,0.0331,-0.0235,-0.0463,0.023,0.0968,0.0094,-0.0887,-0.0791,0.0131,-0.0208,0.0096,-0.0013,0.0362,-0.0509,0.027,0.0577,0.0576,-0.0892,0.0193,-0.051,0.0313,-0.0019,-0.079,-0.0378,0.0189,0.0205,-0.0415,0.0091,-0.0649,0.0226,0.0701,-0.0255,0.0179,-0.0059,-0.029,-0.041,-0.0742,-0.0181,0.035,-0.0016,-0.0475,0.0013,-0.0034,-0.0098,0.0179,0.0106,0.0054,-0.0096,-0.0402,0.034,-0.0136,0.02,0.0489,0.0536,0.0041,-0.0313,-0.0284,0.0317,0.0781,-0.0066,0.052,0.0151,-0.0187,-0.0644,-0.2617,0.0439,-0.0136,-0.0142,0.0403,-0.0833,0.0238,-0.0198,0.0722,0.0764,0.0656,0.0155,-0.0236,0.0271,-0.0187,0.0462,0.0264,0.02,0.032,0.002,-0.017,0.0173,0.0095,-0.0804,0.0535,-0.0023,0.2152,0.0388,0.0265,-0.0238,0.0365,0.0273,-0.0506,-0.0711,0.0488,0.0336,0.0479,-0.0256,-0.0447,-0.0132,-0.0031,0.0592,0.0384,-0.1361,-0.0216,-0.0961,-0.0336,0.0126,-0.0875,0.0241,0.0524,-0.0245,0.08,-0.0153,0.0002,-0.0248,-0.1307,-0.0003,-0.0507,0.01,-0.0163,-0.0671,0.0205,-0.0529,-0.0051,0.047,-0.053,-0.0605,0.025,-0.0407,-0.0398,0.0606,0.0223,0.0009,0.054,0.0049,0.0091,-0.0188,-0.0636,-0.0142,0.0447,-0.0023,0.0599,0.0253,0.0343,0.0012,0.0642,0.0049,0.0234,-0.0579,0.0188,0.0291,-0.0366,0.0193,0.0415,-0.0207,-0.2503,-0.0001,-0.0083,0.0833,-0.0228,0.0379,0.0582,0.0701,-0.0578,0.028,-0.0115,0.0038,0.0292,-0.048,-0.0052,0.0491,0.0614,-0.1035,0.0631,-0.0123,0.0181,0.0243,0.1578,-0.008,0.0048,-0.0155,-0.054,0.0209,0.0211,-0.0058,0.0147,0.0265,0.0715,-0.0302,0.0472,0.0748,-0.0418,0.009,0.012,-0.0365,0.0075,-0.0213,-0.0879,-0.0346,0.0721,0.0497,0.0053,-0.0184,-0.0125,0.017,-0.0123,0.0393,0.0021,0.0136,0.0166,0.0297,-0.0108,-0.0977,0.0027,-0.0282,0.0257,-0.0387,-0.0315,-0.011,-0.0475]}
{"key":"[Unsupervised Dictionary Learning for Anomaly Detection] We investigate the possibilities of employing dictionary learning to address the requirements of most anomaly detection applications, such as absence of supervision, online formulations, low false positive rates. We present new results of our recent semi-supervised online algorithm, TODDLeR, on a anti-money laundering application. We also introduce a novel unsupervised method of using the performance of the learning algorithm as indication of the nature of the samples.","layer":1,"vector":[-0.0509,-0.0495,0.0221,-0.0253,0.0924,-0.0155,0.0793,0.0187,-0.0187,-0.0259,0.0441,-0.0368,0.0033,0.0542,0.0093,-0.0014,0.0087,0.0122,-0.043,0.0188,0.0619,-0.0214,-0.0078,-0.0681,0.0391,0.069,-0.0357,-0.0598,-0.0671,-0.1867,0.0456,-0.0475,0.0279,-0.0012,0.0574,-0.0058,-0.0502,0.059,-0.0218,0.0361,-0.0074,0.0251,-0.006,-0.0511,-0.0304,-0.073,-0.003,0.0161,-0.0261,-0.0479,0.0262,-0.0229,0.0246,0.0101,-0.0088,-0.0086,0.0491,0.0133,0.0513,0.0626,0.0132,0.0731,-0.1771,0.0474,0.0624,0.0499,0.0068,0.0024,0.0439,0.0273,0.0295,0.0271,-0.0065,0.0687,-0.0148,-0.0054,-0.0079,-0.0602,-0.0083,0.0419,-0.0314,-0.0262,-0.0236,0.0027,-0.0597,-0.0971,-0.009,-0.0489,0.0753,-0.03,-0.0308,0.0192,0.0191,0.043,-0.0349,-0.0108,0.0222,0.0439,-0.0448,0.2321,-0.049,0.0261,0.0314,-0.0292,0.0302,-0.0375,-0.0391,-0.0308,-0.0234,-0.0293,-0.0119,0.0186,0.0553,-0.0939,0.0124,-0.0016,0.0217,0.0455,-0.0391,0.0324,0.0032,-0.0131,0.0485,-0.0433,0.0093,-0.0479,0.0086,0.1536,0.0194,0.0453,0.0005,-0.0271,-0.0634,-0.0075,0.0064,0.0096,-0.0355,0.0453,0.0356,-0.0087,-0.0614,-0.073,0.0101,-0.0561,-0.0334,0.1025,-0.0433,0.0681,-0.0351,-0.013,-0.0082,0.0131,-0.0415,-0.0575,0.0066,0.0454,0.1062,0.0152,-0.0388,0.0398,-0.0079,-0.0577,-0.0228,0.1096,-0.0244,-0.1106,-0.0146,-0.0309,0.0006,-0.004,0.0323,0.0592,-0.0418,0.009,0.0525,0.0333,-0.0629,-0.0088,-0.0055,-0.0053,0.0007,-0.0618,-0.0756,0.0531,0.0378,-0.0555,0.0267,-0.0309,0.0586,0.0477,-0.0512,0.0129,-0.0733,-0.0458,-0.0059,-0.0212,-0.0065,-0.0263,-0.0041,-0.0374,0.0183,0.0214,-0.0283,0.0347,0.0181,0.0382,-0.015,-0.0292,0.0198,0.0129,0.0062,0.0267,0.0537,-0.0198,-0.0312,-0.0065,0.0245,0.0493,0.009,0.0394,-0.0253,-0.0351,-0.0572,-0.2589,0.0038,0.0482,-0.0072,0.0388,-0.0245,0.0193,-0.0272,0.0238,0.0735,0.0265,-0.0428,-0.0209,0.0112,-0.0078,0.0649,0.0272,0.0287,-0.0313,0.0436,-0.0465,0.0126,-0.0069,-0.0822,0.0033,-0.0222,0.2031,0.0078,0.0296,-0.0836,0.0137,-0.0115,-0.0147,-0.0929,0.0759,0.024,0.0334,-0.021,-0.0408,-0.0188,-0.0196,0.0356,0.0044,-0.0274,-0.0185,-0.0417,-0.0326,-0.0053,-0.0247,0.0521,0.0328,0.0059,0.1012,0.0036,-0.0075,-0.0658,-0.0722,0.0392,-0.0458,0.0099,0.0345,-0.049,0.0343,-0.0896,0.0637,-0.0343,-0.0573,-0.0244,0.0401,0.0057,-0.057,0.1064,0.009,-0.0213,0.0059,0.0112,0.0019,-0.0602,-0.0711,0.0098,0.0811,-0.0085,0.0104,-0.0235,0.0388,0.033,0.0605,0.0372,0.0379,-0.0116,0.0157,0.025,-0.0693,0.0175,0.0128,0.0397,-0.2701,-0.0123,0.0057,0.0385,-0.0052,-0.0092,0.0729,0.0247,-0.0036,-0.0196,-0.018,0.0259,0.0504,-0.0261,-0.0282,0.0449,0.0664,-0.067,0.0192,-0.0194,0.0104,0.0952,0.244,-0.023,0.0289,0.0203,-0.0141,0.0056,0.0559,-0.0227,0.0571,0.0312,0.0563,-0.0542,0.0138,0.0498,-0.0372,0.0005,0.0139,-0.0349,-0.0313,0.0084,-0.0444,-0.0289,0.1114,-0.0457,0.0251,-0.0174,0.0231,0.0169,-0.0289,-0.0194,-0.0201,0.0028,0.0291,0.025,-0.0429,-0.0531,-0.0253,-0.026,0.0257,-0.0418,-0.0147,0.0065,0.0077]}
{"key":"[Integration of Convolutional Neural Networks for Pulmonary Nodule Malignancy Assessment in a Lung Cancer Classification Pipeline] The early identification of malignant pulmonary nodules is critical for better lung cancer prognosis and less invasive chemo or radio therapies. Nodule malignancy assessment done by radiologists is extremely useful for planning a preventive intervention but is, unfortunately, a complex, time-consuming and error-prone task. This explains the lack of large datasets containing radiologists malignancy characterization of nodules. In this article, we propose to assess nodule malignancy through 3D convolutional neural networks and to integrate it in an automated end-to-end existing pipeline of lung cancer detection. For training and testing purposes we used independent subsets of the LIDC dataset. Adding the probabilities of nodules malignity in a baseline lung cancer pipeline improved its F1-weighted score by 14.7%, whereas integrating the malignancy model itself using transfer learning outperformed the baseline prediction by 11.8% of F1-weighted score. Despite the limited size of the lung cancer datasets, integrating predictive models of nodule malignancy improves prediction of lung cancer.","layer":1,"vector":[-0.0086,-0.0351,0.0086,0.002,0.0578,0.0317,0.0651,0.0163,0.0203,0.003,0.0212,-0.0672,0.0445,0.0517,-0.004,0.0193,0.0369,0.0318,-0.0647,0.0096,0.0046,0.0205,0.0157,-0.0247,0.0341,0.0008,-0.0256,-0.042,-0.076,-0.2424,0.0073,-0.0685,0.0068,-0.0166,-0.0124,-0.0371,-0.0022,0.0327,-0.0462,0.0314,-0.0067,0.0033,-0.0235,-0.0542,-0.0105,-0.0701,-0.0364,-0.0426,0.013,-0.0451,0.0388,-0.0332,-0.0084,0.0408,0.0065,0.0263,0.0606,0.0424,0.0351,0.0588,0.0158,0.0879,-0.1585,0.0613,0.0424,0.0411,-0.0553,-0.0587,0.0262,0.0458,-0.0163,0.0554,0.0336,0.0657,-0.017,-0.0048,0.0089,-0.0038,-0.0141,0.0062,0.0376,0.0278,-0.0532,-0.054,-0.0043,-0.0143,-0.0282,-0.0719,0.0141,0.0493,-0.0122,-0.0099,-0.0619,0.0585,-0.0685,-0.019,0.1017,0.0288,-0.029,0.1472,-0.0409,0.0071,0.0244,-0.0107,0.0117,-0.0049,-0.0336,-0.072,-0.0102,0.0119,0.0121,-0.0308,0.0122,0.0058,0.0191,0.024,0.0489,0.0639,-0.002,-0.0316,-0.0179,0.0012,0.0764,-0.0054,0.0126,-0.0352,0.0194,0.1354,0.0259,0.0136,0.0511,0.0234,-0.0392,-0.0443,-0.0332,0.0163,0.0259,-0.0095,-0.0348,-0.0024,-0.0678,-0.0831,0.0454,-0.0546,-0.0643,0.0692,-0.0763,-0.0012,-0.0192,-0.0545,0.0203,0.0599,-0.0282,-0.0451,0.0411,0.0148,0.0129,0.0354,-0.0463,0.0179,-0.0122,-0.0777,-0.0545,0.1128,0.0129,-0.0795,-0.0692,-0.0161,-0.003,0.0075,0.0756,0.0376,-0.0067,-0.003,0.0515,0.0549,-0.0747,-0.0229,0.0107,0.012,0.015,-0.0328,-0.0114,0.0248,0.005,-0.0212,0.0318,-0.0851,0.0578,0.0783,-0.0756,0.0468,-0.0693,0.0076,-0.014,-0.0508,-0.0177,-0.0187,0.0061,-0.0322,0.0146,0.005,0.0139,0.0024,-0.0395,0.0274,-0.0414,0.0786,0.039,-0.0055,-0.0397,-0.0252,0.0703,-0.0299,-0.0316,0.0003,0.0312,0.0316,-0.0235,0.0355,0.0218,0.0075,-0.0338,-0.2006,-0.0149,0.0542,0.0089,0.0335,-0.051,0.0332,0.0448,0.025,0.0556,0.0599,0.0247,-0.0124,0.0042,0.0232,0.037,-0.0018,0.0218,-0.0674,-0.0153,-0.0119,-0.0035,-0.0554,-0.1119,0.0313,-0.0117,0.2327,0.0108,0.0329,0.0051,0.0476,-0.0021,-0.0604,-0.0929,0.0811,0.0047,0.0412,0.0183,-0.1024,-0.0041,-0.0444,-0.0023,-0.008,-0.1268,-0.0204,0.0207,-0.0096,0.1008,-0.0516,-0.0116,0.0087,-0.0405,0.0275,-0.0119,0.0321,-0.0022,-0.0857,0.0409,-0.0425,0.0073,0.0306,-0.0478,0.0335,-0.0692,0.0329,-0.0071,-0.06,-0.0414,0.015,-0.0264,-0.024,0.074,-0.0024,-0.0091,0.1206,0.0095,0.049,-0.0197,-0.0477,-0.0133,0.074,-0.0411,0.0268,0.0263,0.0121,0.0661,0.076,0.0216,-0.021,-0.0323,-0.0166,0.0537,-0.0326,-0.0213,0.035,0.0053,-0.3084,0.0452,-0.0037,0.0512,-0.0038,-0.0055,0.0391,-0.0,0.0104,-0.0144,0.013,0.0015,0.0427,-0.044,0.0108,0.0203,0.0606,-0.0699,0.0464,-0.0443,-0.0161,0.0097,0.2147,-0.0581,-0.0051,0.0582,-0.0346,-0.0117,0.0343,-0.0103,0.0658,0.0261,0.0607,-0.0644,0.029,0.1243,-0.0139,0.0518,-0.0171,-0.0338,0.0261,0.0314,-0.0071,-0.0248,0.047,-0.0479,-0.0504,-0.0308,0.0141,0.008,-0.0141,0.0338,-0.0079,0.0129,0.0217,0.0178,-0.0328,-0.0529,-0.0479,0.0007,0.0356,-0.0667,-0.0062,0.0293,-0.003]}
{"key":"[Improved generator objectives for GANs] We present a framework to understand GAN training as alternating density ratio estimation and approximate divergence minimization. This provides an interpretation for the mismatched GAN generator and discriminator objectives often used in practice, and explains the problem of poor sample diversity. We also derive a family of generator objectives that target arbitrary $f$-divergences without minimizing a lower bound, and use them to train generative image models that target either improved sample quality or greater sample diversity.","layer":1,"vector":[-0.0226,-0.0171,0.0087,-0.0485,0.0358,0.0361,0.0119,-0.022,-0.0231,0.0361,0.0183,-0.074,0.0423,0.0777,-0.0091,0.0143,0.0396,-0.0066,-0.0249,0.0117,0.0317,0.0009,0.0136,-0.0462,-0.0078,-0.0195,-0.0195,-0.0746,-0.0197,-0.2332,0.0179,-0.0235,0.0513,-0.0269,-0.0065,-0.0404,-0.034,0.0344,-0.0026,0.0646,-0.0137,0.0276,-0.0414,-0.0702,0.0104,-0.0028,-0.0853,-0.0143,-0.0119,-0.0549,0.034,-0.0439,0.0132,0.026,0.0268,0.0275,0.0771,0.0352,0.0291,0.0947,-0.0004,0.0679,-0.1348,0.0426,0.0347,0.0081,-0.056,-0.0452,0.0015,0.0333,-0.0171,0.0271,-0.0074,0.0355,0.0019,-0.0088,-0.0199,-0.0487,-0.0524,-0.0116,0.0214,-0.007,-0.0548,0.0358,0.0196,-0.0138,0.014,-0.0328,0.0381,0.0164,-0.0085,-0.0032,-0.0597,0.0072,-0.0091,-0.0045,0.0201,-0.0138,-0.0575,0.2331,-0.0313,0.0435,0.0712,-0.0343,-0.0103,-0.0254,-0.0733,0.0125,-0.0493,-0.0317,-0.0015,-0.0238,-0.0084,-0.0083,0.0216,-0.0422,0.0439,0.0106,0.0025,-0.0075,-0.0648,0.0183,0.0241,-0.0436,0.0329,-0.0695,0.0153,0.161,0.0464,0.0546,-0.0021,-0.0211,-0.0569,0.0038,0.011,0.0172,0.0393,0.037,-0.0076,0.0159,-0.0339,-0.0486,0.0173,-0.0286,-0.0152,0.1191,-0.0416,0.0309,-0.0518,-0.0244,0.0051,0.0302,-0.0416,-0.0044,0.0437,0.0798,0.0364,0.0632,-0.0482,0.0429,-0.0208,-0.0687,-0.0502,0.1004,0.0341,-0.0693,-0.0261,0.0227,0.0077,0.0053,-0.0168,0.0222,-0.0052,0.046,0.0671,0.0284,-0.1022,-0.025,-0.009,0.0063,0.0057,-0.0491,-0.0509,0.0525,0.0033,-0.0283,0.0196,-0.0625,-0.0007,0.0878,-0.0225,0.0075,-0.0135,-0.024,-0.0057,-0.0363,-0.0358,-0.0054,-0.0034,-0.0618,-0.0152,0.0263,-0.0196,0.0093,-0.0136,0.0274,-0.0034,-0.0619,0.0321,0.0763,-0.025,-0.0184,0.0527,0.0114,-0.0385,0.0118,0.018,0.0302,-0.0164,0.0324,0.0229,-0.0595,-0.068,-0.2473,0.0392,-0.0249,-0.0019,0.0528,-0.0693,0.0297,0.0259,0.0217,0.0508,0.0342,0.0059,-0.0014,0.0351,-0.0263,0.0321,0.0061,0.0301,0.0022,-0.0145,0.0062,0.053,0.0231,-0.0966,0.0533,-0.02,0.2275,0.0234,0.0453,0.0035,0.0633,0.0365,-0.0282,-0.0695,0.0522,0.044,0.0678,-0.015,-0.0529,0.0228,-0.0416,0.0391,0.0121,-0.1149,-0.0103,-0.0214,-0.0456,0.0507,-0.0502,0.021,0.0565,-0.0441,0.0799,-0.0311,0.0033,-0.0299,-0.1306,0.0348,-0.0535,0.0282,0.0088,-0.0673,0.0497,-0.0617,0.0682,0.0064,-0.0542,-0.049,0.0405,0.0158,0.0149,0.0268,-0.023,-0.0047,0.0385,0.0071,0.0076,-0.0528,-0.0605,-0.0333,0.0127,-0.0158,0.0399,0.0111,0.0321,0.0335,0.0677,0.0144,0.0516,-0.0177,-0.0014,0.0297,-0.0622,0.0148,0.0253,-0.0365,-0.3044,0.0212,-0.016,0.0391,-0.0142,0.0361,0.0895,0.0126,-0.0365,0.0103,0.0018,0.0284,0.0323,-0.0322,0.0164,0.0606,0.0864,-0.0705,0.025,-0.046,0.0028,0.0268,0.2128,-0.0516,-0.0557,0.0048,-0.0456,-0.0253,0.0318,-0.0412,0.0029,0.0142,0.0863,-0.0105,0.0451,0.1037,-0.0274,0.0405,-0.0024,-0.0376,-0.0468,0.0044,-0.0384,0.0259,0.0761,-0.0285,-0.031,0.0097,-0.023,-0.012,-0.0168,0.0332,-0.0239,0.0413,0.0303,0.0149,-0.044,-0.0285,-0.0016,-0.0507,0.0504,-0.0188,-0.0289,-0.007,0.01]}
{"key":"[Adaptive Semantic Segmentation with a Strategic Curriculum of Proxy Labels] Training deep networks for semantic segmentation requires annotation of large amounts of data, which can be time-consuming and expensive. Unfortunately, these trained networks still generalize poorly when tested in domains not consistent with the training data. In this paper, we show that by carefully presenting a mixture of labeled source domain and proxy-labeled target domain data to a network, we can achieve state-of-the-art unsupervised domain adaptation results. With our design, the network progressively learns features specific to the target domain using annotation from only the source domain. We generate proxy labels for the target domain using the network's own predictions. Our architecture then allows selective mining of easy samples from this set of proxy labels, and hard samples from the annotated source domain. We conduct a series of experiments with the GTA5, Cityscapes and BDD100k datasets on synthetic-to-real domain adaptation and geographic domain adaptation, showing the advantages of our method over baselines and existing approaches.","layer":9,"vector":[-0.0375,-0.0452,0.0247,0.0074,0.048,0.0012,0.033,-0.0067,-0.0141,-0.0092,0.0095,-0.0473,0.0152,0.0592,0.0374,0.0352,0.0178,0.0766,-0.0039,0.0039,0.0627,-0.0178,-0.0096,-0.027,-0.0177,0.0441,0.019,-0.0161,-0.0604,-0.2224,0.005,-0.058,0.0146,0.0108,0.0063,-0.0302,-0.0671,0.0612,0.0162,0.0339,0.0544,-0.0091,-0.0443,-0.0553,-0.0404,-0.0166,-0.0172,-0.0117,0.003,-0.0203,0.0313,-0.047,0.0411,0.0048,-0.0183,0.0473,0.0557,0.0462,0.0434,0.0487,0.0465,0.0766,-0.1558,0.0726,0.021,-0.0077,-0.0407,0.009,-0.028,0.0551,0.0004,0.0479,-0.0269,0.0484,0.0438,-0.009,-0.0134,-0.0232,-0.0171,-0.0102,-0.0064,0.0275,-0.0692,-0.0164,0.0335,-0.0426,0.0042,-0.051,0.0387,0.0116,-0.0322,-0.0539,0.0041,0.0217,-0.0585,-0.0292,0.0403,-0.0146,-0.0392,0.2104,-0.0524,0.0515,0.012,-0.0347,-0.0145,-0.0245,-0.0239,0.0007,-0.0134,0.0241,-0.0132,-0.016,0.0289,0.0061,0.0201,-0.0312,0.1064,0.0151,-0.0279,-0.017,-0.0456,-0.0207,0.0429,-0.0333,0.0184,-0.0288,0.0164,0.1137,0.0773,0.0518,0.0559,0.0032,-0.0743,-0.0048,0.0156,0.0384,0.0317,-0.0115,-0.0003,0.0114,-0.0205,-0.0381,0.0116,-0.06,-0.0292,0.1173,-0.0725,0.0106,-0.0414,-0.0428,-0.0211,0.0162,-0.0503,-0.0245,0.0322,-0.0039,0.0546,0.0662,-0.0503,0.0265,0.0143,-0.0447,-0.0246,0.0941,0.0178,-0.096,-0.0757,0.0131,-0.0047,-0.0315,0.0321,0.0439,0.0018,0.0181,0.0835,-0.0003,-0.1216,0.011,0.0202,0.0202,0.012,-0.091,-0.0239,0.0432,0.0343,-0.0786,-0.0239,-0.0309,0.0367,0.0711,0.003,0.0586,-0.0257,-0.003,0.0433,-0.0306,-0.0083,0.0297,-0.0066,-0.0421,0.0114,-0.0064,-0.0384,0.0117,-0.0451,-0.0032,-0.0332,0.0488,0.0459,0.0171,-0.0245,0.0321,0.022,-0.0174,-0.0044,0.0049,0.0526,0.017,0.0214,0.043,0.0323,-0.0236,-0.0581,-0.2171,0.0372,0.0246,-0.076,0.0129,-0.0408,0.0036,0.0336,0.0616,0.0602,0.057,-0.0425,-0.045,0.0418,-0.0219,0.0026,0.0368,0.0289,-0.0171,-0.0233,-0.0241,0.036,-0.0061,-0.075,0.0948,-0.0246,0.2311,0.0297,0.0569,-0.0328,0.0249,0.0082,0.0139,-0.1332,0.0559,0.0232,0.0661,-0.0071,-0.0514,-0.0447,-0.0272,0.0168,0.0122,-0.124,-0.0227,-0.0306,-0.0151,0.0159,-0.0109,-0.0263,0.0015,-0.036,0.0559,0.005,-0.0319,-0.0512,-0.053,0.0178,-0.009,0.0193,0.0084,-0.0376,0.0467,-0.0988,0.0561,0.0062,-0.0289,-0.0503,0.0147,-0.0149,-0.0695,0.0466,0.0292,-0.0021,0.0396,-0.0266,0.0027,0.0001,-0.0777,-0.0091,0.0909,-0.0619,0.0233,-0.0208,0.0396,0.0472,0.0987,-0.0624,0.0313,-0.0223,0.0149,-0.0014,-0.0846,-0.0591,0.0489,-0.0541,-0.2728,0.0612,0.0351,0.0329,-0.0034,-0.0113,0.074,0.0695,-0.0311,0.0078,0.0009,0.0183,0.047,-0.0276,-0.0067,0.0639,0.0755,-0.0263,0.0651,-0.0331,0.0059,0.0485,0.1968,-0.0731,0.0055,-0.0129,-0.0278,-0.0074,0.022,-0.0074,-0.0087,0.0141,0.103,-0.0551,0.0173,0.1048,-0.0172,-0.0176,-0.0326,-0.0022,-0.0072,0.0056,-0.0337,-0.0071,0.0442,0.0292,0.0039,-0.0439,-0.0391,0.0704,-0.0239,-0.021,0.0096,0.0169,0.0686,-0.0161,-0.0341,-0.0456,-0.1131,-0.0241,0.0064,-0.091,-0.0196,0.007,-0.0208]}
{"key":"[Efficient forward propagation of time-sequences in convolutional neural networks using Deep Shifting] When a Convolutional Neural Network is used for on-the-fly evaluation of continuously updating time-sequences, many redundant convolution operations are performed. We propose the method of Deep Shifting, which remembers previously calculated results of convolution operations in order to minimize the number of calculations. The reduction in complexity is at least a constant and in the best case quadratic. We demonstrate that this method does indeed save significant computation time in a practical implementation, especially when the networks receives a large number of time-frames.","layer":1,"vector":[-0.0642,-0.008,0.0328,-0.0306,0.0485,0.0379,-0.0078,-0.0066,0.0379,-0.0194,-0.0244,-0.0535,0.0394,0.0687,0.01,-0.0013,-0.0129,0.037,-0.04,-0.0273,0.049,-0.0536,0.0181,0.0053,0.0187,0.0089,-0.002,-0.0184,-0.0585,-0.2238,0.0138,-0.0452,0.0421,-0.0222,-0.0272,-0.0511,-0.0532,0.047,-0.0409,0.0526,0.0481,0.0446,-0.0433,-0.0921,-0.0391,-0.0464,-0.0113,-0.0369,0.0275,-0.0107,-0.0155,-0.0371,0.0368,0.0409,0.0229,-0.0105,0.1047,0.0398,0.0737,-0.0137,0.0343,0.061,-0.1706,0.0637,-0.0003,0.0116,-0.0209,-0.0363,0.0161,0.0328,-0.0478,0.0248,0.0016,0.0035,0.0026,0.015,-0.0118,-0.0018,-0.0159,0.0014,0.0361,-0.0072,-0.0133,-0.0633,0.0202,-0.0362,0.0015,-0.0335,0.0093,-0.0004,-0.0504,-0.02,-0.0306,0.0198,-0.0623,0.0116,0.0462,0.0137,-0.037,0.1829,-0.0469,0.0563,0.0206,-0.0316,0.036,-0.0141,-0.0052,-0.0252,-0.0723,0.0009,-0.019,-0.0246,0.0428,-0.0012,0.0153,0.0587,0.0297,0.065,-0.0207,0.0196,-0.008,0.0298,0.033,-0.0295,0.0105,-0.0535,0.0637,0.0942,0.0398,0.0235,0.0709,0.0067,-0.0267,0.0081,-0.0052,0.074,0.0087,-0.0182,0.008,-0.0449,-0.0644,-0.0518,0.0043,-0.0118,0.0099,0.1055,-0.0149,0.0449,-0.0122,-0.0557,-0.0258,-0.0002,-0.0229,-0.0294,0.0564,0.0016,0.0564,0.0279,-0.0785,0.0205,-0.0343,-0.0322,-0.0266,0.1112,0.016,-0.0981,-0.0113,-0.03,0.0101,-0.0566,0.0401,0.0505,-0.0393,0.0101,0.0995,0.0399,-0.0724,-0.0121,0.0012,0.0134,0.0324,-0.0475,-0.014,0.0558,0.0462,-0.0357,0.0088,-0.0501,-0.0198,0.0698,-0.0197,0.0512,-0.0697,0.0487,-0.0058,-0.032,-0.0093,0.0193,-0.0098,-0.0369,-0.0207,-0.0315,-0.0154,-0.0316,-0.0196,0.0157,-0.0378,0.0107,0.0348,0.069,-0.0226,0.0075,0.0884,-0.05,-0.019,0.0058,0.0047,0.0268,-0.001,0.0225,0.008,-0.0615,0.0038,-0.2368,0.0094,0.0109,-0.0218,0.0705,-0.0782,0.0053,-0.0092,0.0933,0.0439,0.0453,-0.0411,0.0113,-0.0333,0.0104,0.0528,0.0213,0.0563,-0.0239,0.0204,-0.0017,-0.0134,-0.002,-0.134,0.0494,0.0069,0.2238,0.0171,0.0532,-0.0405,0.0045,0.0216,0.0033,-0.0602,0.0592,-0.0037,0.0746,0.0281,-0.0073,-0.0651,-0.0238,0.0139,0.0166,-0.0828,-0.0272,-0.0008,-0.0079,0.014,-0.04,-0.0152,-0.0085,-0.083,0.0135,0.003,0.0083,-0.0451,-0.0751,0.0514,-0.0538,-0.0077,-0.0394,-0.0177,-0.0092,-0.0261,0.052,-0.0134,-0.0191,-0.0635,-0.0007,-0.0105,0.0012,0.0937,0.029,0.0387,0.0924,-0.0204,0.0116,-0.018,-0.0665,-0.0473,0.0696,-0.0024,0.0153,0.037,0.0628,0.0016,0.0818,-0.0036,0.0161,-0.0163,0.0101,0.0325,-0.0812,0.0095,0.0375,-0.0438,-0.3113,0.0934,0.0027,0.0168,0.0271,0.0032,0.0256,0.0306,-0.0286,-0.0259,-0.0732,0.0617,0.0542,-0.0384,0.0419,0.0096,0.0688,-0.064,0.0465,-0.0702,0.0186,0.0288,0.2048,-0.0567,0.0263,0.0076,-0.0556,-0.0158,0.0463,-0.0271,0.0067,0.0369,0.0708,-0.0481,-0.0004,0.0636,-0.0463,0.0627,0.0242,0.01,-0.0178,0.0238,-0.0172,-0.013,0.1283,-0.0223,-0.0077,-0.0384,0.0135,0.0237,-0.0478,-0.0274,0.0142,0.0172,0.0366,0.0198,-0.0512,-0.0093,-0.0352,0.0243,0.0397,-0.111,-0.0271,-0.0268,-0.0325]}
{"key":"[APP: Anytime Progressive Pruning] With the latest advances in deep learning, there has been a lot of focus on the online learning paradigm due to its relevance in practical settings. Although many methods have been investigated for optimal learning settings in scenarios where the data stream is continuous over time, sparse networks training in such settings have often been overlooked. In this paper, we explore the problem of training a neural network with a target sparsity in a particular case of online learning: the anytime learning at macroscale paradigm (ALMA). We propose a novel way of progressive pruning, referred to as \\textit{Anytime Progressive Pruning} (APP); the proposed approach significantly outperforms the baseline dense and Anytime OSP models across multiple architectures and datasets under short, moderate, and long-sequence training. Our method, for example, shows an improvement in accuracy of $\\approx 7\\%$ and a reduction in the generalization gap by $\\approx 22\\%$, while being $\\approx 1/3$ rd the size of the dense baseline model in few-shot restricted imagenet training. We further observe interesting nonmonotonic transitions in the generalization gap in the high number of megabatches-based ALMA. The code and experiment dashboards can be accessed at \\url{https://github.com/landskape-ai/Progressive-Pruning} and \\url{https://wandb.ai/landskape/APP}, respectively.","layer":0,"vector":[-0.0232,0.0088,0.0097,-0.0149,0.0499,0.0461,-0.0292,-0.0323,0.0201,-0.0378,0.052,-0.0526,0.0399,0.0622,0.0289,0.0481,0.0444,0.0149,-0.0266,-0.0242,0.0231,-0.0562,0.0335,-0.0813,0.0142,0.0251,-0.0099,-0.0608,-0.0467,-0.2624,0.0179,-0.0211,0.0595,-0.0278,0.0025,0.0036,-0.0297,0.0716,-0.051,0.0284,0.0098,0.0442,-0.0196,-0.044,-0.0109,-0.0234,0.0027,-0.0651,-0.0124,-0.0473,0.0451,-0.05,0.0387,0.0292,0.0413,0.0128,0.0239,0.0521,0.0611,0.0161,0.0036,0.0105,-0.1439,0.0236,0.057,0.0478,0.007,0.0019,-0.0348,0.0559,-0.0115,0.067,0.0023,0.0635,0.0198,0.0037,-0.0067,-0.0214,-0.0201,-0.0243,0.0248,-0.0148,-0.0777,-0.0393,-0.0201,-0.024,0.0084,-0.0623,0.0033,-0.0333,-0.0254,-0.0044,-0.0187,-0.0048,-0.0523,-0.0132,0.0504,0.018,-0.0166,0.1949,-0.0451,0.063,0.0616,-0.0119,0.0448,-0.0612,-0.023,0.0041,-0.0123,-0.0198,0.0051,-0.0393,0.0114,-0.0426,0.0572,-0.0192,0.0735,0.0316,-0.01,0.0405,-0.0598,0.0125,0.0091,-0.0026,0.0504,-0.044,0.0237,0.152,0.0451,0.0414,0.0724,-0.0335,-0.032,-0.0376,0.0357,-0.0007,0.0302,-0.0411,0.0373,-0.0129,-0.0672,-0.0301,0.0258,-0.0858,-0.0405,0.1249,-0.0346,0.0605,0.0022,-0.0294,-0.0346,-0.0027,-0.0381,-0.0197,0.0359,0.0105,0.0336,0.0424,-0.0323,-0.0271,-0.0374,-0.0554,-0.0188,0.0966,0.024,-0.0772,-0.0319,-0.011,-0.0082,-0.003,0.0452,0.0232,0.0022,0.028,0.0764,0.0381,-0.057,-0.0114,0.0064,0.0023,0.0072,-0.0515,0.0073,0.0513,0.0257,-0.0215,0.0113,-0.023,0.048,0.005,-0.03,0.0259,-0.0553,-0.0285,-0.0199,-0.0403,0.0269,0.0031,-0.0027,-0.0057,0.0246,-0.006,-0.0404,0.0022,0.0253,0.0313,0.0254,0.0132,0.0386,0.0511,-0.0543,-0.0325,0.0497,-0.0108,-0.0186,0.017,0.0331,0.0051,-0.005,0.0525,0.0135,-0.054,-0.0634,-0.2431,0.0493,0.0297,-0.0339,0.0715,-0.0752,0.0465,-0.0245,0.0513,0.0314,0.0443,-0.022,0.0192,0.0231,0.0014,0.0692,0.0186,-0.0029,-0.0544,-0.0285,-0.0104,0.0163,-0.002,-0.0718,0.0548,0.008,0.2248,0.0075,0.0328,-0.0663,0.0269,0.0234,-0.0603,-0.1122,0.0403,0.0154,0.1159,0.0082,-0.0533,-0.0034,-0.0099,0.0107,0.0098,-0.1339,-0.0658,-0.023,-0.0076,0.0406,-0.0094,0.0017,0.0445,-0.0309,0.0335,-0.052,0.0088,-0.0488,-0.1174,0.0591,-0.0363,0.0294,0.0424,-0.0491,-0.0398,-0.073,0.079,0.0022,-0.0159,-0.0612,0.055,-0.0364,-0.0124,0.0574,-0.0211,0.0099,0.0415,0.0087,0.0313,-0.0135,-0.0496,-0.0264,0.0667,-0.0402,0.0228,0.0315,0.0531,0.0509,0.1,-0.0227,-0.0085,0.0041,0.0026,0.0136,-0.0778,-0.0108,0.0341,-0.0114,-0.2902,0.0762,0.0359,0.0307,-0.0603,0.0414,0.0555,0.0115,-0.0389,-0.0286,-0.0162,0.0582,0.0307,0.0023,-0.0285,0.0726,0.0677,-0.0441,0.0448,-0.0927,-0.01,-0.0016,0.2037,-0.0542,0.0246,0.0067,0.0088,-0.0054,0.0355,0.0026,0.0116,0.0127,0.0377,-0.0852,0.0167,0.0663,-0.0398,0.0455,0.0073,-0.0026,-0.0608,-0.0221,-0.003,-0.0195,0.05,-0.0298,-0.0234,-0.0005,-0.0412,0.0107,-0.0243,0.0012,0.0082,-0.0092,-0.0042,0.0188,-0.0222,-0.02,-0.0501,-0.0444,0.0483,-0.0419,-0.0657,-0.0066,0.0011]}
{"key":"[NAS-VAD: Neural Architecture Search for Voice Activity Detection] Various neural network-based approaches have been proposed for more robust and accurate voice activity detection (VAD). Manual design of such neural architectures is an error-prone and time-consuming process, which prompted the development of neural architecture search (NAS) that automatically design and optimize network architectures. While NAS has been successfully applied to improve performance in a variety of tasks, it has not yet been exploited in the VAD domain. In this paper, we present the first work that utilizes NAS approaches on the VAD task. To effectively search architectures for the VAD task, we propose a modified macro structure and a new search space with a much broader range of operations that includes attention operations. The results show that the network structures found by the propose NAS framework outperform previous manually designed state-of-the-art VAD models in various noise-added and real-world-recorded datasets. We also show that the architectures searched on a particular dataset achieve improved generalization performance on unseen audio datasets. Our code and models are available at https://github.com/daniel03c1/NAS_VAD.","layer":1,"vector":[-0.0509,-0.0247,0.055,-0.0474,0.0167,0.0323,0.0439,-0.0227,0.0627,-0.0429,0.0073,-0.0132,0.0411,0.0347,0.0243,-0.0281,0.0184,0.0469,0.0033,0.002,0.0015,0.0283,0.0061,-0.0488,0.002,-0.0173,-0.042,-0.0106,-0.0854,-0.2184,0.0084,-0.0487,0.1282,-0.0556,-0.0095,-0.0083,-0.0227,0.067,-0.0387,0.0428,0.027,0.015,-0.0037,-0.0909,-0.0108,-0.0588,-0.0698,-0.0332,-0.0474,-0.0502,0.0239,-0.0558,0.0383,0.037,0.0413,0.031,0.0571,0.0517,0.0428,0.0232,-0.0117,0.0361,-0.1746,0.0617,0.0216,0.0373,-0.0018,-0.0374,0.0099,0.018,-0.0083,0.0498,-0.0109,0.0306,0.0119,0.0289,0.0284,-0.0415,0.0253,-0.0058,0.0158,-0.0094,-0.0124,-0.0029,-0.0008,-0.0531,-0.0079,-0.0123,-0.0008,-0.017,-0.1125,0.0338,-0.0292,0.027,-0.0501,-0.0147,0.0283,0.0321,-0.0567,0.2336,-0.0325,0.0326,0.0098,-0.0303,0.0227,0.0157,-0.0335,-0.0249,-0.0309,0.0145,0.0205,-0.0353,0.0249,-0.038,0.0412,0.0323,0.0425,0.014,-0.0194,-0.0205,-0.0508,-0.0197,0.0622,-0.0509,0.0494,-0.0449,0.0369,0.1254,0.0301,0.0411,0.0918,-0.0136,-0.0355,-0.0281,0.0553,0.0398,0.0096,-0.0281,0.0007,-0.0311,-0.0597,-0.0876,0.018,-0.0852,-0.0489,0.106,-0.0368,-0.0208,-0.0469,-0.0091,-0.0145,0.0391,-0.0095,-0.028,0.059,0.0252,0.0394,0.0343,-0.0535,-0.0389,-0.0353,-0.0915,-0.0263,0.1202,0.0287,-0.0801,-0.0641,-0.0242,0.0135,-0.0105,0.0045,0.0372,-0.0323,0.0137,0.0434,0.0087,-0.0459,0.0577,-0.0077,-0.0048,0.0142,-0.0601,0.0077,0.0159,0.026,-0.0349,0.0294,-0.0101,0.0522,0.0689,-0.062,0.0227,-0.0238,-0.0076,-0.0601,-0.0366,0.0211,-0.0174,0.0377,0.0031,0.0311,-0.0226,-0.0154,0.0165,-0.0093,0.0095,-0.0718,0.0323,0.0573,0.0402,-0.0075,0.0162,0.0774,-0.0126,-0.0553,-0.0372,0.0216,0.0292,-0.0169,0.0576,0.0403,-0.0358,-0.0671,-0.2376,-0.0197,0.0013,-0.0248,0.0404,-0.0457,0.0559,-0.0183,0.057,0.0441,0.0441,-0.0221,-0.0329,0.0366,0.0209,0.0847,0.0075,-0.0012,-0.0433,-0.0082,0.0573,-0.0037,-0.0059,-0.0704,0.0666,0.0311,0.2159,0.0127,0.0773,0.0084,0.0417,0.0025,-0.0167,-0.0998,0.068,0.0105,0.0664,0.0092,-0.0775,-0.0457,-0.0677,0.0372,-0.0098,-0.0824,-0.0359,-0.007,-0.0079,-0.003,-0.0592,-0.0097,0.0403,-0.0077,0.0777,0.0081,0.0265,-0.0329,-0.0794,-0.0068,-0.0547,0.0556,0.0186,-0.01,0.0604,-0.05,-0.0002,0.0129,-0.0351,-0.0366,0.0332,-0.0312,-0.0163,0.0736,-0.0097,0.0044,0.0356,-0.0067,0.0016,-0.061,0.0058,-0.0117,0.0309,-0.0498,0.0339,-0.0739,0.0236,-0.0042,0.0744,-0.0064,0.0332,-0.049,0.0309,0.0072,0.0008,0.0048,0.027,0.021,-0.2886,0.0085,0.0387,0.0209,-0.0584,-0.0053,0.0065,0.0218,-0.0704,-0.0005,-0.0086,0.0559,-0.0037,0.0129,-0.0179,0.0708,0.0774,-0.0063,0.0526,-0.0439,0.0115,0.0585,0.2166,-0.0027,-0.0065,-0.0349,-0.0176,-0.038,0.0428,-0.0502,-0.0025,-0.0141,0.1073,-0.0533,-0.0182,0.0718,-0.0357,0.0452,0.0045,0.0031,0.0013,0.0121,-0.0251,-0.0393,0.0752,-0.006,-0.0205,-0.0289,0.0145,0.06,-0.0112,-0.0064,0.012,0.0007,0.0565,0.0212,-0.0743,-0.04,-0.0122,-0.017,0.0292,-0.0394,0.0207,-0.0195,-0.0043]}
{"key":"[Constrained Monotonic Neural Networks] Deep neural networks are becoming increasingly popular in approximating arbitrary functions from noisy data. But wider adoption is being hindered by the need to explain such models and to impose additional constraints on them. Monotonicity constraint is one of the most requested properties in real-world scenarios and is the focus of this paper. One of the oldest ways to construct a monotonic fully connected neural network is to constrain its weights to be non-negative while employing a monotonic activation function. Unfortunately, this construction does not work with popular non-saturated activation functions such as ReLU, ELU, SELU etc, as it can only approximate convex functions. We show this shortcoming can be fixed by employing the original activation function for a part of the neurons in the layer, and employing its point reflection for the other part. Our experiments show this approach of building monotonic deep neural networks have matching or better accuracy when compared to other state-of-the-art methods such as deep lattice networks or monotonic networks obtained by heuristic regularization. This method is the simplest one in the sense of having the least number of parameters, not requiring any modifications to the learning procedure or steps post-learning steps.","layer":3,"vector":[-0.0466,-0.0445,0.0358,-0.0429,-0.038,0.0874,0.0591,0.0144,0.0454,-0.0521,-0.0043,-0.0667,0.0254,0.0796,0.0268,0.0219,0.021,0.0618,-0.0649,0.0236,0.0159,-0.0367,-0.0076,-0.049,0.064,-0.027,-0.0072,-0.0414,-0.0328,-0.272,-0.0238,-0.0318,0.0758,-0.0597,0.0102,-0.013,-0.021,0.0337,-0.049,0.0264,0.0137,0.0439,0.011,-0.0677,-0.0274,-0.0779,-0.0581,-0.0246,0.015,-0.014,0.0258,-0.038,0.0126,-0.0171,0.0082,0.0204,0.043,0.0519,0.0696,0.0769,0.0392,0.0388,-0.1456,0.0299,0.0513,0.0094,-0.0479,-0.0692,-0.0013,0.1029,0.0113,0.0305,0.0191,-0.0018,0.0083,0.0325,0.0149,-0.0251,0.0013,-0.0028,0.0168,-0.0463,-0.0202,-0.0099,0.0149,-0.0011,0.0088,-0.0495,-0.0163,-0.0099,-0.0173,0.0084,-0.018,0.0168,-0.0582,-0.0212,0.0194,0.0214,-0.0811,0.2022,-0.053,0.0804,0.0551,-0.0145,0.0647,0.0049,-0.0754,-0.0168,-0.0182,-0.0154,-0.0334,-0.0295,0.0352,-0.0363,0.037,0.0566,0.0641,-0.0062,0.0067,-0.0159,0.0039,0.011,0.0026,0.0158,-0.0024,-0.0407,-0.0079,0.1166,0.0514,0.061,0.0096,-0.0429,-0.0293,-0.001,0.0338,0.0133,0.0105,-0.013,-0.0008,0.0177,-0.0343,-0.0454,0.033,-0.064,-0.0418,0.1459,-0.0145,0.0081,-0.049,-0.0059,-0.0034,0.0016,-0.0452,-0.0165,0.0473,0.0363,0.0147,0.0265,-0.0486,-0.0109,-0.0204,-0.0366,0.0043,0.1251,0.0358,-0.0611,-0.0361,0.0091,0.0125,-0.0244,0.0443,0.0613,-0.0021,0.0381,0.0806,0.037,-0.0837,-0.0102,0.0067,0.0072,-0.0151,-0.0675,-0.0212,0.0361,0.0102,-0.0314,0.0099,-0.0848,0.0344,0.0237,-0.0629,0.0237,-0.044,0.0019,-0.0066,-0.0414,-0.007,0.0237,0.0272,-0.0378,-0.0047,-0.0188,-0.0168,0.0159,0.0037,0.0135,-0.0393,-0.0162,0.029,0.041,-0.0546,0.0084,0.0945,-0.0606,-0.0078,-0.008,0.0195,-0.0185,-0.0169,0.0363,0.0486,-0.0702,-0.0618,-0.2455,0.0004,0.0139,-0.0347,0.0831,-0.0604,0.028,-0.004,0.0074,0.034,0.0802,0.022,-0.0126,0.0235,-0.0172,0.0588,0.0209,0.0044,-0.0037,0.0041,-0.0067,0.058,0.0154,-0.0919,0.0628,0.0113,0.2103,-0.0184,0.0945,-0.0257,0.0272,-0.024,-0.021,-0.0868,0.0456,0.0233,0.0773,-0.0285,-0.0637,-0.0286,-0.0115,0.0227,0.0184,-0.0766,0.0056,-0.0062,-0.011,0.029,-0.0918,-0.0037,0.0271,-0.0448,0.0443,-0.0013,-0.0094,-0.0103,-0.0891,0.0222,-0.0542,0.0084,0.0052,-0.0584,-0.0284,-0.0449,0.0339,0.027,-0.0135,-0.0358,0.0555,0.0006,-0.0291,0.0757,0.0067,0.0564,0.0406,-0.002,0.033,-0.0165,-0.0316,-0.0239,0.0647,-0.0017,0.0265,0.0073,0.0185,0.0269,0.0827,-0.0661,0.0603,0.0102,-0.0284,-0.0004,-0.0437,-0.0139,0.0562,-0.0152,-0.2568,0.0086,-0.0085,0.0445,-0.0193,-0.0016,0.0561,0.0113,-0.0787,-0.0629,0.0033,0.0321,0.0043,0.0179,0.0018,0.0152,0.0425,-0.0299,0.0471,-0.061,-0.0046,0.0214,0.2314,-0.0842,0.0097,0.042,-0.0177,-0.0471,0.0261,-0.0413,0.0177,0.0257,0.0362,-0.055,0.0264,0.1167,-0.0185,0.0239,0.0379,-0.022,-0.021,0.0113,-0.045,-0.0224,0.1034,0.0027,-0.0081,-0.0034,0.015,0.0463,-0.0356,0.0242,0.0233,0.0018,0.0294,0.008,-0.0766,-0.0815,-0.0296,-0.0151,0.0458,-0.0766,-0.0045,-0.0028,-0.0247]}
{"key":"[Ensemble Grammar Induction For Detecting Anomalies in Time Series] Time series anomaly detection is an important task, with applications in a broad variety of domains. Many approaches have been proposed in recent years, but often they require that the length of the anomalies be known in advance and provided as an input parameter. This limits the practicality of the algorithms, as such information is often unknown in advance, or anomalies with different lengths might co-exist in the data. To address this limitation, previously, a linear time anomaly detection algorithm based on grammar induction has been proposed. While the algorithm can find variable-length patterns, it still requires preselecting values for at least two parameters at the discretization step. How to choose these parameter values properly is still an open problem. In this paper, we introduce a grammar-induction-based anomaly detection method utilizing ensemble learning. Instead of using a particular choice of parameter values for anomaly detection, the method generates the final result based on a set of results obtained using different parameter values. We demonstrate that the proposed ensemble approach can outperform existing grammar-induction-based approaches with different criteria for selection of parameter values. We also show that the proposed approach can achieve performance similar to that of the state-of-the-art distance-based anomaly detection algorithm.","layer":2,"vector":[-0.0328,-0.0109,0.0246,0.0049,0.0643,-0.0083,0.0305,0.0213,0.0453,-0.048,0.0088,-0.0334,0.0145,0.0389,-0.0221,-0.0238,-0.0106,0.0145,-0.0569,-0.0227,0.0306,0.0129,-0.0267,-0.076,0.0022,0.0716,0.0086,-0.041,-0.0726,-0.211,0.0123,-0.0684,0.0658,-0.0327,0.0187,0.0104,-0.0814,0.061,-0.0202,0.0612,-0.0098,0.0047,0.0215,-0.1002,-0.0127,-0.0775,0.0153,-0.0131,-0.036,-0.0117,0.0027,-0.0025,0.0541,0.0278,-0.0079,0.0334,0.0157,0.0051,0.0707,0.0469,0.0375,0.0584,-0.1679,0.0464,0.0615,0.0263,0.0261,0.0097,0.0233,0.031,-0.006,0.0155,0.0142,0.0489,0.0208,0.0258,0.0111,-0.022,0.002,0.0268,0.0287,-0.043,-0.0509,-0.014,-0.0159,-0.0639,0.004,-0.0247,0.0938,0.0014,-0.0518,-0.0046,0.0064,0.0442,-0.0808,-0.0376,0.0225,0.0546,-0.0142,0.2011,-0.0518,0.0366,0.0189,-0.0115,0.0539,-0.03,-0.025,-0.0403,0.0088,-0.0175,-0.0328,-0.0013,0.0016,-0.0546,-0.0033,0.0212,0.0538,0.0381,-0.0335,-0.0032,-0.0122,-0.0349,0.0688,-0.0472,0.0013,-0.0557,0.0122,0.1451,0.0371,-0.0052,0.0277,0.0123,-0.0697,-0.0058,-0.0046,0.0129,-0.0238,0.0197,0.0277,-0.0274,-0.0784,-0.0611,0.0269,-0.0577,-0.0572,0.1009,-0.0593,-0.0022,-0.0532,-0.022,-0.0468,-0.0054,-0.0456,-0.0599,0.0593,0.0158,0.091,0.0125,-0.0149,0.0031,-0.0488,-0.0387,-0.024,0.1065,0.0176,-0.1027,-0.0425,0.0106,0.011,0.0042,0.0324,0.0346,-0.0324,0.0308,0.0778,0.0386,-0.0283,0.0367,0.0076,-0.0149,0.052,-0.0533,-0.0272,0.0633,0.0804,-0.0712,-0.0197,-0.0431,0.0521,0.0051,-0.0712,-0.0099,-0.0157,0.0056,-0.0183,-0.0153,-0.0008,-0.0057,0.0509,-0.1014,-0.0151,0.013,-0.029,0.0092,-0.0082,0.0064,-0.0195,0.014,0.0364,0.0501,-0.0021,0.0329,0.0604,-0.038,-0.0558,0.0199,-0.0163,0.0371,0.0163,0.0405,0.0171,0.0028,-0.0503,-0.2641,-0.0153,0.0618,-0.0296,0.043,-0.0358,0.0155,-0.0006,0.0585,0.0579,0.0125,-0.0181,-0.0247,-0.013,-0.0141,0.0665,0.0556,0.0506,-0.0449,0.0115,-0.0225,0.0075,-0.0061,-0.0802,0.0246,0.0234,0.1359,0.0584,0.0521,-0.0955,0.0319,0.0056,0.0054,-0.0146,0.0845,0.0445,0.0234,-0.0194,-0.0764,-0.0474,-0.0063,0.0382,-0.0082,-0.0515,-0.036,-0.0176,0.0063,0.0043,-0.0533,0.0284,0.076,-0.0101,0.0692,0.0009,0.0109,-0.0649,-0.1116,0.0345,-0.0094,0.0017,0.0382,-0.0205,0.035,-0.0839,0.0266,-0.0039,-0.0711,-0.0318,0.0088,-0.0034,-0.0514,0.1275,-0.0074,0.0026,0.0527,0.0136,-0.0014,-0.0265,-0.0099,-0.0278,0.0602,-0.0355,0.0452,0.0334,0.0265,0.0071,0.0676,0.0324,0.0456,-0.0036,0.0148,-0.004,-0.0354,-0.0229,0.0461,-0.0096,-0.3113,0.056,-0.0203,0.0227,0.0041,-0.031,0.015,0.0143,-0.0342,-0.0335,-0.0386,0.0008,0.0328,-0.0209,-0.005,0.0369,0.0493,-0.0805,0.0254,-0.0831,0.0397,0.0644,0.2419,0.0064,0.0337,0.026,-0.0124,-0.0353,0.042,-0.0351,0.01,-0.0264,0.0996,-0.0505,0.0204,0.048,-0.0135,0.0498,-0.0058,-0.0385,0.0052,0.0195,-0.0245,-0.015,0.0844,0.0126,-0.0175,-0.036,0.043,0.0634,-0.0162,0.0045,-0.0525,0.0355,0.0046,0.0479,-0.0347,-0.031,-0.0225,-0.0431,-0.0033,-0.061,-0.0228,-0.0253,-0.0161]}
{"key":"[Gamma Processes, Stick-Breaking, and Variational Inference] While most Bayesian nonparametric models in machine learning have focused on the Dirichlet process, the beta process, or their variants, the gamma process has recently emerged as a useful nonparametric prior in its own right. Current inference schemes for models involving the gamma process are restricted to MCMC-based methods, which limits their scalability. In this paper, we present a variational inference framework for models involving gamma process priors. Our approach is based on a novel stick-breaking constructive definition of the gamma process. We prove correctness of this stick-breaking process by using the characterization of the gamma process as a completely random measure (CRM), and we explicitly derive the rate measure of our construction using Poisson process machinery. We also derive error bounds on the truncation of the infinite process required for variational inference, similar to the truncation analyses for other nonparametric models based on the Dirichlet and beta processes. Our representation is then used to derive a variational inference algorithm for a particular Bayesian nonparametric latent structure formulation known as the infinite Gamma-Poisson model, where the latent variables are drawn from a gamma process prior with Poisson likelihoods. Finally, we present results for our algorithms on nonnegative matrix factorization tasks on document corpora, and show that we compare favorably to both sampling-based techniques and variational approaches based on beta-Bernoulli priors.","layer":4,"vector":[-0.0556,-0.0173,0.0303,-0.037,0.0291,0.051,0.0264,0.0086,0.0681,-0.024,0.0281,-0.0192,-0.0232,0.0406,0.0061,0.0016,0.013,0.0539,-0.0426,0.0131,0.0106,-0.0582,-0.0011,-0.0199,0.07,0.0221,-0.0535,-0.028,-0.046,-0.2553,-0.0083,-0.0572,0.0391,-0.0235,0.0247,-0.009,-0.065,0.0017,0.0033,0.0684,0.0244,0.0076,-0.06,-0.0378,-0.0169,-0.0566,-0.0175,-0.0196,-0.0537,-0.0224,0.0095,-0.0433,0.0699,0.0298,0.0508,0.0299,0.0613,0.0456,0.0828,0.0398,0.0209,0.0578,-0.1628,0.0759,0.0684,0.0389,-0.0613,-0.003,0.0156,0.0764,-0.0475,0.0307,-0.0405,0.0571,0.0255,0.0203,-0.0089,-0.0087,-0.0505,0.0118,-0.0085,-0.0015,-0.0017,-0.0057,-0.0611,-0.0538,-0.0056,-0.0569,0.0258,0.0065,-0.0094,-0.0306,-0.0311,0.0215,-0.0497,-0.0321,0.0305,0.0503,-0.0037,0.1862,-0.0496,0.0559,0.0613,0.0002,0.0134,-0.0057,-0.0372,-0.0434,-0.0058,0.0003,0.0023,-0.0278,0.0285,-0.0747,0.0344,-0.0387,0.0951,0.0342,-0.0051,-0.0388,-0.0226,0.0283,0.0382,-0.0494,-0.0071,-0.0315,0.0163,0.1256,0.0483,0.0025,0.073,-0.0594,-0.0523,0.0047,0.0375,0.0223,0.0247,-0.0157,0.0102,0.0233,-0.0338,-0.0654,0.0171,-0.0759,-0.0459,0.1167,-0.0463,0.0211,-0.0711,-0.032,-0.0181,0.0382,0.0073,-0.0428,0.0366,0.0032,0.0086,0.0515,-0.0708,0.0456,-0.0035,-0.0259,0.0102,0.1052,0.0032,-0.0545,-0.0515,-0.0416,0.0172,0.0165,0.0844,0.0879,-0.0026,0.0232,0.0685,0.0265,-0.0501,0.0181,0.0481,0.0205,0.0241,-0.0134,-0.0373,0.0488,0.0186,-0.0258,-0.0025,-0.0621,0.0587,0.029,-0.0265,-0.0038,-0.0233,-0.041,0.0174,-0.0412,-0.0225,-0.0014,0.0331,-0.0248,0.0045,-0.0049,-0.0865,0.0092,0.0065,0.0222,-0.0476,0.0214,0.0622,0.0533,-0.001,-0.0381,0.0619,0.034,0.0048,0.0311,-0.0119,0.0384,0.0238,0.073,0.0009,-0.0462,-0.0489,-0.2143,0.0103,-0.0157,-0.0046,0.0422,-0.0339,0.0283,0.0003,0.0605,0.118,0.0267,-0.0131,-0.0419,0.0056,0.0005,0.0362,0.0141,0.0195,-0.0166,0.0448,-0.0346,-0.0292,-0.0531,-0.0534,0.0484,-0.0183,0.1895,0.0631,-0.001,-0.0515,0.0058,0.0578,-0.0442,-0.0716,0.0876,0.0278,0.0532,-0.0014,-0.0389,0.0244,-0.0061,0.0017,-0.0238,-0.087,-0.0731,-0.067,-0.0246,0.0322,-0.0905,0.0315,0.0298,-0.0347,0.0556,-0.0251,-0.0196,-0.0766,-0.068,0.0303,-0.0538,-0.0076,0.0098,-0.0659,0.047,-0.0486,0.0199,-0.0284,-0.0549,-0.0509,-0.018,-0.0337,-0.0359,0.0994,-0.0188,0.0034,0.0417,0.0014,0.0207,-0.06,-0.0778,-0.0556,0.0469,-0.0564,0.0493,0.0154,-0.0192,-0.0075,0.0709,-0.013,0.0432,-0.0115,-0.0002,0.0167,-0.0546,0.0087,0.0113,-0.0337,-0.2679,0.0385,-0.0108,0.0273,-0.002,-0.0073,0.0702,0.0214,-0.038,0.0213,-0.0162,0.0456,0.0285,-0.0555,-0.0137,0.002,0.0788,-0.0566,0.0243,-0.0616,0.0201,0.0421,0.2259,-0.0136,0.0254,0.0136,0.018,0.024,0.0176,-0.0509,0.0055,-0.0174,0.1138,-0.0243,0.0507,0.0604,-0.0223,0.0689,0.028,-0.0573,0.0347,-0.0043,-0.0387,-0.0731,0.1099,-0.0312,0.0045,-0.0331,-0.009,0.0388,0.0236,0.0476,0.0093,0.0471,-0.0127,-0.0073,-0.0592,-0.027,-0.0168,-0.0184,-0.0329,-0.0717,-0.0152,0.0224,-0.0043]}
{"key":"[Happenstance: Utilizing Semantic Search to Track Russian State Media Narratives about the Russo-Ukrainian War On Reddit] In the buildup to and in the weeks following the Russian Federation's invasion of Ukraine, Russian disinformation outlets output torrents of misleading and outright false information. In this work, we study the coordinated information campaign to understand the most prominent disinformation narratives touted by the Russian government to English-speaking audiences. To do this, we first perform sentence-level topic analysis using the large-language model MPNet on articles published by nine different Russian disinformation websites and the new Russian \"fact-checking\" website waronfakes.com. We show that smaller websites like katehon.com were highly effective at producing topics that were later echoed by other disinformation sites. After analyzing the set of Russian information narratives, we analyze their correspondence with narratives and topics of discussion on the r/Russia and 10 other political subreddits. Using MPNet and a semantic search algorithm, we map these subreddits' comments to the set of topics extracted from our set of disinformation websites, finding that 39.6% of r/Russia comments corresponded to narratives from Russian disinformation websites, compared to 8.86% on r/politics.","layer":0,"vector":[-0.0386,-0.0472,0.0144,0.0084,0.0516,0.0108,0.0197,0.0612,-0.0067,0.0081,-0.0081,-0.0276,0.0548,0.0811,0.0611,-0.0106,0.0243,-0.023,-0.0382,0.0345,0.0536,-0.0422,0.0485,-0.0184,0.0368,0.038,-0.0261,-0.0356,-0.0593,-0.1924,-0.0312,-0.0783,0.0552,0.023,0.0084,-0.0594,-0.0338,0.03,-0.0139,0.016,0.0166,0.006,-0.0115,-0.0435,-0.0234,-0.0479,-0.0319,0.0275,-0.0204,-0.0421,0.012,-0.0381,0.0154,0.007,0.0441,0.0059,0.0613,0.0402,0.0534,0.0431,0.048,0.0278,-0.1771,0.0758,-0.0006,0.0164,-0.0276,-0.0122,0.0107,0.0022,-0.0066,0.0417,0.0064,0.0498,0.0262,-0.0072,-0.037,-0.0038,-0.005,0.0107,0.0273,0.0311,-0.0249,-0.0437,-0.0258,-0.0867,0.011,0.0177,0.0472,-0.0022,-0.0281,-0.014,0.0338,0.0212,-0.0627,0.007,0.011,0.0315,-0.0052,0.1892,-0.0536,-0.0102,0.0353,-0.0699,0.02,-0.0487,-0.027,0.0163,-0.0067,-0.033,-0.0182,-0.0368,0.0331,-0.0351,0.0561,-0.028,0.0823,0.0346,-0.0411,0.0028,-0.0493,0.0094,0.0448,-0.0335,0.0326,-0.0479,0.0457,0.1351,0.0289,0.0264,0.0133,0.0137,-0.062,0.0079,0.0094,-0.0284,-0.0268,0.0035,0.0393,-0.0064,-0.0738,-0.0804,-0.0175,-0.053,-0.1085,0.157,-0.0605,0.0105,-0.0242,0.0364,-0.013,0.0417,-0.013,-0.0858,0.0353,0.0181,0.0491,0.0312,-0.0723,-0.0245,-0.0112,-0.0195,-0.036,0.0937,0.022,-0.0852,-0.0253,0.0365,0.0432,-0.044,0.0346,-0.0113,0.0074,0.0103,0.0258,-0.0002,-0.0529,-0.0005,0.029,0.0462,0.0534,-0.0499,-0.0425,0.0425,0.0247,-0.044,-0.0049,-0.0452,0.0426,0.0435,-0.0459,0.015,-0.0376,-0.0475,-0.0435,-0.018,0.021,-0.0081,0.0259,-0.0016,0.0246,0.002,-0.0902,-0.0056,0.0103,0.0181,-0.0084,-0.0365,0.0218,0.0375,-0.0173,0.0216,-0.0221,-0.0325,0.0127,-0.0622,-0.0192,0.041,-0.0152,0.0723,0.0392,-0.0581,-0.0186,-0.253,-0.0209,-0.0432,-0.0081,0.0629,-0.0925,0.05,-0.027,0.0305,0.1235,0.0629,-0.0161,-0.0043,0.0103,0.0276,0.0527,0.0378,0.044,-0.0198,0.0239,0.006,-0.0296,-0.0161,-0.1108,0.0272,-0.0281,0.2001,0.1069,-0.0176,-0.0111,0.0598,0.0105,-0.0317,-0.1348,0.0554,0.0561,0.0709,0.0128,-0.0083,-0.0357,0.0091,0.0181,-0.0218,-0.059,-0.0238,-0.0337,-0.0387,-0.0478,-0.0502,0.0596,0.0041,0.0071,0.029,0.0483,-0.0125,-0.0496,-0.0628,0.0055,-0.0595,0.0414,-0.0356,-0.033,0.0069,-0.0456,0.0655,-0.0016,-0.0328,0.0268,0.0568,-0.0025,-0.0296,0.1426,-0.0218,-0.0092,0.0359,0.0191,0.0431,-0.0215,-0.0171,-0.0244,0.0903,-0.0088,0.0094,-0.0296,0.0121,-0.0248,0.0354,-0.0092,0.0295,-0.0035,0.0219,0.0267,-0.0408,0.0122,0.0307,-0.0001,-0.3097,0.0622,0.0436,0.029,-0.0101,-0.0218,0.042,0.0175,-0.0526,-0.0231,0.0005,0.0818,-0.0437,-0.0372,-0.0595,0.064,0.0375,-0.0312,0.0314,0.0272,-0.0073,0.0257,0.2073,-0.0249,0.0136,0.0016,0.0244,-0.0041,-0.008,0.0213,0.028,-0.0094,0.0577,-0.0213,0.027,0.0206,-0.0091,0.0198,0.0381,-0.0411,-0.0639,-0.0158,0.005,0.003,0.1085,-0.0111,0.0313,-0.0852,0.0137,-0.0164,-0.0393,-0.0032,-0.0164,0.0148,0.0576,0.0462,-0.0506,-0.0577,0.0309,-0.0273,-0.0195,-0.0863,-0.0006,0.0227,0.0051]}
{"key":"[Unsupervised Feature Learning for low-level Local Image Descriptors] Unsupervised feature learning has shown impressive results for a wide range of input modalities, in particular for object classification tasks in computer vision. Using a large amount of unlabeled data, unsupervised feature learning methods are utilized to construct high-level representations that are discriminative enough for subsequently trained supervised classification algorithms. However, it has never been \\emph{quantitatively} investigated yet how well unsupervised learning methods can find \\emph{low-level representations} for image patches without any additional supervision. In this paper we examine the performance of pure unsupervised methods on a low-level correspondence task, a problem that is central to many Computer Vision applications. We find that a special type of Restricted Boltzmann Machines (RBMs) performs comparably to hand-crafted descriptors. Additionally, a simple binarization scheme produces compact representations that perform better than several state-of-the-art descriptors.","layer":5,"vector":[-0.0288,-0.0353,0.0207,-0.0019,0.0456,0.0041,0.0699,0.0229,-0.0124,-0.0011,0.0456,-0.0638,-0.0073,0.0168,0.0473,-0.0135,-0.0039,0.084,-0.0347,-0.023,0.0026,-0.0183,-0.0262,-0.0207,0.0048,-0.0371,-0.0253,-0.0479,-0.04,-0.2471,0.0416,-0.0196,0.0624,0.0147,0.0263,-0.0095,-0.0441,0.0675,-0.0298,-0.0024,0.024,-0.0138,-0.0015,-0.0638,-0.0496,-0.0639,0.0006,-0.0644,-0.0218,-0.0451,0.0514,-0.0356,0.0213,0.0345,-0.0196,0.0148,0.0656,0.0528,0.0614,0.023,-0.0015,0.0531,-0.1577,0.016,0.0477,0.0619,-0.0614,-0.053,0.0218,0.0198,-0.014,0.0203,0.0201,0.0426,-0.0465,-0.0187,-0.0168,-0.0494,-0.0244,-0.0228,0.0085,-0.0022,-0.0256,-0.0214,-0.0237,-0.0046,0.0302,-0.0931,0.016,-0.0282,-0.0605,-0.038,-0.0277,0.0014,-0.0796,-0.0488,0.0502,0.0161,-0.0462,0.2145,-0.0727,0.0417,0.0383,-0.0293,0.0336,-0.0459,-0.0441,-0.0165,-0.0874,-0.0333,-0.018,-0.0004,-0.0208,-0.0334,0.0265,-0.0022,0.0428,0.0503,0.002,-0.0125,-0.0242,-0.0073,0.037,-0.0382,0.0237,-0.0285,0.0004,0.1385,0.0482,0.0397,0.0434,-0.0126,-0.0222,0.0046,0.0222,0.0382,0.0499,0.0196,-0.0025,-0.0003,-0.0205,-0.0531,0.0194,-0.041,-0.0064,0.1047,-0.0557,0.0852,-0.0367,-0.0372,-0.0192,-0.0392,-0.0702,-0.0488,0.0351,0.0399,0.0344,0.043,-0.0987,0.0087,0.012,-0.0893,-0.0038,0.053,0.0117,-0.0615,-0.0333,-0.0339,-0.0205,-0.0187,0.0324,0.061,0.0054,0.0199,0.0915,0.0516,-0.1013,-0.0108,0.0164,-0.0006,0.0179,-0.0424,-0.0522,0.0317,0.0397,-0.0013,-0.0294,-0.0473,0.0329,0.0691,-0.0262,0.0151,-0.0471,-0.026,-0.0102,0.0159,-0.0016,-0.019,0.0117,-0.0197,0.039,0.033,-0.0521,0.0668,-0.0185,-0.0049,-0.003,-0.0031,0.0514,0.0425,-0.023,0.0219,0.0554,0.0299,-0.007,0.0162,0.0307,-0.0038,-0.0206,-0.0024,0.0305,-0.0662,-0.0567,-0.2365,0.0231,0.0087,-0.0196,0.0485,-0.0644,0.002,0.0226,0.0463,0.0596,0.0711,-0.0532,-0.0196,0.0179,0.021,0.0664,0.0266,0.0632,0.008,0.0259,-0.0167,0.0246,0.0186,-0.0444,0.023,-0.0089,0.2256,-0.0206,0.0369,-0.0365,-0.0051,0.009,-0.0641,-0.0755,0.0455,0.033,0.026,-0.0029,-0.0107,0.0028,-0.0775,0.0292,0.0451,-0.0877,-0.0677,-0.026,-0.0178,0.0114,-0.048,0.0019,-0.004,-0.0288,0.0596,0.0067,-0.0452,-0.0431,-0.0896,0.0033,-0.0186,0.0399,0.0262,-0.1084,0.0273,-0.0818,0.0359,0.0101,-0.049,-0.0483,0.0327,-0.0047,-0.0134,0.0814,0.0012,-0.0102,0.0702,0.0159,0.0846,-0.0219,-0.0729,0.0004,0.1038,0.0127,0.0541,0.0287,0.0708,0.0302,0.0775,-0.0303,-0.0232,0.0056,0.0387,0.0026,-0.02,0.0201,0.0471,0.0001,-0.2921,0.036,0.0554,0.0543,-0.004,-0.019,0.0904,0.0071,-0.0279,0.0106,0.0128,-0.0094,0.0933,0.0144,0.0221,0.0434,0.0302,-0.0614,0.0636,-0.0125,0.0165,0.0454,0.1891,-0.0549,-0.0025,0.0154,-0.0292,0.0264,0.0025,-0.0141,0.0297,0.0045,0.0979,-0.0384,0.0214,0.0969,-0.0108,0.0126,0.0294,-0.0282,0.0033,0.0127,-0.0456,-0.0578,0.0768,-0.0091,-0.0066,-0.0121,-0.0273,-0.0148,-0.0337,0.0094,-0.0007,0.0126,0.0772,0.0198,-0.041,-0.0229,-0.0149,-0.0225,0.0502,-0.0501,-0.0381,-0.0168,-0.0379]}
{"key":"[Temporal Convolution Domain Adaptation Learning for Crops Growth Prediction] Existing Deep Neural Nets on crops growth prediction mostly rely on availability of a large amount of data. In practice, it is difficult to collect enough high-quality data to utilize the full potential of these deep learning models. In this paper, we construct an innovative network architecture based on domain adaptation learning to predict crops growth curves with limited available crop data. This network architecture overcomes the challenge of data availability by incorporating generated data from the developed crops simulation model. We are the first to use the temporal convolution filters as the backbone to construct a domain adaptation network architecture which is suitable for deep learning regression models with very limited training data of the target domain. We conduct experiments to test the performance of the network and compare our proposed architecture with other state-of-the-art methods, including a recent LSTM-based domain adaptation network architecture. The results show that the proposed temporal convolution-based network architecture outperforms all benchmarks not only in accuracy but also in model size and convergence rate.","layer":2,"vector":[-0.0465,-0.0107,0.0128,-0.0162,0.0808,0.0083,-0.0126,0.0164,0.0075,-0.0044,0.021,-0.0751,0.0439,0.0459,0.0253,0.0051,-0.0096,0.0529,-0.0308,0.002,0.0576,-0.0012,0.0146,-0.0383,0.0268,-0.0048,0.0028,-0.02,-0.0584,-0.2329,0.0288,-0.0251,0.0354,-0.0225,0.0039,-0.0363,-0.0568,0.0441,-0.0122,0.0414,0.0131,0.0211,-0.0075,-0.0739,-0.0229,-0.0743,-0.0192,-0.0166,-0.0063,-0.0314,0.0432,-0.0448,0.0433,0.026,0.0219,0.029,0.0605,0.0682,0.0687,0.0165,0.014,0.0362,-0.1784,0.0512,0.0212,0.0362,-0.0374,-0.0074,-0.0018,0.0317,-0.0025,0.0673,0.0138,0.0337,0.0168,0.0323,0.0031,-0.0146,-0.0044,0.0295,0.0403,0.0114,-0.0813,-0.0461,-0.0062,-0.0831,0.0039,-0.0202,0.0354,0.0206,-0.0667,-0.0622,-0.0632,-0.0019,-0.0925,0.0128,0.0376,-0.0209,-0.0771,0.2224,-0.0409,0.0279,0.0183,-0.0223,0.0022,0.0078,-0.0345,0.0086,0.0151,0.0463,-0.0348,-0.0261,0.0083,0.0209,0.0286,-0.0345,0.059,0.0064,0.003,0.006,-0.0261,0.0565,0.0456,-0.0548,0.037,-0.0277,0.0094,0.1268,0.0264,0.0383,0.071,-0.0373,-0.0806,-0.0469,-0.005,0.0291,0.0577,-0.0185,-0.02,-0.0354,-0.0521,-0.0542,0.0234,-0.0532,-0.0449,0.1081,0.0282,0.0365,-0.0315,-0.014,-0.048,0.0524,0.0066,-0.0208,0.0583,0.05,0.0177,0.0173,-0.0028,0.0223,-0.0334,-0.0273,-0.0508,0.0976,0.0266,-0.0986,-0.0179,0.0197,0.0392,-0.0136,0.0467,-0.0055,-0.0344,-0.018,0.096,0.0185,-0.0547,-0.0213,0.0116,-0.0008,0.0511,-0.0572,-0.0122,0.0763,0.0461,-0.0311,0.0075,-0.0533,-0.0154,0.04,-0.0502,0.0303,-0.0703,0.0551,-0.0405,-0.0507,-0.0342,0.0149,0.0161,-0.0497,-0.0177,-0.0057,-0.0195,0.0023,-0.0298,-0.0138,0.0,0.0093,0.0316,0.0257,-0.0466,0.0338,0.0635,-0.0496,-0.0711,0.0439,0.0243,0.0258,0.002,0.0568,0.0496,-0.0184,-0.0438,-0.235,0.0164,0.0084,-0.0293,0.0681,-0.0347,0.0125,-0.0081,0.0738,0.0552,0.0481,-0.0239,0.0038,0.0043,-0.0063,-0.0039,0.0705,0.0017,-0.0007,-0.0289,0.0009,-0.0229,0.0354,-0.1256,0.0401,0.0104,0.1743,-0.0163,0.0614,-0.0421,0.0115,0.0262,-0.0169,-0.107,0.0149,0.0027,0.0554,-0.0136,-0.0772,-0.05,-0.0208,0.0272,0.0097,-0.1131,-0.0396,-0.0135,0.0277,0.068,-0.0511,0.009,-0.0183,-0.0689,0.095,0.0051,-0.0046,-0.0394,-0.1031,0.0715,0.0024,0.0014,0.0021,-0.0337,-0.0023,-0.0741,0.0388,0.0005,-0.0276,-0.0701,0.0048,-0.0143,0.0091,0.0558,-0.0077,0.0219,0.051,-0.0347,0.0044,-0.0152,-0.0661,-0.0216,0.0783,-0.0186,0.0234,0.0448,0.0536,0.0505,0.0763,-0.0143,0.0262,-0.0101,0.0145,-0.0109,-0.0761,-0.016,0.0149,-0.0095,-0.2892,0.0636,0.0192,0.038,0.0137,0.0287,0.0347,0.0419,-0.0243,-0.0132,0.0042,0.0034,0.0623,-0.0357,-0.001,0.0283,0.0417,-0.0326,0.0128,-0.0602,0.0067,0.0353,0.206,-0.0701,0.0177,0.0072,-0.0362,-0.0606,0.0324,-0.0075,0.0334,0.0487,0.0984,-0.061,0.0249,0.1157,-0.0043,0.0194,0.0185,0.0288,-0.0045,-0.0035,-0.0219,-0.0011,0.0695,-0.0138,0.0301,-0.0444,-0.0408,0.0679,-0.0341,-0.0106,-0.0028,0.0208,0.0247,-0.0077,-0.0298,-0.0442,-0.0536,-0.0203,0.053,-0.0902,-0.0554,-0.0099,-0.0244]}
{"key":"[Learning to Slide Unknown Objects with Differentiable Physics Simulations] We propose a new technique for pushing an unknown object from an initial configuration to a goal configuration with stability constraints. The proposed method leverages recent progress in differentiable physics models to learn unknown mechanical properties of pushed objects, such as their distributions of mass and coefficients of friction. The proposed learning technique computes the gradient of the distance between predicted poses of objects and their actual observed poses and utilizes that gradient to search for values of the mechanical properties that reduce the reality gap. The proposed approach is also utilized to optimize a policy to efficiently push an object toward the desired goal configuration. Experiments with real objects using a real robot to gather data show that the proposed approach can identify the mechanical properties of heterogeneous objects from a small number of pushing actions.","layer":6,"vector":[-0.0575,-0.0106,0.0548,-0.029,0.012,0.0514,0.0224,0.0141,-0.0088,0.0177,-0.0085,-0.0497,-0.0012,0.067,0.0082,-0.0164,-0.0126,0.0961,-0.0448,0.0379,0.0432,-0.0102,0.029,-0.0226,-0.0239,0.0345,-0.0442,-0.0346,-0.0237,-0.2353,-0.0105,-0.0713,-0.0023,-0.0491,0.0375,0.0157,-0.0484,0.0296,-0.0335,0.0419,0.0391,-0.0169,-0.0401,-0.082,0.0098,-0.013,0.016,-0.0159,-0.0051,-0.0305,-0.0045,-0.071,0.0401,0.003,0.042,0.0385,0.0474,0.0042,0.0415,0.0185,0.039,-0.0159,-0.1371,0.0547,0.0788,0.0221,-0.0256,-0.0056,0.0639,0.0893,-0.0135,0.0376,0.0527,0.0556,0.0107,-0.0263,0.0074,-0.038,-0.0196,-0.0013,-0.0144,-0.0476,-0.0572,-0.0152,-0.0595,0.0007,0.0191,-0.0525,0.0365,0.0116,-0.0728,0.0273,-0.0306,0.0516,-0.0409,-0.0004,0.0063,-0.001,-0.0405,0.2146,-0.0274,0.0311,0.053,-0.011,0.0124,-0.0263,-0.04,-0.0305,-0.0282,0.0324,-0.0198,0.0043,-0.0039,-0.0129,0.0024,0.0209,0.0226,0.0091,-0.0272,-0.0252,-0.0073,0.0248,0.0259,-0.005,-0.0076,-0.1187,-0.0121,0.1379,0.0322,0.0161,0.0462,-0.0535,-0.0454,-0.0267,0.0398,0.0613,0.0083,0.006,0.0456,0.027,-0.0532,-0.0311,0.0266,-0.1244,-0.0683,0.1198,-0.0517,0.0561,-0.0283,-0.0229,0.0088,0.0282,-0.0349,0.0003,0.028,-0.005,0.0272,0.0407,-0.0705,0.0043,-0.0163,-0.0237,-0.0487,0.0645,0.0041,-0.0777,-0.0151,-0.0059,0.0329,-0.0138,0.0453,0.0857,-0.065,0.0301,0.1213,0.0202,-0.0854,0.0073,0.0183,0.0352,0.0209,-0.0863,-0.0293,0.0346,0.0265,-0.0306,-0.0191,-0.0605,0.012,0.0517,0.0135,0.0066,-0.0336,-0.0211,0.0054,-0.0373,-0.0127,-0.0038,-0.0201,-0.0268,-0.0282,0.0155,-0.0516,0.0384,-0.0117,-0.0064,-0.0013,-0.0207,0.0272,0.003,-0.0621,-0.0282,0.0547,-0.0845,-0.037,0.0155,0.0218,0.0336,-0.0249,0.0329,0.0173,-0.004,-0.0047,-0.2258,0.0289,-0.0414,0.0116,0.0805,-0.0197,0.0381,-0.0384,0.0521,0.0114,0.0836,-0.0701,-0.0282,0.0197,-0.0143,0.0487,0.026,-0.0041,-0.025,0.0146,-0.0221,-0.0032,-0.0481,-0.0607,0.0257,-0.0135,0.216,0.0177,0.0416,-0.065,0.0073,0.0404,-0.0299,-0.0653,0.066,0.0143,0.0595,-0.0351,0.0048,0.0037,-0.0296,0.0487,0.0015,-0.078,0.0071,-0.0596,-0.0359,0.0683,-0.0237,0.0246,0.0431,-0.0368,0.0279,-0.0128,-0.0646,-0.0245,-0.0436,0.0208,-0.0293,0.0233,-0.0532,-0.0567,0.0281,-0.0366,0.0635,-0.0341,-0.0037,-0.0566,0.0492,-0.0462,0.0144,0.0673,0.0931,0.0375,0.0371,-0.0066,0.0331,0.0109,-0.0195,-0.007,0.0568,0.0063,0.0075,0.0279,0.0459,-0.0383,0.0887,-0.0892,0.0235,-0.0057,0.0383,0.0219,-0.0461,0.0189,0.0608,-0.002,-0.3169,0.0549,0.0377,0.0608,-0.0246,-0.002,0.066,-0.0122,-0.0375,0.0075,-0.0524,0.0487,0.0215,0.0397,0.0258,0.0095,0.0545,-0.0227,0.0435,-0.0916,-0.0094,0.0442,0.218,-0.0184,0.0527,0.0289,-0.0566,-0.0486,0.0336,-0.0087,0.0051,0.0078,0.0154,-0.068,0.032,0.0929,-0.0003,0.0455,-0.0184,-0.0135,-0.0237,0.0127,-0.0002,0.0052,0.1012,0.0026,-0.0116,-0.0225,-0.0046,0.0094,-0.0258,-0.0079,-0.0197,0.0206,0.0741,0.0216,-0.0118,-0.0235,-0.0574,-0.022,0.0254,-0.0742,-0.0189,-0.0622,0.009]}
{"key":"[Being Negative but Constructively: Lessons Learnt from Creating Better Visual Question Answering Datasets] Visual question answering (Visual QA) has attracted a lot of attention lately, seen essentially as a form of (visual) Turing test that artificial intelligence should strive to achieve. In this paper, we study a crucial component of this task: how can we design good datasets for the task? We focus on the design of multiple-choice based datasets where the learner has to select the right answer from a set of candidate ones including the target (\\ie the correct one) and the decoys (\\ie the incorrect ones). Through careful analysis of the results attained by state-of-the-art learning models and human annotators on existing datasets, we show that the design of the decoy answers has a significant impact on how and what the learning models learn from the datasets. In particular, the resulting learner can ignore the visual information, the question, or both while still doing well on the task. Inspired by this, we propose automatic procedures to remedy such design deficiencies. We apply the procedures to re-construct decoy answers for two popular Visual QA datasets as well as to create a new Visual QA dataset from the Visual Genome project, resulting in the largest dataset for this task. Extensive empirical studies show that the design deficiencies have been alleviated in the remedied datasets and the performance on them is likely a more faithful indicator of the difference among learning models. The datasets are released and publicly available via http://www.teds.usc.edu/website_vqa/.","layer":2,"vector":[-0.0269,0.0129,-0.0025,0.0274,0.0278,0.0291,0.0153,-0.0077,-0.0352,-0.014,0.0036,-0.0899,0.0734,0.0811,0.0208,0.0139,0.0053,-0.0028,-0.0382,0.0191,0.0388,-0.0401,-0.0412,-0.0816,-0.0006,0.0241,-0.0284,-0.0579,-0.0168,-0.2382,0.0345,-0.0338,0.0607,-0.0121,-0.0119,-0.026,-0.0102,0.0391,-0.0189,0.0304,0.0326,-0.0296,-0.0389,-0.0698,-0.026,-0.0271,-0.0146,-0.0231,-0.0263,-0.0531,0.0064,-0.0509,-0.0161,0.0039,0.0005,0.0236,0.0887,0.0333,0.0081,0.0257,0.0332,0.0333,-0.1378,0.1008,0.0548,0.0064,-0.0487,0.0303,-0.0114,0.062,-0.0262,0.016,0.0265,0.0211,0.0186,-0.0018,-0.0203,-0.0323,0.0377,-0.0405,0.0013,-0.0155,-0.0228,0.0068,0.0075,-0.0226,-0.0428,0.0203,0.0428,0.0156,-0.0196,-0.0249,-0.0382,0.0134,-0.0437,-0.0301,0.0114,0.0159,-0.0529,0.2387,-0.0588,0.0029,0.0503,-0.0773,0.0262,-0.0342,0.0019,-0.0668,0.0001,-0.005,-0.0032,-0.021,0.0394,-0.0389,0.0086,0.0226,0.0281,0.0362,-0.0383,0.0129,-0.0042,0.0145,0.0462,-0.0361,0.0298,-0.0537,0.0277,0.1248,0.0086,-0.0185,0.096,-0.0418,-0.0413,-0.0358,-0.0184,0.0233,0.0433,0.0218,-0.0056,-0.0194,-0.0351,-0.0413,-0.0187,-0.0546,-0.0617,0.1092,-0.0498,0.0542,-0.0339,-0.0426,0.0085,0.0166,-0.0342,-0.0403,0.0304,0.0625,0.0292,0.0151,-0.0333,0.0029,0.0025,-0.0795,-0.0406,0.075,-0.0012,-0.078,-0.0639,-0.0148,0.0342,-0.0064,0.0534,0.0373,-0.0071,0.0281,0.0528,0.0466,-0.0772,0.0317,0.0264,0.0368,0.0131,-0.0983,0.0112,0.0612,0.0419,-0.0453,0.0316,-0.0525,0.0232,0.0335,0.0412,0.0121,-0.0427,0.0558,-0.0486,-0.008,0.0179,-0.0288,-0.0011,-0.0236,-0.0131,-0.0191,-0.0576,0.0262,-0.0342,-0.0061,-0.0037,0.0126,0.1146,0.0466,-0.0541,0.0143,0.0049,-0.0227,-0.0202,-0.0359,0.033,-0.0099,0.007,0.0266,0.0209,-0.0585,-0.048,-0.2481,-0.0275,0.0175,-0.0268,0.0164,-0.0482,0.0051,0.009,0.0688,0.0792,0.0339,0.0041,-0.0407,-0.0224,-0.0087,0.0384,0.0374,0.0257,-0.0353,-0.0637,-0.0228,0.0139,0.0503,-0.1055,0.0406,0.0104,0.2547,0.0648,0.0236,-0.019,0.0758,0.0524,-0.0478,-0.0929,0.0861,0.022,0.0429,-0.0133,-0.0363,-0.0017,0.0074,-0.0104,-0.0256,-0.0857,-0.0367,-0.0357,-0.0097,0.0293,-0.0404,0.0655,0.0202,-0.0391,0.0401,0.0262,0.0217,-0.0022,-0.111,0.0095,-0.0443,0.0607,0.0236,-0.0108,-0.0217,-0.0623,0.0393,0.0218,-0.0377,-0.0145,0.0422,-0.0471,-0.0165,0.0998,0.0005,-0.0047,0.0148,0.0708,0.0248,-0.0457,-0.0287,-0.0521,0.036,0.014,0.0069,-0.0378,0.055,0.0243,0.0244,0.0037,0.0535,-0.0201,0.0073,0.0176,-0.0259,-0.0434,0.0266,-0.0014,-0.2886,0.0382,0.0447,0.0086,-0.017,0.0675,0.0569,0.0049,-0.0291,-0.0172,-0.0188,0.0391,0.0765,-0.0283,0.0034,0.0029,0.1064,-0.0436,0.0739,-0.0208,0.0202,0.0225,0.2008,-0.0244,0.0427,0.0258,-0.0133,-0.0252,0.0361,-0.0283,-0.0112,0.0254,0.0936,-0.0255,0.0296,0.0542,-0.054,0.0135,0.0166,0.0199,-0.0138,-0.0114,-0.0165,-0.0248,0.1056,0.0405,-0.0081,-0.063,-0.0072,-0.0275,-0.0177,-0.0411,-0.0191,-0.0086,0.0448,-0.024,-0.0143,-0.0345,-0.0289,-0.0306,0.0234,-0.0817,0.0225,0.0463,-0.0184]}
{"key":"[A Thermodynamical Approach for Probability Estimation] The issue of discrete probability estimation for samples of small size is addressed in this study. The maximum likelihood method often suffers over-fitting when insufficient data is available. Although the Bayesian approach can avoid over-fitting by using prior distributions, it still has problems with objective analysis. In response to these drawbacks, a new theoretical framework based on thermodynamics, where energy and temperature are introduced, was developed. Entropy and likelihood are placed at the center of this method. The key principle of inference for probability mass functions is the minimum free energy, which is shown to unify the two principles of maximum likelihood and maximum entropy. Our method can robustly estimate probability functions from small size data.","layer":0,"vector":[-0.0341,0.0302,0.0906,-0.0682,0.0593,0.046,0.0685,-0.0063,0.0362,-0.0236,0.0332,-0.0583,0.0361,0.0021,0.0058,0.0594,0.0157,0.0241,-0.029,0.0165,0.0586,-0.0669,-0.0113,-0.069,0.0282,0.0298,-0.0441,0.0165,-0.0571,-0.2302,0.0314,-0.0443,0.0946,-0.0808,0.0027,-0.0259,-0.0397,0.0247,-0.0264,0.0708,0.0439,0.0134,-0.0465,-0.0572,-0.0486,-0.0416,-0.0278,-0.0101,-0.0055,-0.0088,0.0098,-0.0025,0.0136,0.0387,0.0435,0.0248,0.0647,-0.0097,0.049,0.054,0.0032,0.0465,-0.2129,0.0532,0.0609,-0.001,-0.0512,-0.0042,-0.0093,0.0088,-0.0171,0.0852,0.0242,0.0579,-0.0047,-0.0444,-0.0394,-0.0318,-0.0052,0.0131,-0.0231,-0.0441,-0.002,0.0274,-0.0239,-0.0685,0.0408,-0.0235,0.0497,0.0005,-0.0204,0.0248,-0.0719,0.0298,-0.0847,-0.0163,0.0501,0.024,0.0205,0.184,-0.0018,0.0411,0.0291,-0.0317,0.0252,-0.0215,-0.0354,-0.0298,-0.0115,0.0002,0.0309,0.0086,-0.0277,-0.0432,0.0306,-0.0076,0.0639,0.0166,-0.0036,-0.0078,-0.0822,0.011,0.0537,-0.0067,0.0106,-0.0584,-0.004,0.1515,0.0103,0.0678,0.0428,-0.0541,-0.0652,-0.0301,0.0075,-0.0144,0.0428,0.0152,0.0502,-0.0022,-0.0237,-0.0988,0.0112,-0.1169,-0.0499,0.1449,-0.0217,0.0326,-0.0474,-0.0452,0.0132,0.0281,-0.0452,-0.0196,0.0796,0.0161,-0.0048,0.0467,-0.048,0.0324,-0.0281,-0.0162,0.0142,0.1076,-0.0229,-0.0261,-0.0234,0.0072,0.0237,-0.0255,0.05,0.0265,-0.0652,0.0197,0.0577,0.0034,-0.0624,-0.0296,0.0293,0.0453,0.0109,-0.0353,-0.0385,0.0533,0.038,-0.0413,-0.0186,-0.0331,0.027,0.1054,-0.026,-0.0002,0.0366,-0.0172,-0.0343,-0.0148,-0.0461,-0.0097,-0.0099,-0.0515,0.0317,0.0125,-0.0905,0.0508,0.0188,0.0485,0.0011,0.0163,0.0384,-0.0038,0.0037,-0.0199,0.0259,-0.0079,-0.0287,0.0241,0.0382,0.0031,0.0286,0.0131,0.0677,-0.0719,-0.0392,-0.2379,-0.0023,-0.0118,-0.0226,0.0853,-0.0525,0.0468,-0.008,0.0402,0.037,0.0736,0.0167,-0.0357,0.0334,-0.0457,0.0005,0.0419,-0.033,-0.0269,0.0246,0.0174,0.0108,-0.0604,-0.0644,0.0506,-0.0112,0.1698,0.0067,0.023,-0.0118,0.0527,0.0144,-0.0255,-0.0661,0.0484,0.0377,0.0205,-0.0084,-0.0438,-0.0062,-0.0104,0.0232,-0.0126,-0.0685,-0.0716,-0.0668,-0.0084,0.0179,-0.0601,0.0191,0.031,0.0205,0.0606,-0.0375,0.0122,-0.0518,-0.0418,0.0244,-0.0498,0.0032,0.0167,-0.0374,0.0315,-0.0439,0.0271,-0.0502,-0.0107,-0.0652,0.0169,-0.0459,-0.0035,0.1105,0.0296,-0.0037,0.0551,0.0196,0.0428,-0.0542,-0.0438,-0.0263,0.0643,-0.0328,0.0186,0.0423,0.0218,0.0321,0.0539,-0.0079,-0.0182,-0.0194,0.0085,0.0002,0.0122,0.0085,0.0174,0.0328,-0.2736,0.0428,-0.0185,0.0429,-0.0455,-0.0192,0.0241,0.0131,-0.0373,-0.0004,0.0026,-0.0133,0.0329,-0.0282,0.0012,-0.0241,-0.0075,-0.0537,0.0074,-0.0672,0.0037,0.0218,0.2425,-0.0151,0.001,0.0374,-0.0158,0.0812,0.0054,-0.044,0.0254,-0.0131,0.0733,-0.0654,0.0201,0.0925,-0.0254,0.0256,0.0077,-0.0265,0.0212,-0.031,-0.0393,-0.0292,0.1341,-0.0326,-0.0215,-0.0455,-0.0221,0.0477,-0.0417,0.0154,-0.0114,-0.0235,0.0543,0.0166,-0.0455,-0.0309,0.0072,-0.0626,0.023,-0.0313,-0.0394,0.0318,-0.0119]}
{"key":"[Deep One-bit Compressive Autoencoding] Parameterized mathematical models play a central role in understanding and design of complex information systems. However, they often cannot take into account the intricate interactions innate to such systems. On the contrary, purely data-driven approaches do not need explicit mathematical models for data generation and have a wider applicability at the cost of interpretability. In this paper, we consider the design of a one-bit compressive autoencoder, and propose a novel hybrid model-based and data-driven methodology that allows us to not only design the sensing matrix for one-bit data acquisition, but also allows for learning the latent-parameters of an iterative optimization algorithm specifically designed for the problem of one-bit sparse signal recovery. Our results demonstrate a significant improvement compared to state-of-the-art model-based algorithms.","layer":0,"vector":[-0.0447,0.0146,0.0365,-0.0313,0.0104,0.0382,0.0141,0.0091,0.0284,-0.0484,0.022,-0.0273,0.0796,0.0505,0.0558,0.0228,0.0175,0.0634,-0.0223,-0.038,0.0495,-0.0659,0.0123,-0.0328,0.0317,-0.0258,0.0002,-0.0552,-0.0496,-0.2723,-0.0238,-0.0204,0.1017,-0.0354,0.0735,-0.0411,-0.0466,0.0123,-0.0322,0.0716,-0.0012,0.0361,0.0013,0.0166,0.0034,-0.056,0.0019,-0.0379,-0.0267,-0.053,0.0175,0.0371,-0.0058,0.0332,0.0275,0.0051,0.0674,0.0704,0.029,0.0138,0.0717,0.0583,-0.1629,0.0735,0.0629,0.0377,-0.0471,-0.0428,0.039,0.0525,-0.0258,0.0057,0.0086,0.0396,-0.0178,-0.0059,0.0076,0.0117,-0.0093,0.0186,-0.0206,-0.0526,-0.0503,0.0182,-0.0531,-0.1171,0.0369,-0.0599,0.0595,-0.0192,-0.0853,-0.0182,-0.0344,0.0366,-0.0463,-0.0015,0.0001,0.0699,-0.0606,0.1837,-0.0378,0.01,0.0484,-0.0354,0.0114,-0.0567,-0.0235,-0.0591,-0.0139,0.0071,0.0055,-0.0208,0.0527,-0.0809,0.055,-0.0223,-0.0008,0.0429,-0.0032,0.0213,-0.0281,-0.0257,0.0677,-0.0418,0.0597,-0.057,0.0142,0.1152,0.0541,0.0384,0.0403,-0.0188,-0.0177,-0.0432,0.015,0.0253,0.0244,0.0064,-0.0142,0.0037,-0.0361,-0.0248,0.0509,-0.0811,-0.1058,0.1104,-0.039,-0.0016,-0.0462,-0.0531,-0.0283,0.0204,-0.0174,-0.0444,0.0214,0.0544,0.0267,0.004,-0.0573,0.0435,-0.0047,-0.0654,0.0008,0.0843,0.0257,-0.0784,-0.0394,-0.0057,0.0253,-0.0254,0.0487,0.0289,-0.057,0.0197,0.0708,0.0257,-0.0804,-0.0143,-0.0217,0.0514,0.0039,-0.0232,0.0229,0.042,0.0526,-0.027,0.0057,-0.0287,-0.0085,-0.0089,0.0175,0.0,-0.0293,0.0279,-0.0112,-0.0049,-0.0383,-0.009,-0.02,-0.0154,0.0271,-0.0051,-0.0406,0.0549,0.0214,0.0063,0.0055,-0.0021,0.0317,0.0159,-0.0209,-0.037,0.058,-0.0395,-0.0016,0.0001,0.02,0.033,0.0108,0.0487,-0.0165,-0.0886,-0.0461,-0.1972,-0.0267,-0.0455,-0.0347,0.05,-0.0583,0.0436,-0.0315,0.0527,0.0891,0.0357,-0.0296,-0.0417,0.0481,-0.0053,0.0001,0.0459,0.0599,0.0051,0.0217,-0.0172,-0.0058,-0.0336,-0.0688,0.0133,-0.0046,0.1892,0.0028,0.0553,0.0077,0.007,0.0393,0.0,-0.1116,0.0248,0.0409,0.0906,0.0334,-0.0627,-0.0361,-0.0365,0.0025,-0.0111,-0.0979,-0.0218,-0.0577,-0.0495,0.0035,-0.0434,0.0482,0.0213,-0.0264,0.0355,-0.0357,0.019,-0.0703,-0.093,0.0192,-0.0268,0.0242,-0.0026,-0.0889,0.0082,-0.0794,0.0428,0.0418,-0.0286,-0.0309,0.011,-0.0366,-0.0228,0.0894,-0.0129,0.0243,0.0501,0.0321,0.0249,-0.042,-0.0587,-0.0386,0.034,-0.0367,0.0487,0.0215,0.0361,0.0515,0.1021,0.0109,0.0165,-0.0278,-0.039,0.0033,-0.0445,-0.0281,0.0318,-0.0118,-0.2977,0.0032,0.0162,0.0037,0.0028,0.0133,0.0765,0.0077,-0.0643,0.0272,-0.0468,0.0063,0.0213,-0.0106,0.0538,0.0482,0.068,-0.05,0.0156,-0.0944,0.0121,0.0526,0.1844,-0.0263,0.0329,0.0557,0.0239,0.0261,0.0121,-0.033,-0.019,0.0179,0.0679,-0.0269,0.0386,0.0854,-0.0626,0.0966,0.0142,0.0115,0.0413,0.0203,-0.0345,-0.0242,0.0633,0.0101,-0.0383,-0.0134,-0.0036,0.0121,-0.0188,0.0129,0.0166,-0.0112,0.0512,0.0757,-0.0349,-0.0068,-0.0101,-0.003,0.0148,-0.0417,-0.0403,-0.0086,-0.0395]}
{"key":"[Differentiable Neural Architecture Learning for Efficient Neural Network Design] Automated neural network design has received ever-increasing attention with the evolution of deep convolutional neural networks (CNNs), especially involving their deployment on embedded and mobile platforms. One of the biggest problems that neural architecture search (NAS) confronts is that a large number of candidate neural architectures are required to train, using, for instance, reinforcement learning and evolutionary optimisation algorithms, at a vast computation cost. Even recent differentiable neural architecture search (DNAS) samples a small number of candidate neural architectures based on the probability distribution of learned architecture parameters to select the final neural architecture. To address this computational complexity issue, we introduce a novel \\emph{architecture parameterisation} based on scaled sigmoid function, and propose a general \\emph{Differentiable Neural Architecture Learning} (DNAL) method to optimize the neural architecture without the need to evaluate candidate neural networks. Specifically, for stochastic supernets as well as conventional CNNs, we build a new channel-wise module layer with the architecture components controlled by a scaled sigmoid function. We train these neural network models from scratch. The network optimization is decoupled into the weight optimization and the architecture optimization. We address the non-convex optimization problem of neural architecture by the continuous scaled sigmoid method with convergence guarantees. Extensive experiments demonstrate our DNAL method delivers superior performance in terms of neural architecture search cost. The optimal networks learned by DNAL surpass those produced by the state-of-the-art methods on the benchmark CIFAR-10 and ImageNet-1K dataset in accuracy, model size and computational complexity.","layer":3,"vector":[-0.0251,-0.0037,0.0392,-0.0322,0.029,0.0377,0.0157,0.0457,0.0617,-0.0085,0.008,-0.0229,0.0539,0.0674,0.0142,0.0182,-0.031,0.025,-0.0209,0.0336,0.0274,-0.0193,0.018,-0.0314,0.0021,-0.023,-0.0018,-0.0178,-0.053,-0.2484,0.0388,-0.0308,0.0553,-0.0517,-0.0309,-0.0502,-0.0557,0.0446,-0.0021,0.0668,0.0178,0.0203,-0.0004,-0.0797,-0.0058,-0.0089,-0.0246,0.0008,0.0052,-0.0463,0.0204,-0.0515,0.0112,0.023,0.0096,0.0282,0.0366,0.018,0.0532,0.0084,0.0001,0.0661,-0.1811,0.0552,0.0338,-0.001,-0.0514,-0.0225,0.0188,0.0738,-0.0164,0.0497,0.0057,0.0168,0.0196,0.0266,-0.0185,-0.0577,-0.0111,0.0107,0.0371,-0.0343,-0.0397,-0.0244,-0.0185,-0.0422,-0.0178,0.0024,0.0317,0.0084,-0.0514,0.0188,-0.0613,-0.0025,-0.0571,0.0218,0.0011,-0.0014,-0.1047,0.2274,-0.0291,0.0435,0.043,-0.0151,0.038,-0.0214,-0.0333,-0.0,-0.0396,-0.0166,-0.0103,-0.026,0.0134,-0.0121,0.0304,0.0193,-0.0249,0.0001,-0.0115,-0.0263,-0.0783,-0.0312,0.0457,-0.0135,0.0311,-0.0289,0.0114,0.1266,0.0037,0.0431,0.0353,-0.0245,-0.0217,-0.0296,0.0651,0.0357,-0.044,-0.0365,-0.0168,-0.04,-0.0569,-0.0338,0.0485,-0.1033,-0.0547,0.108,-0.0233,0.0329,-0.068,-0.0128,-0.0063,0.0466,-0.0046,0.0048,0.0341,0.0414,0.0127,0.0682,-0.0547,-0.0149,-0.0306,-0.0639,-0.0148,0.14,0.0194,-0.1134,-0.0752,-0.0212,0.0392,0.0038,0.0062,0.0339,-0.0511,0.015,0.071,-0.0003,-0.1039,0.0233,-0.0019,0.0148,0.0027,-0.039,-0.0198,0.0003,0.0516,-0.0611,0.0314,-0.081,0.0117,0.0183,-0.0289,0.0521,-0.0237,-0.0065,-0.0493,-0.0478,0.0183,-0.0088,-0.0039,-0.0147,0.0439,0.0023,-0.0483,0.0045,-0.0259,-0.0278,-0.0181,0.0207,0.0418,0.0413,-0.0251,0.0042,0.0786,-0.0257,0.0011,-0.0076,0.0023,0.028,0.0192,0.05,0.0194,-0.053,-0.0657,-0.2079,-0.0086,-0.0569,-0.0368,0.0723,-0.03,0.0412,0.0288,0.0314,0.0668,0.0741,0.0093,-0.0331,0.0067,-0.0094,0.0754,0.0751,0.0166,-0.0177,-0.0254,0.047,0.0207,0.0165,-0.0851,0.0465,0.028,0.2297,-0.0073,0.0796,-0.0008,-0.0016,0.0274,-0.0081,-0.0781,0.0605,0.0268,0.0809,-0.0012,-0.0586,-0.0293,-0.0259,0.0804,0.0053,-0.1391,-0.0762,-0.0384,-0.0279,0.0475,-0.062,0.0096,0.0099,-0.0379,0.0537,0.0045,0.0039,-0.0333,-0.0786,0.0301,-0.072,0.0309,0.0365,-0.0582,0.0037,-0.0378,0.0317,-0.0167,0.0075,-0.0256,0.0358,-0.0457,-0.0048,0.0797,0.0049,0.0296,0.0538,-0.001,0.0652,-0.0006,0.0086,0.0123,0.0381,-0.0222,0.0509,-0.0188,0.0258,0.0178,0.0752,-0.0122,0.0398,-0.0002,-0.0075,-0.0091,-0.0282,0.0332,0.0302,-0.0097,-0.2911,0.0366,0.0426,0.0303,-0.0082,-0.0058,0.0372,-0.0017,-0.0523,0.0287,0.003,0.0096,0.0305,0.0265,0.0298,-0.003,0.0977,-0.0388,0.0448,-0.0373,-0.0107,0.0314,0.2029,-0.0774,-0.0058,-0.0032,-0.0376,0.0075,0.0114,-0.033,0.0343,0.0091,0.084,-0.0653,0.021,0.08,-0.0213,0.0047,0.0223,0.012,-0.003,0.0071,-0.0565,-0.0058,0.1001,-0.0167,-0.0342,-0.0167,-0.0164,0.0471,-0.0106,0.0246,-0.0088,-0.0393,0.0544,0.0129,-0.0585,-0.0611,-0.0663,-0.0215,0.0444,-0.0352,0.0083,-0.0156,-0.0297]}
{"key":"[Construction de variables \\`a l'aide de classifieurs comme aide \\`a la r\\'egression] This paper proposes a method for the automatic creation of variables (in the case of regression) that complement the information contained in the initial input vector. The method works as a pre-processing step in which the continuous values of the variable to be regressed are discretized into a set of intervals which are then used to define value thresholds. Then classifiers are trained to predict whether the value to be regressed is less than or equal to each of these thresholds. The different outputs of the classifiers are then concatenated in the form of an additional vector of variables that enriches the initial vector of the regression problem. The implemented system can thus be considered as a generic pre-processing tool. We tested the proposed enrichment method with 5 types of regressors and evaluated it in 33 regression datasets. Our experimental results confirm the interest of the approach.","layer":0,"vector":[-0.0415,0.0274,0.0279,-0.0244,0.0596,0.0143,0.0348,0.0537,0.06,-0.0365,0.0288,-0.0336,0.0078,0.027,0.0629,-0.0204,0.0056,0.091,-0.0622,-0.0044,0.0012,-0.0106,-0.0066,-0.0515,0.0388,0.0209,-0.0068,-0.0211,-0.0118,-0.2146,0.0165,-0.0415,0.0503,-0.0405,0.0134,-0.0002,-0.0493,0.043,-0.0428,0.0872,-0.0014,-0.01,-0.0546,-0.0439,-0.0458,-0.0687,-0.0216,-0.0053,-0.0488,-0.0196,0.0384,-0.0068,0.0016,0.0005,-0.0055,0.0261,0.0094,0.0585,0.0526,0.0428,0.044,0.0464,-0.1947,0.0345,0.0125,0.0558,-0.0366,-0.01,-0.0236,0.078,0.0014,0.0818,0.0251,0.0362,0.0029,0.0102,0.0271,-0.0298,-0.0164,0.0571,0.0604,-0.005,-0.0125,0.0498,-0.0041,-0.0692,0.0364,-0.0614,0.0443,-0.0081,-0.017,-0.0564,-0.0033,0.05,-0.0529,-0.0248,0.0623,0.0122,-0.0051,0.194,-0.0535,0.0104,0.0383,-0.0478,0.025,-0.0035,-0.0625,-0.0165,-0.0245,-0.0331,-0.0367,-0.0444,0.0295,-0.012,0.0168,-0.0047,0.0134,-0.0177,0.0187,-0.0105,0.0224,-0.0309,0.0371,-0.0172,0.0197,-0.0591,0.0399,0.1352,0.0056,0.036,0.0351,-0.0048,-0.0662,-0.0338,-0.0062,-0.0205,0.0393,0.0073,0.0036,0.018,-0.0451,-0.0398,-0.0016,-0.0903,-0.049,0.1534,-0.0776,0.0271,-0.04,-0.0516,-0.0466,-0.0017,-0.0386,-0.0262,0.0331,0.0368,0.0268,-0.0143,-0.0295,0.0131,-0.0222,-0.0656,-0.0783,0.0925,0.0151,-0.0582,-0.0646,0.0287,0.0014,0.0141,0.0105,0.0121,-0.0663,0.0397,0.0805,-0.0062,-0.0332,0.0274,0.0364,0.0049,0.0426,-0.0751,-0.0492,0.0349,0.0168,-0.0265,-0.0252,-0.033,0.0185,0.0717,-0.014,0.0298,-0.0449,-0.0276,-0.0005,-0.0383,0.0001,-0.0077,0.026,-0.0397,0.0262,0.0106,0.001,0.0299,-0.0116,0.0413,-0.0039,-0.0344,0.0703,0.0303,-0.0025,0.0198,0.1072,-0.0493,-0.0423,0.0685,0.0161,0.0368,0.0293,0.0521,0.0425,-0.0498,-0.0583,-0.2482,-0.0118,0.0453,-0.0191,0.0343,-0.0608,0.0344,-0.0056,0.0038,0.0334,0.0319,-0.0102,-0.0444,0.0406,-0.0169,0.0256,0.0496,0.0062,-0.0319,0.0228,-0.0417,-0.0055,0.0035,-0.0382,0.0557,-0.0156,0.1822,0.0135,0.033,-0.0377,0.0104,-0.0357,-0.044,-0.0912,0.0762,-0.0197,0.0157,0.0144,-0.022,-0.0334,0.0059,0.0353,-0.0005,-0.0843,-0.0371,-0.0141,-0.0351,0.0055,-0.0507,0.0333,0.0303,-0.0081,0.0506,0.0244,0.023,-0.0425,-0.1152,0.075,0.0056,0.0014,0.0477,-0.0567,0.0159,-0.0576,0.0395,0.031,-0.0084,-0.0379,0.0263,-0.0136,-0.0292,0.1288,0.002,-0.0148,0.0308,-0.0083,-0.0282,-0.027,-0.041,0.0138,0.04,-0.0471,0.0221,0.0308,0.0552,-0.0062,0.0784,-0.0256,0.0288,-0.0003,-0.0192,-0.0283,-0.059,0.023,0.0288,-0.0007,-0.2893,0.0278,0.0491,-0.0016,0.0516,0.0176,0.0411,0.0297,-0.0207,0.0035,0.0167,-0.0087,0.028,-0.009,0.0307,0.0247,0.0713,-0.0372,0.0474,-0.0947,0.0055,0.0205,0.2318,-0.0533,0.0554,0.0268,-0.0376,-0.0442,0.0174,-0.019,0.032,0.0486,0.1217,-0.0375,0.0286,0.0742,-0.0629,0.0158,0.0185,-0.0587,0.0096,-0.0056,-0.0666,-0.0137,0.1105,-0.0239,-0.0337,-0.0996,-0.0115,0.0239,-0.0347,0.0165,-0.0068,0.0054,0.0295,-0.0003,-0.057,-0.004,-0.0478,-0.0494,-0.0196,-0.0726,-0.0119,0.0179,-0.0218]}
{"key":"[Learning spatiotemporal features from incomplete data for traffic flow prediction using hybrid deep neural networks] Urban traffic flow prediction using data-driven models can play an important role in route planning and preventing congestion on highways. These methods utilize data collected from traffic recording stations at different timestamps to predict the future status of traffic. Hence, data collection, transmission, storage, and extraction techniques can have a significant impact on the performance of the traffic flow model. On the other hand, a comprehensive database can provide the opportunity for using complex, yet reliable predictive models such as deep learning methods. However, most of these methods have difficulties in handling missing values and outliers. This study focuses on hybrid deep neural networks to predict traffic flow in the California Freeway Performance Measurement System (PeMS) with missing values. The proposed networks are based on a combination of recurrent neural networks (RNNs) to consider the temporal dependencies in the data recorded in each station and convolutional neural networks (CNNs) to take the spatial correlations in the adjacent stations into account. Various architecture configurations with series and parallel connections are considered based on RNNs and CNNs, and several prevalent data imputation techniques are used to examine the robustness of the hybrid networks to missing values. A comprehensive analysis performed on two different datasets from PeMS indicates that the proposed series-parallel hybrid network with the mean imputation technique achieves the lowest error in predicting the traffic flow and is robust to missing values up until 21% missing ratio in both complete and incomplete training data scenarios when applied to an incomplete test data.","layer":1,"vector":[-0.0223,-0.0387,0.0495,-0.0147,0.0109,0.0437,0.0288,0.0188,0.0394,0.0033,-0.0126,-0.0518,0.0482,0.0419,0.0188,-0.0072,0.0131,0.0349,-0.014,0.0053,-0.0144,-0.0229,-0.0198,-0.0451,0.0567,0.0164,0.0056,-0.0181,-0.063,-0.2482,0.0009,-0.0411,0.0329,-0.0375,-0.0068,-0.0612,-0.039,0.0735,-0.0166,0.0559,0.0107,0.0084,-0.0027,-0.014,-0.013,0.0027,-0.0098,-0.0266,0.0158,-0.0475,0.0104,-0.0366,0.009,0.0451,0.0119,0.0706,0.0558,0.0462,0.0416,0.0389,0.0227,-0.0058,-0.1825,0.0158,0.0331,0.0206,-0.0523,-0.0125,0.0135,0.0483,-0.0394,0.053,0.0037,0.0546,-0.0001,0.0185,0.0122,-0.006,-0.0371,0.0142,0.0261,-0.0369,-0.0187,-0.0338,-0.0087,-0.0558,-0.0174,-0.0359,0.0076,-0.0082,-0.0268,-0.019,-0.0337,0.0671,-0.0502,0.0135,0.0138,-0.0109,-0.0318,0.2282,-0.0789,0.0556,0.0421,-0.013,0.0461,-0.0174,0.0076,-0.0471,-0.0668,0.0355,0.0069,-0.0591,0.0754,-0.0026,0.0564,0.0098,0.0487,0.0663,-0.0127,0.0215,-0.0371,-0.0138,0.0647,-0.0283,0.0251,-0.062,0.0186,0.1249,0.0173,0.0165,0.0281,-0.0095,-0.0487,-0.0058,-0.0043,-0.014,0.0457,-0.0139,-0.0409,-0.0486,-0.0255,-0.0341,-0.0056,-0.0581,-0.0664,0.1037,-0.0451,0.0302,-0.0228,-0.045,-0.0626,0.0285,-0.0511,-0.0421,0.0199,0.0385,0.0581,0.0366,-0.042,0.0424,-0.0241,-0.0445,-0.075,0.0949,-0.0005,-0.0767,-0.026,0.0407,0.0147,-0.0232,-0.0192,0.0102,-0.0313,0.0053,0.1104,0.0315,-0.0505,0.0477,-0.0465,0.026,-0.0099,-0.0503,-0.0474,0.0551,0.0413,-0.0347,-0.0062,-0.0672,-0.0451,0.0778,-0.0514,0.0052,-0.0349,0.0158,-0.0151,-0.0316,-0.0047,-0.0159,0.0205,-0.0631,0.0126,-0.0125,0.029,0.0007,-0.0245,0.0285,0.0024,0.0318,0.03,0.0347,0.0051,-0.0092,0.0879,-0.0323,0.0029,0.0158,0.0157,0.0114,0.0143,0.041,0.0149,-0.0092,-0.0579,-0.2145,-0.0273,-0.0112,0.0032,0.0787,-0.0748,0.0306,0.0485,0.0431,0.096,0.0499,0.0033,-0.0386,0.0328,-0.0115,0.0623,0.0071,0.0538,-0.069,-0.0034,-0.0099,0.024,-0.027,-0.0636,0.0451,0.027,0.166,-0.0115,0.018,-0.0761,0.0141,0.0495,-0.0267,-0.0802,0.0537,-0.0247,0.0667,0.0315,-0.0264,-0.0603,-0.0336,0.0371,-0.0232,-0.06,-0.0149,-0.05,-0.0335,0.0476,-0.0564,0.0507,0.004,-0.0538,0.0428,-0.0212,0.0501,0.0175,-0.1089,0.0652,-0.0175,-0.0115,0.0067,-0.036,0.0027,-0.04,0.0787,0.0169,-0.0525,-0.0543,-0.021,0.0095,-0.0518,0.1226,-0.0037,-0.0068,0.0439,-0.0258,0.0338,-0.0355,-0.0184,0.0064,0.0714,-0.0517,0.0748,0.0902,0.0294,0.0621,0.1093,-0.0307,0.0163,-0.0132,0.0371,-0.0187,0.0208,-0.0414,0.022,-0.0005,-0.2696,0.0286,0.0074,0.0161,-0.0263,-0.0116,0.0296,0.023,-0.0018,-0.0111,-0.0082,0.0328,0.0545,-0.062,-0.0097,0.0168,0.0868,-0.0236,0.0305,-0.0552,0.0183,0.066,0.2328,-0.0816,0.0319,0.0192,-0.0525,-0.008,0.057,-0.0449,0.0237,0.0062,0.0854,-0.0867,-0.0011,0.0338,-0.0324,0.0553,0.0292,0.0041,-0.0033,0.0268,-0.0262,-0.0382,0.0605,-0.0221,-0.0011,-0.0381,0.0195,0.0332,-0.0463,-0.0162,0.0091,-0.0028,0.0474,0.0521,-0.0671,-0.0777,-0.0753,0.0024,0.0115,-0.1015,-0.0033,-0.0287,-0.0493]}
{"key":"[Norm-Agnostic Linear Bandits] Linear bandits have a wide variety of applications including recommendation systems yet they make one strong assumption: the algorithms must know an upper bound $S$ on the norm of the unknown parameter $\\theta^*$ that governs the reward generation. Such an assumption forces the practitioner to guess $S$ involved in the confidence bound, leaving no choice but to wish that $\\|\\theta^*\\|\\le S$ is true to guarantee that the regret will be low. In this paper, we propose novel algorithms that do not require such knowledge for the first time. Specifically, we propose two algorithms and analyze their regret bounds: one for the changing arm set setting and the other for the fixed arm set setting. Our regret bound for the former shows that the price of not knowing $S$ does not affect the leading term in the regret bound and inflates only the lower order term. For the latter, we do not pay any price in the regret for now knowing $S$. Our numerical experiments show standard algorithms assuming knowledge of $S$ can fail catastrophically when $\\|\\theta^*\\|\\le S$ is not true whereas our algorithms enjoy low regret.","layer":0,"vector":[-0.0397,-0.0306,-0.0007,-0.0317,0.0121,0.0157,0.0627,0.0325,0.0567,-0.0098,0.0764,0.0213,0.0287,0.0692,-0.0122,0.0069,0.0315,0.0409,-0.0574,0.0214,0.0447,-0.0799,0.0157,-0.0896,0.038,0.0091,-0.018,-0.0632,-0.0272,-0.2123,0.0326,-0.0287,0.0435,-0.052,-0.0223,-0.044,-0.0074,0.0556,-0.0198,0.0679,0.0182,0.0264,-0.0135,-0.0733,-0.0118,-0.0419,0.0295,-0.0074,-0.0112,-0.029,-0.0186,-0.0386,0.05,0.0076,0.0327,0.0187,-0.0021,0.0622,0.0196,0.018,0.0273,0.0134,-0.1548,0.0002,0.0241,0.0061,-0.0821,-0.0137,-0.0059,0.0465,-0.0024,0.0269,0.036,0.0557,0.024,0.0084,0.0133,-0.0197,-0.0237,0.0336,-0.0333,-0.0414,-0.0523,0.003,-0.0152,-0.0719,0.0104,-0.0266,0.0369,0.0211,-0.0231,-0.0012,-0.0006,0.0131,-0.0653,-0.0358,0.0325,0.0441,-0.0813,0.2011,-0.0487,0.0463,0.0039,-0.031,0.0214,-0.0525,-0.0069,-0.0157,-0.0214,-0.0388,-0.0108,-0.0169,0.0606,-0.0358,0.002,0.0331,0.0362,0.0617,0.0149,-0.0091,-0.0505,0.0339,0.0957,0.0172,0.0169,-0.0643,0.0188,0.1462,0.0472,0.0542,0.0338,-0.0792,-0.0224,-0.0401,0.0097,0.0313,-0.0298,0.0308,0.0458,-0.0217,0.0027,-0.0441,0.0483,-0.0587,-0.0258,0.1034,0.0008,0.0238,-0.0222,-0.0303,0.0051,-0.0376,-0.0376,-0.0167,0.0301,0.0217,0.046,0.034,-0.0587,0.0429,-0.0462,-0.0431,-0.0031,0.0757,-0.0028,-0.0611,-0.0042,-0.0087,-0.0097,0.025,0.0236,0.0298,-0.0296,0.0246,0.0781,-0.0076,-0.0901,-0.0337,0.0239,-0.0086,0.0025,-0.0054,-0.0358,0.0426,0.0128,0.012,-0.0021,-0.0429,0.0367,0.0421,-0.011,0.0061,0.0044,-0.0384,-0.0304,-0.0401,-0.0156,0.0142,0.0168,-0.0249,0.0084,0.025,-0.0453,-0.0121,0.0401,0.0387,0.0362,-0.0229,0.0636,0.0295,-0.0326,-0.0143,0.0172,-0.0319,-0.0796,0.0273,0.039,0.021,-0.0287,0.0309,0.0721,-0.006,0.0179,-0.2369,0.0063,-0.0093,0.007,0.0496,-0.0594,0.0546,-0.0986,0.033,0.0654,0.046,-0.0459,-0.0309,0.0375,0.0083,0.0647,0.0366,-0.0042,-0.0459,0.0048,-0.063,0.038,-0.0713,-0.0558,0.0221,0.0382,0.24,0.03,0.0241,-0.033,0.0327,0.0264,0.0122,-0.0831,-0.0024,0.0447,0.0242,-0.0182,0.0066,-0.0522,-0.0199,0.0203,-0.0296,-0.0743,-0.0759,-0.0394,-0.0595,0.0385,-0.0485,0.0354,0.0226,-0.0064,0.0308,-0.0267,0.0058,-0.0406,-0.1114,0.0123,-0.0109,0.1073,0.0226,-0.0462,0.0458,-0.1013,0.072,-0.018,0.0152,-0.0244,0.028,-0.03,-0.0053,0.038,-0.0011,0.0173,-0.0048,0.0154,0.0223,-0.0544,-0.0576,-0.0239,0.0462,-0.0649,0.0519,0.0031,-0.0248,0.0332,0.0559,-0.0138,0.0237,0.0166,-0.0048,0.0108,-0.0897,-0.0053,0.0998,-0.0336,-0.3071,0.0495,0.0345,0.019,-0.0464,0.0174,0.049,-0.033,-0.0387,0.0248,0.0193,0.0816,0.0381,-0.036,0.0288,0.0431,0.0411,-0.0341,0.0248,-0.051,0.0229,0.0489,0.2274,-0.0279,0.0213,0.0313,-0.0351,-0.0298,-0.0071,-0.0013,0.0155,-0.0288,0.0645,-0.0716,0.0731,0.0792,-0.0807,0.0389,0.014,-0.0161,-0.0383,0.0156,-0.0534,0.0097,0.1297,-0.0053,-0.0305,-0.0307,0.0041,0.0151,-0.0373,-0.0038,0.0158,-0.0129,0.0226,0.0068,-0.064,-0.0777,-0.0382,-0.0051,-0.0036,-0.0125,0.0067,-0.0098,0.0111]}
{"key":"[Tightly Robust Optimization via Empirical Domain Reduction] Data-driven decision-making is performed by solving a parameterized optimization problem, and the optimal decision is given by an optimal solution for unknown true parameters. We often need a solution that satisfies true constraints even though these are unknown. Robust optimization is employed to obtain such a solution, where the uncertainty of the parameter is represented by an ellipsoid, and the scale of robustness is controlled by a coefficient. In this study, we propose an algorithm to determine the scale such that the solution has a good objective value and satisfies the true constraints with a given confidence probability. Under some regularity conditions, the scale obtained by our algorithm is asymptotically $O(1/\\sqrt{n})$, whereas the scale obtained by a standard approach is $O(\\sqrt{d/n})$. This means that our algorithm is less affected by the dimensionality of the parameters.","layer":2,"vector":[-0.0165,-0.0152,0.0318,-0.0214,0.0069,-0.0117,0.0393,0.0509,0.0174,-0.0094,-0.0001,-0.0353,0.0334,0.0583,-0.0118,0.035,0.0215,0.0851,-0.0413,0.0886,0.0665,-0.0453,-0.0097,-0.054,0.0681,0.0035,-0.0132,-0.044,-0.0098,-0.2497,0.0184,-0.0289,0.0614,-0.0888,0.0328,0.0068,-0.0429,0.0495,0.0032,0.0163,0.0208,0.0248,-0.0036,-0.0596,-0.0166,-0.0426,0.0385,0.0199,-0.0427,-0.0139,0.0124,-0.0282,0.0246,0.0077,0.0594,0.0408,0.045,0.0166,0.0532,0.0701,-0.0072,0.0348,-0.1351,0.0612,0.0461,-0.0013,-0.0463,-0.0549,0.0105,0.0704,0.017,0.0363,0.0346,0.0514,0.0183,-0.0049,0.0184,-0.0542,-0.046,0.029,0.0181,-0.0489,-0.0606,0.0174,-0.0336,-0.0542,0.0048,-0.0526,0.0544,0.005,-0.016,-0.0313,-0.0453,-0.0201,-0.0782,-0.0269,0.0355,0.0445,-0.0411,0.1949,-0.0372,0.0043,0.0214,-0.0443,0.0339,-0.0335,-0.0316,-0.016,-0.0066,0.0219,-0.0192,-0.0285,0.0064,0.0032,-0.0233,0.0288,-0.0017,0.0004,-0.007,-0.0094,-0.0514,0.034,0.069,0.003,0.0814,-0.0636,0.0058,0.1339,-0.0105,0.0563,0.0026,-0.0356,-0.0439,-0.0441,-0.0016,0.0248,-0.039,0.0098,0.0134,0.0088,-0.0806,-0.0746,0.0048,-0.0963,-0.0548,0.131,-0.0322,0.0176,-0.0253,-0.0737,0.0078,-0.0192,-0.0326,0.0006,0.0199,0.0344,-0.0193,0.0145,-0.0374,-0.0104,-0.0163,-0.0533,0.0209,0.1275,-0.0496,-0.0722,-0.0489,0.0287,0.0322,0.0246,0.0334,0.0395,0.0098,0.0668,0.0758,-0.0294,-0.0546,0.0014,0.0077,0.0103,0.0475,-0.0525,0.018,-0.0163,0.0384,-0.0363,0.0045,-0.0395,0.0283,0.0238,-0.0726,-0.0228,-0.0239,0.0016,-0.0564,-0.0387,0.0073,-0.0296,0.0026,0.0002,0.0267,0.0044,-0.0279,0.0756,-0.0144,0.0182,0.0199,-0.0002,-0.0098,0.0644,-0.0623,-0.0083,0.0504,-0.0413,-0.0303,0.0014,0.0539,0.0589,0.0027,0.0343,0.0814,-0.0072,-0.0688,-0.2328,-0.0353,-0.0327,0.0162,0.0777,-0.0355,0.0623,-0.0505,0.0592,0.0576,0.0609,-0.021,-0.0598,0.0475,-0.0556,0.0072,0.0499,-0.0305,-0.0615,0.0184,-0.051,0.0468,-0.0314,-0.0568,0.0412,-0.001,0.1945,-0.0082,0.0294,-0.0278,0.0405,-0.0239,0.0156,-0.0223,0.0584,0.0108,0.052,-0.031,-0.0259,-0.0199,0.0094,0.0051,-0.0212,-0.075,-0.0535,-0.0354,-0.0164,0.0575,-0.0591,0.0469,0.044,-0.0155,0.0628,-0.0321,-0.0056,-0.0105,-0.072,0.0337,-0.0162,0.0194,-0.003,-0.0821,-0.0241,-0.0568,0.0213,0.0197,-0.0215,-0.0243,0.0384,-0.0416,0.0038,0.0336,0.0241,0.0207,0.0515,-0.0072,0.0121,0.0068,-0.0323,-0.0167,0.0567,-0.0258,0.0663,0.0015,0.0255,0.0193,0.0675,-0.0057,-0.0034,-0.0189,0.0084,-0.029,-0.0514,-0.0081,0.0483,0.0431,-0.3256,-0.0159,0.0079,0.0061,-0.0318,0.0253,0.0574,0.0191,-0.0698,0.0194,-0.046,0.0415,-0.0028,0.0087,0.0295,0.0087,0.0462,-0.0368,0.059,-0.0685,0.0393,0.0114,0.2366,-0.0753,0.0168,0.0603,-0.0292,-0.0054,-0.0099,-0.0399,-0.003,0.0147,0.0673,-0.0508,0.0627,0.099,-0.06,0.0378,0.0232,0.0181,0.0106,0.0083,0.0217,-0.0115,0.0999,-0.0159,-0.0107,-0.0283,0.0209,0.0285,-0.0707,0.0129,0.0001,-0.019,0.0255,0.0424,-0.0564,-0.0256,-0.0405,-0.0586,0.0044,-0.0343,-0.0401,0.0054,0.0173]}
{"key":"[Efficient Clustering with Limited Distance Information] Given a point set S and an unknown metric d on S, we study the problem of efficiently partitioning S into k clusters while querying few distances between the points. In our model we assume that we have access to one versus all queries that given a point s 2 S return the distances between s and all other points. We show that given a natural assumption about the structure of the instance, we can efficiently find an accurate clustering using only O(k) distance queries. We use our algorithm to cluster proteins by sequence similarity. This setting nicely fits our model because we can use a fast sequence database search program to query a sequence against an entire dataset. We conduct an empirical study that shows that even though we query a small fraction of the distances between the points, we produce clusterings that are close to a desired clustering given by manual classification.","layer":6,"vector":[-0.0619,-0.0174,0.0182,0.0028,0.0031,0.0114,0.0553,0.0166,0.0048,-0.0013,-0.0145,-0.0743,-0.0128,0.0611,0.0072,-0.0286,0.008,0.0716,-0.0537,0.0131,0.0277,-0.0231,-0.016,-0.0145,0.0265,0.0655,-0.0001,-0.005,-0.0832,-0.2158,0.0119,-0.0379,0.0837,-0.0192,0.0013,0.008,0.0085,0.0604,-0.0377,0.0211,0.0861,0.0157,-0.0208,-0.0507,-0.0234,-0.0633,-0.045,-0.0028,0.0046,-0.0774,-0.005,-0.0401,-0.014,0.045,0.0396,0.0589,0.0253,0.0078,0.0072,-0.0287,0.0493,0.0251,-0.1057,0.0346,0.0316,0.0132,0.0087,-0.0382,0.0339,0.0459,-0.0177,0.0903,-0.0034,0.0323,0.0247,0.0111,-0.0028,0.0085,-0.015,0.0327,-0.0255,-0.0612,-0.0092,-0.0242,-0.0313,-0.0369,-0.017,-0.0596,0.0373,0.0434,-0.031,-0.0031,-0.0079,0.0514,-0.1072,-0.0728,0.0175,0.0054,-0.015,0.2093,-0.0505,0.0935,0.042,-0.0533,0.0074,-0.0646,-0.0048,-0.0231,0.0127,-0.0021,0.0191,-0.0067,-0.0357,-0.0626,0.0253,0.0003,0.0601,0.0546,-0.0278,0.023,-0.0211,0.0074,0.046,0.0073,0.0392,-0.0535,0.0102,0.0982,0.0461,-0.012,0.0934,-0.0008,-0.0639,0.0154,0.0158,0.0213,-0.0141,-0.0179,0.0246,-0.0343,-0.0229,-0.0954,0.0178,-0.0808,-0.0343,0.1687,-0.0155,0.0195,-0.0598,-0.0509,0.0183,0.0137,-0.0661,-0.0212,-0.0067,0.0428,0.0718,0.0176,-0.0405,0.0104,-0.0505,-0.0378,0.0141,0.0946,0.046,-0.0873,-0.032,0.0212,0.0104,-0.0235,0.0461,0.0769,0.0119,0.0701,0.0687,-0.0073,-0.0768,-0.0046,0.0187,0.0346,-0.0018,-0.0049,-0.0152,0.0328,0.0647,-0.0626,-0.0298,0.0051,0.0124,0.0748,-0.024,0.029,-0.0266,-0.002,-0.0263,-0.0602,0.0043,0.0063,-0.0087,-0.0201,0.0506,0.0259,-0.0279,0.0738,-0.0142,0.0142,-0.0101,-0.043,0.0285,0.0297,-0.0657,-0.0267,-0.0257,-0.0535,-0.022,-0.0154,0.0454,0.0343,0.0568,0.0618,0.0563,-0.0395,-0.0568,-0.2082,-0.014,0.0088,-0.0038,0.0437,-0.0456,-0.0156,0.0378,0.0613,0.0581,0.0434,-0.0245,0.0054,0.0521,-0.0438,0.0878,0.069,0.0262,-0.052,0.0215,0.0363,0.0324,-0.0161,-0.0622,0.0255,-0.0099,0.2277,0.041,0.0181,-0.0275,0.0245,0.0177,-0.0604,-0.0816,0.041,0.0232,0.0189,-0.0251,-0.0207,-0.0206,-0.0624,0.0067,-0.0237,-0.0838,-0.0349,-0.039,0.0038,0.0231,-0.0152,0.0104,0.0424,-0.0345,0.0581,-0.0077,-0.0021,-0.0762,-0.074,-0.0089,-0.0588,0.0381,0.0257,-0.0706,-0.0369,-0.0287,0.0727,-0.049,-0.0424,-0.0341,0.014,-0.0353,-0.0484,0.0598,-0.011,0.0281,0.0558,-0.0053,0.0523,-0.0409,-0.0433,-0.0197,0.0931,-0.0274,0.0456,-0.0005,0.0111,0.0223,0.1094,-0.0034,0.0124,0.0008,0.0475,-0.0173,-0.0633,-0.0046,0.027,0.0041,-0.2759,0.0533,-0.0231,0.0142,-0.0344,0.0129,0.0486,-0.0252,0.0026,0.0032,0.0669,0.0394,0.024,-0.0765,-0.0147,0.0613,0.0371,-0.0605,0.0274,-0.0583,0.0169,0.0289,0.2145,-0.0311,0.0222,0.0364,-0.0225,-0.0095,0.0075,-0.0144,-0.0275,-0.0086,0.0959,-0.0371,0.0532,0.0794,0.0138,0.0168,0.0374,-0.0223,-0.0047,-0.0383,-0.0371,-0.0229,0.1075,-0.0205,-0.0529,-0.0845,0.0059,0.018,-0.0734,0.0194,-0.0389,0.0159,0.0093,0.0221,-0.0329,-0.0086,-0.0632,-0.0757,-0.0123,-0.0692,-0.0421,0.0165,0.0493]}
{"key":"[Solving OSCAR regularization problems by proximal splitting algorithms] The OSCAR (octagonal selection and clustering algorithm for regression) regularizer consists of a L_1 norm plus a pair-wise L_inf norm (responsible for its grouping behavior) and was proposed to encourage group sparsity in scenarios where the groups are a priori unknown. The OSCAR regularizer has a non-trivial proximity operator, which limits its applicability. We reformulate this regularizer as a weighted sorted L_1 norm, and propose its grouping proximity operator (GPO) and approximate proximity operator (APO), thus making state-of-the-art proximal splitting algorithms (PSAs) available to solve inverse problems with OSCAR regularization. The GPO is in fact the APO followed by additional grouping and averaging operations, which are costly in time and storage, explaining the reason why algorithms with APO are much faster than that with GPO. The convergences of PSAs with GPO are guaranteed since GPO is an exact proximity operator. Although convergence of PSAs with APO is may not be guaranteed, we have experimentally found that APO behaves similarly to GPO when the regularization parameter of the pair-wise L_inf norm is set to an appropriately small value. Experiments on recovery of group-sparse signals (with unknown groups) show that PSAs with APO are very fast and accurate.","layer":6,"vector":[0.0106,0.0092,0.0367,0.0024,0.0572,-0.0069,-0.0209,-0.007,0.0155,-0.0174,0.0224,-0.0652,0.0241,0.0353,0.0109,0.0117,0.039,0.0439,-0.0552,-0.0038,-0.0033,-0.0354,-0.0105,-0.0388,0.0229,0.0362,-0.0226,-0.0488,-0.0295,-0.2698,0.0083,-0.0409,0.0808,-0.0375,0.0446,0.0315,-0.0126,0.0767,-0.0725,0.07,-0.0193,0.0303,-0.0263,-0.0287,-0.0117,-0.0474,-0.0266,-0.0186,-0.0115,-0.0196,0.0646,-0.0041,0.0247,0.0474,0.0319,0.0103,0.0316,0.0126,0.0373,0.0312,0.0492,0.0357,-0.1499,0.0506,0.0618,-0.0046,0.0372,-0.0503,0.0252,0.0638,-0.0327,0.0551,0.0385,0.0296,0.002,0.0035,0.0074,-0.0286,-0.0082,-0.0229,0.0535,-0.0348,-0.0536,0.0059,-0.0183,-0.033,0.0579,-0.0556,0.0228,-0.012,-0.0626,-0.0141,-0.0365,0.0461,-0.0489,-0.0536,0.0089,0.0351,-0.0221,0.2249,-0.0525,0.1118,0.042,-0.0214,0.0316,-0.047,-0.0449,-0.0066,0.0072,-0.018,-0.0056,0.0045,0.0331,-0.0455,0.0231,-0.0257,0.0486,0.0598,-0.0293,-0.0153,-0.0287,-0.03,0.0447,0.0186,0.03,-0.0624,0.0313,0.105,0.0756,0.0479,0.0421,-0.0113,-0.0562,-0.0336,-0.0047,0.0065,0.0213,-0.0125,0.0168,0.0296,-0.0278,-0.0701,0.0342,-0.0516,-0.0407,0.1173,-0.0774,0.0272,-0.0863,-0.0395,-0.0306,-0.027,-0.0641,0.0266,0.0522,0.0039,0.0579,0.0386,-0.0125,0.0216,-0.0427,-0.0579,-0.0422,0.1055,0.0061,-0.0704,-0.0464,0.0128,0.0302,0.0022,0.048,0.0095,-0.0214,0.0452,0.0779,0.0405,-0.0379,-0.0038,-0.0112,-0.0309,0.0035,-0.0482,-0.02,0.0317,0.0393,-0.0404,-0.0057,-0.0108,0.0118,0.0137,-0.0226,-0.0144,-0.0356,-0.0166,-0.0252,-0.0102,-0.0151,-0.0366,0.0098,-0.0289,0.0358,-0.0174,-0.0577,-0.0043,0.0018,0.0321,0.0339,-0.0348,-0.0004,0.0633,-0.0305,-0.0539,0.0751,0.0091,-0.0292,0.0234,0.0374,0.0228,0.0088,0.0851,0.0448,-0.0423,-0.0981,-0.1921,-0.0337,0.0337,0.0146,0.0116,-0.0753,0.0663,-0.0519,0.0911,0.0884,0.02,-0.0139,-0.03,0.0897,-0.02,0.0594,0.0461,0.0333,-0.0117,0.0057,-0.0351,0.0691,-0.0508,-0.0357,0.0524,-0.0075,0.1773,0.053,0.0413,-0.0175,-0.0017,0.0256,-0.0329,-0.0571,0.026,0.0122,0.0859,-0.0299,-0.053,-0.0264,-0.0455,0.0183,-0.0173,-0.0733,-0.0271,-0.0191,-0.0442,0.0464,-0.0571,0.0118,0.0465,-0.0175,0.0549,-0.0442,0.0177,-0.0378,-0.0884,0.0113,0.0049,0.0411,0.0417,-0.0533,-0.0272,-0.0474,0.0566,0.0366,-0.0126,-0.0439,0.0068,-0.0027,-0.0176,0.0537,-0.0337,0.0115,0.0162,0.0139,0.0581,-0.0374,-0.0074,-0.0459,0.0869,-0.0239,0.0077,-0.0016,0.0508,0.0228,0.0704,0.0002,-0.0052,-0.0154,-0.0148,0.0087,-0.0967,-0.0003,0.0279,-0.0061,-0.3154,0.0292,0.0162,-0.0301,-0.0602,0.0476,0.0237,0.0035,-0.0247,0.0074,-0.0371,0.041,0.0082,-0.0242,0.0122,0.066,0.0442,-0.0715,0.0146,-0.1104,-0.0298,0.011,0.2318,-0.0552,0.0302,0.0298,-0.0366,-0.043,0.0007,-0.0316,-0.0043,0.0091,0.0755,-0.0591,0.0478,0.0865,0.0111,0.0258,0.0054,-0.0209,-0.0044,-0.0326,-0.0414,-0.0268,0.1181,-0.0365,-0.0076,-0.0135,0.0235,0.0209,-0.0278,0.0205,-0.0116,-0.016,0.0224,0.0194,-0.0785,-0.0136,-0.0306,-0.0266,0.0139,-0.0447,-0.0405,-0.0063,0.0103]}
{"key":"[Continuous-Variable Quantum Key Distribution with a Real Local Oscillator and without Auxiliary Signals] Continuous-variable quantum key distribution (CV-QKD) is realized with coherent detection and is therefore very suitable for a cost-efficient implementation. The major challenge in CV-QKD is mitigation of laser phase noise at a signal to noise ratio of much less than 0 dB. So far, this has been achieved with a remote local oscillator or with auxiliary signals. For the first time, we experimentally demonstrate that CV-QKD can be performed with a real local oscillator and without auxiliary signals which is achieved by applying Machine Learning methods. It is shown that, with the most established discrete modulation protocol, the experimental system works down to a quantum channel signal to noise ratio of -19.1 dB. The performance of the experimental system allows CV-QKD at a key rate of 9.2 Mbit/s over a fiber distance of 26 km. After remote local oscillator and auxiliary signal aided CV-QKD, this could mark a starting point for a third generation of CV-QKD systems that are even more attractive for a wide implementation because they are almost identical to standard coherent systems.","layer":0,"vector":[-0.0794,0.0085,-0.0223,-0.0272,-0.0001,0.026,0.0361,0.0278,0.0229,0.0032,0.0571,-0.0298,0.0214,0.0554,0.0427,-0.0054,-0.0275,-0.0237,-0.0458,0.0165,0.0286,-0.0186,-0.0157,-0.0744,0.0202,-0.0405,0.0009,-0.0503,-0.0376,-0.2055,-0.002,-0.0238,0.0471,-0.0109,-0.0057,-0.069,-0.0519,0.0239,-0.025,0.0649,0.0815,0.0227,-0.0122,-0.015,-0.0195,-0.0858,-0.0635,-0.0356,-0.016,-0.0416,0.0033,0.0412,0.0196,0.0341,0.0617,0.0227,0.0303,0.0489,0.0376,0.0291,0.0138,0.0367,-0.1639,0.0287,0.0259,0.0276,-0.0275,-0.0576,0.0485,0.0161,-0.0473,0.0809,-0.0039,0.0267,-0.011,-0.0546,-0.0029,-0.035,-0.0373,0.055,0.0271,-0.0528,-0.0275,-0.0318,-0.075,-0.0212,0.0247,-0.0522,0.0066,0.0147,-0.0537,-0.0363,-0.0178,-0.0023,-0.0466,-0.014,0.0081,0.0113,-0.0173,0.1851,-0.0422,-0.0171,-0.0204,-0.0826,0.0493,-0.0536,-0.0404,-0.0492,-0.018,0.0192,0.0079,-0.0419,0.0498,-0.0398,-0.0127,0.0227,0.0562,0.0846,0.0345,-0.0241,-0.0373,0.0065,0.0393,-0.011,-0.0011,-0.0432,-0.0309,0.1045,0.0487,0.07,0.0131,-0.0174,-0.0203,-0.0445,-0.0219,0.0314,-0.0024,0.0326,0.0719,-0.0023,0.0025,-0.0262,0.0288,-0.112,-0.0003,0.0936,-0.0317,0.0594,-0.0031,-0.0258,0.032,-0.0077,0.0115,-0.0396,0.0639,0.0451,0.0278,0.0678,-0.0319,-0.0005,-0.0362,-0.0924,-0.0241,0.0932,0.0237,-0.1072,-0.0655,0.0127,0.0262,-0.0487,0.0211,0.0831,-0.0898,0.0729,0.0527,-0.0505,-0.0266,0.027,-0.0135,-0.0041,-0.0646,-0.0624,-0.0354,0.0502,0.0991,-0.0173,-0.0046,-0.0285,-0.0141,-0.0179,0.008,0.0122,-0.0264,-0.0277,0.0069,-0.0307,-0.0314,-0.0233,0.0205,0.0049,0.0632,-0.0283,-0.0358,0.0214,-0.0354,-0.0192,0.0412,-0.0168,0.052,0.0283,0.007,-0.0141,0.0685,-0.0485,-0.1172,-0.0048,-0.0059,0.0304,0.0132,0.0231,0.0288,-0.0258,-0.0809,-0.2253,-0.0263,0.0254,-0.0599,0.1003,-0.0552,0.0067,-0.0023,0.0434,0.042,0.043,0.0525,-0.0222,0.0087,-0.0057,0.0644,0.0925,0.0615,-0.027,0.0283,-0.0101,0.0392,-0.0684,-0.0674,0.0445,-0.0254,0.1545,0.0126,0.0605,-0.0412,0.0382,0.0504,-0.0294,-0.1059,0.0073,0.0143,0.0265,0.053,-0.0259,-0.0018,-0.0084,0.0163,-0.0152,-0.0596,0.0031,-0.0474,-0.0526,0.0319,-0.0356,0.0167,0.0512,-0.0307,0.0686,0.0213,0.0108,-0.0851,-0.0541,-0.001,-0.0369,0.0212,0.0139,-0.0078,0.0122,-0.0049,0.0257,0.0003,-0.0324,-0.0172,0.115,-0.0501,0.0028,0.0885,0.0079,0.0382,0.0793,-0.0165,0.0755,-0.0422,0.0107,-0.0111,0.1052,-0.0091,0.0463,0.0414,0.0189,-0.0012,0.0866,0.051,-0.0013,0.0009,-0.0046,0.0387,-0.0002,-0.0067,0.0118,0.0121,-0.3017,0.0189,0.023,0.0001,-0.0212,0.0086,0.0543,0.0137,-0.0779,-0.0135,-0.0599,0.0383,-0.0053,0.0213,0.0615,0.0496,0.0375,-0.0556,-0.0179,-0.027,0.0365,0.0432,0.2326,0.001,0.017,-0.0194,-0.0205,0.0781,-0.0203,-0.0336,0.0056,0.0132,0.0604,-0.0344,0.0287,0.0021,-0.0323,0.0454,0.0197,-0.028,0.0337,0.029,-0.0642,-0.0356,0.1442,-0.0365,-0.0221,-0.0639,0.0203,-0.003,0.0027,-0.0068,-0.0021,-0.004,0.0095,0.0455,-0.0138,-0.052,-0.0185,0.0003,0.0186,-0.0702,-0.0256,-0.0141,-0.0084]}
{"key":"[ElderSim: A Synthetic Data Generation Platform for Human Action Recognition in Eldercare Applications] To train deep learning models for vision-based action recognition of elders' daily activities, we need large-scale activity datasets acquired under various daily living environments and conditions. However, most public datasets used in human action recognition either differ from or have limited coverage of elders' activities in many aspects, making it challenging to recognize elders' daily activities well by only utilizing existing datasets. Recently, such limitations of available datasets have actively been compensated by generating synthetic data from realistic simulation environments and using those data to train deep learning models. In this paper, based on these ideas we develop ElderSim, an action simulation platform that can generate synthetic data on elders' daily activities. For 55 kinds of frequent daily activities of the elders, ElderSim generates realistic motions of synthetic characters with various adjustable data-generating options, and provides different output modalities including RGB videos, two- and three-dimensional skeleton trajectories. We then generate KIST SynADL, a large-scale synthetic dataset of elders' activities of daily living, from ElderSim and use the data in addition to real datasets to train three state-of the-art human action recognition models. From the experiments following several newly proposed scenarios that assume different real and synthetic dataset configurations for training, we observe a noticeable performance improvement by augmenting our synthetic data. We also offer guidance with insights for the effective utilization of synthetic data to help recognize elders' daily activities.","layer":2,"vector":[-0.0573,-0.01,0.0033,-0.0377,0.0345,0.0642,0.0244,-0.0268,-0.0093,-0.0037,-0.0073,-0.0373,0.0027,0.0668,0.0241,-0.0189,-0.0072,0.0471,-0.045,0.0342,0.0262,-0.0194,-0.0024,-0.0192,0.0085,0.0318,-0.0239,-0.0495,-0.0145,-0.2347,-0.0036,-0.0131,0.0658,0.0058,0.0023,-0.0295,-0.0201,0.0563,-0.0576,0.0315,0.0066,-0.0136,0.0066,-0.0566,-0.0055,-0.0382,-0.0252,-0.0431,-0.0391,0.0043,-0.0055,-0.0277,0.0106,0.0399,0.0126,-0.0101,0.072,0.03,0.052,-0.0041,0.0431,0.0421,-0.1786,0.0714,-0.0196,0.0276,0.0058,-0.007,0.0665,-0.0099,-0.0425,0.0365,0.049,0.0681,0.0274,-0.0308,0.0018,-0.0495,-0.0218,-0.0452,-0.0026,-0.0037,-0.0402,0.0225,-0.0232,-0.0022,-0.012,-0.0695,0.0032,0.0108,-0.0648,0.0484,-0.0387,0.046,-0.0355,-0.0178,0.0259,0.019,-0.0556,0.223,-0.0361,0.0551,0.0536,0.0104,0.0508,-0.031,-0.0444,-0.0193,0.0072,-0.009,-0.0375,-0.0221,0.0111,-0.0126,0.0219,-0.0079,0.0758,0.073,-0.0139,-0.037,0.0076,0.0191,0.0537,-0.0434,0.0034,-0.0719,0.0282,0.1845,0.0241,0.0357,0.0577,-0.021,-0.0481,-0.0521,0.019,0.0442,0.0194,0.0118,0.0407,-0.0422,-0.0112,0.014,0.0121,-0.0446,-0.0329,0.0925,-0.0258,0.0423,-0.0205,0.0145,0.0026,0.0491,-0.0054,-0.0009,0.0229,0.0338,0.0712,0.0201,-0.0434,-0.0438,0.0029,-0.0917,-0.0239,0.0646,-0.0076,-0.0897,0.0,0.0108,0.0093,0.0223,0.0711,0.0399,-0.0227,0.0524,0.1009,0.0602,-0.0557,-0.0058,0.0163,-0.0137,0.0118,-0.0718,-0.0519,-0.0019,0.0353,-0.0315,0.0309,0.0016,0.0663,0.0497,-0.0035,0.0509,-0.0408,-0.003,-0.0011,-0.0334,-0.0388,0.0073,0.0301,-0.0472,0.0269,-0.0033,-0.0423,-0.0116,-0.0041,-0.0064,-0.0533,0.0059,0.0601,0.0063,-0.022,0.0119,0.134,-0.0106,-0.0402,0.0252,-0.0344,-0.0213,-0.0243,0.0249,0.0295,-0.0363,-0.0682,-0.261,0.0165,0.0289,-0.061,0.0173,-0.0674,0.0522,0.0027,0.0507,0.0418,0.0499,-0.0144,-0.0108,-0.0121,-0.0168,0.0716,0.017,0.0423,-0.0484,-0.023,0.0096,-0.0307,0.0095,-0.0903,0.0654,-0.0021,0.2246,0.0258,0.0327,-0.0392,0.0047,0.0248,-0.0507,-0.1175,0.0505,0.0033,0.0748,0.0069,-0.0233,-0.0301,-0.0832,0.0263,0.0324,-0.1015,-0.0257,-0.014,-0.0288,0.0415,-0.0476,-0.0114,0.0432,-0.0278,0.0819,-0.0082,-0.0343,-0.0313,-0.0569,0.0307,-0.0549,-0.0128,-0.0457,-0.0135,0.0354,-0.0485,0.0857,-0.0307,-0.0582,-0.0665,0.0392,-0.0275,0.0116,0.1058,-0.0077,-0.0063,0.0407,0.0261,-0.0091,-0.0228,-0.0014,0.0034,0.0107,-0.0208,0.0257,0.0354,0.0374,-0.0193,0.0424,0.0002,0.0292,-0.0425,0.0474,0.0318,-0.0431,-0.0346,0.0226,-0.0332,-0.2983,0.0356,-0.0342,0.0077,-0.0263,-0.0457,0.0679,0.0131,-0.04,0.0254,-0.0217,0.0552,0.0572,0.0159,0.0069,0.033,0.0487,-0.037,0.0423,-0.0954,0.0352,0.0305,0.1656,-0.0478,0.0273,0.0198,-0.0051,-0.0348,0.0512,-0.0605,-0.0142,-0.0336,0.0941,-0.0364,0.024,0.0817,-0.0364,0.021,-0.0142,0.0012,-0.0034,0.0029,-0.0228,-0.0371,0.0876,-0.0044,-0.0171,0.0072,-0.0208,0.0364,-0.0539,0.004,-0.0409,0.018,0.0773,0.0265,-0.0071,-0.0383,-0.0222,-0.0558,-0.0082,-0.0226,0.0018,-0.0218,-0.0484]}
{"key":"[Non Asymptotic Bounds for Optimization via Online Multiplicative Stochastic Gradient Descent] The gradient noise of Stochastic Gradient Descent (SGD) is considered to play a key role in its properties (e.g. escaping low potential points and regularization). Past research has indicated that the covariance of the SGD error done via minibatching plays a critical role in determining its regularization and escape from low potential points. It is however not much explored how much the distribution of the error influences the behavior of the algorithm. Motivated by some new research in this area, we prove universality results by showing that noise classes that have the same mean and covariance structure of SGD via minibatching have similar properties. We mainly consider the Multiplicative Stochastic Gradient Descent (M-SGD) algorithm as introduced by Wu et al., which has a much more general noise class than the SGD algorithm done via minibatching. We establish nonasymptotic bounds for the M-SGD algorithm mainly with respect to the Stochastic Differential Equation corresponding to SGD via minibatching. We also show that the M-SGD error is approximately a scaled Gaussian distribution with mean $0$ at any fixed point of the M-SGD algorithm. We also establish bounds for the convergence of the M-SGD algorithm in the strongly convex regime.","layer":0,"vector":[-0.048,-0.0191,0.0299,-0.0261,-0.0106,0.0022,-0.0014,0.0035,0.0652,-0.0045,0.0444,-0.0077,0.0201,0.0831,0.0265,0.0622,-0.0286,-0.033,-0.0754,0.0131,0.0317,-0.0362,0.0403,-0.0877,0.0585,-0.0141,-0.0445,-0.1133,-0.025,-0.2763,-0.0015,-0.0575,0.0362,-0.0226,0.0339,-0.0337,-0.0138,0.0709,-0.0346,0.051,-0.0191,0.0849,-0.0611,-0.0686,0.002,-0.0723,-0.0254,-0.0427,-0.0504,-0.003,0.038,-0.0217,0.0038,0.0207,-0.0232,0.0197,0.0203,0.036,0.0216,0.0456,0.0053,0.0791,-0.1706,0.0615,0.0569,-0.021,-0.0128,-0.0624,0.0045,0.0576,-0.0044,0.0361,0.0284,0.0621,0.0253,-0.0259,-0.0025,-0.0121,-0.0233,0.0537,0.0345,-0.0265,-0.0401,-0.0253,-0.0585,-0.0413,0.0365,-0.0315,0.0606,0.0152,-0.0074,-0.0376,-0.0458,0.0064,-0.0531,0.0413,0.0115,0.0262,-0.033,0.1732,-0.0612,0.0447,0.0189,-0.0157,0.0257,-0.0062,-0.0488,-0.0084,0.0072,-0.0083,-0.0037,-0.027,0.0603,-0.0383,-0.0071,0.0134,0.0361,0.0324,-0.0261,-0.0051,-0.0773,0.0211,0.053,0.0125,0.0279,-0.0417,0.008,0.1418,0.0593,0.0286,0.0545,-0.0312,-0.0411,-0.0445,0.0428,-0.0122,-0.0289,-0.0128,0.0401,-0.0128,-0.0572,-0.0552,-0.0006,-0.1019,-0.0279,0.1186,-0.0395,0.0525,-0.0435,-0.0271,0.0115,0.0061,0.0217,-0.0436,0.0172,0.0437,0.0196,0.0373,-0.0606,0.0128,-0.0718,-0.0361,0.0092,0.116,-0.0075,-0.0393,-0.0463,-0.0127,0.0051,-0.0058,0.0706,0.0487,-0.0566,-0.0211,0.0542,0.0434,-0.1166,0.0125,-0.0269,0.0048,-0.0128,-0.0131,-0.0092,0.0135,0.0339,-0.0362,0.0205,-0.0333,0.0285,0.0191,-0.0276,-0.0212,-0.0276,-0.0422,-0.0425,-0.0545,0.0025,-0.0332,0.0315,0.0146,0.0143,-0.0136,-0.0453,0.0578,0.043,-0.0071,-0.0172,-0.0118,-0.012,0.0637,-0.0398,-0.0372,0.0464,-0.018,-0.0173,0.0341,0.0643,0.0381,-0.0076,0.0319,0.0255,-0.0294,-0.0752,-0.1887,-0.047,0.0138,0.0039,0.0425,-0.0897,0.0353,-0.0243,0.0716,0.0582,0.0636,-0.0321,-0.0001,0.0116,0.0443,0.0847,0.0325,0.0142,0.0036,0.0168,0.024,0.0289,0.0108,-0.0604,0.0792,-0.0064,0.1735,-0.01,0.0536,-0.0407,0.0085,0.0068,0.0209,-0.0461,0.045,0.0222,0.068,-0.012,-0.0357,-0.0209,-0.031,0.0317,0.0251,-0.0843,-0.058,-0.0399,-0.0281,0.0196,-0.0649,0.0151,0.0481,0.0013,0.0525,-0.0271,0.0364,-0.0225,-0.0712,0.0376,-0.0079,0.0442,-0.0248,-0.0727,0.0526,-0.0747,0.0396,-0.0092,-0.03,-0.0474,0.0403,-0.0383,0.0163,0.0515,0.0094,0.0126,0.0281,0.0018,0.051,-0.0171,-0.0337,-0.0156,0.0414,-0.0207,0.0685,0.0206,0.0073,0.0042,0.0703,-0.001,-0.016,0.0003,0.0004,-0.0019,-0.0483,0.0166,0.0296,-0.0031,-0.2985,0.0399,0.0091,0.0142,-0.0357,0.0136,0.0678,0.0049,-0.082,0.0214,0.0012,0.0798,0.0144,0.0178,0.0304,0.0568,0.0332,-0.0253,0.0793,-0.0645,0.0135,0.0575,0.2354,-0.0864,0.0137,-0.0051,-0.0282,0.0378,0.0377,-0.0341,0.021,-0.0063,0.0743,-0.0447,0.029,0.0873,-0.0592,0.0156,0.0568,-0.0219,-0.0025,-0.0242,-0.0097,0.0071,0.1123,-0.0255,-0.0224,-0.0208,0.011,0.0169,-0.0528,0.0484,-0.0054,-0.0159,0.006,0.0344,-0.0429,-0.035,-0.0382,-0.0494,0.0114,-0.051,-0.0756,-0.0075,-0.0199]}
{"key":"[CLIP4Caption ++: Multi-CLIP for Video Caption] This report describes our solution to the VALUE Challenge 2021 in the captioning task. Our solution, named CLIP4Caption++, is built on X-Linear/X-Transformer, which is an advanced model with encoder-decoder architecture. We make the following improvements on the proposed CLIP4Caption++: We employ an advanced encoder-decoder model architecture X-Transformer as our main framework and make the following improvements: 1) we utilize three strong pre-trained CLIP models to extract the text-related appearance visual features. 2) we adopt the TSN sampling strategy for data enhancement. 3) we involve the video subtitle information to provide richer semantic information. 3) we introduce the subtitle information, which fuses with the visual features as guidance. 4) we design word-level and sentence-level ensemble strategies. Our proposed method achieves 86.5, 148.4, 64.5 CIDEr scores on VATEX, YC2C, and TVC datasets, respectively, which shows the superior performance of our proposed CLIP4Caption++ on all three datasets.","layer":2,"vector":[-0.0376,0.0084,-0.0005,-0.0343,0.0701,0.0342,0.0211,0.0222,0.0261,-0.008,-0.013,-0.0915,0.0606,0.0368,0.0536,-0.0152,0.0157,0.0015,-0.0488,0.012,0.0469,-0.0591,0.045,-0.0671,0.0407,-0.0026,-0.0407,-0.0526,-0.046,-0.2224,-0.0226,-0.034,0.0818,0.0076,-0.0281,-0.0354,-0.0596,0.0141,-0.0103,0.0582,0.001,-0.0003,-0.0159,-0.0307,-0.0864,-0.0894,0.0057,0.005,0.0104,-0.0424,0.03,-0.0109,0.0165,0.0583,-0.0174,0.0332,-0.0003,0.0319,0.0498,0.0133,0.0519,0.0231,-0.1762,0.1024,0.0178,0.0212,-0.0783,0.0384,-0.0142,0.0108,-0.0101,0.0157,-0.0028,0.0421,-0.0243,-0.002,0.0229,-0.0401,-0.0328,-0.0077,0.0226,-0.0626,-0.0343,-0.0141,-0.0321,-0.0265,0.0122,-0.0185,0.0147,0.0024,-0.0697,0.0256,-0.0387,0.0336,-0.0714,-0.0345,0.0625,0.0156,-0.0337,0.2179,-0.0399,0.0285,0.0436,-0.0673,0.032,-0.0369,-0.0424,-0.0233,-0.0544,0.0214,-0.0041,0.0049,0.0387,0.0076,-0.0007,0.0078,0.0363,-0.0021,-0.0152,-0.0281,0.0019,0.0246,0.0259,-0.0396,0.0084,-0.0506,0.0702,0.1374,0.0287,-0.0055,0.037,0.0371,-0.038,-0.0019,-0.0097,0.0116,0.0442,-0.0336,0.0282,-0.052,-0.0083,-0.0829,-0.0419,-0.0567,-0.0213,0.1229,-0.0317,0.0183,-0.0171,-0.0154,-0.0007,0.0314,-0.0102,-0.0027,0.0146,0.0066,0.0458,0.0459,-0.0739,0.0347,0.023,-0.0803,-0.0479,0.083,0.0231,-0.0979,-0.028,0.0087,-0.003,-0.0269,0.0089,-0.0016,-0.0326,0.0236,0.1189,0.0355,-0.0809,0.0183,-0.0054,-0.0037,0.0068,-0.088,-0.028,0.0422,0.0217,-0.0624,0.012,-0.0547,0.0529,0.0735,-0.0418,0.0433,-0.0239,0.0181,-0.009,0.0101,0.0174,-0.0083,-0.0068,-0.0355,0.0065,0.0139,-0.0306,0.0144,0.0195,-0.0552,0.016,0.0067,0.0355,0.0451,-0.0163,0.0356,0.0612,-0.0264,-0.0244,-0.0266,0.0315,0.016,0.0019,0.0776,0.0168,-0.0271,-0.026,-0.2359,0.0084,0.0494,-0.0367,0.0452,-0.0445,0.0118,0.0038,0.0708,0.0754,0.0273,-0.0792,0.0099,0.0165,0.0511,0.0467,0.0477,0.0223,0.0058,0.0071,-0.0083,0.0041,-0.0141,-0.0396,0.0344,-0.0456,0.2016,0.0619,0.049,-0.0072,0.0641,0.0731,-0.042,-0.0864,0.048,0.0434,0.0329,0.0122,-0.0638,-0.0212,0.0016,0.0059,-0.0049,-0.1057,-0.0381,-0.0356,-0.0325,0.0347,-0.0459,0.0555,0.0557,-0.0447,0.0127,0.0232,0.0185,-0.0477,-0.0749,0.0404,0.0156,0.0108,-0.0142,-0.0724,0.0431,-0.0286,0.0346,0.0583,-0.0125,-0.0178,-0.0091,-0.0624,-0.0439,0.0366,0.0193,0.0312,0.0356,0.0568,0.0064,-0.0304,-0.0286,-0.0389,0.0685,-0.0283,-0.0194,-0.0307,0.0361,0.0377,0.0145,-0.0035,0.0382,-0.0325,-0.0004,0.0098,-0.0605,-0.0101,0.013,-0.0419,-0.3258,0.026,0.0618,0.0084,0.0036,0.0127,0.0367,-0.015,-0.043,-0.0036,-0.034,0.0552,0.0434,-0.032,-0.0515,0.0199,0.117,-0.0264,0.0308,-0.0134,-0.0103,0.0094,0.2138,-0.023,0.0063,0.0184,-0.0593,-0.0176,0.0718,-0.0155,-0.0109,-0.0277,0.0853,-0.0253,0.0012,0.1086,-0.0363,0.0406,0.0511,0.0365,0.0009,-0.0229,-0.005,-0.0585,0.0725,0.0062,0.0403,-0.0321,-0.0334,0.009,-0.0171,-0.0027,-0.0163,-0.0301,0.0383,0.0529,-0.0385,-0.0442,-0.0268,0.0099,-0.0054,-0.024,-0.0473,0.0446,0.0034]}
{"key":"[Robust Spectral Inference for Joint Stochastic Matrix Factorization] Spectral inference provides fast algorithms and provable optimality for latent topic analysis. But for real data these algorithms require additional ad-hoc heuristics, and even then often produce unusable results. We explain this poor performance by casting the problem of topic inference in the framework of Joint Stochastic Matrix Factorization (JSMF) and showing that previous methods violate the theoretical conditions necessary for a good solution to exist. We then propose a novel rectification method that learns high quality topics and their interactions even on small, noisy data. This method achieves results comparable to probabilistic techniques in several domains while maintaining scalability and provable optimality.","layer":0,"vector":[-0.0458,0.0054,-0.0295,-0.0341,0.0428,0.0382,0.0327,0.0423,0.0429,-0.0197,0.0219,-0.0279,0.0448,0.0681,0.0657,0.047,0.0388,0.0371,-0.0508,0.0044,0.0073,-0.0166,0.0204,-0.0292,0.0259,-0.0159,-0.0494,-0.0885,-0.0397,-0.2344,-0.0258,-0.0049,0.0647,-0.0271,0.0597,-0.0377,-0.0135,0.0644,-0.0142,0.0148,0.0001,0.0258,-0.0316,-0.0198,-0.051,-0.0241,0.011,-0.0026,-0.0566,-0.0172,0.0194,-0.0248,0.0169,0.0164,0.0483,0.0472,0.0683,0.0193,0.046,0.0785,0.0246,0.0486,-0.1793,0.0706,0.052,-0.0209,-0.0395,-0.0059,-0.0008,0.03,-0.0232,0.0548,0.0139,0.0554,0.044,0.0381,0.0156,-0.0475,0.0057,0.0093,-0.0032,-0.0014,-0.0648,0.0089,-0.0292,-0.0623,0.0147,-0.0226,0.0209,0.0038,-0.0547,-0.0459,0.0176,0.0535,-0.0756,-0.0343,0.019,0.024,-0.0032,0.1818,-0.0499,0.0265,0.0638,-0.0262,-0.0238,-0.0226,-0.0685,-0.0184,-0.0209,0.0082,0.0212,-0.0079,0.0259,-0.0681,0.0715,-0.0072,0.1141,0.0559,-0.0049,-0.0346,-0.0387,0.0103,0.037,0.0009,0.0256,-0.0942,0.0323,0.1424,0.0684,0.0215,0.0434,-0.0512,-0.0647,-0.0515,0.0371,0.0065,0.0183,-0.0164,0.0202,0.0342,-0.0346,-0.0669,0.0088,-0.0422,-0.0759,0.1197,-0.0238,-0.021,-0.0329,-0.007,-0.017,0.0073,0.0103,-0.0219,0.0595,0.0303,0.0384,-0.0168,-0.0433,0.0093,-0.0334,-0.0656,-0.006,0.0822,0.0115,-0.0804,-0.0145,-0.017,0.0218,-0.0188,0.0304,0.0322,-0.016,0.0188,0.0702,0.0469,-0.064,0.0336,0.0333,0.0249,0.0233,-0.0697,-0.0477,0.0542,0.0062,-0.0539,0.0021,-0.0269,-0.0062,0.0301,-0.0455,-0.0413,-0.0284,-0.001,-0.0232,-0.0232,-0.0249,-0.0087,-0.0079,-0.034,0.0104,-0.0071,-0.0813,0.0579,-0.0054,0.0197,-0.0268,0.0004,0.0224,0.0308,0.0225,-0.0257,0.0295,-0.0384,0.0005,-0.0152,0.0309,0.0125,-0.0033,0.0295,0.0404,-0.045,-0.0557,-0.2368,-0.0025,-0.0045,0.0009,0.0272,-0.0717,0.0586,-0.0174,0.0674,0.1481,0.0387,-0.0125,-0.0101,0.0086,0.0518,0.0482,0.0182,0.0111,0.0148,0.0037,-0.0316,0.0252,-0.032,-0.0401,0.0748,-0.0248,0.2375,0.0368,-0.0094,-0.0343,0.0324,0.0134,-0.0367,-0.0748,0.0544,0.0183,0.1233,0.0083,-0.0483,-0.0092,0.0105,-0.0025,-0.0108,-0.1126,-0.087,-0.0538,0.0148,0.0193,-0.0123,0.0041,0.0239,-0.0106,0.0339,-0.0201,-0.0295,-0.0579,-0.083,0.0148,-0.072,-0.0133,0.0345,-0.0197,0.0612,-0.0995,0.0396,0.0029,-0.0164,-0.0271,0.0087,-0.0201,-0.0418,0.0616,-0.0226,0.0082,0.0665,0.0255,0.053,-0.0393,-0.0148,-0.012,0.0998,-0.0543,0.0255,-0.0028,0.0442,0.0086,0.0635,-0.0136,0.015,-0.0101,-0.0001,-0.0018,-0.0318,-0.0123,0.0225,-0.0417,-0.2788,0.0328,-0.0027,0.0162,-0.038,-0.0007,0.0205,0.02,-0.0516,0.0054,0.0111,0.0445,0.0368,0.0091,-0.0051,0.0289,0.0592,-0.0214,0.0448,-0.0788,-0.0173,-0.0152,0.2326,-0.0394,0.0561,-0.0029,0.0109,0.0112,0.047,-0.0638,0.0154,-0.0064,0.0792,-0.0351,0.0264,0.0268,0.0003,0.0304,0.0198,0.0001,-0.0266,-0.0149,-0.0169,-0.0431,0.1103,-0.0451,0.0055,-0.0579,0.0042,0.0356,-0.0205,-0.0039,-0.0042,-0.0062,-0.0152,-0.0121,-0.015,-0.0897,0.0317,-0.0317,-0.0461,-0.0537,-0.0414,0.0113,-0.0106]}
{"key":"[GERNERMED -- An Open German Medical NER Model] The current state of adoption of well-structured electronic health records and integration of digital methods for storing medical patient data in structured formats can often considered as inferior compared to the use of traditional, unstructured text based patient data documentation. Data mining in the field of medical data analysis often needs to rely solely on processing of unstructured data to retrieve relevant data. In natural language processing (NLP), statistical models have been shown successful in various tasks like part-of-speech tagging, relation extraction (RE) and named entity recognition (NER). In this work, we present GERNERMED, the first open, neural NLP model for NER tasks dedicated to detect medical entity types in German text data. Here, we avoid the conflicting goals of protection of sensitive patient data from training data extraction and the publication of the statistical model weights by training our model on a custom dataset that was translated from publicly available datasets in foreign language by a pretrained neural machine translation model. The sample code and the statistical model is available at: https://github.com/frankkramer-lab/GERNERMED","layer":0,"vector":[-0.0441,-0.0074,0.0062,0.0092,0.0484,0.0063,0.0477,0.0259,0.0247,-0.0352,-0.018,-0.0515,0.0219,0.0281,-0.0043,0.0302,0.0222,0.0432,-0.0463,0.0602,0.0112,-0.0293,0.0169,-0.0163,-0.006,0.0475,-0.0458,-0.0392,-0.017,-0.2047,-0.0299,-0.0524,0.0535,-0.0069,-0.0062,0.003,-0.0713,0.0645,-0.0471,0.0049,-0.0272,-0.0252,0.0182,-0.0347,0.0337,-0.0351,-0.0165,-0.0241,-0.04,0.0075,-0.0095,-0.0331,0.0284,0.036,0.0542,-0.0097,0.0593,0.004,0.0366,0.0536,0.044,0.059,-0.1722,0.0559,0.0077,0.0289,-0.0547,-0.0171,0.0115,0.0306,0.011,0.0362,0.0115,0.074,-0.0047,0.0245,0.0477,-0.0419,0.0071,0.02,-0.0223,-0.0044,-0.0243,-0.0222,-0.0432,-0.0427,-0.0058,-0.0924,0.0021,-0.0055,-0.0819,-0.0077,0.0042,0.0027,-0.056,-0.0525,0.0206,0.0539,-0.0479,0.1888,-0.0823,0.0203,0.0443,-0.0533,0.0443,-0.0296,-0.0433,-0.0421,-0.0012,-0.037,-0.0089,0.0067,0.0412,-0.0614,0.0576,0.0177,0.0875,-0.0057,-0.0139,0.0023,0.0033,0.0117,-0.0166,-0.0177,0.0267,-0.0062,0.0396,0.1028,0.0365,0.0353,0.067,0.0323,-0.0784,-0.0014,0.0068,0.0351,0.029,-0.0102,-0.0119,-0.0222,-0.0336,-0.1098,-0.0217,-0.0755,-0.0947,0.1291,-0.0611,0.015,-0.0424,-0.0502,-0.0132,0.0681,-0.0161,-0.0503,0.0705,0.0354,0.0706,-0.0113,-0.0292,-0.0051,0.0572,-0.0892,-0.0091,0.1463,0.022,-0.0864,-0.0464,-0.005,0.0436,-0.044,0.0743,0.0543,-0.0383,0.0424,0.0534,0.01,0.001,-0.0472,-0.0308,-0.0108,0.0441,0.0161,-0.0206,0.0888,0.0041,-0.0699,-0.0215,-0.0146,0.0243,0.0517,-0.0412,0.0283,0.004,-0.0093,-0.0232,-0.0438,-0.0621,-0.0009,0.0074,-0.0472,0.038,0.036,-0.0383,0.0466,-0.0025,0.004,-0.0088,-0.0055,0.0365,0.0268,-0.0155,0.0061,0.0736,-0.0075,-0.0353,-0.0193,-0.0073,-0.0057,-0.0083,0.0749,0.0135,-0.0275,-0.0538,-0.2192,0.0105,0.0373,-0.0101,0.0242,-0.0533,0.0805,-0.0197,0.022,0.0854,0.0242,-0.0447,-0.0569,0.0269,-0.0269,0.0274,0.0851,0.0249,-0.0216,-0.0006,0.0357,-0.0077,-0.0164,-0.0725,0.0395,-0.034,0.2238,0.0409,0.0172,-0.0192,0.0424,-0.0197,-0.0087,-0.1326,0.1021,-0.0074,0.0589,-0.0078,-0.036,-0.0301,-0.0524,0.017,0.0046,-0.0551,-0.0481,-0.0365,-0.0223,-0.017,-0.0604,0.0547,-0.0031,-0.0119,0.0269,0.0714,0.0058,-0.0377,-0.0701,0.0058,-0.0676,0.009,0.034,-0.0112,0.0201,-0.0685,0.0326,-0.0258,-0.0543,-0.0211,0.0284,-0.0578,-0.0525,0.1162,-0.0002,-0.019,0.0252,0.0681,-0.0136,-0.0612,-0.0281,-0.0164,0.032,0.0029,0.0797,0.035,0.0172,0.0052,0.096,-0.0251,0.0144,-0.0293,0.0067,0.0177,-0.0411,-0.0046,0.0389,-0.0292,-0.289,0.0458,0.008,0.0311,-0.0196,0.0316,-0.0186,-0.0039,-0.0345,-0.0363,0.0097,-0.0192,0.0365,-0.028,-0.0477,0.0697,0.0771,-0.0488,0.0404,-0.0143,0.0605,0.0225,0.2093,-0.0115,0.0348,0.0145,-0.021,0.0012,0.0295,0.0641,-0.0159,-0.0152,0.0947,-0.0148,0.0929,0.0642,-0.0266,0.0228,0.0602,-0.0258,-0.0166,0.0094,-0.039,-0.0426,0.0742,-0.0076,-0.0177,-0.0317,-0.0165,0.0235,-0.0116,-0.0189,-0.0288,0.0362,0.0285,0.0131,-0.0089,-0.0382,-0.0002,-0.0563,-0.0169,-0.0292,-0.0529,0.0468,-0.0137]}
{"key":"[Event-Driven Models] In Reinforcement Learning we look for meaning in the flow of input/output information. If we do not find meaning, the information flow is not more than noise to us. Before we are able to find meaning, we should first learn how to discover and identify objects. What is an object? In this article we will demonstrate that an object is an event-driven model. These models are a generalization of action-driven models. In Markov Decision Process we have an action-driven model which changes its state at each step. The advantage of event-driven models is their greater sustainability as they change their states only upon the occurrence of particular events. These events may occur very rarely, therefore the state of the event-driven model is much more predictable.","layer":5,"vector":[-0.0492,0.0302,0.0259,-0.0446,-0.0031,0.0384,0.0791,0.0132,0.0548,0.0106,0.0049,-0.0354,0.0198,0.0676,0.0098,-0.0062,-0.0248,0.0263,-0.0402,-0.0217,0.0443,-0.0326,-0.0145,-0.0464,0.0216,0.071,-0.0037,0.0022,-0.0034,-0.2009,-0.0268,-0.0426,0.0286,-0.0397,0.0184,-0.0012,-0.0355,0.0476,-0.0314,0.0472,0.0368,0.0203,0.0224,-0.0757,-0.0149,-0.0681,0.0045,-0.0322,-0.0282,-0.04,-0.0113,-0.0211,0.0131,0.0002,0.0507,0.0333,0.0748,0.0596,0.0555,-0.0102,0.0018,0.0177,-0.1707,0.0639,0.0714,0.0176,-0.0445,-0.0135,0.0484,0.0396,-0.0332,0.0589,0.0152,0.0537,0.0185,-0.034,0.0124,-0.0414,0.0175,0.0232,0.0012,-0.0285,-0.0354,-0.0334,-0.0467,-0.0859,-0.0103,-0.0668,0.0582,0.0166,-0.0615,0.0267,-0.0098,0.0441,-0.0347,0.0385,0.0113,0.0252,-0.0341,0.2104,-0.0207,0.0454,0.0185,0.0068,0.0442,-0.0305,-0.0229,-0.0332,-0.024,-0.0021,-0.0199,-0.012,0.0414,-0.0591,0.0207,0.0127,0.0454,0.0541,-0.0159,0.0094,0.0228,0.012,0.0651,-0.0366,-0.0021,-0.0859,0.056,0.152,0.0145,0.0186,0.0617,-0.0623,-0.0833,-0.014,0.0411,0.0123,0.0224,-0.0453,-0.0054,-0.0234,-0.0238,0.0133,0.0041,-0.1171,-0.0666,0.1142,-0.0126,-0.009,-0.0272,-0.0004,-0.0321,0.0509,0.005,-0.0435,0.0062,0.0403,0.029,0.0333,-0.072,0.0111,-0.0318,-0.0581,-0.03,0.104,-0.0315,-0.0519,-0.0374,-0.042,0.0304,-0.0209,0.0354,0.0652,-0.0252,0.0197,0.0766,0.029,-0.0649,-0.0132,-0.0071,-0.0009,0.0671,-0.055,-0.0339,0.0079,0.0312,-0.0468,-0.0061,-0.0385,0.0223,0.0243,-0.0098,0.0108,0.0204,-0.0142,-0.0329,-0.0421,-0.0039,-0.0335,-0.0041,-0.0588,-0.0282,-0.0086,-0.0605,0.0188,-0.0449,0.0016,-0.0571,-0.0255,0.0175,-0.0276,-0.0621,0.016,0.0579,-0.0107,-0.0442,0.0048,0.0366,0.0426,-0.0268,-0.0012,0.0035,-0.0098,0.0041,-0.2508,-0.0098,-0.0064,-0.0088,0.0689,-0.0371,-0.0142,-0.0364,0.0088,0.0572,0.0618,-0.0712,-0.0291,0.011,-0.0117,0.0659,0.014,0.0051,-0.0516,0.0582,-0.0383,-0.0119,-0.0266,-0.1086,0.0337,-0.0146,0.2243,0.0439,0.0443,0.0007,0.0233,0.0307,-0.0523,-0.0994,0.0697,0.0364,0.043,0.0087,-0.0251,-0.0492,-0.0184,0.0398,-0.0227,-0.0716,-0.0262,0.0069,0.0268,0.031,-0.0371,0.0081,0.0377,-0.0259,0.0343,0.0205,-0.0583,0.0027,-0.0393,0.0124,-0.0213,0.0286,0.0066,-0.0193,0.0245,-0.0426,0.0548,0.0038,-0.0044,-0.1051,0.0126,-0.0204,-0.0247,0.069,0.0058,-0.0257,0.0385,0.0108,-0.0027,-0.0444,-0.0329,-0.0125,0.0448,-0.0344,0.0209,-0.0004,0.0558,-0.0282,0.0568,-0.0312,-0.0155,-0.0311,0.031,-0.0261,-0.0649,-0.0084,0.0467,-0.0407,-0.3298,0.0565,0.0558,0.0612,0.0002,0.0511,0.0136,0.0147,-0.0591,0.0102,0.0152,0.0377,0.0541,0.0372,-0.0206,0.0944,0.0762,-0.037,0.0133,-0.0553,0.0334,0.0609,0.1975,0.0005,0.0655,-0.0119,-0.0296,-0.0164,0.0315,0.0192,0.0374,0.0345,0.0936,-0.0185,0.0738,0.0385,-0.0076,0.0459,-0.0037,-0.0096,-0.0208,0.0124,-0.0102,-0.0391,0.1001,-0.0258,0.0007,-0.0399,-0.0235,0.0532,-0.0126,-0.0136,-0.0294,-0.0173,0.0544,0.0384,-0.0661,-0.0368,-0.0256,-0.0252,0.07,-0.0595,0.0338,0.0149,-0.0193]}
{"key":"[Pursuing Sources of Heterogeneity in Modeling Clustered Population] Researchers often have to deal with heterogeneous population with mixed regression relationships, increasingly so in the era of data explosion. In such problems, when there are many candidate predictors, it is not only of interest to identify the predictors that are associated with the outcome, but also to distinguish the true sources of heterogeneity, i.e., to identify the predictors that have different effects among the clusters and thus are the true contributors to the formation of the clusters. We clarify the concepts of the source of heterogeneity that account for potential scale differences of the clusters and propose a regularized finite mixture effects regression to achieve heterogeneity pursuit and feature selection simultaneously. As the name suggests, the problem is formulated under an effects-model parameterization, in which the cluster labels are missing and the effect of each predictor on the outcome is decomposed to a common effect term and a set of cluster-specific terms. A constrained sparse estimation of these effects leads to the identification of both the variables with common effects and those with heterogeneous effects. We propose an efficient algorithm and show that our approach can achieve both estimation and selection consistency. Simulation studies further demonstrate the effectiveness of our method under various practical scenarios. Three applications are presented, namely, an imaging genetics study for linking genetic factors and brain neuroimaging traits in Alzheimer's disease, a public health study for exploring the association between suicide risk among adolescents and their school district characteristics, and a sport analytics study for understanding how the salary levels of baseball players are associated with their performance and contractual status.","layer":0,"vector":[0.0009,-0.0063,0.0197,0.0094,0.0564,0.062,0.0395,0.0171,0.0148,-0.016,0.0472,-0.0638,0.0031,0.0238,0.0056,0.0281,-0.0129,0.0505,-0.0647,-0.0038,-0.0435,-0.0422,0.0043,-0.0352,0.0395,0.0012,-0.0326,-0.0147,-0.0643,-0.2489,0.0293,0.0056,0.0895,-0.0077,0.0278,0.0047,-0.011,0.0469,-0.0375,0.0579,0.002,0.0176,-0.0398,-0.0236,-0.033,-0.0346,-0.0408,0.0298,-0.045,-0.0301,0.0202,-0.0442,0.0038,0.0452,0.0261,0.011,0.0437,0.0313,0.0231,0.0312,0.0323,0.0356,-0.191,0.0368,0.0861,0.0583,-0.0614,0.0144,0.0489,0.0502,0.0001,0.0967,0.022,-0.0016,0.0115,-0.0072,-0.0111,-0.0249,0.0036,0.0353,0.0297,-0.0369,-0.044,0.0045,-0.0108,-0.0386,-0.0025,-0.074,0.0291,-0.0184,-0.0288,0.018,-0.0385,0.0234,-0.0581,-0.0337,0.056,-0.0006,0.0008,0.2001,-0.0348,0.0626,0.0418,-0.0155,0.0213,-0.0373,-0.0362,-0.0301,-0.0296,0.0221,0.0159,-0.0028,0.0084,-0.0231,-0.0081,-0.0287,0.0514,0.0186,0.018,-0.0329,-0.0001,0.0123,0.0342,-0.0222,0.0503,-0.0688,0.007,0.1575,0.0439,-0.0228,0.043,-0.0024,-0.0498,-0.0174,0.0174,-0.0011,-0.004,0.0244,0.0302,0.0218,-0.0097,-0.0729,0.0373,-0.0792,-0.07,0.1865,-0.0294,0.0137,-0.0351,-0.0069,-0.0159,0.0405,-0.0891,0.0129,-0.0187,0.0456,0.035,-0.0107,-0.028,-0.0027,-0.0244,-0.0427,-0.0305,0.1071,0.0007,-0.0642,-0.0271,0.0349,0.0069,-0.0082,0.0307,0.061,-0.0171,0.0478,0.0487,0.026,-0.0687,0.0293,-0.0139,0.0009,0.0304,-0.0359,-0.0696,0.0627,0.0282,-0.0669,-0.0024,-0.0386,0.0149,0.0301,-0.0495,-0.0014,-0.0253,-0.023,0.014,-0.0336,-0.0244,-0.0305,0.0458,-0.0499,-0.0182,0.0337,-0.0226,0.0221,0.0252,0.0318,-0.0036,-0.0327,0.0476,0.0102,-0.0608,-0.0306,0.1005,0.0022,-0.0076,0.0575,0.0212,0.0577,0.0709,0.0784,0.0554,-0.0271,-0.0615,-0.2182,-0.0538,0.0065,0.0075,0.0182,-0.0475,0.0264,0.0109,0.0815,0.1329,0.0061,-0.0127,-0.0682,0.079,-0.0103,0.0165,-0.0043,0.0191,-0.0315,0.0408,-0.0208,-0.0034,-0.0397,-0.0388,0.0634,-0.0323,0.1787,0.0476,-0.0186,-0.016,-0.0074,0.0298,-0.014,-0.068,0.0536,0.022,0.0524,0.0032,-0.0728,-0.038,-0.0564,0.023,-0.0114,-0.0734,-0.0559,-0.0103,-0.0042,0.0375,-0.0926,0.0315,0.0431,-0.0438,0.0726,-0.063,0.024,-0.0438,-0.1077,0.038,-0.0485,0.01,0.0168,-0.0558,0.0125,-0.1112,0.0614,-0.0017,-0.054,-0.0078,-0.0067,-0.0519,0.0194,0.092,-0.017,-0.007,0.0387,-0.007,-0.003,-0.0311,-0.0344,0.0054,0.0599,-0.0695,0.0262,0.0566,-0.0223,-0.0226,0.0812,-0.0386,0.0427,-0.0292,-0.0092,-0.009,-0.0392,-0.0057,0.0044,0.0146,-0.2673,0.0238,0.015,-0.0071,0.0241,-0.0011,0.0397,0.0215,-0.0123,-0.0086,0.004,0.016,0.0479,-0.0485,-0.0139,0.0653,0.0741,-0.0752,0.05,-0.0872,-0.0061,0.0523,0.1943,-0.0491,0.0291,0.0159,-0.0454,0.0017,-0.0077,-0.0284,0.0036,0.0281,0.081,-0.0465,0.041,0.065,-0.0382,0.0225,-0.0097,-0.0143,0.0191,-0.0058,-0.0458,-0.0372,0.1271,0.0103,-0.0223,-0.0572,-0.0369,0.0247,-0.0346,0.0056,-0.0421,-0.0002,0.0315,0.0075,-0.0396,-0.0195,-0.0076,-0.0212,0.0111,0.008,-0.0762,0.0047,-0.0195]}
{"key":"[On Newton Screening] Screening and working set techniques are important approaches to reducing the size of an optimization problem. They have been widely used in accelerating first-order methods for solving large-scale sparse learning problems. In this paper, we develop a new screening method called Newton screening (NS) which is a generalized Newton method with a built-in screening mechanism. We derive an equivalent KKT system for the Lasso and utilize a generalized Newton method to solve the KKT equations. Based on this KKT system, a built-in working set with a relatively small size is first determined using the sum of primal and dual variables generated from the previous iteration, then the primal variable is updated by solving a least-squares problem on the working set and the dual variable updated based on a closed-form expression. Moreover, we consider a sequential version of Newton screening (SNS) with a warm-start strategy. We show that NS possesses an optimal convergence property in the sense that it achieves one-step local convergence. Under certain regularity conditions on the feature matrix, we show that SNS hits a solution with the same signs as the underlying true target and achieves a sharp estimation error bound with high probability. Simulation studies and real data analysis support our theoretical results and demonstrate that SNS is faster and more accurate than several state-of-the-art methods in our comparative studies.","layer":0,"vector":[-0.0236,0.0375,0.0515,0.0082,0.0171,0.0265,0.0041,0.0252,0.0282,-0.0435,0.0023,-0.0407,0.0408,0.055,-0.0029,0.0252,0.0794,0.0313,0.0036,0.0305,0.0502,-0.0515,-0.0259,-0.069,0.0717,0.0281,-0.0496,-0.0396,-0.0386,-0.2853,0.0259,-0.0129,0.0663,-0.0011,0.0481,0.0112,-0.0174,0.0625,-0.0569,0.049,-0.0061,-0.0219,-0.0353,-0.0546,-0.0104,-0.0147,0.0097,-0.0634,-0.0414,-0.0175,0.0122,-0.0592,0.0408,0.0478,0.0251,0.0036,0.0358,0.0243,0.0312,0.0177,-0.009,0.0111,-0.1908,0.0394,0.0641,0.031,-0.0198,-0.0022,0.0244,0.1055,-0.0272,0.0673,0.0291,0.0398,0.0082,-0.0116,-0.0047,0.0175,0.0051,0.0326,0.0591,-0.0433,-0.0343,-0.0015,-0.0002,-0.0136,-0.0057,-0.0672,0.0771,-0.0003,-0.0283,0.0039,-0.0426,0.0053,-0.1006,-0.0367,0.0398,0.0463,-0.0396,0.1997,-0.0246,0.0402,0.0145,-0.028,0.0208,-0.0467,-0.0478,-0.0469,0.0099,0.0175,0.0052,0.0126,0.0004,-0.0179,-0.011,-0.0231,0.0444,0.0062,0.0027,0.0003,-0.0147,0.0044,0.0542,-0.017,0.0425,-0.0628,-0.0134,0.1261,0.0312,0.0273,0.0512,-0.0257,-0.0481,-0.0556,0.0042,0.0397,0.0133,0.0565,0.0324,-0.0197,-0.0315,-0.0319,0.0325,-0.0665,-0.0449,0.1043,-0.0646,0.0332,-0.047,-0.0829,-0.0037,-0.0145,-0.0482,-0.0325,0.0163,-0.0088,0.0361,0.0333,-0.0586,0.0284,-0.0332,-0.0443,-0.0498,0.0276,-0.0019,-0.067,0.0008,-0.0382,-0.0125,-0.0079,0.0491,0.0218,-0.0501,0.033,0.0644,0.0076,-0.0487,0.0225,0.0192,0.0385,0.0105,-0.0227,-0.0662,0.022,0.0846,-0.0496,0.0298,-0.04,0.0378,0.0031,-0.0768,-0.0098,-0.0282,-0.0166,0.0088,-0.0216,-0.02,-0.005,0.0116,-0.0048,0.0289,-0.0059,-0.0506,0.0171,0.0055,0.0345,-0.0225,-0.0229,0.0338,0.0377,-0.0718,0.0214,0.0818,-0.0374,-0.017,0.0138,0.0022,0.0191,0.0138,0.0426,0.0281,-0.0344,-0.0545,-0.2432,-0.0645,0.0229,0.0012,0.0224,-0.0742,0.059,-0.0123,0.0625,0.0864,0.0461,0.0162,-0.0323,0.0376,-0.0047,0.0311,0.0239,0.0212,-0.0055,0.0154,-0.0212,-0.0295,0.0188,-0.0364,0.0525,0.0221,0.2194,0.0548,0.0366,-0.019,0.0437,0.0322,0.0204,-0.0947,0.0481,0.0327,0.0801,-0.0007,-0.049,-0.0083,0.0127,0.0081,-0.0337,-0.0641,-0.028,-0.0228,-0.0361,0.0414,-0.0101,0.0237,0.0457,-0.007,0.043,-0.0833,0.0441,-0.0479,-0.0901,0.0175,-0.0415,0.014,0.0152,-0.0751,-0.0092,-0.0266,0.06,-0.014,-0.0106,-0.0065,0.027,-0.0267,-0.0296,0.0274,0.0271,0.0024,0.0712,0.032,0.0308,-0.0154,-0.0398,-0.0082,0.0703,-0.0394,0.0021,-0.0289,0.0181,0.0196,0.1071,0.0012,0.0112,-0.0288,-0.0369,-0.017,-0.0714,-0.0074,0.0367,0.017,-0.2886,0.0424,-0.0083,-0.0002,-0.015,-0.0179,0.0622,-0.0318,-0.0473,-0.0153,-0.0414,0.0305,0.017,-0.0127,-0.0014,0.0053,0.0614,-0.0289,0.0445,-0.1395,0.027,0.0534,0.2116,-0.0577,0.0268,0.0428,0.014,-0.0439,0.0198,0.0147,0.0305,0.0138,0.0329,-0.0765,0.041,0.0874,-0.0652,0.0164,0.0198,-0.0156,0.0581,-0.0207,-0.0712,-0.0115,0.0805,-0.0273,-0.0299,-0.0522,-0.0042,0.0159,-0.0552,0.0186,0.0233,0.0043,0.0233,0.0116,-0.0456,-0.0111,-0.0499,-0.0528,-0.0011,-0.0377,-0.0593,0.0211,0.036]}
{"key":"[Knowledge Cross-Distillation for Membership Privacy] A membership inference attack (MIA) poses privacy risks for the training data of a machine learning model. With an MIA, an attacker guesses if the target data are a member of the training dataset. The state-of-the-art defense against MIAs, distillation for membership privacy (DMP), requires not only private data for protection but a large amount of unlabeled public data. However, in certain privacy-sensitive domains, such as medicine and finance, the availability of public data is not guaranteed. Moreover, a trivial method for generating public data by using generative adversarial networks significantly decreases the model accuracy, as reported by the authors of DMP. To overcome this problem, we propose a novel defense against MIAs that uses knowledge distillation without requiring public data. Our experiments show that the privacy protection and accuracy of our defense are comparable to those of DMP for the benchmark tabular datasets used in MIA research, Purchase100 and Texas100, and our defense has a much better privacy-utility trade-off than those of the existing defenses that also do not use public data for the image dataset CIFAR10.","layer":9,"vector":[-0.02,-0.0354,-0.0082,-0.0047,0.0428,0.024,0.0737,-0.004,0.01,-0.0268,0.0248,-0.0399,0.0546,0.0589,-0.0116,0.0223,-0.015,0.0182,-0.0355,0.0521,0.0125,-0.0632,-0.0067,-0.0477,-0.0033,0.0226,-0.0633,-0.0448,-0.065,-0.2247,0.0361,-0.0779,0.0256,0.0081,0.0085,-0.0551,-0.0394,0.0649,-0.0528,0.0511,0.0334,0.0369,-0.0107,-0.0503,0.0003,-0.0127,-0.0299,0.0001,-0.0218,-0.0456,0.0454,-0.02,0.0127,0.0792,0.0328,0.0473,0.0582,0.0273,0.0239,0.0669,0.0149,0.0255,-0.1714,0.0462,0.0105,0.0538,-0.0209,-0.0227,-0.004,0.0028,0.0274,0.0503,0.0178,0.0345,0.0198,0.0152,-0.0523,-0.0123,-0.0422,-0.0103,0.0134,-0.0093,-0.0135,0.0114,-0.0093,-0.0273,0.03,-0.0517,0.0247,0.0055,-0.0516,-0.0285,-0.0104,0.0353,-0.0289,-0.0141,0.0029,0.039,-0.0614,0.2055,-0.056,0.0154,0.0108,-0.0332,-0.0143,-0.0139,-0.0281,-0.0256,-0.045,-0.0069,0.0175,-0.0079,0.0245,-0.0173,0.0119,-0.0002,0.0808,0.0152,-0.0372,-0.0098,0.0106,0.0127,0.0281,0.0478,0.0203,-0.0666,0.0154,0.1402,0.0155,0.0356,0.0393,-0.0299,-0.0159,-0.0338,0.0305,0.0124,0.0135,0.0012,0.0161,-0.0292,-0.0658,-0.0335,0.0338,-0.0502,-0.0239,0.1173,0.0356,0.0432,-0.0206,-0.021,0.0097,0.0085,-0.0348,-0.0486,-0.0056,0.0018,0.0086,0.0668,-0.0177,-0.0128,-0.0197,-0.0477,-0.0284,0.1152,0.0306,-0.0963,0.0048,0.0243,0.0082,-0.0132,0.0225,0.0293,-0.0571,0.0558,0.074,0.0112,-0.0624,-0.0193,-0.003,0.0212,-0.001,-0.0899,-0.0396,-0.0124,0.0153,-0.0085,-0.001,-0.0419,0.0065,0.0535,-0.0614,0.0575,-0.0955,-0.0093,-0.057,-0.0349,-0.0035,-0.0032,0.0341,-0.0322,-0.0285,0.0149,-0.0394,-0.0048,-0.0242,0.044,0.0419,0.01,0.065,0.0078,-0.0422,0.0064,0.0372,-0.0457,-0.0171,0.0153,0.0106,0.0613,-0.0011,0.0529,0.0506,-0.0192,-0.0293,-0.257,0.0208,0.0022,-0.0079,0.0435,-0.0785,0.0635,-0.0157,0.0232,0.0503,0.0757,-0.0142,-0.0299,0.0162,-0.0312,0.05,0.0654,0.0286,-0.0233,-0.0164,-0.0064,0.0448,-0.008,-0.0792,0.0346,0.0685,0.2275,0.0542,0.0316,0.0078,-0.0012,0.0528,-0.0362,-0.1314,0.0293,0.013,0.0029,-0.0056,-0.0406,-0.0425,-0.01,0.0398,-0.03,-0.084,-0.0107,-0.0617,-0.0276,0.044,-0.0775,0.0432,0.0552,-0.0227,0.0823,0.0029,0.029,-0.0587,-0.0896,0.0433,-0.0692,0.0539,0.0254,-0.0033,0.0005,-0.1036,0.0663,-0.0663,-0.0695,-0.0718,0.0827,-0.0012,-0.0102,0.0652,0.0254,0.0241,0.0389,0.04,0.0335,-0.0483,-0.1058,-0.0267,0.0913,0.002,0.0229,0.0088,0.0233,-0.0105,0.0702,0.0605,0.0377,-0.062,0.0,0.01,-0.0815,-0.0366,0.0561,0.0166,-0.2902,0.0047,-0.0351,0.055,-0.0318,0.0138,0.0627,0.0027,-0.0606,-0.0411,0.0254,0.0488,0.0349,-0.0136,-0.0083,0.0387,0.0309,-0.0531,0.0135,-0.0175,0.0337,0.0307,0.2113,0.0063,0.0043,-0.0211,-0.0204,0.0241,0.0142,0.0107,-0.0375,0.0069,0.0849,-0.0061,0.0016,0.0548,-0.0528,0.0185,0.0387,-0.0305,-0.002,-0.052,-0.0541,0.0152,0.0612,0.0235,-0.0185,-0.0288,0.004,-0.016,-0.0069,0.0203,-0.047,0.0246,0.0463,0.0196,-0.052,-0.0409,0.0195,-0.034,-0.0272,-0.026,-0.035,0.029,-0.0119]}
{"key":"[SampleFix: Learning to Generate Functionally Diverse Fixes] Automatic program repair holds the potential of dramatically improving the productivity of programmers during the software development process and correctness of software in general. Recent advances in machine learning, deep learning, and NLP have rekindled the hope to eventually fully automate the process of repairing programs. However, previous approaches that aim to predict a single fix are prone to fail due to uncertainty about the true intend of the programmer. Therefore, we propose a generative model that learns a distribution over potential fixes. Our model is formulated as a deep conditional variational autoencoder that can efficiently sample fixes for a given erroneous program. In order to ensure diverse solutions, we propose a novel regularizer that encourages diversity over a semantic embedding space. Our evaluations on common programming errors show for the first time the generation of diverse fixes and strong improvements over the state-of-the-art approaches by fixing up to 45% of the erroneous programs. We additionally show that for the 65% of the repaired programs, our approach was able to generate multiple programs with diverse functionalities.","layer":5,"vector":[-0.0486,-0.0338,0.0224,-0.0542,0.0241,-0.0012,-0.0131,0.0063,0.024,-0.0072,-0.0289,-0.0442,0.0426,0.0502,0.019,0.0282,-0.0002,0.0969,-0.0581,-0.0205,0.0083,-0.0274,0.0046,-0.0551,-0.0041,0.0064,-0.0192,-0.0647,-0.0226,-0.2419,-0.002,-0.018,0.0713,-0.0279,0.0606,0.0052,-0.0789,0.0066,-0.0336,0.0728,0.0239,0.0229,-0.0172,-0.059,-0.0308,-0.0538,-0.0428,-0.0155,-0.0346,-0.0115,0.0157,-0.0728,0.0246,0.0671,0.0324,0.0355,0.08,0.0503,0.0294,0.0969,0.047,0.0554,-0.142,0.0382,0.0742,0.0381,-0.0459,-0.0558,-0.0024,0.0753,-0.0411,0.0417,-0.0076,0.0646,0.0216,0.0156,0.0044,-0.0083,-0.0497,0.0404,-0.0107,-0.0613,-0.0147,-0.013,0.0033,-0.0373,0.0583,0.0032,0.0731,0.0227,-0.0265,-0.0082,0.0157,0.034,-0.03,0.0391,0.0294,0.0186,-0.0738,0.2337,-0.045,0.0041,0.0612,-0.027,0.0142,-0.0431,-0.0206,0.0239,-0.019,-0.0596,0.0036,0.008,0.0511,-0.0614,-0.0047,-0.0,0.0635,0.0076,0.0095,0.0034,-0.0128,0.0038,0.015,-0.031,0.0086,-0.049,0.0161,0.1523,0.0183,0.0425,0.0332,0.0013,-0.0446,-0.0294,0.0345,0.0199,-0.0313,0.0023,0.0206,-0.0117,-0.0223,-0.0257,-0.0405,-0.0766,-0.0691,0.1027,-0.0589,0.0296,-0.0366,-0.0562,-0.0204,0.0151,-0.0381,-0.0042,0.0788,0.0538,0.0691,0.0007,-0.0401,0.0611,0.0128,-0.0249,-0.0662,0.0952,0.0043,-0.0459,-0.0269,0.0079,-0.0188,0.018,0.0331,0.0407,0.016,0.0252,0.0422,0.0298,-0.0667,0.0012,0.029,0.033,0.0553,-0.032,-0.0148,0.0606,0.0403,-0.0811,0.0232,-0.0203,0.0312,0.0106,0.0028,0.032,0.0114,-0.0515,0.0118,-0.0257,-0.0441,-0.0133,0.0068,-0.0284,-0.0118,0.0256,-0.036,0.0266,-0.0446,0.0244,-0.0263,-0.0308,0.0582,0.0042,-0.0674,-0.0163,0.0639,0.0297,-0.0522,-0.0056,-0.0199,0.0182,0.0073,0.0419,0.0047,-0.0414,-0.0105,-0.2222,-0.0087,-0.0031,-0.0099,0.0443,-0.0506,0.0137,-0.0181,0.0325,0.0759,0.0107,-0.0591,0.0077,0.0402,-0.0133,0.0575,0.0243,0.0504,-0.0364,-0.0183,-0.0007,0.0,0.0006,-0.115,0.0534,-0.0144,0.2421,0.0369,0.0596,-0.0604,0.0344,0.0161,0.0076,-0.1126,0.091,0.0685,0.0404,-0.0103,-0.0323,0.0156,-0.0056,0.0237,-0.0581,-0.1341,-0.0153,-0.0716,-0.0716,-0.041,-0.0445,0.0285,0.018,-0.035,0.0117,0.0275,-0.0027,-0.0118,-0.0985,0.0273,-0.0512,-0.0054,0.0273,-0.056,0.0181,-0.0477,0.0735,0.0069,-0.0042,-0.0727,0.0325,-0.0462,-0.0189,0.0802,-0.0423,-0.0167,0.0227,-0.0033,0.011,-0.0049,-0.0625,-0.0658,0.0462,-0.0253,0.0318,-0.0048,0.0317,0.0131,0.047,0.0007,0.0586,-0.0197,0.013,0.0252,-0.0499,-0.0284,0.0376,0.0092,-0.261,-0.0023,0.0104,-0.0067,-0.0466,0.0335,0.0398,-0.0,-0.0276,-0.0224,-0.0141,0.0143,0.0303,-0.0494,0.0373,0.0283,0.1232,-0.0706,0.0624,-0.0744,-0.0005,0.083,0.2232,-0.0277,0.0083,0.0143,0.0155,-0.0148,0.0699,-0.0154,-0.0142,0.0103,0.0586,-0.0104,0.0229,0.0636,-0.0198,0.023,0.0104,-0.0203,-0.0425,-0.0159,-0.0182,-0.0382,0.0579,-0.0102,-0.0068,-0.0235,-0.0133,0.0243,-0.0202,0.012,-0.006,-0.0029,0.0195,0.01,-0.0503,-0.0339,-0.0524,0.0016,0.0184,-0.0419,0.015,0.0288,-0.0244]}
{"key":"[Move As You Like: Image Animation in E-Commerce Scenario] Creative image animations are attractive in e-commerce applications, where motion transfer is one of the import ways to generate animations from static images. However, existing methods rarely transfer motion to objects other than human body or human face, and even fewer apply motion transfer in practical scenarios. In this work, we apply motion transfer on the Taobao product images in real e-commerce scenario to generate creative animations, which are more attractive than static images and they will bring more benefits. We animate the Taobao products of dolls, copper running horses and toy dinosaurs based on motion transfer method for demonstration.","layer":1,"vector":[-0.059,-0.0365,0.0604,-0.0083,0.0127,0.0534,0.0183,-0.002,-0.0298,0.0069,0.0338,-0.0309,0.0214,0.0203,-0.0033,-0.0139,0.0322,0.0185,-0.0544,0.0487,0.0507,-0.0191,-0.0021,-0.047,-0.0177,-0.0076,-0.0454,-0.021,-0.028,-0.1797,-0.0082,-0.037,0.0062,0.0076,0.0124,-0.0202,-0.0173,0.0503,-0.0344,0.0206,0.0074,0.0227,-0.055,-0.0641,0.0152,-0.0427,-0.0238,0.0138,-0.0031,-0.016,0.0149,-0.0411,0.0137,0.0301,-0.0217,0.0543,0.0679,0.0027,0.0625,0.0154,0.0748,0.0532,-0.1546,0.0936,0.0314,0.0093,-0.0088,-0.0019,0.0165,0.0629,-0.0554,0.0149,0.0233,0.0608,0.0138,-0.0239,0.0121,-0.0461,-0.0587,-0.0147,-0.0145,-0.0263,-0.0225,-0.0153,-0.0376,0.0115,0.0459,-0.0732,0.0313,0.0314,-0.0373,0.0182,-0.0541,-0.0009,-0.0407,-0.0241,-0.017,0.0053,-0.0222,0.2393,-0.0191,0.0421,0.0826,-0.039,0.0116,-0.0513,-0.0269,-0.0051,-0.0187,0.0423,-0.0423,0.004,0.0223,-0.0427,0.0194,0.0364,-0.0276,0.0127,0.0067,-0.038,-0.0019,0.0612,0.0417,-0.0491,-0.0037,-0.0973,0.0716,0.1251,0.0426,0.0282,0.0328,-0.0386,-0.0246,-0.0135,0.014,0.016,-0.0183,0.0138,-0.0153,0.0021,-0.0024,-0.0481,0.0172,-0.1055,-0.0055,0.0707,0.0412,0.0685,-0.0468,0.0214,-0.0152,0.0248,-0.0389,0.0138,-0.0271,0.0202,0.0728,0.0807,-0.0592,-0.0136,-0.0244,-0.0672,-0.0362,0.0467,0.0084,-0.1146,-0.0098,0.0136,0.006,-0.0091,-0.009,0.0156,-0.0663,0.0079,0.1136,0.0465,-0.0537,-0.0001,-0.0266,0.0316,0.0764,-0.0677,-0.0278,0.0373,0.0151,-0.0401,-0.0076,-0.0017,0.0344,0.0653,0.0139,0.0153,-0.0781,0.0057,-0.029,-0.0044,-0.0118,-0.0094,0.02,-0.028,0.063,-0.0052,-0.0555,0.013,0.007,0.0207,-0.0224,-0.0227,0.0112,0.0622,-0.039,0.0229,0.0533,0.0045,-0.0306,0.0139,0.0028,0.0463,0.0165,0.0263,0.037,-0.0521,-0.0087,-0.2862,0.0274,0.0102,-0.0126,0.048,-0.04,0.0349,-0.043,0.0611,0.0298,0.0588,-0.046,-0.0212,0.0452,-0.0092,0.0449,-0.0074,0.0369,0.0037,0.0046,-0.0264,-0.001,0.0013,-0.0821,0.0583,-0.0314,0.2163,0.0504,0.0157,-0.0206,0.0374,0.0175,-0.0444,-0.0933,0.0692,0.0283,0.077,-0.0621,-0.0009,-0.0231,-0.0377,0.0097,0.0245,-0.0727,0.0098,0.0063,-0.0476,0.0131,-0.0318,0.043,0.0192,-0.0319,0.027,0.0234,-0.0643,-0.0418,-0.0204,0.0504,-0.0389,0.0669,-0.0481,-0.0672,0.0489,-0.0375,0.0607,-0.0201,0.0037,-0.0338,0.0465,0.007,-0.0364,0.1201,0.0185,-0.0172,0.0745,0.0303,0.0222,-0.0065,-0.0319,-0.0412,0.0129,0.0008,0.0101,0.0781,0.0091,-0.0126,0.0399,-0.088,-0.003,-0.0511,0.0323,-0.0322,-0.074,0.0022,0.0118,0.0026,-0.3132,0.0205,0.0049,0.0676,-0.031,0.0277,0.053,0.0298,-0.0225,-0.0074,-0.0212,-0.0306,0.0561,-0.0276,0.0079,0.0223,0.0486,-0.0529,0.0642,-0.0675,-0.0079,0.0036,0.2444,-0.0038,-0.0084,-0.0089,-0.019,-0.0121,0.0483,-0.0315,0.0247,0.0252,0.0668,-0.0529,0.0045,0.0351,-0.0671,0.0482,-0.0044,-0.0103,-0.0402,0.0112,-0.0476,-0.0271,0.0906,0.0169,-0.0551,-0.0057,-0.0183,0.0337,-0.0585,-0.0153,-0.0144,0.003,0.025,0.05,-0.0476,-0.0032,-0.0682,-0.0263,0.0431,-0.0662,-0.0291,0.0264,0.0264]}
{"key":"[Rank-one matrix estimation: analytic time evolution of gradient descent dynamics] We consider a rank-one symmetric matrix corrupted by additive noise. The rank-one matrix is formed by an $n$-component unknown vector on the sphere of radius $\\sqrt{n}$, and we consider the problem of estimating this vector from the corrupted matrix in the high dimensional limit of $n$ large, by gradient descent for a quadratic cost function on the sphere. Explicit formulas for the whole time evolution of the overlap between the estimator and unknown vector, as well as the cost, are rigorously derived. In the long time limit we recover the well known spectral phase transition, as a function of the signal-to-noise ratio. The explicit formulas also allow to point out interesting transient features of the time evolution. Our analysis technique is based on recent progress in random matrix theory and uses local versions of the semi-circle law.","layer":1,"vector":[-0.0931,-0.019,0.039,-0.0047,-0.0078,0.0248,0.0091,0.0207,0.0928,0.0054,0.0578,-0.0409,0.0353,0.0478,0.0135,0.0237,0.0059,0.0223,-0.0483,-0.0148,0.0198,-0.0026,-0.0239,-0.0582,0.0715,0.0048,-0.0336,-0.024,-0.043,-0.2441,0.0414,-0.0271,0.0669,-0.0205,0.051,-0.0642,-0.0067,0.0506,-0.0052,0.0108,0.0072,0.0477,-0.0562,-0.0049,-0.0314,-0.0436,-0.0014,-0.016,-0.0513,-0.0359,-0.0219,0.0088,0.0248,0.0013,0.0545,-0.0179,0.0686,0.0423,0.0269,0.0507,0.0134,0.0003,-0.1853,0.0497,0.0683,0.0024,-0.0114,-0.0716,0.0142,0.0346,0.0032,0.0329,0.0454,0.0192,0.0259,-0.0329,-0.0143,-0.0168,0.026,0.0289,0.0455,-0.0249,-0.0419,-0.0053,-0.0377,-0.0379,0.0437,-0.0208,0.0171,0.0255,-0.0569,0.0026,-0.0353,0.0304,-0.0793,-0.0044,0.0514,0.0505,0.0065,0.1954,-0.0595,0.0461,0.0386,-0.0043,-0.0186,-0.0336,-0.0175,-0.0422,0.0116,-0.0183,-0.0085,-0.0227,0.0383,-0.0467,0.0314,-0.0289,0.0471,0.0837,-0.0167,-0.0057,-0.024,-0.0056,0.0571,-0.0521,0.0112,-0.0349,0.0047,0.1397,0.0409,0.0568,0.0419,-0.0147,-0.0397,-0.049,0.0139,0.0253,-0.0061,0.0188,0.0405,-0.0071,-0.0623,-0.0884,0.0235,-0.0778,-0.0245,0.109,-0.013,0.0277,-0.0492,-0.0469,-0.0237,0.0389,-0.0472,-0.0074,0.0391,0.0271,0.0431,0.0466,-0.0607,0.0285,-0.1179,-0.0425,0.0116,0.1322,0.008,-0.0515,-0.0386,0.0052,0.0231,-0.0295,0.0372,0.0364,-0.033,-0.0095,0.0706,-0.0159,-0.0629,0.0062,-0.0027,-0.0012,0.0136,-0.0317,-0.0442,0.0558,0.0365,-0.0283,-0.004,-0.0341,0.0326,0.0021,-0.0377,0.0009,-0.0306,0.022,-0.0573,-0.0511,-0.053,0.0038,0.0325,-0.0492,0.0023,-0.0259,-0.0162,0.0603,0.0173,0.0028,-0.0032,0.001,-0.0337,0.0028,0.0019,-0.0335,0.0441,-0.0389,-0.0197,0.013,0.0123,0.0139,-0.0155,0.0602,0.0063,-0.0347,-0.0914,-0.1909,-0.0091,-0.0317,0.0159,0.0492,-0.0779,0.0724,-0.0498,0.0796,0.0568,0.0605,0.0391,0.0043,0.0199,0.0068,0.0541,0.0278,0.0657,0.0296,-0.0025,-0.0085,-0.0067,-0.0424,-0.0547,0.0616,-0.0365,0.1963,0.0253,0.0335,0.0136,-0.0079,0.0284,0.0197,-0.0101,0.0675,0.036,0.1007,-0.0258,-0.0585,-0.0146,0.0106,0.0052,0.0153,-0.0313,-0.0235,-0.0408,-0.008,0.0108,-0.0467,-0.0166,0.0468,-0.041,0.0719,-0.0324,-0.0002,-0.0921,-0.0648,0.0256,-0.0214,0.0273,-0.0188,-0.059,0.0261,-0.097,0.0833,0.005,-0.0135,-0.0379,0.0416,-0.0595,0.0011,0.0578,0.0322,-0.0466,0.076,-0.0157,0.0215,-0.0298,-0.0573,-0.0104,0.0292,-0.0745,0.0402,0.014,0.0301,-0.0109,0.0729,-0.0058,-0.0101,-0.0197,-0.0292,0.0308,-0.084,0.0063,0.0528,-0.007,-0.3004,0.0121,0.0222,0.0129,-0.0207,0.0041,0.0481,0.0159,-0.0721,-0.0127,-0.0449,0.0735,0.0496,-0.0119,0.0382,0.071,0.0475,-0.0752,0.0417,-0.094,-0.0032,0.0146,0.1958,-0.057,0.0327,-0.0237,-0.0671,0.0298,0.0008,-0.0422,0.0125,0.0481,0.1021,-0.0205,0.0602,0.0505,-0.0112,0.0517,-0.0228,0.0153,0.0506,0.0061,-0.0102,-0.0288,0.1211,-0.0164,-0.0221,-0.0459,-0.0207,0.0156,-0.0213,0.0338,0.0167,-0.0001,-0.0115,0.0517,-0.0902,-0.0129,0.0041,-0.0399,0.0073,-0.0646,-0.0542,-0.0532,0.0058]}
{"key":"[ECG Heart-beat Classification Using Multimodal Image Fusion] In this paper, we present a novel Image Fusion Model (IFM) for ECG heart-beat classification to overcome the weaknesses of existing machine learning techniques that rely either on manual feature extraction or direct utilization of 1D raw ECG signal. At the input of IFM, we first convert the heart beats of ECG into three different images using Gramian Angular Field (GAF), Recurrence Plot (RP) and Markov Transition Field (MTF) and then fuse these images to create a single imaging modality. We use AlexNet for feature extraction and classification and thus employ end to end deep learning. We perform experiments on PhysioNet MIT-BIH dataset for five different arrhythmias in accordance with the AAMI EC57 standard and on PTB diagnostics dataset for myocardial infarction (MI) classification. We achieved an state of an art results in terms of prediction accuracy, precision and recall.","layer":4,"vector":[-0.0101,-0.0061,0.0072,0.017,0.0345,0.022,0.0287,0.0048,0.0003,-0.0232,0.0523,-0.0953,-0.0012,0.0149,0.029,0.0274,0.0276,0.0234,-0.0425,0.0193,0.0118,0.0373,0.0045,-0.0327,0.0267,0.0176,-0.0008,-0.0684,-0.0608,-0.2055,0.0383,-0.0154,0.0122,0.0081,0.0216,-0.0567,-0.0338,0.069,-0.0734,0.0462,-0.0147,-0.0347,-0.0215,-0.07,0.0002,-0.0485,-0.0184,-0.0564,0.0064,-0.016,0.0332,-0.0121,0.0302,0.0382,0.0253,0.0301,0.0347,0.0658,0.0243,0.0195,0.0229,0.1023,-0.1824,0.0374,0.0412,0.0404,-0.0293,0.0131,0.0255,0.0478,-0.0227,0.0309,0.0334,0.0167,-0.046,0.0011,-0.0129,-0.0283,-0.013,-0.0134,-0.014,-0.0116,-0.0145,-0.0317,0.02,0.0056,0.0466,-0.083,0.0245,-0.0021,-0.0423,-0.0194,-0.035,0.0206,-0.0378,-0.0146,-0.0027,0.0239,-0.0514,0.1947,-0.0324,0.0223,0.0566,-0.0113,0.0524,-0.0322,-0.0232,-0.0435,-0.0593,-0.0232,0.0208,-0.0374,0.0533,-0.0333,0.0632,0.0329,-0.0067,0.0446,0.049,-0.0024,-0.0364,0.0089,0.053,-0.0286,0.0218,-0.0642,0.0192,0.1317,0.0506,-0.0115,0.0529,0.0008,-0.043,-0.0366,-0.0039,0.0058,0.0441,0.0036,0.0183,0.01,-0.029,-0.0578,0.0473,-0.0955,-0.0794,0.0796,-0.0955,0.0048,-0.0645,-0.0242,-0.0273,0.0054,-0.0749,0.006,0.0255,0.0356,0.0473,0.0746,-0.0373,-0.0156,-0.0465,-0.0589,-0.0472,0.1289,0.0188,-0.0629,-0.0135,0.009,0.0092,-0.01,0.039,0.0167,0.0001,0.0325,0.0814,0.026,-0.0162,-0.0345,0.0297,-0.0088,0.0171,-0.0355,-0.0047,0.0213,0.0137,-0.0532,0.0206,-0.0739,0.0063,0.0195,0.0186,0.0427,0.0264,0.0673,0.0008,-0.0503,-0.0662,0.0237,-0.0068,-0.0271,0.011,-0.0166,-0.0005,0.0271,-0.0015,-0.007,0.0008,0.0372,0.0242,0.0392,0.0021,0.0002,0.0594,-0.017,-0.0314,-0.0053,0.032,0.0627,-0.0059,0.0588,0.0276,-0.0583,-0.0276,-0.2316,-0.0237,0.0336,0.0174,0.0351,-0.1079,0.0184,0.0299,0.0578,0.051,0.0634,0.0437,-0.0188,-0.0041,0.0017,0.0591,0.0096,0.0641,-0.045,0.0141,-0.0121,0.0244,0.0053,-0.0854,0.0838,-0.014,0.233,-0.0103,0.0149,-0.0249,0.0109,0.0432,-0.0363,-0.1055,0.0478,-0.0246,0.0563,0.0062,-0.0743,-0.0921,-0.0477,0.0273,0.0282,-0.1149,-0.0634,-0.0375,-0.0676,0.0058,-0.0335,0.0436,0.0429,-0.0729,0.0368,-0.0044,0.0365,-0.0263,-0.0858,0.0293,-0.0223,-0.0067,-0.0242,-0.0205,0.0044,-0.0617,0.0682,-0.001,-0.0197,-0.0314,0.0252,-0.1015,-0.0448,0.0661,0.0443,-0.0156,0.0734,0.0179,0.0533,-0.0163,-0.0513,-0.0261,0.0124,-0.0278,0.0364,0.0346,0.0587,-0.0088,0.0464,0.0221,-0.0005,-0.0072,0.0145,0.06,-0.0164,-0.0112,0.0326,-0.0052,-0.3167,0.0511,0.0167,-0.0015,-0.0336,0.0292,0.0155,0.0017,-0.0521,0.0153,-0.0559,0.019,0.0958,-0.0426,-0.0087,0.0251,0.0589,-0.0596,0.0564,-0.0177,0.0133,0.0536,0.2171,-0.0562,0.0159,0.0511,-0.0421,0.0035,0.0012,-0.0155,-0.01,0.002,0.0635,-0.0494,0.0389,0.0344,-0.0274,0.0372,-0.0047,-0.0254,0.048,0.0185,-0.0669,-0.0378,0.0781,-0.0292,-0.0369,-0.0069,0.0159,0.0283,-0.0204,-0.0148,0.0193,0.0095,0.0264,0.01,-0.0531,-0.0291,-0.0156,-0.0555,0.0491,-0.069,-0.0343,0.0117,-0.0034]}
{"key":"[Explanations in Autonomous Driving: A Survey] The automotive industry has witnessed an increasing level of development in the past decades; from manufacturing manually operated vehicles to manufacturing vehicles with a high level of automation. With the recent developments in Artificial Intelligence (AI), automotive companies now employ blackbox AI models to enable vehicles to perceive their environments and make driving decisions with little or no input from a human. With the hope to deploy autonomous vehicles (AV) on a commercial scale, the acceptance of AV by society becomes paramount and may largely depend on their degree of transparency, trustworthiness, and compliance with regulations. The assessment of the compliance of AVs to these acceptance requirements can be facilitated through the provision of explanations for AVs' behaviour. Explainability is therefore seen as an important requirement for AVs. AVs should be able to explain what they have 'seen', done, and might do in environments in which they operate. In this paper, we provide a comprehensive survey of the existing body of work around explainable autonomous driving. First, we open with a motivation for explanations by highlighting and emphasising the importance of transparency, accountability, and trust in AVs; and examining existing regulations and standards related to AVs. Second, we identify and categorise the different stakeholders involved in the development, use, and regulation of AVs and elicit their explanation requirements for AV. Third, we provide a rigorous review of previous work on explanations for the different AV operations (i.e., perception, localisation, planning, control, and system management). Finally, we identify pertinent challenges and provide recommendations, such as a conceptual framework for AV explainability. This survey aims to provide the fundamental knowledge required of researchers who are interested in explainability in AVs.","layer":5,"vector":[0.0,-0.0322,0.0218,-0.0117,0.028,0.0373,0.0602,0.0417,0.0014,-0.0297,-0.0087,-0.0802,0.0058,0.0393,0.0187,-0.0135,-0.0348,-0.0029,0.0111,-0.0362,0.0338,-0.0446,-0.026,-0.0241,-0.0276,0.0833,-0.0207,-0.0154,-0.0604,-0.2127,-0.0065,-0.092,0.0763,-0.0243,0.0001,-0.0021,-0.0584,0.0556,-0.0266,-0.0061,0.0652,-0.0169,-0.0205,-0.0375,-0.0344,-0.0201,-0.004,-0.0057,-0.0503,-0.0778,0.0265,-0.0002,0.0281,-0.0053,0.0009,0.0245,0.063,0.0676,0.0227,0.0194,0.0438,0.0406,-0.1827,0.0848,0.0841,0.0477,-0.0418,-0.0329,-0.0045,0.0246,-0.0205,-0.0144,0.0051,0.0723,0.0091,-0.0353,-0.0044,-0.048,0.0081,-0.0223,-0.0164,-0.0153,-0.0701,0.007,-0.0253,-0.0365,0.0228,-0.0293,0.0553,-0.0037,-0.0242,0.0229,-0.0313,-0.0275,-0.0323,-0.01,0.0272,-0.0459,-0.0827,0.2038,-0.0365,-0.0097,0.0246,-0.0198,-0.0146,0.0257,-0.0219,-0.0285,-0.0073,0.0242,-0.0266,0.0122,0.0497,0.0114,0.0267,0.0523,0.0362,0.0422,0.0332,-0.0318,-0.0021,-0.0061,0.0501,-0.007,0.0299,-0.0405,0.0553,0.1301,0.0044,-0.0023,0.0544,-0.0408,-0.0613,0.0045,0.0437,-0.0135,-0.009,0.0176,-0.0133,0.0225,-0.0355,-0.0679,-0.0051,-0.0993,-0.036,0.0709,-0.0411,-0.0048,-0.0016,0.0175,0.0017,-0.0032,-0.0402,-0.0434,0.0397,0.0439,0.0508,0.0397,-0.0764,0.051,0.0175,-0.0475,-0.0402,0.1016,0.0062,-0.1102,-0.0095,0.0284,0.0621,0.0032,-0.0075,0.0231,-0.054,0.0307,0.0812,0.042,-0.0585,0.0365,-0.0421,0.0142,0.0044,-0.0923,-0.0414,0.064,0.0719,-0.0273,-0.0196,-0.0565,0.0176,0.0461,0.0007,0.0454,-0.0105,0.0224,-0.0348,0.0025,0.0177,-0.0461,-0.0245,-0.0283,-0.0085,0.0027,-0.0295,0.0088,-0.058,0.0113,-0.0181,-0.0068,0.0766,0.0632,-0.0649,0.0185,0.0147,-0.0407,-0.0388,-0.0134,-0.0394,0.0278,0.0146,0.021,-0.0076,0.0207,-0.0369,-0.2116,-0.0022,-0.0104,-0.0295,0.0275,-0.048,0.0335,-0.0183,0.0139,0.0617,0.0819,-0.039,-0.0102,0.0188,-0.0093,0.0492,-0.0169,0.0169,-0.0475,0.0243,0.0018,0.0209,-0.0175,-0.0817,0.0191,0.0163,0.2229,-0.0169,0.0698,0.0111,0.0478,0.0421,-0.0108,-0.1149,0.059,-0.0143,0.0295,-0.0381,-0.0251,-0.0307,-0.0206,0.0458,-0.0463,-0.072,-0.0206,-0.0352,-0.0479,0.0446,-0.0128,0.014,0.0233,-0.0142,0.0465,0.0161,-0.0205,-0.0249,-0.0562,0.0152,0.0062,0.0397,-0.0368,0.0042,0.0141,-0.0819,0.0802,-0.005,-0.0351,-0.0556,0.0102,-0.0082,-0.0182,0.1481,0.0093,-0.0437,0.085,0.0202,0.0243,-0.0193,-0.0187,-0.0493,0.0434,0.0122,0.0514,0.0418,0.0347,-0.0088,0.0401,-0.0296,-0.0072,0.0002,0.0335,0.0522,-0.0367,-0.0608,0.0812,-0.0149,-0.2858,0.0389,0.0279,0.054,-0.0582,0.0061,0.0226,0.0256,-0.0436,0.0137,-0.0252,0.0545,0.0418,0.06,0.0215,0.0566,0.0644,-0.0496,0.0453,-0.0559,0.0261,0.0635,0.2384,-0.0355,0.0203,0.0505,-0.0483,-0.044,0.0297,-0.0022,0.0226,-0.02,0.0845,-0.0419,0.0205,0.0326,-0.0531,0.0558,0.0455,-0.0241,-0.0148,0.0024,0.0186,-0.0327,0.0887,-0.0014,-0.0552,-0.0894,0.0051,0.0275,0.0273,-0.0217,-0.058,-0.0392,0.0503,-0.0187,-0.0174,-0.0598,-0.0175,-0.0564,0.0568,-0.0802,0.0344,0.041,0.0072]}
{"key":"[Bayesian Clustering of Shapes of Curves] Unsupervised clustering of curves according to their shapes is an important problem with broad scientific applications. The existing model-based clustering techniques either rely on simple probability models (e.g., Gaussian) that are not generally valid for shape analysis or assume the number of clusters. We develop an efficient Bayesian method to cluster curve data using an elastic shape metric that is based on joint registration and comparison of shapes of curves. The elastic-inner product matrix obtained from the data is modeled using a Wishart distribution whose parameters are assigned carefully chosen prior distributions to allow for automatic inference on the number of clusters. Posterior is sampled through an efficient Markov chain Monte Carlo procedure based on the Chinese restaurant process to infer (1) the posterior distribution on the number of clusters, and (2) clustering configuration of shapes. This method is demonstrated on a variety of synthetic data and real data examples on protein structure analysis, cell shape analysis in microscopy images, and clustering of shaped from MPEG7 database.","layer":0,"vector":[-0.0693,-0.0216,0.0425,0.0321,0.022,0.0906,0.046,0.0089,0.0087,-0.0067,0.067,-0.112,-0.0199,0.0469,-0.0207,-0.0109,-0.0244,0.0767,-0.0505,-0.0148,0.0161,-0.0617,-0.0325,-0.0336,0.0534,0.0631,-0.0225,0.013,-0.0522,-0.2414,-0.0009,-0.0299,0.0643,-0.015,-0.0024,-0.034,-0.0237,0.0758,-0.0272,0.0206,0.0525,0.03,-0.0217,-0.0378,0.0286,-0.0802,-0.0253,0.009,-0.0041,-0.0377,0.0285,-0.036,0.0398,0.0217,0.0278,0.0456,0.0262,0.0317,0.03,0.0754,0.023,0.017,-0.1802,0.0899,0.0542,0.0192,-0.0296,-0.0332,0.0244,0.0441,-0.0494,0.0803,-0.0098,0.0485,0.0128,-0.0371,0.0302,-0.0319,-0.0516,0.015,-0.0196,-0.0256,-0.0029,-0.0012,-0.0304,-0.0189,-0.0142,-0.063,0.0567,0.0146,-0.036,0.037,-0.0392,0.0269,-0.103,-0.0409,0.0315,0.0087,0.0021,0.1824,-0.0388,0.0468,0.0478,0.0054,0.0124,-0.0414,-0.048,-0.0319,-0.0121,0.0214,0.0249,0.0035,-0.0221,-0.0753,-0.0033,-0.0185,0.0486,0.0345,-0.0415,-0.0126,-0.031,0.0391,0.0549,0.0181,0.0167,-0.0686,0.02,0.0886,0.0778,0.0012,0.0853,0.0256,-0.0634,-0.0079,0.0201,0.025,-0.0071,-0.0169,0.0017,-0.027,-0.0294,-0.0675,0.0131,-0.0732,-0.0036,0.1428,-0.0511,0.055,-0.077,-0.0201,0.0133,0.0011,-0.0395,-0.0395,0.0082,0.0213,0.0068,0.0248,-0.0394,-0.003,-0.0607,-0.0393,-0.0129,0.1092,0.04,-0.0785,-0.0051,-0.0031,0.0559,-0.0065,0.047,0.039,-0.007,0.0687,0.1212,0.0289,-0.0298,-0.014,0.046,0.0319,0.0687,0.0076,-0.0267,0.0026,0.0452,-0.0622,-0.0329,0.0189,0.0242,0.04,-0.0147,0.0022,-0.0191,-0.022,-0.0416,-0.0478,-0.0397,-0.006,0.0035,-0.0055,0.0218,-0.0155,-0.0816,0.0144,-0.0142,0.0349,-0.0033,-0.0078,0.0045,0.0292,-0.024,-0.0173,0.0323,-0.0171,-0.0008,0.0117,0.0427,0.054,0.0192,0.0287,0.0225,-0.0495,-0.06,-0.1936,0.0041,0.0446,-0.0225,0.055,-0.0404,-0.0013,-0.0068,0.0672,0.0468,0.0302,-0.0036,-0.0433,0.023,-0.0317,0.0391,0.026,0.0113,-0.0538,0.0136,-0.0298,-0.0196,-0.077,-0.0366,0.0051,0.0043,0.2266,0.037,-0.0286,0.0107,0.0032,0.0406,-0.0572,-0.0718,0.0462,0.0561,0.038,-0.0265,-0.0348,-0.0275,-0.0434,-0.005,0.0301,-0.0887,-0.0311,-0.0678,-0.0256,0.039,-0.0465,-0.0075,-0.0004,-0.0303,0.0581,-0.0233,0.0017,-0.012,-0.0318,-0.002,-0.0212,0.0243,0.0323,-0.0624,0.0287,-0.082,0.04,-0.0394,-0.0523,-0.0245,0.0255,-0.0819,-0.0283,0.1012,0.0208,0.0028,0.0626,0.0236,0.0301,-0.0496,-0.0637,-0.0327,0.0786,-0.0256,-0.0114,0.0388,0.0016,0.0277,0.042,-0.0324,0.0165,-0.0232,0.0734,-0.0375,-0.023,0.0092,0.0138,0.0311,-0.2794,0.0093,-0.016,0.0256,-0.018,-0.0162,0.0516,0.0158,0.017,-0.0225,0.0231,0.0343,0.0708,-0.0609,-0.0309,0.0024,0.0433,-0.0445,0.0757,-0.077,-0.0047,-0.005,0.2381,-0.0376,0.0298,0.0188,-0.035,0.0421,0.0456,-0.0386,0.0309,0.0011,0.0702,-0.0588,0.0792,0.1,-0.0455,0.0235,0.0309,-0.0094,0.0311,0.0073,-0.0856,-0.0245,0.1111,0.0379,-0.0226,-0.0336,0.0224,0.0005,-0.0398,0.0144,-0.0699,0.009,-0.0001,0.0293,-0.0002,-0.0336,-0.0264,-0.0364,-0.0134,-0.0436,-0.0365,0.0291,-0.0091]}
{"key":"[A Unified View of Causal and Non-causal Feature Selection] In this paper, we aim to develop a unified view of causal and non-causal feature selection methods. The unified view will fill in the gap in the research of the relation between the two types of methods. Based on the Bayesian network framework and information theory, we first show that causal and non-causal feature selection methods share the same objective. That is to find the Markov blanket of a class attribute, the theoretically optimal feature set for classification. We then examine the assumptions made by causal and non-causal feature selection methods when searching for the optimal feature set, and unify the assumptions by mapping them to the restrictions on the structure of the Bayesian network model of the studied problem. We further analyze in detail how the structural assumptions lead to the different levels of approximations employed by the methods in their search, which then result in the approximations in the feature sets found by the methods with respect to the optimal feature set. With the unified view, we are able to interpret the output of non-causal methods from a causal perspective and derive the error bounds of both types of methods. Finally, we present practical understanding of the relation between causal and non-causal methods using extensive experiments with synthetic data and various types of real-word data.","layer":0,"vector":[-0.0321,0.0488,0.0028,-0.026,0.0641,-0.0027,0.0861,0.0539,0.0366,-0.0529,0.0189,-0.0308,0.0139,0.0794,0.0247,0.0342,0.0317,0.0393,-0.0458,-0.0186,0.0339,-0.0359,0.0123,-0.0361,0.0355,0.0464,-0.0309,0.0155,-0.0336,-0.2575,0.0105,-0.043,0.0316,-0.0271,-0.0017,0.0056,-0.0189,0.0611,-0.022,0.043,0.004,0.0263,-0.0272,-0.0661,-0.0159,-0.0568,0.0279,0.0025,-0.0388,-0.0205,0.0118,-0.0162,0.0457,0.0517,0.02,0.0442,0.0159,0.0486,0.0148,0.056,0.0066,0.0345,-0.1902,0.0472,0.0686,-0.0023,-0.0365,-0.0045,0.0422,0.0687,-0.0044,0.0735,-0.0217,0.053,0.0153,-0.0036,0.0154,-0.0187,-0.0269,0.0064,0.0321,0.0078,-0.0306,-0.0257,-0.0099,-0.0206,0.0183,-0.0767,0.0111,-0.0016,-0.0358,0.0002,-0.0127,-0.0099,-0.0774,-0.0523,0.0625,0.0024,-0.0008,0.1679,-0.0699,0.049,-0.0325,-0.0421,0.051,-0.0517,-0.0347,-0.0466,-0.0504,0.0099,0.0356,0.0166,-0.0367,-0.0115,0.0079,0.0081,0.0582,0.0119,-0.0013,-0.0199,-0.005,-0.0142,0.0568,-0.0437,-0.0118,-0.0684,0.0135,0.1223,0.0393,-0.0036,0.078,-0.0541,-0.0696,-0.0084,0.0448,-0.0021,0.032,0.0293,-0.001,-0.0252,-0.033,-0.0659,0.046,-0.0734,-0.094,0.1306,-0.0762,0.0239,-0.03,-0.0383,-0.0294,0.009,-0.0241,-0.0636,-0.0062,0.0446,-0.0105,0.0167,-0.0643,0.0062,-0.0043,-0.0324,-0.0247,0.0995,-0.0057,-0.0889,-0.0465,0.0198,-0.0191,-0.0173,0.025,0.0339,-0.0573,0.0587,0.1025,0.0561,-0.0747,0.0226,0.0395,0.0194,0.0421,-0.0028,-0.025,0.0165,0.0397,-0.0013,0.0062,-0.001,0.0379,0.0299,0.0075,-0.0158,0.025,-0.0098,-0.0473,-0.011,0.0096,-0.0022,0.0323,-0.0307,-0.0057,0.0241,-0.0713,-0.0305,-0.0377,0.0311,-0.0252,0.0117,0.0546,0.0243,-0.0168,0.0022,0.0273,0.0001,-0.0291,0.0246,0.0202,0.0503,0.005,0.0436,0.0431,-0.0436,-0.0413,-0.2486,-0.0147,0.0022,0.029,0.0039,-0.0803,0.0054,0.0211,0.0635,0.0889,0.0289,-0.0134,-0.0731,0.005,-0.0094,0.0409,0.0216,0.0189,-0.024,0.0049,-0.0348,-0.002,-0.0146,-0.0759,0.0488,-0.0209,0.1888,0.0241,0.0608,-0.0252,0.0261,0.0366,-0.0302,-0.1136,0.0548,0.0482,0.0353,-0.0015,-0.0725,-0.0299,-0.0014,0.0475,-0.0096,-0.068,-0.0412,-0.0423,-0.0346,-0.0011,-0.0845,0.0353,0.0634,0.0246,0.0559,0.0069,-0.0067,-0.0536,-0.0712,0.0071,-0.0189,-0.0051,0.0263,-0.046,-0.0009,-0.0732,0.0156,0.011,-0.0349,-0.0266,0.0044,-0.0214,-0.0206,0.1149,-0.0027,0.0042,0.0501,0.0029,0.0363,-0.056,-0.0242,-0.0483,0.0599,-0.0349,0.0056,0.0283,-0.0064,-0.0313,0.1043,-0.0188,0.001,0.0033,-0.0043,-0.0098,-0.0179,0.0059,0.0417,0.0046,-0.2907,0.0606,0.0157,0.0117,-0.0056,-0.0119,0.0252,0.0132,-0.026,-0.0042,0.0261,-0.0033,0.0464,-0.0138,-0.0527,0.0371,0.0455,-0.0631,0.0506,-0.0573,0.0322,0.045,0.2508,-0.0369,0.0329,-0.0038,-0.0139,0.013,-0.0271,-0.017,0.0694,0.0384,0.0693,-0.0522,0.0347,0.0857,-0.0148,-0.0065,-0.0095,-0.0471,0.0054,0.0167,-0.0697,-0.0364,0.134,0.0094,-0.0082,-0.0498,-0.0196,0.0323,-0.0373,0.0243,-0.0819,0.0137,0.0152,0.0464,-0.0555,-0.0259,-0.0253,-0.0352,0.0251,-0.0092,-0.0039,0.0068,-0.0252]}
{"key":"[Energy-efficient Amortized Inference with Cascaded Deep Classifiers] Deep neural networks have been remarkable successful in various AI tasks but often cast high computation and energy cost for energy-constrained applications such as mobile sensing. We address this problem by proposing a novel framework that optimizes the prediction accuracy and energy cost simultaneously, thus enabling effective cost-accuracy trade-off at test time. In our framework, each data instance is pushed into a cascade of deep neural networks with increasing sizes, and a selection module is used to sequentially determine when a sufficiently accurate classifier can be used for this data instance. The cascade of neural networks and the selection module are jointly trained in an end-to-end fashion by the REINFORCE algorithm to optimize a trade-off between the computational cost and the predictive accuracy. Our method is able to simultaneously improve the accuracy and efficiency by learning to assign easy instances to fast yet sufficiently accurate classifiers to save computation and energy cost, while assigning harder instances to deeper and more powerful classifiers to ensure satisfiable accuracy. With extensive experiments on several image classification datasets using cascaded ResNet classifiers, we demonstrate that our method outperforms the standard well-trained ResNets in accuracy but only requires less than 20% and 50% FLOPs cost on the CIFAR-10/100 datasets and 66% on the ImageNet dataset, respectively.","layer":0,"vector":[-0.036,0.0129,0.0556,-0.0107,0.0604,0.0535,0.0145,0.027,0.0233,-0.0066,0.0406,-0.0376,0.0027,0.0864,0.053,0.0232,0.0466,0.0342,-0.0347,0.008,0.0568,-0.0542,-0.0062,-0.051,-0.0098,-0.0134,0.0026,-0.0221,-0.0588,-0.2321,0.0226,-0.0522,0.0186,-0.0192,-0.0036,-0.0284,-0.0397,0.0365,-0.0519,0.0467,0.0117,0.0148,-0.0376,-0.0542,0.0028,-0.0567,-0.0168,-0.0079,-0.0439,-0.0191,0.0259,-0.0255,-0.005,0.0317,0.0273,0.0516,0.0224,0.0529,0.0916,0.0069,0.0014,0.0539,-0.1644,0.0305,0.0377,-0.0019,-0.0251,-0.0313,0.0151,0.0335,0.0131,0.0492,0.0327,0.0627,0.0124,0.0498,-0.02,-0.0302,0.0067,-0.0274,0.0117,-0.0233,-0.0097,0.032,-0.006,-0.052,0.0334,-0.0436,0.0352,0.0003,-0.0668,-0.0209,-0.0273,0.0284,-0.0928,-0.0061,0.0293,0.0096,-0.0686,0.1939,-0.0382,0.0433,0.0008,-0.0223,0.0213,-0.038,-0.0824,-0.0034,-0.1009,0.0275,0.0036,-0.0045,0.0265,0.0075,0.047,0.041,0.0532,0.0556,-0.0084,0.0076,-0.0329,-0.0148,0.0754,-0.0147,0.0048,-0.0846,0.0125,0.1497,-0.0056,0.0342,0.0361,-0.0491,-0.0639,-0.0387,0.0657,-0.0095,0.0304,0.0131,0.0173,0.0181,-0.0309,-0.0276,0.0352,-0.0789,-0.0534,0.076,-0.0554,0.0491,-0.0394,-0.0739,-0.0076,0.0145,-0.0139,-0.0492,0.0326,0.0366,0.0268,0.0336,-0.0533,-0.0006,0.0031,-0.0345,-0.0502,0.0912,0.0047,-0.0851,-0.046,-0.0183,0.0145,-0.0326,0.0274,0.0507,-0.0335,0.0238,0.052,0.0376,-0.0983,0.0066,0.0111,-0.0015,0.0041,-0.0238,-0.0162,0.0425,0.0571,-0.0279,-0.021,-0.0925,-0.0185,0.0332,-0.0282,0.0337,-0.0377,0.0015,-0.0042,0.0121,-0.0246,0.007,-0.0055,-0.0542,0.0231,-0.0106,-0.0084,0.0514,0.037,0.0108,-0.0296,0.0312,0.0065,0.0271,0.0168,0.0052,0.0566,-0.017,-0.0592,-0.0307,0.0424,0.0237,-0.0111,0.0577,0.0666,-0.0976,-0.0004,-0.2128,0.0218,0.016,-0.0533,0.007,-0.1029,0.0486,0.0065,0.0133,0.0181,0.0221,-0.0349,-0.0381,0.0212,-0.0003,0.0658,0.0428,0.0368,-0.0364,0.0221,0.0097,0.0153,-0.0213,-0.0881,0.1024,0.0304,0.2325,0.0102,0.0678,0.0276,0.0238,0.0082,-0.0352,-0.1044,0.0424,0.0187,0.0643,0.0014,-0.0765,-0.0542,-0.0509,0.0633,-0.0172,-0.1297,-0.0062,-0.0036,-0.0697,0.0289,-0.0226,-0.0242,0.0353,-0.0065,0.0316,-0.0159,-0.0025,-0.0171,-0.0813,0.0655,-0.087,0.0047,0.018,-0.0599,0.0183,-0.0426,0.0369,-0.0141,-0.0291,-0.0588,-0.0216,-0.0028,-0.0249,0.0479,0.0025,-0.0047,0.0438,-0.0129,0.0126,-0.0333,-0.0288,-0.0053,0.0945,-0.0101,0.0307,0.0078,0.0392,0.0304,0.0869,-0.0011,0.0137,0.0008,-0.0085,0.0556,-0.0558,-0.0178,0.0506,0.0221,-0.2968,0.0542,-0.0084,0.0371,-0.0669,0.0336,0.0515,0.0245,-0.0068,-0.0126,-0.0195,0.0274,0.0596,-0.0023,0.0486,0.0279,0.0549,-0.0384,-0.0025,-0.0689,-0.0072,0.0372,0.2145,-0.0224,0.05,0.062,-0.0233,0.0036,0.0334,-0.0208,-0.0056,0.0234,0.0612,-0.0463,0.0191,0.1061,-0.0307,-0.0033,0.0147,-0.0277,0.0062,0.0077,-0.0264,-0.0132,0.096,-0.0036,-0.0008,-0.0464,-0.0475,0.0329,0.0001,-0.0143,-0.0289,-0.0016,0.0082,0.0121,-0.0843,-0.0552,-0.0636,-0.0555,0.0439,-0.005,-0.0237,-0.0136,-0.0279]}
{"key":"[P2M: A Processing-in-Pixel-in-Memory Paradigm for Resource-Constrained TinyML Applications] The demand to process vast amounts of data generated from state-of-the-art high resolution cameras has motivated novel energy-efficient on-device AI solutions. Visual data in such cameras are usually captured in the form of analog voltages by a sensor pixel array, and then converted to the digital domain for subsequent AI processing using analog-to-digital converters (ADC). Recent research has tried to take advantage of massively parallel low-power analog/digital computing in the form of near- and in-sensor processing, in which the AI computation is performed partly in the periphery of the pixel array and partly in a separate on-board CPU/accelerator. Unfortunately, high-resolution input images still need to be streamed between the camera and the AI processing unit, frame by frame, causing energy, bandwidth, and security bottlenecks. To mitigate this problem, we propose a novel Processing-in-Pixel-in-memory (P2M) paradigm, that customizes the pixel array by adding support for analog multi-channel, multi-bit convolution, batch normalization, and ReLU (Rectified Linear Units). Our solution includes a holistic algorithm-circuit co-design approach and the resulting P2M paradigm can be used as a drop-in replacement for embedding memory-intensive first few layers of convolutional neural network (CNN) models within foundry-manufacturable CMOS image sensor platforms. Our experimental results indicate that P2M reduces data transfer bandwidth from sensors and analog to digital conversions by ~21x, and the energy-delay product (EDP) incurred in processing a MobileNetV2 model on a TinyML use case for visual wake words dataset (VWW) by up to ~11x compared to standard near-processing or in-sensor implementations, without any significant drop in test accuracy.","layer":0,"vector":[-0.0136,0.0035,0.0371,-0.0246,0.0134,0.0229,-0.0042,0.0401,0.0196,-0.0367,0.0312,-0.0796,0.0542,0.0473,0.0112,0.0086,0.0312,-0.0031,-0.0017,0.0281,0.0317,-0.0336,-0.0376,-0.0269,-0.0281,0.0129,-0.0007,-0.0178,-0.0803,-0.2505,0.0234,-0.0116,0.0715,-0.0331,-0.001,-0.0408,-0.0177,0.0086,-0.0072,0.0298,0.0026,0.0014,-0.0316,-0.0728,0.0074,-0.0626,-0.025,-0.0215,0.001,-0.0509,0.0062,-0.0198,0.0628,0.0475,0.0281,0.0158,0.0271,0.0479,0.096,0.0022,0.0435,0.0644,-0.1843,0.0641,0.0286,-0.006,-0.0479,-0.0209,-0.0165,0.0049,-0.0416,0.0234,0.0598,0.0274,0.0189,-0.0064,0.0067,-0.0506,-0.0183,-0.0357,0.0,-0.017,-0.0279,0.0006,-0.0322,-0.0198,-0.0149,0.0015,0.0503,-0.0148,-0.0566,-0.0374,-0.0962,0.0237,-0.0445,-0.0019,0.0284,0.0428,-0.0529,0.208,-0.0349,0.0634,0.0476,-0.0526,0.0507,-0.0375,-0.0447,-0.0351,-0.0724,0.0014,-0.0137,-0.0211,0.023,0.0316,0.0667,0.0046,-0.0033,0.0393,0.0399,0.0113,-0.0422,0.0034,-0.0036,0.0004,0.0156,-0.0853,0.029,0.1459,0.0123,0.0291,0.0264,-0.0206,-0.0255,-0.0063,0.0309,0.0415,0.054,-0.0099,0.0188,-0.02,-0.0479,0.0143,0.0594,-0.0798,-0.0177,0.1152,-0.0311,0.0388,-0.0575,-0.0391,-0.0123,0.0405,-0.0499,0.0157,0.0511,0.0317,0.0102,0.0161,-0.0453,0.0274,-0.0013,-0.0344,-0.0219,0.095,-0.0033,-0.0596,-0.0225,-0.0688,0.0195,-0.0215,0.0444,0.0192,-0.0541,-0.0303,0.0529,0.0357,-0.0884,-0.0537,-0.0411,-0.0069,0.0179,-0.0466,0.0346,-0.0062,0.0271,-0.0546,0.0104,-0.009,-0.0495,0.0454,-0.0698,0.0467,-0.0512,0.0032,-0.0098,0.0048,-0.0219,0.0062,-0.0104,-0.0243,0.0244,-0.023,-0.0519,0.0451,0.0249,-0.041,-0.0299,0.0072,0.002,0.0772,-0.0085,-0.0286,0.0834,0.0004,-0.036,-0.0233,-0.0062,0.0418,0.04,0.0505,0.0373,-0.0521,-0.0538,-0.2363,0.0388,-0.0154,-0.0022,0.0106,-0.0821,0.0095,-0.0179,0.0131,0.0235,0.0454,-0.0401,0.0021,0.0395,-0.043,0.0716,0.0399,0.0364,-0.0296,-0.0199,0.0035,0.0691,-0.0039,-0.0734,0.0654,0.0239,0.2323,0.0335,0.0657,-0.0094,0.0271,0.0393,-0.0159,-0.0569,0.0163,0.0187,0.0621,0.0461,0.0002,-0.0075,-0.059,0.0275,-0.0049,-0.1195,-0.0134,-0.0285,-0.0074,0.0175,-0.0637,-0.0075,0.0168,-0.0725,0.0154,-0.0183,0.0222,-0.0278,-0.069,0.0811,-0.0087,-0.0123,-0.0031,-0.0791,-0.0453,-0.015,0.0559,-0.0261,0.0061,-0.0656,0.0332,0.0014,0.0057,0.1138,0.0157,-0.0265,0.0635,-0.0067,0.0579,-0.0339,-0.0385,-0.0486,0.0527,0.0015,0.0306,0.0535,0.059,0.0469,0.0645,-0.0451,-0.0023,-0.017,-0.0171,0.0088,-0.0103,0.0209,0.0119,-0.0172,-0.3048,0.045,0.0102,0.0162,-0.0385,0.0264,0.0697,0.0369,-0.0338,0.054,-0.015,0.0472,0.0504,0.0295,-0.0259,0.031,0.0541,-0.0533,0.0405,-0.0483,-0.0163,0.0171,0.186,-0.0759,-0.0373,0.0022,-0.0431,0.0383,0.0532,-0.0021,-0.0067,0.0385,0.0693,-0.048,0.032,0.0838,-0.0795,0.0328,0.0967,0.02,-0.0116,0.0142,-0.0263,-0.0549,0.0673,-0.0139,-0.0079,-0.0207,-0.0319,0.0378,-0.032,-0.0368,-0.0031,-0.0021,0.0156,0.0492,-0.063,-0.0131,-0.0273,-0.0018,0.0485,-0.0392,-0.0137,0.0055,-0.0048]}
{"key":"[Rdimtools: An R package for Dimension Reduction and Intrinsic Dimension Estimation] Discovering patterns of the complex high-dimensional data is a long-standing problem. Dimension Reduction (DR) and Intrinsic Dimension Estimation (IDE) are two fundamental thematic programs that facilitate geometric understanding of the data. We present Rdimtools - an R package that supports 133 DR and 17 IDE algorithms whose extent makes multifaceted scrutiny of the data in one place easier. Rdimtools is distributed under the MIT license and is accessible from CRAN, GitHub, and its package website, all of which deliver instruction for installation, self-contained examples, and API documentation.","layer":2,"vector":[-0.0438,-0.0052,0.0768,0.0109,0.0169,0.0382,0.0178,0.05,0.0169,-0.0412,0.0125,-0.0538,0.054,0.0244,0.0046,-0.0119,0.0262,0.0605,-0.0637,0.047,0.0571,-0.0326,-0.0537,-0.0556,0.0213,0.0534,-0.0425,-0.052,-0.0303,-0.2795,0.0182,-0.0109,0.0635,-0.0071,0.0191,-0.0084,-0.0524,0.0922,-0.0289,0.0102,-0.0006,0.0164,-0.0059,0.0181,-0.0434,-0.0306,-0.0435,0.0221,0.0166,-0.0502,-0.0116,-0.0657,0.0093,0.0448,0.0292,0.0224,0.0283,0.0166,0.0883,0.0167,0.0388,0.0257,-0.164,0.0658,0.058,0.0344,-0.0573,-0.0206,0.015,0.0089,-0.0353,-0.0005,0.0051,0.0221,0.0285,-0.054,-0.0243,-0.0479,-0.0215,0.0105,-0.0044,0.0131,-0.0516,0.0112,-0.0041,-0.0139,0.0466,-0.0731,0.0415,-0.0249,-0.027,-0.0363,-0.0196,0.0047,-0.0754,0.0023,0.0374,-0.0024,0.0112,0.2156,-0.0415,0.0145,0.0211,0.0035,0.0276,-0.063,-0.0288,-0.0582,0.005,0.0367,0.0008,-0.015,0.0075,-0.0254,0.017,-0.032,0.0469,0.0506,0.011,0.0006,-0.0111,0.0187,0.072,-0.0089,0.0614,-0.0712,0.0232,0.1317,0.024,0.0195,0.0098,0.0343,-0.0275,0.0022,-0.0153,0.0318,0.0384,0.0064,0.0124,-0.0201,-0.0612,-0.0518,0.0299,-0.0706,-0.0302,0.1271,-0.0839,0.034,-0.0532,-0.0035,0.0054,0.0689,-0.0367,-0.0407,-0.0054,0.0311,0.0141,0.0247,-0.0603,0.0282,-0.0316,-0.0023,0.0058,0.1457,-0.0417,-0.0858,-0.0225,0.0458,0.0468,0.0022,0.03,0.0378,-0.0372,0.0419,0.1071,0.0223,-0.0712,-0.0212,0.0171,-0.0203,0.0584,-0.045,-0.0243,-0.0147,0.0406,-0.0426,-0.007,0.0241,0.0514,0.0281,-0.0102,0.019,-0.0287,-0.0253,-0.0073,-0.0158,0.048,0.004,0.0409,-0.0142,0.0595,0.023,-0.0399,0.045,-0.0196,0.0617,0.0195,0.0102,0.03,0.0615,-0.012,-0.0265,0.0211,-0.0365,0.0025,-0.009,0.0616,0.0176,-0.0218,0.0555,0.038,-0.0876,-0.0723,-0.2378,-0.0115,0.0179,-0.0564,0.0392,-0.0388,0.0456,-0.0461,0.0526,0.0806,0.0457,0.0012,-0.0376,0.0178,-0.0117,0.0547,0.036,0.0326,-0.0985,-0.0326,-0.0371,0.0094,-0.0145,-0.0164,0.068,-0.0082,0.2097,0.0449,-0.0154,-0.03,0.0101,-0.0102,-0.0243,-0.0923,0.0682,0.0014,0.0434,-0.004,-0.017,-0.0703,-0.0523,-0.0069,-0.0225,-0.0913,-0.0275,-0.0123,-0.0119,-0.0015,-0.0357,0.0244,0.0147,-0.031,0.0535,-0.0228,0.0033,-0.066,-0.0859,0.0076,-0.0059,-0.0094,-0.008,-0.0576,-0.0144,-0.1009,0.0496,-0.0198,-0.0309,-0.0161,0.0079,-0.036,-0.0132,0.0447,-0.0045,-0.025,0.0385,-0.026,0.0087,-0.001,0.0124,-0.0023,0.0229,-0.0405,0.0383,0.0371,0.034,0.0186,0.0531,-0.0061,0.0631,-0.0485,0.0665,-0.0037,-0.0123,-0.0421,0.0196,0.0219,-0.2965,0.0423,-0.0314,0.0035,-0.0485,0.016,0.0486,0.0272,-0.0107,-0.042,0.0169,0.052,0.0334,-0.0277,0.0098,0.0368,0.0479,-0.0579,0.0419,-0.0679,-0.0215,0.0355,0.2014,-0.0449,-0.0037,-0.0066,-0.0119,-0.0348,-0.0241,-0.01,-0.0096,-0.0017,0.0679,-0.0318,0.0571,0.1024,-0.022,0.0346,0.0223,0.0049,-0.003,-0.0159,-0.0151,-0.0123,0.0869,-0.0207,-0.0234,-0.0418,0.0501,0.0264,-0.0089,0.0031,-0.0436,0.0633,0.0261,0.0629,-0.0365,-0.0442,-0.0077,-0.0333,-0.0043,-0.0813,-0.05,-0.0094,-0.0094]}
{"key":"[Contextual Unsupervised Outlier Detection in Sequences] This work proposes an unsupervised learning framework for trajectory (sequence) outlier detection that combines ranking tests with user sequence models. The overall framework identifies sequence outliers at a desired false positive rate (FPR), in an otherwise parameter-free manner. We evaluate our methodology on a collection of real and simulated datasets based on user actions at the websites last.fm and msnbc.com, where we know ground truth, and demonstrate improved accuracy over existing approaches. We also apply our approach to a large real-world dataset of Pinterest and Facebook users, where we find that users tend to re-share Pinterest posts of Facebook friends significantly more than other types of users, pointing to a potential influence of Facebook friendship on sharing behavior on Pinterest.","layer":1,"vector":[-0.0479,-0.0488,0.008,-0.0225,0.0711,-0.0059,0.0617,0.06,0.0404,-0.0112,0.0353,0.0001,0.0044,0.0746,-0.0125,-0.003,0.0088,0.0541,-0.0575,0.0029,0.0147,-0.0653,-0.0141,-0.0428,0.0321,0.0227,-0.0545,-0.0565,-0.0785,-0.2223,-0.0009,-0.0628,0.052,-0.0418,0.0183,-0.0023,-0.0181,-0.0141,0.0161,0.0079,0.0236,0.0259,-0.0003,-0.0743,-0.0516,-0.0214,0.0238,-0.0059,-0.0449,-0.0464,0.0084,-0.0472,0.0102,0.0582,0.0564,0.0041,0.0366,0.0032,0.0617,0.0571,0.042,0.008,-0.1524,0.0264,0.0244,0.0278,-0.0546,0.0242,0.0203,0.0208,-0.0023,0.0274,-0.0069,0.0595,0.0383,0.0065,-0.013,-0.0192,-0.0391,0.0346,0.013,-0.0263,-0.0081,-0.0304,0.0175,-0.0763,-0.0087,-0.0538,0.0168,-0.0405,-0.015,-0.0031,0.0057,0.0224,-0.0674,-0.0469,0.0234,0.0259,-0.0191,0.2216,-0.0707,0.0317,0.0403,-0.0156,0.0238,0.0056,0.0135,-0.0564,0.0109,-0.0294,-0.0178,0.0035,0.05,-0.0672,0.0454,0.0092,0.0616,0.0216,-0.0383,0.0002,-0.0498,0.0078,0.0887,-0.0905,0.0245,-0.0321,0.0008,0.1113,0.0118,0.0029,0.0222,0.0065,-0.0635,0.0135,-0.0268,0.0285,0.0045,0.0547,0.0656,-0.0443,0.0122,-0.0599,0.0264,-0.0616,-0.0339,0.1381,-0.0,0.0294,-0.0553,-0.002,-0.0098,-0.0003,-0.0312,-0.0339,0.0276,0.0199,0.0862,0.0337,-0.0398,0.0501,-0.0468,-0.0637,-0.016,0.05,0.0143,-0.0855,-0.0119,0.021,0.0192,0.0219,0.0714,0.0256,-0.0515,0.057,0.0746,-0.0015,-0.0473,0.0032,0.0133,-0.0162,0.0104,-0.0381,-0.0404,0.0743,0.0469,-0.0129,-0.0182,-0.0088,0.0866,0.0403,-0.0184,-0.0193,-0.0693,0.0137,-0.0407,-0.0271,-0.0034,-0.0008,-0.0042,-0.0448,0.015,-0.0216,-0.0356,0.0135,-0.0287,0.0397,-0.0158,-0.0364,0.0469,0.0338,-0.049,-0.0064,0.0005,-0.0012,0.0121,-0.0446,0.0219,0.0113,0.0092,0.0762,0.0326,-0.0309,-0.0195,-0.2636,-0.0254,0.0085,-0.022,0.0179,-0.0431,0.0085,0.0027,0.0843,0.0741,0.0607,-0.0608,0.0078,0.0264,0.0227,0.0725,-0.0038,0.0206,-0.032,0.0139,-0.0481,0.0338,0.0024,-0.0967,0.0527,0.0003,0.2043,0.0757,0.0115,-0.0381,0.0371,0.0021,-0.0222,-0.0685,0.0634,0.0183,0.0879,-0.0452,-0.0489,-0.0302,-0.0557,-0.0026,0.0221,-0.0744,-0.0279,-0.0188,-0.0277,-0.008,-0.0459,0.0688,0.0444,-0.0217,0.0664,-0.0062,-0.0136,-0.083,-0.093,0.0343,-0.0271,0.0299,0.0162,-0.0453,0.0216,-0.0892,0.0602,-0.0122,-0.0485,-0.0422,0.0773,-0.0026,-0.006,0.0901,0.0001,0.0063,0.0378,-0.0125,0.0064,-0.0549,-0.0655,-0.0376,0.0993,-0.0141,0.0205,0.0131,0.0165,-0.0172,0.0457,0.0186,0.0419,-0.0243,0.0123,0.0128,-0.0285,-0.037,0.0277,0.0276,-0.2697,0.0135,-0.0097,0.0349,-0.0235,-0.0031,0.0737,0.0063,0.0125,0.0087,-0.0129,0.0384,0.0018,-0.0486,0.0247,0.0632,0.0248,-0.0336,0.0262,-0.0645,0.0477,0.0635,0.2269,-0.0181,-0.0005,0.018,-0.0128,0.0076,0.0761,-0.0347,0.0172,-0.0015,0.0802,-0.0293,0.0275,0.0657,-0.0256,0.0228,-0.0072,-0.0384,-0.0452,-0.0056,-0.0445,-0.0562,0.1274,-0.0595,-0.012,-0.0244,0.0329,0.048,-0.0267,-0.0544,-0.0393,0.0411,-0.0044,0.0685,-0.0413,-0.0376,-0.0518,-0.0617,-0.0156,-0.0456,0.0064,-0.0032,0.0084]}
{"key":"[Modulating Bottom-Up and Top-Down Visual Processing via Language-Conditional Filters] How to best integrate linguistic and perceptual processing in multi-modal tasks that involve language and vision is an important open problem. In this work, we argue that the common practice of using language in a top-down manner, to direct visual attention over high-level visual features, may not be optimal. We hypothesize that the use of language to also condition the bottom-up processing from pixels to high-level features can provide benefits to the overall performance. To support our claim, we propose a model for language-vision problems involving dense prediction, and perform experiments on two different multi-modal tasks: image segmentation from referring expressions and language-guided image colorization. We compare results where either one or both of the top-down and bottom-up visual branches are conditioned on language. Our experiments reveal that using language to control the filters for bottom-up visual processing in addition to top-down attention leads to better results on both tasks and achieves state-of-the-art performance. Our analysis of different word types in input expressions suggest that the bottom-up conditioning is especially helpful in the presence of low level visual concepts like color.","layer":5,"vector":[-0.0328,-0.0144,0.0072,-0.0208,-0.0031,0.0528,0.057,0.0058,0.0164,-0.0151,0.0085,-0.067,0.0473,0.0768,0.0265,0.0281,0.0142,0.0331,-0.0473,-0.0442,0.0634,-0.0288,-0.002,-0.0487,-0.008,0.0065,-0.0201,-0.056,-0.0421,-0.2132,-0.0022,-0.0347,0.0377,-0.0203,-0.0053,-0.042,-0.0337,0.0583,-0.0405,-0.0026,0.0039,-0.0085,-0.0467,-0.0572,-0.0334,-0.0851,-0.0304,-0.0344,-0.0701,-0.0343,0.0282,-0.0531,0.0558,0.0148,-0.0052,0.0568,0.0718,0.0469,0.0418,0.033,0.0185,0.0518,-0.1565,0.0902,0.0035,0.0064,-0.0508,0.0085,0.0019,0.0326,-0.0031,0.0391,0.0023,0.0516,0.0166,-0.0639,-0.0066,-0.0552,-0.0036,-0.0374,0.0241,-0.0012,-0.0347,-0.0306,-0.0085,-0.0125,0.0014,-0.0078,0.0297,-0.0128,-0.038,-0.0577,-0.0452,0.004,-0.0075,-0.02,0.0145,0.0157,-0.0276,0.1919,-0.0297,0.0271,0.0439,-0.0857,0.0325,-0.0442,0.0003,0.027,-0.0317,0.0098,-0.0603,-0.0318,0.0419,0.0366,0.0302,-0.0013,0.0846,0.0208,0.0074,-0.0311,-0.0271,-0.0314,0.0274,-0.063,0.024,-0.0771,0.0658,0.1487,0.0503,0.0211,0.0909,-0.0441,-0.0037,-0.0553,-0.0095,-0.0233,0.0567,0.0009,0.0344,-0.0122,-0.0429,-0.0396,-0.0034,-0.0709,-0.0218,0.0974,-0.0944,0.0584,-0.0179,-0.0297,-0.0371,0.0471,-0.0041,-0.0091,0.0082,0.0017,0.0336,-0.0092,-0.0292,0.0244,0.0062,-0.05,-0.0691,0.0539,0.0231,-0.0372,-0.0743,-0.0072,-0.0046,-0.0216,-0.0026,0.0375,0.0118,-0.0204,0.0531,0.0633,-0.1183,0.0224,0.0103,0.0005,0.0618,-0.0471,0.0247,0.0301,0.0462,-0.0692,0.0028,-0.0158,0.0552,0.0449,0.0004,0.0678,-0.0189,-0.0135,-0.0097,-0.0253,-0.0307,-0.0104,-0.0288,-0.0171,0.0087,0.015,-0.0243,0.0279,-0.0089,-0.0137,-0.0306,0.038,0.0665,0.0296,-0.0119,-0.0,0.0656,0.0278,-0.0044,-0.0329,0.0369,0.0176,0.0227,0.0373,0.0401,-0.0557,-0.0705,-0.2504,0.0021,0.0605,-0.0216,0.0324,-0.0731,0.0157,-0.0017,0.0499,0.0607,0.0396,-0.0653,-0.0004,0.0351,0.0193,0.0545,0.0116,0.0544,-0.0298,-0.0121,0.0065,0.0273,-0.0264,-0.0906,0.0357,-0.0152,0.2426,0.0414,0.0311,-0.0463,0.0231,0.0239,-0.0564,-0.0936,0.0637,0.0225,0.0445,-0.0184,-0.0095,-0.0255,-0.0034,-0.0017,0.0151,-0.0778,-0.0363,-0.0432,-0.02,0.0157,-0.0103,0.0144,0.0217,-0.0536,0.0216,0.0395,-0.0283,-0.0517,-0.0709,-0.0037,-0.043,0.0445,0.021,-0.0516,0.0223,-0.0345,0.0494,0.007,0.0006,-0.0949,0.0386,0.0285,-0.0702,0.0651,-0.0177,0.0014,0.0323,0.1008,0.0018,0.0026,-0.0215,-0.0648,0.0857,-0.0213,0.0419,-0.0077,0.0526,0.0014,0.0881,-0.0418,0.0353,0.0174,-0.0053,0.0073,-0.0513,-0.012,0.0509,0.0009,-0.2902,0.0761,0.0479,-0.0133,0.0038,0.0535,0.0524,0.0158,-0.0192,0.0087,-0.06,-0.0001,0.0885,0.0009,-0.0308,0.021,0.0787,-0.0189,0.0721,0.0026,0.03,0.0531,0.216,-0.0359,0.0103,-0.0103,-0.0087,-0.0262,0.0055,0.018,0.0193,0.0616,0.1211,-0.053,-0.0004,0.0524,-0.0338,0.0248,0.0197,-0.0297,-0.0079,0.0693,-0.0404,-0.0672,0.0587,0.0134,-0.0275,-0.0236,-0.0137,0.0205,-0.0609,0.0021,-0.0047,0.0004,0.032,-0.0044,-0.0184,0.0037,-0.0418,-0.0174,-0.0155,-0.0756,-0.0214,0.0434,-0.0339]}
{"key":"[Dynamically Sampled Nonlocal Gradients for Stronger Adversarial Attacks] The vulnerability of deep neural networks to small and even imperceptible perturbations has become a central topic in deep learning research. Although several sophisticated defense mechanisms have been introduced, most were later shown to be ineffective. However, a reliable evaluation of model robustness is mandatory for deployment in safety-critical scenarios. To overcome this problem we propose a simple yet effective modification to the gradient calculation of state-of-the-art first-order adversarial attacks. Normally, the gradient update of an attack is directly calculated for the given data point. This approach is sensitive to noise and small local optima of the loss function. Inspired by gradient sampling techniques from non-convex optimization, we propose Dynamically Sampled Nonlocal Gradient Descent (DSNGD). DSNGD calculates the gradient direction of the adversarial attack as the weighted average over past gradients of the optimization history. Moreover, distribution hyperparameters that define the sampling operation are automatically learned during the optimization scheme. We empirically show that by incorporating this nonlocal gradient information, we are able to give a more accurate estimation of the global descent direction on noisy and non-convex loss surfaces. In addition, we show that DSNGD-based attacks are on average 35% faster while achieving 0.9% to 27.1% higher success rates compared to their gradient descent-based counterparts.","layer":2,"vector":[-0.0327,-0.0233,-0.0105,-0.0335,-0.0242,0.019,0.0233,0.0189,0.022,-0.0005,-0.0006,-0.0241,0.0386,0.071,-0.0303,0.0053,-0.0385,0.0192,-0.0595,0.0051,0.0536,-0.0059,0.037,-0.0464,0.0439,0.004,0.0058,0.0047,-0.0519,-0.2702,0.0263,-0.0557,0.0513,-0.0468,-0.0033,-0.0213,-0.0353,0.0338,-0.0067,0.0389,0.0001,0.0555,-0.044,-0.073,-0.015,-0.0559,-0.0537,0.0016,-0.0074,-0.0142,0.0464,-0.0356,0.0213,0.0186,0.0523,-0.0165,0.104,0.0303,0.0973,0.0988,0.0055,0.0314,-0.1557,0.0419,0.0222,0.0473,-0.0142,-0.0187,0.0242,0.0415,-0.0187,0.0482,0.0096,0.0259,0.034,0.0258,-0.0016,-0.0212,-0.0181,-0.0099,0.0608,-0.0377,-0.0497,0.0084,-0.0146,-0.0396,0.0107,0.0022,0.0634,0.0362,-0.0202,-0.0264,-0.0254,0.0147,-0.0461,-0.0096,0.041,0.0324,-0.0901,0.2384,-0.0678,0.0354,0.0147,-0.0199,0.0352,-0.0211,-0.0435,-0.0799,-0.0326,0.0029,-0.0126,-0.0152,0.0414,-0.0008,0.046,0.0118,0.0531,0.0179,-0.0261,-0.0184,-0.0094,-0.0141,0.0382,-0.0266,0.0644,-0.0626,0.0067,0.1669,0.0577,0.0417,0.0008,-0.0339,-0.0308,-0.0269,0.0346,0.0134,-0.029,0.0178,0.0116,-0.0079,-0.0688,-0.0225,0.0079,-0.0677,-0.0,0.1092,-0.0242,0.0543,-0.0638,-0.0281,0.0058,0.0516,-0.0327,-0.0135,-0.0195,0.0246,-0.0073,0.0805,-0.0679,0.0204,-0.031,-0.0383,-0.0127,0.1467,-0.0057,-0.0746,-0.0072,0.0026,0.0009,-0.017,0.0212,0.0253,-0.0479,-0.0109,0.0438,0.0132,-0.1004,-0.0259,-0.0255,0.0388,0.003,-0.0171,-0.0406,0.0275,0.0356,-0.0121,0.0161,-0.0463,0.0385,0.0422,-0.0538,0.0053,-0.0664,-0.032,-0.0429,-0.0424,-0.02,-0.0124,0.0106,0.0091,0.0154,-0.017,-0.0343,0.0185,0.0324,0.0241,0.0058,-0.0345,0.0066,0.0225,-0.0219,-0.0353,0.0284,-0.012,-0.0258,-0.0186,0.0099,0.0391,-0.0126,0.0771,0.0129,-0.0437,-0.0373,-0.2018,-0.0185,-0.0105,-0.037,0.0684,-0.0777,0.044,-0.017,0.0641,0.0645,0.0394,-0.0085,0.0162,0.0211,0.0051,0.1085,0.0533,0.0353,-0.0227,0.0102,-0.0227,0.0127,-0.0213,-0.0516,0.0542,-0.0208,0.2087,0.0011,0.0507,-0.052,0.0186,0.0013,0.0005,-0.0963,0.0503,0.0115,0.0506,-0.0257,-0.0462,0.0006,-0.0353,-0.0049,0.0077,-0.1199,-0.0446,-0.0682,-0.0753,0.0206,-0.0509,0.0103,0.0342,-0.0084,0.0781,-0.0062,0.0383,-0.0622,-0.08,0.0253,-0.0573,0.0454,-0.0297,-0.0539,0.034,-0.0684,0.0633,0.0062,0.0118,-0.0535,0.0835,-0.0466,0.0021,0.0577,0.0399,0.0351,0.0492,-0.0125,0.0247,-0.0413,-0.0652,-0.0259,0.0513,0.0028,0.031,0.0127,0.0453,-0.0345,0.0742,-0.0155,0.0364,-0.0414,-0.0384,-0.0058,-0.0751,0.0028,0.0174,0.0251,-0.2836,0.0143,0.0394,0.042,-0.0359,0.0059,0.0958,0.0254,-0.0617,0.0124,-0.0326,0.0507,0.0297,-0.0225,0.015,-0.0214,0.0676,-0.0191,0.0725,-0.0296,0.0039,0.0669,0.2119,0.0057,-0.0064,-0.0107,0.0019,0.0355,-0.0051,-0.059,0.0044,-0.017,0.0479,-0.0384,0.0129,0.05,0.0096,-0.0165,0.0344,-0.0224,0.001,0.0158,-0.0122,0.0226,0.0681,-0.0325,-0.0278,-0.04,0.0399,0.0146,-0.0341,-0.0042,0.023,-0.0127,0.0617,0.0259,-0.0475,-0.0427,-0.0501,0.0125,0.0562,-0.0332,-0.0433,-0.0574,-0.0054]}
{"key":"[Neural Networks with Divisive normalization for image segmentation with application in cityscapes dataset] One of the key problems in computer vision is adaptation: models are too rigid to follow the variability of the inputs. The canonical computation that explains adaptation in sensory neuroscience is divisive normalization, and it has appealing effects on image manifolds. In this work we show that including divisive normalization in current deep networks makes them more invariant to non-informative changes in the images. In particular, we focus on U-Net architectures for image segmentation. Experiments show that the inclusion of divisive normalization in the U-Net architecture leads to better segmentation results with respect to conventional U-Net. The gain increases steadily when dealing with images acquired in bad weather conditions. In addition to the results on the Cityscapes and Foggy Cityscapes datasets, we explain these advantages through visualization of the responses: the equalization induced by the divisive normalization leads to more invariant features to local changes in contrast and illumination.","layer":9,"vector":[-0.0454,-0.0394,0.0374,0.037,0.056,0.0493,0.0248,-0.0105,0.0082,-0.0104,-0.0431,-0.0508,0.0276,0.0607,0.015,0.0148,-0.0088,0.0453,-0.0288,0.032,0.0163,-0.0194,-0.0007,-0.0774,0.018,0.0105,-0.0054,-0.0033,-0.0099,-0.2418,0.0407,-0.048,0.0474,-0.0028,0.0243,-0.0624,-0.0493,0.0162,-0.034,0.0436,0.0449,0.0301,-0.0477,-0.062,-0.0327,-0.0699,-0.0406,-0.0242,-0.0055,-0.0698,0.0824,-0.0662,0.0167,0.0421,0.0304,0.0308,0.0484,0.0468,0.0549,0.0471,0.0126,0.0503,-0.1795,0.051,0.0824,-0.007,-0.041,-0.0201,0.0367,0.0176,-0.0134,0.0149,0.033,0.0119,0.0218,-0.03,-0.0186,-0.0471,-0.0164,0.0292,0.0182,0.031,-0.0094,-0.025,0.0409,-0.0325,-0.015,-0.0425,0.0173,-0.0467,-0.0885,-0.0615,-0.0563,0.0337,-0.0161,-0.0137,0.0215,-0.0014,-0.0228,0.1941,-0.0909,0.0339,0.0605,-0.0187,0.059,-0.0075,-0.0387,-0.0253,-0.0628,0.0428,-0.0035,-0.0413,0.0219,-0.0067,0.0247,-0.0241,0.0321,0.0799,0.0162,-0.0078,-0.0406,0.0089,0.0124,-0.0147,0.0395,-0.0557,0.0265,0.1356,0.0584,0.0519,0.0411,-0.0234,-0.0319,-0.0271,0.0106,-0.0014,0.0176,-0.008,-0.0279,0.0104,-0.0363,-0.0187,0.041,-0.0814,-0.021,0.0863,-0.0852,0.0088,-0.0269,-0.02,-0.0021,-0.0148,-0.0429,-0.0409,0.0548,0.0211,0.0224,0.0183,-0.0243,-0.0006,-0.0033,-0.0664,-0.0358,0.1176,0.009,-0.0643,-0.0108,0.0188,0.0196,-0.0049,0.0179,0.015,-0.0212,0.0439,0.0902,0.0122,-0.0833,-0.0342,0.0194,-0.0005,0.0273,-0.0336,0.0085,0.045,0.049,-0.0432,0.0027,-0.0152,0.0147,0.0369,-0.0473,0.0223,-0.0753,-0.0069,-0.0132,-0.0082,-0.0098,-0.0002,-0.0181,-0.037,0.0323,0.0154,-0.005,-0.0451,-0.0175,0.034,-0.0169,-0.0134,0.0226,0.0286,-0.0064,0.0135,0.0442,-0.0262,-0.0085,0.0101,0.043,0.0373,0.0235,0.0343,0.059,-0.0338,-0.0784,-0.2067,0.0066,0.0023,-0.0392,0.0657,-0.0675,0.0182,0.0066,0.0237,0.0415,0.0459,-0.0541,0.0375,0.0045,-0.0029,0.069,0.0284,0.0548,-0.0259,-0.0364,-0.0103,0.0387,-0.0271,-0.1263,0.0827,0.0215,0.2359,0.0026,0.0668,-0.0308,-0.0123,0.02,-0.0087,-0.0875,0.0329,0.0422,0.0486,-0.0017,-0.0599,-0.0594,0.0223,-0.0097,0.023,-0.0729,-0.0239,-0.0177,-0.002,0.0096,-0.0112,-0.0491,-0.0315,-0.0523,0.0439,0.0101,-0.0015,-0.0382,-0.0879,0.0074,-0.0492,0.0415,0.0159,-0.0766,0.0214,-0.0889,0.0687,0.0109,-0.0415,-0.0623,0.0204,0.011,-0.0666,0.064,0.0173,-0.0252,0.0497,0.0142,0.0376,0.0417,-0.0224,-0.0154,0.0719,-0.0393,0.0103,-0.0168,0.0501,0.0121,0.0652,-0.0179,0.0097,-0.012,0.026,0.0147,-0.0637,-0.0575,0.0626,-0.0046,-0.3015,0.0412,0.0247,-0.0112,-0.0067,0.0086,0.0318,0.0622,-0.0171,-0.057,-0.0323,0.0113,0.047,-0.0253,0.0184,0.0568,0.0435,-0.0078,0.1102,-0.0159,0.0069,0.0732,0.2278,-0.0533,0.0081,0.0152,-0.0225,-0.0068,0.0034,-0.0271,0.0408,0.0504,0.0906,-0.0454,0.049,0.0964,-0.0107,0.0285,-0.005,-0.0317,-0.0389,0.0204,-0.0049,-0.0137,0.1013,0.0043,-0.0194,0.0111,0.0135,0.0324,-0.0265,-0.017,-0.0052,-0.0028,0.0547,-0.0041,-0.0695,-0.0489,-0.0453,-0.0272,-0.0089,-0.081,-0.0056,-0.0272,-0.0182]}
{"key":"[Refinement of Unsupervised Cross-Lingual Word Embeddings] Cross-lingual word embeddings aim to bridge the gap between high-resource and low-resource languages by allowing to learn multilingual word representations even without using any direct bilingual signal. The lion's share of the methods are projection-based approaches that map pre-trained embeddings into a shared latent space. These methods are mostly based on the orthogonal transformation, which assumes language vector spaces to be isomorphic. However, this criterion does not necessarily hold, especially for morphologically-rich languages. In this paper, we propose a self-supervised method to refine the alignment of unsupervised bilingual word embeddings. The proposed model moves vectors of words and their corresponding translations closer to each other as well as enforces length- and center-invariance, thus allowing to better align cross-lingual embeddings. The experimental results demonstrate the effectiveness of our approach, as in most cases it outperforms state-of-the-art methods in a bilingual lexicon induction task.","layer":7,"vector":[-0.0195,-0.0559,0.006,0.0225,-0.0054,0.0265,0.0192,0.0077,-0.013,-0.0261,0.0209,-0.0851,0.0466,0.0489,0.0869,0.0251,0.0114,0.082,-0.0699,-0.0217,0.0802,-0.0442,0.0136,0.0007,0.01,0.0305,-0.0509,-0.0152,-0.0095,-0.236,-0.0066,-0.0565,0.0158,0.0006,-0.0087,-0.0109,-0.0699,0.0413,-0.038,0.0302,0.0551,0.0193,-0.0085,-0.0702,-0.0103,-0.0734,-0.0866,-0.0082,-0.0386,-0.0265,0.0335,-0.0262,0.0411,0.0385,0.0217,0.0448,0.0671,-0.0113,0.0191,0.0401,0.0433,0.0525,-0.157,0.0444,0.0047,0.0629,-0.0192,0.0085,0.0041,0.0703,0.0051,0.0294,0.0234,0.039,0.0534,0.0103,0.0131,0.0107,-0.0189,-0.0024,0.0366,0.0205,-0.0461,-0.0204,0.0305,-0.037,0.0156,-0.039,0.0089,0.0071,-0.0449,-0.0446,-0.0302,0.0214,-0.0567,-0.0495,0.0491,0.0588,-0.0562,0.2132,-0.0447,0.0037,0.0521,-0.045,0.0158,-0.0318,-0.0401,0.001,-0.0215,-0.0119,-0.0405,-0.0311,0.0256,-0.0586,0.0576,-0.0119,0.0799,0.0243,-0.0091,0.0264,-0.0779,-0.0076,-0.0123,-0.0273,0.0074,-0.0372,0.0642,0.0946,0.071,0.0302,0.0534,0.0447,-0.0061,-0.0042,-0.0221,0.0593,-0.0275,-0.0068,0.0186,0.0027,0.0036,-0.0794,0.0241,-0.0626,-0.0648,0.1253,-0.0631,-0.0065,-0.0247,0.0113,-0.035,-0.0037,-0.0285,-0.0529,-0.0015,0.0301,0.0481,0.0077,-0.0515,-0.0257,-0.0084,-0.035,-0.0183,0.09,0.0218,-0.1126,-0.0322,0.008,0.0185,-0.0447,0.0629,0.0258,-0.0428,0.0211,0.0763,0.048,-0.0799,-0.0135,0.0085,0.0333,0.0263,-0.0505,-0.0224,0.0615,0.0454,-0.0312,-0.0127,-0.0433,0.0814,0.023,-0.0322,0.0629,-0.0224,-0.0125,-0.0442,-0.0155,-0.0151,-0.0335,0.0093,-0.0387,0.0129,0.0424,-0.0051,-0.0015,0.0118,0.002,0.0385,0.0255,0.0593,0.0337,-0.0158,-0.0099,0.0714,-0.0236,-0.0424,-0.0056,-0.0392,0.0794,0.034,0.0539,-0.0274,-0.0733,-0.0504,-0.2246,-0.0244,0.0241,-0.007,0.0089,-0.0648,0.0009,0.0223,0.0792,0.0639,0.05,-0.0677,-0.0186,0.0376,0.0135,0.0699,0.0629,0.0502,0.0031,0.0062,0.0179,0.011,-0.0193,-0.0428,0.0207,-0.0199,0.1931,0.0477,0.0467,-0.0443,0.0656,-0.0085,-0.0638,-0.1454,0.0624,0.0253,0.0387,-0.0384,-0.0387,-0.0275,-0.0341,0.0017,-0.0076,-0.1071,-0.0126,-0.0123,-0.0618,-0.0423,-0.0489,0.0202,0.0002,-0.0152,0.0566,0.0226,-0.0311,-0.0377,-0.0788,0.022,-0.0321,-0.0189,0.0372,-0.0132,0.0396,-0.0841,0.0166,0.0008,-0.0284,-0.0085,0.0232,0.0465,-0.0787,0.0793,-0.0723,-0.007,0.0122,0.0535,0.0054,-0.0553,-0.0509,-0.0015,0.0954,-0.0219,0.0544,0.016,0.0655,0.0183,0.0891,-0.0143,0.0354,0.014,0.0167,0.0094,-0.0384,-0.0091,0.0361,-0.0259,-0.2827,0.0617,0.0269,0.0403,0.0043,0.0015,-0.0014,-0.0215,-0.0475,0.0495,0.014,0.0167,0.0863,-0.014,-0.0095,0.0211,0.0937,-0.0255,0.0678,-0.0206,0.0006,0.0553,0.203,-0.0235,0.012,-0.0322,-0.0275,0.0059,0.0104,-0.0069,-0.0328,-0.0173,0.0845,-0.0182,0.0051,0.0815,-0.0385,0.0007,0.056,-0.0186,-0.009,0.0083,-0.0659,-0.0299,0.0248,0.0173,0.0149,-0.0266,0.0013,-0.015,-0.0427,0.0127,-0.0226,-0.0103,-0.0142,0.012,-0.0482,-0.0633,-0.0452,-0.0302,0.0114,-0.0708,-0.0618,0.0251,0.0004]}
{"key":"[Estimating uncertainty of earthquake rupture using Bayesian neural network] Bayesian neural networks (BNN) are the probabilistic model that combines the strengths of both neural network (NN) and stochastic processes. As a result, BNN can combat overfitting and perform well in applications where data is limited. Earthquake rupture study is such a problem where data is insufficient, and scientists have to rely on many trial and error numerical or physical models. Lack of resources and computational expenses, often, it becomes hard to determine the reasons behind the earthquake rupture. In this work, a BNN has been used (1) to combat the small data problem and (2) to find out the parameter combinations responsible for earthquake rupture and (3) to estimate the uncertainty associated with earthquake rupture. Two thousand rupture simulations are used to train and test the model. A simple 2D rupture geometry is considered where the fault has a Gaussian geometric heterogeneity at the center, and eight parameters vary in each simulation. The test F1-score of BNN (0.8334), which is 2.34% higher than plain NN score. Results show that the parameters of rupture propagation have higher uncertainty than the rupture arrest. Normal stresses play a vital role in determining rupture propagation and are also the highest source of uncertainty, followed by the dynamic friction coefficient. Shear stress has a moderate role, whereas the geometric features such as the width and height of the fault are least significant and uncertain.","layer":0,"vector":[-0.0601,-0.0247,-0.0006,-0.006,0.029,0.0629,0.0066,0.0281,0.0314,-0.0117,-0.0204,-0.0741,0.0349,0.082,-0.0002,0.0092,-0.0352,0.0726,0.0152,0.001,0.0403,-0.0099,-0.0079,-0.0353,0.0116,0.0385,-0.023,0.0144,-0.051,-0.222,0.0223,-0.0864,0.0439,-0.056,-0.0512,-0.0014,-0.0518,0.076,0.0174,0.0538,0.0501,-0.0005,0.026,-0.0859,0.0002,-0.001,0.0402,-0.0128,0.0157,-0.0271,0.0262,-0.0222,0.0672,-0.0148,0.0173,0.0174,0.0306,0.0248,0.0415,0.0587,0.0243,0.0216,-0.2049,0.0674,0.0676,-0.003,-0.0104,-0.0124,0.0054,0.0467,-0.044,0.0425,0.043,0.0606,0.0519,-0.0207,0.0045,-0.02,-0.03,0.0111,-0.0133,-0.0524,-0.0272,-0.0167,-0.0152,-0.0211,-0.0063,-0.0397,-0.0038,-0.0053,0.02,0.0252,-0.0247,0.0389,-0.0599,0.0095,0.0399,0.0164,-0.0535,0.1928,-0.0242,0.0501,0.0259,-0.0002,0.0446,-0.0254,-0.0418,-0.05,-0.0896,-0.0033,0.0065,-0.0311,-0.0022,-0.0101,-0.0493,0.0198,0.0067,-0.0294,-0.0427,-0.0249,-0.0593,0.014,0.0223,0.0057,0.0025,-0.0849,-0.0106,0.1059,0.0501,0.0248,0.0235,-0.0281,-0.0446,-0.0321,0.0348,0.0356,0.0096,-0.0373,-0.0018,-0.0127,-0.0488,-0.0861,-0.004,-0.0651,-0.0743,0.0745,-0.043,0.0473,-0.0332,-0.0297,-0.0096,0.0176,-0.0365,0.0012,0.0393,0.0231,0.0058,0.0438,-0.0892,-0.0153,0.0035,-0.0212,-0.0681,0.1004,0.057,-0.0781,-0.0511,0.0201,0.0346,-0.0464,0.0395,0.0471,0.0123,0.0012,0.0835,0.0468,-0.0024,-0.011,0.0098,0.0688,0.0151,-0.0188,0.0041,0.0253,0.014,-0.0413,0.0028,-0.046,-0.0105,0.0463,-0.0029,-0.0539,-0.0249,0.0119,-0.003,0.0002,-0.0062,-0.0206,0.0226,-0.0245,0.0088,-0.0008,-0.0805,0.023,-0.0257,0.0398,-0.008,0.0282,-0.014,0.0316,0.0085,-0.031,0.071,-0.067,0.0181,0.0372,0.0212,0.0389,0.0324,0.0741,0.0521,-0.0287,-0.0501,-0.2249,0.0235,0.0416,-0.0089,0.1405,-0.0088,0.0206,-0.0218,0.0169,0.026,0.0984,0.0042,-0.0271,-0.0127,-0.0169,0.0115,0.0234,0.004,-0.0613,-0.009,-0.0512,0.0304,-0.0882,-0.082,0.0129,-0.014,0.1669,-0.0051,0.0357,-0.0611,0.0247,0.0007,-0.0053,-0.112,0.0785,0.0381,0.0584,-0.0249,-0.0866,-0.0931,-0.0166,0.019,0.0093,-0.0792,0.0038,-0.043,-0.0459,0.0529,-0.0804,0.0146,0.0417,-0.0171,0.0946,0.0091,0.031,-0.0101,-0.0887,0.043,-0.0044,-0.0153,0.016,-0.0768,0.011,-0.0459,0.0353,0.0136,-0.0338,-0.0071,0.043,-0.0442,0.0116,0.1387,0.017,0.0219,0.0527,-0.0367,0.0324,-0.0364,-0.0039,-0.0364,0.0554,-0.063,0.0039,0.0643,-0.0064,-0.0361,0.0515,-0.0221,0.0472,-0.0257,-0.0019,0.0023,-0.0008,-0.0033,0.0127,0.0228,-0.2828,0.0877,0.002,-0.0221,-0.0425,-0.0192,0.0615,0.0122,-0.0093,-0.0244,-0.0177,0.0219,0.0484,-0.0368,-0.0286,0.0038,0.0245,-0.0307,0.0839,-0.0405,0.0151,0.0437,0.2317,-0.0387,0.0636,0.0394,-0.0384,0.0174,0.0513,-0.0459,0.0192,0.0021,0.0162,-0.0665,0.0682,0.0848,0.008,0.0387,0.0333,-0.0189,0.0034,-0.0124,-0.0215,-0.0226,0.1375,-0.0391,-0.0451,-0.0462,-0.0274,0.0306,-0.0323,0.0228,0.0217,-0.0154,0.0094,0.0337,-0.0585,-0.0005,-0.0584,-0.0172,0.0341,-0.0142,0.0313,-0.0208,-0.0248]}
{"key":"[Exoplanet Characterization using Conditional Invertible Neural Networks] The characterization of an exoplanet's interior is an inverse problem, which requires statistical methods such as Bayesian inference in order to be solved. Current methods employ Markov Chain Monte Carlo (MCMC) sampling to infer the posterior probability of planetary structure parameters for a given exoplanet. These methods are time consuming since they require the calculation of a large number of planetary structure models. To speed up the inference process when characterizing an exoplanet, we propose to use conditional invertible neural networks (cINNs) to calculate the posterior probability of the internal structure parameters. cINNs are a special type of neural network which excel in solving inverse problems. We constructed a cINN using FrEIA, which was then trained on a database of $5.6\\cdot 10^6$ internal structure models to recover the inverse mapping between internal structure parameters and observable features (i.e., planetary mass, planetary radius and composition of the host star). The cINN method was compared to a Metropolis-Hastings MCMC. For that we repeated the characterization of the exoplanet K2-111 b, using both the MCMC method and the trained cINN. We show that the inferred posterior probability of the internal structure parameters from both methods are very similar, with the biggest differences seen in the exoplanet's water content. Thus cINNs are a possible alternative to the standard time-consuming sampling methods. Indeed, using cINNs allows for orders of magnitude faster inference of an exoplanet's composition than what is possible using an MCMC method, however, it still requires the computation of a large database of internal structures to train the cINN. Since this database is only computed once, we found that using a cINN is more efficient than an MCMC, when more than 10 exoplanets are characterized using the same cINN.","layer":0,"vector":[-0.0481,-0.0211,0.0123,-0.0252,0.0076,0.0518,0.0255,0.0501,0.0185,0.0004,-0.0291,-0.0847,0.0531,0.0342,-0.0036,-0.0002,0.0067,0.0563,-0.0017,0.0103,0.0255,-0.0582,-0.007,-0.0102,0.0334,0.0476,0.0047,-0.0094,-0.052,-0.2596,0.0206,-0.0547,0.068,-0.0392,0.0025,-0.0262,-0.0421,0.0202,-0.0154,0.0428,0.0734,-0.0208,-0.0007,-0.0321,-0.0142,-0.06,-0.023,-0.0205,-0.0002,-0.0177,0.0384,-0.0493,-0.0013,0.0239,0.0252,0.0596,0.0414,0.023,0.0713,0.0363,-0.02,0.064,-0.1754,0.0459,0.0493,0.0695,-0.0388,-0.0385,0.0425,-0.0022,-0.0252,0.0394,0.0494,0.0575,0.0161,0.0156,0.0022,-0.0807,-0.044,0.0193,0.0331,-0.0227,-0.0417,0.0076,0.0108,-0.0149,-0.0006,-0.0052,0.0284,0.0141,-0.0206,-0.0429,-0.0067,0.0388,-0.0778,0.0228,0.0471,-0.002,0.0122,0.2039,-0.0182,0.0315,0.0457,-0.0103,0.0206,-0.0362,-0.0215,-0.009,-0.0151,0.0258,-0.0219,-0.028,0.0003,-0.0378,-0.0248,-0.0267,0.0311,0.0417,-0.0043,-0.0292,-0.0281,0.0099,0.0636,0.0093,0.0212,-0.081,-0.0183,0.068,0.0072,0.0466,0.0461,-0.049,-0.0855,-0.0193,0.0398,0.0073,0.048,-0.0166,-0.0459,-0.0324,-0.0365,-0.1272,0.0262,-0.0537,-0.0714,0.1099,-0.04,0.0259,-0.0525,-0.03,-0.0356,0.0511,-0.0279,-0.0588,0.0389,0.0441,-0.021,0.018,-0.091,0.04,-0.0055,-0.0238,-0.0893,0.1343,0.0628,-0.0602,-0.0187,0.0015,0.0182,0.0139,0.0265,0.0204,-0.0535,0.0264,0.1242,0.016,-0.0811,0.0099,0.0191,0.0244,-0.0,-0.0494,-0.029,0.0502,0.0427,-0.0441,-0.0024,-0.022,-0.0395,0.027,-0.0326,0.0119,-0.0176,0.0098,-0.1058,-0.02,0.0223,0.0048,-0.0074,-0.0475,0.0454,-0.0203,-0.0537,0.0306,-0.0406,0.019,0.0121,0.0535,0.0387,0.0151,-0.022,-0.0093,0.0537,-0.0524,-0.004,0.0236,0.0351,0.0415,0.0188,0.0239,0.0331,-0.0828,-0.0694,-0.2348,0.0172,0.042,-0.0063,0.075,-0.0388,0.0265,-0.042,0.0665,0.0276,0.0357,-0.0078,-0.0078,0.0408,-0.0544,0.0126,0.0163,0.034,0.0015,-0.0128,-0.0002,0.0104,-0.0183,-0.0405,0.0267,0.0053,0.1981,0.0085,0.0641,-0.0116,-0.0045,0.0043,-0.0206,-0.0923,0.088,0.0206,0.0848,-0.0085,-0.1164,-0.0114,-0.0181,0.0151,0.0002,-0.0987,-0.0206,-0.0045,-0.0325,0.0638,-0.0448,0.0015,0.0321,0.0012,0.0434,-0.0156,-0.0089,-0.0204,-0.0959,-0.0097,-0.0286,0.0364,0.0213,-0.0212,-0.0452,-0.0521,0.0619,-0.0213,-0.0181,-0.0506,0.0424,-0.0611,-0.0222,0.0899,-0.0176,0.0047,0.0472,0.0019,0.0639,-0.0018,-0.0194,-0.0133,0.0807,-0.0557,0.0478,0.0259,-0.0048,0.0239,0.0398,-0.04,0.0464,-0.0368,0.0257,0.0493,-0.0548,0.0183,0.0432,0.0274,-0.2802,0.0368,-0.0132,0.0282,-0.0214,0.0069,0.0457,0.0347,-0.032,-0.0036,-0.018,0.0645,0.0203,-0.0016,-0.0224,0.0004,0.0209,-0.0419,0.0587,-0.0645,-0.0378,0.0409,0.2223,-0.0518,0.033,0.0214,0.0058,0.019,0.0247,-0.0026,0.0429,-0.0054,0.0536,-0.044,0.0496,0.1002,-0.0357,0.0467,0.0354,-0.0371,0.0045,-0.0014,-0.0663,-0.0541,0.112,0.0275,-0.009,-0.028,-0.004,-0.0251,-0.0365,0.0189,-0.0199,-0.0179,0.004,0.0022,-0.0343,-0.0496,-0.0192,-0.0285,0.0612,-0.0472,0.0255,-0.0087,-0.0055]}
{"key":"[Anomaly detection in the dynamics of web and social networks] In this work, we propose a new, fast and scalable method for anomaly detection in large time-evolving graphs. It may be a static graph with dynamic node attributes (e.g. time-series), or a graph evolving in time, such as a temporal network. We define an anomaly as a localized increase in temporal activity in a cluster of nodes. The algorithm is unsupervised. It is able to detect and track anomalous activity in a dynamic network despite the noise from multiple interfering sources. We use the Hopfield network model of memory to combine the graph and time information. We show that anomalies can be spotted with a good precision using a memory network. The presented approach is scalable and we provide a distributed implementation of the algorithm. To demonstrate its efficiency, we apply it to two datasets: Enron Email dataset and Wikipedia page views. We show that the anomalous spikes are triggered by the real-world events that impact the network dynamics. Besides, the structure of the clusters and the analysis of the time evolution associated with the detected events reveals interesting facts on how humans interact, exchange and search for information, opening the door to new quantitative studies on collective and social behavior on large and dynamic datasets.","layer":2,"vector":[-0.0601,-0.0061,0.0125,0.0005,0.062,0.0258,0.0604,0.0062,0.0568,-0.0372,0.0356,-0.0104,0.0389,0.0636,-0.0042,0.0204,-0.0369,-0.0132,-0.0171,-0.0154,-0.0032,-0.0326,0.0266,-0.0385,0.0215,0.0593,-0.0368,-0.0235,-0.0727,-0.2096,0.0451,-0.0841,0.0451,-0.0158,0.0444,-0.0428,0.0122,0.0342,-0.0004,0.0551,0.0235,0.045,-0.0238,-0.0689,-0.0583,-0.0296,0.033,-0.0205,-0.0393,-0.0373,0.0044,-0.0332,0.0657,0.0092,0.0322,0.0481,0.0759,0.0271,0.0711,0.058,0.0763,0.0524,-0.1204,0.0571,0.0643,0.043,-0.0344,0.0158,0.0262,0.0236,0.0062,0.0502,-0.0129,0.0518,0.0424,0.0562,-0.0449,-0.0377,-0.0061,0.0026,-0.0218,-0.0489,-0.0203,-0.0228,0.0198,-0.0602,0.0263,-0.0138,0.0454,-0.0253,-0.0179,0.0248,0.0145,0.0166,-0.0551,0.0017,0.028,0.0501,-0.0147,0.2341,-0.0671,0.0283,0.0448,-0.0136,0.0231,-0.0425,-0.0108,-0.0565,0.0084,-0.0044,-0.0027,-0.0493,0.0326,-0.071,0.0574,-0.0083,0.0441,0.0285,-0.0221,-0.0081,-0.0363,0.0135,0.0695,-0.0703,0.0305,-0.035,-0.009,0.1477,0.0144,0.0096,-0.0135,0.0677,-0.0486,-0.0078,-0.0027,0.0081,-0.0167,-0.0072,0.0203,-0.0327,-0.0364,-0.0485,0.0122,-0.0658,-0.0839,0.1212,0.0132,0.0107,-0.0298,-0.0333,-0.0619,0.0149,-0.0486,-0.0455,-0.0132,0.0359,0.0483,0.0616,-0.0696,0.0365,-0.0601,-0.0419,-0.0263,0.0942,0.0418,-0.1049,0.0076,-0.0246,-0.0096,-0.0336,0.0309,0.0227,-0.0063,0.0341,0.0535,0.0219,-0.0537,0.0009,-0.006,-0.0377,0.0441,-0.0175,-0.0398,0.0659,0.0248,-0.0429,0.0037,0.0052,0.0506,0.0383,-0.0312,-0.0037,-0.0445,-0.0288,-0.0382,-0.0279,-0.0104,-0.0133,0.0246,-0.0496,0.0222,-0.0228,-0.0542,-0.0121,0.0034,0.032,-0.0564,-0.0097,-0.0302,-0.003,-0.0209,-0.0115,0.0019,-0.0213,-0.0064,-0.0055,0.0091,0.0346,0.0064,0.026,0.0467,-0.0496,-0.0354,-0.2711,-0.0422,0.0166,-0.0383,0.0469,-0.0542,-0.0111,-0.0333,0.0554,0.0611,0.0412,0.0341,-0.0214,-0.034,0.01,0.0834,0.0105,0.0569,-0.0174,0.0247,-0.0301,-0.0422,-0.0128,-0.0897,0.0311,0.0142,0.2046,0.0598,0.0233,-0.0665,0.0138,-0.0063,-0.0348,-0.0782,0.0651,0.0653,0.0654,-0.0165,-0.0403,-0.0262,-0.0832,0.0118,-0.0203,-0.0527,-0.0284,-0.0154,0.0228,-0.0373,-0.0452,-0.008,0.0754,-0.0014,0.0814,0.0562,0.0016,-0.103,-0.054,0.0177,-0.0218,0.036,-0.0005,-0.0391,0.0128,-0.0566,0.0693,-0.0084,-0.0315,-0.0546,0.0069,-0.015,-0.0182,0.0954,0.0022,-0.0218,0.0522,0.006,0.0117,-0.0472,-0.0382,-0.0044,0.0825,-0.0553,0.0626,0.0047,0.0052,-0.0165,0.0378,-0.0036,0.0467,-0.0045,0.0316,0.0026,-0.0655,-0.0447,-0.0264,-0.0061,-0.3054,0.0322,-0.0127,0.0333,-0.0121,0.0202,0.0136,0.0466,-0.0006,-0.0233,-0.0216,0.0354,0.044,-0.0076,-0.0034,0.094,0.0492,-0.0128,0.0134,-0.0316,0.0402,0.0485,0.2276,0.0148,0.0398,0.0386,0.0137,-0.003,0.0234,-0.009,0.0045,0.001,0.0476,-0.0591,0.0524,0.0356,-0.0093,0.0294,-0.0007,-0.017,-0.0283,0.0078,-0.0417,-0.0177,0.0968,-0.0198,-0.0003,-0.0826,0.0122,0.0682,-0.0316,-0.0238,-0.0462,0.0515,0.0217,0.0352,-0.0257,-0.0318,-0.0598,-0.053,-0.0106,-0.0325,-0.0049,-0.0251,-0.0123]}
{"key":"[Energy-efficient Training of Distributed DNNs in the Mobile-edge-cloud Continuum] We address distributed machine learning in multi-tier (e.g., mobile-edge-cloud) networks where a heterogeneous set of nodes cooperate to perform a learning task. Due to the presence of multiple data sources and computation-capable nodes, a learning controller (e.g., located in the edge) has to make decisions about (i) which distributed ML model structure to select, (ii) which data should be used for the ML model training, and (iii) which resources should be allocated to it. Since these decisions deeply influence one another, they should be made jointly. In this paper, we envision a new approach to distributed learning in multi-tier networks, which aims at maximizing ML efficiency. To this end, we propose a solution concept, called RightTrain, that achieves energy-efficient ML model training, while fulfilling learning time and quality requirements. RightTrain makes high-quality decisions in polynomial time. Further, our performance evaluation shows that RightTrain closely matches the optimum and outperforms the state of the art by over 50%.","layer":2,"vector":[-0.0379,-0.0316,0.026,-0.0189,0.0531,0.06,0.0022,-0.0035,0.0075,-0.0017,0.049,-0.0465,0.0092,0.0718,0.0432,0.032,-0.0045,-0.0192,-0.0494,-0.0035,0.0131,-0.0556,-0.0336,-0.0282,0.0317,0.0247,-0.0424,-0.0486,-0.0739,-0.2273,0.0287,-0.0538,0.0467,0.0125,-0.0207,-0.0308,0.0107,0.0118,-0.0286,0.01,0.0099,0.0221,-0.0494,-0.0845,-0.0143,-0.0148,-0.032,-0.0384,-0.0248,-0.0458,0.0318,-0.0434,-0.0563,0.0117,0.0359,0.0758,0.0171,0.063,0.0368,0.0474,-0.013,0.0721,-0.163,0.055,0.0368,-0.0049,-0.0571,-0.0068,0.0041,0.0393,0.0102,0.0151,0.0448,0.0456,-0.0144,0.0035,0.0091,0.0058,0.0247,-0.0074,-0.0075,-0.0353,-0.0483,-0.0337,0.0113,-0.0056,0.0218,-0.0386,0.0236,0.0073,-0.0364,0.024,-0.0135,0.0365,-0.0415,0.0111,0.0263,0.0205,-0.0614,0.2007,-0.0509,0.0236,0.0184,-0.0222,0.0055,-0.01,-0.0351,-0.044,-0.0476,0.0245,-0.0207,-0.0124,0.0106,-0.0266,0.0343,0.0235,0.0403,0.0549,0.0014,-0.0093,-0.0447,0.0041,0.0595,-0.0246,0.0376,-0.0751,0.0318,0.1474,-0.0067,0.0376,0.0038,-0.0114,-0.0278,-0.0364,0.0497,0.0248,0.0113,-0.0624,0.0145,0.0182,-0.0105,-0.026,0.0542,-0.091,-0.0458,0.1159,-0.0215,0.0416,-0.0446,-0.059,-0.0277,-0.011,-0.0255,0.0281,0.0259,0.0446,0.025,0.0407,-0.069,-0.0141,-0.0294,0.0179,-0.041,0.1353,-0.0092,-0.0961,0.0118,0.0093,0.0325,-0.0637,0.058,0.0137,-0.061,0.0519,0.0336,0.0084,-0.1179,-0.033,-0.0291,0.019,0.007,-0.0157,0.0063,0.0435,0.0568,-0.0271,0.0053,-0.0415,0.0172,0.0592,-0.0582,0.0417,-0.0258,-0.0235,0.0115,-0.0628,-0.0054,0.0102,0.0091,-0.0228,0.0,0.0043,-0.0477,0.0111,-0.0187,0.0362,-0.013,0.0445,0.0342,0.0219,-0.0108,-0.0094,0.0729,-0.0114,-0.0209,0.0272,0.0585,0.0658,-0.0068,0.0696,0.0659,-0.0103,-0.0657,-0.1945,-0.0191,0.0153,-0.0081,0.0624,-0.0402,0.0454,-0.0013,0.0427,0.045,0.112,-0.0127,-0.0499,-0.0099,-0.0187,0.0482,0.0918,0.0303,-0.0128,0.0213,0.006,0.0024,-0.021,-0.0999,0.0883,0.0184,0.2418,-0.0131,0.0862,-0.0331,0.0366,0.0284,-0.0248,-0.0973,0.0232,-0.0047,0.0688,-0.0314,-0.0258,-0.0464,-0.0504,0.0586,0.0142,-0.1611,-0.0461,-0.0411,-0.0745,0.0353,-0.0459,-0.0235,0.0219,-0.0299,0.0671,0.0065,-0.031,-0.0191,-0.0868,0.0514,-0.0843,0.0024,0.0016,-0.0625,-0.002,-0.0607,0.0553,-0.0309,-0.0452,-0.0399,-0.0121,-0.0196,-0.0104,0.0443,0.0107,0.0218,0.03,-0.0138,0.0662,-0.0108,-0.0331,-0.0147,0.0644,-0.0711,0.0567,0.0536,0.0375,0.0047,0.0569,0.0081,0.0104,-0.0338,-0.0172,0.0413,-0.0495,-0.0307,0.0485,-0.0077,-0.286,0.0649,0.0141,0.0378,-0.0655,0.0273,0.0623,0.0125,-0.0537,0.0208,0.0266,0.0453,0.0274,0.0209,0.0288,0.0375,0.0733,-0.0464,0.0078,-0.0394,0.0184,0.0348,0.167,-0.0388,0.0387,0.0438,-0.018,-0.0161,0.0321,0.027,-0.0041,0.0555,0.0598,-0.0655,0.0204,0.0659,-0.0781,0.0125,0.0506,-0.0362,-0.0133,0.0025,-0.0171,-0.0081,0.0359,0.0274,0.0016,-0.0707,-0.0135,0.0476,0.0184,-0.0179,-0.0101,0.0121,-0.0225,0.0735,-0.0268,-0.0606,-0.0377,-0.067,0.021,-0.0706,-0.0063,-0.0405,-0.0081]}
{"key":"[Interpretability Aware Model Training to Improve Robustness against Out-of-Distribution Magnetic Resonance Images in Alzheimer's Disease Classification] Owing to its pristine soft-tissue contrast and high resolution, structural magnetic resonance imaging (MRI) is widely applied in neurology, making it a valuable data source for image-based machine learning (ML) and deep learning applications. The physical nature of MRI acquisition and reconstruction, however, causes variations in image intensity, resolution, and signal-to-noise ratio. Since ML models are sensitive to such variations, performance on out-of-distribution data, which is inherent to the setting of a deployed healthcare ML application, typically drops below acceptable levels. We propose an interpretability aware adversarial training regime to improve robustness against out-of-distribution samples originating from different MRI hardware. The approach is applied to 1.5T and 3T MRIs obtained from the Alzheimer's Disease Neuroimaging Initiative database. We present preliminary results showing promising performance on out-of-distribution samples.","layer":2,"vector":[-0.0132,-0.0314,0.0321,0.0025,0.0316,0.0251,0.0261,-0.0163,-0.0018,-0.0078,0.0143,-0.0502,0.0344,0.0684,-0.0153,0.0004,-0.0326,0.0369,-0.0334,0.0531,0.0028,-0.0454,0.0099,-0.0178,0.0081,0.0445,-0.0291,-0.0415,-0.0798,-0.2627,0.0132,-0.0554,0.007,-0.0424,0.027,-0.0232,-0.0378,0.0388,-0.0459,0.0068,0.0205,0.0262,-0.0178,-0.0979,-0.0389,-0.0193,-0.0368,-0.0268,-0.0117,-0.0132,0.0341,0.004,0.026,0.0444,0.0254,0.0201,0.0212,0.0227,0.0299,0.068,0.0306,0.0916,-0.1261,0.0522,0.0463,0.0285,-0.0377,-0.0343,0.0078,0.0175,0.0442,0.0153,0.0121,0.0444,0.0268,0.0125,-0.003,-0.0319,-0.0163,-0.0154,0.0892,0.0054,-0.0277,0.0012,-0.0335,-0.0639,-0.0066,-0.0573,0.0362,-0.0105,-0.0205,0.034,-0.0535,0.0593,-0.0376,-0.0261,0.0332,-0.0185,-0.0647,0.2097,-0.0419,-0.0027,0.0513,-0.0316,0.019,-0.0128,-0.0372,-0.0179,-0.0383,0.0035,-0.041,-0.0123,0.0343,-0.0298,0.0182,0.0216,0.0314,0.0117,-0.0087,-0.0084,-0.0722,0.0197,0.049,-0.018,0.0404,-0.0397,0.0365,0.1702,0.0443,-0.0086,-0.0152,-0.0187,-0.0153,-0.0132,0.0352,0.0256,-0.004,0.0125,0.0329,0.0406,-0.0337,-0.053,0.0442,-0.0488,-0.0584,0.1031,-0.0357,0.034,-0.074,-0.0083,-0.0277,0.0465,-0.0547,-0.0093,0.0253,0.0161,-0.0243,0.0288,-0.0528,-0.0246,-0.0082,-0.1232,-0.0671,0.1113,-0.0036,-0.0616,-0.0206,-0.006,0.0315,-0.0215,0.0736,0.0112,0.0015,0.019,0.0657,0.0089,-0.0546,-0.04,-0.0365,0.0037,0.0055,-0.0724,-0.0508,0.0195,0.0316,-0.0465,0.0386,-0.035,0.054,0.0039,-0.0246,0.0218,-0.0613,-0.0196,-0.0637,-0.0338,-0.03,0.0079,-0.0158,0.0071,-0.0098,0.0063,-0.051,0.0413,-0.0051,0.0399,0.016,-0.0152,0.0103,0.0968,-0.0381,-0.0026,0.0592,0.0018,0.0012,0.0116,0.0098,0.0359,0.0286,0.0379,0.0616,-0.0305,-0.0604,-0.2361,-0.025,0.0372,-0.0011,0.0232,-0.1044,0.0261,-0.0018,0.0697,0.0825,0.0557,0.0061,-0.0259,0.0066,-0.0009,0.0494,0.0053,0.0264,-0.0341,0.0074,0.0018,0.0599,0.014,-0.0571,0.061,0.0441,0.2118,-0.0144,0.0316,-0.0078,-0.0286,0.03,-0.0074,-0.0832,0.0676,-0.0289,0.0431,-0.0003,-0.0393,-0.0019,-0.0431,0.0266,0.0129,-0.0915,-0.0511,0.0026,-0.0483,0.0492,-0.0674,0.057,0.0222,-0.0516,0.0599,0.0048,0.0228,-0.0112,-0.1222,0.042,-0.0528,0.0087,-0.0021,-0.0642,0.0022,-0.0749,0.0568,0.0007,-0.03,-0.07,0.0267,-0.0236,0.0107,0.0317,0.0062,0.034,0.0705,0.0208,0.0402,-0.0384,-0.0715,-0.0002,0.054,-0.0019,0.0408,0.0113,0.0571,0.0086,0.0603,-0.029,-0.008,-0.0088,-0.0301,0.0595,-0.0548,-0.0463,0.0534,-0.0292,-0.2977,0.0605,0.0453,0.0699,-0.0164,0.0347,0.0428,0.0165,-0.0606,-0.0255,-0.039,0.0283,0.0195,-0.037,0.0312,0.0251,0.0855,-0.0494,0.055,-0.0558,0.0265,0.029,0.1919,-0.0231,-0.0281,0.0211,-0.0193,0.0059,0.0104,-0.0206,0.0258,-0.031,0.0719,-0.0181,0.0621,0.1156,-0.0577,0.0316,0.0132,-0.0417,0.0149,0.0347,-0.0683,0.0093,0.0838,-0.0017,-0.0501,-0.0073,-0.0124,0.0075,-0.0165,-0.0118,-0.0045,0.0341,0.0128,0.0643,0.0046,-0.0786,0.0058,-0.037,0.038,-0.0283,-0.0291,0.0365,-0.0481]}
{"key":"[Convolutional Networks with Dense Connectivity] Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion.Whereas traditional convolutional networks with L layers have L connections - one between each layer and its subsequent layer - our network has L(L+1)/2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, encourage feature reuse and substantially improve parameter efficiency. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less parameters and computation to achieve high performance.","layer":0,"vector":[-0.0174,-0.0181,-0.011,-0.0566,0.0539,0.0237,0.049,0.0042,0.0071,-0.0098,0.0156,-0.0847,0.0568,0.1065,0.0773,-0.0182,0.071,0.0403,-0.0285,-0.0279,0.0168,-0.0436,0.0166,-0.0215,-0.0323,0.0247,0.001,-0.023,-0.0552,-0.233,-0.0072,-0.0488,0.0667,-0.0214,-0.0194,-0.0648,-0.0339,0.0705,-0.0441,-0.0077,0.0705,0.0264,-0.073,-0.0685,0.0044,-0.0451,-0.0394,-0.0586,0.0175,-0.0563,0.0335,-0.0614,0.0294,0.0293,-0.023,0.0338,0.0607,0.0791,0.05,0.0371,0.0469,0.045,-0.1169,0.0492,0.0435,0.0062,-0.0404,-0.0075,-0.0282,0.0191,0.0196,0.0343,-0.0247,-0.0133,0.0131,0.0127,-0.0086,-0.0772,-0.0097,-0.0151,0.0309,-0.0053,-0.0486,-0.0383,-0.0031,-0.0211,-0.0124,-0.0494,0.0251,0.0265,-0.0615,-0.0264,-0.0672,0.0163,-0.0192,-0.0047,0.0182,-0.0097,-0.058,0.2009,-0.0668,0.0362,0.0245,-0.0436,0.0284,-0.0156,0.0182,-0.0081,-0.0378,0.0264,-0.0143,-0.0141,0.0244,-0.007,0.0235,-0.0238,0.0868,0.0469,-0.0259,-0.0186,-0.0196,-0.0322,0.0577,-0.0381,0.0078,-0.0614,0.0267,0.1425,0.0509,0.0334,0.0699,-0.0144,-0.0806,-0.0234,0.0058,0.055,0.0224,-0.0236,-0.0092,-0.011,-0.031,-0.0249,0.0283,-0.0295,-0.0374,0.114,-0.0633,0.0231,-0.0069,-0.04,0.0013,0.0088,0.0023,-0.0369,-0.0147,-0.0036,0.0639,0.0918,-0.0571,0.0382,-0.0601,-0.0754,-0.0238,0.0937,-0.0129,-0.1238,-0.0421,-0.0449,0.0037,-0.0163,0.028,0.0403,0.003,0.0447,0.1252,0.0649,-0.091,-0.0274,-0.0234,0.047,0.0029,-0.0506,-0.0358,0.0524,0.0479,-0.0697,0.0403,-0.0927,0.0296,0.0552,-0.0384,0.0673,-0.0276,0.0275,-0.0092,-0.0425,-0.0225,0.0106,-0.0124,-0.0084,-0.0043,0.0064,0.0067,-0.0114,-0.0453,0.0015,-0.0311,0.0069,0.0164,0.025,-0.0318,0.0403,0.0398,-0.0527,-0.0269,-0.0382,0.0296,0.0382,-0.0286,0.0374,0.0108,-0.08,-0.0376,-0.2034,-0.0042,0.0371,-0.0367,0.0463,-0.0644,0.0274,0.0085,0.0347,0.0516,0.0696,0.0164,-0.0344,0.03,0.0408,0.0877,0.0006,0.0426,-0.0209,-0.0365,-0.0142,0.0108,0.0137,-0.0478,0.033,0.031,0.2235,0.0266,0.0594,-0.0002,0.0471,0.0361,-0.042,-0.0827,0.0102,0.0544,0.0469,0.0124,-0.04,-0.0085,-0.0483,0.0113,0.0349,-0.1012,-0.0079,-0.0049,-0.0502,0.0507,-0.0339,0.002,0.0399,-0.0572,0.0554,-0.0071,0.0075,-0.0261,-0.0774,0.0273,-0.0599,0.0045,-0.0204,-0.0625,-0.0168,-0.0653,0.023,0.007,-0.0528,-0.0353,0.0251,-0.0071,-0.0228,0.0603,0.0347,0.0028,0.1151,-0.0052,0.0549,-0.016,-0.0162,-0.0353,0.1056,0.028,0.0164,0.005,0.0405,0.0322,0.1045,-0.0349,0.032,-0.0106,-0.0002,0.0181,-0.0427,-0.0311,0.0475,-0.0077,-0.31,0.0673,0.0651,0.058,0.029,0.0154,0.0495,0.0071,0.0032,0.0202,0.0144,0.0287,0.0713,-0.0221,-0.0008,0.0313,0.0227,-0.0257,0.0543,-0.0493,-0.0045,0.0027,0.142,-0.0304,0.0116,-0.0143,-0.0201,0.0068,0.0617,-0.0331,-0.0172,0.0004,0.0759,-0.0852,0.0274,0.0797,-0.0284,0.0435,0.0004,-0.0026,0.0161,0.0029,-0.0314,-0.0457,0.0375,0.0133,-0.0303,0.0005,0.0149,-0.0012,-0.0364,-0.038,-0.0327,-0.0043,0.0501,-0.0189,-0.0179,0.0081,-0.0679,-0.0061,0.0437,-0.0737,-0.0377,-0.0241,-0.0325]}
{"key":"[Towards Socially Responsible AI: Cognitive Bias-Aware Multi-Objective Learning] Human society had a long history of suffering from cognitive biases leading to social prejudices and mass injustice. The prevalent existence of cognitive biases in large volumes of historical data can pose a threat of being manifested as unethical and seemingly inhuman predictions as outputs of AI systems trained on such data. To alleviate this problem, we propose a bias-aware multi-objective learning framework that given a set of identity attributes (e.g. gender, ethnicity etc.) and a subset of sensitive categories of the possible classes of prediction outputs, learns to reduce the frequency of predicting certain combinations of them, e.g. predicting stereotypes such as `most blacks use abusive language', or `fear is a virtue of women'. Our experiments conducted on an emotion prediction task with balanced class priors shows that a set of baseline bias-agnostic models exhibit cognitive biases with respect to gender, such as women are prone to be afraid whereas men are more prone to be angry. In contrast, our proposed bias-aware multi-objective learning methodology is shown to reduce such biases in the predictied emotions.","layer":8,"vector":[-0.0175,-0.0188,0.0259,-0.0115,0.0336,0.0254,0.0552,-0.0036,0.0411,-0.0009,0.0207,-0.0678,0.0342,0.055,0.0308,0.0487,0.0065,0.0474,-0.03,0.0342,-0.0017,-0.0198,0.0327,-0.0365,-0.0291,0.0229,-0.0494,-0.0614,-0.0705,-0.1991,0.0411,-0.0343,0.0506,-0.048,0.0412,-0.034,-0.0208,0.0767,-0.0725,0.0671,-0.0031,0.0027,0.0195,-0.0368,-0.0422,0.0009,-0.0206,0.0333,-0.0527,-0.067,0.0189,-0.0686,-0.0147,0.0414,-0.0056,0.0278,0.0505,0.0501,0.0454,0.0586,0.0346,0.0077,-0.1104,0.0874,0.0217,0.0409,-0.0591,-0.0078,0.0103,0.014,-0.0437,0.0593,0.0882,0.0249,0.008,0.0142,0.0288,-0.0285,0.0217,-0.0234,0.0254,0.011,-0.0282,0.0228,0.0012,-0.0796,0.022,-0.0303,0.0478,0.0448,-0.039,-0.039,0.0397,0.0405,-0.0304,-0.024,0.0183,0.0263,-0.0717,0.2034,-0.0282,0.0042,0.0447,-0.0374,0.0503,-0.0221,-0.0183,-0.0577,-0.0637,-0.0436,-0.0127,-0.0638,0.039,-0.0312,0.0461,0.0364,0.0505,0.0181,-0.0078,-0.0242,-0.0384,-0.0036,0.0544,-0.0371,0.0096,-0.079,0.0323,0.168,0.0035,-0.0195,0.0167,-0.0657,-0.0636,-0.0119,0.0312,0.0408,-0.0105,-0.0189,0.0172,0.0077,-0.0742,-0.0783,-0.0329,-0.0849,-0.0797,0.1201,-0.0082,0.0359,-0.0171,0.0196,-0.0343,0.0085,-0.0499,-0.0223,0.0199,0.0123,0.0221,0.0506,-0.0505,0.043,0.0061,-0.0804,-0.0606,0.0883,0.0111,-0.0548,-0.0046,0.0076,0.0214,-0.015,0.0426,-0.0126,-0.0215,0.0533,0.0678,0.0351,-0.0366,0.0333,-0.001,0.0233,0.0375,-0.0573,-0.0475,0.0613,0.0365,-0.0289,-0.0174,-0.061,0.0292,0.0168,-0.0242,0.029,-0.0186,0.0049,-0.0342,-0.0286,-0.0097,-0.0178,-0.0162,-0.0024,-0.027,0.0262,-0.064,-0.017,0.0335,0.0413,0.0181,-0.0182,0.0558,0.001,-0.0251,0.0029,0.0676,-0.0027,-0.0415,0.0164,0.0391,0.0263,0.0199,0.0305,0.0206,-0.0253,-0.0812,-0.2496,-0.0075,0.0246,-0.0134,0.0323,-0.071,0.0456,-0.0209,0.0415,0.128,0.0712,-0.0119,-0.0253,0.0217,0.0101,0.0378,-0.02,0.0118,-0.0475,0.04,-0.0213,0.0088,-0.0128,-0.0775,0.0719,-0.0252,0.2142,0.0313,0.0183,-0.0254,0.029,0.0216,-0.0082,-0.147,0.0725,-0.006,0.0319,-0.0819,-0.0529,-0.0002,-0.0149,0.0268,0.0083,-0.097,-0.034,-0.0246,0.0131,0.0259,-0.0338,0.0294,0.0275,-0.0543,0.0483,0.0286,-0.0402,-0.0119,-0.112,0.058,-0.0413,0.0242,0.0629,-0.0797,-0.0058,-0.0556,0.0403,0.0122,-0.0118,-0.0356,0.0168,-0.0367,-0.0142,0.1101,0.001,-0.0477,0.0322,-0.0258,0.0506,-0.0443,-0.0331,-0.0081,0.0443,-0.0149,0.0102,0.0393,0.0524,0.0112,0.066,0.0034,0.0759,-0.0237,0.0029,0.012,-0.0386,-0.0014,0.022,-0.0247,-0.2887,-0.0089,-0.0192,0.0431,-0.0166,-0.0156,0.0407,-0.0229,-0.0486,-0.0017,-0.023,0.0503,0.0507,0.0186,0.0145,-0.004,0.0623,-0.0372,0.0406,-0.0244,-0.0122,0.044,0.2218,-0.0119,0.0541,0.0074,-0.0448,-0.0372,-0.0118,0.0143,0.002,0.0002,0.0765,-0.0437,0.0351,0.0663,-0.0789,-0.0121,0.0263,0.012,-0.0388,0.0297,-0.018,-0.0182,0.1093,0.0213,-0.0217,-0.0543,0.0053,-0.0047,-0.0036,0.0302,-0.0339,0.0186,0.0276,0.0282,-0.061,-0.0217,-0.0385,-0.0539,0.0099,-0.0015,-0.0279,0.0148,-0.0376]}
{"key":"[Utilizing Import Vector Machines to Identify Dangerous Pro-active Traffic Conditions] Traffic accidents have been a severe issue in metropolises with the development of traffic flow. This paper explores the theory and application of a recently developed machine learning technique, namely Import Vector Machines (IVMs), in real-time crash risk analysis, which is a hot topic to reduce traffic accidents. Historical crash data and corresponding traffic data from Shanghai Urban Expressway System were employed and matched. Traffic conditions are labelled as dangerous (i.e. probably leading to a crash) and safe (i.e. a normal traffic condition) based on 5-minute measurements of average speed, volume and occupancy. The IVM algorithm is trained to build the classifier and its performance is compared to the popular and successfully applied technique of Support Vector Machines (SVMs). The main findings indicate that IVMs could successfully be employed in real-time identification of dangerous pro-active traffic conditions. Furthermore, similar to the \"support points\" of the SVM, the IVM model uses only a fraction of the training data to index kernel basis functions, typically a much smaller fraction than the SVM, and its classification rates are similar to those of SVMs. This gives the IVM a computational advantage over the SVM, especially when the size of the training data set is large.","layer":0,"vector":[-0.0643,-0.041,0.0489,-0.0093,0.0293,0.0193,0.0832,0.0507,0.0223,-0.0083,0.0157,-0.0264,-0.0023,0.0424,-0.0257,0.0079,0.02,0.0313,-0.0096,-0.0252,0.0003,-0.032,-0.0229,-0.0918,-0.0068,0.0819,-0.0033,0.0028,-0.0608,-0.1982,0.019,-0.0382,0.0915,-0.0054,-0.0378,-0.0011,-0.0656,0.0997,0.0059,0.02,0.0689,-0.0134,-0.0213,-0.0261,0.0051,-0.0614,0.0123,-0.0369,0.007,-0.0334,0.0323,-0.0453,0.0713,0.0549,-0.0055,0.0231,0.0555,-0.0013,0.0511,0.0849,0.0146,0.0044,-0.1703,0.0204,0.0356,0.0176,-0.0387,0.0103,0.0508,0.0387,-0.0654,0.0211,0.0128,0.0619,-0.0495,-0.0008,0.052,-0.0367,-0.0039,0.0358,0.0555,-0.0711,-0.0514,-0.0164,-0.0072,-0.0367,0.0038,-0.0232,0.039,-0.0234,-0.0394,-0.0245,-0.0281,0.0545,-0.0272,-0.0175,0.0516,-0.0151,-0.0148,0.1851,-0.0704,0.0659,0.0268,-0.0347,-0.0269,-0.0222,-0.0452,-0.0556,-0.0141,-0.0296,0.0171,-0.0247,0.0493,0.0004,0.0333,0.0102,0.0526,0.0208,-0.0117,0.0183,-0.0222,-0.0219,0.0529,0.0138,0.031,-0.0833,0.0335,0.1063,0.0049,0.0286,0.0017,0.0025,-0.0878,-0.012,0.0118,-0.0118,0.0155,0.0342,-0.0139,-0.0387,-0.014,-0.0901,0.0075,-0.0901,-0.031,0.0958,-0.0585,-0.0074,-0.0402,-0.0129,0.0036,0.0012,-0.0385,-0.0143,0.0058,0.025,0.081,0.0568,-0.0203,0.0031,-0.011,-0.0696,-0.0309,0.1011,-0.0053,-0.0687,-0.039,0.0157,0.0041,-0.0512,0.0215,0.0141,-0.0272,0.0332,0.0711,0.0567,-0.0388,0.0006,-0.0234,-0.0245,0.0422,-0.0738,-0.058,0.0229,0.0534,-0.0384,-0.0402,-0.0499,0.0056,0.0228,-0.0156,0.0062,-0.0254,-0.0204,-0.0582,0.0029,-0.0108,0.0138,0.0614,-0.0117,0.0307,0.0026,-0.0157,0.026,-0.0469,0.0265,-0.0182,-0.0047,0.0033,0.0392,-0.0318,0.0042,0.0535,-0.0414,-0.0437,-0.0166,0.0034,0.0717,0.0205,0.0369,0.0525,-0.0019,-0.0733,-0.2423,-0.0189,-0.0161,0.0019,0.0342,-0.0735,0.0496,-0.0203,0.0424,0.0747,0.0947,0.011,-0.0645,-0.0024,0.0281,0.0686,-0.003,0.0313,-0.0504,-0.0413,-0.053,0.0358,-0.0484,-0.0318,0.0598,-0.0117,0.1723,-0.0327,0.036,-0.0577,0.0417,-0.005,-0.0143,-0.1077,0.076,-0.0156,0.071,0.0066,-0.0596,-0.0232,-0.0183,-0.0033,-0.005,-0.051,-0.0177,-0.0365,-0.0157,0.0421,-0.0363,0.0057,0.0196,0.0344,0.0305,0.0169,0.0315,-0.042,-0.0878,0.0498,-0.0327,-0.0093,-0.0334,-0.0457,0.0169,-0.0652,0.056,0.0137,-0.0189,-0.0589,0.0121,0.01,-0.0534,0.1687,0.0339,-0.0387,0.061,0.037,-0.003,-0.0018,-0.0022,0.0206,0.0752,0.015,0.0765,0.069,-0.0038,-0.0116,0.0209,-0.0001,-0.0118,-0.0328,0.0497,-0.0031,-0.013,0.0101,0.0212,-0.0137,-0.2878,0.0109,-0.0212,0.0154,-0.0498,-0.0055,0.03,-0.0131,-0.0214,0.0104,0.0057,0.0533,0.0609,-0.0294,0.0401,0.0141,0.0443,-0.0014,0.0094,-0.0388,0.0147,0.0668,0.2654,-0.0824,0.0446,0.0388,-0.0232,-0.0175,0.0159,-0.049,0.0364,-0.0122,0.093,-0.0479,0.0494,0.0311,-0.0758,0.0671,0.0287,-0.0019,0.0145,0.0049,-0.0627,-0.0161,0.0869,0.0087,-0.0157,-0.0757,0.006,0.0253,-0.0139,-0.0402,0.0025,-0.0054,0.0238,0.0399,-0.0418,-0.0064,-0.0453,-0.0209,0.0743,-0.0898,0.0042,-0.0011,-0.017]}
{"key":"[Arrhythmia Classifier Using Convolutional Neural Network with Adaptive Loss-aware Multi-bit Networks Quantization] Cardiovascular disease (CVDs) is one of the universal deadly diseases, and the detection of it in the early stage is a challenging task to tackle. Recently, deep learning and convolutional neural networks have been employed widely for the classification of objects. Moreover, it is promising that lots of networks can be deployed on wearable devices. An increasing number of methods can be used to realize ECG signal classification for the sake of arrhythmia detection. However, the existing neural networks proposed for arrhythmia detection are not hardware-friendly enough due to a remarkable quantity of parameters resulting in memory and power consumption. In this paper, we present a 1-D adaptive loss-aware quantization, achieving a high compression rate that reduces memory consumption by 23.36 times. In order to adapt to our compression method, we need a smaller and simpler network. We propose a 17 layer end-to-end neural network classifier to classify 17 different rhythm classes trained on the MIT-BIH dataset, realizing a classification accuracy of 93.5%, which is higher than most existing methods. Due to the adaptive bitwidth method making important layers get more attention and offered a chance to prune useless parameters, the proposed quantization method avoids accuracy degradation. It even improves the accuracy rate, which is 95.84%, 2.34% higher than before. Our study achieves a 1-D convolutional neural network with high performance and low resources consumption, which is hardware-friendly and illustrates the possibility of deployment on wearable devices to realize a real-time arrhythmia diagnosis.","layer":2,"vector":[-0.045,-0.0059,0.0071,-0.0119,0.0068,0.0442,0.0117,0.0094,0.0388,-0.0153,-0.0033,-0.0609,0.0331,0.0416,0.0409,-0.0009,0.0374,0.0001,-0.0057,0.0096,0.0501,0.0103,0.0003,-0.0216,0.0118,-0.0106,0.0249,-0.0394,-0.0502,-0.203,0.0347,-0.0342,0.0541,-0.0037,-0.0079,-0.0212,-0.0183,0.0474,-0.0318,0.0347,0.0244,-0.0148,-0.0113,-0.0403,-0.0144,-0.0401,-0.015,-0.0568,0.0314,-0.0536,0.05,0.0443,0.0324,0.0269,0.0423,0.0022,0.0559,0.0927,0.0376,0.0185,0.0569,0.03,-0.1817,0.065,0.0041,0.0206,-0.0093,0.0326,0.0106,0.0236,-0.0387,-0.0179,0.0522,0.025,-0.0012,0.001,0.0156,-0.0096,0.004,0.0141,0.0027,-0.0266,-0.066,-0.014,-0.0008,-0.0721,0.0384,-0.0718,-0.0166,-0.027,-0.0646,-0.0138,-0.0487,0.0125,-0.0398,0.0036,-0.015,0.0452,-0.0896,0.2169,-0.0637,-0.0104,-0.0396,0.0065,0.0262,-0.0241,-0.0305,-0.042,-0.056,0.0129,0.0119,-0.0467,0.0443,-0.0007,0.0334,0.0236,0.0326,0.0219,0.0448,0.023,-0.0252,0.0187,0.0568,-0.0667,0.0339,-0.0642,-0.0055,0.1685,0.0233,0.032,0.0324,0.0121,-0.0622,0.0001,0.0255,0.0084,0.0275,-0.0114,0.0014,-0.0016,-0.0394,-0.0609,0.0605,-0.111,-0.0536,0.0597,-0.0338,0.0143,-0.0325,-0.0526,0.0141,-0.0005,-0.0594,-0.029,0.0589,0.0346,0.0378,0.0561,-0.044,-0.0074,-0.0187,-0.0666,-0.028,0.111,0.029,-0.061,0.0117,-0.0288,0.0297,-0.0464,0.039,0.0404,-0.0363,0.0655,0.0967,0.0224,-0.0503,-0.0352,0.0073,0.0027,0.0023,-0.0227,0.0097,0.0037,0.0598,-0.0125,0.0178,-0.0596,0.0297,0.0573,-0.0262,0.0011,-0.0248,0.0122,-0.0027,-0.0219,-0.0518,-0.0187,0.0135,-0.0109,0.0258,-0.0265,-0.0238,0.0743,0.007,0.0313,-0.0275,-0.0074,0.0097,0.0353,0.0137,-0.0039,0.0891,-0.0178,-0.0061,-0.0021,0.0013,0.066,0.0102,0.0214,0.0249,-0.0593,-0.0461,-0.2221,-0.0269,0.0269,-0.0394,0.0279,-0.082,-0.0066,0.0096,0.0389,0.0349,0.0405,0.0205,-0.0199,0.0283,-0.0273,0.0692,0.0797,0.0335,-0.0237,-0.0261,-0.0133,0.0479,-0.007,-0.0849,0.0294,0.0164,0.2348,0.0185,0.0522,-0.0154,-0.0153,0.0537,-0.0293,-0.1138,0.051,0.0262,0.0365,-0.0042,-0.0659,-0.0388,-0.0757,0.017,0.0402,-0.0791,-0.0538,-0.0202,-0.0312,0.0035,-0.088,0.0347,0.0595,-0.0747,0.0316,-0.0021,0.031,-0.0398,-0.1077,0.0143,-0.0278,-0.0264,-0.0218,-0.0438,0.0381,-0.0266,0.0594,0.0312,-0.0681,-0.0183,0.0199,-0.0643,-0.0398,0.0914,0.017,-0.0342,0.1051,-0.0101,0.0468,-0.027,-0.0053,-0.0361,0.0531,-0.0239,0.011,0.0039,-0.0038,0.0495,0.0853,0.0362,-0.0043,-0.0165,-0.0027,0.0176,-0.054,-0.0203,0.023,-0.0412,-0.2883,0.0246,0.0032,0.0226,-0.0552,0.0159,0.0154,0.0327,-0.0263,0.0138,-0.0689,0.0268,0.0817,-0.0417,0.0142,0.0144,0.0306,-0.0715,0.0801,-0.018,0.0372,0.0562,0.1778,-0.0456,0.0359,0.037,-0.0084,0.0237,0.0194,-0.0211,-0.006,0.0085,0.1176,-0.0528,0.0236,0.0516,-0.0205,0.0429,0.0031,-0.005,0.0501,0.0196,-0.0863,-0.0415,0.0931,-0.0131,-0.0803,-0.0469,-0.0155,0.0282,-0.0053,0.0259,-0.0001,0.0169,0.0172,0.0274,-0.052,-0.0913,0.0221,-0.0401,0.0673,-0.0756,-0.0631,0.0175,-0.0251]}
{"key":"[AiAds: Automated and Intelligent Advertising System for Sponsored Search] Sponsored search has more than 20 years of history, and it has been proven to be a successful business model for online advertising. Based on the pay-per-click pricing model and the keyword targeting technology, the sponsored system runs online auctions to determine the allocations and prices of search advertisements. In the traditional setting, advertisers should manually create lots of ad creatives and bid on some relevant keywords to target their audience. Due to the huge amount of search traffic and a wide variety of ad creations, the limits of manual optimizations from advertisers become the main bottleneck for improving the efficiency of this market. Moreover, as many emerging advertising forms and supplies are growing, it's crucial for sponsored search platform to pay more attention to the ROI metrics of ads for getting the marketing budgets of advertisers. In this paper, we present the AiAds system developed at Baidu, which use machine learning techniques to build an automated and intelligent advertising system. By designing and implementing the automated bidding strategy, the intelligent targeting and the intelligent creation models, the AiAds system can transform the manual optimizations into multiple automated tasks and optimize these tasks in advanced methods. AiAds is a brand-new architecture of sponsored search system which changes the bidding language and allocation mechanism, breaks the limit of keyword targeting with end-to-end ad retrieval framework and provides global optimization of ad creation. This system can increase the advertiser's campaign performance, the user experience and the revenue of the advertising platform simultaneously and significantly. We present the overall architecture and modeling techniques for each module of the system and share our lessons learned in solving several key challenges.","layer":0,"vector":[-0.0394,-0.029,-0.0012,0.0132,0.0519,0.0012,0.0394,0.0181,0.0413,-0.0181,0.0422,-0.0391,0.0302,0.0414,0.0366,-0.0136,0.0156,0.028,-0.0227,-0.0049,0.0502,-0.0038,-0.0227,-0.0713,0.034,-0.0052,0.0101,-0.0088,-0.0083,-0.1897,0.0271,-0.1001,0.1059,-0.0234,-0.0137,-0.0411,-0.0495,0.0292,-0.0182,0.0348,0.0033,0.0255,-0.0236,-0.0351,-0.0011,-0.0219,-0.0097,-0.0104,-0.0388,-0.0083,0.0326,-0.0087,0.0173,0.0215,0.0316,0.0217,0.046,0.0169,0.0097,0.0625,0.0669,0.003,-0.1747,0.1017,0.009,0.0268,-0.0164,0.0052,-0.0026,0.0879,-0.0089,-0.0111,-0.0009,0.062,-0.0254,0.0105,-0.0254,-0.0334,-0.0266,-0.0237,-0.0015,-0.0412,-0.0228,0.0178,-0.0343,-0.0441,0.0569,-0.0442,0.0567,-0.0094,0.0144,0.019,-0.0507,0.0405,-0.0706,-0.0029,-0.0118,-0.0052,-0.0737,0.1871,-0.025,0.053,0.0213,-0.088,0.0148,-0.0485,-0.0417,0.0308,-0.027,0.0068,-0.0104,-0.0481,0.0653,-0.0403,0.0156,0.0194,0.0057,0.0591,0.0369,-0.0486,-0.0334,-0.042,0.0313,0.0075,-0.0101,-0.0316,0.0366,0.1305,-0.0152,0.0045,0.0314,-0.0231,-0.0092,-0.04,0.0453,-0.0032,-0.0606,0.0169,0.0156,0.0261,-0.0483,-0.0342,0.0149,-0.0931,-0.0247,0.1251,-0.0079,0.0215,-0.0588,0.0066,0.0095,0.0275,-0.0428,-0.0046,-0.0092,0.0469,0.0662,0.0895,-0.0655,-0.0246,-0.0339,-0.0518,-0.0516,0.0981,0.0061,-0.112,-0.0542,0.002,-0.024,-0.0022,0.0357,0.0289,-0.0921,0.0192,0.0777,-0.0207,-0.0643,0.0187,-0.0262,0.0028,0.0438,-0.0361,0.0209,0.0227,0.028,-0.078,-0.0223,-0.0432,0.0179,0.0225,-0.008,0.0577,-0.0326,0.0154,-0.0252,-0.0354,-0.0027,-0.0246,0.0899,-0.0397,0.0464,-0.0078,-0.0617,-0.0394,0.0106,0.0202,-0.0169,-0.0084,0.0476,-0.0015,-0.0286,0.0378,0.0261,-0.0308,-0.0434,-0.033,-0.0039,0.0244,0.0297,0.1061,0.0084,-0.0048,-0.0241,-0.1963,-0.0269,0.0012,0.0017,0.0267,-0.0799,0.027,-0.0158,0.0763,0.1108,0.0664,-0.0556,-0.0132,0.0603,-0.0332,0.0243,0.0099,0.0475,-0.0025,-0.0316,0.0292,0.0004,0.0466,-0.0557,-0.0033,0.0146,0.1868,0.0691,-0.0006,-0.0371,0.0449,0.0261,-0.0362,-0.1258,0.0199,0.0474,0.0747,0.0049,-0.0549,0.0048,-0.0543,0.0488,-0.0096,-0.0753,0.0047,-0.0135,-0.025,0.0051,-0.0437,0.064,0.0056,0.0064,0.0582,0.0129,-0.0151,-0.0439,-0.0612,0.0232,-0.0373,0.037,-0.0035,-0.0752,-0.0074,-0.0155,0.0369,-0.0135,-0.0251,-0.0489,0.0229,-0.0322,-0.0366,0.0971,0.0073,-0.0005,0.0658,0.0191,0.0348,-0.0618,0.0042,0.0193,0.0578,-0.0355,0.0697,-0.0012,-0.0077,0.0045,0.0499,-0.0197,0.0366,-0.0086,-0.0033,0.027,-0.0624,-0.0082,0.0293,-0.02,-0.346,0.0511,0.0085,0.0259,-0.0377,0.0372,0.0088,0.0068,-0.0547,0.034,-0.0343,0.0387,0.0002,-0.0356,0.0131,0.0192,0.0196,-0.024,0.0702,-0.0423,-0.0024,0.0526,0.2527,-0.028,0.0253,-0.0146,-0.0094,0.0169,0.0222,-0.0269,0.0368,-0.0046,0.0854,-0.0443,0.0341,0.1052,-0.0532,0.0321,0.0081,0.011,-0.0227,0.0313,-0.0528,-0.0074,0.0734,-0.0091,-0.0167,-0.0713,-0.0278,0.0719,-0.016,-0.0392,-0.0165,0.0013,0.0676,-0.0005,-0.0649,-0.0447,-0.0207,-0.0559,0.0349,-0.0338,-0.0537,0.0197,0.0211]}
{"key":"[Checking Tests for Read-Once Functions over Arbitrary Bases] A Boolean function is called read-once over a basis B if it can be expressed by a formula over B where no variable appears more than once. A checking test for a read-once function f over B depending on all its variables is a set of input vectors distinguishing f from all other read-once functions of the same variables. We show that every read-once function f over B has a checking test containing O(n^l) vectors, where n is the number of relevant variables of f and l is the largest arity of functions in B. For some functions, this bound cannot be improved by more than a constant factor. The employed technique involves reconstructing f from its l-variable projections and provides a stronger form of Kuznetsov's classic theorem on read-once representations.","layer":11,"vector":[-0.0802,-0.0191,0.0319,-0.0233,0.004,0.0104,0.0652,0.0636,0.0217,-0.0304,0.0212,-0.0607,0.0652,0.0461,-0.0022,-0.021,-0.0025,0.0604,-0.058,-0.0177,0.0859,-0.0389,-0.0611,-0.0027,0.0237,0.0209,-0.0265,-0.0134,0.0259,-0.2508,-0.0314,-0.0331,0.0253,-0.046,0.0222,-0.0281,-0.0397,0.031,-0.0381,0.0188,0.0317,0.031,-0.0011,-0.0505,-0.0366,-0.0462,-0.0439,0.0098,-0.0321,-0.0428,0.0155,0.0184,0.0514,0.014,0.0608,0.0117,0.0141,0.0513,0.042,0.0303,0.0232,0.0387,-0.1582,0.0496,0.047,0.0365,-0.0096,-0.0441,0.0345,0.1053,0.0107,0.0422,-0.0186,0.0332,0.0513,-0.0111,-0.021,-0.0113,-0.0302,0.0437,-0.021,-0.0234,-0.0214,0.0277,-0.0394,-0.0413,0.0059,0.008,0.0644,0.0148,-0.0017,0.024,0.0217,0.0376,-0.03,-0.015,0.0644,0.0228,-0.0141,0.2064,-0.0749,0.007,0.0193,-0.0421,-0.0124,-0.0389,-0.0137,-0.0338,-0.0295,-0.0202,-0.0449,-0.0911,0.0745,-0.0674,0.0166,0.032,0.084,-0.0019,-0.0282,-0.0141,0.0018,0.0464,0.024,-0.0331,0.0305,-0.0631,0.0191,0.1433,0.0114,0.0711,0.0304,-0.0502,-0.042,-0.0387,0.0045,0.0255,-0.0165,0.0295,0.0054,0.0033,-0.027,-0.0526,0.064,-0.0572,-0.0358,0.0932,-0.043,0.0411,-0.0265,-0.0152,-0.0001,0.0395,0.0134,-0.0611,0.0274,0.0089,0.0058,0.0265,-0.0376,0.0171,-0.0607,-0.0133,-0.0326,0.0715,0.0031,-0.0242,-0.0183,0.0039,0.0107,-0.0067,0.0404,0.0047,-0.076,0.0041,0.0374,-0.0184,-0.0444,0.0116,-0.0222,0.0329,0.0145,-0.085,-0.0697,0.058,0.0086,-0.012,-0.0005,-0.0418,0.048,0.0176,-0.0114,0.0179,-0.0542,0.0038,-0.081,-0.0541,-0.0152,0.0172,0.0093,-0.0502,0.0094,0.0093,-0.0274,0.0342,-0.0329,-0.0018,-0.0224,-0.0425,0.038,0.0495,-0.0287,-0.0001,0.0034,-0.0252,-0.0156,0.0063,0.0115,0.0107,0.0406,0.0101,0.0419,-0.0926,-0.0354,-0.2434,-0.0491,-0.0094,-0.016,0.0369,-0.0655,0.0447,0.0114,0.0277,0.0441,0.0014,0.0,-0.074,0.0363,-0.0187,0.0447,0.0066,0.0151,-0.046,0.0168,-0.0146,0.0046,-0.0255,-0.076,0.0662,-0.02,0.227,-0.0069,0.0351,-0.0098,0.0677,-0.0168,0.0157,-0.0369,0.07,0.0539,0.0012,0.032,0.006,-0.0435,0.0318,-0.0141,-0.046,-0.0614,-0.0179,-0.035,-0.0378,-0.0225,-0.0572,0.0346,0.0478,-0.0679,0.0134,-0.0049,0.057,-0.0468,-0.0672,-0.0287,-0.0221,0.0792,0.0114,-0.0292,0.014,-0.0526,0.1157,0.0164,-0.0204,-0.0263,0.0803,0.0103,-0.0168,0.0316,-0.0439,-0.0133,0.0703,0.0293,0.073,-0.0203,-0.0397,-0.0158,0.039,-0.02,0.0124,-0.0161,0.0173,-0.0296,0.0424,0.0307,0.0152,-0.0124,-0.0375,0.0062,-0.0481,-0.0085,0.0646,-0.02,-0.2885,-0.0004,0.0016,0.0058,-0.0589,0.024,0.0535,0.0118,-0.0854,-0.0101,0.0294,0.0597,0.0583,-0.0206,0.0318,0.0628,0.0572,-0.0372,0.0592,-0.0785,0.0544,0.034,0.2196,-0.0633,0.0221,0.0268,-0.0131,0.0026,0.0295,0.004,0.0628,0.0169,0.0696,-0.0485,-0.0279,0.0852,-0.0392,0.0421,0.0656,-0.036,0.0323,-0.0217,-0.0828,-0.0046,0.1235,0.0027,-0.0489,-0.0628,-0.0014,0.0002,-0.0195,-0.0133,0.0448,-0.0102,0.0315,0.0209,-0.0334,-0.0381,-0.046,-0.0552,0.0107,-0.0639,0.0469,0.0639,0.001]}
{"key":"[What if Neural Networks had SVDs?] Various Neural Networks employ time-consuming matrix operations like matrix inversion. Many such matrix operations are faster to compute given the Singular Value Decomposition (SVD). Previous work allows using the SVD in Neural Networks without computing it. In theory, the techniques can speed up matrix operations, however, in practice, they are not fast enough. We present an algorithm that is fast enough to speed up several matrix operations. The algorithm increases the degree of parallelism of an underlying matrix multiplication $H\\cdot X$ where $H$ is an orthogonal matrix represented by a product of Householder matrices. Code is available at www.github.com/AlexanderMath/fasth .","layer":0,"vector":[-0.0917,-0.0031,0.0308,-0.0172,0.0003,0.0103,0.0325,0.0387,0.0572,0.0143,0.0157,-0.042,0.0511,0.0245,0.0506,-0.0041,0.0076,0.0643,-0.0005,-0.0175,-0.0056,-0.0054,-0.0625,-0.0478,0.0482,0.0175,-0.0111,-0.027,-0.0443,-0.2144,0.0297,-0.0311,0.107,-0.0057,-0.0006,-0.0494,-0.0378,0.0429,-0.0665,0.0063,0.0233,0.0095,-0.0316,-0.0252,-0.0406,-0.0297,-0.0514,-0.0036,0.0176,-0.0362,0.0224,0.0029,0.0119,-0.0063,0.0618,0.0082,0.0111,-0.0002,-0.0011,0.0433,0.0178,0.0393,-0.1711,0.0531,0.071,-0.0047,-0.0035,-0.0552,0.0099,0.0651,-0.038,0.0254,0.0805,-0.0072,0.0491,0.0187,-0.0103,-0.0568,-0.0048,0.0109,0.019,-0.0143,-0.0315,-0.0258,-0.0032,-0.0026,0.0131,-0.0432,0.0114,-0.0017,-0.034,-0.0153,-0.0163,0.0328,-0.057,-0.0155,0.0269,0.0402,-0.0261,0.2062,-0.0465,0.0351,0.0647,-0.0686,0.0132,-0.0271,-0.0408,-0.0217,-0.0465,-0.0354,0.0001,-0.0519,0.0067,-0.0404,0.0274,0.0015,0.036,0.0597,-0.0484,0.0181,-0.055,0.0234,0.0644,0.0241,0.014,-0.0649,0.012,0.1439,0.0411,0.1057,0.0599,-0.0097,-0.0136,-0.0491,0.0296,0.0213,0.0146,-0.0103,-0.0317,-0.0255,-0.0716,-0.0518,0.0142,-0.0538,-0.0061,0.1328,0.0049,0.0148,0.0011,-0.0184,-0.0348,0.0468,-0.0555,-0.0575,-0.0199,0.0137,0.0308,0.0098,-0.0578,0.0171,-0.0273,-0.0369,-0.0597,0.1272,0.0484,-0.06,-0.0241,-0.016,0.0564,-0.017,0.0513,0.0411,-0.05,0.0115,0.0719,-0.0043,-0.1012,0.0098,-0.0014,0.0093,0.0362,-0.0396,-0.0576,0.0295,0.0623,-0.0495,-0.0011,-0.0455,-0.0309,0.0158,-0.0623,0.0009,-0.0722,-0.0196,-0.0334,-0.0206,0.0148,0.0031,0.0353,-0.0337,0.0208,-0.0068,-0.0158,0.0405,-0.0084,0.039,-0.004,0.0202,-0.0159,0.0659,-0.0324,-0.0023,0.0884,-0.036,-0.0232,0.0006,-0.0355,0.0356,-0.024,0.0765,0.0315,-0.098,-0.0658,-0.1927,-0.0442,-0.0075,-0.0284,0.0672,-0.0835,0.0254,-0.045,0.0195,0.0598,0.0406,0.0289,-0.0382,-0.0009,0.0232,0.0761,0.0505,0.0494,-0.027,-0.039,-0.0147,0.0407,0.008,-0.0451,0.0653,-0.0145,0.2163,0.0077,0.0609,-0.0112,0.0293,0.0143,0.003,-0.0589,0.0614,0.0082,0.0772,0.0228,-0.0199,-0.0396,-0.0272,0.0256,0.0257,-0.0791,-0.01,0.0133,-0.0109,0.0126,-0.0188,0.0014,0.0385,-0.0281,0.0333,0.0262,0.0408,-0.0374,-0.0844,0.0096,-0.067,0.0371,-0.0132,-0.1039,-0.021,-0.0465,0.062,0.0093,0.0017,-0.0405,0.0276,-0.032,-0.0279,0.0811,0.0047,-0.0034,0.0742,-0.0322,-0.0229,0.0018,-0.0131,0.0273,0.0583,-0.0223,0.0294,0.0056,0.0472,-0.0284,0.0509,0.005,-0.0266,-0.0051,-0.0022,0.0275,-0.0424,0.0474,0.0201,-0.0161,-0.2891,0.0527,0.0214,-0.013,-0.0406,0.022,0.0247,-0.0231,-0.0495,-0.0103,-0.041,0.0676,0.0709,-0.0077,0.0476,0.0339,0.0719,-0.0416,0.0598,-0.0635,0.0092,0.0359,0.2629,-0.0511,0.0076,0.016,0.0109,-0.0369,0.0257,-0.0324,0.0511,-0.0211,0.0707,-0.0763,0.0451,0.0732,-0.0243,0.0384,0.0203,0.0091,0.0106,-0.0321,-0.0371,-0.0407,0.1085,-0.005,-0.0193,-0.0728,-0.0143,0.0065,-0.0043,0.0045,0.0118,-0.0207,0.0198,-0.001,-0.0516,-0.0311,-0.0359,-0.0448,0.0405,-0.0966,-0.0143,-0.0012,-0.01]}
{"key":"[Sexism Identification in Tweets and Gabs using Deep Neural Networks] Through anonymisation and accessibility, social media platforms have facilitated the proliferation of hate speech, prompting increased research in developing automatic methods to identify these texts. This paper explores the classification of sexism in text using a variety of deep neural network model architectures such as Long-Short-Term Memory (LSTMs) and Convolutional Neural Networks (CNNs). These networks are used in conjunction with transfer learning in the form of Bidirectional Encoder Representations from Transformers (BERT) and DistilBERT models, along with data augmentation, to perform binary and multiclass sexism classification on the dataset of tweets and gabs from the sEXism Identification in Social neTworks (EXIST) task in IberLEF 2021. The models are seen to perform comparatively to those from the competition, with the best performances seen using BERT and a multi-filter CNN model. Data augmentation further improves these results for the multi-class classification task. This paper also explores the errors made by the models and discusses the difficulty in automatically classifying sexism due to the subjectivity of the labels and the complexity of natural language used in social media.","layer":4,"vector":[-0.0073,0.0002,-0.034,-0.007,0.0554,0.0437,0.0468,0.0385,0.0357,-0.0262,0.0056,-0.0488,0.0358,0.0183,0.0446,0.0536,0.0216,0.0143,-0.0317,-0.0244,0.0463,-0.0191,0.0139,-0.0474,0.034,-0.0232,-0.0152,-0.0527,-0.0664,-0.2034,0.0203,-0.042,0.0529,-0.048,0.0266,-0.0256,-0.0313,0.0211,-0.0494,0.0361,-0.0005,-0.0232,-0.023,-0.0762,-0.0383,-0.028,-0.0527,-0.0276,-0.0541,-0.0779,0.0219,-0.0558,0.017,0.0425,-0.0067,0.0395,0.0954,0.0281,0.039,0.0267,0.0585,0.022,-0.1642,0.0789,-0.0007,0.04,-0.0468,0.0149,0.0282,0.0221,-0.0328,0.0243,0.0513,0.0438,0.0215,0.0578,0.0129,-0.0076,0.0088,-0.0073,0.0015,0.0052,-0.0044,-0.0058,0.0019,-0.0857,-0.0212,-0.0139,0.0088,0.0255,-0.0504,-0.0081,0.051,0.0879,-0.0302,-0.0202,-0.0048,0.0275,-0.0711,0.2145,-0.0332,0.0002,0.0212,-0.0372,0.0489,0.0064,-0.0215,-0.0451,-0.0798,0.0086,-0.0389,-0.048,0.0176,-0.0164,0.0373,0.0068,0.0677,0.0185,-0.0301,-0.0201,-0.0261,0.0107,0.0461,-0.0331,0.0471,-0.047,0.0338,0.1433,0.0563,0.0312,0.0322,-0.0085,-0.0459,0.0321,-0.0013,0.0194,-0.0029,-0.0118,-0.0183,-0.0597,-0.0351,-0.0813,0.0184,-0.0669,-0.0558,0.1211,-0.0488,-0.0317,-0.0414,0.0334,0.0243,0.0339,-0.0445,-0.0468,0.036,0.0482,0.0311,0.0748,-0.0583,0.005,0.0405,-0.0473,-0.0676,0.0957,0.0423,-0.0939,0.0012,0.0049,-0.0065,-0.0208,0.0718,0.0208,-0.0467,0.0303,0.0324,0.0289,-0.0183,-0.008,0.0073,0.0172,-0.0002,-0.0464,-0.0429,0.0216,0.0318,-0.0389,-0.0031,-0.0929,0.0353,0.0733,-0.0111,0.0688,-0.0525,-0.0152,0.0124,-0.0277,-0.016,-0.0248,0.0322,-0.0217,0.0025,0.027,-0.0394,0.0307,-0.0093,0.0186,-0.0254,0.0083,0.0607,0.0255,-0.0425,-0.0311,0.0461,-0.0366,-0.0464,-0.021,0.0407,-0.0021,0.0322,0.0351,0.0311,-0.0238,-0.0551,-0.2408,-0.0246,0.0217,-0.02,0.1089,-0.1156,0.0424,-0.0218,0.0725,0.1011,0.0606,-0.0189,-0.0658,0.0107,-0.0119,0.0257,0.0236,0.0451,0.014,0.0327,-0.0661,-0.007,0.0009,-0.0977,0.042,-0.0027,0.1845,0.0358,0.0229,-0.0295,0.009,0.0417,-0.0102,-0.1282,0.1014,-0.0381,0.066,-0.0383,-0.0219,-0.0064,-0.0413,0.0225,0.0398,-0.0728,0.001,-0.0105,-0.0116,-0.0279,-0.0651,0.0356,0.0393,-0.042,0.0639,0.0467,0.0202,-0.032,-0.1143,0.0332,-0.0216,-0.0153,0.0242,-0.0578,-0.0106,-0.1011,0.0358,0.0277,-0.069,-0.0393,0.0335,-0.0168,0.0033,0.1057,0.0404,0.0024,0.0742,-0.0012,0.0237,-0.0141,-0.0558,-0.0127,0.0333,0.0163,0.0524,0.0147,0.0166,-0.0019,0.0515,0.0012,0.0505,0.0146,0.0013,0.0223,-0.0642,0.0012,0.0212,-0.0448,-0.2858,0.0194,-0.0218,0.0573,-0.0148,0.0075,0.0055,-0.0067,-0.0325,-0.0047,-0.0235,0.0541,0.0374,-0.0641,-0.0162,0.0289,0.0561,-0.0334,0.0165,-0.0289,0.0402,0.0116,0.1929,-0.0411,0.0165,-0.0053,-0.0124,-0.0091,0.0126,0.0074,0.0272,0.0283,0.1016,-0.0491,0.0101,0.0733,-0.026,0.0299,0.0705,0.0288,-0.0178,0.0026,-0.0462,0.0059,0.0682,-0.0078,-0.0288,-0.0745,-0.0076,0.0291,-0.0354,0.0439,0.0045,0.0306,0.035,0.0048,-0.0546,-0.0457,-0.031,-0.0498,-0.0058,-0.0502,-0.0484,-0.0066,0.0045]}
{"key":"[Bayesian Optimization on Large Graphs via a Graph Convolutional Generative Model: Application in Cardiac Model Personalization] Personalization of cardiac models involves the optimization of organ tissue properties that vary spatially over the non-Euclidean geometry model of the heart. To represent the high-dimensional (HD) unknown of tissue properties, most existing works rely on a low-dimensional (LD) partitioning of the geometrical model. While this exploits the geometry of the heart, it is of limited expressiveness to allow partitioning that is small enough for effective optimization. Recently, a variational auto-encoder (VAE) was utilized as a more expressive generative model to embed the HD optimization into the LD latent space. Its Euclidean nature, however, neglects the rich geometrical information in the heart. In this paper, we present a novel graph convolutional VAE to allow generative modeling of non-Euclidean data, and utilize it to embed Bayesian optimization of large graphs into a small latent space. This approach bridges the gap of previous works by introducing an expressive generative model that is able to incorporate the knowledge of spatial proximity and hierarchical compositionality of the underlying geometry. It further allows transferring of the learned features across different geometries, which was not possible with a regular VAE. We demonstrate these benefits of the presented method in synthetic and real data experiments of estimating tissue excitability in a cardiac electrophysiological model.","layer":12,"vector":[-0.0245,-0.0063,0.035,-0.0208,-0.0082,0.0247,0.0011,0.0199,0.0154,0.0059,0.0096,-0.0904,0.0459,0.0483,0.0165,0.0338,0.0191,0.0594,-0.0506,0.0093,0.0244,-0.0088,0.0074,-0.0876,0.03,-0.0248,0.0001,-0.0218,-0.0601,-0.2368,0.0255,-0.0197,0.0599,0.0173,-0.0351,-0.0561,-0.0156,0.0293,-0.0562,0.0749,0.0054,-0.0188,-0.0139,-0.0039,-0.0029,-0.0411,-0.0202,-0.014,0.0017,-0.0356,0.0369,-0.0358,0.02,0.0485,0.0695,0.0161,0.0951,0.0442,0.0457,0.0454,0.0347,0.0714,-0.1239,0.0612,0.028,0.0333,-0.0584,0.0144,0.0124,0.0418,-0.0106,0.0372,0.0218,0.0131,0.0152,-0.0153,0.0001,0.0125,-0.0419,0.0205,-0.0277,-0.0356,-0.0495,0.024,-0.0166,-0.017,0.0582,-0.061,-0.0179,0.0213,-0.063,-0.0509,-0.052,0.0005,-0.0605,-0.0198,-0.0026,0.0103,-0.073,0.199,-0.0217,0.0473,0.0337,0.003,0.0062,-0.0432,-0.0518,-0.0019,-0.0142,0.0145,0.0091,-0.0039,-0.0203,-0.0316,0.0261,-0.0043,0.0449,0.0239,0.0103,-0.0243,-0.0585,0.0506,0.0169,-0.0576,0.0406,-0.0697,-0.0001,0.1463,0.0688,0.0411,0.0691,0.0862,-0.0346,-0.0131,0.0108,-0.0383,0.0491,-0.0245,-0.0301,0.0069,-0.0362,-0.0072,-0.0015,-0.086,-0.0407,0.1184,-0.0488,0.0205,-0.0832,-0.0506,-0.0167,0.008,-0.0302,-0.0052,0.0238,0.024,-0.0175,0.0383,-0.0594,-0.0097,-0.0273,-0.0414,-0.0429,0.1253,0.0084,-0.0713,-0.0276,0.0074,0.0526,0.0229,0.0237,0.0339,-0.0188,0.0863,0.0887,0.0294,-0.0456,-0.0046,0.0437,0.0134,0.0235,-0.0082,-0.0237,-0.0132,0.0144,-0.038,0.023,-0.0225,0.0288,0.0654,0.046,0.0171,-0.0279,0.0008,-0.066,-0.0271,-0.0423,-0.0385,0.0091,0.0166,0.0192,-0.0211,-0.0463,0.0613,-0.0074,0.0067,0.004,-0.0204,0.0309,0.0299,-0.0555,-0.0225,0.0675,0.0098,-0.0402,0.0078,-0.0116,0.0218,0.0281,0.0292,-0.0003,-0.0557,-0.0647,-0.2041,0.0066,-0.0057,-0.016,0.0435,-0.0637,-0.0202,-0.0066,0.0629,0.0521,0.0098,-0.0027,-0.0485,0.0389,0.0098,0.0286,0.0101,0.0426,-0.015,-0.0042,0.0048,-0.0025,-0.0115,-0.0788,0.0301,0.0168,0.2701,0.0396,0.0217,-0.0046,-0.0103,0.0261,-0.032,-0.0999,0.0563,0.0378,0.058,0.0168,-0.0708,-0.0073,-0.0742,0.0012,0.0038,-0.0894,-0.0624,-0.0542,-0.0413,0.0275,-0.0903,0.0342,0.0938,-0.058,0.0541,-0.0192,0.0318,-0.0604,-0.0953,0.0366,-0.0424,0.0052,-0.0139,-0.0376,-0.015,-0.0708,0.0379,0.0188,-0.0168,-0.0438,0.0323,-0.063,-0.0038,0.0664,0.0523,0.015,0.112,0.0423,0.0324,-0.0132,-0.0234,-0.0493,0.0202,-0.0617,0.0248,0.0351,0.036,-0.0048,0.0541,0.0085,0.0221,-0.0613,-0.0301,0.0229,-0.0549,-0.0177,-0.0015,-0.0402,-0.3057,0.0182,0.0411,0.0292,-0.0424,0.0043,0.025,0.0393,-0.08,-0.0209,-0.0076,0.0301,0.0556,0.0085,-0.0134,0.0311,0.0723,-0.0615,0.0174,-0.0695,0.0184,0.0051,0.2247,-0.0345,0.0155,0.0458,-0.0165,0.0221,0.0274,-0.0427,0.0091,0.0061,0.0768,-0.0477,0.0612,0.0579,-0.0332,0.0623,-0.0169,0.0114,0.0047,0.0047,-0.0393,-0.0314,0.0527,0.0081,-0.0153,0.0185,0.0063,0.0224,0.0261,0.0486,0.0047,0.0197,0.0379,0.0242,-0.0284,-0.0664,0.0132,-0.0447,0.0016,-0.0328,-0.0213,-0.009,-0.0226]}
{"key":"[Sample Complexity of Offline Reinforcement Learning with Deep ReLU Networks] We study the statistical theory of offline reinforcement learning (RL) with deep ReLU network function approximation. We analyze a variant of fitted-Q iteration (FQI) algorithm under a new dynamic condition that we call Besov dynamic closure, which encompasses the conditions from prior analyses for deep neural network function approximation. Under Besov dynamic closure, we prove that the FQI-type algorithm enjoys the sample complexity of $\\tilde{\\mathcal{O}}\\left( \\kappa^{1 + d/\\alpha} \\cdot \\epsilon^{-2 - 2d/\\alpha} \\right)$ where $\\kappa$ is a distribution shift measure, $d$ is the dimensionality of the state-action space, $\\alpha$ is the (possibly fractional) smoothness parameter of the underlying MDP, and $\\epsilon$ is a user-specified precision. This is an improvement over the sample complexity of $\\tilde{\\mathcal{O}}\\left( K \\cdot \\kappa^{2 + d/\\alpha} \\cdot \\epsilon^{-2 - d/\\alpha} \\right)$ in the prior result [Yang et al., 2019] where $K$ is an algorithmic iteration number which is arbitrarily large in practice. Importantly, our sample complexity is obtained under the new general dynamic condition and a data-dependent structure where the latter is either ignored in prior algorithms or improperly handled by prior analyses. This is the first comprehensive analysis for offline RL with deep ReLU network function approximation under a general setting.","layer":0,"vector":[-0.1038,-0.0102,0.0414,-0.0216,-0.0303,0.0552,0.0519,0.033,0.0674,-0.043,0.0098,-0.0187,0.0243,0.0923,0.0108,0.0014,0.0309,0.0249,-0.0466,-0.008,0.0433,-0.0536,0.0027,-0.0368,0.0175,-0.02,-0.0215,-0.0864,-0.0169,-0.2613,0.0137,-0.0535,0.0005,-0.0475,0.0337,-0.0106,-0.064,0.0006,-0.0228,0.0406,0.0439,0.0568,-0.008,-0.07,-0.0285,-0.0234,-0.01,-0.0297,0.008,-0.0306,0.0055,-0.0349,0.022,0.0622,0.0347,0.0337,0.0873,0.0474,0.0514,0.0674,0.0179,0.0262,-0.1598,0.0279,0.0082,-0.001,-0.0318,-0.022,0.0416,0.0578,-0.0355,0.0576,-0.0163,0.0211,0.0084,0.0062,-0.0143,-0.0396,0.0082,0.0044,0.0293,-0.0237,-0.0215,-0.0086,-0.024,-0.0665,-0.0024,-0.0331,0.0316,0.0157,-0.0242,-0.0035,-0.0186,-0.0131,-0.058,-0.0145,0.0461,0.0068,-0.0581,0.1873,-0.0146,0.0644,0.0182,0.0148,0.0122,-0.0249,-0.0454,-0.0227,-0.0303,-0.0269,-0.0547,-0.0092,0.0791,-0.0443,-0.0011,0.0434,0.041,-0.0054,-0.0143,0.0063,0.0046,0.0163,0.0228,-0.0115,0.0273,-0.0354,0.0014,0.1159,0.017,0.0563,0.0181,-0.0163,-0.0412,-0.0508,0.029,0.0131,0.0075,0.0016,0.0234,-0.0203,-0.0302,0.0071,0.0003,-0.1172,-0.0018,0.1025,-0.0208,0.0397,-0.0373,-0.003,-0.0146,0.0212,-0.0088,-0.0502,0.0507,0.0425,0.0158,0.048,-0.0767,0.015,-0.0389,-0.0982,-0.0021,0.1038,-0.0128,-0.0641,-0.0246,-0.0203,0.0072,0.0142,0.0332,0.0124,-0.0289,0.024,0.0784,-0.0262,-0.0783,-0.0249,0.0227,0.0162,0.0018,-0.0245,-0.0313,0.0421,0.0112,-0.0355,0.0059,-0.0417,0.0434,0.014,-0.0125,-0.0143,-0.0287,0.0159,-0.0545,-0.0497,-0.0008,-0.0149,0.0217,0.0018,-0.0319,-0.0176,-0.0551,0.0029,-0.0262,0.0416,-0.0222,0.0089,0.0486,0.0383,-0.0271,0.0349,0.0276,-0.032,0.0287,0.0035,0.0362,-0.0223,-0.0339,0.0612,0.0068,-0.0639,-0.0276,-0.2333,-0.0296,-0.0185,-0.0326,0.0602,-0.0816,0.0594,-0.0178,0.0573,0.0726,0.0285,0.0054,-0.0197,0.0255,-0.0151,0.0596,0.0753,0.0283,-0.0285,-0.0227,0.0028,0.0152,-0.0399,-0.08,0.0563,0.0121,0.2417,-0.0377,0.0726,-0.0149,0.0289,0.068,-0.0014,-0.051,0.0697,0.0173,0.1049,-0.029,0.0123,-0.0511,0.0204,-0.004,0.0058,-0.1089,-0.0082,-0.0127,-0.0342,0.0025,-0.0801,-0.0181,0.0486,-0.0513,0.0441,-0.0281,0.0103,-0.0086,-0.091,0.0334,-0.0181,0.0141,-0.0138,-0.0456,-0.0028,-0.0333,0.0486,0.0098,0.0402,-0.01,0.0632,-0.0281,-0.0283,0.0528,0.0097,0.0239,0.0738,-0.0305,0.0367,-0.0277,-0.0512,-0.0178,0.107,-0.035,0.0024,0.0208,0.0204,-0.0281,0.0698,-0.0281,0.0197,0.0053,0.0148,-0.0338,-0.0322,-0.0119,0.0472,0.0179,-0.304,0.0552,0.0062,0.0503,-0.0195,0.0237,0.0477,0.0261,-0.0441,-0.0091,0.0272,0.1001,0.0424,-0.0279,-0.0014,0.0124,0.0785,-0.027,0.0555,-0.0923,0.0297,0.0539,0.1986,-0.0819,0.0379,-0.0073,-0.049,-0.0155,0.0215,-0.0307,0.0185,0.0052,0.0626,-0.0815,0.031,0.0531,-0.0113,0.0561,0.0086,-0.027,0.0024,0.0066,-0.0205,-0.0045,0.1011,-0.0356,-0.0283,-0.0746,-0.0159,0.0906,-0.0118,-0.0408,0.0066,-0.0065,0.0326,0.0458,-0.035,-0.0614,-0.0351,-0.0452,0.0221,-0.058,0.0186,0.0105,-0.0112]}
{"key":"[ALLNet: A Hybrid Convolutional Neural Network to Improve Diagnosis of Acute Lymphocytic Leukemia (ALL) in White Blood Cells] Due to morphological similarity at the microscopic level, making an accurate and time-sensitive distinction between blood cells affected by Acute Lymphocytic Leukemia (ALL) and their healthy counterparts calls for the usage of machine learning architectures. However, three of the most common models, VGG, ResNet, and Inception, each come with their own set of flaws with room for improvement which demands the need for a superior model. ALLNet, the proposed hybrid convolutional neural network architecture, consists of a combination of the VGG, ResNet, and Inception models. The ALL Challenge dataset of ISBI 2019 (available here) contains 10,691 images of white blood cells which were used to train and test the models. 7,272 of the images in the dataset are of cells with ALL and 3,419 of them are of healthy cells. Of the images, 60% were used to train the model, 20% were used for the cross-validation set, and 20% were used for the test set. ALLNet outperformed the VGG, ResNet, and the Inception models across the board, achieving an accuracy of 92.6567%, a sensitivity of 95.5304%, a specificity of 85.9155%, an AUC score of 0.966347, and an F1 score of 0.94803 in the cross-validation set. In the test set, ALLNet achieved an accuracy of 92.0991%, a sensitivity of 96.5446%, a specificity of 82.8035%, an AUC score of 0.959972, and an F1 score of 0.942963. The utilization of ALLNet in the clinical workspace can better treat the thousands of people suffering from ALL across the world, many of whom are children.","layer":2,"vector":[-0.0633,-0.0266,-0.0179,0.0063,0.0117,0.0311,0.0464,0.0263,0.0203,-0.0201,0.0421,-0.0634,0.0287,0.0612,-0.0053,0.0261,0.0554,0.034,-0.0784,0.0046,-0.0318,-0.0498,0.0042,-0.0589,0.0587,-0.0301,-0.0189,-0.0148,-0.0709,-0.2535,0.0269,-0.0362,0.0312,-0.0287,0.0361,0.0006,-0.0121,0.0313,-0.039,-0.0121,-0.0046,0.0272,-0.0013,-0.0205,-0.0261,-0.0665,-0.0612,-0.0059,-0.039,-0.0264,0.0131,-0.0803,0.0342,0.0334,-0.0154,0.0134,0.0158,0.0422,0.0668,0.0532,0.017,0.0295,-0.2034,0.0691,0.0554,-0.0076,-0.0376,0.007,0.0301,0.0443,-0.01,0.02,0.0541,0.0661,-0.0185,-0.0094,0.011,-0.0518,0.0054,0.0098,0.0091,-0.0109,-0.011,-0.0363,0.0115,-0.0205,-0.0482,-0.0448,0.0341,0.0048,-0.0664,-0.0316,-0.0186,0.0358,-0.0348,-0.0158,0.0205,-0.0234,-0.042,0.2277,-0.0525,-0.003,0.0371,-0.0419,0.0482,-0.059,-0.0045,-0.0355,-0.0403,-0.0094,-0.0298,-0.0522,0.0374,-0.0388,0.0207,0.0261,0.0677,0.0168,-0.0227,0.0227,-0.0054,-0.022,0.0246,-0.0141,0.0669,-0.0175,0.0658,0.1224,0.0449,0.0174,0.0772,0.0113,-0.0585,-0.0117,-0.0081,0.0457,-0.0027,0.0033,-0.013,0.0033,-0.0114,-0.0671,0.0065,-0.1117,-0.0457,0.0708,-0.0496,0.0206,-0.0517,-0.0416,-0.005,0.013,-0.0295,-0.0139,0.0245,0.0399,0.0005,0.0463,-0.0448,-0.0288,-0.001,-0.033,-0.0637,0.1183,-0.0235,-0.0751,-0.0192,-0.0631,-0.0231,0.0229,0.0118,0.0315,-0.0239,0.0167,0.0644,0.0261,-0.0782,-0.0168,0.0031,0.0534,0.0213,-0.0767,0.0294,0.0515,0.0446,-0.0391,0.0149,-0.0535,0.0265,0.0712,-0.0579,0.068,-0.036,0.0062,-0.0125,-0.0303,-0.0023,-0.0572,0.0027,-0.025,0.0392,0.0308,0.0018,0.0315,0.0161,0.0366,-0.0082,-0.0235,0.0608,0.0067,-0.0167,0.0066,0.0388,-0.0277,-0.0244,0.015,-0.0246,0.0217,0.0049,0.0729,0.0353,-0.0305,-0.0756,-0.1805,0.0229,0.0198,-0.0115,0.0258,-0.035,0.0023,0.0081,0.0434,0.0622,0.0903,0.0407,0.0117,-0.0509,0.0031,0.0626,0.0656,0.0384,-0.0043,0.0045,0.0149,0.0392,0.0286,-0.0886,0.0327,0.0252,0.2461,0.0141,0.0663,-0.0091,0.0165,0.0302,-0.0212,-0.104,0.0614,0.0147,0.0568,0.009,-0.0609,-0.0258,-0.0429,0.0081,0.0279,-0.1008,-0.0081,0.0065,0.0182,0.0607,-0.0563,0.0307,0.0136,-0.0663,0.0711,0.0143,0.032,0.0036,-0.1206,0.0457,-0.0621,-0.0003,0.0253,-0.0368,0.0042,-0.0528,0.0201,0.0018,-0.0347,-0.0255,0.0227,-0.0241,-0.0146,0.0688,0.0071,-0.0289,0.017,-0.0182,0.0187,-0.0359,-0.0322,-0.0149,0.059,-0.0225,0.0305,-0.0004,0.0389,0.0443,0.0578,0.0126,-0.0022,-0.0173,0.0095,-0.0416,-0.0598,-0.0065,0.0275,-0.017,-0.2794,0.055,0.0092,0.0464,-0.0366,-0.0378,0.0105,0.001,0.0125,-0.0197,0.009,-0.0016,0.0708,-0.047,-0.0121,0.0041,0.0874,-0.0515,0.0467,-0.007,-0.0038,0.0242,0.2112,-0.0638,0.0237,0.0911,-0.0342,0.0166,0.0388,-0.0188,0.0212,0.0259,0.013,-0.0622,0.0391,0.1009,-0.0759,0.0628,0.0209,-0.0228,0.0326,0.041,-0.0486,-0.0062,0.0933,0.0255,-0.0041,-0.0121,0.012,-0.0072,-0.0355,-0.0443,-0.0427,-0.0041,0.0376,0.0065,-0.0379,-0.0856,-0.0638,-0.0381,0.0642,-0.0459,-0.0215,0.0261,-0.0335]}
{"key":"[Feature functional theory - binding predictor (FFT-BP) for the blind prediction of binding free energies] We present a feature functional theory - binding predictor (FFT-BP) for the protein-ligand binding affinity prediction. The underpinning assumptions of FFT-BP are as follows: i) representability: there exists a microscopic feature vector that can uniquely characterize and distinguish one protein-ligand complex from another; ii) feature-function relationship: the macroscopic features, including binding free energy, of a complex is a functional of microscopic feature vectors; and iii) similarity: molecules with similar microscopic features have similar macroscopic features, such as binding affinity. Physical models, such as implicit solvent models and quantum theory, are utilized to extract microscopic features, while machine learning algorithms are employed to rank the similarity among protein-ligand complexes. A large variety of numerical validations and tests confirms the accuracy and robustness of the proposed FFT-BP model. The root mean square errors (RMSEs) of FFT-BP blind predictions of a benchmark set of 100 complexes, the PDBBind v2007 core set of 195 complexes and the PDBBind v2015 core set of 195 complexes are 1.99, 2.02 and 1.92 kcal/mol, respectively. Their corresponding Pearson correlation coefficients are 0.75, 0.80, and 0.78, respectively.","layer":0,"vector":[-0.0766,-0.0173,0.0153,0.0175,0.054,-0.0148,0.1185,0.069,0.0382,-0.0106,0.0144,-0.0366,-0.0065,0.0545,-0.0049,-0.0197,0.0271,0.054,-0.0557,0.0116,0.0364,-0.0073,-0.0544,-0.0639,-0.0002,0.0514,-0.0247,-0.0145,-0.0375,-0.2469,0.0186,-0.0208,0.0289,-0.0123,-0.0061,-0.0077,-0.0145,0.0493,-0.0913,0.0231,0.0262,0.0056,-0.0126,-0.0295,-0.014,-0.0335,-0.026,0.0026,-0.0399,-0.0139,0.0399,-0.0505,0.0004,0.0507,0.0256,0.0616,0.0429,0.0272,0.028,0.0463,0.0255,0.0645,-0.163,0.0787,0.0711,-0.0011,-0.0261,-0.0335,0.0771,0.0711,-0.027,0.0615,-0.0101,0.0039,0.0256,-0.0392,-0.0206,0.0051,-0.0502,0.0484,0.0108,0.0008,-0.0742,-0.0631,-0.0617,-0.0139,0.0174,-0.0257,0.0649,0.0402,-0.0475,-0.0494,-0.0097,0.0251,-0.0773,0.0076,0.0673,-0.0155,-0.0561,0.1884,-0.0251,0.0674,-0.0373,-0.0693,0.0463,-0.0693,-0.0212,-0.0587,-0.0261,-0.0268,-0.0227,0.0029,0.0138,-0.0668,0.0254,0.0196,0.0548,0.0501,-0.0132,-0.0135,-0.0182,0.0043,0.0479,0.047,0.0392,-0.0221,-0.0277,0.1092,0.0082,0.0374,0.0493,-0.0112,-0.0386,-0.0401,0.0242,0.0359,0.0315,-0.0222,-0.0307,0.0244,-0.0245,-0.0374,-0.0231,-0.0916,-0.0459,0.1051,-0.0505,0.003,-0.0313,-0.0057,-0.0142,0.0683,-0.0134,0.0009,0.0551,0.026,-0.0014,0.0234,-0.0218,0.021,-0.0873,-0.0307,-0.0287,0.0571,-0.0216,-0.0705,-0.0203,-0.0136,0.0068,-0.0242,0.0361,0.0006,-0.0348,0.0283,0.0903,0.0588,-0.0402,0.0277,-0.0152,0.0089,0.0292,-0.0249,-0.0598,0.0289,0.0123,-0.0312,-0.0031,-0.0226,0.0051,0.0658,-0.018,0.0384,-0.029,0.0086,-0.0418,-0.0006,-0.0505,-0.0048,-0.0132,-0.059,0.0886,0.0018,-0.0792,0.036,-0.0319,0.0163,0.0145,0.0004,0.0364,-0.0034,-0.0427,-0.0041,0.0198,-0.0732,-0.0594,-0.012,0.0368,0.0262,-0.0075,0.0606,0.0756,-0.0427,-0.1069,-0.1961,0.005,0.0104,-0.0165,0.0305,-0.0429,0.056,-0.002,0.0349,0.0474,0.0506,0.0347,-0.0322,-0.0037,-0.0522,0.0017,0.022,-0.0211,-0.0595,0.0352,-0.0079,0.0112,-0.0271,-0.0225,0.0142,-0.028,0.168,0.0211,0.0098,-0.01,0.032,0.0617,-0.0497,-0.0922,0.0217,0.0311,0.0507,-0.0446,0.0166,-0.0011,-0.0289,0.0464,0.0038,-0.0869,-0.0622,0.0051,-0.0202,0.0164,-0.0708,0.0519,0.0693,-0.0411,0.0526,-0.0104,-0.004,-0.0326,-0.053,0.0644,-0.0359,0.0197,0.017,-0.0524,0.014,-0.0319,0.0437,-0.0067,-0.0058,-0.0501,0.019,-0.0357,-0.0572,0.0709,-0.0163,0.0706,0.0845,0.0027,0.0278,-0.037,-0.0131,0.0046,0.073,0.0154,0.0392,0.0019,0.0018,0.0055,0.0668,0.0058,0.0257,-0.0404,-0.0167,0.0016,-0.0338,-0.0299,0.0605,0.0298,-0.3028,0.0826,-0.0032,0.042,-0.0267,-0.0265,0.0686,-0.0278,-0.0765,-0.014,0.0271,0.0227,0.0344,-0.0273,-0.0226,-0.0031,0.0729,-0.0532,0.0125,-0.0353,0.0256,0.0602,0.2655,-0.0581,0.0282,0.0667,0.0158,0.0142,0.0037,-0.0336,0.0261,0.0109,0.0871,-0.0782,0.0078,0.0801,-0.0265,-0.0199,0.0012,-0.0168,0.0676,0.0163,-0.0705,-0.0186,0.0943,-0.0348,-0.036,0.003,-0.0141,0.057,-0.0657,0.0338,-0.0236,0.0089,0.0079,0.0104,-0.0233,-0.0386,-0.0225,0.008,-0.006,-0.024,-0.0498,0.0657,-0.0166]}
{"key":"[Deep Aesthetic Assessment and Retrieval of Breast Cancer Treatment Outcomes] Treatments for breast cancer have continued to evolve and improve in recent years, resulting in a substantial increase in survival rates, with approximately 80\\% of patients having a 10-year survival period. Given the serious impact that breast cancer treatments can have on a patient's body image, consequently affecting her self-confidence and sexual and intimate relationships, it is paramount to ensure that women receive the treatment that optimizes both survival and aesthetic outcomes. Currently, there is no gold standard for evaluating the aesthetic outcome of breast cancer treatment. In addition, there is no standard way to show patients the potential outcome of surgery. The presentation of similar cases from the past would be extremely important to manage women's expectations of the possible outcome. In this work, we propose a deep neural network to perform the aesthetic evaluation. As a proof-of-concept, we focus on a binary aesthetic evaluation. Besides its use for classification, this deep neural network can also be used to find the most similar past cases by searching for nearest neighbours in the highly semantic space before classification. We performed the experiments on a dataset consisting of 143 photos of women after conservative treatment for breast cancer. The results for accuracy and balanced accuracy showed the superior performance of our proposed model compared to the state of the art in aesthetic evaluation of breast cancer treatments. In addition, the model showed a good ability to retrieve similar previous cases, with the retrieved cases having the same or adjacent class (in the 4-class setting) and having similar types of asymmetry. Finally, a qualitative interpretability assessment was also performed to analyse the robustness and trustworthiness of the model.","layer":0,"vector":[-0.0186,-0.023,0.0298,0.0014,0.0397,0.0037,0.0523,-0.0143,0.0107,-0.0647,0.0194,-0.0703,0.0241,0.0156,0.0212,0.0306,0.0383,0.0517,-0.0382,0.0484,-0.0067,-0.0095,-0.0153,-0.0284,0.0388,-0.032,-0.0413,-0.0413,-0.0685,-0.2185,0.021,-0.0501,0.0399,-0.0399,-0.0032,-0.0422,-0.0199,0.0457,-0.0314,-0.0054,-0.0001,0.0134,-0.0364,-0.043,0.0237,-0.0308,-0.0634,-0.0361,-0.0075,-0.0193,0.0153,-0.0728,0.0247,0.0651,0.057,0.0488,0.1045,0.0309,0.0287,0.0281,0.0281,0.0206,-0.1388,0.0423,0.032,0.0062,-0.0546,-0.0622,0.0242,0.0519,0.0226,0.0113,0.029,0.0394,0.017,-0.0162,0.0593,0.0099,-0.0387,0.0097,0.0184,0.0202,-0.0104,-0.0376,0.0052,-0.0574,0.0075,-0.04,0.0143,0.0092,-0.062,-0.0105,-0.0523,-0.0001,-0.0524,-0.0134,0.0003,0.0093,-0.0166,0.2177,-0.0665,0.0171,0.0306,-0.0557,0.0347,-0.0131,-0.0015,-0.0168,-0.0254,0.0152,-0.0185,-0.0402,0.0233,-0.0338,0.004,0.0304,0.0392,0.0315,0.0147,-0.0527,-0.0353,0.0106,0.0636,-0.0104,0.0069,-0.0169,0.0495,0.1285,0.0219,-0.004,0.0382,-0.0011,-0.0281,-0.0112,-0.0278,0.0009,0.0056,-0.0009,0.0102,-0.0095,0.0072,-0.1198,0.0363,-0.0688,-0.0468,0.1202,-0.0294,0.0583,-0.0477,-0.0181,-0.0269,0.0227,-0.0603,-0.0083,0.0338,0.0142,0.007,0.0093,-0.0623,0.0016,0.0355,-0.0526,-0.0567,0.1031,0.0053,-0.067,-0.0428,-0.0165,-0.01,-0.0179,0.0769,0.03,-0.019,0.0388,0.0492,0.0215,-0.034,-0.0697,0.015,0.0061,0.0307,-0.0475,-0.0231,0.0573,0.0435,-0.0572,-0.005,-0.0548,0.0399,0.0455,-0.0141,-0.0005,-0.0191,0.0399,0.0221,-0.0521,-0.0368,-0.0145,-0.0216,0.0001,0.0293,0.0111,-0.0231,0.0295,-0.0067,0.0243,-0.0162,0.0292,0.0975,0.026,-0.0744,0.0361,0.0915,-0.0032,0.0107,-0.0099,0.0286,0.0109,-0.0216,0.0626,0.0492,-0.0566,-0.076,-0.2175,0.0431,0.0387,-0.0314,0.0539,-0.0739,-0.0116,-0.0028,0.0207,0.0306,0.0795,0.0232,-0.0308,0.0412,0.0045,0.0108,0.0358,0.0499,-0.0753,-0.0503,-0.0158,0.0303,0.0238,-0.0922,0.0551,0.0183,0.2303,0.048,0.0014,0.0045,0.0151,0.0186,-0.0861,-0.1149,0.1097,-0.0006,0.0685,0.013,-0.0752,0.0027,-0.0164,-0.0092,-0.0064,-0.0543,-0.024,-0.0157,-0.0008,0.0091,-0.0806,0.0301,0.0219,-0.0285,0.0169,0.0377,0.0228,-0.0164,-0.1393,0.0225,-0.0387,0.0089,0.0424,-0.0631,0.0099,-0.082,0.023,-0.019,-0.0556,-0.0381,0.0025,-0.0702,-0.0288,0.098,-0.0262,-0.0559,0.0639,0.0478,0.0185,0.0266,-0.0407,-0.0315,0.0878,-0.0037,0.0121,-0.0161,0.0346,0.0375,0.0784,-0.0527,0.0318,-0.0198,-0.004,0.0331,-0.1008,-0.0191,0.0063,-0.0038,-0.2803,0.0507,0.027,0.0717,-0.0573,0.0031,0.0254,0.0163,-0.0165,-0.014,0.0052,0.0044,0.0778,-0.0433,0.0029,-0.0442,0.0661,-0.0564,0.0718,-0.0401,0.0354,-0.0039,0.1995,-0.0516,0.0245,0.0236,-0.0019,-0.0197,0.0245,0.0223,0.0209,0.0513,0.0617,-0.0304,0.0443,0.1017,-0.0213,0.0447,0.0149,-0.0218,0.0131,0.0299,-0.0149,0.0105,0.1232,-0.0376,-0.0312,-0.0067,-0.0173,0.0201,0.004,0.0078,-0.0149,-0.0306,0.0496,0.0137,-0.0683,-0.0342,-0.0292,-0.0153,0.0339,-0.063,-0.0126,0.0402,-0.0073]}
{"key":"[Impact of classification difficulty on the weight matrices spectra in Deep Learning and application to early-stopping] Much research effort has been devoted to explaining the success of deep learning. Random Matrix Theory (RMT) provides an emerging way to this end: spectral analysis of large random matrices involved in a trained deep neural network (DNN) such as weight matrices or Hessian matrices with respect to the stochastic gradient descent algorithm. To have more comprehensive understanding of weight matrices spectra, we conduct extensive experiments on weight matrices in different modules, e.g., layers, networks and data sets. Following the previous work of \\cite{martin2018implicit}, we classify the spectra in the terminal stage into three main types: Light Tail (LT), Bulk Transition period (BT) and Heavy Tail(HT). These different types, especially HT, implicitly indicate some regularization in the DNNs. A main contribution from the paper is that we identify the difficulty of the classification problem as a driving factor for the appearance of heavy tail in weight matrices spectra. Higher the classification difficulty, higher the chance for HT to appear. Moreover, the classification difficulty can be affected by the signal-to-noise ratio of the dataset, or by the complexity of the classification problem (complex features, large number of classes) as well. Leveraging on this finding, we further propose a spectral criterion to detect the appearance of heavy tails and use it to early stop the training process without testing data. Such early stopped DNNs have the merit of avoiding overfitting and unnecessary extra training while preserving a much comparable generalization ability. These findings from the paper are validated in several NNs, using Gaussian synthetic data and real data sets (MNIST and CIFAR10).","layer":0,"vector":[-0.0795,-0.0048,-0.0021,-0.0073,0.019,0.0624,0.0111,0.009,0.0901,-0.0245,0.0352,-0.017,0.0245,0.0894,0.0281,0.0311,0.0228,0.0188,-0.0524,0.0085,0.0046,0.0121,-0.0215,-0.0356,0.0345,0.0119,-0.0243,-0.0313,-0.0447,-0.275,0.0369,-0.0501,0.0542,-0.0428,0.0413,-0.0297,-0.0319,0.033,-0.0366,0.0085,0.0169,0.0291,-0.0168,0.0244,-0.0184,-0.046,-0.0159,-0.0268,-0.0338,-0.0174,0.0019,-0.0046,0.0381,0.0432,0.0226,0.0293,0.0646,0.033,0.0581,0.0565,0.0355,0.025,-0.1828,0.039,0.0698,0.0192,-0.0516,-0.0565,0.0055,0.0493,0.0123,0.0621,0.0231,0.0451,0.0389,0.0164,0.0335,-0.0421,0.0029,-0.0056,0.0375,-0.0242,-0.0304,0.0097,-0.0243,-0.0152,0.0305,-0.0483,0.0349,0.0226,-0.0371,0.0031,-0.0443,0.0403,-0.0382,-0.0143,0.0393,0.0075,-0.0421,0.1797,-0.0604,0.0051,0.0451,0.0033,0.0129,0.0083,-0.0528,-0.0447,0.0044,-0.01,0.0419,-0.0158,0.0288,-0.0706,0.0032,-0.021,0.0577,0.0607,-0.0316,-0.0493,-0.0559,-0.0285,0.0723,-0.0015,0.0243,-0.0546,0.0121,0.1691,0.0536,0.0198,0.0014,-0.0292,-0.0715,-0.0196,0.0305,0.0301,0.0245,0.0247,0.0184,-0.0472,-0.0302,-0.0232,0.0053,-0.0898,-0.044,0.0901,-0.0833,-0.0019,-0.02,-0.008,-0.0323,0.0049,-0.0077,-0.0293,0.0426,0.0299,0.028,0.0459,-0.0759,0.012,-0.0329,-0.0607,0.011,0.1008,0.0376,-0.057,-0.0341,-0.0218,-0.0165,-0.0617,0.0157,0.0275,-0.0518,-0.0079,0.0701,0.0218,-0.0962,0.0113,-0.0053,0.0217,0.016,-0.0178,-0.0586,0.0535,0.0351,-0.0421,-0.0083,-0.0613,0.022,0.0297,-0.0513,-0.0099,-0.0492,-0.0142,-0.0362,-0.048,-0.0647,0.0219,0.0134,-0.0069,0.0061,0.0114,0.0173,0.0612,-0.02,0.0147,0.0068,-0.0078,0.0321,0.0283,-0.0196,0.0017,0.0395,-0.0126,-0.0357,-0.0253,-0.0086,0.0497,-0.0231,0.0312,0.0585,-0.0565,-0.079,-0.2106,0.0114,-0.0034,-0.0105,0.0843,-0.0876,0.0578,0.0019,0.0769,0.0558,0.0536,0.0276,-0.0256,-0.0431,0.015,0.0523,0.0299,0.0207,-0.0095,0.0039,-0.0162,0.0162,-0.0184,-0.1045,0.0377,-0.0193,0.2123,-0.0023,0.0574,-0.0139,0.0069,0.0493,-0.0198,-0.0569,0.089,0.0367,0.0999,0.0157,-0.0336,-0.0178,0.0351,0.0135,0.0089,-0.1044,-0.0295,-0.042,-0.0051,-0.0023,-0.0524,0.0129,0.0536,0.005,0.028,0.0034,0.0189,-0.0454,-0.1099,0.0451,-0.0627,-0.041,-0.0008,-0.0904,0.0059,-0.0621,0.0288,-0.0245,-0.032,-0.0529,0.0284,-0.0003,-0.0549,0.0895,-0.0147,-0.0204,0.0703,-0.0276,-0.0015,-0.0146,-0.0308,0.0188,0.0514,-0.0398,0.015,0.0043,0.0588,0.0179,0.0819,-0.0248,0.0237,0.0236,0.024,0.0676,-0.0484,-0.0064,0.0536,-0.0134,-0.2604,0.0347,-0.0209,0.0375,0.018,0.0002,0.014,0.0212,-0.044,-0.0045,-0.0037,0.0192,0.0778,-0.0351,0.0101,0.0321,0.0665,-0.0247,0.0394,-0.0408,-0.0368,-0.0053,0.2379,-0.0588,0.0198,0.0154,-0.043,0.0193,0.0303,-0.0276,0.0325,0.0169,0.1008,-0.0852,0.0466,0.0723,-0.0291,0.0463,0.0283,-0.0256,0.0385,-0.0318,-0.065,-0.0459,0.1082,-0.0406,-0.0118,-0.0427,-0.0219,0.0484,-0.0136,-0.0034,-0.0313,-0.0277,0.0055,-0.0013,-0.0505,-0.0425,-0.0142,-0.0161,0.0123,-0.0307,-0.0328,-0.0246,0.0062]}
{"key":"[Finding trainable sparse networks through Neural Tangent Transfer] Deep neural networks have dramatically transformed machine learning, but their memory and energy demands are substantial. The requirements of real biological neural networks are rather modest in comparison, and one feature that might underlie this austerity is their sparse connectivity. In deep learning, trainable sparse networks that perform well on a specific task are usually constructed using label-dependent pruning criteria. In this article, we introduce Neural Tangent Transfer, a method that instead finds trainable sparse networks in a label-free manner. Specifically, we find sparse networks whose training dynamics, as characterized by the neural tangent kernel, mimic those of dense networks in function space. Finally, we evaluate our label-agnostic approach on several standard classification tasks and show that the resulting sparse networks achieve higher classification performance while converging faster.","layer":4,"vector":[0.0078,-0.027,-0.0047,0.0423,0.0085,0.0344,0.0255,0.0134,0.0448,-0.0329,0.0176,-0.0534,0.0804,0.0585,-0.0003,-0.0038,0.0349,0.0822,-0.0411,-0.0044,0.0256,-0.0491,-0.03,-0.031,0.0489,0.0119,-0.0178,-0.04,-0.0232,-0.2546,0.0546,-0.0587,0.0356,-0.0366,0.0578,-0.0465,-0.0041,0.013,-0.0628,0.0689,0.0326,0.039,-0.0403,-0.0515,0.0095,-0.0551,-0.012,-0.0332,-0.0004,-0.0936,0.0201,-0.0472,0.0008,0.0498,0.0074,0.0348,0.0365,0.0319,0.0655,0.0368,0.0215,0.052,-0.1503,0.0739,0.0796,0.0355,-0.0201,-0.0048,0.0009,0.079,0.0001,0.0708,0.0116,0.0415,0.0111,-0.0266,0.0312,-0.0333,0.0065,0.0298,0.0427,0.0006,-0.0419,-0.0502,-0.025,-0.0404,0.0253,-0.0768,0.018,0.0246,-0.0157,0.0175,-0.0426,0.0239,-0.0627,0.0094,0.0242,0.0107,-0.0648,0.1814,-0.0936,0.0659,0.0475,-0.0194,0.0065,-0.0527,-0.0322,-0.0227,-0.0454,-0.0258,-0.0337,-0.0158,0.031,-0.0512,0.0483,-0.0208,0.0678,0.0374,-0.0023,0.0002,-0.0183,0.0148,0.0278,-0.0258,0.0361,-0.0168,0.0039,0.1524,0.0843,0.0352,0.0286,-0.0244,-0.034,-0.0016,0.032,0.0161,0.0199,-0.0322,-0.0374,0.0277,-0.0163,-0.0351,0.0587,-0.109,-0.0647,0.0883,-0.0508,0.0008,-0.0104,-0.0031,-0.0252,0.0135,-0.019,-0.0334,-0.0024,0.022,0.0632,0.0425,-0.0659,0.0364,-0.0025,-0.0457,-0.0303,0.1034,0.0258,-0.0788,-0.0411,-0.0035,-0.0195,-0.016,0.0342,0.0343,-0.0255,0.0308,0.0523,0.0756,-0.081,-0.0325,-0.0225,-0.0027,0.0232,-0.0338,-0.0451,0.0519,0.0298,-0.0123,-0.0145,-0.0873,0.0029,0.0652,-0.0304,0.0118,-0.0091,-0.0277,-0.0349,-0.0471,0.0157,0.0292,0.0324,-0.01,0.0161,-0.0322,-0.027,0.0096,0.0079,-0.0011,-0.0208,-0.0013,0.0152,0.0315,-0.034,0.0192,0.0537,-0.0578,-0.0532,0.0052,-0.0271,0.022,-0.0324,0.0327,0.0533,-0.062,-0.063,-0.2333,-0.0007,0.0276,-0.007,0.0558,-0.0844,0.0445,0.0067,0.0582,0.0552,0.0285,0.0094,-0.03,0.0027,-0.0075,0.0646,0.0222,0.0179,0.0009,-0.049,0.0197,0.0108,-0.0028,-0.0866,0.054,0.0022,0.2277,-0.0009,0.0316,-0.056,0.0026,0.0562,0.0065,-0.1092,0.0793,-0.0029,0.0709,-0.0109,-0.0153,-0.0372,-0.032,-0.02,-0.0008,-0.1196,-0.0118,-0.014,-0.0339,-0.0324,-0.047,0.0332,0.0338,0.0119,0.0425,-0.026,-0.0062,-0.0545,-0.0659,0.0558,-0.0556,0.0419,-0.0271,-0.0647,0.0128,-0.0511,0.0346,0.0471,-0.0139,-0.007,0.0457,-0.0293,-0.0009,0.0684,0.0076,0.0269,0.0766,-0.0125,0.0075,0.0047,-0.0312,-0.03,0.0661,-0.0117,0.0364,0.0036,0.0352,0.0402,0.0928,-0.0309,0.026,-0.0339,0.0049,0.0066,-0.0493,-0.0272,0.0568,-0.0263,-0.27,0.0486,0.0219,-0.0023,-0.0016,0.0279,0.0622,0.0077,-0.0379,0.0224,-0.0221,-0.0039,0.0523,-0.014,0.0081,0.0513,0.068,-0.0624,0.0609,-0.085,0.0125,0.0164,0.202,-0.0363,0.0225,-0.0016,-0.0231,0.0044,0.0087,-0.0274,0.0428,0.0154,0.0418,-0.0677,0.0155,0.0719,-0.0106,0.0358,0.022,-0.0108,0.0579,-0.0073,-0.063,-0.0511,0.1013,-0.0298,-0.0291,-0.0287,-0.0545,0.0175,-0.0325,0.0021,0.0284,0.0077,0.0184,0.0299,-0.0409,-0.0512,-0.0539,-0.0597,0.0034,-0.115,-0.014,0.0186,-0.0173]}
{"key":"[Random Reshuffling: Simple Analysis with Vast Improvements] Random Reshuffling (RR) is an algorithm for minimizing finite-sum functions that utilizes iterative gradient descent steps in conjunction with data reshuffling. Often contrasted with its sibling Stochastic Gradient Descent (SGD), RR is usually faster in practice and enjoys significant popularity in convex and non-convex optimization. The convergence rate of RR has attracted substantial attention recently and, for strongly convex and smooth functions, it was shown to converge faster than SGD if 1) the stepsize is small, 2) the gradients are bounded, and 3) the number of epochs is large. We remove these 3 assumptions, improve the dependence on the condition number from $\\kappa^2$ to $\\kappa$ (resp. from $\\kappa$ to $\\sqrt{\\kappa}$) and, in addition, show that RR has a different type of variance. We argue through theory and experiments that the new variance type gives an additional justification of the superior performance of RR. To go beyond strong convexity, we present several results for non-strongly convex and non-convex objectives. We show that in all cases, our theory improves upon existing literature. Finally, we prove fast convergence of the Shuffle-Once (SO) algorithm, which shuffles the data only once, at the beginning of the optimization process. Our theory for strongly-convex objectives tightly matches the known lower bounds for both RR and SO and substantiates the common practical heuristic of shuffling once or only a few times. As a byproduct of our analysis, we also get new results for the Incremental Gradient algorithm (IG), which does not shuffle the data at all.","layer":2,"vector":[-0.0672,-0.049,0.0445,-0.0128,0.0382,0.0599,-0.0006,-0.0012,0.0354,-0.0344,0.0469,-0.0247,0.0492,0.0594,0.0098,0.0475,0.0076,0.0193,-0.0362,-0.0317,0.0256,-0.0429,-0.0393,-0.0759,0.0541,-0.0162,-0.022,-0.0484,-0.0064,-0.2738,0.0302,-0.0338,-0.0129,-0.0309,0.023,-0.0067,-0.0145,0.0663,-0.0478,0.0465,0.0182,0.0325,-0.0513,-0.0243,-0.0156,-0.0529,-0.0283,-0.033,-0.0274,0.0246,0.01,-0.0327,0.0204,0.0345,0.0257,0.0325,0.0489,0.0257,0.0731,0.0509,0.015,0.0182,-0.1331,0.0397,0.0627,-0.0065,-0.0301,-0.0812,-0.0248,0.0835,-0.0145,0.065,0.0422,0.0277,0.0335,0.0012,-0.0001,-0.0417,-0.0149,0.0335,0.0055,-0.0469,-0.0621,0.0189,-0.0382,-0.0459,0.0214,-0.0953,0.0249,0.0029,-0.0119,-0.014,-0.0149,0.0139,-0.0732,0.0157,0.0576,0.0515,-0.0125,0.2067,-0.0724,0.0398,0.0225,0.0218,0.008,-0.0325,-0.0352,-0.0099,-0.0067,-0.0028,0.0146,-0.0639,0.0588,-0.0576,0.0134,-0.0012,0.0451,0.0401,-0.0078,-0.0039,-0.0644,0.0256,0.0475,-0.0122,0.0237,-0.0438,0.004,0.1388,0.0068,0.06,0.0793,0.0004,-0.0173,-0.0273,0.0159,0.0086,-0.0278,0.0088,0.0375,-0.0017,-0.035,-0.0349,0.0073,-0.1234,-0.0421,0.1696,-0.0337,0.0319,-0.0436,-0.0947,0.022,-0.0055,-0.0126,-0.0289,0.0062,-0.001,0.0578,0.0604,-0.0405,0.0269,-0.0533,-0.0622,0.0006,0.0965,0.0051,-0.0644,-0.0296,-0.0009,0.0049,-0.0249,0.0293,0.0327,-0.0135,0.0392,0.0626,0.0491,-0.0933,-0.018,0.0052,0.0218,0.0241,-0.0124,-0.0552,0.0447,0.0234,-0.0561,0.0115,0.0134,0.0002,0.0445,-0.0724,-0.0049,-0.0134,-0.0262,-0.0101,-0.018,-0.0027,0.0089,0.0214,0.005,0.0075,-0.0171,-0.0212,0.0693,0.0231,0.0153,-0.0084,-0.0345,0.0141,0.0438,0.0159,-0.0384,0.0584,-0.0312,0.0034,0.0225,0.0176,0.0367,0.0038,0.025,0.0561,-0.0397,-0.0467,-0.2021,-0.0183,0.0072,0.0055,0.041,-0.0899,0.0182,0.0253,0.0767,0.0584,0.039,-0.0341,-0.0157,0.0392,0.015,0.0395,0.0129,0.0284,-0.0417,-0.0167,0.0482,0.0442,-0.0151,-0.0888,0.0723,0.0017,0.2079,0.0307,0.0582,-0.0238,0.0453,0.0184,0.0109,-0.0702,0.0495,0.0362,0.0964,-0.0222,-0.0616,-0.014,-0.0222,0.034,0.0145,-0.095,-0.0378,-0.04,-0.0478,-0.0227,-0.0345,0.0253,0.0053,-0.0283,0.093,-0.023,0.0142,-0.0213,-0.0765,-0.0052,-0.0408,-0.0005,-0.0242,-0.0851,0.0064,-0.0736,0.045,-0.0036,-0.0134,-0.0177,-0.0123,-0.0085,-0.0197,0.0421,-0.0505,0.0082,0.0268,0.0364,0.0306,-0.015,-0.0219,-0.0369,0.0747,-0.0823,0.0519,0.0213,0.0226,-0.0612,0.0635,-0.0163,0.0266,0.0048,0.0047,0.0033,-0.0906,0.0206,0.0191,0.0127,-0.2859,0.0005,-0.0232,0.0031,-0.0082,0.0294,0.0566,-0.0103,-0.0641,0.003,0.0028,0.0603,0.0374,-0.0354,0.0529,0.0252,0.0577,-0.0347,0.0298,-0.0752,0.0196,0.0372,0.231,-0.0216,-0.0048,0.02,-0.0194,0.011,0.0046,-0.0444,-0.0141,-0.0282,0.0624,-0.0606,0.0582,0.0379,-0.0205,0.0376,0.0429,-0.003,-0.0075,-0.0127,0.0073,0.0094,0.1076,-0.041,0.002,-0.0425,0.0106,0.0351,-0.0575,0.0191,-0.0124,-0.0203,0.0369,0.0208,-0.0783,-0.0435,-0.0304,-0.0154,0.0469,-0.072,-0.0596,0.0197,-0.0142]}
{"key":"[CHAI: A CHatbot AI for Task-Oriented Dialogue with Offline Reinforcement Learning] Conventionally, generation of natural language for dialogue agents may be viewed as a statistical learning problem: determine the patterns in human-provided data and generate appropriate responses with similar statistical properties. However, dialogue can also be regarded as a goal directed process, where speakers attempt to accomplish a specific task. Reinforcement learning (RL) algorithms are designed specifically for solving such goal-directed problems, but the most direct way to apply RL -- through trial-and-error learning in human conversations, -- is costly. In this paper, we study how offline reinforcement learning can instead be used to train dialogue agents entirely using static datasets collected from human speakers. Our experiments show that recently developed offline RL methods can be combined with language models to yield realistic dialogue agents that better accomplish task goals.","layer":1,"vector":[-0.059,0.0197,0.0336,-0.0221,-0.0466,-0.0083,0.0433,0.0467,0.013,-0.0354,0.0232,-0.0286,0.0509,0.0699,0.0509,0.0163,-0.004,0.0265,-0.0585,0.026,0.0465,-0.0757,-0.0203,-0.0377,-0.0258,0.0086,-0.0857,-0.0885,-0.0145,-0.1931,0.013,-0.0357,0.0637,-0.0188,-0.0191,0.0232,-0.0265,0.0323,-0.0274,-0.0036,0.0159,0.0314,0.0098,-0.0867,-0.0267,-0.0255,-0.033,-0.0128,-0.0084,-0.018,0.0012,-0.0131,0.0299,-0.0084,0.0464,0.0538,0.0465,0.0885,0.0506,0.0344,-0.0147,0.0352,-0.184,0.0731,-0.0081,0.0394,-0.0413,-0.0022,0.045,0.034,-0.0286,0.0272,0.0217,0.0506,0.028,0.0027,-0.0131,-0.0479,0.0705,0.0126,0.0188,-0.0392,-0.0425,0.0018,-0.0309,-0.0852,0.0173,-0.0282,0.0151,0.0312,-0.0379,0.023,-0.0067,0.0509,-0.0646,-0.0222,0.0058,0.0114,-0.0537,0.2155,-0.0116,0.0183,-0.0126,-0.0348,0.0144,-0.0477,-0.0621,-0.0569,0.0053,-0.0028,-0.0287,0.0094,0.0218,-0.0177,0.0447,0.0157,0.1054,-0.0084,-0.0237,0.0116,-0.0252,0.0421,0.0292,-0.0472,0.04,-0.076,0.0599,0.1052,0.0271,0.0455,0.0871,-0.0388,-0.0257,-0.0136,0.0291,0.006,0.0436,0.0046,0.0304,-0.0218,-0.031,-0.0423,-0.0336,-0.0974,-0.0478,0.0994,-0.0067,0.0444,-0.0487,-0.0073,-0.0264,0.0267,0.0465,-0.031,-0.015,0.0185,0.0354,0.0316,-0.0887,0.0095,-0.0026,-0.0817,-0.0338,0.0652,0.003,-0.0893,-0.0806,-0.0022,0.0161,0.0196,0.0374,0.0308,-0.0318,0.0118,0.0472,0.0289,-0.0887,-0.0036,-0.0115,0.0059,0.0131,-0.0541,-0.0109,0.0188,-0.0334,-0.0831,0.0165,-0.0363,0.0302,-0.0072,-0.0085,0.0313,0.0093,0.009,-0.056,-0.0218,0.0277,-0.0222,-0.0218,-0.0113,-0.0187,-0.0098,-0.0609,-0.0004,-0.04,0.0076,0.0106,0.0236,0.1062,0.0202,-0.0329,0.0245,0.0422,0.0074,-0.0679,-0.0122,0.0417,-0.0139,0.0165,0.0258,0.0094,0.0132,-0.0206,-0.2521,0.019,0.0053,-0.0469,0.0329,-0.0342,0.0431,-0.0088,0.0454,0.0924,0.0745,-0.0681,-0.0211,0.0328,-0.0146,0.0694,0.0165,0.004,-0.0231,0.0143,0.0445,0.0119,-0.0396,-0.1378,0.0303,0.0072,0.2182,0.0297,0.0396,-0.0245,0.0411,0.0423,-0.0435,-0.1412,0.0907,0.027,0.0903,-0.0253,0.0004,0.0159,-0.0286,0.0375,-0.0072,-0.0923,-0.0389,-0.0224,-0.0424,0.0028,-0.057,-0.0088,0.0442,-0.0238,0.073,-0.0288,-0.0447,-0.0361,-0.0765,0.0328,-0.0363,-0.0031,0.0271,0.0161,0.0455,-0.0423,0.0198,0.0011,0.0136,-0.0636,0.0681,0.0306,-0.0399,0.1077,-0.0116,0.0395,0.0062,0.0142,0.0043,-0.042,-0.0431,-0.0255,0.0755,-0.033,0.0475,0.0274,0.0328,-0.0341,0.0585,-0.0239,0.0682,0.0,0.0297,0.0194,-0.0549,-0.0336,0.0524,-0.0092,-0.2841,0.0504,0.0075,0.0286,-0.0133,0.0218,0.0548,0.0045,-0.0612,-0.0182,-0.0041,0.0573,0.005,-0.0039,0.0336,0.0306,0.0908,-0.011,0.0218,-0.0399,0.0331,0.0372,0.1949,-0.0436,0.0988,-0.0098,-0.034,-0.0448,0.0559,-0.0285,0.0143,0.0045,0.0777,-0.0299,0.03,0.0488,-0.0234,0.0303,0.0081,-0.0121,-0.0629,0.026,0.0034,-0.0318,0.0746,0.0408,-0.0211,-0.0452,-0.0142,0.0385,-0.008,-0.0121,-0.0056,-0.0043,0.0098,0.0018,-0.0291,-0.0539,-0.0223,-0.0313,-0.0128,-0.0401,0.0333,-0.0193,-0.0206]}
{"key":"[A Survey of Unsupervised Deep Domain Adaptation] Deep learning has produced state-of-the-art results for a variety of tasks. While such approaches for supervised learning have performed well, they assume that training and testing data are drawn from the same distribution, which may not always be the case. As a complement to this challenge, single-source unsupervised domain adaptation can handle situations where a network is trained on labeled data from a source domain and unlabeled data from a related but different target domain with the goal of performing well at test-time on the target domain. Many single-source and typically homogeneous unsupervised deep domain adaptation approaches have thus been developed, combining the powerful, hierarchical representations from deep learning with domain adaptation to reduce reliance on potentially-costly target data labels. This survey will compare these approaches by examining alternative methods, the unique and common elements, results, and theoretical insights. We follow this with a look at application areas and open research directions.","layer":1,"vector":[-0.0155,-0.0455,-0.0084,-0.0132,0.0704,0.0364,0.0023,0.0114,-0.0037,-0.0045,0.0013,-0.0371,0.0311,0.0527,0.0806,0.0,0.0149,0.0912,-0.0305,-0.0274,0.0345,-0.0286,0.0086,0.0047,0.0067,-0.0019,0.0135,-0.0198,-0.0339,-0.2603,0.0475,-0.0433,0.0161,-0.0126,0.0535,-0.0341,-0.0519,0.0506,-0.015,0.017,0.0314,0.0057,-0.0438,-0.0663,-0.0358,-0.0595,-0.0297,-0.0277,-0.0002,-0.0164,0.0358,-0.0226,0.0308,0.0009,0.0201,0.0003,0.0754,0.0474,0.063,0.0421,0.0078,0.0693,-0.1497,0.0546,0.0298,0.0443,-0.0226,-0.0019,-0.021,0.0491,-0.0059,0.0458,-0.0007,0.0394,0.0198,0.0216,-0.0134,-0.011,-0.0211,-0.0024,0.0443,0.0003,-0.0643,-0.0195,0.0372,-0.066,0.0091,-0.0588,0.0547,0.0188,-0.0723,-0.025,-0.0103,0.0361,-0.0781,-0.0007,0.0571,0.0011,-0.0543,0.1969,-0.0604,0.0147,0.0124,-0.0037,0.0167,0.0179,0.0112,-0.0181,-0.0051,0.0293,-0.031,-0.0233,0.0095,-0.018,0.0226,-0.0256,0.0951,0.0175,-0.0158,-0.0463,-0.0177,-0.0125,0.0606,-0.0626,0.0409,-0.0278,0.02,0.109,0.037,0.0219,0.0442,-0.0415,-0.0835,-0.0271,-0.0014,0.0616,0.0489,0.0004,0.0105,0.0422,-0.0369,-0.0598,0.0358,-0.0201,-0.0659,0.1113,-0.0344,-0.0028,-0.0536,-0.0312,-0.0496,0.006,-0.0389,-0.0323,-0.0152,0.0303,0.0492,0.0445,-0.0492,0.0263,0.0062,-0.0648,-0.0118,0.1058,-0.0205,-0.1217,-0.0721,-0.0141,-0.003,-0.043,0.0348,0.0293,-0.0135,0.0404,0.0619,0.033,-0.0869,-0.0127,0.022,0.0206,0.0211,-0.0569,-0.0366,0.073,0.0506,-0.0652,-0.0054,-0.042,0.016,0.0488,-0.04,0.0197,-0.0615,0.0265,0.0248,-0.0444,-0.0196,-0.0037,0.0039,-0.0617,-0.0014,0.0124,-0.0374,-0.0138,-0.0733,0.0251,-0.012,0.0059,0.0501,0.0286,-0.0214,0.0431,0.0557,-0.0285,-0.0274,-0.0095,0.0245,0.0461,-0.0009,0.0389,0.0234,-0.045,-0.0195,-0.2102,0.0078,0.0026,-0.0649,0.0407,-0.0376,0.028,0.0331,0.0777,0.07,0.0417,-0.0224,-0.0571,0.0118,-0.0064,0.0094,0.0407,0.0113,-0.0034,-0.029,-0.0139,0.0147,0.0484,-0.136,0.0542,-0.0058,0.1945,-0.0158,0.0856,-0.0252,0.0256,0.0068,0.0481,-0.139,0.0344,0.026,0.0755,-0.0244,-0.0288,-0.0001,-0.004,0.0215,0.0336,-0.1375,-0.024,-0.0367,-0.0032,0.0022,-0.0512,0.0426,0.0147,-0.0639,0.0826,-0.0105,-0.0327,-0.0286,-0.0675,0.038,-0.0267,-0.0139,0.0044,-0.0437,0.0417,-0.0851,0.0415,-0.0142,-0.0554,-0.0344,0.0502,-0.0252,-0.0469,0.0482,0.0022,0.039,0.0518,-0.0189,0.0037,-0.0333,-0.0639,-0.0215,0.0707,-0.008,0.0227,0.0183,0.0424,0.0427,0.1062,-0.0313,0.0246,-0.0181,-0.0155,0.0026,-0.0664,-0.0426,0.0402,-0.0065,-0.2663,0.0398,0.0522,0.0412,-0.0084,0.0008,0.0532,0.0486,-0.0411,0.0264,-0.0005,0.0396,0.0498,-0.0128,-0.0201,0.0469,0.0555,-0.0331,0.0658,-0.0793,0.0321,0.0352,0.1986,-0.0694,0.024,-0.0021,-0.0128,-0.0022,0.0582,-0.0255,-0.0182,-0.0081,0.089,-0.022,0.0191,0.1394,-0.0364,-0.0062,0.0086,0.0216,0.0258,-0.0107,-0.0184,0.005,0.0564,0.0065,-0.0031,-0.0214,-0.0511,0.0445,-0.0452,-0.0258,-0.0064,0.0173,0.0365,-0.0182,-0.028,-0.0587,-0.0701,-0.0674,0.0558,-0.0376,-0.0416,0.0051,-0.0312]}
{"key":"[Bandana: Using Non-volatile Memory for Storing Deep Learning Models] Typical large-scale recommender systems use deep learning models that are stored on a large amount of DRAM. These models often rely on embeddings, which consume most of the required memory. We present Bandana, a storage system that reduces the DRAM footprint of embeddings, by using Non-volatile Memory (NVM) as the primary storage medium, with a small amount of DRAM as cache. The main challenge in storing embeddings on NVM is its limited read bandwidth compared to DRAM. Bandana uses two primary techniques to address this limitation: first, it stores embedding vectors that are likely to be read together in the same physical location, using hypergraph partitioning, and second, it decides the number of embedding vectors to cache in DRAM by simulating dozens of small caches. These techniques allow Bandana to increase the effective read bandwidth of NVM by 2-3x and thereby significantly reduce the total cost of ownership.","layer":8,"vector":[-0.0623,0.0207,0.031,0.0018,0.0167,0.0374,-0.0258,0.0264,0.0234,-0.0048,0.0104,-0.0357,0.0633,0.0764,0.0287,-0.0103,0.0174,0.0182,-0.0092,-0.0205,0.0569,-0.0451,-0.0094,-0.0311,-0.0036,0.0076,-0.0408,-0.018,-0.0662,-0.2646,0.0149,-0.0548,0.0422,-0.0036,-0.0238,-0.0352,0.0224,0.0311,-0.0092,0.0413,0.0184,0.0361,-0.0549,-0.0309,0.0077,-0.024,-0.0032,-0.0161,0.0046,-0.0229,0.0435,-0.0158,0.0287,0.0052,0.0077,0.0512,0.0788,0.0578,0.0099,0.0305,0.0668,0.0404,-0.1319,0.0373,0.009,0.0007,-0.0348,0.01,0.0146,0.0619,0.0001,0.0487,0.0279,0.0683,0.0196,0.0127,0.0079,0.0158,0.0031,-0.014,-0.0185,-0.008,-0.0403,-0.0806,0.0126,-0.0223,0.0021,-0.0429,-0.0374,-0.0171,-0.0166,-0.0037,0.0222,0.0013,-0.0484,-0.0237,0.0504,0.0431,-0.0578,0.2318,-0.0443,0.0344,0.0253,-0.0227,0.0341,-0.0505,0.0042,-0.0124,-0.034,-0.0318,-0.0026,-0.02,0.0105,-0.0149,0.0711,0.0051,0.0654,0.0302,-0.0079,0.0166,-0.0738,0.0308,0.0342,-0.0092,0.0132,-0.0457,-0.0165,0.1391,0.0327,0.0286,-0.0007,-0.0284,-0.0688,-0.0067,0.0352,0.0406,0.0303,-0.0455,0.0198,-0.0205,-0.0679,0.0063,0.0241,-0.0645,-0.0412,0.0854,0.0042,-0.013,-0.0869,-0.0566,-0.0057,0.0228,-0.0164,-0.0437,0.0548,0.0136,0.0263,0.0668,-0.0822,0.0219,-0.0091,-0.0598,-0.0326,0.0757,-0.0045,-0.0674,-0.0378,-0.0147,0.0195,-0.068,0.0479,0.0517,-0.0141,0.0377,0.0874,0.0441,-0.0776,-0.0559,0.015,-0.0009,0.0092,-0.0318,-0.0496,0.0378,0.0203,-0.0232,-0.0181,-0.0648,0.0042,0.0393,-0.0308,0.0363,-0.0333,-0.0143,0.0084,-0.0167,-0.0235,0.0128,0.01,-0.0321,0.0298,0.0199,-0.0524,0.0657,-0.0078,0.0226,-0.0589,-0.0513,0.034,-0.0012,-0.0585,-0.0148,0.0224,0.0016,-0.0846,0.0161,0.024,0.0505,0.0419,0.0444,0.0628,-0.0664,-0.0538,-0.2288,-0.0008,-0.0096,-0.0158,0.0557,-0.0592,0.0584,0.0078,0.0618,0.0498,0.0284,-0.0593,-0.0166,0.0314,-0.0024,0.0864,0.018,0.0186,-0.0398,-0.0355,0.0039,0.0343,-0.0102,-0.0571,0.0766,0.0054,0.2246,0.0245,0.0,-0.0624,0.0472,0.0391,-0.0332,-0.158,0.052,0.0256,0.0504,0.0276,-0.0163,-0.026,-0.0224,0.0253,0.0287,-0.0993,-0.0092,-0.0255,-0.0383,0.0384,-0.0535,0.0377,0.0208,-0.0561,0.0433,0.0303,-0.0309,-0.0664,-0.0706,0.0033,-0.0682,0.0497,-0.028,-0.0711,0.0109,-0.0707,0.0226,-0.0137,-0.0287,0.0038,0.0123,-0.0219,-0.0108,0.0408,-0.0694,-0.0052,0.0785,-0.005,0.0368,-0.0535,-0.0777,-0.0109,0.0651,-0.0111,0.0262,0.036,0.0363,0.0117,0.073,0.0063,0.0298,-0.0044,0.0324,-0.0172,-0.0793,-0.0355,0.0344,-0.0091,-0.3142,0.0577,-0.0244,0.0374,-0.0272,-0.0275,0.025,0.0261,-0.0006,0.0413,-0.0048,0.0593,0.0557,-0.039,-0.0285,0.0235,0.0771,-0.0456,0.0581,-0.0178,0.0051,0.0102,0.2074,0.0191,0.0225,0.008,-0.0178,-0.001,0.0231,-0.018,0.0221,-0.0206,0.0939,-0.0324,0.0352,0.0778,-0.0295,0.0094,0.0653,0.0003,0.0217,-0.0183,-0.0431,0.0083,0.0647,-0.0027,0.0228,-0.0269,0.011,0.0183,-0.0096,-0.0016,-0.0157,-0.003,0.0404,0.057,-0.0472,-0.0574,-0.0339,-0.0587,0.0172,-0.0575,-0.0181,-0.0393,0.0133]}
{"key":"[Self-Enhanced GNN: Improving Graph Neural Networks Using Model Outputs] Graph neural networks (GNNs) have received much attention recently because of their excellent performance on graph-based tasks. However, existing research on GNNs focuses on designing more effective models without considering much about the quality of the input data. In this paper, we propose self-enhanced GNN (SEG), which improves the quality of the input data using the outputs of existing GNN models for better performance on semi-supervised node classification. As graph data consist of both topology and node labels, we improve input data quality from both perspectives. For topology, we observe that higher classification accuracy can be achieved when the ratio of inter-class edges (connecting nodes from different classes) is low and propose topology update to remove inter-class edges and add intra-class edges. For node labels, we propose training node augmentation, which enlarges the training set using the labels predicted by existing GNN models. SEG is a general framework that can be easily combined with existing GNN models. Experimental results validate that SEG consistently improves the performance of well-known GNN models such as GCN, GAT and SGC across different datasets.","layer":2,"vector":[0.0177,-0.0192,0.0428,-0.0241,0.0607,0.03,-0.0035,0.0418,-0.0125,-0.0077,0.0206,-0.0597,0.0614,0.0802,0.038,0.0226,0.0212,0.0904,-0.0334,-0.019,-0.0221,-0.0161,0.0035,-0.028,0.0644,0.0002,-0.008,-0.0057,-0.0588,-0.2441,0.0131,-0.0376,0.0365,-0.0029,-0.0086,-0.0862,0.0158,0.0127,-0.0375,0.0416,0.0215,-0.0069,-0.0245,-0.0219,-0.0019,-0.0019,-0.0035,-0.0385,-0.0266,-0.0641,0.0333,-0.0325,0.0115,0.0036,0.0116,0.039,0.0188,0.0138,0.0151,0.029,0.0471,0.0853,-0.1476,0.0513,0.0447,0.0451,-0.0866,0.0202,-0.0,0.0639,0.0321,0.0232,0.0124,-0.0144,0.0137,0.0594,0.0213,0.009,0.0273,0.0067,-0.0257,-0.0215,-0.0297,-0.0427,0.041,-0.0216,-0.0071,-0.0562,0.0194,0.04,-0.0091,-0.0227,-0.0379,0.0438,-0.0495,-0.0429,0.0449,-0.0082,-0.0907,0.2207,-0.0591,0.0069,0.0361,-0.0139,0.0513,-0.0396,-0.0005,-0.057,-0.0515,-0.0066,-0.0271,-0.0227,-0.0232,-0.028,-0.005,0.001,0.0896,0.0558,-0.0157,-0.0066,-0.0338,0.042,0.0115,-0.0317,0.041,-0.0567,-0.0229,0.1171,0.0053,0.0169,0.0377,0.0091,0.004,0.014,-0.0145,-0.0029,0.0381,0.0101,0.0106,0.0005,0.0018,-0.0426,0.0045,-0.0784,-0.0886,0.1206,-0.0746,-0.0014,-0.0257,-0.052,-0.0494,0.0154,-0.0566,0.001,-0.0382,0.0566,0.0719,0.0473,-0.0903,-0.0049,-0.0194,0.0129,-0.0741,0.0845,0.0512,-0.1086,-0.0489,0.0098,0.0117,-0.0419,0.0246,0.0737,-0.0396,0.0286,0.0964,0.051,-0.0631,-0.0288,-0.0323,-0.0086,0.0244,-0.0026,-0.0475,0.0422,0.033,-0.0247,-0.0076,-0.0592,-0.0001,0.0705,-0.0583,0.0546,-0.0056,-0.0159,-0.0272,-0.0277,-0.0209,-0.0396,0.0007,-0.037,0.026,-0.0203,0.0135,0.021,-0.0429,0.019,-0.0359,0.0459,0.0004,-0.009,-0.0629,0.0118,0.0646,-0.0348,-0.0472,-0.0166,-0.0024,0.0503,-0.0019,0.0431,0.0417,-0.026,-0.0512,-0.1832,-0.0116,0.0195,-0.0238,0.0701,-0.0986,0.0368,0.0116,0.0688,0.0654,0.0709,0.0214,-0.0426,0.0038,-0.01,0.044,0.0512,0.0281,0.0002,0.0032,-0.03,0.0601,-0.0012,-0.0705,0.038,-0.0017,0.2294,-0.0147,0.0448,-0.0416,-0.0011,0.0137,-0.082,-0.1101,0.0915,0.0539,0.0648,-0.0107,-0.062,-0.0568,-0.0323,-0.0074,-0.0053,-0.1276,-0.0136,-0.0054,-0.0214,-0.0253,-0.043,0.009,0.0655,0.0166,0.0632,0.0048,-0.022,-0.0296,-0.0918,0.0461,-0.0482,0.0345,-0.0006,-0.0755,0.0046,-0.0579,0.0656,0.0302,-0.0588,-0.0297,0.0346,-0.0051,-0.0119,0.083,0.0279,-0.0164,0.0309,-0.0158,0.0093,0.0155,-0.0367,-0.0281,0.0226,-0.064,0.0482,0.0199,0.0199,0.0479,0.0592,0.0044,0.0381,-0.0132,-0.0078,0.0089,-0.0359,-0.0196,0.0668,-0.0235,-0.2658,0.0364,0.0163,0.073,-0.0278,0.0235,0.0213,0.0694,-0.0381,0.0168,0.0297,0.0437,0.0259,-0.0103,-0.0271,0.0282,0.0282,-0.0359,0.0507,-0.035,0.0685,0.0498,0.2216,-0.0195,0.032,0.0446,-0.0504,-0.0157,0.038,-0.0194,-0.0114,0.0158,0.075,-0.0472,0.0446,0.0876,-0.0347,0.0395,0.0077,-0.009,0.0016,-0.015,-0.0554,-0.0297,0.0751,-0.0163,-0.0167,-0.0683,0.009,0.0534,-0.028,-0.0239,-0.0097,-0.003,0.0075,-0.01,-0.0565,-0.0224,-0.0705,-0.0323,0.016,-0.0711,0.0092,-0.0053,-0.0256]}
{"key":"[Gated Graph Recurrent Neural Networks] Graph processes exhibit a temporal structure determined by the sequence index and and a spatial structure determined by the graph support. To learn from graph processes, an information processing architecture must then be able to exploit both underlying structures. We introduce Graph Recurrent Neural Networks (GRNNs) as a general learning framework that achieves this goal by leveraging the notion of a recurrent hidden state together with graph signal processing (GSP). In the GRNN, the number of learnable parameters is independent of the length of the sequence and of the size of the graph, guaranteeing scalability. We prove that GRNNs are permutation equivariant and that they are stable to perturbations of the underlying graph support. To address the problem of vanishing gradients, we also put forward gated GRNNs with three different gating mechanisms: time, node and edge gates. In numerical experiments involving both synthetic and real datasets, time-gated GRNNs are shown to improve upon GRNNs in problems with long term dependencies, while node and edge gates help encode long range dependencies present in the graph. The numerical results also show that GRNNs outperform GNNs and RNNs, highlighting the importance of taking both the temporal and graph structures of a graph process into account.","layer":4,"vector":[-0.0096,-0.0502,-0.0037,-0.0141,0.0082,0.0333,0.0127,0.0032,0.0458,-0.0254,0.0138,-0.0084,0.0675,0.0665,0.0454,0.0345,-0.0305,0.0687,-0.0223,-0.0329,0.0522,-0.0496,-0.0054,-0.0466,0.0382,0.013,-0.0022,-0.0119,-0.066,-0.2264,0.0138,-0.0593,0.0285,-0.0244,-0.0034,-0.035,-0.0094,0.0043,-0.0466,0.0449,0.0425,0.0043,-0.049,-0.0529,-0.0042,0.0015,-0.0477,-0.024,-0.0136,-0.0368,0.024,-0.0291,0.0297,0.0095,0.056,0.0478,0.0679,0.0276,0.029,0.0355,0.0247,0.064,-0.1299,0.0194,0.035,0.0367,-0.0416,-0.0021,0.0335,0.071,-0.003,0.0233,0.0083,0.0087,0.0023,0.0319,0.0049,-0.0102,-0.0186,-0.0278,0.0192,-0.0067,-0.0336,-0.0449,-0.0073,-0.0405,-0.0098,-0.0399,0.0408,0.0112,-0.0436,-0.0113,-0.0103,0.029,-0.0642,-0.027,0.0577,0.0318,-0.0511,0.1892,-0.0754,0.0276,0.0543,-0.0045,0.0182,-0.0432,-0.015,-0.0217,-0.0432,0.0333,-0.0143,-0.0446,0.0416,-0.0466,0.0163,-0.0096,0.0411,0.0352,-0.0116,0.0154,-0.0172,0.0348,0.0076,-0.0214,0.0329,-0.0623,-0.0018,0.1131,0.029,0.0439,0.0666,0.0158,0.0084,0.0036,0.0039,-0.0028,0.0378,-0.0288,-0.0086,-0.0231,-0.0574,0.0204,0.0337,-0.0523,-0.0899,0.1277,-0.0525,0.0074,-0.0525,-0.0318,-0.0691,-0.0031,-0.0111,-0.0418,0.0227,0.054,0.0048,0.0206,-0.0565,0.04,-0.0573,-0.0104,-0.0346,0.0743,0.0268,-0.1081,-0.02,-0.0437,0.0083,-0.0191,0.0734,0.0371,-0.02,-0.0194,0.0632,0.0785,-0.0759,-0.0325,0.0092,0.0173,0.0183,-0.0352,-0.0315,0.0518,0.0361,-0.0336,0.0093,-0.0458,-0.026,0.0396,-0.0636,0.0364,-0.0041,0.0188,-0.0488,-0.0419,-0.0275,-0.0571,-0.0227,-0.033,0.0119,-0.0494,-0.0017,-0.0011,0.0031,0.0312,-0.0483,0.0053,0.0002,0.0122,-0.0452,0.0191,0.0456,-0.0395,-0.0252,-0.0094,0.015,0.0306,0.023,0.0484,0.0392,-0.055,-0.0762,-0.2178,-0.0336,0.0169,-0.0349,0.0567,-0.0575,0.0088,0.0052,0.0825,0.0408,0.029,-0.019,-0.0473,0.0001,0.0212,0.08,0.0179,0.0485,-0.0251,-0.0176,-0.0092,0.0182,0.0181,-0.0966,0.0434,0.0283,0.2351,0.016,0.0666,-0.0628,0.0206,0.0116,-0.044,-0.0656,0.0778,0.0559,0.0562,0.0307,-0.0094,-0.0519,-0.0495,-0.0161,0.0024,-0.0948,-0.07,0.0048,-0.0046,-0.0242,-0.0502,-0.0072,0.0528,0.0018,0.0688,0.0302,-0.0331,-0.052,-0.0655,0.0333,-0.0611,0.0363,0.0245,-0.0487,-0.0093,-0.047,0.0717,0.0263,0.0032,-0.0515,0.0145,0.0293,0.0101,0.0921,0.0559,-0.0304,0.0521,-0.0113,0.0085,-0.0197,-0.0576,-0.019,0.0384,-0.0386,0.0783,0.0414,0.043,0.0012,0.1097,-0.0172,0.0433,0.0222,0.0332,0.0288,-0.0414,-0.0496,0.0359,-0.0353,-0.3057,0.0569,0.0165,0.0693,-0.0398,0.0237,0.0223,0.0529,-0.0643,0.0025,-0.0114,0.0614,0.0374,-0.0192,-0.0165,0.0784,0.0856,-0.0383,0.0285,-0.0527,0.0678,0.0522,0.2125,-0.0213,0.0733,-0.0019,0.0,-0.0222,0.0265,-0.0245,-0.0016,0.0006,0.0848,-0.0245,0.0397,0.074,-0.0297,0.0925,-0.0033,-0.0194,0.0358,-0.0062,-0.0263,-0.0166,0.0719,-0.0599,-0.0495,-0.0632,0.0131,0.0481,-0.0681,-0.0164,-0.0041,0.0308,0.0054,0.0296,-0.0491,-0.0307,-0.0307,-0.062,-0.0025,-0.0997,-0.032,-0.0152,-0.0638]}
{"key":"[CoDraw: Collaborative Drawing as a Testbed for Grounded Goal-driven Communication] In this work, we propose a goal-driven collaborative task that combines language, perception, and action. Specifically, we develop a Collaborative image-Drawing game between two agents, called CoDraw. Our game is grounded in a virtual world that contains movable clip art objects. The game involves two players: a Teller and a Drawer. The Teller sees an abstract scene containing multiple clip art pieces in a semantically meaningful configuration, while the Drawer tries to reconstruct the scene on an empty canvas using available clip art pieces. The two players communicate with each other using natural language. We collect the CoDraw dataset of ~10K dialogs consisting of ~138K messages exchanged between human players. We define protocols and metrics to evaluate learned agents in this testbed, highlighting the need for a novel \"crosstalk\" evaluation condition which pairs agents trained independently on disjoint subsets of the training data. We present models for our task and benchmark them using both fully automated evaluation and by having them play the game live with humans.","layer":4,"vector":[-0.0393,-0.0232,0.0112,-0.0118,-0.0213,0.0294,0.0315,0.0201,0.0054,-0.0117,0.0011,-0.0462,0.0351,0.0665,0.0152,0.0139,-0.0361,0.0387,-0.0382,0.0095,0.0436,-0.0598,0.0264,-0.0847,0.0029,0.0244,-0.0702,-0.0864,-0.0548,-0.1945,0.0157,-0.0274,0.0498,-0.0019,-0.0176,0.0072,-0.0079,0.0316,-0.0419,0.0005,0.0089,0.0186,0.0007,-0.0371,-0.0407,-0.0673,-0.0541,-0.0326,-0.0169,-0.0216,0.0171,-0.0662,0.0363,0.0248,0.0475,0.0813,0.0716,0.0409,0.0987,0.0266,0.0205,0.0309,-0.1604,0.0699,0.005,0.027,-0.0387,0.0172,0.0041,0.0278,-0.0048,0.0273,0.0331,0.0539,0.0473,-0.0156,-0.0021,-0.0647,0.0015,0.0004,-0.0207,-0.0146,-0.0063,0.0079,-0.0094,-0.0234,0.0292,0.0058,0.0141,0.0116,-0.0597,0.0112,-0.012,-0.0063,-0.0714,0.0143,0.0204,-0.0075,-0.0543,0.2473,-0.0454,0.0011,0.0595,-0.0642,0.027,-0.049,-0.0152,-0.072,-0.0042,0.0043,-0.0601,-0.0007,0.0315,-0.0294,0.0455,0.0144,0.0672,0.0044,0.0077,-0.0009,-0.0082,0.067,0.0297,-0.0451,0.0447,-0.0823,0.0521,0.1683,0.0377,0.0524,0.0398,0.0149,0.007,0.0027,-0.0071,0.0417,0.0185,0.0129,0.0348,0.0159,0.0388,-0.0264,0.0049,-0.0583,-0.031,0.1095,0.0138,0.0417,-0.0432,0.0101,-0.05,-0.029,0.0034,0.0197,-0.0217,-0.0237,0.0476,0.0277,-0.0874,0.0135,0.0196,-0.0763,-0.0547,0.0616,-0.0269,-0.0781,-0.0238,-0.0064,0.0472,-0.0247,0.0003,0.0551,-0.0124,-0.0241,0.0351,0.0269,-0.1085,-0.014,0.0026,0.0044,0.028,-0.0257,-0.0055,0.0504,0.0037,-0.0745,0.0069,-0.0567,0.0337,0.0349,0.0326,0.0413,-0.0174,0.0484,-0.0297,-0.0108,0.0018,-0.0198,-0.0711,-0.0079,0.0174,-0.0225,-0.0457,0.0114,0.0163,-0.012,0.0016,0.016,0.0577,0.0257,-0.0742,-0.0366,-0.0068,0.0178,-0.0155,-0.0054,-0.0074,0.0299,-0.0053,0.0234,-0.0055,-0.0129,-0.0126,-0.2402,0.0058,0.0281,-0.0441,0.0474,-0.0226,0.0408,-0.0358,0.0193,0.0576,0.1117,-0.0245,-0.0131,0.0326,0.0041,0.0515,-0.0139,0.032,-0.0133,-0.0025,0.0224,0.0021,-0.0424,-0.0757,0.0512,-0.0049,0.2528,0.0484,0.0167,0.0167,0.0322,0.0473,-0.0617,-0.161,0.0309,0.0473,0.0861,-0.012,-0.0425,-0.0266,-0.0699,-0.0079,-0.0005,-0.0831,-0.0368,-0.0234,-0.0337,0.0053,-0.0655,0.0134,0.0555,-0.0233,0.0083,0.0288,-0.0898,-0.0082,-0.0871,0.024,-0.0252,0.0447,0.0119,-0.0099,-0.0126,-0.037,0.0746,0.0187,-0.0011,-0.0505,0.0623,0.0154,-0.0146,0.0303,-0.0212,0.0082,0.0386,0.0539,0.0474,-0.0437,-0.0372,-0.0178,0.0823,-0.0067,0.03,-0.008,0.0322,-0.0157,0.0637,-0.0142,0.024,-0.0435,0.0152,0.0015,-0.0662,-0.0246,0.0086,-0.0503,-0.3174,0.0239,0.0449,0.0351,-0.0354,0.0339,0.0654,0.0387,-0.0539,-0.0401,0.0368,0.0455,0.002,-0.0021,0.0059,0.0329,0.0524,-0.0383,0.059,-0.0479,0.001,0.0304,0.2053,-0.0305,0.0535,-0.0151,0.019,-0.0425,0.0347,-0.031,-0.007,0.0016,0.1132,-0.0248,0.0424,0.0411,-0.0117,0.027,0.0263,-0.0248,-0.0373,0.0159,0.0113,-0.0317,0.0526,0.0516,-0.0116,-0.0177,-0.0229,-0.0138,-0.0347,0.0057,-0.0171,-0.0322,0.0201,0.0103,-0.0394,-0.0431,-0.0305,0.0079,0.0063,-0.0693,-0.0047,-0.0082,-0.0019]}
{"key":"[Reconfigurable Intelligent Surfaces in Action for Non-Terrestrial Networks] Next-generation communication technology will be made possible by cooperation between terrestrial networks with non-terrestrial networks (NTN) comprised of high-altitude platform stations and satellites. Further, as humanity embarks on the long road to establish new habitats on other planets, cooperation between NTN and deep-space networks (DSN) will be necessary. In this regard, we propose the use of reconfigurable intelligent surfaces (RIS) to improve coordination between these networks given that RIS perfectly match the size, weight, and power restrictions of operating in space. A comprehensive framework of RIS-assisted non-terrestrial and interplanetary communications is presented that pinpoints challenges, use cases, and open issues. Furthermore, the performance of RIS-assisted NTN under environmental effects such as solar scintillation and satellite drag is discussed in light of simulation results.","layer":11,"vector":[-0.0586,-0.0152,0.0374,-0.026,0.0003,0.0096,0.0214,0.0422,0.0123,-0.0158,0.02,-0.0461,0.0594,0.0685,0.0344,0.0077,-0.0066,0.0378,0.0048,0.0257,0.068,-0.025,-0.0336,-0.0143,0.0269,0.0402,-0.0492,-0.0355,-0.0618,-0.2726,0.0214,-0.0635,0.0305,-0.0272,-0.0216,-0.0166,-0.0439,-0.0336,-0.0286,0.0535,0.03,-0.0158,-0.0067,-0.0327,-0.0121,-0.0656,-0.0276,-0.0358,0.0,-0.0502,0.0514,-0.0722,-0.0024,0.0658,0.0449,0.0729,0.0048,0.0048,0.0268,0.0336,0.0314,0.0436,-0.1955,0.0775,0.0301,0.0044,-0.026,-0.0248,0.0439,0.0044,-0.003,0.064,0.0158,0.0315,0.0713,0.0531,-0.0163,-0.0601,0.0278,-0.0232,0.0184,-0.0274,-0.0698,0.0267,-0.0298,-0.028,-0.0346,-0.0068,0.0017,0.0192,-0.0901,0.0364,-0.0115,-0.0169,-0.082,-0.0082,0.0302,-0.0222,-0.0458,0.1734,-0.0471,0.0167,0.0425,-0.0073,0.0336,-0.0349,-0.0201,-0.0319,-0.0366,0.0059,-0.0006,-0.0521,-0.0105,0.0189,0.0273,0.0228,0.0244,0.0546,0.0099,-0.0442,-0.0292,-0.0137,0.0411,-0.0071,0.029,-0.0492,0.0389,0.1212,0.0213,0.0845,0.0555,-0.0102,-0.0117,-0.0089,0.0234,0.033,-0.0205,-0.0307,-0.0146,0.0122,-0.0113,-0.0614,0.033,-0.0942,0.0065,0.0745,-0.0128,0.056,-0.0327,0.0003,-0.0292,0.0244,0.0097,-0.019,-0.0105,-0.008,0.0113,0.0664,-0.0969,0.0401,0.0176,-0.0271,-0.0532,0.1403,0.0073,-0.1209,-0.0248,-0.0006,0.0595,0.0163,-0.0026,0.0198,-0.0252,-0.0158,0.0832,0.0264,-0.1056,-0.0039,0.0038,0.0367,-0.0073,-0.0403,0.0121,0.0007,0.0359,-0.0466,-0.0398,0.0073,0.0189,0.0117,-0.0315,0.0216,-0.0114,0.0328,0.012,-0.0327,-0.0133,-0.0222,0.0239,0.0108,0.0292,-0.0244,-0.0455,0.046,0.0074,0.0103,-0.0277,-0.0005,0.009,-0.0033,-0.0071,-0.0531,0.0527,-0.0659,-0.0279,0.0328,0.0338,0.0432,0.0199,0.0173,0.046,-0.0435,-0.0657,-0.2168,-0.019,-0.0063,-0.0272,0.0448,-0.0082,0.0529,0.0427,0.0148,0.0155,0.1096,-0.0083,0.0078,0.0268,-0.0161,0.0447,-0.0074,0.0309,-0.0165,-0.0657,-0.0154,0.0443,-0.0055,-0.0456,0.0541,0.0286,0.1884,0.0054,0.042,-0.0236,0.0596,0.0489,-0.0025,-0.1185,0.0549,0.0979,0.0769,-0.0005,-0.0251,-0.0621,-0.0379,0.0096,0.0039,-0.0811,0.0105,-0.0202,-0.0796,-0.0197,-0.0371,-0.0337,0.033,-0.0336,0.0004,0.0024,0.0265,-0.0152,-0.0721,-0.0089,-0.0308,0.0356,-0.0108,0.0105,-0.0348,-0.0543,0.0854,0.0281,0.0078,-0.0023,0.0617,-0.0112,0.0163,0.1294,-0.015,0.0546,0.0332,-0.0429,0.0441,0.0048,0.02,-0.0292,0.0916,-0.0073,0.049,0.0334,0.0041,0.006,0.0523,-0.0445,0.0278,-0.0302,-0.0277,0.0281,-0.0308,-0.0006,0.0557,-0.0303,-0.3165,0.039,-0.0192,-0.0015,-0.0429,-0.0187,0.0472,0.0587,-0.0748,0.0055,-0.0302,0.041,-0.0251,0.0134,0.0241,0.0256,0.0658,-0.0451,-0.0062,-0.0475,0.0474,0.0052,0.2062,-0.0271,0.0284,0.0105,-0.04,0.0435,0.0141,-0.006,0.0232,0.0152,0.0372,-0.078,0.0137,0.1015,0.0039,0.0261,0.0069,-0.0215,-0.0211,0.0195,0.035,0.0091,0.0803,-0.0304,-0.0674,-0.071,0.0364,-0.0284,-0.0436,-0.012,-0.0497,-0.0207,0.0282,-0.0105,-0.0493,-0.038,-0.0551,-0.0051,0.0553,-0.0745,-0.0406,-0.0309,0.014]}
{"key":"[Classification Confidence Estimation with Test-Time Data-Augmentation] Machine learning plays an increasingly significant role in many aspects of our lives (including medicine, transportation, security, justice and other domains), making the potential consequences of false predictions increasingly devastating. These consequences may be mitigated if we can automatically flag such false predictions and potentially assign them to alternative, more reliable mechanisms, that are possibly more costly and involve human attention. This suggests the task of detecting errors, which we tackle in this paper for the case of visual classification. To this end, we propose a novel approach for classification confidence estimation. We apply a set of semantics-preserving image transformations to the input image, and show how the resulting image sets can be used to estimate confidence in the classifier's prediction. We demonstrate the potential of our approach by extensively evaluating it on a wide variety of classifier architectures and datasets, including ResNext/ImageNet, achieving state of the art performance. This paper constitutes a significant revision of our earlier work in this direction (Bahat & Shakhnarovich, 2018).","layer":4,"vector":[-0.0042,-0.0463,-0.0074,0.0182,0.0646,0.0403,0.0529,0.0235,0.0039,-0.003,0.0029,-0.0706,-0.0021,0.0467,0.0155,0.0214,0.031,0.057,-0.0226,0.0058,0.0131,-0.0489,0.0339,-0.0554,0.0303,0.0089,0.0195,-0.0571,-0.0603,-0.2256,0.0011,-0.1107,0.0137,-0.0681,0.0403,-0.0476,-0.0582,0.0797,-0.058,0.0014,-0.0048,-0.0214,-0.0505,-0.062,-0.0207,-0.0386,-0.0081,-0.0335,-0.0399,-0.0264,0.0363,-0.0058,0.0233,0.0456,0.0231,0.0227,0.0515,0.0388,0.0771,0.0701,0.009,0.0589,-0.134,0.0485,0.0073,0.0079,-0.0513,-0.0456,0.0107,0.0389,0.0099,0.0456,0.0043,0.1081,-0.0028,-0.0146,-0.0014,-0.0597,-0.0098,0.0211,0.0144,-0.0089,-0.0132,0.0051,0.0393,-0.0701,0.0085,-0.0187,0.0475,0.0079,-0.0388,-0.0134,-0.0461,0.0198,-0.0505,-0.0073,0.0251,0.0327,-0.0301,0.1987,-0.0885,-0.0056,0.0233,-0.0293,0.0548,-0.0408,0.0032,-0.0535,-0.065,-0.0221,-0.0332,-0.0117,0.0529,-0.001,0.0185,-0.0146,0.0297,0.0182,-0.0214,-0.0154,-0.0585,0.0066,0.0384,-0.0306,0.0118,-0.059,0.0509,0.157,-0.0167,0.0114,0.0224,-0.0301,-0.0617,-0.0281,-0.0233,0.0383,0.0111,0.0302,0.0205,-0.0088,-0.0151,-0.0855,0.0358,-0.0296,-0.0567,0.13,-0.0371,0.0307,-0.0115,-0.042,-0.0246,0.0667,-0.07,-0.0024,0.0437,0.0122,0.0331,0.0417,-0.053,0.0081,0.0258,-0.0692,-0.0769,0.0993,0.0075,-0.0543,-0.0304,0.0097,0.0209,-0.005,0.0246,0.0439,-0.0096,0.0178,0.0182,0.0144,-0.055,-0.0028,0.0183,-0.006,0.0335,-0.0326,-0.0322,0.0495,0.0528,0.0081,-0.0234,-0.077,0.0246,0.0525,-0.0016,0.0276,-0.0225,-0.0165,-0.0228,-0.052,-0.0422,0.0097,-0.0055,-0.0283,0.0096,0.0096,0.0245,0.0273,-0.0225,0.0268,-0.0442,0.0372,0.0399,0.0314,-0.0271,-0.0052,0.0547,-0.0248,-0.0365,-0.0113,0.0341,0.0437,0.0113,0.0084,0.0807,-0.02,-0.0257,-0.2196,0.0023,0.0243,0.0038,0.0482,-0.1137,0.0278,0.0025,0.0656,0.0256,0.0617,-0.0119,-0.0175,0.0266,0.013,0.0138,0.0415,0.0062,-0.0609,-0.0159,-0.0219,0.0568,-0.021,-0.1162,0.0664,0.038,0.2246,0.0221,0.0278,-0.0062,0.0384,0.0317,-0.0518,-0.1025,0.092,-0.0103,0.0221,0.0083,-0.0412,-0.0407,-0.0084,0.0282,0.0065,-0.0947,-0.0092,-0.0152,-0.0358,0.0373,-0.0368,-0.0074,0.0042,-0.0312,0.0396,-0.0011,-0.0331,-0.0322,-0.0544,0.0319,0.0015,0.032,0.0154,-0.0272,0.0284,-0.0733,0.0535,-0.0221,-0.0684,-0.0637,0.0241,-0.0426,-0.0271,0.103,0.0128,-0.0152,0.0533,0.0298,-0.0032,-0.0262,-0.0235,-0.0237,0.0308,0.015,0.0001,0.0127,0.0532,0.0248,0.0881,0.0081,0.0356,0.0248,0.062,0.017,-0.0524,-0.016,0.0384,0.0185,-0.3149,0.0197,0.0093,0.0183,-0.0625,0.0161,0.0601,0.0154,-0.0399,-0.0306,-0.0159,0.0415,0.1026,-0.0179,0.0021,0.0292,0.0689,-0.061,0.0665,-0.0277,0.0208,0.0209,0.2426,-0.057,-0.0066,0.0431,-0.0348,-0.0342,0.0365,-0.0154,0.0281,-0.0047,0.0734,-0.0316,0.0237,0.0775,-0.0688,0.0045,0.0014,-0.0023,0.0216,-0.0069,-0.0187,-0.005,0.1087,0.0049,0.0104,-0.0427,0.0002,0.0107,-0.0441,-0.009,-0.0183,-0.0242,0.0109,0.0247,-0.0384,-0.0438,-0.0302,-0.0202,0.0337,-0.0721,0.0145,-0.003,-0.0215]}
{"key":"[A Fast and Greedy Subset-of-Data (SoD) Scheme for Sparsification in Gaussian processes] In their standard form Gaussian processes (GPs) provide a powerful non-parametric framework for regression and classificaton tasks. Their one limiting property is their $\\mathcal{O}(N^{3})$ scaling where $N$ is the number of training data points. In this paper we present a framework for GP training with sequential selection of training data points using an intuitive selection metric. The greedy forward selection strategy is devised to target two factors - regions of high predictive uncertainty and underfit. Under this technique the complexity of GP training is reduced to $\\mathcal{O}(M^{3})$ where $(M \\ll N)$ if $M$ data points (out of $N$) are eventually selected. The sequential nature of the algorithm circumvents the need to invert the covariance matrix of dimension $N \\times N$ and enables the use of favourable matrix inverse update identities. We outline the algorithm and sequential updates to the posterior mean and variance. We demonstrate our method on selected one dimensional functions and show that the loss in accuracy due to using a subset of data points is marginal compared to the computational gains.","layer":0,"vector":[-0.0379,-0.0038,0.0333,0.0076,0.0419,-0.0204,0.0213,-0.0074,0.0548,-0.0196,0.0165,-0.0325,0.0144,0.0179,0.0043,0.0273,0.0225,0.0501,-0.0674,0.0381,0.0348,-0.0422,-0.0194,-0.0304,0.0765,0.0233,-0.0164,-0.0561,-0.0347,-0.2427,0.0195,-0.0549,0.0513,-0.0049,0.0354,0.0158,-0.0254,0.0534,-0.0322,0.0199,-0.0378,0.0098,-0.0829,-0.0293,-0.0277,-0.0354,-0.036,-0.0174,-0.0614,-0.0413,-0.0004,-0.009,-0.0023,0.0101,0.0371,0.034,0.0398,0.0055,0.0462,0.0211,-0.0296,0.0381,-0.1554,0.036,0.0097,0.0607,-0.0401,-0.0432,-0.0016,0.0411,-0.0501,0.0572,0.0083,0.0494,0.0073,-0.0175,0.0098,0.0379,-0.0272,0.0108,0.0481,-0.005,-0.0474,-0.0086,-0.0369,-0.0668,0.0384,-0.0722,0.0458,-0.0205,-0.0294,-0.0092,-0.0451,0.037,-0.0684,-0.0274,0.0146,0.0524,0.0131,0.1746,-0.0358,0.0509,0.0062,0.0035,0.0445,-0.076,-0.0417,-0.0294,-0.0267,0.0115,0.0033,-0.0282,-0.0167,-0.0387,-0.0027,-0.0296,0.0416,0.0305,-0.0159,0.0277,0.0189,-0.0041,0.0907,-0.0605,0.0174,-0.0687,0.0175,0.1363,0.0391,0.034,0.0609,-0.0279,-0.0791,-0.0448,0.0066,-0.0134,0.0373,0.0175,0.0465,0.0069,-0.0562,-0.0547,0.0277,-0.0579,-0.046,0.142,-0.0588,0.0391,-0.0429,-0.0676,-0.0427,0.0394,-0.0164,-0.0186,0.0061,0.037,-0.018,0.001,-0.0408,0.0727,-0.0396,-0.0463,0.0056,0.1003,0.0307,-0.092,-0.062,0.0072,0.0006,0.046,0.0283,0.0328,-0.0577,0.0037,0.1071,0.0061,-0.0674,0.0259,0.0414,0.0223,0.0394,-0.0445,-0.0204,0.0548,0.0573,-0.0712,0.0053,-0.0293,0.0296,0.0275,-0.057,0.0081,-0.0564,-0.0666,-0.0107,-0.0256,-0.0127,0.0058,0.022,-0.039,-0.0009,0.0303,-0.0259,-0.0114,0.0151,0.033,-0.0073,-0.0147,0.027,0.0507,0.0157,-0.0064,0.0822,-0.0006,-0.0169,0.0325,0.0191,0.0649,-0.0315,0.0032,0.0416,-0.0169,-0.0935,-0.2432,-0.0225,0.0212,-0.0147,0.0336,-0.067,0.0659,-0.0043,0.0403,0.0985,0.0351,-0.0132,-0.0296,0.0165,-0.0266,0.0373,0.0715,0.0232,0.002,0.0336,-0.0183,0.0163,-0.0277,-0.0517,0.0555,-0.0044,0.1941,0.0197,0.0408,-0.0402,0.0216,-0.0027,-0.0244,-0.0718,0.0568,0.042,0.0627,0.0289,-0.0314,-0.0248,0.0003,0.0171,0.0158,-0.0885,-0.067,-0.0327,0.0023,0.0308,-0.0583,0.0212,0.0453,-0.0202,0.0965,-0.0314,-0.0106,-0.0363,-0.0696,0.0363,-0.0339,0.0144,0.0672,-0.0764,0.0217,-0.0509,0.0524,0.0069,-0.0185,-0.0616,0.0318,-0.0268,-0.0003,0.049,0.0173,-0.0025,0.0473,-0.0066,-0.0092,-0.0272,-0.0885,-0.0182,0.0685,-0.0386,-0.0025,0.0041,0.0321,0.0024,0.1092,0.0009,-0.0176,-0.026,-0.0135,0.0277,-0.0211,0.0108,0.0483,0.0324,-0.3052,0.0196,0.0228,-0.0059,0.0008,-0.0109,0.0378,0.0053,-0.0385,0.0031,-0.0411,0.0443,0.0446,0.0067,0.0061,0.0638,0.0499,-0.0317,0.0095,-0.0922,0.0475,0.0245,0.2112,-0.0543,0.0281,0.0218,-0.0305,-0.013,0.0243,-0.0337,0.0467,0.0171,0.0524,-0.0375,0.0653,0.0999,-0.0399,0.0806,0.027,-0.0235,0.0269,-0.0024,-0.0279,-0.0608,0.1096,-0.0564,-0.0404,-0.0474,-0.0172,0.0478,-0.0463,-0.0216,0.0355,0.0107,-0.0058,0.0192,-0.0516,-0.0194,-0.0121,-0.0388,0.0304,-0.0467,-0.0452,-0.0128,0.0169]}
{"key":"[Deep Multimodal Image-Repurposing Detection] Nefarious actors on social media and other platforms often spread rumors and falsehoods through images whose metadata (e.g., captions) have been modified to provide visual substantiation of the rumor/falsehood. This type of modification is referred to as image repurposing, in which often an unmanipulated image is published along with incorrect or manipulated metadata to serve the actor's ulterior motives. We present the Multimodal Entity Image Repurposing (MEIR) dataset, a substantially challenging dataset over that which has been previously available to support research into image repurposing detection. The new dataset includes location, person, and organization manipulations on real-world data sourced from Flickr. We also present a novel, end-to-end, deep multimodal learning model for assessing the integrity of an image by combining information extracted from the image with related information from a knowledge base. The proposed method is compared against state-of-the-art techniques on existing datasets as well as MEIR, where it outperforms existing methods across the board, with AUC improvement up to 0.23.","layer":2,"vector":[-0.0008,-0.0445,0.0342,-0.0087,0.0447,0.0289,0.0532,-0.013,-0.0226,0.0059,0.0488,-0.0542,0.0192,0.0292,0.0283,0.0193,0.0403,0.0332,-0.055,-0.0325,0.0318,-0.0428,0.0287,-0.0214,0.0536,-0.0131,-0.0268,-0.0487,-0.0584,-0.2156,0.0281,-0.0439,0.001,-0.0006,0.0132,-0.0128,-0.0551,0.042,-0.0432,0.0024,-0.0301,-0.0081,-0.022,-0.0808,-0.0528,-0.0568,-0.0115,-0.0342,-0.0311,0.0011,0.0314,-0.0578,0.02,0.065,0.0119,0.0191,0.0847,0.0726,0.1136,0.0482,0.0447,0.0888,-0.1212,0.0393,0.0415,0.0184,-0.0154,-0.0193,0.0077,0.0209,-0.0173,0.0575,-0.0198,0.0344,0.0011,0.0176,0.0014,0.0143,-0.0472,0.013,-0.005,-0.0013,-0.0047,-0.005,0.0358,-0.0696,-0.0151,-0.0579,0.0764,-0.0177,-0.0384,0.0251,0.0205,0.0289,-0.0613,-0.0078,-0.0444,0.0574,-0.0019,0.2013,-0.0604,0.0305,0.0619,-0.0272,0.0714,-0.0394,-0.026,-0.0018,-0.0477,-0.003,-0.012,-0.0168,0.0419,-0.0417,0.079,0.0169,0.0667,0.0462,-0.0303,-0.0224,-0.0179,0.0027,0.0365,0.0004,-0.0116,-0.059,0.0367,0.1129,0.0467,0.0228,0.0275,0.0234,-0.0419,-0.0052,0.0155,0.0487,-0.0018,-0.0096,0.0336,-0.0212,-0.031,-0.0858,0.0054,-0.0469,-0.0802,0.1109,-0.0661,0.0628,-0.0467,-0.0097,-0.018,0.0412,-0.0822,-0.0064,0.0585,-0.0349,0.0288,0.042,-0.0282,0.0483,0.0594,-0.0589,-0.0458,0.0968,0.0049,-0.1063,-0.0131,-0.001,0.0281,0.0043,0.0455,-0.0011,-0.0071,0.0058,0.0585,0.0223,-0.0935,-0.0004,0.0069,0.0284,0.0289,-0.07,-0.0623,0.0812,0.032,-0.053,-0.0095,-0.0052,0.0475,0.0494,-0.0143,0.0402,-0.0295,-0.0111,0.0055,-0.0242,-0.0377,-0.0044,0.0179,-0.0324,0.0075,-0.0109,-0.0409,0.017,-0.0257,0.0089,-0.0133,0.0203,0.043,0.0292,0.0005,-0.0125,0.0209,-0.0445,0.0068,-0.0196,0.0602,0.0256,0.0096,0.0269,0.0303,-0.038,-0.0382,-0.2508,-0.0105,0.0087,0.0006,0.0051,-0.0657,0.0384,-0.0186,0.072,0.0668,0.0345,-0.0911,-0.0014,0.043,0.0185,0.0451,-0.0017,0.0488,-0.0478,-0.02,-0.039,0.0235,-0.0286,-0.0748,0.0947,-0.002,0.2127,0.0837,-0.0236,-0.0241,0.0562,0.0509,-0.0323,-0.1337,0.0558,-0.0119,0.0609,0.0005,-0.0537,0.006,-0.0458,0.0355,-0.0229,-0.0948,0.0106,-0.0284,-0.057,0.0092,-0.043,0.0596,0.0223,-0.0405,-0.0147,0.035,-0.014,-0.0308,-0.0965,0.0124,-0.0398,0.0122,0.0112,-0.0426,0.0094,-0.0953,0.0901,-0.0379,-0.0384,-0.0334,0.0355,-0.035,-0.0345,0.0871,0.0184,-0.0002,0.0107,0.0168,0.0132,-0.0668,-0.0673,-0.0309,0.0612,0.03,0.0139,0.0105,0.0511,-0.0143,0.0538,-0.0366,0.0416,-0.0128,0.044,-0.0005,-0.071,-0.0351,0.028,-0.0011,-0.3166,-0.0003,0.0081,0.0575,0.0019,0.0558,0.0417,0.0509,-0.0067,-0.0247,-0.0192,0.0166,0.0524,-0.0084,-0.0361,0.0263,0.0316,-0.0503,0.0304,-0.0491,-0.0052,0.0107,0.2184,-0.0138,-0.0126,0.013,0.0025,-0.0213,0.0181,-0.0121,0.0151,-0.0122,0.0554,-0.0133,0.0406,0.041,-0.0647,0.0306,-0.0071,-0.0228,-0.0319,0.0093,-0.0163,-0.041,0.0867,-0.0063,0.0257,0.013,0.0302,0.0519,-0.0418,-0.0261,-0.0409,-0.0058,0.0352,0.0199,-0.0452,-0.0105,-0.0367,-0.0048,0.0062,-0.0116,-0.061,0.0154,-0.0278]}
{"key":"[ReSSL: Relational Self-Supervised Learning with Weak Augmentation] Self-supervised Learning (SSL) including the mainstream contrastive learning has achieved great success in learning visual representations without data annotations. However, most of methods mainly focus on the instance level information (\\ie, the different augmented images of the same instance should have the same feature or cluster into the same class), but there is a lack of attention on the relationships between different instances. In this paper, we introduced a novel SSL paradigm, which we term as relational self-supervised learning (ReSSL) framework that learns representations by modeling the relationship between different instances. Specifically, our proposed method employs sharpened distribution of pairwise similarities among different instances as \\textit{relation} metric, which is thus utilized to match the feature embeddings of different augmentations. Moreover, to boost the performance, we argue that weak augmentations matter to represent a more reliable relation, and leverage momentum strategy for practical efficiency. Experimental results show that our proposed ReSSL significantly outperforms the previous state-of-the-art algorithms in terms of both performance and training efficiency. Code is available at \\url{https://github.com/KyleZheng1997/ReSSL}.","layer":4,"vector":[-0.0354,-0.0361,-0.0391,-0.0179,0.0248,0.0373,0.0221,0.0285,-0.0141,-0.0423,0.0364,-0.082,0.0878,0.0888,0.0631,0.0241,0.0289,0.102,-0.0872,-0.0475,0.0109,-0.0666,-0.0315,-0.0512,0.0366,0.0267,-0.0186,-0.0383,-0.0027,-0.2329,0.0063,-0.0263,0.0032,0.0202,0.0006,-0.0636,-0.0075,0.0487,-0.0424,-0.0234,0.0252,-0.0143,0.0031,-0.0546,-0.0243,-0.0843,-0.0008,-0.0591,-0.011,-0.0215,0.0382,-0.0176,0.0061,0.0172,-0.0043,0.0337,0.0555,0.0326,0.0495,0.0198,0.0461,0.0671,-0.1475,0.0449,0.0172,0.0417,-0.0911,0.011,0.0351,0.0215,-0.0014,0.0374,0.0044,0.003,0.0077,0.0044,0.0187,-0.0109,0.0108,-0.0065,0.0031,-0.0138,-0.0643,-0.0213,-0.0051,-0.0229,-0.0002,-0.0883,0.0616,0.0499,-0.0841,-0.0346,-0.0448,0.0321,-0.0439,-0.02,0.0283,0.0125,-0.0468,0.2183,-0.0356,0.0364,0.0747,0.014,0.0394,-0.0507,-0.0434,-0.0313,-0.0469,0.0077,0.0006,-0.0132,-0.0433,-0.059,0.071,-0.0204,0.101,0.0383,-0.0106,-0.0008,0.0131,0.0072,0.0198,0.0066,0.0034,-0.0694,0.003,0.1552,0.0339,0.0031,0.0324,0.0053,-0.0204,-0.0209,-0.0264,0.0403,0.0122,-0.0209,-0.0014,-0.0051,-0.0279,-0.0577,0.0208,-0.0583,-0.0589,0.1467,-0.0298,0.061,-0.0492,-0.0419,0.001,-0.0064,-0.0311,-0.0265,-0.0046,0.0304,0.0268,0.017,-0.0375,0.004,-0.0159,-0.1072,-0.011,0.0707,0.045,-0.1046,-0.0281,0.0314,0.0342,-0.0271,0.0376,0.0502,-0.0424,0.0476,0.0822,0.0135,-0.089,-0.013,-0.0034,-0.0062,0.0404,-0.037,-0.0415,0.0778,0.0382,-0.054,0.0054,-0.0562,0.0016,0.0323,0.0267,0.048,-0.0065,-0.0039,-0.0035,0.0064,-0.0182,-0.0278,-0.0043,-0.0004,0.0026,0.004,-0.0273,0.0576,-0.0026,0.0007,-0.013,-0.0155,0.0347,0.0005,-0.0362,-0.014,0.0317,-0.0388,-0.0169,-0.0339,-0.0393,0.0721,-0.0157,0.018,0.0258,-0.055,-0.0261,-0.2095,-0.0088,0.0059,-0.0512,0.0532,-0.0801,0.0115,0.0283,0.0529,0.0792,0.0597,-0.0158,-0.0286,0.0186,-0.0228,0.0394,0.0687,0.0438,0.0026,-0.0408,-0.0312,0.0573,0.0034,-0.0608,0.0602,0.0077,0.2217,0.0557,0.0036,-0.0118,-0.0075,0.0215,-0.0792,-0.1034,0.0155,0.0038,0.054,-0.0351,-0.0198,-0.0625,-0.031,0.0121,0.0413,-0.0797,0.0143,-0.0259,-0.0187,0.0359,-0.0546,0.0507,0.0378,-0.038,0.0162,0.0178,-0.0313,-0.0007,-0.0801,0.0283,-0.0558,0.0026,0.004,-0.0738,0.0137,-0.0705,0.0905,-0.0096,-0.0377,-0.009,0.0246,-0.0019,-0.0403,0.096,-0.0277,-0.0129,0.0257,0.0195,0.0333,0.0036,-0.0337,-0.0229,0.0573,-0.0026,0.0403,0.005,0.0334,0.0439,0.0609,0.0113,0.0257,-0.0197,-0.0092,-0.0154,-0.0665,-0.0041,0.0844,0.0223,-0.2959,0.0476,0.0225,0.0672,-0.0347,0.0631,0.0502,0.0142,-0.0447,-0.073,0.0295,0.0046,0.0296,-0.0397,-0.0007,-0.0317,0.0753,-0.0491,0.0765,-0.0418,-0.0022,0.0432,0.1902,-0.0073,0.0062,0.0287,-0.0459,0.0028,0.0261,-0.0109,0.0001,0.0363,0.0993,-0.0157,0.0225,0.0845,-0.0409,0.0151,0.0065,0.0015,-0.0043,-0.0084,-0.0509,-0.0502,0.0993,0.0095,0.0099,-0.0173,-0.0265,0.0369,-0.0161,0.0234,0.0028,-0.0129,0.0105,0.0036,-0.0499,0.0005,-0.017,-0.0382,0.0378,-0.0129,-0.0307,0.0154,0.0198]}
{"key":"[Multiset-Equivariant Set Prediction with Approximate Implicit Differentiation] Most set prediction models in deep learning use set-equivariant operations, but they actually operate on multisets. We show that set-equivariant functions cannot represent certain functions on multisets, so we introduce the more appropriate notion of multiset-equivariance. We identify that the existing Deep Set Prediction Network (DSPN) can be multiset-equivariant without being hindered by set-equivariance and improve it with approximate implicit differentiation, allowing for better optimization while being faster and saving memory. In a range of toy experiments, we show that the perspective of multiset-equivariance is beneficial and that our changes to DSPN achieve better results in most cases. On CLEVR object property prediction, we substantially improve over the state-of-the-art Slot Attention from 8% to 77% in one of the strictest evaluation metrics because of the benefits made possible by implicit differentiation.","layer":0,"vector":[-0.0336,-0.0255,0.0598,-0.0277,-0.0021,0.0339,0.0522,0.0162,0.0219,0.0256,-0.0428,-0.0327,0.0001,0.0735,0.0198,-0.0206,0.0138,0.0298,-0.0695,0.026,0.0595,-0.0193,0.0063,-0.0675,0.0427,0.004,-0.0074,-0.0366,-0.0345,-0.2479,0.0233,-0.0393,0.0223,0.0087,0.0174,-0.0379,-0.045,0.0403,-0.0695,0.069,0.0257,0.0209,-0.0268,-0.0652,-0.0318,-0.0159,0.0136,-0.0331,-0.0451,-0.0379,0.0444,-0.0879,0.051,0.0354,0.037,0.0429,0.033,0.0239,0.0347,0.0252,0.0482,-0.0007,-0.1328,0.0271,0.0279,0.0424,-0.0151,-0.0339,0.0429,0.0485,0.0101,0.0651,0.039,0.045,-0.0033,-0.0055,0.0016,-0.0267,-0.0459,-0.0017,0.0511,0.0042,-0.0731,-0.0481,-0.0173,-0.0261,0.0072,-0.0311,0.0107,0.0177,-0.0788,-0.0492,-0.0053,0.0016,-0.048,0.0325,0.0177,0.017,-0.0549,0.212,-0.0531,0.0455,0.0096,-0.0743,0.027,-0.0383,0.0161,-0.0303,-0.0366,-0.0182,-0.0141,-0.0486,0.007,-0.0302,-0.0146,0.0055,0.0764,0.0816,-0.0037,-0.0321,0.014,-0.0055,0.031,0.0009,0.0154,-0.0733,0.0453,0.1476,0.0119,-0.0087,-0.0028,0.0026,-0.0723,-0.022,0.0336,0.026,0.0248,0.0487,0.0388,0.0149,-0.0468,0.0022,0.0253,-0.0629,-0.0454,0.1223,-0.04,0.0202,-0.0799,-0.021,-0.0202,0.0463,-0.0134,-0.0403,0.0076,0.0387,0.014,0.0269,-0.0629,0.02,-0.0112,-0.0252,-0.0372,0.0585,-0.0648,-0.0602,-0.0639,0.0024,0.0216,-0.0506,0.0462,0.0162,-0.0768,-0.0028,0.1325,0.0407,-0.0888,0.0227,-0.021,0.0359,0.0067,-0.0312,-0.0549,0.0854,0.0205,-0.0014,0.0175,-0.0433,0.0194,0.0575,-0.0015,0.0503,-0.0711,0.0292,-0.019,-0.0283,0.0015,0.0213,0.0259,-0.0332,-0.0144,0.0154,0.0046,0.0105,-0.0492,0.0544,-0.0049,-0.0045,0.0187,0.0238,-0.0648,-0.0141,0.0502,-0.0478,-0.0277,0.0125,0.013,0.0329,-0.0057,0.017,0.0471,-0.0887,-0.028,-0.2167,0.0072,-0.0031,-0.0338,0.0382,-0.0606,0.0331,0.0163,0.0666,0.092,0.0464,-0.0342,-0.013,0.008,0.0004,0.0644,0.0099,0.0436,-0.0504,0.0044,0.0058,0.006,-0.0032,-0.0981,0.0582,0.0278,0.1892,-0.0002,0.0392,-0.0493,0.0077,0.0695,-0.0174,-0.1005,0.0469,0.0368,0.0593,0.0173,-0.0104,-0.0278,-0.0093,0.0269,-0.0124,-0.12,-0.0401,-0.0394,-0.0484,0.073,-0.0519,0.038,0.0692,-0.0583,0.0193,0.0025,0.0003,-0.0371,-0.1192,0.0407,-0.0154,0.0507,0.0304,-0.0528,0.0087,-0.067,0.056,-0.0019,-0.006,-0.0654,-0.0247,-0.0266,-0.0443,0.0443,-0.0057,0.0212,0.0088,-0.026,0.0464,-0.001,-0.0477,0.0188,0.0882,-0.0025,-0.0012,0.0045,0.0503,0.0232,0.0756,0.0054,0.0374,-0.017,-0.0104,-0.0148,-0.0185,0.0061,0.0659,0.0191,-0.2962,0.0641,0.0158,0.0182,0.0183,0.004,0.0174,0.0083,-0.0287,-0.0219,0.0007,0.0285,0.0574,-0.0115,-0.0255,0.0061,0.0851,-0.0526,0.0368,-0.0383,0.034,0.0332,0.2289,-0.0142,0.0296,-0.0117,-0.0084,-0.02,0.0356,-0.0078,0.0156,0.0057,0.0594,-0.0662,0.0395,0.1121,-0.0184,0.0217,0.0412,-0.0044,-0.0163,-0.0182,-0.047,-0.0296,0.1097,-0.0024,-0.0224,0.0105,0.012,0.0087,-0.0496,0.0204,-0.0175,-0.0304,0.0108,0.0063,-0.05,-0.0282,-0.0411,-0.0368,0.0368,-0.0739,-0.0431,-0.0094,-0.0387]}
{"key":"[Harnessing spectral representations for subgraph alignment] With the rise and advent of graph learning techniques, graph data has become ubiquitous. However, while several efforts are being devoted to the design of new convolutional architectures, pooling or positional encoding schemes, less effort is being spent on problems involving maps between (possibly very large) graphs, such as signal transfer, graph isomorphism and subgraph correspondence. With this paper, we anticipate the need for a convenient framework to deal with such problems, and focus in particular on the challenging subgraph alignment scenario. We claim that, first and foremost, the representation of a map plays a central role on how these problems should be modeled. Taking the hint from recent work in geometry processing, we propose the adoption of a spectral representation for maps that is compact, easy to compute, robust to topological changes, easy to plug into existing pipelines, and is especially effective for subgraph alignment problems. We report for the first time a surprising phenomenon where the partiality arising in the subgraph alignment task is manifested as a special structure of the map coefficients, even in the absence of exact subgraph isomorphism, and which is consistently observed over different families of graphs up to several thousand nodes.","layer":2,"vector":[-0.0085,-0.0656,0.0092,-0.0434,0.0146,0.0181,0.0031,0.0315,0.0178,0.0167,0.0257,-0.0998,0.0505,0.0595,0.0257,0.0227,-0.0331,0.0877,-0.0218,0.0197,0.0016,-0.0358,-0.0205,-0.0422,0.0424,0.0231,-0.0305,-0.0277,-0.057,-0.3031,0.0178,-0.0146,0.0743,-0.0512,-0.0072,-0.0621,0.0048,0.0584,-0.0412,0.0249,0.0275,-0.0169,-0.0448,-0.0125,-0.0425,-0.0361,-0.0063,-0.0027,-0.0144,-0.0696,0.0349,-0.0397,0.0118,0.0531,0.0279,0.064,0.0777,0.0443,0.0543,0.0426,0.0231,0.0269,-0.1311,0.0738,0.0371,0.0457,-0.0366,-0.0256,0.0143,0.0557,0.006,0.0177,0.0385,-0.0071,0.0523,0.0277,-0.0024,-0.0234,-0.0551,-0.0387,-0.0062,-0.0268,-0.0255,-0.0003,0.0166,-0.0225,-0.0045,-0.0516,0.014,0.0155,-0.0469,-0.0372,-0.0182,0.0581,-0.0742,-0.046,0.042,0.0238,0.0021,0.1757,-0.0419,0.0195,0.0525,0.0059,-0.0105,-0.046,-0.0559,-0.0321,-0.0272,0.0026,-0.0206,-0.0512,-0.0121,-0.0569,0.0108,-0.0275,0.0682,0.0635,-0.0318,-0.0312,-0.0351,0.0613,0.0305,-0.0326,0.0389,-0.033,-0.0176,0.1149,0.0808,0.0515,0.0393,0.0297,0.0026,-0.0047,-0.0346,0.0158,0.0267,-0.018,0.028,0.0168,-0.0268,-0.0466,0.0069,-0.0332,-0.0409,0.1161,-0.0814,-0.0041,-0.0226,-0.0061,-0.0473,0.0203,-0.0172,0.0059,-0.0149,0.0368,0.0029,0.0326,-0.0559,0.0142,-0.0408,-0.0255,-0.0662,0.1291,0.0374,-0.1205,-0.0239,-0.0515,-0.0138,-0.0199,0.0338,0.0549,0.0389,0.0361,0.0734,0.0561,-0.0767,-0.0538,0.0242,-0.0097,0.0364,-0.0469,-0.0336,0.012,0.0349,-0.0168,-0.0055,0.0131,0.0106,0.0661,-0.0037,0.0482,-0.032,-0.0116,-0.0604,-0.0075,0.0007,-0.0205,-0.0238,-0.0281,0.0068,0.0063,-0.0222,0.0265,-0.0335,0.009,0.0001,-0.0005,-0.0296,-0.0019,-0.0148,0.0031,0.0342,-0.0175,-0.0383,-0.0215,0.031,0.0388,0.009,0.0608,-0.0047,-0.0558,-0.047,-0.2192,-0.0025,-0.03,0.0152,0.0221,-0.0426,-0.0012,-0.0,0.0697,0.049,0.0577,-0.0237,-0.0219,0.0088,-0.001,0.0606,0.0303,0.0708,0.0007,-0.0191,-0.0191,0.0318,-0.0395,-0.0723,0.0561,0.0094,0.237,0.0177,0.0214,-0.0298,-0.0197,-0.0048,-0.05,-0.0699,0.0342,0.0726,0.0653,0.0136,-0.0176,-0.0173,-0.0601,0.0037,0.0008,-0.0931,-0.0232,0.0213,-0.0359,-0.0054,-0.0348,0.0136,0.0368,-0.0297,0.0568,0.0094,-0.0136,-0.0346,-0.087,0.019,-0.0144,-0.017,0.0175,-0.067,0.0175,-0.0119,0.0623,0.0338,-0.019,-0.0299,0.0469,0.0072,-0.0372,0.0518,0.0241,-0.0339,0.0582,0.032,0.0627,-0.0072,-0.0016,0.0158,0.0556,-0.0674,0.0342,0.003,0.0669,0.048,0.0704,-0.0166,0.0192,-0.0456,0.0362,0.0355,-0.0268,-0.0349,0.029,-0.0382,-0.2799,0.0249,0.0363,0.025,-0.0649,-0.0012,0.044,0.032,-0.0579,-0.0296,0.0516,0.0593,0.0255,-0.0153,-0.0064,0.0417,0.0682,-0.0523,0.0646,-0.0587,0.0138,0.0037,0.2522,-0.0304,0.0409,0.0334,-0.0419,-0.0039,0.0269,-0.0095,-0.0419,0.0178,0.0853,-0.0202,0.0531,0.0837,-0.0545,0.013,0.0282,0.0119,0.008,-0.0283,-0.002,-0.0527,0.0939,0.0285,-0.0344,-0.0196,0.0286,0.0341,-0.0043,-0.0065,-0.0212,-0.004,0.0064,0.01,-0.0667,-0.0753,-0.0383,-0.0285,0.001,-0.084,-0.0162,-0.0448,-0.03]}
{"key":"[Imbalanced Learning-based Automatic SAR Images Change Detection by Morphologically Supervised PCA-Net] Change detection is a quite challenging task due to the imbalance between unchanged and changed class. In addition, the traditional difference map generated by log-ratio is subject to the speckle, which will reduce the accuracy. In this letter, an imbalanced learning-based change detection is proposed based on PCA network (PCA-Net), where a supervised PCA-Net is designed to obtain the robust features directly from given multitemporal SAR images instead of a difference map. Furthermore, to tackle with the imbalance between changed and unchanged classes, we propose a morphologically supervised learning method, where the knowledge in the pixels near the boundary between two classes are exploited to guide network training. Finally, our proposed PCA-Net can be trained by the datasets with available reference maps and applied to a new dataset, which is quite practical in change detection projects. Our proposed method is verified on five sets of multiple temporal SAR images. It is demonstrated from the experiment results that with the knowledge in training samples from the boundary, the learned features benefit for change detection and make the proposed method outperforms than supervised methods trained by randomly drawing samples.","layer":0,"vector":[-0.0359,-0.0313,0.0263,-0.0069,0.0686,0.0232,0.019,-0.0194,-0.0025,-0.0285,0.0369,-0.0668,-0.006,0.0727,0.0179,0.0621,-0.0224,0.0043,-0.0053,0.0038,-0.0143,-0.0161,-0.0205,0.0048,0.055,0.0232,0.004,-0.0136,-0.0836,-0.2109,0.0283,-0.0334,0.0173,0.0002,-0.0106,-0.0388,-0.0711,0.0468,-0.006,0.0375,0.0193,0.0555,0.007,-0.0597,-0.0327,-0.0818,-0.0019,-0.053,0.0204,-0.0533,0.0272,-0.101,0.0285,0.0419,0.0234,0.0087,0.024,0.0727,0.0978,0.0635,0.0708,0.051,-0.2232,0.0287,0.0628,0.027,-0.0196,-0.0554,0.0385,0.0271,-0.0238,0.0969,0.019,0.0092,0.008,-0.0076,-0.0179,0.0072,-0.0323,0.0184,0.0388,0.0183,-0.0063,-0.0426,-0.0179,-0.0622,-0.0034,-0.0781,0.0594,-0.0151,-0.0369,-0.0399,0.0029,-0.0161,-0.0419,-0.0313,0.0203,0.0398,-0.0204,0.1627,-0.0605,-0.0093,0.0478,-0.0279,0.035,-0.0279,0.0085,-0.028,-0.0367,0.0108,0.0273,-0.0144,0.0553,-0.0121,-0.001,-0.0402,0.0549,0.0266,0.0006,0.0062,-0.0345,-0.0307,0.0309,-0.0021,0.0375,-0.0359,0.0566,0.122,0.045,0.0342,0.0496,-0.0012,-0.067,-0.0604,0.032,0.02,-0.0023,0.0223,0.0086,0.0079,-0.082,-0.0675,0.0593,-0.1014,-0.0641,0.1368,-0.0347,0.0491,-0.049,-0.0157,-0.0336,-0.0105,-0.0292,-0.0082,0.0125,0.0436,0.0907,0.0286,-0.0319,-0.0114,0.0095,-0.0626,-0.055,0.0726,0.0459,-0.0707,-0.0338,-0.0143,0.0435,-0.0014,0.0316,0.0414,-0.0247,-0.0064,0.1056,0.0047,-0.013,-0.0543,0.0349,-0.0031,0.0723,-0.0257,-0.0588,0.0974,0.0599,-0.0295,-0.0027,-0.0276,0.045,0.0182,-0.0181,-0.0001,0.0008,-0.0233,-0.0251,0.0084,-0.0063,-0.0246,-0.0192,-0.0573,0.0599,0.0147,-0.0253,-0.0031,0.0018,0.0432,-0.0054,-0.0046,-0.0049,0.0473,0.0064,0.0278,0.0477,-0.0102,-0.0268,-0.0271,0.0177,0.084,-0.0287,0.0612,0.0485,-0.0253,-0.0604,-0.221,-0.015,0.0176,-0.0395,0.0143,-0.0527,0.0107,-0.0001,0.0466,0.0547,0.0942,-0.01,-0.0038,0.006,-0.0121,0.0502,0.0313,0.0301,-0.0251,-0.0549,-0.0473,0.0381,0.0012,-0.0694,0.0812,-0.0076,0.1931,0.0209,0.0525,-0.0083,-0.0043,0.0231,-0.0437,-0.0738,0.0793,0.05,0.0373,-0.0329,-0.0962,-0.0579,0.014,0.0287,-0.0151,-0.0539,-0.026,-0.0455,-0.0348,0.0401,-0.044,0.0184,-0.0112,-0.0251,-0.0118,-0.0235,0.0064,-0.045,-0.0602,0.0481,-0.0307,0.0121,0.0368,-0.0189,-0.0138,-0.071,0.0842,0.0013,-0.069,-0.0843,0.0053,0.0098,-0.0352,0.1355,0.0197,-0.0504,0.0507,-0.0102,0.0257,0.0132,-0.0703,-0.0628,0.0407,-0.0615,-0.0128,0.0122,-0.0036,0.0304,0.0499,0.0022,0.0124,-0.0121,0.0294,0.0351,-0.0328,-0.0105,0.003,-0.0157,-0.2804,0.0431,0.0441,0.0349,-0.0094,0.0581,0.0099,0.0215,-0.0558,-0.0032,-0.0242,-0.0186,0.0009,-0.0153,-0.0061,0.0718,0.0336,-0.0472,0.0795,-0.0518,0.0095,0.0118,0.2077,-0.0377,0.0432,-0.0078,0.0124,-0.0291,0.0265,-0.0012,0.0454,0.0108,0.0593,-0.0151,0.0499,0.0945,-0.0491,0.0554,-0.0473,-0.0083,-0.0098,0.0143,-0.0393,-0.0452,0.097,0.0033,-0.0316,-0.0241,0.0143,-0.0078,-0.0377,0.0268,0.0076,-0.0269,0.0073,0.0356,-0.0591,-0.0528,-0.0453,-0.0308,0.0608,-0.064,-0.0468,-0.0283,-0.0113]}
{"key":"[Three-class Overlapped Speech Detection using a Convolutional Recurrent Neural Network] In this work, we propose an overlapped speech detection system trained as a three-class classifier. Unlike conventional systems that perform binary classification as to whether or not a frame contains overlapped speech, the proposed approach classifies into three classes: non-speech, single speaker speech, and overlapped speech. By training a network with the more detailed label definition, the model can learn a better notion on deciding the number of speakers included in a given frame. A convolutional recurrent neural network architecture is explored to benefit from both convolutional layer's capability to model local patterns and recurrent layer's ability to model sequential information. The proposed overlapped speech detection model establishes a state-of-the-art performance with a precision of 0.6648 and a recall of 0.3222 on the DIHARD II evaluation set, showing a 20% increase in recall along with higher precision. In addition, we also introduce a simple approach to utilize the proposed overlapped speech detection model for speaker diarization which ranked third place in the Track 1 of the DIHARD III challenge.","layer":0,"vector":[-0.0418,-0.0071,0.0005,-0.0099,0.01,0.023,0.0293,-0.0149,0.0375,-0.0522,0.0231,-0.0297,0.0355,0.0617,0.0555,-0.006,0.0559,0.0507,-0.0195,-0.0225,0.0456,-0.0219,0.0034,0.0144,0.0266,0.0113,-0.0509,-0.0303,-0.0666,-0.2095,0.0214,-0.0454,0.0593,-0.0405,-0.0128,-0.0163,-0.0182,0.079,-0.0524,0.0201,-0.0151,0.0131,-0.0333,-0.0843,-0.0335,-0.0483,-0.0494,-0.0258,-0.0092,-0.0598,0.0448,-0.0481,0.0359,0.0111,-0.006,0.0511,0.0542,0.088,0.0308,0.0527,0.0102,0.0425,-0.2084,0.0307,0.0428,0.0305,0.0028,-0.017,0.0365,0.0555,-0.0152,0.0329,0.0333,0.051,-0.0241,0.0037,0.0395,-0.047,0.0234,0.0138,0.0151,-0.0067,0.007,-0.0486,-0.002,-0.0936,0.0241,-0.0255,-0.007,-0.0204,-0.085,-0.0018,-0.055,0.0288,-0.0668,-0.0524,0.0542,0.0042,-0.0322,0.2093,-0.0248,0.0553,0.0184,-0.0671,0.0166,-0.0075,-0.045,-0.0193,-0.0131,-0.001,0.0074,-0.0582,0.0388,-0.0295,0.0295,0.0191,0.0818,0.0338,-0.0217,-0.0016,0.0123,-0.0247,0.0257,-0.0573,0.0205,-0.0673,0.0791,0.1125,0.0525,0.0155,0.0688,-0.0025,-0.0342,-0.0107,0.0332,0.0483,0.0048,-0.0049,0.0408,-0.0266,-0.0573,-0.1017,0.0024,-0.0787,-0.0324,0.1045,-0.0835,0.0057,-0.021,-0.0606,-0.0136,-0.0007,-0.0007,-0.0429,0.0353,-0.0127,0.0707,0.0166,-0.0294,0.0255,0.0145,-0.0619,-0.0392,0.0884,0.0505,-0.1275,-0.0404,-0.0041,0.012,-0.0191,0.0585,0.0182,-0.0584,0.0081,0.0603,0.0432,-0.0687,0.0004,0.0226,0.0262,-0.0016,-0.088,-0.0779,0.0101,0.0329,-0.054,0.0059,-0.0522,0.0024,0.0559,-0.06,0.0095,-0.0354,0.0044,-0.0565,-0.0071,0.0217,-0.0412,0.0355,-0.0263,-0.0117,-0.0164,-0.0232,0.0054,0.0137,0.0352,-0.0269,0.0446,0.0236,0.0167,-0.0058,-0.0125,0.0605,-0.0335,-0.0145,-0.0476,0.0446,0.0406,0.0059,0.0461,0.0047,-0.0444,-0.0617,-0.2297,0.0207,0.0372,-0.0057,0.0361,-0.0676,0.0776,0.0164,0.0858,0.0314,0.0036,-0.0102,-0.0026,0.0437,0.0122,0.0657,0.0427,0.018,-0.0385,-0.0055,0.0124,0.026,-0.0124,-0.0884,0.0768,0.0059,0.2003,0.0288,0.0571,-0.002,0.0306,0.0281,-0.0446,-0.0677,0.055,0.0153,0.0749,-0.0073,-0.0482,-0.0219,-0.0596,0.034,-0.0184,-0.0924,-0.029,-0.0579,-0.0342,-0.0145,-0.0511,0.0447,0.0586,-0.0562,0.0485,0.0345,0.0392,0.0046,-0.1353,-0.0006,-0.0142,0.0189,0.0163,0.0066,0.0242,-0.0342,0.0478,0.0065,-0.0656,-0.0285,0.0423,0.0624,0.0106,0.0997,0.0312,-0.0199,0.072,0.0346,0.0406,-0.0877,-0.0434,-0.0177,0.0817,-0.0238,0.0303,-0.0139,-0.0052,0.0119,0.0551,0.036,0.0128,-0.0291,0.0385,0.0451,-0.018,0.0025,0.0003,-0.0029,-0.2805,0.036,-0.0164,0.027,0.015,0.018,0.0143,0.0229,-0.0283,-0.0018,-0.0145,0.044,0.0097,-0.0523,-0.0262,0.0705,0.1088,-0.0124,0.0387,-0.0464,0.0016,0.036,0.1587,-0.0076,0.043,-0.0388,-0.028,-0.0188,-0.0176,-0.0525,-0.0059,-0.0195,0.1021,-0.0684,-0.0316,0.0723,-0.0109,0.0612,0.0208,-0.0111,-0.0075,-0.0164,-0.0362,-0.0193,0.055,-0.0322,-0.0339,-0.0433,0.0207,0.0261,0.0102,-0.0333,-0.0057,0.0079,0.0522,0.0401,-0.0608,-0.0324,-0.0265,-0.0066,0.0434,-0.0787,-0.023,0.0134,-0.0059]}
{"key":"[The Effect of Model Size on Worst-Group Generalization] Overparameterization is shown to result in poor test accuracy on rare subgroups under a variety of settings where subgroup information is known. To gain a more complete picture, we consider the case where subgroup information is unknown. We investigate the effect of model size on worst-group generalization under empirical risk minimization (ERM) across a wide range of settings, varying: 1) architectures (ResNet, VGG, or BERT), 2) domains (vision or natural language processing), 3) model size (width or depth), and 4) initialization (with pre-trained or random weights). Our systematic evaluation reveals that increasing model size does not hurt, and may help, worst-group test performance under ERM across all setups. In particular, increasing pre-trained model size consistently improves performance on Waterbirds and MultiNLI. We advise practitioners to use larger pre-trained models when subgroup labels are unknown.","layer":1,"vector":[0.0159,-0.0355,0.0253,-0.0049,0.045,0.0414,0.0217,0.0272,-0.0039,-0.0215,0.0157,-0.0801,0.0577,0.0748,-0.0069,0.0128,0.0063,0.0242,-0.0412,0.0133,-0.0158,-0.0349,-0.0008,-0.0394,-0.001,0.0257,-0.0531,-0.027,-0.0288,-0.2835,0.0057,-0.0355,0.0589,-0.0412,-0.0036,0.0194,-0.02,0.0191,-0.0398,0.0255,0.0034,0.0342,-0.0383,-0.0377,-0.0187,-0.0315,-0.0516,-0.0462,-0.0343,-0.0307,0.0328,-0.0499,-0.0105,0.0346,0.0272,0.0246,0.0746,-0.0018,0.0371,0.0404,-0.0166,0.0403,-0.1569,0.0613,0.0119,0.0581,-0.0513,0.0048,-0.0098,0.0324,-0.0107,0.0362,0.0383,0.0548,0.0107,0.0368,-0.0289,-0.0158,-0.0076,0.0253,0.0183,-0.0236,-0.0313,-0.0412,0.0311,-0.0687,0.012,-0.0246,0.0465,-0.0104,0.0082,0.0148,-0.0225,-0.0031,-0.0383,-0.0044,0.0484,-0.0158,-0.0909,0.2299,-0.0063,0.016,0.0447,-0.0207,0.0469,-0.0463,-0.0559,-0.0435,-0.0005,-0.0096,-0.0375,0.0143,0.0194,-0.0361,0.0092,0.0014,0.0736,0.0093,0.0004,0.0105,-0.046,0.0097,0.0315,-0.0091,0.0438,-0.0518,0.0117,0.1386,0.057,0.0156,0.0481,-0.0359,-0.0582,-0.0348,0.0284,0.0307,0.0278,0.0356,0.0424,-0.0005,-0.0292,-0.0534,0.0285,-0.0516,-0.0534,0.1662,-0.061,0.0092,-0.0574,-0.0003,-0.0015,0.024,-0.0525,-0.0331,0.0144,-0.001,0.0187,0.0327,-0.0329,-0.0103,-0.0074,0.0003,-0.0251,0.0805,0.0296,-0.0328,-0.0506,-0.0093,0.011,-0.0159,0.0217,0.0277,-0.0338,0.0329,-0.0121,0.0252,-0.064,-0.0071,-0.0437,-0.0132,0.0358,-0.0268,-0.045,0.0232,0.0177,0.0131,-0.0283,-0.0497,0.0209,0.0221,-0.0064,0.0201,-0.026,-0.0386,-0.0356,-0.0017,-0.0134,-0.0157,0.0249,-0.0224,0.0183,-0.0003,0.0136,0.0196,-0.0199,0.0328,0.0504,-0.0417,0.0708,0.0411,-0.0402,-0.0417,0.0459,-0.0354,-0.0306,0.0272,0.027,0.0483,0.0117,0.0584,0.0435,-0.0218,-0.0573,-0.2336,-0.0393,0.0341,-0.0102,0.0582,-0.0694,0.0241,0.0235,0.0676,0.0759,0.0217,-0.0598,-0.0151,0.0388,-0.0216,0.0664,-0.0324,0.0025,-0.0225,0.0461,-0.01,0.0419,-0.0365,-0.0715,0.045,-0.0244,0.2046,0.0435,0.0471,-0.0056,0.0334,0.034,-0.0429,-0.0428,0.0833,0.0532,0.031,-0.0245,-0.027,-0.0435,-0.0153,-0.0069,0.046,-0.1254,-0.0729,-0.0275,-0.0442,0.051,-0.1025,0.0031,0.0409,-0.0384,0.0717,0.0145,-0.0051,-0.0283,-0.1166,0.0208,-0.058,0.0185,0.0566,-0.0247,0.0251,-0.0643,0.0406,-0.0308,-0.048,-0.0313,0.0264,-0.0044,-0.0475,0.0712,-0.0365,0.0027,0.0399,0.0285,0.027,-0.1026,-0.0354,-0.0507,0.0519,-0.0147,0.0113,0.0055,0.0529,0.0348,0.0585,0.0009,0.0768,-0.0188,-0.006,-0.0185,-0.0627,-0.0042,0.0564,-0.0023,-0.2581,0.0637,-0.0036,0.0248,-0.0371,0.0371,0.0029,-0.0166,-0.019,-0.0168,0.0345,0.0546,0.0736,-0.0103,0.0148,0.0694,0.0344,-0.0749,0.0153,-0.0573,0.0215,0.0347,0.247,-0.032,0.0049,0.0169,-0.0088,-0.0168,0.0076,-0.0222,0.0803,0.0202,0.0883,-0.0954,0.0644,0.0784,-0.0457,-0.0155,0.0284,-0.0033,0.0033,-0.0137,0.0085,-0.0341,0.1055,0.009,-0.0125,-0.0302,-0.0322,0.0399,-0.0387,-0.0533,-0.0451,-0.0027,0.0386,0.0209,0.0023,-0.031,-0.0185,-0.0413,0.0005,-0.0567,-0.0106,0.0186,0.0341]}
{"key":"[ABL: Alignment-Based Learning] This paper introduces a new type of grammar learning algorithm, inspired by string edit distance (Wagner and Fischer, 1974). The algorithm takes a corpus of flat sentences as input and returns a corpus of labelled, bracketed sentences. The method works on pairs of unstructured sentences that have one or more words in common. When two sentences are divided into parts that are the same in both sentences and parts that are different, this information is used to find parts that are interchangeable. These parts are taken as possible constituents of the same type. After this alignment learning step, the selection learning step selects the most probable constituents from all possible constituents. This method was used to bootstrap structure on the ATIS corpus (Marcus et al., 1993) and on the OVIS (Openbaar Vervoer Informatie Systeem (OVIS) stands for Public Transport Information System.) corpus (Bonnema et al., 1997). While the results are encouraging (we obtained up to 89.25 % non-crossing brackets precision), this paper will point out some of the shortcomings of our approach and will suggest possible solutions.","layer":0,"vector":[-0.0329,-0.0149,0.009,-0.0216,0.0088,-0.0209,-0.0003,0.0236,-0.0127,-0.058,-0.015,-0.0747,0.023,0.003,0.0314,0.0082,-0.0169,0.0168,-0.0516,0.0461,0.02,-0.0162,-0.0106,-0.053,0.0091,0.0818,-0.0167,-0.0433,-0.0504,-0.2064,0.0104,-0.0137,0.0575,-0.0245,-0.023,0.0169,-0.0318,0.0222,-0.0129,0.005,0.0324,-0.0153,-0.03,-0.0411,-0.0509,-0.0321,-0.0168,0.0272,-0.0222,-0.0337,-0.0047,-0.0501,0.061,-0.0026,0.0159,0.0395,0.0443,0.0615,0.0054,0.0366,0.0035,0.05,-0.2319,0.0915,0.0406,-0.0024,-0.0203,-0.0008,0.0057,0.0589,-0.0413,0.0322,0.0614,0.0565,0.0115,0.0234,-0.0241,-0.0315,-0.0122,0.0048,0.0269,-0.0266,-0.0454,0.0009,-0.0379,-0.0338,-0.0062,-0.0482,0.0418,0.0393,-0.0676,-0.0524,-0.0034,0.0314,-0.0848,-0.084,0.0214,0.0757,-0.0175,0.19,-0.0581,0.0618,0.0331,-0.0463,0.0359,-0.0115,-0.0085,-0.0068,-0.0224,-0.0253,-0.0316,0.0183,-0.0127,-0.0344,0.0274,0.0043,0.0865,0.0132,-0.0151,0.0217,-0.0373,0.0003,0.0077,0.0143,-0.0166,-0.0347,0.0288,0.0741,0.0562,0.0257,0.0786,-0.0176,-0.0543,-0.0453,0.0179,0.0301,0.0459,0.0029,0.0191,-0.0108,-0.0444,-0.128,-0.011,-0.0705,-0.0759,0.1374,-0.0428,0.0124,-0.0528,-0.0389,-0.0208,0.0066,0.0048,-0.074,-0.0084,0.0474,0.0794,0.0122,-0.0003,-0.0105,0.0046,-0.0611,-0.053,0.1032,0.0243,-0.1044,-0.046,0.015,0.0203,-0.0024,0.0478,0.0399,-0.0143,0.0378,0.0421,0.0483,-0.0383,-0.0025,0.0527,-0.0159,0.0153,-0.043,-0.0601,0.0536,0.0587,-0.0238,-0.0121,-0.0698,0.04,0.0023,-0.0161,0.0322,0.0277,-0.0195,-0.1032,-0.0057,0.0173,-0.0337,-0.0012,-0.0423,0.0127,0.038,-0.0646,-0.0483,-0.0396,-0.0089,0.0135,-0.0123,0.0314,0.0145,-0.015,0.0009,0.0484,0.0168,-0.0438,-0.0445,0.0073,0.0495,0.0306,0.0995,-0.0037,-0.0028,-0.0058,-0.2093,0.0092,0.0454,-0.0025,0.0516,-0.0522,0.0683,-0.0027,0.012,0.0411,0.0145,-0.039,0.0015,0.0461,-0.0083,0.0757,0.0494,0.0163,0.0157,0.0179,-0.0023,0.0401,-0.0218,-0.0811,0.0347,0.0299,0.2225,0.0239,0.0595,-0.0588,0.0692,-0.0036,0.0095,-0.0875,0.0881,0.0363,0.0442,-0.0013,-0.0245,-0.0248,0.0135,0.0033,-0.0143,-0.1107,-0.0384,-0.0512,-0.0428,-0.0286,-0.0504,0.0603,0.0269,-0.0363,0.026,-0.0035,-0.0486,-0.048,-0.0968,0.0471,0.0094,-0.0123,0.0333,-0.0096,0.0201,-0.0532,0.04,0.0212,-0.0324,-0.0291,0.0192,0.0176,-0.0458,0.0753,0.005,-0.016,0.0229,0.0224,0.0047,0.0054,-0.0057,-0.0015,0.0571,-0.0271,0.0469,-0.0024,0.0685,0.0489,0.1059,-0.0085,0.0319,0.0143,0.0617,-0.005,-0.0197,-0.0406,0.0459,-0.007,-0.2974,0.0571,0.0067,0.0232,-0.0265,0.0135,0.0415,-0.0235,-0.0237,-0.0058,0.0174,0.0503,0.0154,-0.035,-0.0318,0.0438,0.0978,-0.0408,0.0277,-0.1013,0.0353,0.0283,0.2064,-0.0064,0.062,-0.0248,-0.0437,-0.0188,0.023,-0.0248,-0.0239,-0.0275,0.091,0.0079,0.0152,0.0703,-0.0181,0.0425,0.0541,-0.0271,-0.0338,0.0312,-0.0304,-0.0106,0.0919,0.0134,-0.0013,-0.0427,0.0266,0.0268,0.0125,0.0169,-0.0302,0.0292,0.0009,-0.018,-0.0587,-0.0492,-0.0591,-0.0709,-0.0219,-0.0695,-0.0058,0.0157,0.0096]}
{"key":"[TCN: Table Convolutional Network for Web Table Interpretation] Information extraction from semi-structured webpages provides valuable long-tailed facts for augmenting knowledge graph. Relational Web tables are a critical component containing additional entities and attributes of rich and diverse knowledge. However, extracting knowledge from relational tables is challenging because of sparse contextual information. Existing work linearize table cells and heavily rely on modifying deep language models such as BERT which only captures related cells information in the same table. In this work, we propose a novel relational table representation learning approach considering both the intra- and inter-table contextual information. On one hand, the proposed Table Convolutional Network model employs the attention mechanism to adaptively focus on the most informative intra-table cells of the same row or column; and, on the other hand, it aggregates inter-table contextual information from various types of implicit connections between cells across different tables. Specifically, we propose three novel aggregation modules for (i) cells of the same value, (ii) cells of the same schema position, and (iii) cells linked to the same page topic. We further devise a supervised multi-task training objective for jointly predicting column type and pairwise column relation, as well as a table cell recovery objective for pre-training. Experiments on real Web table datasets demonstrate our method can outperform competitive baselines by +4.8% of F1 for column type prediction and by +4.1% of F1 for pairwise column relation prediction.","layer":1,"vector":[-0.0182,-0.0225,0.0274,-0.0196,0.043,0.0365,0.0357,0.0359,0.0603,-0.0198,0.025,-0.0419,0.0906,0.0926,0.0418,0.0251,0.023,0.0647,-0.0709,0.0295,0.0237,-0.0794,-0.0009,-0.0597,0.0556,0.0191,-0.0459,-0.0323,-0.0525,-0.2252,-0.0187,-0.0219,0.0082,-0.0072,0.0497,-0.0547,-0.0012,0.0144,-0.0132,0.0206,-0.0001,0.0191,-0.0555,-0.0055,-0.0219,-0.0575,-0.0234,-0.0272,-0.0261,-0.0369,0.0092,-0.0193,0.0676,0.0124,0.037,0.0374,0.0404,0.0748,0.0643,0.0752,0.0906,0.0493,-0.1784,0.0808,0.0246,0.0203,-0.0802,0.0391,0.0171,0.032,-0.0115,0.0229,0.0041,0.0652,0.0292,-0.0197,-0.0191,-0.0218,0.0017,-0.0102,0.0197,-0.0001,-0.0575,-0.032,0.0281,-0.0587,-0.0269,-0.0546,0.0434,-0.0001,-0.0641,-0.0149,-0.0618,0.0091,-0.0628,-0.0217,0.0168,-0.0165,-0.0475,0.1733,-0.0694,0.0173,0.0516,-0.0162,-0.0026,-0.0329,0.0128,-0.013,-0.0172,-0.0167,-0.0352,-0.0341,-0.011,-0.0383,0.0581,0.0038,0.1016,0.022,-0.0257,-0.0546,-0.0358,0.0204,0.0618,-0.0222,0.0266,-0.0177,-0.001,0.1345,0.0587,0.0183,0.0558,0.0105,-0.0085,-0.0393,0.0198,0.0133,0.0128,-0.018,-0.0193,-0.0127,-0.0221,-0.076,0.0036,-0.0755,-0.1044,0.1288,-0.0157,-0.017,-0.0613,-0.0396,-0.0057,0.0014,0.0345,-0.0413,-0.0023,0.02,0.0058,0.0537,-0.0676,-0.0099,0.0129,-0.0531,-0.0563,0.1174,0.0469,-0.1162,-0.0646,-0.0186,-0.0128,-0.0208,0.0435,0.0243,-0.072,0.0296,0.0999,0.0383,-0.0514,-0.0356,0.0186,-0.0231,0.0535,-0.0561,-0.0228,0.0483,0.0188,-0.0275,0.0219,-0.0188,0.013,0.0633,-0.0192,0.045,-0.0414,0.0141,0.0228,-0.0192,0.0041,-0.0065,-0.0157,-0.0558,-0.0125,-0.0228,-0.0765,0.0323,0.003,0.0065,-0.0176,-0.0153,0.0557,-0.0073,-0.0663,-0.0072,0.0177,-0.0372,0.0016,-0.0212,0.0171,0.0263,-0.0134,0.0602,0.0125,-0.0505,-0.0265,-0.2077,-0.0058,-0.0153,-0.0508,0.0142,-0.0435,0.0107,0.0072,0.0658,0.0892,0.0428,-0.0094,-0.03,0.0048,-0.0229,0.0608,0.0645,0.0414,-0.029,0.004,0.0207,0.0325,-0.0135,-0.0635,0.0666,0.0411,0.2109,0.006,0.0094,-0.0383,0.0351,0.0179,-0.0564,-0.1077,0.0406,0.0159,0.0425,-0.0287,-0.045,-0.0149,-0.0433,-0.0384,-0.013,-0.1137,-0.026,-0.0079,-0.0182,0.0052,-0.0256,0.0439,0.0661,-0.0464,0.0273,0.0676,-0.0344,0.0111,-0.064,0.0464,-0.0461,-0.0114,0.0312,-0.0444,0.0059,-0.0365,0.048,0.0093,-0.0308,0.0019,0.0569,-0.0402,-0.0544,0.0523,0.0392,0.022,0.0419,-0.0069,0.0489,-0.043,-0.0329,-0.0283,0.0535,-0.045,0.1004,0.0104,0.0624,0.0204,0.083,-0.0187,0.0627,-0.0182,-0.0073,0.0075,-0.0593,-0.0271,0.0354,-0.009,-0.2942,0.0434,0.0287,0.0373,0.0056,0.0422,0.0203,0.0238,-0.0213,-0.0227,0.0041,0.0101,0.0238,-0.0299,0.0002,0.037,0.0628,-0.0529,0.0147,-0.0063,0.0214,0.0273,0.2144,0.0064,0.0484,0.0151,0.0163,-0.0366,0.0422,0.0474,-0.0332,0.0103,0.0829,-0.0194,0.0353,0.0728,-0.0569,0.0506,0.014,-0.05,-0.0317,0.0151,-0.0592,-0.0495,0.075,0.0234,-0.0263,-0.073,0.0045,0.0063,-0.0129,-0.0136,-0.0188,-0.0256,0.056,-0.0302,0.0084,-0.0047,-0.028,0.0008,-0.0044,-0.0641,-0.018,0.0313,-0.0392]}
{"key":"[Learn to Combine Modalities in Multimodal Deep Learning] Combining complementary information from multiple modalities is intuitively appealing for improving the performance of learning-based approaches. However, it is challenging to fully leverage different modalities due to practical challenges such as varying levels of noise and conflicts between modalities. Existing methods do not adopt a joint approach to capturing synergies between the modalities while simultaneously filtering noise and resolving conflicts on a per sample basis. In this work we propose a novel deep neural network based technique that multiplicatively combines information from different source modalities. Thus the model training process automatically focuses on information from more reliable modalities while reducing emphasis on the less reliable modalities. Furthermore, we propose an extension that multiplicatively combines not only the single-source modalities, but a set of mixtured source modalities to better capture cross-modal signal correlations. We demonstrate the effectiveness of our proposed technique by presenting empirical results on three multimodal classification tasks from different domains. The results show consistent accuracy improvements on all three tasks.","layer":9,"vector":[0.0015,-0.0266,0.0188,-0.0294,0.0125,0.0365,0.0261,-0.0144,0.011,-0.0138,0.0052,-0.0737,0.0351,0.0492,0.0638,-0.0081,0.036,0.0518,-0.0588,0.0038,0.0132,0.0163,0.0272,-0.0338,0.0443,-0.0021,-0.0135,-0.0793,-0.044,-0.2662,0.0201,0.0167,0.0065,-0.0487,0.0102,-0.0212,-0.0498,0.0806,-0.0542,0.0137,0.0173,0.0078,-0.0012,-0.0782,-0.0321,-0.0769,-0.0165,0.0022,0.0125,-0.0176,0.0724,-0.0702,0.0393,0.044,-0.0266,0.0284,0.0433,0.0857,0.041,0.0387,-0.0042,0.0805,-0.1351,0.0686,0.0341,0.0168,-0.0052,-0.0323,-0.0035,0.0356,-0.0142,0.0457,0.0294,-0.0059,-0.01,-0.0154,-0.0186,0.0086,-0.0283,0.0156,0.0323,-0.0029,-0.0368,-0.0387,0.0147,-0.0427,0.0061,-0.0387,0.0202,-0.0194,-0.0908,-0.0195,-0.0297,0.0246,-0.0282,0.0208,0.0102,0.0312,-0.0225,0.1789,-0.0252,-0.0015,0.0424,-0.0362,0.017,-0.039,-0.0502,-0.029,-0.0286,0.0392,0.0011,-0.0268,0.0472,-0.0404,0.052,0.0287,0.0654,0.0015,0.0269,-0.0669,-0.0403,-0.0469,0.0569,0.021,0.03,-0.0687,0.0521,0.1274,0.0403,0.0092,0.0259,0.0152,-0.0286,-0.0547,0.0221,0.0003,-0.0012,-0.0026,0.067,0.0247,-0.0144,-0.0871,0.0398,-0.0741,-0.0691,0.1177,-0.0422,0.0107,-0.0743,-0.0004,-0.0597,0.0101,-0.0198,-0.0327,0.0695,0.0336,0.0483,0.018,-0.0325,0.0471,0.0275,-0.0694,-0.0297,0.1069,0.0256,-0.0898,-0.033,0.0144,-0.0079,-0.0481,0.0485,0.0265,-0.0403,-0.0171,0.0605,0.0425,-0.1002,0.0131,-0.0211,0.0146,0.0229,-0.0644,-0.0631,0.0682,0.0434,-0.0368,0.0198,-0.0727,0.0383,0.0334,0.0164,0.0149,0.0094,0.027,-0.0097,-0.0066,-0.0369,-0.0194,0.0061,-0.0198,-0.0187,-0.0094,-0.0358,-0.0105,-0.0229,0.0121,-0.0,0.0576,0.0623,0.0326,-0.0244,0.0132,0.0775,-0.0038,0.0049,-0.0258,0.0427,0.0449,-0.0278,0.0503,0.0222,-0.0653,-0.0426,-0.27,0.0142,0.028,0.0038,0.0735,-0.0897,0.0437,0.0208,0.0501,0.0938,0.0498,-0.0062,-0.026,0.0118,0.0065,0.0545,0.0423,0.042,-0.0284,0.0224,-0.0182,0.0161,-0.0122,-0.0841,0.0742,-0.0058,0.2005,-0.0022,0.0215,-0.0244,0.0059,0.0728,-0.0291,-0.1143,0.052,0.008,0.1089,-0.0068,-0.066,-0.0368,-0.0554,0.0063,-0.0152,-0.1149,-0.0365,-0.0158,-0.0627,0.0132,-0.0472,-0.0037,0.0375,-0.0115,0.0088,-0.0025,-0.003,-0.0204,-0.0886,0.0123,-0.0549,-0.012,0.011,-0.039,0.0586,-0.0785,0.0159,0.029,-0.0329,-0.0622,0.0295,0.0148,-0.0384,0.0674,0.0042,0.0195,0.0413,-0.0247,0.0378,-0.0056,-0.0602,-0.0213,0.0792,0.0135,-0.0159,0.0307,0.003,-0.0043,0.0986,-0.0283,0.0585,-0.0393,0.0333,0.008,-0.0274,-0.0184,0.0391,-0.0025,-0.281,0.012,0.0474,0.0192,-0.0268,-0.0125,0.0073,-0.0101,-0.0069,0.0004,-0.0125,0.0206,0.0053,-0.0162,0.0124,0.07,0.0661,-0.0398,0.0238,-0.0495,0.0107,0.04,0.2121,-0.0491,0.0224,0.0364,-0.0456,-0.0029,-0.0076,-0.0071,0.0192,0.0172,0.0741,-0.0408,-0.0273,0.0625,-0.0392,0.04,0.023,-0.0195,0.0421,0.0137,-0.0505,-0.0322,0.1049,-0.0208,0.0,0.0154,-0.0084,0.0111,-0.0294,0.0335,0.0007,0.0001,0.0051,-0.0118,-0.0229,-0.0286,-0.0097,-0.0197,0.031,-0.0513,-0.042,0.0023,-0.0395]}
{"key":"[Disturbance Grassmann Kernels for Subspace-Based Learning] In this paper, we focus on subspace-based learning problems, where data elements are linear subspaces instead of vectors. To handle this kind of data, Grassmann kernels were proposed to measure the space structure and used with classifiers, e.g., Support Vector Machines (SVMs). However, the existing discriminative algorithms mostly ignore the instability of subspaces, which would cause the classifiers misled by disturbed instances. Thus we propose considering all potential disturbance of subspaces in learning processes to obtain more robust classifiers. Firstly, we derive the dual optimization of linear classifiers with disturbance subject to a known distribution, resulting in a new kernel, Disturbance Grassmann (DG) kernel. Secondly, we research into two kinds of disturbance, relevant to the subspace matrix and singular values of bases, with which we extend the Projection kernel on Grassmann manifolds to two new kernels. Experiments on action data indicate that the proposed kernels perform better compared to state-of-the-art subspace-based methods, even in a worse environment.","layer":0,"vector":[-0.0158,-0.0147,0.0373,0.0,0.0206,0.0312,0.0226,0.0397,0.0538,0.0081,0.0344,-0.0432,-0.0108,0.0546,0.0208,-0.0144,0.0317,0.073,-0.0582,0.0084,-0.0042,-0.015,-0.0015,-0.073,-0.0098,0.0003,-0.0402,-0.0542,-0.0298,-0.2409,0.0346,-0.0357,0.0858,-0.0379,0.0126,-0.099,-0.0237,0.0474,-0.0536,0.0566,0.0159,0.0169,-0.0211,-0.0655,-0.0561,-0.0686,0.0379,-0.0452,0.0016,-0.0219,0.0171,-0.0565,0.0288,0.014,0.0424,0.0151,0.0201,0.0622,0.0563,0.0395,0.0208,0.0335,-0.1812,0.0282,0.0325,0.0081,-0.0498,-0.0018,0.045,0.0461,-0.0496,0.0467,0.0291,0.0531,-0.0147,0.0052,0.0099,-0.0026,0.0172,-0.0089,0.0037,-0.0124,-0.0371,-0.0105,-0.0658,-0.0556,0.028,-0.031,0.0418,0.0491,-0.0477,-0.0318,0.0055,0.0438,-0.0664,-0.0569,0.0667,0.0262,-0.048,0.213,-0.0493,-0.0022,0.0747,-0.0211,0.017,-0.051,-0.0325,-0.025,-0.0123,-0.0441,0.0033,0.0267,-0.0063,-0.0309,0.0255,-0.0118,0.0707,0.0468,-0.0341,-0.0044,0.004,0.0079,0.0739,-0.0409,0.0634,-0.0514,0.0307,0.1316,0.006,0.0459,0.0203,-0.0352,-0.0366,-0.0387,-0.0047,0.0322,0.0067,0.0239,-0.0032,0.0178,-0.021,-0.0695,0.022,-0.0394,-0.0275,0.1214,-0.0435,0.0075,0.0229,0.0035,-0.002,0.0149,-0.0381,0.01,-0.0078,0.0033,-0.0066,0.0322,-0.0378,0.0035,-0.0394,-0.0711,-0.0159,0.0968,0.0301,-0.0625,-0.0671,-0.0068,0.0242,-0.0043,0.0229,0.0616,-0.0305,0.0417,0.0621,0.0134,-0.0656,0.008,-0.0052,-0.0164,0.0762,-0.0181,-0.0435,-0.0103,0.0752,-0.0211,-0.0153,-0.0141,0.0325,0.0225,-0.0391,-0.0461,-0.0164,-0.0245,-0.0562,-0.0418,-0.0191,-0.0299,0.0411,-0.0384,0.01,0.0358,-0.0269,0.0369,0.008,0.0502,0.0199,0.014,0.0126,0.0612,-0.0373,0.0152,0.065,-0.0468,-0.0308,-0.0291,-0.0289,0.0432,-0.027,0.0498,0.0711,-0.0159,-0.0708,-0.2435,-0.0408,-0.0485,-0.0054,0.0133,-0.0708,0.062,-0.035,0.0389,0.0508,0.0384,0.0151,-0.0312,0.01,0.0226,0.0636,0.0299,0.0314,-0.0345,-0.0216,-0.0481,0.0151,-0.0137,-0.0296,0.0523,-0.019,0.176,0.0602,0.0541,-0.0544,0.035,0.0428,-0.0353,-0.0962,0.0602,-0.009,0.0405,-0.0355,-0.043,-0.0381,0.0212,0.0117,0.0269,-0.0734,-0.0057,-0.0413,-0.0098,0.0224,-0.0523,0.0294,0.0682,-0.0358,0.0637,-0.0553,-0.002,0.0007,-0.0616,0.0374,-0.0536,0.0325,-0.0188,-0.1073,0.0366,-0.0532,0.0496,0.0206,-0.0078,-0.0192,0.0465,-0.0358,-0.0127,0.0943,0.0153,0.0101,0.046,-0.0434,-0.009,-0.0069,-0.0564,-0.0122,0.037,0.0077,0.0458,0.0164,0.0377,-0.0073,0.0908,-0.0051,0.0343,-0.0583,-0.0192,0.0383,-0.0847,0.0304,0.0576,0.0112,-0.2863,0.0009,0.0205,0.0161,-0.0409,-0.0308,-0.0131,-0.0551,-0.0428,-0.0178,0.0217,0.0698,0.0524,-0.008,0.0008,0.0366,0.0932,-0.0477,0.0698,-0.0628,0.038,0.0598,0.2198,-0.0372,0.0374,0.0028,-0.0199,-0.0325,0.0117,-0.0349,-0.0074,-0.0177,0.1316,-0.0316,0.0686,0.0482,-0.0544,0.0284,0.0291,0.0087,0.0075,0.0056,-0.0238,-0.0399,0.0914,-0.0237,0.0279,-0.0491,0.0082,0.002,0.002,0.019,0.025,-0.0052,0.0258,0.0066,-0.0582,-0.0491,-0.0347,-0.0314,0.0371,-0.0551,-0.0329,-0.0393,-0.0014]}
{"key":"[Hierarchical Clustering with Prior Knowledge] Hierarchical clustering is a class of algorithms that seeks to build a hierarchy of clusters. It has been the dominant approach to constructing embedded classification schemes since it outputs dendrograms, which capture the hierarchical relationship among members at all levels of granularity, simultaneously. Being greedy in the algorithmic sense, a hierarchical clustering partitions data at every step solely based on a similarity / dissimilarity measure. The clustering results oftentimes depend on not only the distribution of the underlying data, but also the choice of dissimilarity measure and the clustering algorithm. In this paper, we propose a method to incorporate prior domain knowledge about entity relationship into the hierarchical clustering. Specifically, we use a distance function in ultrametric space to encode the external ontological information. We show that popular linkage-based algorithms can faithfully recover the encoded structure. Similar to some regularized machine learning techniques, we add this distance as a penalty term to the original pairwise distance to regulate the final structure of the dendrogram. As a case study, we applied this method on real data in the building of a customer behavior based product taxonomy for an Amazon service, leveraging the information from a larger Amazon-wide browse structure. The method is useful when one wants to leverage the relational information from external sources, or the data used to generate the distance matrix is noisy and sparse. Our work falls in the category of semi-supervised or constrained clustering.","layer":5,"vector":[-0.0133,-0.0235,0.0404,-0.0179,0.0176,0.0026,0.0214,0.0052,0.0707,0.0123,-0.0074,-0.0752,0.0099,0.0359,0.0105,0.0236,0.0518,0.0598,-0.0442,0.0013,0.0315,-0.0631,-0.0368,-0.0258,0.0392,0.0543,-0.0033,-0.015,-0.0659,-0.211,0.0105,-0.0408,0.0697,0.015,0.0548,-0.0202,-0.0222,0.0551,-0.0321,0.0442,0.032,0.0296,-0.0378,-0.0336,-0.0299,-0.0826,-0.0106,0.0131,-0.0482,-0.0666,0.039,-0.0323,-0.025,0.026,0.0305,0.0339,0.0519,-0.0061,0.0298,0.0309,0.0574,0.0218,-0.1352,0.053,0.0436,0.0188,-0.0625,-0.0031,0.0342,0.0504,0.0199,0.057,-0.0101,0.0739,0.0196,0.0082,-0.0069,-0.0298,-0.0315,0.01,-0.01,-0.0171,-0.0724,-0.0019,-0.0274,-0.0422,0.0273,-0.0725,0.0481,0.0179,-0.0542,-0.0199,-0.022,-0.0107,-0.0971,-0.0702,0.0123,-0.017,-0.0179,0.2021,-0.0511,0.0671,0.0477,-0.0398,0.0025,-0.0526,-0.0504,-0.0141,-0.0142,-0.0247,-0.0257,-0.0121,-0.021,-0.0419,0.0271,-0.0335,0.1076,0.0475,0.0063,-0.0551,-0.0013,0.0462,0.0419,-0.0319,0.0487,-0.0604,0.047,0.1136,0.052,0.0255,0.0477,-0.0051,-0.0687,0.0169,0.0128,0.0378,0.0092,-0.0488,0.034,-0.0052,-0.0309,-0.0726,0.0372,-0.0773,-0.0517,0.1307,-0.0039,0.0077,-0.0385,0.0009,-0.0043,0.0041,-0.0457,-0.0671,-0.0066,0.0176,0.0805,0.0094,-0.0236,0.0067,-0.0047,-0.0676,-0.0021,0.1142,0.0236,-0.0998,-0.0192,0.0076,0.0285,0.0002,0.0663,0.0673,-0.0049,0.0942,0.0765,-0.0017,-0.0594,0.0036,0.0147,0.0083,0.0685,-0.0248,-0.0321,0.0449,0.035,-0.0429,-0.0031,0.0208,0.0324,0.0229,-0.047,-0.0003,0.0061,-0.0142,0.0243,-0.0294,-0.0013,0.032,0.0008,-0.0199,0.013,0.0335,-0.0568,0.0391,-0.0151,0.0483,0.0173,-0.0333,0.0549,-0.0213,-0.0187,0.0094,0.0373,-0.016,-0.0025,0.0084,0.0414,0.0578,-0.0041,0.042,0.0603,-0.0314,-0.0574,-0.2203,-0.0137,-0.0087,-0.0043,-0.0173,-0.0279,0.0293,-0.0044,0.021,0.0593,0.0548,-0.0129,0.0034,0.0246,-0.0469,0.032,0.0887,0.0551,-0.0407,0.0299,0.005,0.0211,-0.0248,-0.0793,0.05,0.0193,0.2245,0.0286,0.0346,-0.0555,0.0337,0.0153,-0.0661,-0.0644,0.0364,-0.0113,0.0333,-0.0307,-0.0015,-0.0386,-0.0285,0.0025,-0.0081,-0.0727,-0.0077,-0.031,0.0021,-0.0014,-0.0431,0.0193,0.0266,-0.0221,0.0108,0.0034,-0.0563,-0.0645,-0.0561,0.0027,-0.0745,0.0091,0.0674,-0.0689,-0.01,-0.092,0.0571,-0.0621,-0.0761,0.0171,0.0006,-0.0379,-0.0594,0.0985,-0.0022,-0.0016,0.0584,-0.0212,0.0287,-0.0159,-0.0658,-0.0058,0.076,-0.05,0.0324,0.0267,0.0218,0.0112,0.067,-0.0267,0.0408,-0.0266,0.0288,0.0054,-0.0796,-0.0086,0.0296,-0.0005,-0.2717,0.0178,-0.0103,0.0204,-0.0195,-0.0026,0.0648,0.0032,-0.0062,-0.0353,0.0339,0.0308,0.0044,-0.0509,-0.0103,0.0553,0.0227,-0.0519,0.046,-0.0264,0.0195,0.0147,0.2599,-0.019,-0.0012,0.0347,-0.0581,0.0279,-0.0165,-0.0284,-0.0022,0.0165,0.1061,-0.0404,0.0376,0.0333,0.0135,0.027,0.0746,-0.0322,0.0108,0.0033,-0.0792,-0.0228,0.1171,0.018,-0.0179,-0.0895,-0.0066,0.0191,-0.0271,-0.0199,0.0024,0.0407,0.0142,-0.0053,-0.045,-0.0397,-0.0658,-0.0597,-0.0446,-0.0513,-0.0345,0.0229,0.0427]}
{"key":"[Automatic Home-based Screening of Obstructive Sleep Apnea Using Single Channel Electrocardiogram and SPO2 Signals] Obstructive sleep apnea (OSA) is one of the most widespread respiratory diseases today. Complete or relative breathing cessations due to upper airway subsidence during sleep is OSA. It has confirmed potential influence on Covid-19 hospitalization and mortality, and is strongly associated with major comorbidities of severe Covid-19 infection. Un-diagnosed OSA may also lead to a variety of severe physical and mental side-effects. To score OSA severity, nocturnal sleep monitoring is performed under defined protocols and standards called polysomnography (PSG). This method is time-consuming, expensive, and requiring professional sleep technicians. Automatic home-based detection of OSA is welcome and in great demand. It is a fast and effective way for referring OSA suspects to sleep clinics for further monitoring. On-line OSA detection also can be a part of a closed-loop automatic control of the OSA therapeutic/assistive devices. In this paper, several solutions for online OSA detection are introduced and tested on 155 subjects of three different databases. The best combinational solution uses mutual information (MI) analysis for selecting out of ECG and SpO2-based features. Several methods of supervised and unsupervised machine learning are employed to detect apnoeic episodes. To achieve the best performance, the most successful classifiers in four different ternary combination methods are used. The proposed configurations exploit limited use of biological signals, have online working scheme, and exhibit uniform and acceptable performance (over 85%) in all the employed databases. The benefits have not been gathered all together in the previous published methods.","layer":1,"vector":[-0.0262,0.0149,0.0125,0.0119,0.0286,0.0157,0.045,0.0287,0.0285,-0.023,-0.0014,-0.0334,0.0112,0.046,0.046,0.016,-0.0011,0.04,-0.0056,0.0023,0.0446,0.0211,0.02,-0.0554,0.0564,0.013,-0.0115,-0.0576,-0.0631,-0.1811,-0.0414,-0.0412,0.0583,-0.0106,0.0331,-0.0294,-0.0231,0.0529,-0.0341,0.0181,0.0182,-0.0028,0.0442,-0.0411,-0.0065,-0.0702,-0.0293,-0.023,-0.0034,0.0076,0.0639,-0.0049,-0.0009,0.0134,0.0174,0.0445,-0.011,0.0472,0.05,0.0571,0.0296,0.0899,-0.183,0.0513,0.0591,0.0056,-0.0613,-0.033,0.0631,0.0505,-0.0073,0.0376,0.0239,0.0247,-0.0218,0.0449,0.0046,-0.0442,-0.0291,0.0287,-0.0201,0.0252,0.0055,-0.0325,-0.0309,-0.0437,-0.0051,-0.0377,0.0302,-0.0465,-0.0466,-0.0135,0.0017,0.0292,-0.0315,-0.0132,0.0047,0.0232,-0.0543,0.2099,-0.051,0.0463,0.0466,-0.0367,0.0177,-0.0922,-0.0542,-0.0667,-0.0161,0.0145,-0.0417,0.0128,0.0375,-0.0445,0.0339,0.0721,0.0732,0.0422,0.0459,0.0204,0.011,-0.039,0.0775,-0.001,0.0564,-0.0512,0.0312,0.1188,0.0363,0.023,0.0346,0.0061,-0.0404,-0.0401,0.0128,0.008,0.0324,0.0163,0.0193,0.0009,-0.04,-0.0852,0.0646,-0.0959,-0.0506,0.1389,-0.0594,0.0873,-0.0362,-0.0337,-0.0328,-0.0052,-0.0649,-0.0401,0.043,0.0455,0.0878,0.0327,-0.0099,0.0163,-0.035,-0.0484,-0.0148,0.1062,0.011,-0.1052,-0.0442,-0.0224,0.01,-0.0186,0.0654,0.0535,-0.0365,-0.0164,0.0539,-0.0021,-0.0188,0.0075,-0.0201,-0.0078,-0.0262,-0.028,-0.0453,0.0109,0.0414,-0.0534,0.0047,-0.0177,0.0305,0.0195,-0.0423,-0.0285,0.0043,-0.0135,-0.0036,-0.013,0.0079,-0.0032,0.0077,-0.0036,0.0343,-0.0037,-0.0104,0.0024,0.0148,0.0389,-0.0371,-0.0024,0.0538,0.0244,0.0125,-0.0029,0.045,0.0144,-0.0691,-0.0273,-0.0296,0.0257,-0.0056,0.066,0.0646,-0.0353,-0.0415,-0.232,-0.0167,0.0165,-0.0296,-0.0168,-0.0744,0.0048,-0.0056,0.0084,0.0735,0.075,0.0266,-0.0378,0.0309,-0.0116,0.1075,0.0312,0.0149,-0.033,-0.0376,-0.0051,0.0272,0.0119,-0.0433,0.0139,-0.0235,0.2018,-0.0196,0.033,-0.0296,0.0157,0.0329,-0.0245,-0.0998,0.0686,0.0327,0.0618,-0.0089,-0.0403,-0.0587,-0.0918,0.0406,0.0445,-0.0722,-0.0753,-0.0479,-0.0455,0.0131,-0.068,0.0123,0.0722,-0.0005,0.024,-0.0475,0.0332,-0.0693,-0.0844,0.0261,-0.0215,-0.013,-0.0117,-0.0525,0.0556,-0.0607,0.0157,-0.0142,-0.0592,-0.0507,0.0401,-0.0109,-0.0854,0.1091,-0.0368,-0.0305,0.075,0.0344,0.0946,-0.0267,-0.0727,-0.013,0.066,-0.0791,-0.0168,0.0525,-0.0076,-0.0196,0.0784,0.0474,-0.0231,-0.0277,0.0006,0.0129,-0.011,-0.0373,0.0363,0.0238,-0.2858,0.0069,-0.0245,0.0156,-0.0225,-0.0281,0.0029,0.0207,-0.0514,-0.0268,-0.0408,0.0101,-0.0103,-0.0127,-0.0122,0.0429,0.0562,-0.0703,0.0762,-0.0372,-0.0135,0.0515,0.1982,-0.0245,0.0683,-0.0025,0.0092,0.0073,0.0217,0.0039,0.0533,0.0201,0.0791,-0.0723,0.0391,0.0546,-0.042,0.0092,0.0144,-0.0335,0.0113,0.0165,-0.0507,-0.0521,0.0957,0.004,-0.0477,-0.0114,-0.0169,0.0113,-0.0408,-0.0467,-0.0013,0.0301,0.0101,0.044,-0.0239,0.0077,0.0149,-0.0229,0.016,-0.0717,-0.0641,0.0507,0.0286]}
{"key":"[Robust Policies via Mid-Level Visual Representations: An Experimental Study in Manipulation and Navigation] Vision-based robotics often separates the control loop into one module for perception and a separate module for control. It is possible to train the whole system end-to-end (e.g. with deep RL), but doing it \"from scratch\" comes with a high sample complexity cost and the final result is often brittle, failing unexpectedly if the test environment differs from that of training. We study the effects of using mid-level visual representations (features learned asynchronously for traditional computer vision objectives), as a generic and easy-to-decode perceptual state in an end-to-end RL framework. Mid-level representations encode invariances about the world, and we show that they aid generalization, improve sample complexity, and lead to a higher final performance. Compared to other approaches for incorporating invariances, such as domain randomization, asynchronously trained mid-level representations scale better: both to harder problems and to larger domain shifts. In practice, this means that mid-level representations could be used to successfully train policies for tasks where domain randomization and learning-from-scratch failed. We report results on both manipulation and navigation tasks, and for navigation include zero-shot sim-to-real experiments on real robots.","layer":1,"vector":[-0.0318,-0.0508,0.015,-0.0372,0.0009,0.0315,0.0351,0.0528,-0.004,-0.01,0.0696,-0.04,0.032,0.1062,-0.0096,0.0162,-0.0254,0.0567,-0.0218,0.0225,0.01,-0.052,-0.0129,-0.0499,-0.012,0.0377,-0.0414,-0.0475,-0.0257,-0.2633,0.0287,-0.0692,0.0055,-0.0248,0.0174,-0.0049,-0.0445,0.054,-0.0008,-0.0071,0.0286,-0.0461,-0.0533,-0.0798,-0.0677,-0.0399,-0.0212,-0.0257,-0.0162,-0.046,-0.0151,-0.0357,0.0165,0.0301,0.0212,0.0409,0.0823,0.0603,0.0759,0.0426,0.0271,0.0485,-0.1606,0.0666,0.0299,0.032,-0.0511,-0.0197,-0.001,0.0385,-0.0408,0.0021,0.0231,0.0589,-0.0023,-0.0622,-0.0197,-0.0468,-0.0172,-0.01,0.0275,-0.0378,-0.042,0.0389,-0.0269,-0.0263,-0.0204,-0.0667,0.0712,0.0346,-0.0842,-0.012,-0.0419,0.0119,-0.0444,-0.0033,0.0318,0.0127,-0.0311,0.2111,-0.0011,0.0349,0.0684,0.0045,0.0511,-0.0563,-0.0097,0.0047,-0.0278,0.0222,-0.0115,-0.0047,0.0059,0.0045,0.0193,-0.0031,0.0672,0.0249,0.0153,-0.0094,0.0056,0.0005,0.0145,-0.0186,0.0137,-0.0661,0.0312,0.157,0.0046,0.0253,0.0267,-0.0363,-0.0194,-0.013,0.0003,0.047,0.037,0.0317,0.0102,-0.0042,-0.044,0.0151,0.0185,-0.1094,-0.0477,0.1165,-0.036,0.0332,-0.0336,-0.0079,0.0041,0.0462,-0.0093,-0.0259,0.0195,0.019,0.0347,0.036,-0.054,0.022,-0.0411,-0.0264,-0.0133,0.0459,0.0164,-0.0498,-0.0517,-0.0013,-0.0086,-0.0306,-0.0178,0.0088,-0.0469,0.009,0.0668,0.0371,-0.1238,0.0044,-0.0068,0.0121,0.0441,-0.117,0.0043,0.016,0.0561,0.022,0.0078,-0.0005,0.0051,0.0052,-0.0061,0.0414,-0.0238,0.0081,-0.0026,-0.0011,0.0064,-0.016,-0.0209,-0.0066,-0.0158,0.015,-0.0416,0.0157,-0.0083,0.0028,-0.0285,0.009,0.0555,0.0326,-0.0249,0.0416,0.0561,-0.0138,-0.0165,-0.0036,0.0078,0.029,-0.0036,0.003,0.0176,-0.0503,-0.0356,-0.2393,0.0006,-0.0035,0.003,0.0584,-0.0886,-0.0223,-0.0353,0.053,0.023,0.0491,-0.0401,-0.016,0.0302,-0.0168,0.064,0.0458,0.0413,-0.043,0.0124,-0.0333,0.0185,-0.0251,-0.1094,0.0357,-0.0048,0.2401,0.0099,0.0359,-0.0065,0.0093,0.0233,-0.0351,-0.0615,0.0657,0.0145,0.0562,-0.0227,0.0413,-0.0257,-0.0206,-0.0045,0.0313,-0.1147,0.0029,-0.079,-0.0472,0.061,-0.0328,0.0064,0.0242,-0.0635,0.0375,-0.0269,-0.0608,-0.0047,-0.0652,0.0089,-0.0487,0.0864,0.0232,-0.047,0.024,-0.048,0.0429,0.0161,0.005,-0.0721,0.07,-0.0252,-0.0367,0.0505,-0.0055,0.0078,0.036,0.051,0.0332,0.013,-0.0269,-0.0104,0.0476,0.007,0.0194,0.0521,0.037,-0.0274,0.0023,-0.0388,0.063,0.0141,0.0084,-0.0071,-0.0461,0.0137,0.0743,-0.0062,-0.2969,0.0257,0.0448,0.0361,-0.019,0.0101,0.0445,-0.0015,-0.0244,0.0076,-0.0163,0.0522,0.102,0.0466,0.0251,0.0322,0.0666,-0.0317,0.0594,-0.0336,-0.0087,0.0719,0.2129,-0.0367,0.0083,-0.0315,-0.0501,-0.0451,0.0232,0.0067,-0.0036,0.0386,0.0879,-0.0325,0.0456,0.0442,-0.0341,0.0396,0.0086,0.0049,0.0039,0.015,0.0157,-0.0449,0.0973,-0.0034,-0.0316,-0.028,-0.0026,0.0386,-0.0458,-0.0242,-0.0233,-0.0525,0.032,0.0276,-0.0114,-0.0594,-0.0734,-0.0485,0.0264,-0.0653,0.036,-0.034,-0.0344]}
{"key":"[The Boomerang Sampler] This paper introduces the Boomerang Sampler as a novel class of continuous-time non-reversible Markov chain Monte Carlo algorithms. The methodology begins by representing the target density as a density, $e^{-U}$, with respect to a prescribed (usually) Gaussian measure and constructs a continuous trajectory consisting of a piecewise elliptical path. The method moves from one elliptical orbit to another according to a rate function which can be written in terms of $U$. We demonstrate that the method is easy to implement and demonstrate empirically that it can out-perform existing benchmark piecewise deterministic Markov processes such as the bouncy particle sampler and the Zig-Zag. In the Bayesian statistics context, these competitor algorithms are of substantial interest in the large data context due to the fact that they can adopt data subsampling techniques which are exact (ie induce no error in the stationary distribution). We demonstrate theoretically and empirically that we can also construct a control-variate subsampling boomerang sampler which is also exact, and which possesses remarkable scaling properties in the large data limit. We furthermore illustrate a factorised version on the simulation of diffusion bridges.","layer":8,"vector":[-0.0748,0.0065,0.0442,-0.026,0.0377,0.0234,0.0271,0.0134,0.0279,-0.0031,0.0558,-0.0285,0.0165,0.0885,-0.0066,0.0282,-0.0189,0.0254,0.0061,0.0003,0.0496,-0.0603,-0.0062,-0.0588,0.0341,0.0504,0.002,-0.0272,-0.0353,-0.2612,0.0311,-0.0414,0.02,-0.039,-0.0032,-0.0119,0.002,0.0434,0.0151,0.031,0.0452,0.0217,-0.032,-0.0414,-0.0064,-0.0534,-0.0176,-0.0409,0.0106,-0.0107,-0.0249,-0.0374,0.067,0.0059,0.0272,0.0586,0.0479,0.0069,0.1055,0.0494,-0.0075,0.0449,-0.1594,0.0371,0.047,0.0205,-0.0498,-0.0074,0.0222,0.051,-0.0562,0.0236,-0.0096,0.0816,0.0568,-0.038,-0.0059,-0.0511,-0.0427,0.0346,0.0177,-0.0635,-0.0255,-0.0033,-0.0374,-0.0522,0.0009,-0.061,0.072,0.001,-0.0068,0.021,-0.0079,0.0195,-0.0636,-0.0334,0.0626,0.0467,0.031,0.1858,-0.0412,0.0472,0.0476,0.0198,0.0477,-0.064,-0.0201,-0.0377,0.0011,0.0115,-0.0048,-0.048,0.0583,-0.1055,0.0223,0.0001,0.0427,0.0017,-0.0501,-0.0371,-0.0274,0.0246,0.0496,-0.0193,0.0107,-0.0111,-0.016,0.1385,0.0659,0.016,0.0763,-0.0293,-0.0758,-0.0207,-0.0109,-0.0081,0.003,-0.0202,0.0109,0.0074,-0.0236,-0.0613,-0.0121,-0.114,-0.0241,0.1452,-0.0142,0.0591,-0.0446,-0.0149,-0.0157,-0.0015,0.0216,-0.0698,0.0292,0.0178,0.0057,0.0742,-0.0531,0.0212,-0.0707,-0.0396,-0.0195,0.1121,0.0126,-0.0985,-0.0072,0.0172,0.0239,0.0183,0.029,0.0214,-0.0576,0.0208,0.1043,-0.0354,-0.0817,-0.0281,0.0364,-0.0032,0.0142,-0.0091,-0.0364,0.0397,0.027,-0.0658,-0.0014,-0.015,0.0264,0.0361,-0.0311,-0.0154,0.0216,-0.0347,-0.0333,-0.0566,-0.0106,0.0104,0.0225,-0.0377,0.0095,-0.0081,-0.0505,0.0241,0.0241,0.0081,-0.0183,-0.0022,0.0386,0.0235,0.0115,-0.005,0.0725,0.031,0.0031,-0.0251,-0.0204,0.0221,-0.0141,0.0258,0.0143,-0.0519,-0.0548,-0.2247,-0.0115,0.0299,0.0293,0.0568,-0.0448,-0.0042,-0.0144,0.0887,0.0686,0.0363,-0.0457,-0.0288,-0.0005,-0.0241,0.0616,0.0099,0.0405,-0.0102,0.0216,0.0082,-0.0356,-0.0918,-0.0233,0.047,-0.0364,0.2087,0.0298,0.0357,0.0078,-0.0009,0.025,-0.0182,-0.0431,0.0238,0.0462,0.0714,-0.0151,-0.036,-0.0385,-0.0271,0.0203,0.0012,-0.0807,-0.0458,-0.0371,-0.0275,0.0402,-0.0374,-0.0067,0.0139,-0.0384,0.0633,-0.0372,0.0206,-0.0568,-0.0426,0.041,-0.0068,0.0228,0.0263,-0.0332,0.0301,0.0218,0.0384,-0.0194,0.009,-0.0229,-0.0201,-0.0675,-0.0159,0.0694,0.0091,0.0474,0.0305,0.0121,0.0144,-0.0453,-0.0808,-0.0381,0.022,-0.0265,-0.005,0.0824,0.0108,-0.0205,0.0469,-0.0033,-0.001,-0.0263,0.0053,0.0119,-0.0617,0.0161,0.0174,0.0045,-0.306,0.0172,0.0056,0.0309,-0.0466,-0.014,0.0644,0.0419,-0.0268,-0.0294,-0.0287,0.0732,0.0314,0.0161,-0.0134,0.0294,0.0507,-0.0228,0.0311,-0.0593,0.0077,-0.0167,0.2267,-0.0046,0.0084,0.0142,-0.0246,0.0433,0.0246,-0.0592,0.015,-0.0267,0.0739,-0.0737,0.08,0.0513,-0.0271,0.0655,0.0364,-0.0574,0.0124,0.011,0.0129,-0.0484,0.1123,-0.0202,-0.0353,-0.087,0.0163,0.0488,-0.0593,0.0357,-0.0206,0.0056,-0.0005,0.0478,-0.0577,-0.0385,-0.0117,-0.051,0.0052,-0.0314,-0.029,-0.0092,0.0162]}
{"key":"[Gone Fishing: Neural Active Learning with Fisher Embeddings] There is an increasing need for effective active learning algorithms that are compatible with deep neural networks. This paper motivates and revisits a classic, Fisher-based active selection objective, and proposes BAIT, a practical, tractable, and high-performing algorithm that makes it viable for use with neural models. BAIT draws inspiration from the theoretical analysis of maximum likelihood estimators (MLE) for parametric models. It selects batches of samples by optimizing a bound on the MLE error in terms of the Fisher information, which we show can be implemented efficiently at scale by exploiting linear-algebraic structure especially amenable to execution on modern hardware. Our experiments demonstrate that BAIT outperforms the previous state of the art on both classification and regression problems, and is flexible enough to be used with a variety of model architectures.","layer":1,"vector":[-0.0405,-0.004,0.0122,-0.001,0.0324,0.0394,0.0536,0.0609,0.043,-0.0145,0.0059,-0.0357,0.0006,0.0175,-0.0088,0.0153,0.0224,0.0624,-0.0183,0.0109,0.0241,-0.0175,-0.032,-0.0866,0.0123,0.0138,-0.0424,-0.0202,-0.0657,-0.2412,-0.0139,-0.0651,0.0478,-0.0323,0.0024,-0.008,-0.0478,0.0248,-0.0188,0.0743,0.0262,0.0266,-0.0228,-0.0448,-0.0221,-0.0034,-0.0188,-0.0107,-0.0103,-0.0021,-0.0071,-0.0218,0.0327,0.0251,0.0445,-0.02,0.0486,0.0224,-0.0102,0.0395,0.0193,0.0593,-0.138,0.0265,0.0072,0.0441,-0.0386,-0.0393,0.0004,0.0261,-0.0289,0.0403,0.015,0.0252,0.0151,0.0177,0.0207,0.0049,0.0045,0.0166,0.0283,-0.0453,-0.0397,-0.0234,-0.0295,-0.0131,0.0186,-0.048,0.052,-0.0265,-0.0436,0.0278,-0.0422,0.0089,-0.0576,-0.0048,0.0527,0.0116,-0.0642,0.2252,-0.0273,0.0385,-0.0131,-0.0183,0.0113,-0.0571,-0.058,0.0251,-0.0323,-0.0286,-0.0169,-0.0086,-0.0201,-0.0127,0.0297,0.0422,0.028,0.0185,0.004,-0.0234,-0.0332,-0.0114,0.0482,-0.0119,0.0186,-0.0261,0.0309,0.1685,0.0509,0.0276,0.0464,-0.0433,-0.0615,-0.0342,0.0098,0.0184,0.0433,0.0325,0.0117,0.0103,-0.064,-0.0389,0.0553,-0.0848,-0.0439,0.1112,-0.0293,-0.025,-0.0614,-0.0227,0.0027,0.0546,-0.0388,-0.0191,0.0341,0.0607,0.0143,0.0363,-0.0374,-0.0091,-0.0658,-0.0521,0.003,0.0987,0.0227,-0.0516,-0.0346,-0.0117,0.0314,0.0131,0.07,0.0238,-0.0396,0.0429,0.0725,0.0151,-0.0812,-0.0254,0.0168,0.0012,0.0146,-0.0761,-0.0272,0.0246,0.0294,-0.0494,0.0245,-0.0812,0.0116,0.0234,-0.0344,-0.0046,0.0027,-0.0149,-0.0385,-0.0222,0.0116,-0.0248,0.0564,-0.0309,-0.0012,0.005,-0.0287,0.0044,0.0316,0.0019,-0.0034,0.0174,0.024,0.0362,-0.0175,0.0099,0.0772,-0.0144,-0.0443,-0.0283,0.0177,0.0375,0.0131,0.0633,0.0258,-0.0652,-0.0437,-0.2539,0.0041,0.008,-0.0668,0.0523,-0.0941,0.0573,-0.0297,0.0352,0.1086,0.006,-0.0292,-0.0301,0.0313,-0.0086,0.0893,0.0139,0.023,-0.0421,0.0023,-0.0108,0.0122,0.0004,-0.0454,0.0825,0.0011,0.2169,0.0831,0.0512,-0.0283,0.0136,0.0389,-0.0216,-0.089,0.0465,0.0148,0.0964,-0.0188,-0.0477,-0.0068,-0.048,0.0617,-0.0128,-0.1189,-0.0306,-0.0243,-0.0041,0.0448,-0.0614,-0.0072,0.0336,-0.0386,0.0435,-0.0196,-0.0479,-0.0491,-0.0873,0.038,-0.0552,0.0475,0.0684,-0.0264,-0.0421,-0.0674,-0.0046,0.0203,-0.0552,-0.0704,0.0259,-0.016,0.0114,0.065,0.0235,0.0035,0.0383,0.0231,0.0057,-0.065,-0.0114,0.0188,0.0387,-0.0426,0.0101,0.0475,0.0137,0.0344,0.1143,-0.0343,0.0367,0.0099,-0.0014,0.0067,-0.0835,0.0051,0.0456,-0.0016,-0.2656,0.0326,0.0367,0.0376,-0.0556,-0.0069,0.0517,-0.0129,-0.0398,0.0217,0.0031,0.0487,0.0226,0.0141,-0.0308,0.0155,0.0592,-0.0581,0.0406,-0.0866,0.0177,0.0306,0.2015,-0.0532,0.0423,-0.0076,-0.039,0.0095,0.0196,-0.0709,0.0177,0.0575,0.0994,-0.0522,0.03,0.1285,-0.0437,0.0145,0.0342,-0.0518,0.0061,0.0123,-0.0665,0.0005,0.0755,-0.037,0.0104,-0.0297,-0.0253,0.0647,0.0087,-0.0132,0.0093,-0.0092,0.0276,0.0125,-0.0523,-0.0416,-0.0788,-0.0063,0.0408,-0.0626,0.0124,-0.0028,0.0232]}
{"key":"[Training Shallow and Thin Networks for Acceleration via Knowledge Distillation with Conditional Adversarial Networks] There is an increasing interest on accelerating neural networks for real-time applications. We study the student-teacher strategy, in which a small and fast student network is trained with the auxiliary information learned from a large and accurate teacher network. We propose to use conditional adversarial networks to learn the loss function to transfer knowledge from teacher to student. The proposed method is particularly effective for relatively small student networks. Moreover, experimental results show the effect of network size when the modern networks are used as student. We empirically study the trade-off between inference time and classification accuracy, and provide suggestions on choosing a proper student network.","layer":0,"vector":[-0.0691,0.0049,0.0174,-0.0067,0.0284,0.0273,0.001,0.0107,0.0291,0.001,0.0144,-0.0171,0.035,0.0703,-0.0026,-0.0063,0.022,0.0582,-0.0312,0.0141,0.0487,-0.0245,-0.005,-0.0383,0.0224,0.017,0.0048,-0.0291,-0.0441,-0.2349,0.0434,-0.0726,0.0165,0.0003,-0.0042,-0.0167,-0.0142,0.0462,-0.0265,0.0374,0.0577,0.0026,-0.0395,-0.0556,0.0015,-0.0129,-0.034,-0.0316,-0.0269,-0.0858,0.035,-0.0583,0.0158,0.0227,-0.0069,0.0309,0.019,0.0334,0.069,0.0402,-0.0084,0.0441,-0.1842,0.0714,0.0034,0.0233,-0.0243,-0.0037,-0.0038,0.0392,0.0175,0.029,0.0497,0.0925,0.0033,0.0101,-0.0239,-0.0403,-0.0065,-0.0134,0.0386,-0.037,-0.044,-0.0136,0.0121,-0.0129,0.0131,-0.0487,0.0186,-0.0438,-0.0519,-0.0002,-0.0308,0.0587,-0.0099,-0.0232,0.0204,0.0165,-0.0768,0.1992,-0.0828,0.0115,0.0436,-0.0244,0.0089,-0.0094,-0.0232,-0.0254,-0.038,0.0215,-0.0214,-0.0171,0.0138,-0.0248,0.0517,0.0458,0.0453,0.0412,-0.0422,0.0104,-0.0212,0.0432,0.049,-0.0403,0.0256,-0.0856,-0.0152,0.131,-0.0033,0.1014,0.0159,-0.0509,-0.0266,0.0068,0.034,0.0028,0.0038,-0.0423,-0.0363,-0.0294,-0.0352,-0.0094,0.0084,-0.0904,-0.032,0.1053,-0.0001,0.0453,-0.0324,-0.0406,-0.0495,0.0224,-0.0453,-0.0246,-0.0104,0.0108,0.0708,0.0453,-0.0613,-0.0493,-0.0632,-0.0176,-0.0625,0.0766,0.0414,-0.077,-0.0219,0.0019,0.0045,-0.0514,0.0362,0.0211,-0.0045,0.0206,0.0891,0.0374,-0.0739,-0.0367,-0.0015,0.015,0.0226,-0.0539,0.0047,0.0648,0.0391,0.0065,0.0378,-0.0813,0.0197,0.0555,-0.0571,0.0382,-0.0754,-0.0218,-0.0466,-0.0138,0.0194,0.0002,-0.0068,-0.0031,-0.0056,0.002,-0.037,-0.0008,-0.0083,0.0386,0.0082,-0.0177,0.0724,0.0126,-0.0222,-0.0183,0.0263,-0.0661,-0.016,0.011,0.0124,0.0686,-0.0227,0.0547,0.0567,-0.051,-0.0039,-0.2415,0.0146,0.0128,-0.0209,0.0768,-0.0466,0.0316,0.0009,0.0395,0.0253,0.0757,-0.0153,-0.0064,0.0034,0.0032,0.0548,0.0447,0.0259,-0.0055,-0.0383,-0.0306,0.0164,-0.0113,-0.0895,0.0642,0.0244,0.2421,0.0173,0.0692,-0.0277,0.0237,0.0374,0.003,-0.0897,0.0586,-0.01,0.0898,0.0063,-0.0249,-0.0293,-0.0342,0.0188,0.0124,-0.1241,-0.0526,-0.0366,-0.0096,0.0145,-0.0479,0.0416,0.0446,-0.0251,0.0591,-0.007,-0.0309,-0.0237,-0.1267,0.0759,-0.0744,0.0338,-0.0171,-0.0576,-0.0275,-0.0553,0.0675,-0.0021,-0.0159,-0.0404,0.0155,-0.013,-0.0282,0.0771,0.0181,0.0598,0.0593,0.0034,0.0479,-0.0358,-0.0538,-0.0187,0.0705,-0.0501,0.0318,0.0208,0.0539,-0.0136,0.0997,0.0041,0.0013,-0.0031,-0.0163,0.0405,-0.0499,0.0013,0.0166,-0.0042,-0.2809,0.0463,0.0465,0.0662,-0.0425,-0.0075,0.0459,0.0249,-0.0421,-0.0292,-0.0211,0.0008,0.0311,-0.0167,-0.0056,0.0586,0.0358,-0.0301,0.0026,-0.0399,0.0366,0.0358,0.1949,-0.0403,0.0306,-0.0019,-0.031,0.0038,0.0769,-0.0507,0.044,0.0046,0.0801,-0.0427,0.0239,0.0792,-0.0424,0.0163,0.0315,-0.0014,-0.0063,-0.0199,-0.0856,0.0033,0.094,0.0158,-0.0285,-0.0681,-0.0321,-0.0098,-0.0089,0.0122,-0.01,0.0223,-0.0002,0.0388,-0.0509,-0.039,-0.0269,-0.0488,0.0104,-0.0412,0.0365,-0.0205,-0.0122]}
{"key":"[Learning to Win by Reading Manuals in a Monte-Carlo Framework] Domain knowledge is crucial for effective performance in autonomous control systems. Typically, human effort is required to encode this knowledge into a control algorithm. In this paper, we present an approach to language grounding which automatically interprets text in the context of a complex control application, such as a game, and uses domain knowledge extracted from the text to improve control performance. Both text analysis and control strategies are learned jointly using only a feedback signal inherent to the application. To effectively leverage textual information, our method automatically extracts the text segment most relevant to the current game state, and labels it with a task-centric predicate structure. This labeled text is then used to bias an action selection policy for the game, guiding it towards promising regions of the action space. We encode our model for text analysis and game playing in a multi-layer neural network, representing linguistic decisions via latent variables in the hidden layers, and game action quality via the output layer. Operating within the Monte-Carlo Search framework, we estimate model parameters using feedback from simulated games. We apply our approach to the complex strategy game Civilization II using the official game manual as the text guide. Our results show that a linguistically-informed game-playing agent significantly outperforms its language-unaware counterpart, yielding a 34% absolute improvement and winning over 65% of games when playing against the built-in AI of Civilization.","layer":3,"vector":[-0.064,0.0418,0.0351,0.0143,0.0153,0.0064,0.0673,0.048,-0.0041,-0.0094,-0.0288,-0.0473,0.0301,0.0706,0.0052,0.0051,-0.0166,0.0259,-0.007,-0.0104,0.0463,-0.0279,-0.0138,-0.0628,-0.0309,0.0192,-0.0536,-0.0469,-0.0468,-0.2303,-0.0079,-0.0361,0.0384,-0.0119,-0.0198,-0.0229,-0.0752,0.0597,-0.0303,0.008,0.0273,-0.0158,0.03,-0.0354,0.0025,-0.0521,-0.0043,-0.002,-0.0175,-0.0675,0.0089,0.0082,0.0321,-0.0047,0.0843,0.0325,0.061,0.0747,0.0897,0.0436,0.0185,0.0399,-0.2078,0.0929,0.0295,0.0326,-0.0277,0.0326,0.0073,0.0873,-0.033,0.0418,0.0194,0.0742,0.0334,-0.0204,-0.0044,-0.0904,0.0258,0.0117,0.0291,-0.0276,-0.0515,0.0005,-0.0145,-0.0349,0.0099,0.0092,0.0695,0.013,0.0036,-0.0139,0.0048,0.0064,-0.0521,0.0015,0.0079,-0.002,-0.0495,0.204,0.0202,-0.0244,-0.0112,-0.0295,0.0809,-0.007,-0.0344,-0.0037,-0.0433,-0.0021,-0.0587,0.0111,0.0234,-0.0064,0.0506,0.0407,0.0948,0.032,-0.0324,-0.0335,-0.0147,0.0052,0.0199,-0.017,0.0401,-0.0631,0.0413,0.1376,0.0406,-0.0053,0.0771,-0.0208,-0.0441,-0.0349,0.0077,0.0044,-0.0328,0.0247,-0.0046,0.0015,-0.0535,-0.041,0.0243,-0.107,-0.089,0.0909,0.0137,-0.0082,-0.0226,-0.0002,-0.0088,0.0144,0.0128,-0.0341,0.0063,0.0057,0.052,0.0015,-0.0485,-0.0222,-0.0158,-0.0332,-0.0306,0.0847,0.0025,-0.0654,-0.0582,-0.0015,0.014,-0.0468,0.0461,0.0247,-0.0682,0.0227,0.052,-0.0227,-0.0815,-0.0034,0.0139,0.0275,0.0644,-0.0313,0.0177,0.0632,0.0136,-0.0418,-0.0174,-0.041,0.0188,0.0028,-0.0507,0.0332,-0.0306,0.0177,-0.0321,0.0178,0.0152,-0.0279,0.0265,-0.0405,-0.0441,0.0142,-0.0702,-0.0042,-0.0192,-0.0205,-0.0088,0.0013,0.0698,0.0061,-0.0445,-0.0086,0.0564,-0.0391,-0.0221,0.0059,0.0054,0.0347,-0.0228,0.0196,-0.0027,0.0042,-0.0536,-0.2136,-0.0116,-0.0013,-0.0467,0.0467,-0.0386,0.0095,-0.047,0.0435,0.0569,0.0348,-0.0433,-0.0188,0.0488,-0.02,0.0449,-0.0024,0.029,-0.0344,0.031,0.0455,-0.0449,-0.0267,-0.0809,0.0055,-0.0235,0.2373,0.0386,0.0677,-0.0185,0.0108,0.0344,-0.0209,-0.0662,0.0845,0.0013,0.074,-0.0051,-0.0503,-0.0545,-0.0027,0.0441,0.0022,-0.0902,-0.0208,-0.0423,-0.0305,0.0238,-0.0457,-0.0044,0.0506,0.0117,0.0526,0.0082,-0.0627,-0.0132,-0.0808,0.0258,-0.005,0.0216,0.0376,-0.0257,-0.0281,-0.0395,0.0765,-0.0061,-0.0072,-0.0561,0.0787,0.0115,-0.0101,0.0354,-0.0039,0.0111,0.0286,0.0189,0.011,-0.0585,-0.0241,-0.0162,0.0582,0.0053,0.0502,0.0278,0.0407,-0.0346,0.0566,-0.0312,0.0547,0.0021,0.0045,0.0587,-0.0763,-0.0104,0.0518,-0.0297,-0.3176,0.052,0.0646,0.027,-0.04,0.0179,0.0788,0.0346,-0.0585,0.037,-0.0025,0.0479,-0.0164,-0.0136,-0.0585,0.0189,0.055,-0.0238,0.0411,-0.0579,0.049,0.0588,0.2192,-0.0312,0.0506,-0.0114,-0.0017,-0.0162,0.0281,0.0118,0.0062,0.0109,0.0923,-0.0491,0.0348,0.0597,-0.0106,0.0064,0.0049,0.0106,-0.0475,0.0481,-0.0067,-0.058,0.0486,0.0178,-0.0286,-0.0449,-0.0231,0.0128,-0.0525,0.05,-0.043,-0.0331,0.0322,0.0069,-0.0345,-0.0562,-0.0338,-0.0414,-0.0135,-0.0844,0.0295,0.0091,-0.0151]}
{"key":"[A Guide to Computational Reproducibility in Signal Processing and Machine Learning] Computational reproducibility is a growing problem that has been extensively studied among computational researchers and within the signal processing and machine learning research community. However, with the changing landscape of signal processing and machine learning research come new obstacles and unseen challenges in creating reproducible experiments. Due to these new challenges most computational experiments have become difficult, if not impossible, to be reproduced by an independent researcher. In 2016 a survey conducted by the journal Nature found that 50% of researchers were unable to reproduce their own experiments. While the issue of computational reproducibility has been discussed in the literature and specifically within the signal processing community, it is still unclear to most researchers what are the best practices to ensure reproducibility without impinging on their primary responsibility of conducting research. We feel that although researchers understand the importance of making experiments reproducible, the lack of a clear set of standards and tools makes it difficult to incorporate good reproducibility practices in most labs. It is in this regard that we aim to present signal processing researchers with a set of practical tools and strategies that can help mitigate many of the obstacles to producing reproducible computational experiments.","layer":0,"vector":[-0.0253,-0.0341,0.0199,-0.0097,0.0363,0.0237,0.0078,0.0109,0.0536,-0.0158,0.0032,-0.0439,0.0522,0.0224,-0.0013,0.0488,-0.0083,0.0267,-0.0378,-0.0164,0.0007,-0.0362,-0.0216,-0.0335,0.0308,-0.0035,-0.0148,-0.0346,-0.056,-0.2809,0.0122,-0.0595,0.0549,-0.0259,0.0522,-0.0071,-0.0118,0.0465,-0.0487,0.0452,0.019,-0.0108,-0.0175,-0.0501,-0.0207,-0.0567,-0.0498,-0.0375,-0.0383,0.0138,0.0103,-0.0454,0.0141,0.0311,0.0402,0.0179,0.0824,0.046,0.0749,0.0093,0.0115,0.0675,-0.2012,0.0264,0.0529,0.0588,-0.0372,-0.0558,0.0171,0.0667,-0.0386,0.0301,0.0199,-0.0159,0.0176,-0.0108,-0.0237,-0.0697,-0.0175,0.0132,0.0147,-0.0241,-0.0279,0.0196,-0.0135,-0.0366,0.0026,-0.0222,0.0388,-0.0076,-0.0295,0.0035,-0.039,0.0351,-0.0877,0.0042,0.0401,0.0082,-0.0138,0.1819,-0.0483,0.0307,0.0135,0.01,0.0221,-0.0652,-0.0379,-0.0268,-0.0521,0.0438,-0.0194,-0.0092,0.0285,-0.0154,0.0346,0.0261,0.0604,0.0321,-0.0197,-0.0285,-0.0292,0.001,0.0072,-0.0093,0.0608,-0.0418,0.0322,0.1258,0.0487,0.0272,0.0476,-0.0134,-0.0513,-0.0152,0.0051,0.0136,0.0272,-0.0088,0.04,0.0083,-0.022,-0.0224,0.0052,-0.0417,-0.0904,0.1421,-0.0687,0.044,-0.0725,-0.0261,-0.0363,-0.0099,-0.0456,-0.0325,0.0486,0.0496,0.0019,0.0182,-0.0218,0.045,-0.0017,-0.0703,-0.0221,0.0939,-0.0038,-0.0681,-0.009,0.0429,0.0516,0.0067,0.0474,-0.0057,-0.0468,0.0155,0.0213,0.0156,-0.0462,-0.0284,0.0322,0.0413,0.0473,-0.0572,-0.0335,0.0044,0.03,-0.0581,-0.0046,-0.0305,-0.0014,0.0602,0.0031,-0.018,-0.0073,0.011,-0.0466,-0.0544,0.0083,-0.0213,-0.02,-0.0217,-0.0204,0.0103,-0.0071,0.0412,-0.001,0.0489,0.009,-0.0019,0.0789,0.0338,-0.0273,-0.0159,0.0504,-0.0479,-0.0686,0.0013,0.0144,0.0639,0.0406,0.0662,0.0214,-0.0536,-0.0767,-0.2261,-0.0172,0.056,0.0373,0.0603,-0.0694,0.0404,-0.0299,0.0694,0.1001,0.035,0.0038,-0.0484,0.0166,-0.0181,0.0233,-0.0059,0.0303,-0.0244,-0.0247,-0.0394,-0.0093,-0.0126,-0.0562,0.0446,-0.0263,0.2106,0.0078,0.0506,-0.005,-0.0192,0.0154,-0.0022,-0.1373,0.0583,0.0696,0.0647,0.0173,-0.0313,-0.0352,-0.0328,0.0141,0.0103,-0.0676,-0.0605,-0.0247,-0.0937,0.014,-0.0574,0.0165,0.0209,-0.0388,0.0262,-0.0351,0.0006,-0.0043,-0.1002,0.0445,-0.0205,0.0253,0.0465,-0.025,0.0415,-0.1019,0.0704,0.0262,-0.0126,-0.0417,0.0537,0.0026,-0.0027,0.121,0.0103,-0.0176,0.0568,0.0046,0.0031,-0.1016,-0.0426,-0.047,0.0705,-0.0114,0.0402,0.048,0.059,0.0114,0.0757,-0.0003,0.0102,-0.0062,-0.0115,0.0007,-0.0247,-0.037,0.0399,0.0152,-0.2661,0.0074,0.0403,0.0427,-0.0147,0.0059,0.0338,0.0248,-0.0761,0.0191,-0.0234,-0.0065,0.0404,0.0115,0.0243,0.0206,0.0867,-0.034,0.0449,-0.0948,0.0294,0.0534,0.196,-0.0324,0.0101,0.0259,0.0083,-0.0127,0.0533,-0.0693,-0.0031,-0.0445,0.0668,-0.0167,0.0217,0.0957,-0.0735,0.0463,-0.0074,-0.0454,0.049,-0.0081,-0.0474,-0.0193,0.0838,-0.002,-0.0118,-0.0396,-0.0116,0.0408,-0.058,0.008,-0.0199,-0.0014,0.0273,0.0123,-0.0289,-0.0154,-0.0296,-0.0186,0.0449,-0.0524,0.0006,0.0258,-0.0077]}
{"key":"[CADA: Multi-scale Collaborative Adversarial Domain Adaptation for Unsupervised Optic Disc and Cup Segmentation] The diversity of retinal imaging devices poses a significant challenge: domain shift, which leads to performance degradation when applying the deep learning models trained on one domain to new testing domains. In this paper, we propose a multi-scale input along with multiple domain adaptors applied hierarchically in both feature and output spaces. The proposed training strategy and novel unsupervised domain adaptation framework, called Collaborative Adversarial Domain Adaptation (CADA), can effectively overcome the challenge. Multi-scale inputs can reduce the information loss due to the pooling layers used in the network for feature extraction, while our proposed CADA is an interactive paradigm that presents an exquisite collaborative adaptation through both adversarial learning and ensembling weights at different network layers. In particular, to produce a better prediction for the unlabeled target domain data, we simultaneously achieve domain invariance and model generalizability via adversarial learning at multi-scale outputs from different levels of network layers and maintaining an exponential moving average (EMA) of the historical weights during training. Without annotating any sample from the target domain, multiple adversarial losses in encoder and decoder layers guide the extraction of domain-invariant features to confuse the domain classifier. Meanwhile, the ensembling of weights via EMA reduces the uncertainty of adapting multiple discriminator learning. Comprehensive experimental results demonstrate that our CADA model incorporating multi-scale input training can overcome performance degradation and outperform state-of-the-art domain adaptation methods in segmenting retinal optic disc and cup from fundus images stemming from the REFUGE, Drishti-GS, and Rim-One-r3 datasets.","layer":0,"vector":[-0.0278,-0.0485,0.0414,-0.015,0.0596,0.0318,0.0392,-0.0084,-0.0293,0.0077,-0.0022,-0.0857,-0.011,0.0584,0.0045,-0.0112,-0.0098,0.0185,-0.0174,0.0079,0.0368,-0.022,0.0081,-0.0384,0.0127,0.0161,-0.0151,-0.0301,-0.0468,-0.264,0.0484,-0.0332,0.0088,-0.0083,0.0349,-0.0389,-0.0675,0.0262,0.0029,0.0449,0.0025,-0.0152,-0.068,-0.0721,-0.0422,-0.0158,0.0045,-0.0121,0.0137,0.0054,0.0478,-0.0524,0.034,-0.0043,0.0058,0.0376,0.0586,0.0172,0.0383,0.0468,0.0373,0.0666,-0.1645,0.0828,0.0319,0.0034,-0.0418,-0.0149,-0.0178,0.0297,-0.0118,0.0433,0.0422,-0.0032,0.0103,-0.002,0.0044,-0.035,-0.0159,0.0403,0.0516,0.0236,-0.0521,0.0119,0.0178,-0.0061,0.0067,-0.0101,0.0425,-0.0316,-0.0455,-0.0542,-0.0039,-0.0009,-0.0748,-0.0248,0.0339,-0.0105,-0.0567,0.2186,-0.0398,0.0203,0.059,-0.0479,0.001,0.0014,-0.0134,-0.0115,0.0187,0.0415,-0.0107,-0.0063,0.001,0.0202,0.0151,-0.0062,0.0127,0.0325,-0.0041,-0.0152,-0.0388,0.0117,0.0514,-0.0486,0.0365,-0.0396,0.0073,0.1354,0.064,0.0234,0.0262,-0.0055,-0.0531,0.0043,0.0169,0.0088,0.0095,-0.0237,-0.0156,0.0276,-0.0088,-0.0621,0.0828,-0.0419,0.0204,0.136,-0.0723,0.023,-0.0406,-0.0234,0.0124,0.0218,-0.0364,-0.0134,0.0282,0.0197,0.0478,0.0179,-0.0515,0.0692,-0.0149,-0.0471,-0.0078,0.0939,0.0166,-0.1091,-0.0567,-0.022,0.0152,-0.0217,0.0085,0.0116,-0.0155,0.0457,0.0862,0.0177,-0.1152,-0.0303,-0.0051,0.0179,0.0378,-0.0692,-0.0524,0.0305,0.059,-0.0188,-0.0061,-0.0396,0.0277,0.0299,-0.0226,0.0185,-0.0733,0.0289,-0.0545,-0.0225,-0.0434,-0.0446,-0.022,-0.0366,0.019,-0.0135,-0.0319,0.0154,-0.0055,0.013,-0.008,-0.0252,0.0038,0.0544,-0.0373,0.0161,0.0506,-0.0074,-0.0039,-0.006,-0.0024,0.0483,-0.024,0.0381,0.0625,-0.0607,-0.0535,-0.2476,-0.01,-0.0103,-0.0503,0.0456,-0.0513,0.0588,0.0287,0.0846,0.0679,0.0451,0.0228,-0.022,0.0264,-0.0238,0.0651,0.0277,0.0009,-0.0243,-0.048,0.0185,0.0821,0.0004,-0.0704,0.0689,-0.0088,0.1927,0.0396,0.0479,0.0004,0.0145,0.0706,0.0166,-0.1033,0.0141,0.0315,0.0435,-0.0299,-0.0495,0.0063,-0.0152,0.0394,0.0398,-0.1234,-0.0639,-0.0156,-0.0297,0.073,-0.0296,0.0323,0.0403,-0.0471,0.0651,0.0214,-0.0306,-0.0433,-0.0956,0.0624,-0.0506,-0.0095,0.0308,-0.025,0.03,-0.1069,0.0526,0.0208,-0.0287,-0.068,0.0687,-0.0116,-0.0469,0.0227,-0.0116,-0.0032,0.037,0.0024,0.0539,-0.0157,-0.0691,-0.0372,0.0742,-0.0555,0.0057,0.0296,0.0617,0.0098,0.0893,0.0133,-0.0247,-0.0131,-0.0088,-0.0197,-0.0791,-0.0133,0.0232,0.0113,-0.2761,0.0016,0.0768,0.0114,-0.032,0.0555,0.0129,0.051,-0.0318,-0.0048,-0.0314,0.0078,0.0281,-0.0053,0.0158,0.0343,0.0492,-0.0232,0.0361,-0.0296,0.0051,0.0564,0.2038,-0.0865,0.0281,-0.0174,-0.0032,0.0315,0.0048,-0.0096,-0.0079,0.072,0.0671,-0.0405,0.053,0.1121,-0.0307,0.0097,-0.005,-0.0193,0.0098,0.0404,-0.0318,-0.0042,0.0675,0.0323,-0.0121,-0.0068,-0.0377,0.0332,-0.0293,-0.0228,-0.0023,0.0197,0.0095,-0.0168,-0.0364,-0.0616,-0.0691,-0.0507,-0.0127,-0.0501,-0.0602,-0.0067,-0.0219]}
{"key":"[Analysis & Computational Complexity Reduction of Monocular and Stereo Depth Estimation Techniques] Accurate depth estimation with lowest compute and energy cost is a crucial requirement for unmanned and battery operated autonomous systems. Robotic applications require real time depth estimation for navigation and decision making under rapidly changing 3D surroundings. A high accuracy algorithm may provide the best depth estimation but may consume tremendous compute and energy resources. A general trade-off is to choose less accurate methods for initial depth estimate and a more accurate yet compute intensive method when needed. Previous work has shown this trade-off can be improved by developing a state-of-the-art method (AnyNet) to improve stereo depth estimation. We studied both the monocular and stereo vision depth estimation methods and investigated methods to reduce computational complexity of these methods. This was our baseline. Consequently, our experiments show reduction of monocular depth estimation model size by ~75% reduces accuracy by less than 2% (SSIM metric). Our experiments with the novel stereo vision method (AnyNet) show that accuracy of depth estimation does not degrade more than 3% (three pixel error metric) in spite of reduction in model size by ~20%. We have shown that smaller models can indeed perform competitively.","layer":3,"vector":[-0.0641,-0.0128,0.0355,-0.0182,0.0338,0.0893,0.0326,0.0799,-0.0075,0.0063,0.0424,-0.0576,0.0305,0.0544,0.0134,-0.0259,-0.0092,0.0216,0.0061,0.0034,0.0543,-0.0694,0.0108,-0.0316,0.0316,0.0297,-0.0211,-0.0527,-0.0365,-0.2385,0.0052,-0.0569,0.0478,-0.03,0.0011,0.001,-0.018,0.0308,-0.0349,0.0207,0.0465,0.0523,0.006,-0.0489,-0.0471,-0.042,-0.0357,0.0007,0.0266,-0.0677,0.0283,-0.0533,0.0289,-0.0017,0.0052,0.0123,0.0422,0.0467,0.0333,0.0235,0.056,0.0482,-0.2325,0.0613,0.0786,0.0135,-0.0128,-0.0604,0.0258,0.0202,-0.0179,0.0141,0.0278,0.0029,-0.0008,-0.0684,0.0209,-0.0382,0.0219,-0.043,-0.0121,-0.0183,-0.0555,0.0172,0.0063,0.0037,0.025,-0.0169,0.0471,-0.0163,-0.0277,-0.0055,-0.0222,0.0174,-0.0436,0.0159,0.0393,-0.0432,-0.0535,0.238,-0.0033,0.0471,0.0629,-0.0338,0.03,-0.0176,-0.0151,-0.0719,-0.0024,0.0561,0.0072,0.0214,0.0427,0.0164,-0.0226,0.0349,0.0218,0.0465,0.0139,0.0052,-0.0466,0.0016,0.0578,-0.0035,0.0096,-0.0715,0.038,0.1004,0.0169,0.074,0.0606,-0.0465,-0.07,-0.0324,0.0254,0.0063,0.0274,0.0075,-0.0049,-0.0212,-0.0504,-0.071,0.0298,-0.1226,-0.0365,0.1018,-0.0429,0.0477,-0.092,-0.0356,0.0045,0.059,-0.0012,0.0224,0.0387,-0.0116,0.0105,0.0573,-0.095,0.0452,-0.0391,-0.0224,-0.0273,0.1083,0.0268,-0.0884,-0.0472,-0.0167,-0.0095,-0.0123,0.0174,0.0278,-0.0222,-0.0156,0.0792,0.0365,-0.0869,0.0025,-0.009,-0.0323,0.0131,-0.0312,0.0281,-0.0056,0.0498,-0.0317,-0.0233,0.0121,-0.0043,0.0407,-0.0048,0.0225,-0.0461,-0.0319,0.0205,0.0195,-0.0187,-0.0261,0.0218,-0.0219,0.0292,-0.0164,0.0144,-0.0163,-0.0007,0.0232,-0.019,-0.0154,-0.0051,0.0544,-0.073,-0.0229,0.0628,-0.0489,-0.0106,0.0232,0.0498,0.0084,-0.0053,0.0207,-0.0008,-0.0968,-0.0589,-0.2074,0.0548,0.0131,-0.0262,0.0332,-0.0515,0.0116,0.0145,0.0189,0.01,0.072,-0.0039,0.0074,0.0559,-0.0339,0.0947,-0.0015,0.0531,-0.056,0.0036,0.0065,0.0426,-0.0107,-0.0843,0.0269,-0.0427,0.2274,-0.0285,0.0611,-0.0043,0.027,-0.0078,-0.0452,-0.0906,0.0219,0.0527,0.0719,-0.0067,-0.0113,-0.054,-0.0378,0.0123,0.0281,-0.0651,-0.0119,-0.0507,-0.0217,0.0174,-0.0032,-0.0096,0.0188,-0.0524,-0.0085,-0.0182,-0.062,-0.0253,-0.0654,0.0103,-0.0573,0.0364,0.0036,-0.02,-0.0252,-0.0792,0.0617,0.0008,-0.0161,-0.0789,-0.0094,-0.0178,-0.0168,0.1001,0.0556,0.0172,0.0475,-0.0031,0.0328,-0.0206,-0.0144,-0.0249,0.0476,-0.0221,0.0261,0.0373,0.0604,0.023,0.0105,-0.0307,-0.0042,-0.0345,0.0068,0.0047,-0.0287,-0.0194,0.0535,0.0137,-0.2795,0.0454,-0.0073,-0.0142,-0.0185,-0.0103,0.0474,0.0357,-0.0298,-0.0125,-0.0329,0.0411,-0.0051,-0.0081,-0.0142,0.0524,0.0622,-0.036,0.0732,-0.0411,0.0234,0.0406,0.1922,-0.0819,0.0181,0.0446,-0.043,-0.0205,-0.0116,-0.0284,-0.0155,0.0021,0.0554,-0.0715,0.0865,0.1222,-0.0017,0.0236,0.0436,0.0126,-0.0385,-0.0099,0.0422,-0.0199,0.1044,0.0051,-0.029,0.0063,0.0107,0.0174,-0.0552,-0.005,-0.0031,0.03,0.0437,0.0489,-0.0477,-0.0411,-0.0566,-0.0038,0.0167,-0.0554,-0.0442,0.0194,0.0042]}
{"key":"[Conversational Semantic Parsing] The structured representation for semantic parsing in task-oriented assistant systems is geared towards simple understanding of one-turn queries. Due to the limitations of the representation, the session-based properties such as co-reference resolution and context carryover are processed downstream in a pipelined system. In this paper, we propose a semantic representation for such task-oriented conversational systems that can represent concepts such as co-reference and context carryover, enabling comprehensive understanding of queries in a session. We release a new session-based, compositional task-oriented parsing dataset of 20k sessions consisting of 60k utterances. Unlike Dialog State Tracking Challenges, the queries in the dataset have compositional forms. We propose a new family of Seq2Seq models for the session-based parsing above, which achieve better or comparable performance to the current state-of-the-art on ATIS, SNIPS, TOP and DSTC2. Notably, we improve the best known results on DSTC2 by up to 5 points for slot-carryover.","layer":1,"vector":[-0.0891,0.0026,0.0117,-0.0353,-0.0459,-0.0221,0.0878,-0.0094,-0.0071,0.0075,-0.0275,-0.0409,0.0681,0.0514,0.0452,0.0119,0.0443,-0.0256,-0.0238,0.021,0.0276,-0.065,-0.0097,-0.0199,-0.0244,0.043,-0.0737,-0.0287,-0.0544,-0.2313,0.0191,-0.0273,0.0188,0.0162,0.0007,-0.0112,-0.0122,0.0415,-0.0229,0.0388,0.0597,0.0103,-0.0397,-0.0424,-0.0545,-0.0584,-0.0522,-0.0457,-0.0383,-0.0325,-0.0262,-0.002,0.0261,0.0222,0.0439,0.0791,0.0273,0.0186,0.0255,0.0276,-0.0154,-0.0018,-0.1534,0.1033,0.018,0.0073,-0.0426,-0.0129,-0.0018,0.038,-0.0389,0.038,-0.0194,0.0579,0.0287,-0.0133,0.0137,-0.0212,-0.0078,0.0332,-0.0425,-0.0221,0.0124,0.014,-0.0076,-0.0541,-0.0184,0.0052,0.0446,-0.0093,-0.0668,-0.0437,-0.022,0.043,-0.0158,-0.0055,0.0138,0.0253,-0.0337,0.2234,-0.0366,0.031,0.02,-0.066,-0.0014,-0.0461,-0.0099,-0.0757,-0.0025,-0.0244,-0.031,0.0455,0.0306,-0.0563,0.0444,-0.0096,0.1019,0.0161,-0.0152,-0.0147,-0.0318,0.0302,0.0293,-0.0502,0.0448,-0.0747,0.0798,0.1235,0.0226,0.0187,0.0573,0.0148,-0.043,0.0106,0.024,0.0215,0.0602,-0.035,0.0244,-0.0516,-0.0091,-0.1006,0.0116,-0.072,-0.0318,0.1729,-0.0096,-0.0121,-0.0746,-0.0147,-0.0405,0.082,0.0341,-0.0348,0.0237,0.0263,0.0592,0.0315,-0.0385,0.0018,0.037,-0.0459,-0.0221,0.0803,0.0407,-0.1123,-0.0718,-0.0168,0.0277,-0.0285,0.069,-0.0007,-0.0609,0.0376,0.0624,0.0344,-0.0644,0.0092,-0.0003,0.0307,0.041,-0.0559,-0.011,0.0295,-0.0404,-0.0723,0.0026,-0.0387,0.0425,0.0017,-0.03,0.0093,-0.0049,0.0054,0.0011,0.009,0.0136,0.0071,-0.0234,-0.0216,0.0411,0.0283,-0.0822,0.0407,-0.0081,0.0039,-0.0359,0.0277,0.0134,0.0006,-0.0215,0.0019,0.0216,0.0077,-0.0154,-0.0411,0.0147,-0.0173,0.0304,0.0568,0.0198,0.0004,-0.019,-0.2448,-0.0148,0.0149,-0.0373,0.0178,-0.0468,0.0077,0.0081,0.04,0.0701,0.0101,-0.0588,-0.0095,0.0333,-0.0101,0.0645,0.0375,0.0285,0.0073,0.008,0.0385,0.0192,-0.051,-0.0526,0.0475,0.0066,0.2306,0.0279,0.0117,-0.0226,0.0427,0.0292,-0.0356,-0.1129,0.0387,0.0064,0.0408,0.0278,0.0039,-0.0595,-0.0391,0.0452,-0.0078,-0.1042,-0.04,-0.0141,-0.0174,-0.0283,-0.0318,0.0387,0.019,-0.0612,0.0182,0.0028,-0.0219,-0.0244,-0.0649,0.0107,-0.0241,-0.0115,-0.0284,-0.0011,0.0317,-0.0208,0.0486,0.0238,-0.0327,-0.0325,0.0035,-0.0713,-0.0482,0.0616,-0.0217,0.0194,0.0347,0.0526,0.0185,-0.0498,-0.0429,-0.0443,0.0794,-0.0551,0.0375,-0.0184,0.0221,0.0272,0.1139,0.0307,0.0714,-0.0439,-0.0003,0.0158,-0.0444,-0.0376,0.0525,-0.025,-0.2917,0.0436,0.018,0.0193,-0.0439,0.0329,0.0676,0.0089,-0.0771,0.0268,0.0284,0.076,-0.0148,0.0455,-0.0462,0.0581,0.0828,-0.0058,0.0492,-0.0382,-0.0042,0.0521,0.1945,0.0002,0.0754,0.0167,-0.0077,-0.0168,0.0499,0.0054,-0.0236,-0.01,0.102,-0.0111,0.0132,0.0197,0.0131,0.0098,0.0429,0.013,-0.0107,0.0076,-0.0108,-0.0048,0.0622,-0.0167,-0.0252,-0.115,0.0121,-0.0195,0.0015,0.0038,-0.0009,-0.0152,0.0286,0.0224,-0.0298,-0.0471,-0.0167,-0.0078,0.0348,-0.0348,0.0094,0.0407,-0.0181]}
{"key":"[Attention augmented differentiable forest for tabular data] Differentiable forest is an ensemble of decision trees with full differentiability. Its simple tree structure is easy to use and explain. With full differentiability, it would be trained in the end-to-end learning framework with gradient-based optimization method. In this paper, we propose tree attention block(TAB) in the framework of differentiable forest. TAB block has two operations, squeeze and regulate. The squeeze operation would extract the characteristic of each tree. The regulate operation would learn nonlinear relations between these trees. So TAB block would learn the importance of each tree and adjust its weight to improve accuracy. Our experiment on large tabular dataset shows attention augmented differentiable forest would get comparable accuracy with gradient boosted decision trees(GBDT), which is the state-of-the-art algorithm for tabular datasets. And on some datasets, our model has higher accuracy than best GBDT libs (LightGBM, Catboost, and XGBoost). Differentiable forest model supports batch training and batch size is much smaller than the size of training set. So on larger data sets, its memory usage is much lower than GBDT model. The source codes are available at https://github.com/closest-git/QuantumForest.","layer":1,"vector":[-0.0463,0.0149,0.0348,-0.0271,0.0334,0.0194,0.0448,0.0347,0.046,0.0192,-0.0166,-0.0243,0.0305,0.0904,0.0046,0.0468,-0.0196,0.0294,-0.0678,-0.0025,0.0328,0.0086,-0.0102,-0.0548,0.0127,-0.0076,-0.0491,-0.0444,-0.0225,-0.2397,0.0411,-0.0238,0.0162,-0.0396,0.0199,-0.0232,-0.0309,0.011,-0.0422,0.0305,-0.0057,0.019,-0.0399,-0.0133,-0.0229,-0.0779,-0.0585,-0.0191,-0.0215,-0.0043,0.0001,-0.0447,0.052,0.0013,0.0376,0.0501,0.0472,0.0418,0.024,0.0069,0.0598,0.0208,-0.1333,0.0614,0.0696,0.0058,-0.0481,-0.0082,0.0432,0.0446,-0.055,0.0336,-0.0006,0.0342,0.0201,-0.0236,-0.0098,0.0108,-0.0246,-0.006,-0.0017,-0.0328,-0.0934,-0.0164,-0.0089,-0.0291,-0.0346,-0.0272,0.0466,-0.0029,-0.025,0.0118,-0.0355,-0.0189,-0.0517,-0.009,0.0409,0.0245,-0.0664,0.2176,-0.0636,0.0429,0.0049,-0.0503,-0.0166,-0.0395,-0.0185,-0.0375,-0.0334,0.0267,0.0036,-0.0498,0.0303,-0.0088,-0.0288,0.0125,0.0743,0.0397,-0.0077,-0.003,-0.0302,0.0048,0.017,-0.0426,0.0503,-0.0593,0.0111,0.1282,0.0256,0.0392,0.0592,-0.0574,-0.027,-0.025,0.0365,0.0017,0.0295,0.0023,0.0444,-0.033,-0.0594,-0.0368,0.0185,-0.1015,-0.0625,0.0928,-0.0414,0.0417,-0.0832,-0.035,0.0199,0.0122,0.016,-0.0282,0.0246,0.0402,0.0172,0.0527,-0.0214,0.0096,-0.0183,-0.0175,-0.0121,0.0859,0.0224,-0.0757,-0.036,-0.0468,0.0261,-0.004,0.0892,0.0557,-0.0485,0.046,0.0714,-0.034,-0.0544,-0.0288,-0.0173,-0.0006,0.0604,-0.0569,-0.0198,0.0413,0.0275,-0.0429,0.011,-0.0415,-0.0065,0.0189,-0.0223,0.0161,-0.0081,0.0156,0.0032,-0.0427,-0.0226,-0.036,0.0294,-0.0147,0.0331,0.0258,-0.0667,0.0281,-0.0278,0.0144,0.0081,-0.023,0.0477,-0.0306,-0.053,-0.005,0.0364,-0.0384,-0.0812,0.002,0.0331,0.0392,-0.0385,0.0683,0.0352,-0.0386,-0.054,-0.2216,-0.0359,0.0107,-0.0064,0.0364,-0.0336,0.0078,0.0123,0.0081,0.1027,0.0638,-0.0233,-0.043,0.0434,-0.0052,0.0577,0.0652,0.0265,-0.0191,-0.0013,-0.003,0.0474,-0.0198,-0.0837,0.04,0.0102,0.2023,0.012,0.0391,-0.012,0.019,0.0446,-0.017,-0.1021,0.0614,0.0264,-0.0068,-0.0053,-0.0452,0.007,-0.0123,0.0371,0.0054,-0.1325,-0.0369,-0.0122,-0.0296,0.0253,-0.0678,0.0197,0.0626,-0.0399,0.0392,0.0322,0.0251,-0.0572,-0.0833,0.058,-0.0325,-0.0066,0.0038,-0.0682,0.0042,-0.0286,-0.0126,-0.0377,-0.0686,-0.0111,0.0316,-0.0513,-0.0721,0.0465,0.0373,-0.0139,0.0463,0.0059,0.0744,0.0164,-0.0301,-0.0128,0.0475,-0.0156,0.0549,-0.0063,0.0414,0.0398,0.0956,-0.02,0.0404,-0.0284,0.0126,-0.0074,-0.0284,-0.0196,0.082,0.0025,-0.3295,0.0536,-0.0038,-0.0063,-0.0155,0.0149,0.0364,0.0169,-0.0175,0.0074,-0.0043,0.0361,0.0228,-0.0072,0.0063,0.0252,0.0863,-0.0334,0.017,-0.0162,0.0518,0.0366,0.2411,0.013,0.0401,0.0174,-0.0468,-0.0148,0.0098,-0.0183,-0.022,0.0048,0.1064,-0.0489,0.0365,0.0945,-0.0266,0.0808,0.0045,-0.0412,0.0281,0.0241,-0.0679,-0.019,0.1305,0.0155,-0.0038,-0.0269,0.0145,0.0244,-0.0123,0.0381,-0.0484,-0.0083,0.0159,0.0321,0.0047,-0.0187,-0.0251,-0.0251,0.0149,-0.0648,-0.0388,-0.0387,0.0301]}
{"key":"[Biologically-Inspired Spatial Neural Networks] We introduce bio-inspired artificial neural networks consisting of neurons that are additionally characterized by spatial positions. To simulate properties of biological systems we add the costs penalizing long connections and the proximity of neurons in a two-dimensional space. Our experiments show that in the case where the network performs two different tasks, the neurons naturally split into clusters, where each cluster is responsible for processing a different task. This behavior not only corresponds to the biological systems, but also allows for further insight into interpretability or continual learning.","layer":1,"vector":[-0.033,-0.0423,0.0522,-0.012,0.014,-0.0134,0.038,0.0278,0.0444,-0.0322,0.0109,-0.0852,0.0214,0.0746,0.0036,-0.0411,-0.0188,0.0841,-0.0615,-0.0091,0.0212,-0.0054,-0.0109,-0.0365,-0.0247,0.0316,-0.007,-0.0149,-0.0471,-0.2021,0.0171,-0.0096,0.0369,-0.0142,-0.0193,-0.0164,-0.0242,0.0474,-0.0257,0.0294,0.0246,0.018,-0.0099,-0.0362,0.0005,-0.0381,-0.0285,-0.011,-0.0014,-0.0599,0.006,-0.0398,0.0009,0.0328,0.0296,0.0201,-0.0089,0.0601,0.0139,0.0334,0.0893,0.0375,-0.1248,0.0409,0.0724,0.0057,-0.0413,-0.0196,0.0223,0.0203,-0.0189,0.0508,-0.0079,0.0363,0.0529,-0.0243,-0.0194,-0.0088,0.0327,0.0306,0.0228,-0.0047,-0.0032,-0.0033,-0.0108,-0.001,0.0037,-0.0367,-0.0125,0.0285,-0.0557,0.0032,-0.0551,0.0129,-0.0564,-0.0572,0.0446,-0.0026,-0.017,0.1859,-0.0641,0.0258,0.0546,-0.0404,0.0264,-0.0391,-0.0298,-0.0398,-0.0394,0.0121,-0.0067,0.002,-0.0422,-0.016,0.0012,0.0492,0.0226,0.0233,-0.002,-0.0227,-0.0469,0.0115,0.0261,-0.0137,0.0264,-0.0507,0.0051,0.1524,0.0439,0.032,0.0552,0.0041,-0.015,-0.0046,0.0274,0.0303,0.0192,-0.0291,-0.0141,0.0033,-0.0234,-0.0332,0.051,-0.1009,-0.0357,0.0904,-0.0043,0.0045,-0.0306,0.0128,-0.0516,0.0042,-0.022,-0.0329,0.0063,0.0657,0.0179,-0.0008,-0.0473,0.0118,-0.0318,-0.0374,-0.0322,0.1071,0.0686,-0.0382,-0.0479,0.0164,0.0544,0.0049,0.047,0.0288,-0.0173,0.04,0.0218,0.0338,-0.0828,-0.0226,0.0185,0.0017,0.0155,-0.058,-0.0091,0.0435,-0.0075,-0.0376,-0.0128,-0.0433,-0.0325,0.0579,-0.0359,0.0431,0.0026,0.0287,-0.0539,-0.051,0.0062,-0.0244,-0.0038,-0.025,0.0151,-0.0156,-0.0833,-0.0153,0.0297,-0.0038,0.0282,0.0001,0.0546,0.0149,-0.0189,0.0185,0.1006,-0.064,-0.0124,-0.0037,-0.022,-0.008,0.0366,0.0404,0.0335,-0.0438,-0.0888,-0.244,-0.0019,0.0041,-0.0364,0.0475,-0.0454,0.0109,0.0102,0.0286,0.0245,0.0571,-0.0116,-0.0164,0.0189,-0.014,0.0668,0.0309,0.0309,-0.0371,0.0035,0.0289,0.0325,0.0084,-0.0953,0.0547,-0.0003,0.2231,0.0352,0.0386,-0.0123,0.0427,0.0195,-0.0034,-0.1124,0.0519,0.0332,0.05,-0.0098,-0.0132,-0.0383,-0.0377,0.0384,-0.0119,-0.1002,-0.0392,-0.0131,-0.0356,0.0208,-0.0289,-0.0156,0.0256,-0.0636,0.0233,-0.0221,0.016,-0.0271,-0.091,0.0332,-0.0702,0.0413,0.0267,-0.0788,-0.0022,-0.0344,0.1107,0.047,-0.0426,-0.0367,0.008,-0.0047,-0.0073,0.1061,0.0465,-0.0232,0.0853,-0.0494,0.0551,-0.0216,-0.0375,0.027,0.0265,-0.0503,0.0562,0.0011,0.046,-0.0072,0.1017,-0.0475,0.0538,-0.0149,0.0128,0.0437,-0.0692,0.0275,0.0228,-0.0138,-0.3053,0.0632,0.0063,0.0554,-0.0433,0.0074,-0.0049,0.0403,-0.0273,-0.0133,-0.018,0.017,0.0439,0.0247,0.0143,0.0706,0.0738,-0.014,0.0822,-0.0765,0.0089,0.0352,0.2415,-0.0521,0.0634,-0.0179,-0.0339,-0.0378,0.0094,-0.0157,0.001,0.0379,0.0893,-0.0969,0.0392,0.0852,-0.0361,0.056,0.0641,-0.0424,-0.0307,0.0219,-0.0558,-0.0392,0.1213,-0.0176,-0.0441,-0.0098,-0.0232,0.0332,-0.0259,0.0002,0.003,-0.0506,0.0107,0.0083,-0.0614,-0.0501,-0.0759,-0.0447,0.0223,-0.0929,0.0121,-0.0068,-0.0445]}
{"key":"[XGPT: Cross-modal Generative Pre-Training for Image Captioning] While many BERT-based cross-modal pre-trained models produce excellent results on downstream understanding tasks like image-text retrieval and VQA, they cannot be applied to generation tasks directly. In this paper, we propose XGPT, a new method of Cross-modal Generative Pre-Training for Image Captioning that is designed to pre-train text-to-image caption generators through three novel generation tasks, including Image-conditioned Masked Language Modeling (IMLM), Image-conditioned Denoising Autoencoding (IDA), and Text-conditioned Image Feature Generation (TIFG). As a result, the pre-trained XGPT can be fine-tuned without any task-specific architecture modifications to create state-of-the-art models for image captioning. Experiments show that XGPT obtains new state-of-the-art results on the benchmark datasets, including COCO Captions and Flickr30k Captions. We also use XGPT to generate new image captions as data augmentation for the image retrieval task and achieve significant improvement on all recall metrics.","layer":0,"vector":[-0.0211,-0.0,0.0375,0.0106,0.0272,0.0307,0.0167,0.0091,0.0067,-0.0009,0.0169,-0.0751,0.0853,0.0752,0.0183,0.0065,0.029,0.0446,-0.075,-0.0052,0.0274,-0.0319,0.0345,-0.0692,0.0194,-0.0418,0.0073,-0.0594,-0.0332,-0.2169,-0.0099,-0.0267,0.0291,-0.0219,-0.0132,-0.0137,-0.09,0.0584,-0.0514,0.0505,0.009,0.0324,-0.0372,-0.0291,-0.0363,-0.0675,-0.0119,-0.063,-0.03,-0.0564,0.0339,-0.0056,0.0201,0.0135,-0.0255,0.0738,0.0178,0.0472,0.0382,0.0391,0.0132,0.0365,-0.1811,0.1044,0.0258,0.0507,-0.0715,0.0147,0.0102,0.0167,-0.0255,0.048,0.0248,0.0401,0.0277,0.0014,-0.0121,-0.0353,-0.0515,-0.0191,0.04,0.009,-0.0288,-0.022,-0.0032,-0.0409,0.018,-0.0228,0.0449,0.0334,-0.0188,-0.0346,-0.0236,0.0245,-0.0654,-0.0189,0.0121,0.0466,-0.057,0.2189,-0.0454,0.0271,0.0236,-0.0645,0.0088,-0.0299,-0.0202,0.012,-0.0281,-0.0328,-0.0117,-0.0053,0.074,-0.0364,0.0546,-0.0204,0.0797,-0.0127,0.0002,0.0023,-0.0158,0.0027,0.008,0.0087,0.0269,-0.0533,0.0233,0.1055,0.0257,-0.0004,0.036,0.0038,-0.0411,-0.0488,0.0168,-0.0024,0.0143,-0.0144,-0.0049,-0.0397,-0.0016,-0.0521,0.0074,-0.067,-0.06,0.1194,-0.0093,0.0484,-0.0542,-0.0356,0.0314,0.0053,0.0092,-0.0105,0.0278,0.0439,0.0335,0.0736,0.001,0.0193,0.0298,-0.0526,-0.0454,0.0469,0.0408,-0.095,-0.0317,-0.0106,0.0024,-0.0154,0.0486,-0.0038,-0.0409,-0.0042,0.0561,0.047,-0.0469,-0.0011,-0.0302,0.0112,0.0071,-0.0862,-0.0014,0.0543,0.0341,-0.0863,-0.0025,-0.0268,0.076,0.0221,0.0185,0.0291,-0.0474,0.0264,-0.0015,0.0043,-0.027,-0.0443,0.0021,-0.0634,0.0151,0.0204,-0.0178,0.0014,-0.0251,-0.0194,0.0232,0.0131,0.0616,0.06,0.0072,-0.0023,0.0549,-0.0237,-0.0212,0.0079,0.0244,0.0399,0.0269,0.0498,0.0211,-0.0143,-0.0234,-0.2696,-0.0094,-0.0162,-0.028,0.0281,-0.0932,0.0371,-0.029,0.0725,0.058,0.0362,-0.0475,0.0208,0.0265,-0.0069,0.029,0.0457,0.0698,0.0171,-0.0002,0.0,-0.0138,-0.0001,-0.0962,0.0397,-0.0289,0.2287,0.0646,0.055,-0.0222,0.039,0.0699,-0.0442,-0.1045,0.047,0.0298,0.0532,0.03,-0.0454,-0.0262,0.0031,0.0415,0.0104,-0.0781,-0.0187,-0.0533,-0.0586,0.0124,-0.0609,0.0569,0.0143,-0.0481,0.004,0.0002,-0.0458,-0.0583,-0.1265,-0.0394,-0.0682,0.0186,0.0071,-0.0269,0.0021,-0.0907,0.0347,0.028,-0.0437,-0.0399,0.0122,-0.0225,-0.0398,0.0646,0.0135,0.0366,0.0377,0.0258,0.0162,-0.0414,-0.0614,-0.0651,0.0662,-0.0072,0.0234,-0.0204,0.0147,0.0149,0.0724,-0.0019,0.0363,-0.0103,-0.0035,0.0431,-0.0507,-0.0294,0.0264,-0.058,-0.3054,0.0206,0.0059,0.0384,0.0158,0.0444,0.0588,0.0245,-0.0329,0.0086,-0.0392,0.0299,0.0741,-0.0338,-0.0626,0.028,0.0749,-0.0384,0.0703,-0.0161,-0.0111,0.0122,0.2024,0.0032,0.0137,-0.0034,-0.0401,-0.0225,0.0454,-0.0052,0.0063,0.0131,0.0811,-0.0116,0.0116,0.1039,-0.0401,0.056,-0.0006,-0.0288,0.0044,0.0185,-0.0053,-0.0506,0.0431,0.0081,0.0398,0.0035,-0.0053,-0.0165,-0.0163,0.0156,-0.0321,-0.0193,0.0412,0.0429,-0.0145,-0.0628,-0.0221,-0.0027,0.0433,-0.0294,-0.0466,0.0408,-0.0199]}
{"key":"[A Level Set Theory for Neural Implicit Evolution under Explicit Flows] Coordinate-based neural networks parameterizing implicit surfaces have emerged as efficient representations of geometry. They effectively act as parametric level sets with the zero-level set defining the surface of interest. We present a framework that allows applying deformation operations defined for triangle meshes onto such implicit surfaces. Several of these operations can be viewed as energy-minimization problems that induce an instantaneous flow field on the explicit surface. Our method uses the flow field to deform parametric implicit surfaces by extending the classical theory of level sets. We also derive a consolidated view for existing methods on differentiable surface extraction and rendering, by formalizing connections to the level-set theory. We show that these methods drift from the theory and that our approach exhibits improvements for applications like surface smoothing, mean-curvature flow, inverse rendering and user-defined editing on implicit geometry.","layer":1,"vector":[-0.0366,-0.0295,0.0566,0.0332,-0.0051,0.0538,-0.0122,0.0281,0.0051,-0.0079,0.0083,-0.0626,0.0601,0.0692,-0.0001,0.0019,-0.0126,0.0988,-0.0697,0.0341,0.0354,-0.0333,-0.0048,-0.0381,0.0195,0.019,-0.0306,-0.0129,-0.0091,-0.2367,0.0276,-0.0293,0.025,-0.0248,0.005,0.0085,-0.0507,0.1074,-0.0093,0.0333,-0.0128,0.0279,-0.0346,-0.0843,-0.0232,0.0206,-0.0433,0.0155,0.0006,-0.0441,0.0276,-0.0454,0.0298,0.0036,0.0406,0.0599,0.049,0.0139,0.0394,0.0577,0.0254,0.0114,-0.1685,0.0808,0.0317,0.031,-0.0058,-0.0605,0.0316,0.072,-0.0437,0.0361,0.0312,-0.0124,0.0445,-0.0387,0.0126,-0.0475,-0.0278,0.0007,0.0124,-0.0135,-0.0298,0.0088,-0.0061,-0.0264,0.0241,-0.022,0.0325,0.0203,-0.0634,-0.0339,-0.0672,0.005,-0.0442,-0.0131,0.0331,0.0219,-0.0224,0.2036,-0.0376,0.035,0.0365,-0.0161,0.03,-0.0335,-0.0009,-0.0108,-0.0365,0.0185,-0.0724,0.0209,-0.0233,-0.0055,0.0222,0.0111,0.0075,0.0338,-0.0414,-0.0343,-0.01,0.0248,-0.0172,-0.0332,0.0425,-0.0732,0.0205,0.1311,0.0649,0.0116,0.0542,0.0174,-0.0299,-0.0063,0.0069,0.0009,-0.0168,0.0091,-0.0126,0.0333,-0.0623,-0.0295,0.007,-0.0778,-0.0531,0.1056,-0.0449,0.07,-0.0563,-0.0243,-0.0595,0.0105,-0.0624,-0.0113,-0.0075,0.0187,-0.0036,0.0114,-0.0894,0.0184,-0.0134,-0.0637,-0.0506,0.1046,-0.0058,-0.0502,-0.0492,-0.012,0.0024,0.0103,0.0101,0.0442,-0.0461,-0.0218,0.0977,0.0559,-0.1029,-0.0156,0.0243,0.0519,0.0736,-0.0352,-0.0316,0.0101,-0.005,-0.0347,-0.0021,-0.0331,0.0469,0.0663,-0.0501,0.0037,-0.0595,-0.0089,0.026,-0.0105,0.0052,0.0463,0.0018,-0.03,0.0056,0.0143,-0.0006,-0.01,-0.0205,0.0243,-0.0488,0.0306,0.0244,0.0932,-0.0422,-0.0495,0.0476,-0.0588,-0.0023,0.0379,-0.0047,-0.0123,-0.036,0.0187,0.0303,-0.0795,-0.031,-0.2243,0.0021,-0.0038,-0.0661,0.0669,-0.0253,0.0264,0.0055,0.0128,0.0603,0.0529,-0.0137,0.0053,0.0074,-0.0137,0.0544,0.0116,-0.0148,-0.0253,-0.0436,-0.0282,0.0235,-0.035,-0.0935,0.0038,0.0524,0.2327,0.0135,0.0483,-0.0299,0.0087,-0.0244,-0.0296,-0.096,0.0512,0.0305,0.0717,-0.0143,-0.0211,-0.0685,-0.0454,0.0014,0.011,-0.0959,-0.0326,-0.0511,-0.0408,0.0104,-0.0454,0.03,0.0274,-0.0398,0.0421,-0.0155,-0.0405,-0.0624,-0.0632,0.0505,-0.0333,0.0512,-0.0019,-0.057,0.0056,-0.0539,0.0694,0.0364,-0.0063,-0.0435,0.0395,-0.0631,-0.0202,0.0612,0.0332,0.0247,0.0534,0.0368,0.0335,0.058,-0.0027,-0.0172,0.0597,-0.0401,0.052,0.0484,0.0105,0.0254,0.0431,-0.0524,0.0147,-0.0338,-0.0017,0.033,-0.0438,0.0373,0.0538,0.0031,-0.2907,0.0376,0.048,0.0152,0.0033,0.0059,0.0266,0.0229,-0.0071,-0.0255,-0.0472,0.0298,0.0225,0.0074,0.0229,0.0335,0.0618,-0.0553,0.0619,-0.0416,0.0012,0.054,0.2475,-0.0297,0.0355,0.0036,-0.0412,-0.0354,0.0808,0.0167,0.0115,0.0417,0.0705,-0.0509,0.0626,0.0881,-0.0142,0.0219,0.0351,-0.0201,-0.0137,0.0011,-0.0172,-0.0116,0.036,0.0072,-0.0605,0.0101,-0.0024,-0.0195,-0.0211,0.0313,-0.0089,0.0077,0.0494,0.0259,-0.0699,-0.0497,-0.0485,-0.0148,0.0341,-0.0884,0.0313,-0.0163,-0.0018]}
{"key":"[Pseudo-marginal Bayesian inference for supervised Gaussian process latent variable models] We introduce a Bayesian framework for inference with a supervised version of the Gaussian process latent variable model. The framework overcomes the high correlations between latent variables and hyperparameters by using an unbiased pseudo estimate for the marginal likelihood that approximately integrates over the latent variables. This is used to construct a Markov Chain to explore the posterior of the hyperparameters. We demonstrate the procedure on simulated and real examples, showing its ability to capture uncertainty and multimodality of the hyperparameters and improved uncertainty quantification in predictions when compared with variational inference.","layer":0,"vector":[-0.0425,-0.0123,0.0106,-0.0197,0.0546,-0.0052,0.0428,0.0135,0.0542,-0.029,0.0561,-0.0486,0.0141,0.0483,0.0356,0.0204,0.024,0.0919,-0.0483,-0.0096,0.0358,-0.0477,-0.0038,0.0028,0.0376,0.0056,-0.0328,-0.0176,-0.0401,-0.2354,-0.0087,-0.0487,0.0274,0.0162,0.0278,-0.0126,-0.0469,0.0206,0.0053,0.0536,0.0292,0.0266,-0.0588,-0.0299,-0.0317,-0.0837,-0.0233,-0.0087,-0.0589,-0.0393,-0.0001,-0.0439,0.0174,0.0306,0.0603,0.0649,0.0784,0.0422,0.0501,0.044,-0.002,0.0693,-0.2103,0.0837,0.0565,0.0287,-0.0902,-0.0317,-0.0025,0.0035,-0.0686,0.066,-0.0194,0.0629,0.0308,-0.0302,0.0028,-0.0036,-0.0246,-0.0023,-0.0003,0.0326,-0.0284,-0.0151,-0.0314,-0.0709,-0.0037,-0.0171,0.0441,-0.005,-0.0249,-0.0298,-0.0256,0.0099,-0.0538,0.002,0.0417,0.0179,0.0,0.1838,-0.0378,0.0114,0.0454,0.004,0.0222,-0.0213,-0.0628,-0.0461,0.0071,0.0499,0.003,-0.0187,0.0332,-0.0709,0.0621,0.0063,0.0647,0.0333,-0.0165,-0.0305,-0.0058,0.0094,0.0507,-0.0059,-0.0133,-0.098,-0.0071,0.1216,0.0284,-0.0105,0.0912,-0.0682,-0.0757,-0.0235,0.0194,0.0059,0.0509,-0.0117,0.028,-0.0185,-0.0577,-0.0385,0.0132,-0.0699,-0.0542,0.1252,-0.0291,0.0247,-0.0932,-0.0125,-0.0186,0.0476,-0.0023,-0.008,0.0287,0.0305,-0.0114,0.0301,-0.0532,0.0455,-0.0154,-0.0643,-0.0494,0.1015,0.0384,-0.0512,-0.0657,0.0208,0.0356,0.0306,0.065,0.0378,-0.0417,-0.0182,0.0759,0.0211,-0.03,0.0548,0.0322,0.0328,0.0008,-0.0276,-0.0086,0.0564,0.0571,-0.0649,0.0292,-0.0405,0.0186,0.0396,-0.0146,-0.0279,-0.017,-0.03,0.0202,-0.0227,-0.0287,-0.0337,0.0259,-0.0224,-0.0046,-0.0015,-0.0733,0.0288,-0.0321,0.0329,-0.0367,0.0206,0.0742,0.0425,-0.0106,0.0279,0.0697,-0.0004,-0.0303,-0.0093,-0.0094,0.0759,0.0201,0.0078,0.0126,-0.0341,-0.0649,-0.2294,0.0301,0.0019,-0.0154,0.0485,-0.0306,0.0222,0.0019,0.0539,0.0819,0.0293,-0.0285,-0.0509,0.023,0.0077,0.0425,0.0208,0.03,0.0075,0.0458,-0.0502,-0.0138,-0.0945,-0.0667,0.0549,0.0124,0.198,0.0127,-0.0075,-0.075,0.0197,0.0185,-0.0458,-0.0911,0.0489,0.0717,0.0605,-0.0046,-0.0529,-0.0259,-0.0472,-0.0131,-0.0164,-0.0974,-0.0784,-0.0079,-0.0174,0.0337,-0.0666,0.0179,0.0488,-0.0266,0.0843,-0.0576,-0.0076,-0.0625,-0.0972,0.0137,-0.046,0.0147,0.0333,-0.0383,0.0452,-0.0492,0.0241,-0.0428,-0.0273,-0.0567,0.0058,-0.0187,-0.0313,0.0921,-0.0102,-0.0045,0.0679,0.0163,0.0219,-0.0365,-0.078,-0.03,0.0472,-0.0376,0.0252,0.006,0.0179,-0.0466,0.064,-0.0073,0.0265,-0.0318,-0.0318,-0.0418,-0.0366,0.033,0.0246,0.0121,-0.2484,0.0397,0.0053,0.0096,-0.0401,-0.0148,0.078,0.0384,-0.0463,0.0223,0.0061,0.048,0.0607,0.0168,-0.0261,0.013,0.0603,-0.0518,0.0263,-0.0664,0.0046,0.0234,0.2034,-0.0102,0.0698,0.0085,-0.0174,0.0162,0.0558,-0.0529,0.0423,0.0094,0.07,-0.0131,0.065,0.0594,-0.0302,0.0715,0.0087,-0.019,0.0503,-0.0126,-0.0389,-0.0645,0.1132,-0.0145,-0.0032,-0.0118,-0.0347,0.0369,-0.0083,0.0146,-0.0099,-0.0159,0.0143,0.0114,0.0037,-0.0384,-0.0311,-0.0337,-0.03,-0.045,0.0059,0.0112,-0.0018]}
{"key":"[Multi-fidelity Stability for Graph Representation Learning] In the problem of structured prediction with graph representation learning (GRL for short), the hypothesis returned by the algorithm maps the set of features in the \\emph{receptive field} of the targeted vertex to its label. To understand the learnability of those algorithms, we introduce a weaker form of uniform stability termed \\emph{multi-fidelity stability} and give learning guarantees for weakly dependent graphs. We testify that ~\\citet{london2016stability}'s claim on the generalization of a single sample holds for GRL when the receptive field is sparse. In addition, we study the stability induced bound for two popular algorithms: \\textbf{(1)} Stochastic gradient descent under convex and non-convex landscape. In this example, we provide non-asymptotic bounds that highly depend on the sparsity of the receptive field constructed by the algorithm. \\textbf{(2)} The constrained regression problem on a 1-layer linear equivariant GNN. In this example, we present lower bounds for the discrepancy between the two types of stability, which justified the multi-fidelity design.","layer":0,"vector":[-0.0001,-0.0438,0.0071,-0.0254,-0.0232,0.0586,-0.0082,0.0127,0.0097,-0.0191,0.0156,-0.0318,0.044,0.0748,-0.0011,0.0463,-0.0122,0.0522,-0.023,0.0083,0.0162,-0.0312,0.0225,-0.0375,0.0528,-0.0061,-0.0479,-0.0875,-0.0368,-0.2615,0.0261,-0.0515,0.0131,-0.0167,0.0201,-0.0407,0.0186,0.0576,-0.0373,0.0723,-0.0349,0.0123,-0.04,-0.0851,0.016,-0.0081,-0.0317,-0.0356,-0.0493,-0.0448,0.018,-0.0461,0.0181,0.0421,0.0268,0.0646,0.031,0.055,0.0344,0.046,0.013,0.053,-0.1494,0.016,0.0465,0.055,-0.0618,-0.0247,0.013,0.0561,0.0346,0.0256,0.0004,0.0159,0.0071,0.0217,0.0395,-0.0007,-0.018,0.024,0.0368,-0.024,-0.047,-0.0018,-0.0099,-0.0584,0.0604,-0.026,0.0041,0.0318,-0.0516,-0.0341,-0.0082,0.0236,-0.0987,-0.0088,0.0486,0.0451,-0.0637,0.1569,-0.064,0.0455,0.0257,-0.0041,0.0243,-0.0273,-0.0026,-0.0124,-0.0529,-0.0269,-0.0269,-0.0393,0.014,-0.0461,0.0011,0.0065,0.0546,0.0591,-0.0209,-0.0165,-0.015,0.0569,0.0469,-0.0103,0.0274,-0.0686,0.0024,0.1123,0.0488,0.047,0.0145,0.0021,-0.0023,-0.014,-0.0034,-0.0049,0.042,0.012,0.0169,-0.0084,-0.0372,-0.0069,0.0229,-0.0684,-0.0937,0.131,-0.0667,0.0111,-0.0331,-0.0213,-0.0329,0.0229,-0.0022,-0.0502,0.0035,0.0394,0.0085,0.0311,-0.0688,0.0351,-0.0279,-0.0392,-0.0594,0.0958,0.007,-0.0578,0.0027,-0.022,-0.015,-0.0298,0.0733,0.0193,-0.0,0.0194,0.0326,0.0527,-0.1348,-0.0028,0.0358,0.0003,0.0048,-0.0361,-0.0427,0.0478,0.0408,-0.0128,-0.0297,-0.0245,0.0316,0.0134,-0.0227,-0.0028,-0.0159,0.0112,-0.0633,-0.0221,-0.0209,-0.0299,0.0029,0.0121,0.0057,-0.0142,-0.0533,0.0333,-0.0059,0.0115,-0.0307,-0.006,0.0072,0.0296,-0.0587,0.0055,0.0146,-0.0532,-0.0073,0.0492,0.0468,0.0315,0.0121,0.0626,0.0445,-0.0486,-0.056,-0.2199,-0.0323,-0.0022,-0.0174,0.0425,-0.0922,0.0328,-0.0188,0.0778,0.0687,0.0454,0.0036,-0.0077,0.0039,0.0279,0.0439,0.0386,0.0043,0.0046,0.0004,-0.0279,0.0058,-0.0269,-0.0675,0.0659,0.0196,0.2295,-0.0129,0.0183,-0.0138,-0.0007,0.0345,-0.0556,-0.0505,0.0827,0.0553,0.0276,-0.0221,-0.0604,0.0007,-0.0095,0.0022,0.0169,-0.1194,-0.0729,-0.0079,-0.0437,0.0286,-0.0504,0.0015,0.0444,-0.0349,0.0732,-0.0073,-0.0143,-0.0184,-0.0995,0.0416,-0.0235,0.018,0.0166,-0.0926,0.0276,-0.0334,0.082,0.0349,-0.0091,-0.0486,0.0394,-0.0196,0.0036,0.045,0.0,-0.0396,0.0556,0.0043,0.0431,-0.0249,-0.0432,0.0165,0.0606,-0.0123,0.071,0.001,0.0712,0.0087,0.0579,-0.017,0.0706,0.0157,0.0095,0.0233,-0.0566,-0.0322,0.0904,-0.0311,-0.2926,0.0187,0.0329,0.0583,-0.0156,0.0288,0.0562,0.0232,-0.1079,-0.0098,0.0008,0.0452,0.0654,0.0178,0.0089,0.0483,0.0822,-0.0548,0.0435,-0.0639,0.0155,0.0599,0.2225,-0.0537,0.0437,0.0307,-0.0247,-0.0128,-0.0121,-0.0397,-0.0028,-0.0069,0.0876,-0.0392,0.0377,0.0708,-0.0863,0.0507,0.0458,-0.0011,0.0242,-0.0217,-0.0192,-0.027,0.0868,-0.0377,-0.0221,0.0086,0.0087,0.007,-0.0169,0.0251,0.0063,-0.0479,0.0069,0.0107,-0.0282,-0.0239,-0.0597,-0.0539,0.0248,-0.0363,-0.0592,-0.0392,-0.0498]}
{"key":"[How to Leverage Unlabeled Data in Offline Reinforcement Learning] Offline reinforcement learning (RL) can learn control policies from static datasets but, like standard RL methods, it requires reward annotations for every transition. In many cases, labeling large datasets with rewards may be costly, especially if those rewards must be provided by human labelers, while collecting diverse unlabeled data might be comparatively inexpensive. How can we best leverage such unlabeled data in offline RL? One natural solution is to learn a reward function from the labeled data and use it to label the unlabeled data. In this paper, we find that, perhaps surprisingly, a much simpler method that simply applies zero rewards to unlabeled data leads to effective data sharing both in theory and in practice, without learning any reward model at all. While this approach might seem strange (and incorrect) at first, we provide extensive theoretical and empirical analysis that illustrates how it trades off reward bias, sample complexity and distributional shift, often leading to good results. We characterize conditions under which this simple strategy is effective, and further show that extending it with a simple reweighting approach can further alleviate the bias introduced by using incorrect reward labels. Our empirical evaluation confirms these findings in simulated robotic locomotion, navigation, and manipulation settings.","layer":0,"vector":[-0.0588,-0.0214,0.0318,-0.019,0.0043,0.0343,0.0408,0.0552,0.0218,0.0195,0.0709,-0.0094,0.0036,0.0906,-0.0036,0.0272,-0.021,0.0838,-0.0408,0.0069,0.0085,-0.0927,-0.0269,-0.0256,0.0248,0.0231,-0.0713,-0.0798,-0.0224,-0.2401,-0.0069,-0.0597,0.0353,-0.0345,0.0198,0.0324,-0.0436,0.0394,-0.0406,0.0065,0.047,0.024,0.0013,-0.0618,-0.036,-0.0403,-0.0218,-0.0356,-0.0182,-0.0054,0.0155,-0.0111,-0.0207,0.0396,0.0617,0.0501,0.0523,0.0475,0.0439,0.0329,-0.0009,0.0298,-0.1377,0.0526,0.0448,0.0274,-0.0774,-0.0034,0.0467,0.0369,-0.0037,0.039,0.0246,0.0385,0.0332,-0.0249,-0.0385,-0.0354,-0.0336,-0.0257,0.0069,-0.0545,-0.0406,-0.009,-0.0241,-0.0474,0.0153,-0.0541,0.0678,0.0497,0.0022,0.0182,-0.0187,0.0535,-0.0865,-0.0282,0.0107,0.0301,-0.0842,0.2105,-0.0371,0.0576,-0.0239,0.021,0.0162,-0.062,-0.027,-0.0156,-0.0373,-0.0011,-0.0043,0.0077,0.0068,-0.036,0.0351,0.0374,0.0517,-0.0009,0.0116,0.0006,-0.0141,-0.0096,0.0535,-0.0023,0.0225,-0.045,0.022,0.1392,0.0229,0.0144,0.0404,-0.0534,-0.0312,-0.0087,0.0198,0.0108,0.0765,0.0016,0.0423,0.0136,-0.0149,0.0116,-0.0028,-0.1416,-0.0463,0.1193,0.0253,0.0328,-0.0279,-0.0062,-0.0115,0.0176,-0.0105,-0.0358,-0.0082,0.0493,0.0222,0.033,-0.0814,0.032,-0.0216,-0.0818,0.0103,0.0882,-0.0226,-0.0817,-0.0428,-0.0141,-0.0161,-0.0068,0.016,-0.009,-0.0578,0.0339,0.0886,-0.0064,-0.0941,-0.0294,0.0065,0.0142,0.022,-0.0693,-0.0507,0.0272,0.0267,-0.0201,-0.0191,-0.0393,0.0299,0.0443,-0.0052,0.0044,-0.0156,-0.0168,-0.0518,-0.0315,0.0108,0.0056,0.0168,-0.0296,-0.01,0.0088,-0.0073,-0.0171,-0.0139,0.0194,-0.0014,-0.016,0.0815,0.0005,-0.0124,0.0307,0.0176,-0.0173,-0.0604,-0.0262,0.0624,0.0064,-0.006,0.0091,-0.0013,-0.016,0.0003,-0.2364,-0.0127,0.0012,-0.019,0.0434,-0.0464,0.0324,-0.009,0.0355,0.0448,0.0783,-0.0495,-0.0661,0.0228,-0.0303,0.0644,0.0539,0.0195,-0.0336,0.0226,0.0164,-0.0011,-0.0155,-0.1076,0.0651,-0.0171,0.2432,0.0424,0.0483,-0.0367,0.0232,0.0559,-0.0349,-0.0894,0.0565,0.0197,0.0583,-0.0214,0.0172,-0.0344,-0.0104,0.0055,0.0409,-0.1225,-0.0089,-0.0269,-0.0691,-0.0039,-0.0332,-0.0045,0.03,-0.008,0.0629,-0.0161,-0.0224,-0.0172,-0.0635,0.0287,-0.0368,0.0173,0.0246,-0.0432,0.0182,-0.0771,0.0421,0.0097,-0.0288,-0.0677,0.0741,-0.0004,-0.045,0.083,0.0292,0.0215,0.0055,0.0189,-0.0194,-0.0366,-0.0698,-0.0126,0.0548,0.0109,0.0078,0.075,0.0319,-0.0236,0.0666,-0.029,0.0373,-0.0073,0.0186,0.0127,-0.0457,-0.0284,0.0691,0.0257,-0.2743,0.041,0.0215,0.0512,-0.0151,0.0094,0.0695,0.0287,-0.0505,-0.0027,0.0105,0.0511,0.0274,0.0069,0.0203,0.0541,0.0869,-0.0259,0.0482,-0.0515,0.0347,0.0662,0.2114,-0.0608,0.051,-0.0185,-0.0802,0.0001,0.0458,-0.0039,-0.021,0.0063,0.0551,-0.0442,0.0433,0.0781,-0.0449,0.0067,0.0169,-0.0022,-0.0456,0.0405,-0.0014,-0.0377,0.0942,0.0083,-0.0139,-0.055,-0.0288,0.0376,-0.0297,-0.0151,-0.0267,0.0006,0.0338,0.0168,-0.038,-0.0815,-0.0346,-0.0511,-0.0088,-0.0285,-0.0035,-0.0238,-0.0096]}
{"key":"[QUOTIENT: Two-Party Secure Neural Network Training and Prediction] Recently, there has been a wealth of effort devoted to the design of secure protocols for machine learning tasks. Much of this is aimed at enabling secure prediction from highly-accurate Deep Neural Networks (DNNs). However, as DNNs are trained on data, a key question is how such models can be also trained securely. The few prior works on secure DNN training have focused either on designing custom protocols for existing training algorithms, or on developing tailored training algorithms and then applying generic secure protocols. In this work, we investigate the advantages of designing training algorithms alongside a novel secure protocol, incorporating optimizations on both fronts. We present QUOTIENT, a new method for discretized training of DNNs, along with a customized secure two-party protocol for it. QUOTIENT incorporates key components of state-of-the-art DNN training such as layer normalization and adaptive gradient methods, and improves upon the state-of-the-art in DNN training in two-party computation. Compared to prior work, we obtain an improvement of 50X in WAN time and 6% in absolute accuracy.","layer":3,"vector":[-0.0252,-0.0268,-0.0137,-0.0066,0.0077,0.0753,0.015,-0.0107,0.0181,-0.0392,-0.0005,-0.0283,0.0556,0.056,0.0413,-0.0192,0.0095,0.0244,-0.0422,0.0369,0.0059,-0.0177,0.0006,-0.0562,-0.0001,0.024,-0.0212,-0.0512,-0.0609,-0.2439,0.0186,-0.0773,0.0245,-0.0373,-0.0022,-0.0567,-0.0313,0.0484,0.0054,0.0675,-0.0088,-0.0038,-0.0074,-0.1156,0.0204,-0.0478,-0.0268,-0.0284,-0.0199,-0.0492,0.0451,0.0004,0.0335,0.0236,0.0551,-0.0028,0.0344,0.0233,-0.005,0.0602,-0.0005,0.0931,-0.1663,0.0635,0.0438,0.0406,-0.0664,-0.0253,0.0055,0.0175,0.0363,0.0572,0.0085,0.0047,-0.0044,0.0127,0.0068,-0.0428,-0.0268,0.0039,-0.012,-0.0159,-0.0672,0.0193,0.0143,-0.0287,0.0184,-0.032,0.0101,-0.0237,-0.0539,-0.0197,-0.0172,0.0103,-0.0242,0.0067,0.0219,0.0165,-0.1026,0.1993,-0.0427,0.0282,0.0101,-0.0307,0.0502,-0.0072,-0.0413,-0.0554,0.0023,-0.003,0.0272,-0.0137,0.0351,-0.0047,-0.0082,0.052,0.0582,0.0541,-0.0146,0.0137,0.0056,-0.0205,0.0547,0.0222,0.0358,-0.0772,-0.0017,0.1249,-0.0119,0.0479,0.0076,-0.0256,-0.0078,-0.0108,0.0307,0.0038,0.015,0.0042,0.0075,-0.0391,-0.0598,-0.0345,0.023,-0.0523,-0.0487,0.1183,-0.0305,0.019,-0.0032,-0.0333,-0.0387,0.0373,-0.0266,-0.0644,0.0642,0.0315,0.0373,0.0294,-0.0713,-0.0118,0.0128,-0.061,-0.0488,0.1079,0.0319,-0.1042,0.031,0.0195,0.0369,-0.0526,0.0462,0.023,-0.028,0.0077,0.0371,0.0003,-0.0469,-0.0317,-0.0116,-0.0142,-0.0151,-0.0591,-0.0794,0.045,0.0282,-0.0212,0.0402,-0.0453,-0.0058,0.0562,-0.0501,0.0417,-0.0816,0.0049,0.018,-0.0248,-0.0278,-0.0016,-0.0056,-0.0408,-0.0077,0.0239,-0.0441,-0.009,0.0138,0.0061,0.0217,-0.0219,-0.0016,0.0605,-0.0274,-0.021,0.0405,-0.0935,-0.0179,0.0447,0.013,0.0548,-0.0289,0.0233,0.0389,-0.0111,-0.0401,-0.206,-0.0169,-0.0055,-0.024,0.0861,-0.0648,0.07,-0.0312,-0.0181,0.0357,0.075,-0.0052,-0.002,0.0177,0.0077,0.0805,0.056,0.0072,-0.0013,-0.0124,-0.0451,0.0324,-0.0191,-0.0862,0.0816,0.0206,0.2225,0.046,0.0497,-0.0301,0.0054,0.0119,-0.0207,-0.1263,0.0579,-0.0177,0.0459,-0.0028,-0.0404,-0.0425,-0.0126,0.0506,0.0181,-0.1221,-0.0224,-0.0551,-0.0848,0.0288,-0.0564,-0.0106,0.0432,0.0091,0.073,0.023,0.0159,-0.0515,-0.0801,0.0504,-0.0556,0.0712,0.0004,-0.0491,0.016,-0.0646,0.035,-0.0075,-0.0036,-0.0509,0.0593,-0.0304,-0.0143,0.0654,0.0381,0.0173,0.0608,-0.0077,0.0114,-0.0072,-0.0342,0.0087,0.1004,-0.0325,0.0382,0.0188,0.0239,-0.0077,0.1073,0.0195,0.0578,-0.0119,-0.0542,-0.0486,-0.0412,-0.0023,0.0439,0.0108,-0.3003,0.0351,-0.007,0.0474,-0.0418,0.0233,0.0669,-0.0013,-0.0867,0.0175,0.001,0.0474,0.0634,0.0306,-0.0167,-0.0012,0.0795,-0.0193,0.0351,-0.0221,0.0079,0.0515,0.2221,-0.0001,0.0141,-0.0138,-0.0166,0.0146,0.0515,-0.0293,0.011,0.026,0.027,-0.0537,0.0176,0.0967,-0.0054,0.0201,0.0116,-0.0178,0.0048,-0.0004,-0.0277,0.0068,0.0568,-0.0101,-0.0017,0.0093,0.005,0.0227,-0.038,0.0397,0.0155,-0.0051,0.0258,0.0114,-0.0491,-0.0596,-0.0676,-0.0119,0.0418,-0.0451,0.0014,0.0112,-0.0571]}
{"key":"[Deep Learning for Functional Data Analysis with Adaptive Basis Layers] Despite their widespread success, the application of deep neural networks to functional data remains scarce today. The infinite dimensionality of functional data means standard learning algorithms can be applied only after appropriate dimension reduction, typically achieved via basis expansions. Currently, these bases are chosen a priori without the information for the task at hand and thus may not be effective for the designated task. We instead propose to adaptively learn these bases in an end-to-end fashion. We introduce neural networks that employ a new Basis Layer whose hidden units are each basis functions themselves implemented as a micro neural network. Our architecture learns to apply parsimonious dimension reduction to functional inputs that focuses only on information relevant to the target rather than irrelevant variation in the input function. Across numerous classification/regression tasks with functional data, our method empirically outperforms other types of neural networks, and we prove that our approach is statistically consistent with low generalization error. Code is available at: \\url{https://github.com/jwyyy/AdaFNN}.","layer":1,"vector":[-0.0298,-0.0183,0.0196,-0.0021,0.024,0.0458,0.0685,0.0607,0.0661,-0.0132,0.005,-0.0514,0.0543,0.0487,0.0204,0.0013,0.0383,0.0904,-0.0737,-0.0033,0.003,-0.0401,-0.0158,-0.0283,-0.0145,-0.0182,-0.0413,-0.0339,-0.0383,-0.2403,0.0419,-0.0351,0.0478,-0.0119,0.0234,0.0043,-0.0383,0.0619,-0.038,0.0421,0.0286,-0.0217,-0.0119,-0.0294,-0.0292,-0.0127,-0.0503,-0.0524,-0.0028,-0.0373,0.0016,-0.03,0.031,0.0607,0.065,0.0117,0.0482,0.0211,0.0501,0.0462,0.0177,0.0623,-0.1393,0.0691,0.0458,0.0178,-0.0328,-0.0554,-0.0153,0.0596,-0.0434,0.0399,-0.0022,0.025,0.0454,-0.003,-0.0193,-0.0026,0.0002,0.012,0.0505,0.0113,-0.0283,-0.0193,0.0008,-0.0543,0.0229,-0.009,0.0113,0.0029,-0.0342,0.002,-0.0322,-0.0098,-0.0707,0.004,0.0696,-0.0006,-0.0706,0.2017,-0.0392,0.0238,0.0239,-0.0283,0.022,-0.0271,-0.0355,-0.0273,-0.045,-0.0209,-0.0475,-0.0144,-0.0059,-0.034,0.0023,-0.0328,0.0422,0.0079,0.0116,0.0006,0.0152,-0.0251,0.0338,-0.0043,0.0761,-0.0755,-0.0118,0.1222,0.0442,0.0326,0.0657,-0.0107,-0.0659,-0.0061,-0.0116,0.0379,0.0015,-0.0067,-0.0166,0.0072,-0.0486,-0.0383,0.0197,-0.0692,-0.0874,0.132,-0.0276,0.0154,-0.0289,-0.0137,-0.0367,0.0625,-0.0033,-0.0407,0.0378,0.0812,-0.0011,0.0109,-0.0582,0.0572,-0.0674,-0.0251,0.0057,0.1155,-0.0067,-0.06,-0.0188,-0.0113,0.0629,-0.0317,0.0427,0.0249,-0.02,0.0191,0.0895,0.0301,-0.0199,-0.0194,-0.009,0.0239,0.0213,-0.0402,-0.0328,0.0291,-0.0106,-0.0338,0.0341,-0.0317,0.0385,0.0287,-0.0447,0.0263,-0.0426,-0.0506,-0.0434,-0.0412,-0.006,-0.0225,0.0151,-0.0257,0.0334,0.0105,-0.0194,0.0179,-0.0181,0.0275,-0.0152,0.0123,0.0404,0.014,-0.0129,0.0115,0.0688,-0.0586,-0.0039,-0.0104,0.0004,-0.0144,-0.0087,-0.0092,0.0549,-0.0359,-0.0875,-0.2376,-0.0053,0.01,-0.0775,0.028,-0.0542,0.0584,-0.0202,0.0085,0.0613,0.0145,0.0345,-0.052,0.0326,-0.014,0.0306,0.0879,-0.0098,-0.0286,0.009,0.0034,0.0398,0.0089,-0.0909,0.0595,0.024,0.2088,-0.0112,0.0547,-0.0217,0.0183,0.0092,-0.0071,-0.0998,0.0676,-0.0067,0.0868,-0.0356,-0.0336,-0.0909,-0.039,-0.0374,-0.005,-0.1065,-0.0363,-0.0154,0.0136,-0.0098,-0.0645,-0.0235,0.0446,-0.0436,0.0296,-0.0055,-0.0099,-0.0494,-0.0663,0.0319,-0.0747,0.005,0.0106,-0.0394,0.0148,-0.0715,0.0712,0.03,-0.0628,-0.0278,0.0396,-0.0216,-0.0146,0.0701,-0.0077,0.0108,0.0627,-0.0058,0.0196,-0.0054,-0.0422,0.0052,0.0683,0.018,0.0372,0.0151,0.0099,0.0302,0.1086,-0.0288,0.0721,-0.0192,0.005,0.0256,-0.0786,-0.0192,0.0639,-0.0078,-0.2773,0.0025,0.004,0.0065,-0.0549,-0.0089,0.0091,0.0301,-0.0505,0.0221,0.0006,0.0114,0.0699,0.0005,0.023,0.0437,0.056,-0.017,0.0732,-0.0622,0.0453,0.0931,0.2399,-0.0787,0.043,0.0165,0.0237,-0.0535,0.0161,-0.0112,0.0453,0.0211,0.0651,-0.0307,0.0294,0.0832,-0.0343,0.0281,0.0332,-0.0505,0.0231,-0.0268,-0.0689,-0.038,0.1137,-0.0202,-0.0448,-0.051,-0.023,0.0362,0.0076,-0.0098,0.0051,0.016,0.0314,0.0178,-0.0306,-0.0559,0.0,-0.013,0.0176,-0.0573,-0.0409,0.0101,-0.0317]}
{"key":"[Stochastic Video Generation with a Learned Prior] Generating video frames that accurately predict future world states is challenging. Existing approaches either fail to capture the full distribution of outcomes, or yield blurry generations, or both. In this paper we introduce an unsupervised video generation model that learns a prior model of uncertainty in a given environment. Video frames are generated by drawing samples from this prior and combining them with a deterministic estimate of the future frame. The approach is simple and easily trained end-to-end on a variety of datasets. Sample generations are both varied and sharp, even many frames into the future, and compare favorably to those from existing approaches.","layer":2,"vector":[-0.0276,-0.0175,0.0263,-0.0156,0.0855,0.0615,-0.0199,0.0008,0.0309,0.0131,0.0096,-0.0687,0.0083,0.0552,-0.0105,0.0082,-0.0385,0.0177,-0.0492,-0.0124,0.0853,-0.0166,0.0268,-0.079,0.0025,-0.0073,0.0021,-0.0476,-0.0295,-0.2169,-0.0018,-0.0717,0.0405,-0.0158,-0.0048,-0.0645,-0.0538,0.0801,-0.0437,0.0686,0.0072,0.0145,-0.0062,-0.0836,-0.0539,-0.0695,0.0262,-0.0188,-0.0255,-0.0461,0.0155,-0.0529,0.028,0.0019,0.0078,0.0079,0.0059,0.0293,0.0288,0.0418,0.0024,0.0069,-0.1909,0.051,0.0219,0.0435,-0.0418,-0.0012,0.0201,0.0195,-0.0568,0.0368,0.0316,0.0724,0.0348,-0.0027,-0.0177,-0.0546,-0.0455,0.0125,0.0029,-0.0109,-0.0609,-0.0002,-0.0196,-0.0301,0.0174,-0.0275,0.0432,0.0234,-0.0818,-0.0051,-0.04,0.0173,-0.056,0.0093,0.0499,0.0117,-0.023,0.2025,-0.0499,0.0212,0.0943,-0.021,0.0455,-0.0243,-0.0986,0.0016,-0.0447,-0.0173,-0.0136,-0.0474,0.0469,-0.0326,0.0225,-0.0131,0.038,0.0512,-0.0356,-0.0481,-0.0157,0.0193,0.0628,-0.0705,0.0313,-0.1113,0.0572,0.153,0.0427,0.0,0.0347,-0.0297,-0.0515,-0.0092,0.0339,-0.0106,0.0026,-0.0092,0.0314,-0.0372,0.024,0.001,-0.0291,-0.0611,-0.0714,0.1049,0.0004,0.0922,-0.0255,-0.0185,0.0109,0.0035,0.0204,-0.0028,0.0253,0.0537,0.0109,0.0204,-0.0207,0.0355,-0.0246,-0.0643,-0.0125,0.0867,0.0149,-0.0777,-0.0631,0.0092,0.0254,0.0244,0.0401,0.0237,0.0013,0.0159,0.0975,0.0396,-0.0725,0.0352,0.0014,0.0259,0.0133,-0.0568,-0.0133,0.0398,0.0304,-0.0578,0.02,-0.0392,0.0394,0.0275,0.0234,0.0075,-0.0365,0.0127,-0.0266,-0.003,-0.0324,-0.0268,0.0381,-0.0519,-0.0047,-0.0354,-0.02,0.0316,-0.0134,0.0002,0.0151,0.0005,0.0526,0.0306,-0.0039,-0.0092,0.0648,-0.0196,-0.0183,0.0037,0.0451,0.0227,0.0114,0.0191,0.0521,-0.0576,-0.0087,-0.2372,0.024,-0.0133,0.0221,0.0442,-0.0136,0.0234,-0.0105,0.0501,0.0268,0.036,-0.0118,-0.008,0.0026,0.0204,0.0064,0.0162,0.0338,-0.0331,0.0533,-0.0532,-0.0047,-0.0266,-0.1106,0.045,-0.0172,0.2131,0.0196,0.0237,-0.0241,0.0538,0.0372,-0.0368,-0.0651,0.0564,0.0533,0.0752,0.0236,-0.0375,-0.0638,0.0047,0.0005,-0.0127,-0.102,-0.047,-0.0669,-0.0464,0.0494,-0.0286,0.003,0.0257,-0.0496,0.075,-0.036,-0.0148,-0.0377,-0.0823,0.0377,0.0053,0.0426,0.0164,-0.0531,-0.001,-0.0455,0.0357,-0.013,-0.0392,-0.049,0.0227,-0.0349,0.0134,0.1074,-0.0387,0.002,0.0546,0.0136,0.0178,-0.0226,-0.0576,-0.0214,0.0622,-0.0565,0.026,0.0541,0.0443,0.0179,0.0413,-0.0069,0.0223,-0.0268,0.0249,0.0054,-0.0593,0.0134,0.0232,-0.0258,-0.3007,0.0225,-0.0168,-0.0014,0.002,-0.038,0.1044,0.0527,-0.0475,0.0371,-0.023,0.0308,0.0602,-0.0051,-0.0347,0.0339,0.075,-0.0363,0.0428,-0.0569,0.0384,0.001,0.2246,-0.0094,0.0402,-0.0098,-0.0469,0.0064,0.0477,-0.035,0.0134,0.0054,0.0561,-0.0673,0.0131,0.0671,-0.0243,0.0175,-0.0058,0.0172,-0.0295,-0.0111,0.0522,-0.0415,0.1255,-0.0081,-0.0025,-0.0069,-0.0197,0.0374,-0.0394,0.0209,-0.0009,-0.0033,0.0244,0.0439,-0.0174,-0.0404,-0.0099,-0.0469,0.0319,-0.065,0.01,-0.0102,-0.0072]}
{"key":"[GIPA: General Information Propagation Algorithm for Graph Learning] Graph neural networks (GNNs) have been popularly used in analyzing graph-structured data, showing promising results in various applications such as node classification, link prediction and network recommendation. In this paper, we present a new graph attention neural network, namely GIPA, for attributed graph data learning. GIPA consists of three key components: attention, feature propagation and aggregation. Specifically, the attention component introduces a new multi-layer perceptron based multi-head to generate better non-linear feature mapping and representation than conventional implementations such as dot-product. The propagation component considers not only node features but also edge features, which differs from existing GNNs that merely consider node features. The aggregation component uses a residual connection to generate the final embedding. We evaluate the performance of GIPA using the Open Graph Benchmark proteins (ogbn-proteins for short) dataset. The experimental results reveal that GIPA can beat the state-of-the-art models in terms of prediction accuracy, e.g., GIPA achieves an average test ROC-AUC of $0.8700\\pm 0.0010$ and outperforms all the previous methods listed in the ogbn-proteins leaderboard.","layer":4,"vector":[-0.0207,-0.0449,0.0239,-0.0359,0.0416,0.0186,0.0393,0.0204,0.0167,-0.0441,-0.0076,-0.0563,0.0492,0.0871,0.0161,0.0658,-0.0113,0.0612,-0.025,-0.0294,0.01,-0.0059,0.0218,-0.0687,0.0342,0.0259,-0.0323,-0.04,-0.0625,-0.2301,-0.0172,-0.0544,0.0669,-0.0258,-0.0217,-0.0387,0.0306,0.0206,-0.0385,0.0678,0.0335,-0.0018,-0.0742,-0.0433,-0.0057,-0.0097,-0.0164,-0.0121,-0.0061,-0.0683,0.0314,-0.0405,0.0278,0.0057,0.0558,0.064,0.0371,0.0271,0.0144,0.0603,0.0462,0.0787,-0.1243,0.0388,0.0638,0.0591,-0.0697,0.0039,0.0241,0.066,0.025,0.0308,0.029,-0.0009,-0.0312,0.0375,0.0012,0.0381,0.0117,0.0197,0.0115,-0.0222,-0.0172,-0.0304,0.0204,-0.0069,0.0021,-0.0481,0.0066,-0.0063,-0.0326,-0.0362,0.0121,-0.0073,-0.0798,-0.016,0.0348,0.0176,-0.097,0.1819,-0.072,0.0058,0.0049,-0.0093,0.0393,-0.04,0.0279,-0.0262,-0.0289,0.0361,-0.0165,-0.0517,-0.0081,-0.0629,0.0251,0.0437,0.0516,0.0367,-0.0299,-0.0111,-0.0376,0.0074,0.0446,-0.0273,0.0124,-0.0611,0.0016,0.0832,0.0309,0.0234,0.0373,0.0054,-0.0649,-0.0008,0.0043,0.0194,0.042,-0.0324,-0.0195,-0.0047,-0.0098,-0.01,-0.0297,-0.0546,-0.0712,0.1007,-0.0614,0.0058,-0.0226,-0.0262,0.0042,0.019,0.0067,-0.001,-0.0131,0.0276,0.0366,0.0583,-0.0639,0.0296,-0.0387,0.007,-0.0668,0.0725,0.042,-0.1287,-0.0376,0.0158,-0.0136,-0.0558,0.0459,0.0614,-0.0377,0.0231,0.0825,0.033,-0.0517,-0.0215,0.0463,-0.007,0.0595,-0.0138,-0.0236,0.0184,0.0177,-0.0262,-0.0241,-0.0005,0.0028,0.0405,-0.0691,0.0375,-0.0172,0.0138,-0.0258,-0.0249,-0.0569,-0.0253,0.006,-0.0201,0.0444,-0.0142,-0.065,0.0304,-0.0643,0.04,-0.0194,0.0132,0.0018,-0.009,-0.0443,-0.0028,0.0345,-0.0105,-0.044,-0.0159,0.0099,0.0111,0.0185,0.0802,0.0353,-0.0444,-0.0906,-0.1973,-0.0161,0.0215,-0.0227,0.0874,-0.0762,0.0233,0.0274,0.0835,0.0935,0.0709,0.0068,-0.0439,-0.0018,-0.0304,0.0836,0.0312,0.0349,-0.0148,-0.0361,0.0032,0.0341,0.0002,-0.0565,0.0183,0.0078,0.2248,0.0326,0.0065,-0.0338,-0.0171,0.0437,-0.0604,-0.0926,0.0605,0.0255,0.0256,-0.0071,-0.0284,-0.0248,-0.0406,0.0118,0.0069,-0.0911,-0.0278,-0.0088,0.0124,0.032,-0.0763,0.0153,0.0866,-0.0166,0.0727,0.0397,-0.0147,-0.0707,-0.0659,0.0391,-0.0457,0.0138,0.0265,-0.0832,0.0065,-0.0419,0.0459,0.0013,-0.0022,-0.0068,0.0141,0.0079,-0.0229,0.0688,0.0311,-0.0149,0.0659,-0.0066,0.0445,-0.0067,-0.0413,0.0338,0.0121,-0.0514,0.0406,-0.0192,0.0456,-0.0002,0.0852,-0.0436,0.0297,-0.04,0.0409,0.0018,-0.027,-0.0736,0.0537,-0.0472,-0.323,0.0715,0.0116,0.0393,-0.0102,0.0064,0.0526,0.0321,-0.0472,0.0236,0.0152,0.0177,0.043,-0.0141,-0.0444,0.0674,0.0168,-0.0417,0.025,-0.0232,0.0385,0.0371,0.2508,-0.0173,0.0645,-0.0166,-0.066,-0.0146,0.0272,0.0225,0.0228,-0.0004,0.0898,-0.053,0.0451,0.0829,-0.0143,0.0351,-0.0022,-0.0061,0.012,-0.0197,-0.0708,-0.0179,0.0751,-0.0129,-0.049,-0.0846,-0.0053,0.048,-0.0326,0.0084,-0.0185,0.0058,0.0204,0.0514,-0.0285,-0.0245,-0.0735,-0.0266,-0.0215,-0.0534,-0.041,0.0306,-0.0198]}
{"key":"[On Abruptly-Changing and Slowly-Varying Multiarmed Bandit Problems] We study the non-stationary stochastic multiarmed bandit (MAB) problem and propose two generic algorithms, namely, the limited memory deterministic sequencing of exploration and exploitation (LM-DSEE) and the Sliding-Window Upper Confidence Bound# (SW-UCB#). We rigorously analyze these algorithms in abruptly-changing and slowly-varying environments and characterize their performance. We show that the expected cumulative regret for these algorithms under either of the environments is upper bounded by sublinear functions of time, i.e., the time average of the regret asymptotically converges to zero. We complement our analytic results with numerical illustrations.","layer":4,"vector":[-0.0819,-0.0035,0.034,-0.033,0.028,0.0092,0.0609,0.0203,0.0262,0.0024,0.0598,0.0039,0.0491,0.0682,-0.0037,0.007,-0.031,0.0081,-0.0113,0.0116,-0.0166,-0.0537,0.0081,-0.0813,0.0391,0.02,-0.0499,-0.0803,-0.0403,-0.2091,0.0107,-0.0256,0.0362,-0.0776,0.0062,-0.002,-0.0397,0.0755,-0.0054,0.0832,0.0157,0.029,-0.0445,-0.071,-0.0329,-0.0636,-0.0032,-0.0095,-0.01,0.0022,-0.0218,-0.0372,0.0324,0.0186,0.0384,-0.0104,-0.0065,0.0649,0.0313,0.0445,-0.0001,0.0431,-0.1676,0.0583,0.043,0.0087,-0.0379,-0.0251,0.0499,0.0495,0.0163,0.0354,0.0017,0.0574,0.0709,0.0114,-0.0267,-0.0388,-0.0161,0.0236,0.0077,-0.0409,-0.047,0.0103,-0.0527,-0.0514,-0.0035,-0.0377,0.0742,0.0185,0.012,-0.0169,0.0012,0.0244,-0.0372,-0.0126,0.04,0.0247,-0.0344,0.202,-0.0281,0.0527,-0.0016,-0.03,0.0258,-0.0542,-0.0407,-0.025,-0.0316,-0.0084,-0.0063,0.0165,0.1032,-0.0439,-0.0085,0.0115,0.0008,0.0526,-0.0065,-0.0002,-0.0612,0.0067,0.0773,0.0054,0.004,-0.0474,-0.0045,0.1569,0.0122,0.0027,0.048,-0.0614,-0.0449,-0.0423,0.0304,0.0433,-0.0372,0.0055,0.0538,-0.0223,-0.0467,-0.0279,0.0606,-0.1178,-0.0135,0.1223,0.0307,0.0208,-0.0712,-0.0357,-0.0178,-0.0002,0.0111,-0.0115,0.0332,0.0357,0.0415,0.0588,-0.0359,0.0446,-0.035,-0.0476,0.0503,0.0861,0.0146,-0.0493,-0.0216,-0.0151,-0.0229,0.0044,0.0213,0.0136,-0.0684,-0.002,0.0634,0.0174,-0.0656,-0.0095,0.0132,-0.0045,-0.0223,0.0118,-0.0378,0.041,0.0526,-0.004,-0.0049,-0.0196,0.0563,0.067,-0.0293,-0.0007,0.0364,-0.0006,-0.0422,-0.0271,-0.0202,-0.0216,0.0466,-0.0161,0.0478,-0.0385,-0.0495,0.013,0.0327,0.0031,0.0156,-0.0392,0.0634,0.0388,-0.0308,0.0256,0.0147,-0.0056,-0.0658,-0.0069,0.0355,0.0503,-0.0075,0.0256,0.0306,-0.0198,0.0024,-0.246,-0.0042,-0.0082,0.0562,0.0388,-0.0281,0.0355,-0.0476,0.074,0.0782,0.0658,-0.0485,-0.0323,0.0517,0.0047,0.0787,0.0259,0.0056,-0.0051,-0.0025,-0.0247,0.0206,-0.0166,-0.0434,0.0176,0.0288,0.2226,0.0223,0.0206,-0.0486,0.0513,0.0511,0.0044,-0.0803,0.039,0.0409,0.0545,-0.0322,-0.0017,-0.065,-0.0192,0.0265,-0.0021,-0.0905,-0.0899,-0.0579,-0.052,-0.0078,-0.0351,-0.0224,0.0257,-0.0306,0.0374,-0.0173,-0.0109,-0.0404,-0.0818,0.0367,-0.0161,0.0451,0.0181,-0.0404,0.037,-0.0709,0.0735,-0.0356,-0.0128,-0.0469,0.0337,-0.0454,-0.0155,0.0329,-0.0005,-0.0084,-0.0137,0.0238,0.0408,-0.0502,-0.0588,-0.0211,-0.0001,-0.0448,-0.0026,0.0056,-0.0113,-0.0056,0.0826,-0.0257,0.0253,0.0334,0.0039,-0.0216,-0.0883,0.0059,0.0449,-0.0294,-0.3129,0.0602,0.0124,0.0126,-0.0092,0.0358,0.0223,-0.02,-0.0552,0.0182,-0.0046,0.1016,0.0493,0.0016,0.0429,0.0081,0.0652,-0.035,0.0349,-0.0838,0.0489,0.0549,0.1991,-0.0165,0.0466,0.008,-0.0264,0.0229,0.0049,-0.0287,0.004,-0.0442,0.0622,-0.0822,0.0592,0.0852,-0.0566,0.0687,-0.0044,-0.0088,-0.0424,-0.0021,-0.0295,-0.0165,0.112,-0.0112,-0.0157,-0.0572,-0.0199,0.0155,-0.0235,0.0091,-0.0088,-0.0401,0.0157,0.0179,-0.0319,-0.0789,-0.0122,-0.0139,-0.0081,-0.044,0.0014,-0.0336,0.0093]}
{"key":"[Provable Hierarchical Imitation Learning via EM] Due to recent empirical successes, the options framework for hierarchical reinforcement learning is gaining increasing popularity. Rather than learning from rewards which suffers from the curse of dimensionality, we consider learning an options-type hierarchical policy from expert demonstrations. Such a problem is referred to as hierarchical imitation learning. Converting this problem to parameter inference in a latent variable model, we theoretically characterize the EM approach proposed by Daniel et al. (2016). The population level algorithm is analyzed as an intermediate step, which is nontrivial due to the samples being correlated. If the expert policy can be parameterized by a variant of the options framework, then under regularity conditions, we prove that the proposed algorithm converges with high probability to a norm ball around the true parameter. To our knowledge, this is the first performance guarantee for an hierarchical imitation learning algorithm that only observes primitive state-action pairs.","layer":4,"vector":[-0.0673,-0.0172,0.0318,-0.0363,-0.0382,0.0041,0.0348,0.0337,0.0458,0.0147,0.0121,-0.0413,0.0227,0.0639,0.0131,0.0094,0.0027,0.0518,-0.0369,0.0311,0.0298,-0.075,-0.0346,-0.0701,0.0272,0.0085,-0.0383,-0.0305,-0.0011,-0.2363,0.0429,-0.056,0.0364,0.015,0.0207,-0.0352,-0.0431,0.0373,-0.0042,0.0388,-0.0139,0.0158,-0.0269,-0.0694,-0.0279,-0.0583,-0.0026,-0.025,-0.0308,-0.0067,0.0298,-0.0413,-0.0093,0.0184,0.0825,0.0233,0.07,0.0616,0.0394,0.0556,-0.0044,0.0561,-0.1489,0.0482,0.0371,0.0632,-0.0144,-0.0192,0.0329,0.0567,-0.0325,0.0527,-0.0012,0.0811,-0.0163,-0.0313,-0.0051,-0.0544,-0.0238,-0.0317,-0.0076,-0.0537,-0.0739,0.0118,-0.0316,-0.0426,0.0231,-0.0327,0.0609,0.0026,-0.0598,0.0017,-0.0402,0.0053,-0.0407,0.0074,0.0356,0.036,-0.0538,0.1989,-0.0155,0.0305,0.0545,0.0026,0.0114,-0.0479,-0.0593,-0.0131,0.0093,0.0214,-0.0725,-0.0109,0.0275,-0.0092,0.0294,0.0298,0.094,0.0234,-0.0429,-0.069,0.0272,0.0144,0.0422,-0.0221,0.013,-0.0967,0.009,0.1482,0.0386,0.0397,0.0471,-0.0723,-0.0323,-0.0403,0.0079,-0.0017,0.0189,-0.0106,0.0116,0.0024,-0.0226,-0.0112,0.0146,-0.0779,-0.0309,0.0817,0.0152,0.037,-0.0416,-0.0011,-0.0451,-0.022,-0.0118,-0.0387,0.0022,0.0369,0.0657,0.0515,-0.0626,-0.0091,-0.0399,-0.0365,-0.0325,0.09,0.0033,-0.0723,-0.0758,-0.0202,0.0356,-0.015,0.0329,0.0448,-0.0407,0.0537,0.0694,0.0334,-0.0953,-0.0105,0.0452,0.0002,0.0202,-0.1227,-0.0006,0.0179,0.0468,0.0041,0.021,-0.0415,0.0494,0.0341,-0.0149,0.0368,-0.0157,-0.0225,-0.0067,-0.0628,-0.0174,-0.007,0.0566,-0.0468,-0.0262,-0.0195,-0.0505,0.0301,-0.0475,0.0374,-0.0009,0.0135,0.0734,0.0075,-0.0185,0.028,0.047,0.0097,-0.0544,0.006,0.0053,0.0451,-0.026,0.0184,-0.0009,-0.0332,-0.0211,-0.2248,-0.0358,-0.0447,0.0199,0.0517,-0.0386,0.0624,-0.0378,0.0727,0.049,0.0419,-0.0194,-0.034,0.0373,0.0083,0.0459,0.0199,0.0093,0.0165,0.0269,-0.0369,0.0088,-0.0533,-0.0533,0.0563,-0.0017,0.1892,0.0495,0.0289,-0.0031,0.0187,0.0532,-0.0567,-0.0685,0.0437,0.0338,0.0663,-0.0529,-0.0254,-0.0833,-0.029,0.0357,-0.007,-0.0995,-0.0154,-0.001,-0.0329,0.0634,-0.0735,-0.0174,0.0663,-0.0167,0.0138,-0.0448,-0.0364,-0.0564,-0.0833,0.025,-0.0494,0.0404,0.0377,0.0025,0.0132,-0.084,0.0248,-0.0353,0.0206,-0.073,0.0599,0.0026,-0.0439,0.0483,0.0028,0.0097,0.0315,0.0217,0.0135,-0.0027,-0.0324,0.0115,0.0706,-0.0479,0.0243,0.0655,0.0313,-0.0209,0.0537,-0.0412,0.0459,-0.0191,0.0218,0.0229,-0.104,0.0184,0.0371,-0.0174,-0.3082,0.0247,0.0364,0.047,-0.0486,0.0036,0.0478,-0.039,0.0007,0.0238,0.0162,0.0301,0.0576,0.0518,0.0137,0.0158,0.0619,-0.0357,0.066,-0.0782,-0.0089,0.0348,0.2228,-0.0398,0.0386,0.0012,-0.0049,0.0043,0.0347,-0.0737,-0.015,-0.0007,0.0707,-0.0618,0.0747,0.0475,-0.0254,0.0339,-0.009,-0.0121,-0.0167,0.0315,-0.0298,-0.0588,0.1016,0.0061,-0.0065,-0.0238,-0.0173,0.0336,0.0081,0.027,0.0388,-0.0212,0.0125,0.0204,-0.024,-0.0393,-0.0348,-0.0568,0.0244,-0.0123,0.0312,0.0041,0.0052]}
{"key":"[Enabling real-time multi-messenger astrophysics discoveries with deep learning] Multi-messenger astrophysics is a fast-growing, interdisciplinary field that combines data, which vary in volume and speed of data processing, from many different instruments that probe the Universe using different cosmic messengers: electromagnetic waves, cosmic rays, gravitational waves and neutrinos. In this Expert Recommendation, we review the key challenges of real-time observations of gravitational wave sources and their electromagnetic and astroparticle counterparts, and make a number of recommendations to maximize their potential for scientific discovery. These recommendations refer to the design of scalable and computationally efficient machine learning algorithms; the cyber-infrastructure to numerically simulate astrophysical sources, and to process and interpret multi-messenger astrophysics data; the management of gravitational wave detections to trigger real-time alerts for electromagnetic and astroparticle follow-ups; a vision to harness future developments of machine learning and cyber-infrastructure resources to cope with the big-data requirements; and the need to build a community of experts to realize the goals of multi-messenger astrophysics.","layer":4,"vector":[-0.0715,-0.0014,-0.0393,-0.0432,0.0286,0.0183,-0.0245,0.0011,0.0234,-0.0423,-0.0126,-0.0697,0.0172,0.0205,0.0327,-0.008,0.0196,-0.053,-0.0198,0.0468,0.0345,-0.0032,-0.0194,-0.0484,0.0487,0.0341,-0.0454,-0.0321,-0.107,-0.2177,0.0224,-0.0459,0.0788,-0.0162,0.0107,-0.0328,0.0262,0.0414,-0.0647,0.0464,0.0524,-0.0258,-0.0018,-0.0683,0.0155,-0.0691,-0.0444,-0.0246,-0.0109,-0.0214,0.0267,-0.0472,0.0128,0.0536,0.0187,0.026,0.0155,0.0039,0.0527,0.0163,0.0104,0.0618,-0.1756,0.0248,0.0118,0.0199,-0.0266,-0.0139,0.0595,0.0361,-0.0053,0.0235,0.0043,0.0579,-0.0142,0.0148,0.0087,-0.036,-0.012,0.0312,0.0143,-0.0164,-0.0535,-0.0095,-0.0471,-0.0128,0.0228,0.0012,0.0077,-0.0026,-0.0636,-0.0218,-0.007,0.0428,-0.0649,-0.0057,0.0146,-0.0103,0.0116,0.2126,-0.0343,0.0159,0.0136,0.0068,0.0386,-0.0428,0.0074,-0.0433,0.0176,0.0258,0.0141,-0.0325,0.043,-0.0264,0.0396,0.058,0.0615,-0.0027,-0.0323,-0.0041,-0.0153,0.0192,0.0736,0.0025,0.0347,-0.0823,0.0098,0.127,-0.0026,0.0211,0.0312,0.0337,-0.0681,-0.0067,0.0093,0.0298,0.0185,-0.0124,0.0252,0.0074,-0.0528,-0.0565,0.0251,-0.0677,-0.0317,0.0845,-0.0302,0.0161,-0.0808,-0.0453,-0.0749,0.0163,-0.0633,0.0198,0.0297,-0.006,0.0225,0.0719,-0.0804,0.0291,-0.012,-0.0201,-0.0191,0.1207,0.0061,-0.0794,0.0045,0.0142,0.0197,-0.0615,0.0377,0.0424,-0.0298,-0.0123,0.0927,0.0499,-0.0681,-0.0071,0.0076,0.0263,-0.0174,-0.0333,-0.012,0.0662,0.0114,-0.0622,-0.0177,-0.0229,0.0001,-0.0148,-0.0072,0.047,-0.0086,0.0029,-0.0397,-0.0548,-0.0062,-0.0065,-0.0078,-0.0202,0.0278,0.0104,-0.0161,0.0195,-0.0259,-0.0046,0.011,0.0175,0.0109,0.0034,0.0134,-0.012,0.0371,-0.0242,-0.0673,0.0027,0.0376,0.0021,-0.0336,0.0948,-0.0034,-0.0444,-0.0775,-0.2594,-0.0276,0.0028,0.0068,0.0985,-0.028,0.0377,-0.002,0.0645,0.051,0.1059,0.0157,-0.0707,-0.0245,0.0352,0.0602,-0.0032,0.0166,-0.034,-0.011,-0.0367,0.0662,-0.0188,-0.0621,0.0137,0.0161,0.1942,0.0851,-0.0116,-0.0495,0.0334,0.0254,0.041,-0.0915,0.0402,0.0306,0.1155,0.0235,-0.043,-0.0244,-0.0271,0.0225,0.0056,-0.0887,-0.0421,-0.0556,0.0077,0.0389,-0.0255,0.0016,0.0439,-0.0247,0.0074,-0.0007,-0.0361,-0.0524,-0.0975,0.0353,-0.0165,0.035,0.0194,0.0083,-0.0059,-0.0906,0.0703,-0.01,-0.0291,-0.0168,0.0206,-0.0484,0.0136,0.0708,-0.0536,-0.0012,0.0561,0.0001,0.0693,-0.0592,-0.0317,-0.0041,0.0555,-0.0247,0.0542,0.0834,-0.0207,0.0302,0.0622,0.0073,0.0462,-0.0631,-0.0099,0.0109,-0.0417,-0.0503,0.0259,-0.0172,-0.3063,0.0805,0.0035,0.012,-0.0344,0.0326,0.0412,0.0086,-0.0386,0.0032,-0.0701,0.0701,-0.0134,-0.0091,-0.0003,0.0556,0.0586,-0.0414,-0.0032,-0.038,0.0207,0.0455,0.2228,0.0038,0.0135,0.0657,-0.0214,0.0318,0.0354,-0.0099,0.0443,0.0052,0.0308,-0.0659,0.0281,0.095,-0.0093,0.0653,0.0172,0.0093,0.0474,0.0388,-0.0113,-0.0409,0.0833,-0.0207,-0.0552,-0.0547,-0.0231,-0.0024,-0.0322,0.0036,-0.0116,0.0108,0.0199,0.0307,-0.0267,-0.02,-0.0013,-0.0543,0.0318,-0.0351,0.0114,-0.0166,-0.0343]}
{"key":"[CURI: A Benchmark for Productive Concept Learning Under Uncertainty] Humans can learn and reason under substantial uncertainty in a space of infinitely many concepts, including structured relational concepts (\"a scene with objects that have the same color\") and ad-hoc categories defined through goals (\"objects that could fall on one's head\"). In contrast, standard classification benchmarks: 1) consider only a fixed set of category labels, 2) do not evaluate compositional concept learning and 3) do not explicitly capture a notion of reasoning under uncertainty. We introduce a new few-shot, meta-learning benchmark, Compositional Reasoning Under Uncertainty (CURI) to bridge this gap. CURI evaluates different aspects of productive and systematic generalization, including abstract understandings of disentangling, productive generalization, learning boolean operations, variable binding, etc. Importantly, it also defines a model-independent \"compositionality gap\" to evaluate the difficulty of generalizing out-of-distribution along each of these axes. Extensive evaluations across a range of modeling choices spanning different modalities (image, schemas, and sounds), splits, privileged auxiliary concept information, and choices of negatives reveal substantial scope for modeling advances on the proposed task. All code and datasets will be available online.","layer":4,"vector":[0.0247,-0.0366,0.0115,-0.052,0.0295,0.0449,0.0482,0.0048,0.0265,-0.0237,-0.0093,-0.0677,0.0674,0.0796,0.0477,-0.0043,0.0022,0.0692,-0.0316,0.004,0.0256,-0.0305,-0.0068,-0.0521,0.0149,0.0077,-0.0378,-0.0426,-0.0589,-0.2349,-0.0232,-0.0251,0.0556,-0.0213,0.0139,-0.0249,-0.0393,0.0523,-0.0311,0.0211,0.0365,0.0281,-0.022,-0.0564,-0.0443,-0.0532,-0.024,-0.02,-0.0418,-0.0719,-0.0116,-0.0757,-0.0205,0.0291,0.0036,0.075,0.0507,0.0418,0.0674,0.0654,0.0068,0.049,-0.122,0.0927,0.0584,0.0094,-0.0615,-0.0089,0.0213,0.0312,-0.0001,0.0668,0.021,0.107,0.0305,0.0169,0.0274,-0.0333,-0.0426,0.0069,-0.0205,-0.0017,-0.0324,0.0067,0.0076,-0.0245,-0.0262,-0.0273,-0.0039,-0.0008,-0.0545,-0.0136,-0.0252,0.0182,-0.0106,-0.0076,0.0183,0.0217,-0.0614,0.1934,-0.0553,-0.0187,0.0311,-0.0403,0.0439,-0.0173,-0.0593,-0.019,-0.0253,-0.0273,0.0005,0.005,0.0329,-0.0662,0.0221,0.0012,0.1069,0.0286,-0.0243,-0.0255,-0.0454,0.0104,0.0222,-0.0217,0.0088,-0.0781,0.0576,0.1274,0.0195,0.0172,0.082,-0.0168,-0.006,-0.0399,0.0594,0.0336,0.0497,0.0124,0.0291,-0.0128,-0.001,-0.0172,-0.0204,-0.1051,-0.0675,0.1193,-0.0405,0.038,-0.0091,0.0149,-0.028,0.0382,-0.0132,-0.0383,-0.0126,0.0214,0.0267,0.0199,-0.0083,-0.006,0.0227,-0.0761,0.0018,0.0826,-0.0129,-0.0614,-0.0703,-0.0085,0.0406,-0.0314,0.0335,0.0301,0.004,0.0478,0.0631,0.0422,-0.0803,0.0026,-0.0071,0.0138,0.0372,-0.0411,-0.001,0.0608,0.0422,-0.0241,0.001,-0.0597,0.0345,0.042,0.0119,-0.0116,-0.0032,-0.0133,-0.051,-0.0321,-0.0313,-0.0209,-0.0294,-0.0045,-0.0088,-0.0056,-0.0498,0.0481,-0.0094,0.0354,0.0038,0.0351,0.0465,0.0347,-0.0414,-0.0238,-0.0145,-0.0338,0.0288,-0.0097,0.0337,0.0642,-0.0059,0.0176,0.0432,-0.0603,-0.0216,-0.2388,0.0294,0.0387,0.0007,0.0286,-0.0487,0.0133,-0.0066,-0.0196,0.0656,0.0326,-0.0532,-0.0368,-0.0045,-0.0436,0.0043,0.0052,0.0048,-0.0408,0.0344,-0.0252,0.0451,-0.0299,-0.0873,0.0579,-0.0111,0.2401,0.0367,0.0517,-0.0378,0.055,0.0332,-0.0452,-0.0803,0.0817,0.0407,0.042,-0.0333,-0.0573,-0.0398,0.01,0.0211,-0.0009,-0.1265,-0.027,-0.0159,-0.0151,0.017,-0.0116,0.0196,0.0635,-0.0189,0.0254,0.0021,-0.072,-0.0368,-0.0962,0.0338,-0.018,0.0052,0.0391,-0.025,0.0139,-0.0309,0.0267,-0.029,-0.0261,-0.0628,0.0392,-0.0658,-0.0369,0.1103,0.0282,-0.0231,0.0591,0.0009,0.0502,-0.0252,-0.0286,-0.0276,0.0713,-0.0059,-0.0134,-0.0233,0.0281,0.0353,0.1038,-0.0386,0.0572,-0.0063,0.0268,0.0263,-0.0545,-0.0079,0.0551,-0.0143,-0.3027,0.0539,0.0068,0.0039,-0.0302,0.0143,0.0529,-0.0065,-0.0736,0.005,0.0115,0.0351,0.0487,-0.015,-0.0284,0.0486,0.048,-0.0753,0.0515,-0.024,0.0026,0.0706,0.2309,-0.0269,0.0198,0.003,-0.0034,-0.0137,0.0035,0.0206,-0.0107,-0.0177,0.076,-0.0635,0.032,0.0784,-0.0377,0.0414,0.0237,-0.0142,-0.0156,-0.0167,-0.01,-0.0576,0.106,-0.01,0.0184,-0.0417,-0.0308,0.0144,-0.0441,-0.0005,-0.0302,-0.0509,0.0424,-0.0191,-0.0189,-0.0189,-0.0344,-0.0268,0.0166,-0.0013,-0.0079,0.0073,-0.0133]}
{"key":"[Understanding the Advisor-advisee Relationship via Scholarly Data Analysis] Advisor-advisee relationship is important in academic networks due to its universality and necessity. Despite the increasing desire to analyze the career of newcomers, however, the outcomes of different collaboration patterns between advisors and advisees remain unknown. The purpose of this paper is to find out the correlation between advisors' academic characteristics and advisees' academic performance in Computer Science. Employing both quantitative and qualitative analysis, we find that with the increase of advisors' academic age, advisees' performance experiences an initial growth, follows a sustaining stage, and finally ends up with a declining trend. We also discover the phenomenon that accomplished advisors can bring up skilled advisees. We explore the conclusion from two aspects: (1) Advisees mentored by advisors with high academic level have better academic performance than the rest; (2) Advisors with high academic level can raise their advisees' h-index ranking. This work provides new insights on promoting our understanding of the relationship between advisors' academic characteristics and advisees' performance, as well as on advisor choosing.","layer":0,"vector":[0.0026,0.056,0.0066,-0.0228,0.0414,-0.012,0.0734,0.0632,0.0361,-0.0375,0.0264,-0.0279,0.0831,0.019,0.016,0.0014,-0.0432,0.0074,-0.0146,0.016,0.0224,-0.0583,-0.034,-0.1013,0.0346,0.0308,-0.0524,-0.0756,-0.0293,-0.1844,-0.0011,-0.0338,0.0385,0.0187,-0.0047,-0.0059,0.0109,0.0618,0.0067,0.0026,0.0364,-0.0111,-0.0193,-0.042,-0.0296,-0.0736,-0.0125,-0.0332,-0.0442,-0.0457,-0.0429,-0.0274,-0.0139,0.0263,0.0051,0.0703,0.0764,0.0041,0.0151,0.0395,0.0637,0.0283,-0.2369,0.081,0.0631,0.009,-0.0598,-0.0484,-0.0179,0.0471,0.0036,-0.0074,-0.001,0.0492,0.045,-0.0045,-0.0158,-0.0236,0.0231,0.042,0.0052,-0.0402,-0.023,-0.0304,0.0267,0.0185,0.0297,-0.0135,0.0476,-0.0305,0.011,-0.0333,-0.0479,-0.0201,-0.0139,-0.0316,0.0298,0.0092,-0.1094,0.2012,-0.08,0.0061,0.0793,0.0088,0.0273,-0.0587,-0.0021,-0.018,0.0163,0.0078,0.0287,0.0046,0.0044,-0.0169,0.0212,0.0298,0.0067,0.0388,0.0272,-0.0046,-0.0076,0.0601,0.0522,-0.0072,0.0066,-0.0823,-0.0131,0.1167,0.0161,0.016,0.0397,0.0001,-0.0971,-0.0397,-0.0157,0.0006,0.009,-0.0188,0.0385,-0.0093,-0.0012,-0.0503,-0.0186,-0.0667,-0.013,0.1375,-0.0501,0.0548,-0.0423,0.0485,-0.0124,0.011,-0.0823,-0.0369,0.0179,0.0489,0.0409,0.0427,-0.0298,0.01,-0.0656,-0.0338,-0.0446,0.1263,0.0334,-0.0734,-0.0015,0.0274,0.0129,-0.02,0.0281,0.0259,-0.0279,0.064,0.0684,-0.0076,-0.0338,-0.0305,-0.0161,0.0018,0.1081,0.001,-0.0406,0.0848,0.0289,0.002,-0.0224,-0.0373,0.019,0.0701,-0.0698,0.0561,-0.0267,0.0077,-0.0067,0.0371,-0.0261,-0.0647,-0.0105,-0.0265,0.0012,-0.0025,-0.0851,0.0518,-0.0627,0.0189,0.0157,-0.0344,0.0599,-0.0125,-0.0411,-0.02,0.0126,-0.0051,-0.0111,-0.0019,0.0232,0.0561,0.0066,0.0171,0.0568,-0.0408,-0.019,-0.2184,-0.018,-0.0288,-0.0267,0.0359,-0.0154,0.056,0.0105,0.0495,0.0513,0.0371,-0.005,-0.0412,0.0341,-0.0045,0.0587,0.0238,0.0373,-0.0047,-0.016,-0.0314,0.0372,-0.0551,-0.0754,0.0399,-0.0068,0.1744,0.0278,0.0009,-0.0285,0.0453,-0.0013,-0.0165,-0.1318,0.0332,0.0516,0.0847,-0.0554,-0.004,-0.0765,-0.0357,0.0365,-0.0118,-0.0398,-0.0684,-0.0076,-0.0069,-0.0192,-0.0475,0.0459,0.0007,-0.0154,0.0348,0.002,-0.0333,-0.0375,-0.0746,0.0427,0.0372,0.028,-0.0058,-0.0467,-0.0517,-0.0323,0.05,-0.0272,0.0057,0.0291,-0.0057,-0.0213,0.0047,0.0715,-0.0332,-0.0279,0.0308,-0.0353,0.0008,-0.0679,-0.0045,-0.0257,0.0287,-0.1154,-0.0057,0.0332,0.0141,-0.027,0.0542,-0.0111,0.0186,-0.0092,0.0031,0.0221,-0.0796,-0.0123,0.0051,-0.017,-0.2754,0.0869,0.0105,0.0309,-0.0648,0.0236,0.044,0.0026,-0.039,0.0344,0.0757,0.0152,-0.0373,-0.0198,0.0088,0.0224,0.0601,-0.0429,0.0158,-0.0061,0.0129,0.0325,0.2421,-0.0108,0.0164,0.0035,-0.0121,0.0835,0.0397,-0.0487,0.0429,0.0321,0.0684,-0.0319,0.0367,0.0739,-0.0071,0.0365,-0.003,0.0042,-0.016,0.002,-0.0429,-0.0384,0.1459,-0.0054,-0.0077,-0.0651,0.0216,0.0045,-0.0668,0.0011,0.0242,0.0075,-0.0017,0.0145,-0.0619,-0.0238,-0.0623,-0.0307,-0.0106,0.0076,-0.0218,0.0787,0.0228]}
{"key":"[Predicting United States policy outcomes with Random Forests] Two decades of U.S. government legislative outcomes, as well as the policy preferences of rich people, the general population, and diverse interest groups, were captured in a detailed dataset curated and analyzed by Gilens, Page et al. (2014). They found that the preferences of the rich correlated strongly with policy outcomes, while the preferences of the general population did not, except via a linkage with rich people's preferences. Their analysis applied the tools of classical statistical inference, in particular logistic regression. In this paper we analyze the Gilens dataset using the complementary tools of Random Forest classifiers (RFs), from Machine Learning. We present two primary findings, concerning respectively prediction and inference: (i) Holdout test sets can be predicted with approximately 70% balanced accuracy by models that consult only the preferences of rich people and a small number of powerful interest groups, as well as policy area labels. These results include retrodiction, where models trained on pre-1997 cases predicted \"future\" (post-1997) cases. The 20% gain in accuracy over baseline (chance), in this detailed but noisy dataset, indicates the high importance of a few wealthy players in U.S. policy outcomes, and aligns with a body of research indicating that the U.S. government has significant plutocratic tendencies. (ii) The feature selection methods of RF models identify especially salient subsets of interest groups (economic players). These can be used to further investigate the dynamics of governmental policy making, and also offer an example of the potential value of RF feature selection methods for inference on datasets such as this.","layer":1,"vector":[-0.0497,0.0056,0.0231,-0.0493,0.0595,0.0249,0.0472,0.0395,0.0095,0.0078,0.0028,0.0089,-0.0016,-0.0068,0.0297,0.0312,-0.0023,0.0285,-0.0477,0.0539,0.0066,-0.0435,-0.0394,-0.0774,0.0221,0.0271,-0.0298,-0.0405,-0.0631,-0.229,0.0497,-0.0756,0.0059,-0.0337,0.0369,-0.0318,0.0025,0.0564,-0.031,0.016,0.021,-0.0065,-0.0421,-0.0394,-0.0373,-0.0512,-0.0092,0.0049,-0.0716,-0.0019,-0.0105,-0.0296,0.0108,0.0397,0.0247,0.0312,0.0504,0.0381,0.0269,0.0373,0.0259,0.0295,-0.2481,0.0558,0.0416,0.0684,-0.0316,-0.0274,0.0174,0.0452,-0.0035,0.0735,0.003,0.0125,-0.005,0.0092,0.0075,-0.0164,-0.0011,0.0667,0.0055,-0.0293,-0.0874,0.0034,-0.0066,-0.0457,0.0185,-0.0522,0.0527,0.0063,-0.0353,0.0307,-0.0178,0.0225,-0.0704,0.0212,0.0716,0.0057,-0.0545,0.1859,-0.056,0.04,0.0259,-0.0304,0.0268,-0.0506,-0.0361,-0.0226,-0.0231,-0.0118,0.0311,-0.0057,0.024,0.0066,-0.0011,-0.0502,0.0691,0.0724,0.0036,-0.0047,-0.035,0.0305,0.0321,-0.0464,0.0248,-0.0686,0.0404,0.1457,0.0242,-0.0143,-0.0008,-0.0619,-0.0895,-0.0588,-0.0024,0.0081,0.0218,0.0273,-0.0068,-0.0006,-0.0384,-0.0162,0.0037,-0.0751,-0.0825,0.1318,-0.05,0.0356,-0.0454,0.0086,-0.0206,0.0194,-0.0275,-0.0251,0.0436,0.0109,0.0201,0.0463,-0.0269,-0.0073,0.0274,-0.0422,-0.0126,0.072,0.0014,-0.0665,-0.0528,0.0121,-0.0215,0.0067,0.0354,0.0385,0.0123,0.0124,0.0777,0.0294,-0.0552,-0.0205,-0.0153,-0.0085,0.0467,-0.0361,-0.0491,0.0358,0.0235,0.003,-0.0349,-0.054,0.012,0.0598,-0.0012,0.0197,-0.0559,-0.008,-0.0095,-0.0335,0.0036,-0.0188,0.0531,-0.0418,-0.0396,0.0312,-0.0494,0.0147,-0.0284,0.0267,-0.0006,-0.001,0.0807,-0.0219,-0.0055,0.0658,0.0697,-0.0067,-0.0329,0.0139,0.0005,0.0288,-0.0041,0.0774,0.0663,-0.0516,-0.0415,-0.2115,-0.0286,-0.0097,-0.0278,0.0438,-0.0949,0.003,-0.0145,0.0249,0.1015,0.0098,-0.0271,-0.0446,0.0987,0.0004,0.0568,0.0133,0.0242,-0.0412,0.0516,0.0015,0.0223,-0.0356,-0.0973,0.0562,-0.0103,0.1799,0.0089,-0.0247,0.0085,0.0312,0.0383,-0.0466,-0.1149,0.0774,0.0744,0.0388,-0.0091,-0.0612,-0.0157,-0.0005,0.0378,-0.0312,-0.0635,-0.0268,-0.005,-0.0285,0.0261,-0.0755,0.0061,0.0638,-0.0182,0.0482,-0.0064,0.0258,0.007,-0.0828,0.0443,0.0052,0.0273,0.0328,-0.0889,0.0247,-0.0565,0.0334,-0.0262,-0.0425,-0.0491,0.023,-0.011,-0.0061,0.1116,-0.0353,-0.0553,0.0572,0.0217,0.043,-0.0034,-0.0138,-0.0149,0.0733,-0.007,0.0222,0.0205,0.0004,-0.0156,0.0822,-0.0059,0.0607,-0.0038,-0.0045,0.038,-0.01,-0.037,0.0539,0.0334,-0.2571,0.0469,-0.0093,0.0155,0.0005,-0.0277,0.0402,0.0041,-0.0001,-0.0109,0.063,0.0779,0.0479,-0.0613,0.0074,0.0135,0.0851,-0.0369,0.0532,-0.0536,0.0486,0.0056,0.2026,-0.0189,0.0259,0.0059,-0.0114,-0.0172,0.0361,-0.0664,0.0573,0.0362,0.077,-0.0729,0.0608,0.0688,-0.0236,-0.0194,0.0044,-0.0227,-0.0369,-0.0178,-0.0379,-0.0509,0.1156,-0.019,-0.0259,-0.0376,0.0208,0.069,-0.0222,-0.0063,-0.0981,-0.0035,0.0048,0.0051,-0.0624,0.0085,-0.0317,-0.0367,-0.0506,-0.0486,0.0008,-0.0379,0.004]}
{"key":"[Translation Between Waves, wave2wave] The understanding of sensor data has been greatly improved by advanced deep learning methods with big data. However, available sensor data in the real world are still limited, which is called the opportunistic sensor problem. This paper proposes a new variant of neural machine translation seq2seq to deal with continuous signal waves by introducing the window-based (inverse-) representation to adaptively represent partial shapes of waves and the iterative back-translation model for high-dimensional data. Experimental results are shown for two real-life data: earthquake and activity translation. The performance improvements of one-dimensional data was about 46% in test loss and that of high-dimensional data was about 1625% in perplexity with regard to the original seq2seq.","layer":0,"vector":[-0.0348,-0.0387,0.0331,-0.0228,0.0194,0.0414,0.0073,0.0029,0.0142,-0.0188,-0.0098,-0.0696,0.0302,0.0383,0.0037,0.0235,-0.0033,0.0692,-0.0318,0.0322,0.0346,-0.0149,-0.0318,-0.0308,0.0184,0.0303,-0.0588,0.0409,-0.039,-0.2343,0.0236,-0.032,0.0262,-0.002,0.0359,0.0141,-0.0058,0.0476,-0.0051,0.0612,0.0058,-0.0173,0.0018,-0.03,0.0314,-0.077,-0.0158,-0.0323,0.0236,-0.0262,0.0177,-0.0132,0.0462,0.0078,0.0578,0.0111,0.0287,0.0338,0.0642,0.0134,0.0418,0.0337,-0.1683,0.0552,0.0223,0.0299,-0.0018,-0.0247,0.0127,0.033,-0.0463,0.0164,0.0312,0.031,0.0302,-0.0072,-0.0243,-0.0445,-0.043,0.0149,0.0465,-0.0588,-0.0119,-0.0425,-0.0352,-0.0221,0.0202,-0.0426,0.0051,0.023,-0.0488,-0.0563,-0.0226,0.0374,-0.082,-0.0568,0.024,0.0651,-0.0213,0.1801,-0.0257,-0.0031,0.0216,0.0008,0.055,-0.0201,-0.0479,-0.0408,-0.0436,0.0358,-0.059,-0.0313,0.0195,-0.0063,-0.0063,-0.0198,0.0565,0.053,-0.0021,-0.0023,-0.0463,-0.0069,0.0427,-0.0278,0.0155,-0.0508,0.0393,0.1067,0.0864,0.0762,0.0508,-0.0194,-0.0634,-0.0089,-0.0029,0.0105,0.0315,-0.0267,-0.0326,0.0541,-0.0237,-0.059,0.0384,-0.0547,-0.0223,0.1132,-0.0393,-0.0128,-0.0558,-0.0392,-0.0417,0.0455,-0.0073,-0.0375,0.0944,0.0602,0.0162,0.0297,-0.0574,0.0124,-0.014,-0.0475,-0.0301,0.0819,0.0151,-0.0938,-0.0635,0.0114,0.0106,-0.0431,0.0507,0.0377,-0.0299,0.0657,0.0955,0.0548,-0.0465,-0.0048,-0.0297,0.0294,-0.0172,-0.0445,-0.0141,0.015,0.019,-0.0584,0.0016,-0.0472,0.0107,0.0572,-0.0201,0.021,-0.0154,0.0217,-0.019,-0.0315,-0.0189,0.0228,0.0053,-0.0316,0.0339,0.0103,-0.0508,0.0159,-0.0209,-0.0107,0.0347,0.0138,0.001,0.0507,0.0339,-0.0092,0.0521,-0.0459,-0.0086,-0.0455,-0.0311,0.0403,0.0194,0.0753,0.0248,-0.0266,-0.0973,-0.2635,-0.0391,0.0469,0.0031,0.0516,-0.0459,0.0113,-0.0017,0.0684,0.0545,0.0716,-0.013,-0.0296,0.0131,-0.0116,0.0165,0.0601,0.0487,-0.0355,-0.0212,-0.0275,0.0405,-0.0318,-0.0729,0.0074,-0.0025,0.1792,0.0142,0.0433,-0.0153,-0.0144,0.0158,0.0038,-0.12,0.0527,0.0318,0.0944,0.0687,-0.0593,-0.0501,-0.0206,-0.0172,0.0121,-0.0729,-0.0658,-0.0473,-0.0243,0.0243,-0.0423,0.0039,0.0073,-0.0331,0.0259,0.007,-0.004,-0.0014,-0.0888,0.0338,-0.0553,-0.043,0.0207,0.0192,-0.015,-0.0367,0.0452,0.0263,-0.0133,-0.0514,0.0001,-0.0183,-0.046,0.0924,0.0002,-0.0117,0.0577,0.0456,0.0214,-0.0497,-0.0361,-0.0277,0.0878,-0.0321,0.0426,0.0564,0.0594,0.0271,0.0665,-0.0316,0.0026,-0.0086,0.0178,0.0198,0.0048,-0.0325,-0.002,-0.0441,-0.3252,0.0203,0.0032,0.0153,-0.0691,-0.0209,0.0102,0.0282,-0.0787,0.0261,-0.0817,0.0163,0.0387,0.0071,0.0329,0.0383,0.081,-0.0566,0.0408,-0.0485,0.0511,0.0606,0.2299,-0.025,0.0379,0.0215,-0.0331,-0.0211,0.0383,-0.0189,0.0093,0.0149,0.0513,-0.0422,0.0338,0.0456,-0.0166,0.0482,0.0321,-0.0199,0.0242,0.0373,-0.0264,-0.0344,0.1226,0.0187,-0.038,-0.0602,-0.0465,0.0415,-0.0324,0.0027,0.0223,0.0088,0.0262,0.0352,-0.0684,-0.0696,-0.0376,0.008,0.0134,-0.0677,-0.0025,-0.0395,0.0173]}
{"key":"[Recursive Multi-model Complementary Deep Fusion forRobust Salient Object Detection via Parallel Sub Networks] Fully convolutional networks have shown outstanding performance in the salient object detection (SOD) field. The state-of-the-art (SOTA) methods have a tendency to become deeper and more complex, which easily homogenize their learned deep features, resulting in a clear performance bottleneck. In sharp contrast to the conventional ``deeper'' schemes, this paper proposes a ``wider'' network architecture which consists of parallel sub networks with totally different network architectures. In this way, those deep features obtained via these two sub networks will exhibit large diversity, which will have large potential to be able to complement with each other. However, a large diversity may easily lead to the feature conflictions, thus we use the dense short-connections to enable a recursively interaction between the parallel sub networks, pursuing an optimal complementary status between multi-model deep features. Finally, all these complementary multi-model deep features will be selectively fused to make high-performance salient object detections. Extensive experiments on several famous benchmarks clearly demonstrate the superior performance, good generalization, and powerful learning ability of the proposed wider framework.","layer":2,"vector":[-0.0253,0.0049,0.0276,-0.0614,0.0517,0.032,0.0499,-0.0081,0.0341,-0.0205,0.0292,-0.0803,0.0031,0.0675,0.0644,-0.0439,0.0446,0.0129,-0.0171,-0.0178,0.0043,-0.0062,-0.0085,-0.0698,0.0501,-0.0177,-0.0162,-0.0381,-0.0569,-0.2374,-0.0116,-0.0267,0.0182,-0.0189,0.0178,-0.0437,-0.0297,0.017,-0.0551,0.0203,0.0035,0.0176,-0.0246,-0.0719,-0.0256,-0.0229,0.0021,-0.008,-0.0085,-0.0634,0.0554,-0.0671,-0.0004,0.0306,0.0018,0.0433,0.0105,0.0365,0.0506,0.0217,0.0014,0.0907,-0.1417,0.0077,0.0372,0.0465,-0.0515,-0.0376,0.0229,0.0146,-0.0126,0.0666,0.0088,-0.0045,0.0144,-0.0105,-0.0007,-0.0669,0.0119,0.0037,0.0324,-0.023,-0.0413,-0.0628,0.011,-0.0482,-0.0093,-0.0538,0.0446,-0.0533,-0.0737,-0.0021,0.0095,0.0367,-0.0172,0.0047,0.0112,-0.0397,-0.043,0.1817,-0.0337,0.0219,0.0834,-0.0071,0.0132,0.0061,-0.0364,0.0102,-0.0484,0.0186,0.0174,-0.0412,0.0062,-0.0433,0.029,-0.0072,0.0619,0.045,-0.0084,-0.012,-0.0487,-0.0141,0.0647,-0.0779,-0.0194,-0.064,0.0239,0.1199,0.0524,0.0441,0.0645,0.0054,-0.0298,-0.0563,-0.005,0.03,0.0435,0.0074,0.0322,-0.0236,-0.079,-0.0571,0.032,-0.0873,-0.0091,0.0909,-0.063,0.0183,-0.0385,-0.0462,-0.028,0.0071,-0.0225,-0.0278,0.0325,0.0095,0.0381,0.048,-0.0429,0.0417,0.0038,-0.0385,-0.0276,0.1054,0.0476,-0.1253,-0.0214,-0.0011,0.0154,-0.0253,0.0374,-0.0165,0.023,-0.0252,0.058,0.0093,-0.1011,0.0239,-0.0375,-0.0105,-0.003,-0.0884,-0.0218,0.0394,0.0471,-0.0522,0.045,-0.0722,0.008,0.0823,-0.0329,0.0343,-0.0462,-0.015,0.007,-0.0138,0.0047,-0.01,0.0133,-0.0024,-0.0097,-0.0174,-0.0369,-0.0031,-0.041,0.023,-0.0313,0.0001,-0.0224,0.0259,-0.0052,0.0434,0.0942,-0.0198,-0.0271,0.0097,0.0469,0.0524,-0.0103,0.0291,0.0454,-0.0711,-0.0471,-0.2496,0.0302,-0.0109,-0.0177,0.03,-0.0678,0.0102,0.0096,0.0495,0.0549,0.0665,-0.0169,-0.0174,0.0008,0.0545,0.0957,-0.0275,0.0574,-0.0205,0.0139,-0.0117,0.0405,0.026,-0.0509,0.049,0.0002,0.2146,0.0177,0.0429,-0.0115,0.0379,0.0667,-0.0204,-0.1034,0.0045,0.0351,0.0695,0.0243,-0.0631,-0.0003,-0.0581,0.0415,0.0397,-0.1188,-0.0179,-0.0203,-0.0127,0.0289,-0.0127,0.0289,0.0467,-0.0514,-0.0018,0.0113,-0.0256,-0.0255,-0.088,0.007,-0.0575,0.0054,-0.0099,-0.0295,-0.0042,-0.0651,0.0971,0.0035,-0.0754,-0.0603,0.0233,-0.0333,-0.0235,0.0683,0.0551,0.0108,0.0549,0.0331,0.0807,-0.0055,-0.0593,-0.0363,0.0974,-0.0333,-0.0348,0.0283,0.0702,0.0139,0.0883,-0.0071,0.0066,-0.0203,0.0381,0.0192,-0.0251,0.0072,0.0165,-0.0046,-0.3077,0.0315,0.0088,0.0423,0.0031,0.0524,0.0503,0.0075,0.0104,-0.0267,0.021,0.0635,0.0128,0.0104,-0.0106,0.0412,0.0631,-0.0474,0.049,-0.0323,-0.038,0.0366,0.1758,-0.0311,0.0014,0.0102,-0.0254,-0.0715,0.0072,-0.0076,0.0094,0.0232,0.0503,-0.0853,0.0113,0.1026,-0.0152,0.0432,0.0542,-0.0047,0.0223,-0.0163,-0.0165,-0.0698,0.0835,0.0128,0.009,0.0116,0.027,0.0342,-0.004,-0.0209,0.0041,-0.0196,0.0572,0.0025,-0.0161,-0.0299,-0.0509,-0.01,0.0722,-0.0431,-0.0344,0.017,-0.0095]}
{"key":"[Sparse Boltzmann Machines with Structure Learning as Applied to Text Analysis] We are interested in exploring the possibility and benefits of structure learning for deep models. As the first step, this paper investigates the matter for Restricted Boltzmann Machines (RBMs). We conduct the study with Replicated Softmax, a variant of RBMs for unsupervised text analysis. We present a method for learning what we call Sparse Boltzmann Machines, where each hidden unit is connected to a subset of the visible units instead of all of them. Empirical results show that the method yields models with significantly improved model fit and interpretability as compared with RBMs where each hidden unit is connected to all visible units.","layer":1,"vector":[-0.0365,-0.008,0.0269,0.0355,0.0478,0.0198,0.0239,0.0128,0.0428,-0.0466,0.0071,-0.0397,0.0156,0.049,0.0753,-0.0086,0.0095,0.0338,-0.0692,-0.0105,0.0524,-0.0511,0.0283,-0.0272,0.0508,0.0085,-0.0301,-0.051,-0.0373,-0.2521,0.0327,-0.0325,0.0597,0.0104,0.0357,0.0207,-0.0564,0.0197,-0.0067,0.06,0.0383,-0.0092,-0.0304,-0.0325,0.0011,-0.055,0.0008,-0.0384,-0.0404,-0.0589,0.0111,-0.0343,0.0455,0.0299,0.0563,0.0151,0.0557,0.0358,0.0421,-0.0042,0.012,0.0577,-0.1628,0.0666,0.0796,0.0518,-0.0719,-0.0151,0.0259,0.0596,0.0014,0.0361,0.0143,0.0678,-0.0261,0.018,0.0128,-0.0469,0.0161,0.0024,-0.0008,-0.0295,-0.0204,-0.0094,-0.0217,-0.0779,0.0195,-0.0644,-0.0024,-0.006,-0.046,-0.0091,0.0025,0.0223,-0.0815,-0.0335,0.0475,0.0137,-0.0157,0.2121,-0.0468,0.0488,0.0465,-0.0297,0.0233,-0.0401,-0.0103,-0.0045,-0.0505,-0.0236,-0.0395,-0.023,-0.0052,-0.0421,0.042,-0.0308,0.079,0.0273,-0.0482,-0.009,-0.0345,0.0157,0.0096,-0.0241,0.0313,-0.0317,0.0283,0.1378,0.067,0.0074,0.0592,-0.0245,-0.0308,-0.0227,0.0083,0.0197,0.0098,0.0045,0.0101,-0.0319,-0.0208,-0.0604,0.0202,-0.081,-0.0914,0.0918,-0.0523,0.0301,-0.0657,-0.0269,-0.0163,-0.0282,-0.0124,-0.0454,0.0535,0.0333,0.0085,0.0712,-0.063,-0.0126,0.0196,-0.0685,-0.0355,0.061,0.0322,-0.0908,0.016,-0.0115,-0.0152,-0.021,0.0447,0.0446,-0.0132,0.0799,0.0461,0.0325,-0.0429,0.0189,0.0135,0.0307,-0.012,-0.0594,-0.0386,0.0522,0.0581,-0.0376,0.0133,-0.0809,0.0208,0.0493,-0.0144,0.05,-0.0472,-0.0045,-0.0278,0.0113,0.0271,-0.0259,-0.013,-0.0246,-0.0003,0.0094,-0.0746,-0.0137,-0.0178,0.0051,0.0072,0.0079,0.0557,0.0454,-0.0393,-0.0145,0.0559,-0.0441,-0.0181,-0.0001,-0.008,0.0208,-0.021,0.013,0.0387,-0.0674,-0.093,-0.2364,0.0042,0.0487,-0.0271,0.0449,-0.0551,0.0231,-0.0252,0.0686,0.0698,0.0495,-0.0481,-0.0563,-0.0259,0.0004,0.0782,-0.0128,0.0449,-0.0294,0.0491,0.0188,-0.0176,-0.0063,-0.0191,0.0521,0.0035,0.1896,0.0333,0.0468,0.0033,0.0246,0.0423,-0.0081,-0.1002,0.0349,0.0431,0.05,0.0268,-0.042,-0.0063,-0.0357,0.0063,0.0002,-0.0897,-0.0641,-0.0265,0.0133,-0.0105,-0.0825,0.0132,0.0316,-0.045,0.0788,0.006,-0.0096,-0.0622,-0.1121,0.0185,-0.0508,0.0245,0.034,-0.0547,-0.0077,-0.0703,0.0342,-0.0103,-0.0272,-0.0057,0.0224,-0.0071,-0.0049,0.0564,-0.0335,0.0264,0.0453,0.0258,0.0466,-0.0333,-0.0826,0.0021,0.0805,-0.0138,0.0501,-0.0088,0.0391,0.0564,0.1149,-0.0404,0.0084,0.0054,0.0155,-0.0003,-0.0568,0.0062,0.0316,-0.0457,-0.2623,0.0303,-0.0023,0.0434,-0.0075,-0.0107,0.0604,0.0129,-0.054,0.0002,-0.0016,0.0589,0.0164,-0.0142,-0.0179,0.0596,0.0686,-0.06,0.0627,-0.0731,-0.0093,0.0344,0.2265,-0.0878,0.0137,0.0066,0.0043,0.0172,-0.0162,-0.0002,0.0156,0.0032,0.1072,-0.0241,0.0426,0.0744,-0.0202,0.0414,0.0646,-0.0205,-0.0067,0.0213,-0.0644,-0.0557,0.0985,-0.0131,-0.012,-0.0464,0.0021,0.0201,-0.0282,-0.0087,-0.0192,0.0224,0.0216,0.0185,-0.0314,-0.0236,0.0091,-0.0311,-0.0279,-0.0459,-0.0361,0.0448,-0.0502]}
{"key":"[Binary Classifier Calibration: Non-parametric approach] Accurate calibration of probabilistic predictive models learned is critical for many practical prediction and decision-making tasks. There are two main categories of methods for building calibrated classifiers. One approach is to develop methods for learning probabilistic models that are well-calibrated, ab initio. The other approach is to use some post-processing methods for transforming the output of a classifier to be well calibrated, as for example histogram binning, Platt scaling, and isotonic regression. One advantage of the post-processing approach is that it can be applied to any existing probabilistic classification model that was constructed using any machine-learning method. In this paper, we first introduce two measures for evaluating how well a classifier is calibrated. We prove three theorems showing that using a simple histogram binning post-processing method, it is possible to make a classifier be well calibrated while retaining its discrimination capability. Also, by casting the histogram binning method as a density-based non-parametric binary classifier, we can extend it using two simple non-parametric density estimation methods. We demonstrate the performance of the proposed calibration methods on synthetic and real datasets. Experimental results show that the proposed methods either outperform or are comparable to existing calibration methods.","layer":0,"vector":[-0.0197,0.0208,0.0453,-0.0586,0.0066,0.0368,0.0651,0.0398,0.0204,-0.0296,0.0241,-0.0613,-0.0242,0.0239,0.0175,-0.0026,-0.001,0.0744,-0.0033,0.0055,0.0327,0.0051,-0.0166,-0.033,0.0353,0.0448,-0.0373,-0.0475,-0.0726,-0.2308,-0.003,-0.0521,0.065,-0.0891,0.0201,-0.032,-0.0055,0.0384,-0.0211,0.0502,0.0307,0.0109,-0.0468,-0.0673,-0.0053,-0.0458,-0.0123,-0.0328,-0.0327,-0.0109,0.025,-0.0617,0.0213,0.014,0.0212,0.0301,0.0368,0.0369,0.081,0.0365,0.0246,0.0649,-0.1683,0.0596,0.0403,0.0473,-0.0258,-0.048,-0.0013,0.0529,0.0112,0.0638,0.017,0.0387,0.025,-0.0209,0.0212,-0.0582,-0.026,-0.0028,-0.0063,0.0032,-0.0534,0.0046,-0.0312,-0.0385,0.0347,-0.0179,0.0519,-0.0053,-0.0303,-0.0157,-0.0713,0.0278,-0.1002,-0.0011,0.0231,0.0086,-0.0117,0.1926,-0.0613,0.0267,0.0114,-0.0045,0.0562,-0.0381,-0.0594,-0.0197,-0.0597,-0.0143,-0.0217,-0.0205,0.0153,-0.0586,0.0083,0.0149,0.0345,0.0517,0.011,-0.0261,-0.006,-0.0348,0.0641,-0.0374,0.0153,-0.0169,0.0106,0.1373,0.016,0.0057,0.0325,-0.0441,-0.08,-0.0225,-0.005,0.0298,0.0541,-0.0102,0.0214,0.0294,-0.0236,-0.0322,-0.0006,-0.0806,-0.084,0.1058,-0.051,0.0873,-0.0377,-0.0454,-0.0314,0.0331,-0.0184,-0.0378,0.0306,0.0154,0.0305,0.04,-0.06,-0.0093,-0.025,-0.0651,-0.0061,0.0917,0.0187,-0.0268,-0.0511,0.0204,0.0181,-0.0124,0.0247,0.0632,-0.0217,0.0384,0.0429,0.0525,-0.056,0.0038,0.0323,0.0006,0.0364,-0.0173,-0.0348,0.0309,0.0352,-0.0287,-0.0272,-0.0613,0.0613,0.0557,0.0333,0.0045,-0.0313,-0.0371,-0.0066,-0.0155,-0.038,-0.0019,0.0441,-0.0868,0.0389,0.0676,-0.029,0.0198,-0.0219,0.0517,-0.0099,0.0088,0.0622,0.0588,-0.0222,-0.019,0.0672,-0.0409,-0.0425,0.0196,0.0511,0.0439,0.02,0.0162,0.0258,-0.0198,-0.0621,-0.2058,0.0219,0.0195,0.0028,0.0556,-0.0777,0.0377,0.0352,0.0118,0.0719,0.0638,-0.0355,-0.0405,0.0521,-0.0133,0.0512,0.0556,0.0154,-0.0393,0.0172,-0.026,0.0274,-0.0376,-0.0694,0.0299,-0.0203,0.1648,-0.0097,0.058,-0.0463,0.0224,-0.0078,-0.0397,-0.1029,0.075,0.0424,0.0344,-0.0341,-0.0988,0.004,-0.032,0.0176,0.018,-0.119,-0.0595,-0.0478,-0.0417,0.0367,-0.0321,-0.0001,0.0274,-0.0124,0.0676,-0.0398,0.0057,-0.0244,-0.0651,0.0544,-0.0216,-0.0007,0.0244,-0.0714,0.047,-0.075,0.0017,-0.0324,-0.0193,-0.0505,0.0011,-0.0457,-0.0051,0.1138,0.0061,-0.0385,0.0737,0.0134,0.0286,-0.06,-0.004,-0.0347,0.0354,-0.0045,0.0686,0.0637,0.0324,0.0139,0.0918,0.0231,0.0163,0.0342,0.0033,0.0309,-0.0772,0.0247,0.0608,-0.003,-0.2923,0.0368,-0.0112,0.029,-0.0013,-0.0352,0.0608,0.0103,-0.0603,-0.0083,-0.0184,-0.0032,0.0656,-0.0236,0.0006,0.0032,0.0274,-0.0665,0.0548,-0.0478,0.0186,-0.002,0.2161,-0.0573,0.0192,0.029,-0.0104,0.0319,0.0402,-0.0148,-0.0051,-0.0046,0.0836,-0.0538,0.0144,0.1024,-0.0134,0.0065,0.0354,-0.0228,0.0236,0.0184,-0.0624,-0.0739,0.1329,-0.015,-0.0388,-0.0307,0.019,0.0385,-0.0374,0.04,-0.0411,0.03,0.0139,0.0157,-0.0432,-0.0022,-0.0669,-0.0464,0.0113,-0.0507,-0.0229,-0.0258,-0.0046]}
{"key":"[Check-N-Run: A Checkpointing System for Training Deep Learning Recommendation Models] Checkpoints play an important role in training long running machine learning (ML) models. Checkpoints take a snapshot of an ML model and store it in a non-volatile memory so that they can be used to recover from failures to ensure rapid training progress. In addition, they are used for online training to improve inference prediction accuracy with continuous learning. Given the large and ever increasing model sizes, checkpoint frequency is often bottlenecked by the storage write bandwidth and capacity. When checkpoints are maintained on remote storage, as is the case with many industrial settings, they are also bottlenecked by network bandwidth. We present Check-N-Run, a scalable checkpointing system for training large ML models at Facebook. While Check-N-Run is applicable to long running ML jobs, we focus on checkpointing recommendation models which are currently the largest ML models with Terabytes of model size. Check-N-Run uses two primary techniques to address the size and bandwidth challenges. First, it applies incremental checkpointing, which tracks and checkpoints the modified part of the model. Incremental checkpointing is particularly valuable in the context of recommendation models where only a fraction of the model (stored as embedding tables) is updated on each iteration. Second, Check-N-Run leverages quantization techniques to significantly reduce the checkpoint size, without degrading training accuracy. These techniques allow Check-N-Run to reduce the required write bandwidth by 6-17x and the required capacity by 2.5-8x on real-world models at Facebook, and thereby significantly improve checkpoint capabilities while reducing the total cost of ownership.","layer":4,"vector":[-0.078,-0.0082,0.0219,0.0088,0.0232,0.0764,0.0019,0.0177,0.0259,-0.055,0.0511,-0.0461,0.0243,0.0493,0.0199,0.0196,0.0665,0.0081,-0.0173,-0.0152,0.0181,-0.0434,-0.028,-0.0539,0.01,0.0298,-0.0328,-0.0519,-0.0651,-0.2457,-0.0014,-0.0335,0.0086,0.0429,0.002,-0.0305,0.0254,0.0302,0.0145,0.0166,0.0422,0.0423,-0.0455,-0.0072,0.0119,-0.0274,-0.0325,-0.045,-0.0213,-0.0292,0.0413,-0.0577,0.037,0.0023,-0.0126,0.0816,0.0429,0.0748,0.0127,0.0283,0.0361,-0.0062,-0.1468,0.0059,-0.0004,0.0117,-0.058,-0.0076,-0.033,0.0915,-0.0062,0.0362,0.0008,0.0679,0.0164,0.0093,0.0207,0.0037,0.0012,0.0226,0.0091,-0.0525,-0.0468,-0.0222,-0.0196,-0.0495,0.0293,0.0045,0.0289,-0.0063,-0.0127,-0.0059,0.0052,0.0355,-0.0585,-0.0416,0.0328,0.0398,-0.0537,0.2191,-0.0413,0.0518,-0.0116,-0.0134,0.0179,-0.0248,-0.0016,-0.0256,-0.0093,-0.0145,-0.0087,-0.0222,0.056,-0.0604,0.0634,0.0589,0.0921,0.0273,-0.0459,0.0276,-0.0217,0.0419,0.0808,-0.0333,0.0053,-0.0686,0.0002,0.1566,0.0535,0.0121,0.0093,-0.0271,-0.0727,-0.0384,0.005,0.0465,0.0036,-0.0499,0.0286,-0.0241,-0.0278,-0.0237,0.0392,-0.071,-0.0216,0.1229,-0.019,-0.0,-0.0537,-0.0503,-0.0149,0.0427,-0.0241,-0.0437,0.0478,0.0166,0.0251,0.0632,-0.0829,0.0129,-0.0277,-0.0778,-0.0697,0.056,0.0301,-0.0691,-0.0458,-0.0057,-0.0046,-0.0463,0.032,0.0084,-0.0202,0.0003,0.0598,0.0481,-0.0968,-0.0167,0.0075,0.0051,0.0093,-0.0596,-0.0442,0.0504,0.0118,-0.051,0.0123,-0.0857,-0.0,0.033,-0.0121,0.0102,-0.0195,-0.0588,-0.0042,-0.0364,-0.0363,0.0072,0.0225,-0.0641,0.017,0.0157,-0.0359,0.0325,0.0101,0.004,-0.0512,-0.0298,0.0591,-0.0003,-0.0294,-0.0073,0.0337,0.0026,-0.0228,-0.0088,0.0312,0.0183,0.0196,0.0804,0.0687,-0.0115,-0.0385,-0.234,-0.0353,0.0025,-0.0057,0.0622,-0.0718,0.0461,0.0137,0.0436,0.0554,0.0537,-0.032,-0.0239,0.0214,-0.0126,0.0784,0.0396,0.0165,-0.0255,0.0023,-0.0173,0.0583,-0.0453,-0.1013,0.0554,0.0174,0.2272,0.0191,0.0323,-0.0365,0.0314,0.0226,-0.025,-0.1308,0.0388,0.0157,0.0697,0.0131,-0.0439,0.0005,-0.0417,0.021,-0.0047,-0.1006,-0.0209,-0.0326,-0.0503,0.0276,-0.0411,0.0177,0.0236,-0.0218,0.0354,0.0271,-0.0125,-0.0392,-0.08,0.0437,-0.0561,0.0549,0.0212,-0.0637,-0.0057,-0.04,0.0743,-0.0436,-0.0096,-0.0303,0.0286,-0.0237,0.001,0.0369,-0.0503,-0.0091,0.0715,-0.0245,0.0302,-0.0463,-0.0745,-0.0011,0.0629,-0.0371,0.043,0.0369,0.0754,0.0255,0.093,0.005,0.0544,0.023,0.01,0.0093,-0.1128,-0.0255,0.0641,0.0032,-0.2946,0.0542,-0.0366,0.0525,-0.0483,0.0143,0.0304,0.0332,-0.0009,0.0203,-0.0186,0.0322,0.0271,-0.0114,0.0211,0.0299,0.051,-0.0392,0.0163,-0.0337,0.0242,0.002,0.1682,-0.0236,0.0388,0.0261,-0.0246,0.0252,0.0756,-0.0185,-0.0088,-0.0178,0.097,-0.0199,0.0283,0.0695,-0.0447,0.041,0.0688,-0.0176,0.0119,-0.0301,-0.0226,-0.019,0.0475,-0.0372,-0.0253,-0.0412,0.0042,0.0168,-0.0098,-0.006,-0.0466,0.0201,0.0558,0.0691,-0.0208,-0.0513,-0.0467,-0.0548,0.0097,-0.028,0.0036,-0.0186,-0.0031]}
{"key":"[A Modality-Adaptive Method for Segmenting Brain Tumors and Organs-at-Risk in Radiation Therapy Planning] In this paper we present a method for simultaneously segmenting brain tumors and an extensive set of organs-at-risk for radiation therapy planning of glioblastomas. The method combines a contrast-adaptive generative model for whole-brain segmentation with a new spatial regularization model of tumor shape using convolutional restricted Boltzmann machines. We demonstrate experimentally that the method is able to adapt to image acquisitions that differ substantially from any available training data, ensuring its applicability across treatment sites; that its tumor segmentation accuracy is comparable to that of the current state of the art; and that it captures most organs-at-risk sufficiently well for radiation therapy planning purposes. The proposed method may be a valuable step towards automating the delineation of brain tumors and organs-at-risk in glioblastoma patients undergoing radiation therapy.","layer":1,"vector":[-0.054,-0.0242,0.0118,-0.0075,0.0137,0.0174,0.0619,-0.0264,0.0057,0.0014,0.0193,-0.0538,0.0183,0.0442,-0.0196,0.0134,-0.0139,0.0416,-0.0395,0.032,0.0023,-0.0405,0.0223,-0.0377,0.0068,-0.0106,-0.0398,-0.0307,-0.0716,-0.2221,0.0371,-0.0165,0.0106,-0.0294,-0.0072,-0.027,-0.0375,0.0808,-0.021,0.0433,0.0091,-0.0013,-0.0286,-0.0432,-0.0393,-0.0542,-0.0248,-0.0173,-0.012,-0.027,0.0439,-0.072,0.0225,0.0553,0.0219,-0.0122,0.0586,0.0793,0.0461,0.0908,0.0045,0.0651,-0.1627,0.051,0.0272,0.0211,-0.0847,-0.0193,0.0194,0.0273,-0.0173,0.029,0.0226,0.0185,0.0045,-0.0524,0.0178,0.006,0.0103,0.0411,0.0118,0.0184,-0.0644,-0.0258,-0.0244,-0.048,-0.0085,-0.0898,0.0167,0.0243,-0.0389,-0.0071,-0.0171,0.049,-0.0316,-0.0773,0.0652,0.0284,-0.0046,0.1767,-0.0381,0.0511,0.0542,0.0089,-0.0041,-0.0187,-0.0454,-0.022,-0.0046,0.0215,-0.003,-0.0214,0.0356,-0.0039,0.0123,0.0259,0.071,0.0459,-0.0248,-0.0534,-0.0439,-0.0081,0.0366,-0.0203,0.0174,-0.0516,0.0624,0.1164,0.0691,0.0195,0.0699,0.035,-0.0285,0.0038,0.0029,0.024,0.0294,-0.0421,-0.0263,0.022,-0.0094,-0.0782,0.0153,-0.0839,-0.0275,0.11,-0.0744,0.0192,-0.0336,-0.0575,-0.0111,0.0096,-0.0112,-0.0142,0.0253,0.049,-0.0046,0.0314,-0.0455,-0.0105,-0.0088,-0.072,-0.0447,0.1541,0.0424,-0.0594,-0.056,-0.0134,0.052,0.0153,0.0458,0.0498,0.0304,-0.0096,0.0721,-0.0018,-0.0807,0.0153,0.0455,-0.0197,0.0011,-0.0616,-0.0379,0.0211,0.0098,-0.0503,0.0219,-0.0322,0.0336,0.0612,-0.0168,0.0604,-0.0455,-0.041,-0.0147,-0.0474,-0.0544,-0.0657,-0.002,-0.0041,0.0207,-0.016,-0.0419,0.034,-0.0054,-0.0131,-0.0288,0.0076,0.0105,0.0724,-0.0589,-0.0176,0.0841,-0.0081,-0.0379,0.0928,0.0054,0.0051,0.0156,0.041,0.0927,-0.0289,-0.067,-0.233,0.0044,-0.0262,-0.0077,0.0206,-0.0947,0.0044,-0.0239,0.0193,0.0459,0.0676,-0.031,0.011,-0.0113,-0.0088,-0.0029,-0.0143,0.0609,-0.0284,-0.0125,-0.0011,0.0409,-0.0304,-0.0585,0.0364,0.0105,0.239,0.0436,0.0647,-0.0059,-0.0016,-0.003,-0.0051,-0.0782,0.0634,0.037,0.0408,0.0334,-0.0617,-0.0336,-0.029,-0.0101,-0.0253,-0.0959,-0.0365,-0.0474,-0.0152,0.045,-0.0446,-0.0122,0.0261,-0.0338,0.0091,-0.0073,0.0241,-0.0435,-0.1099,0.0088,-0.0421,0.0364,0.0445,-0.048,-0.003,-0.0622,0.0245,0.0108,-0.0162,-0.0351,-0.0143,-0.0465,-0.0025,0.074,0.0295,0.0168,0.0692,0.014,0.0498,0.0212,-0.0648,-0.0084,0.0475,-0.0479,0.0455,0.0168,0.0018,0.0471,0.0631,-0.0312,-0.0096,-0.0589,0.0225,0.0092,-0.0878,-0.0113,0.0188,0.023,-0.2745,0.0869,-0.0122,0.0336,0.0014,0.0073,0.0092,0.0699,-0.0067,-0.0034,-0.0439,0.0346,0.057,-0.0187,0.0197,0.001,0.0661,-0.0398,0.0756,-0.0301,-0.0137,0.0375,0.2276,-0.0606,-0.0018,0.0233,-0.0288,0.0254,0.0327,0.0323,0.0103,0.0258,0.0695,-0.0742,0.0567,0.1236,-0.027,0.0349,0.0052,-0.057,-0.0079,0.0198,-0.0086,-0.029,0.0929,-0.0159,-0.0039,-0.0353,0.0385,-0.0323,-0.0418,0.0247,-0.0383,-0.0058,0.0535,-0.0107,0.01,-0.0644,-0.0033,0.0044,0.0282,-0.0388,0.0012,0.0487,-0.0419]}
{"key":"[ChemBO: Bayesian Optimization of Small Organic Molecules with Synthesizable Recommendations] In applications such as molecule design or drug discovery, it is desirable to have an algorithm which recommends new candidate molecules based on the results of past tests. These molecules first need to be synthesized and then tested for objective properties. We describe ChemBO, a Bayesian optimization framework for generating and optimizing organic molecules for desired molecular properties. While most existing data-driven methods for this problem do not account for sample efficiency or fail to enforce realistic constraints on synthesizability, our approach explores the synthesis graph in a sample-efficient way and produces synthesizable candidates. We implement ChemBO as a Gaussian process model and explore existing molecular kernels for it. Moreover, we propose a novel optimal-transport based distance and kernel that accounts for graphical information explicitly. In our experiments, we demonstrate the efficacy of the proposed approach on several molecular optimization problems.","layer":3,"vector":[-0.0589,-0.0104,0.0143,-0.0303,0.0212,0.0397,0.0064,0.0448,-0.0051,-0.003,0.0419,-0.0465,-0.0189,0.0517,0.017,0.0438,-0.0038,0.0394,-0.0586,0.0036,0.042,-0.0414,-0.0256,-0.0693,0.0384,0.061,0.0125,-0.0061,-0.0644,-0.2436,0.0385,-0.0361,0.044,-0.0576,-0.0011,0.014,0.0137,0.0514,-0.036,0.0545,0.0382,0.0039,-0.0303,-0.0226,-0.0205,-0.0439,-0.0369,0.0053,-0.01,-0.0506,0.0158,-0.0396,0.0215,0.0075,0.0309,0.0642,0.053,0.0172,0.0294,0.0292,0.0034,0.0722,-0.1272,0.0779,0.0658,0.0084,-0.0283,-0.0113,0.01,0.1128,-0.0314,0.0334,-0.0002,0.0415,0.0435,0.0051,-0.0075,-0.0246,-0.0107,0.0172,-0.0139,-0.0118,-0.0351,-0.0054,-0.0388,-0.0397,0.0223,-0.0206,0.0547,0.0407,-0.0104,-0.0551,-0.0353,0.0293,-0.1241,-0.0204,0.0223,-0.0017,-0.0214,0.1848,-0.0428,0.0325,0.0145,-0.0315,0.0277,-0.0596,-0.0131,-0.0702,0.0248,-0.0357,0.0309,-0.019,0.0091,-0.0624,-0.0322,0.0426,0.0151,0.0348,-0.0325,-0.009,-0.0396,0.0125,0.0826,0.0236,0.0208,-0.0644,-0.0291,0.1274,0.0263,0.0688,0.0808,-0.0237,-0.0439,-0.0011,0.0155,-0.014,0.0023,-0.0125,0.03,-0.0163,-0.0409,-0.0034,-0.0004,-0.1013,-0.055,0.1278,-0.0263,0.0638,-0.0914,-0.0072,0.0213,0.0323,-0.0107,-0.0154,0.0081,0.001,0.0144,0.0283,-0.0574,0.028,-0.0376,0.0052,-0.019,0.1212,-0.0514,-0.056,-0.0672,0.0341,-0.0086,0.0244,0.0402,0.0413,-0.0492,0.0488,0.0612,0.0099,-0.0505,0.0431,0.0006,-0.0128,0.0536,0.0093,-0.0182,0.0044,0.0235,-0.0239,0.0065,-0.0311,-0.0207,0.0689,-0.0062,0.006,0.0258,0.0172,-0.0495,-0.0397,-0.0231,-0.0117,0.0147,-0.0025,0.0635,-0.0118,-0.1007,0.0479,-0.0095,0.0083,-0.0121,-0.007,0.0654,-0.0021,-0.0431,0.0181,0.018,-0.0356,-0.0409,-0.007,0.0074,0.0615,0.0669,0.0142,0.0296,-0.0569,-0.0506,-0.2353,0.0144,0.0028,-0.0034,0.1081,-0.0164,0.0081,-0.0214,0.043,0.0816,0.0583,-0.0044,-0.0651,0.0012,-0.0611,0.0173,0.018,-0.0004,0.0036,-0.0063,0.014,0.0046,-0.0092,-0.0486,0.0161,0.0053,0.2186,0.0232,0.0252,-0.024,0.0305,0.0172,-0.0105,-0.0824,0.0311,0.0538,0.0414,-0.0265,-0.0581,-0.0548,-0.0044,-0.0208,-0.0013,-0.0957,-0.0547,-0.0533,-0.0118,0.0223,-0.0616,0.0475,0.0308,0.0239,0.0737,-0.0523,-0.0195,-0.0384,-0.0683,0.0266,-0.0123,0.0393,0.0349,-0.0561,0.0154,-0.0054,0.0398,-0.0667,-0.0128,-0.024,0.0524,-0.0187,0.0295,0.0729,-0.0132,0.0414,0.0917,0.014,0.0169,-0.0327,-0.0578,-0.0352,0.0498,-0.0485,0.051,-0.0149,0.0189,0.0048,0.0723,0.0001,0.014,-0.0329,0.0209,-0.0243,-0.0383,0.0044,0.0549,0.0006,-0.2992,0.0218,0.0195,0.0512,-0.0554,-0.0077,0.099,0.0239,-0.0686,-0.0092,0.0081,0.0299,0.0055,0.0136,-0.0144,0.0023,0.0502,-0.0589,0.0273,-0.0894,0.0179,0.0107,0.2332,-0.0181,0.0092,0.0622,0.0157,0.0272,0.0017,-0.0513,0.0236,0.0162,0.0684,-0.0699,0.0396,0.0797,-0.0658,0.0279,-0.009,-0.019,-0.0122,0.0022,-0.0355,-0.0262,0.0983,-0.0527,-0.0784,-0.051,-0.0093,0.0199,-0.0425,0.0229,-0.0205,-0.0354,0.0537,0.0346,-0.033,-0.052,-0.0081,-0.037,0.001,-0.0534,0.012,0.0266,-0.0265]}
{"key":"[Approximation Properties of Variational Bayes for Vector Autoregressions] Variational Bayes (VB) is a recent approximate method for Bayesian inference. It has the merit of being a fast and scalable alternative to Markov Chain Monte Carlo (MCMC) but its approximation error is often unknown. In this paper, we derive the approximation error of VB in terms of mean, mode, variance, predictive density and KL divergence for the linear Gaussian multi-equation regression. Our results indicate that VB approximates the posterior mean perfectly. Factors affecting the magnitude of underestimation in posterior variance and mode are revealed. Importantly, We demonstrate that VB estimates predictive densities accurately.","layer":6,"vector":[-0.064,-0.0299,0.0299,-0.0161,0.0571,0.0198,0.0162,0.0344,0.0312,-0.0049,0.0248,-0.0542,0.0291,0.0523,0.0588,0.0017,-0.0115,0.0193,-0.0318,-0.0098,0.014,-0.0141,-0.0009,-0.0511,0.077,0.0135,-0.0119,-0.0421,-0.0259,-0.2502,0.0138,-0.0514,0.0547,-0.0002,0.0028,-0.031,-0.0536,0.034,-0.0068,0.0377,0.0217,0.0059,-0.0081,-0.0283,-0.0165,-0.0707,-0.0044,0.0134,-0.0232,-0.0325,0.008,0.0066,0.0575,0.0554,0.0342,0.0165,0.0567,0.0401,0.0643,0.0849,0.0509,0.0363,-0.2099,0.0607,0.063,0.021,-0.0396,-0.0311,0.0068,0.0273,-0.0223,0.0471,-0.0234,0.0538,0.0332,-0.0111,0.0284,-0.0173,-0.064,0.0068,0.0181,-0.0333,0.003,-0.0353,-0.0456,-0.0498,0.0323,-0.0759,0.0649,-0.0458,0.0224,-0.0189,-0.0119,0.0101,-0.0573,-0.0109,0.0138,0.0271,-0.0321,0.181,0.0162,0.0753,0.0327,-0.0071,0.0346,-0.0285,-0.005,-0.01,0.0074,0.0191,0.018,-0.0246,0.0496,-0.0867,-0.0021,-0.0276,0.0683,0.0025,0.0171,-0.0472,0.0009,-0.0133,0.0465,-0.0077,0.0071,-0.0683,0.0054,0.1558,0.0125,0.0051,0.0512,-0.0165,-0.0714,-0.0303,0.0482,-0.0331,0.0194,-0.0134,0.0028,0.0051,-0.0791,-0.0758,0.0108,-0.0837,-0.0588,0.1473,-0.0108,0.0019,-0.0947,-0.0271,0.0122,0.0413,-0.0025,0.0117,0.0393,0.0385,-0.0063,0.0516,-0.0469,0.0242,-0.0277,-0.0254,-0.0143,0.0881,0.0251,-0.0466,-0.03,0.0484,0.0103,0.0099,0.0648,0.0441,-0.0358,0.0046,0.0703,0.0293,-0.0635,0.0518,0.0293,0.0106,-0.0134,-0.0152,-0.0443,0.0761,0.0202,-0.0283,-0.0039,-0.0139,0.0512,0.024,0.0409,-0.0208,-0.0077,-0.0196,0.0147,0.0198,-0.0324,-0.0501,0.0563,-0.0603,0.0264,-0.0319,-0.0716,0.0472,-0.021,0.0275,-0.0261,0.0067,0.0663,0.0101,-0.0007,-0.0234,0.0599,0.0167,-0.0501,0.0603,0.0033,0.0469,-0.0076,0.0152,0.0194,-0.045,-0.045,-0.2146,-0.0226,-0.0162,-0.0155,0.0448,-0.0632,0.0395,0.0054,0.0569,0.0955,-0.0017,0.0093,-0.0211,0.0209,-0.0055,0.0429,-0.0161,0.0227,-0.0389,0.0392,-0.0247,-0.0265,-0.0758,-0.0712,0.0648,0.0395,0.1846,0.0066,0.0002,-0.0188,0.0068,0.0801,0.0021,-0.0523,0.0653,0.0569,0.0629,-0.0448,-0.0322,-0.0452,-0.0311,0.017,-0.0043,-0.0981,-0.1074,-0.073,-0.0234,0.055,-0.0995,0.0039,0.0445,-0.0213,0.0697,-0.0509,0.0205,-0.0393,-0.0772,0.0239,-0.0091,0.0169,0.0247,-0.018,0.0155,-0.0216,0.0509,-0.0414,0.0087,-0.0699,-0.0158,0.0009,-0.0403,0.0711,-0.0256,-0.0143,0.0662,0.0007,-0.0034,-0.046,-0.0681,-0.0558,0.0401,-0.023,-0.0031,0.032,0.0122,-0.0038,0.0593,-0.0075,0.0365,-0.032,-0.0442,-0.0272,-0.0288,0.0029,-0.0021,-0.0314,-0.2808,0.0386,0.0128,-0.0121,-0.0377,-0.001,0.0479,-0.002,-0.0381,0.0048,-0.0202,0.0631,0.0569,-0.0156,0.0251,-0.0266,0.0686,-0.0669,0.0358,-0.0625,0.0137,0.0346,0.2133,-0.042,0.0196,0.0168,-0.0177,0.0171,0.0396,-0.0387,0.0189,-0.014,0.0979,-0.0441,0.0613,0.0837,-0.0283,0.0838,0.0113,-0.0566,0.0058,-0.0214,-0.0372,-0.0339,0.118,-0.0177,0.025,-0.0455,-0.002,0.0309,-0.0091,0.0435,-0.0241,-0.0137,-0.0189,0.0038,-0.0432,-0.03,-0.0564,-0.0526,0.023,-0.0532,-0.0164,0.0018,-0.0047]}
{"key":"[An Augmented Transformer Architecture for Natural Language Generation Tasks] The Transformer based neural networks have been showing significant advantages on most evaluations of various natural language processing and other sequence-to-sequence tasks due to its inherent architecture based superiorities. Although the main architecture of the Transformer has been continuously being explored, little attention was paid to the positional encoding module. In this paper, we enhance the sinusoidal positional encoding algorithm by maximizing the variances between encoded consecutive positions to obtain additional promotion. Furthermore, we propose an augmented Transformer architecture encoded with additional linguistic knowledge, such as the Part-of-Speech (POS) tagging, to boost the performance on some natural language generation tasks, e.g., the automatic translation and summarization tasks. Experiments show that the proposed architecture attains constantly superior results compared to the vanilla Transformer.","layer":7,"vector":[-0.0482,-0.0271,0.0019,-0.0284,0.0075,0.0348,-0.0027,0.0372,0.0217,0.0168,-0.0213,-0.0512,0.0492,0.0382,0.0309,0.0202,-0.0163,0.0519,-0.0134,-0.023,0.0777,-0.0183,0.0171,-0.03,-0.0161,0.0007,-0.0448,-0.0105,-0.0409,-0.2246,-0.027,-0.0542,0.062,-0.0158,-0.0665,0.0054,-0.0646,0.0524,0.0213,0.0221,0.0415,0.0115,0.0,-0.0587,0.0124,-0.0405,-0.0069,-0.0294,-0.0364,-0.0496,0.0485,-0.0315,-0.0035,0.0647,0.0207,0.0392,0.034,0.0213,0.0517,0.0318,0.0146,0.0588,-0.2017,0.0839,0.0025,0.0271,-0.0467,0.0079,0.0061,0.0003,-0.0229,-0.0043,0.0532,0.0517,0.0369,0.0552,-0.0052,-0.0501,-0.0116,0.044,0.0141,-0.0504,-0.0804,-0.0173,-0.0348,-0.0398,0.0262,-0.042,-0.0197,0.0182,-0.0399,-0.0032,-0.0064,0.0442,-0.0503,-0.0682,0.0407,0.0428,-0.0596,0.1854,-0.0597,0.0284,0.0451,-0.0836,0.0323,-0.0138,-0.0312,-0.0406,-0.0616,0.0157,-0.0236,0.0224,-0.0256,-0.0231,0.0445,-0.0138,0.0903,0.033,-0.0382,-0.0161,-0.0087,0.0473,-0.0152,-0.0139,0.037,-0.0478,0.1021,0.1129,0.0699,0.081,0.0571,0.0114,-0.0509,-0.0021,0.0409,0.0093,0.0136,-0.0258,0.0196,0.002,-0.0392,-0.0556,-0.0266,-0.0453,-0.0104,0.12,-0.0325,0.013,-0.0605,-0.0265,-0.0049,0.0317,0.0013,-0.0439,0.0112,0.0656,0.0257,0.0321,-0.0457,-0.0348,-0.0031,-0.0725,-0.0388,0.0993,0.0257,-0.1282,-0.0349,-0.0141,0.0159,-0.0113,0.0266,0.0038,-0.0154,0.0276,0.0209,0.0211,-0.0365,-0.0006,-0.0178,0.047,0.0506,-0.0237,0.0006,0.0259,-0.0097,-0.0766,-0.0029,-0.0789,0.0019,0.0269,-0.0437,0.0511,-0.0067,-0.0146,0.0023,-0.0326,-0.0033,0.0195,-0.0108,-0.0269,0.0076,-0.0077,-0.0693,0.0237,0.0126,-0.0143,-0.0233,-0.0165,0.0514,0.0499,0.0005,0.0004,0.1198,0.014,-0.0673,0.0196,-0.0133,0.0147,0.0605,0.0219,-0.0323,-0.0275,-0.026,-0.2544,0.0197,0.0479,-0.002,0.0572,-0.0713,-0.0014,0.0081,0.0534,0.0531,0.0468,-0.0494,-0.0237,0.0212,-0.0287,0.046,0.0263,0.0273,0.0319,0.0139,0.0225,0.0148,0.0036,-0.1165,0.0432,-0.0073,0.2063,0.0362,0.0745,-0.0085,0.0526,0.0394,0.0081,-0.085,0.0678,0.0096,0.0409,-0.0158,0.0103,-0.0231,-0.0404,-0.0119,-0.0056,-0.0611,0.0027,-0.0432,-0.0373,0.008,-0.0295,0.0715,0.039,-0.0503,0.0467,-0.0005,-0.007,-0.0758,-0.0991,-0.0222,-0.0397,-0.0097,0.0127,-0.0503,0.0429,-0.0059,-0.0081,0.0603,-0.005,0.0127,0.0081,0.0037,-0.0206,0.0918,0.0153,0.0004,0.0337,0.0416,0.0074,-0.0224,-0.0104,-0.045,0.0345,-0.0544,0.0602,-0.0056,0.0167,-0.0104,0.0518,0.0056,0.0081,0.0053,0.017,0.0457,-0.0509,-0.0268,0.0231,-0.0484,-0.307,0.0377,0.0042,0.006,-0.0085,0.0109,0.0295,0.024,-0.0508,0.0422,-0.0563,0.0107,0.0218,-0.0346,-0.0282,0.0365,0.0878,-0.0475,0.0294,-0.0498,0.0245,0.0606,0.2309,-0.0264,0.0085,-0.0514,-0.0532,-0.0272,0.0241,-0.0181,-0.0064,0.0065,0.0849,-0.0203,0.0115,0.0809,-0.0407,0.0145,0.0299,0.0148,0.0135,0.0283,-0.046,-0.0493,0.0966,0.0075,-0.0152,-0.0578,0.0102,-0.0254,-0.0329,0.0567,-0.0067,-0.003,0.0475,0.0117,-0.0355,-0.042,-0.0421,-0.0234,0.0252,-0.0866,0.0194,0.0137,0.0202]}
{"key":"[Relational Learning between Multiple Pulmonary Nodules via Deep Set Attention Transformers] Diagnosis and treatment of multiple pulmonary nodules are clinically important but challenging. Prior studies on nodule characterization use solitary-nodule approaches on multiple nodular patients, which ignores the relations between nodules. In this study, we propose a multiple instance learning (MIL) approach and empirically prove the benefit to learn the relations between multiple nodules. By treating the multiple nodules from a same patient as a whole, critical relational information between solitary-nodule voxels is extracted. To our knowledge, it is the first study to learn the relations between multiple pulmonary nodules. Inspired by recent advances in natural language processing (NLP) domain, we introduce a self-attention transformer equipped with 3D CNN, named {NoduleSAT}, to replace typical pooling-based aggregation in multiple instance learning. Extensive experiments on lung nodule false positive reduction on LUNA16 database, and malignancy classification on LIDC-IDRI database, validate the effectiveness of the proposed method.","layer":7,"vector":[-0.029,-0.0302,0.0313,-0.0204,0.0262,0.0144,0.0457,-0.0157,0.0037,-0.0321,0.0051,-0.0853,0.0473,0.0446,0.0307,0.0441,0.0283,0.0444,-0.0562,-0.0224,0.0178,-0.0065,0.006,-0.0392,0.0278,-0.0134,-0.0186,-0.0427,-0.0409,-0.2426,0.0128,-0.0096,0.0009,-0.0038,0.0187,-0.0221,-0.0306,0.0395,-0.031,0.0133,-0.0175,0.0005,0.0109,-0.036,-0.0158,-0.0928,-0.0352,-0.0577,0.0351,-0.0306,0.0475,-0.0364,0.0061,0.053,0.0362,0.0159,0.0638,0.0558,0.0246,0.0382,0.0285,0.0568,-0.1317,0.0544,0.0709,0.0502,-0.0672,-0.0532,0.0299,0.0761,0.0126,0.0213,0.0591,0.0233,-0.0059,0.0278,0.0112,0.0187,-0.0202,0.0165,0.0058,0.046,-0.0619,-0.0602,-0.0061,-0.0154,-0.0332,-0.0763,-0.0168,0.0134,-0.0579,-0.047,-0.0321,0.0468,-0.0598,-0.0075,0.0128,0.0367,-0.0319,0.1712,-0.0428,-0.0061,0.052,-0.0498,0.0004,-0.0313,-0.0344,-0.0583,-0.0184,0.034,-0.0006,-0.0478,0.0031,0.0002,0.0201,0.0283,0.1118,0.0485,0.0122,-0.018,-0.0009,-0.0062,0.041,0.0008,-0.0134,-0.0741,0.0676,0.1503,0.0311,0.0208,0.0473,0.0072,-0.0297,-0.0666,0.0081,0.0072,0.0203,-0.0152,0.005,0.0176,-0.0392,-0.085,0.0299,-0.0439,-0.0699,0.0941,-0.0835,0.0145,-0.0436,-0.0349,-0.0228,0.0195,-0.0056,-0.0387,0.0283,0.025,0.0274,0.0102,-0.0129,-0.0096,-0.0055,-0.0875,-0.0632,0.1161,0.046,-0.1071,-0.0463,-0.0251,-0.0112,-0.0466,0.0756,0.05,-0.0322,0.0021,0.0644,0.0518,-0.0484,-0.0112,0.0038,0.0209,0.0391,-0.0142,-0.0206,0.0386,0.0041,-0.0539,0.0344,-0.0921,0.0628,0.0282,-0.0575,0.0331,-0.0352,-0.0027,-0.0283,-0.0466,-0.0079,-0.012,-0.0218,-0.0088,0.0053,-0.0303,0.0035,0.044,-0.0047,0.0366,-0.0238,0.0412,0.0121,-0.0155,-0.046,-0.0102,0.0506,-0.0383,-0.0074,-0.0322,-0.0196,0.0433,-0.0292,0.0334,0.0546,-0.0401,-0.0366,-0.2146,-0.0495,0.0408,-0.0221,0.0366,-0.0378,0.0487,0.0353,0.0291,0.1203,0.0631,-0.0292,-0.0387,-0.0043,0.0016,0.0551,0.0411,0.0644,-0.0294,-0.0403,0.0197,0.0319,-0.0015,-0.0932,0.0278,-0.0009,0.2335,0.0178,-0.0028,-0.0046,0.0512,0.02,-0.0488,-0.1004,0.0556,-0.0032,0.0485,-0.0027,-0.0697,0.0109,-0.0474,-0.0201,-0.015,-0.0962,-0.0343,-0.0189,-0.0208,0.1042,-0.0279,0.0537,0.0193,-0.0418,-0.0078,-0.0106,0.0179,-0.0303,-0.0835,0.0119,-0.0405,0.0101,0.0125,-0.0342,0.015,-0.0718,0.0652,0.0039,-0.0455,-0.0249,0.0234,-0.0255,-0.0453,0.0641,0.0109,0.0024,0.0532,0.0405,0.0537,-0.0071,-0.0533,-0.0052,0.0758,-0.0551,0.0623,0.0111,0.0269,0.0719,0.0852,0.0309,0.0015,-0.0464,-0.0148,0.0466,-0.0184,-0.0418,0.0158,-0.0383,-0.315,0.0453,-0.0188,0.0357,-0.0208,0.0512,0.0472,0.023,0.013,-0.0305,-0.0089,0.0053,0.0349,-0.0006,-0.0183,0.0502,0.0548,-0.0664,0.0324,-0.0659,0.0153,0.0306,0.2195,-0.0291,0.025,0.0344,-0.0639,-0.0077,0.0096,0.0434,0.0008,0.0054,0.0646,-0.0363,0.0278,0.099,-0.0377,0.041,0.0067,-0.0084,-0.0169,0.0203,-0.0344,-0.0435,0.0672,-0.0078,-0.005,-0.0193,-0.0205,0.0147,0.0237,0.0434,0.0105,0.0176,0.0249,0.0277,-0.034,-0.046,-0.0302,-0.0302,0.0188,-0.0371,-0.0397,0.0307,0.0219]}
{"key":"[FEAST: An Automated Feature Selection Framework for Compilation Tasks] The success of the application of machine-learning techniques to compilation tasks can be largely attributed to the recent development and advancement of program characterization, a process that numerically or structurally quantifies a target program. While great achievements have been made in identifying key features to characterize programs, choosing a correct set of features for a specific compiler task remains an ad hoc procedure. In order to guarantee a comprehensive coverage of features, compiler engineers usually need to select excessive number of features. This, unfortunately, would potentially lead to a selection of multiple similar features, which in turn could create a new problem of bias that emphasizes certain aspects of a program's characteristics, hence reducing the accuracy and performance of the target compiler task. In this paper, we propose FEAture Selection for compilation Tasks (FEAST), an efficient and automated framework for determining the most relevant and representative features from a feature pool. Specifically, FEAST utilizes widely used statistics and machine-learning tools, including LASSO, sequential forward and backward selection, for automatic feature selection, and can in general be applied to any numerical feature set. This paper further proposes an automated approach to compiler parameter assignment for assessing the performance of FEAST. Intensive experimental results demonstrate that, under the compiler parameter assignment task, FEAST can achieve comparable results with about 18% of features that are automatically selected from the entire feature pool. We also inspect these selected features and discuss their roles in program execution.","layer":2,"vector":[-0.0534,-0.0131,0.02,0.0189,0.051,0.0043,0.0271,0.0721,0.0104,-0.0496,-0.0318,0.0,0.0098,0.0202,-0.0065,0.0181,0.0111,0.0428,-0.0387,0.0078,0.028,-0.0319,-0.0358,-0.0736,0.0566,0.0308,-0.046,-0.0107,-0.044,-0.2596,-0.0071,-0.0465,0.0784,-0.0214,0.0205,0.0014,-0.0338,0.0394,-0.069,0.0397,-0.0074,0.0332,-0.0255,-0.0567,-0.0575,-0.0376,-0.0102,-0.0183,-0.0323,0.0005,-0.0243,-0.0427,0.0259,0.0397,0.012,0.0248,0.0456,0.0454,0.0336,0.0343,0.0088,0.0165,-0.1688,0.0325,0.0555,0.0205,-0.0364,-0.0343,0.0142,0.095,-0.0448,0.0649,0.0028,0.0608,-0.0154,0.009,0.0233,-0.0069,-0.0012,0.0242,0.0371,-0.0328,-0.0484,0.0175,0.0,0.0146,0.0333,0.0254,0.1036,-0.0325,-0.0355,-0.02,0.0033,0.0004,-0.0477,0.0094,0.0832,-0.0144,-0.0466,0.2317,-0.0283,-0.0108,0.0022,-0.0191,0.0493,-0.0484,-0.001,-0.0505,-0.0172,-0.0483,-0.0021,-0.0152,-0.006,-0.0487,0.0282,0.0192,0.0377,-0.0086,-0.0322,0.021,0.0116,-0.0289,0.0418,-0.0177,0.0113,-0.0343,0.0045,0.1152,0.0744,-0.0082,0.0565,-0.0195,-0.024,-0.0443,0.0422,0.0158,0.019,0.0254,0.0168,-0.0266,-0.063,-0.0356,0.0192,-0.062,-0.0679,0.1653,-0.0669,0.0249,-0.0052,-0.0218,-0.0036,0.0151,-0.0643,-0.034,0.0102,0.0424,-0.0266,0.0302,-0.0174,0.0331,-0.0232,-0.0201,-0.0236,0.0527,-0.0101,-0.0473,-0.0423,0.0148,-0.0055,0.0278,0.0218,0.0245,-0.0603,0.022,0.0809,0.0293,-0.0603,0.0264,0.029,0.0341,0.0744,-0.0051,-0.0495,0.0273,0.0815,-0.066,-0.0068,-0.0535,0.01,0.0526,-0.0476,0.0188,0.02,-0.0277,-0.0147,-0.0202,-0.0005,0.0394,0.05,-0.0466,0.0045,0.0586,-0.0318,0.007,-0.0145,-0.0074,-0.04,-0.0091,0.0403,0.0548,-0.0338,-0.0076,0.0332,0.0054,-0.0601,0.0201,-0.0029,0.0482,0.0468,0.035,0.0281,-0.0817,-0.0336,-0.2453,-0.021,0.0363,-0.0296,0.0241,-0.104,0.0176,-0.013,0.0628,0.044,0.0243,-0.0561,-0.0548,0.0386,-0.0315,0.0503,0.0133,0.0028,-0.0553,0.0204,-0.0314,-0.0038,-0.0167,-0.0702,0.0079,0.0291,0.1828,0.0416,0.0644,-0.0325,0.0469,-0.0092,-0.0137,-0.1246,0.0375,0.0486,0.0759,-0.0268,-0.0155,0.0476,-0.0278,0.0576,-0.0225,-0.1056,-0.0194,-0.0463,-0.0314,-0.0212,-0.014,0.0508,0.0302,-0.0349,0.0384,0.0232,0.0141,-0.0255,-0.1066,-0.0001,0.0129,0.0162,0.0295,-0.0791,-0.0216,-0.0496,0.0474,0.0279,-0.0372,-0.0275,0.0254,-0.0105,0.0053,0.0364,-0.0249,-0.0482,0.0389,0.017,0.0275,-0.0457,-0.0283,-0.0447,0.0519,-0.021,0.008,0.0048,0.0232,-0.0103,0.0561,0.0403,0.0261,-0.0334,-0.038,-0.0103,-0.0061,-0.0333,0.0324,0.0482,-0.2982,0.0207,0.0189,0.0038,0.0088,0.0025,0.0642,-0.0361,-0.0446,-0.0007,0.0208,-0.0183,0.0204,-0.0397,0.033,0.0437,0.0724,-0.0457,0.0395,-0.0589,0.0091,0.0271,0.2191,-0.0202,0.0335,0.0277,-0.0134,0.0426,0.0279,0.012,0.0394,0.001,0.0984,-0.0728,-0.0203,0.1196,-0.013,0.0129,-0.0223,0.0147,0.0114,-0.0008,-0.0589,-0.0409,0.1226,-0.0345,-0.0237,-0.068,0.0185,0.0234,-0.0452,0.0051,-0.0514,-0.0054,0.023,-0.0075,-0.0436,-0.0219,-0.0709,-0.0157,0.034,-0.0025,0.0441,0.0588,-0.0117]}
{"key":"[A New Analysis of Differential Privacy's Generalization Guarantees] We give a new proof of the \"transfer theorem\" underlying adaptive data analysis: that any mechanism for answering adaptively chosen statistical queries that is differentially private and sample-accurate is also accurate out-of-sample. Our new proof is elementary and gives structural insights that we expect will be useful elsewhere. We show: 1) that differential privacy ensures that the expectation of any query on the posterior distribution on datasets induced by the transcript of the interaction is close to its true value on the data distribution, and 2) sample accuracy on its own ensures that any query answer produced by the mechanism is close to its posterior expectation with high probability. This second claim follows from a thought experiment in which we imagine that the dataset is resampled from the posterior distribution after the mechanism has committed to its answers. The transfer theorem then follows by summing these two bounds, and in particular, avoids the \"monitor argument\" used to derive high probability bounds in prior work. An upshot of our new proof technique is that the concrete bounds we obtain are substantially better than the best previously known bounds, even though the improvements are in the constants, rather than the asymptotics (which are known to be tight). As we show, our new bounds outperform the naive \"sample-splitting\" baseline at dramatically smaller dataset sizes compared to the previous state of the art, bringing techniques from this literature closer to practicality.","layer":1,"vector":[-0.019,-0.0167,-0.0228,-0.0279,0.0006,0.0265,0.0757,0.006,0.0374,0.007,0.0396,-0.0151,0.0339,0.023,0.0094,0.0465,0.0071,0.0154,-0.073,0.0167,0.0176,-0.0234,-0.0139,-0.0628,0.0272,-0.0033,-0.0798,-0.0376,-0.0487,-0.265,-0.0036,-0.0689,0.0296,-0.0328,0.0451,-0.032,-0.0371,0.0369,-0.0179,0.0696,0.0228,0.041,-0.0638,-0.0276,-0.0142,-0.0265,-0.0204,-0.0017,-0.067,-0.0492,-0.0126,0.0024,0.0155,0.0565,0.0584,0.0674,0.0682,0.0014,0.0189,0.0694,0.0169,0.0718,-0.1337,0.0819,0.0296,0.0477,-0.0416,-0.031,-0.0085,0.015,-0.0141,0.0677,-0.0014,0.038,0.0361,0.0208,0.0058,-0.0078,-0.0336,0.0533,0.0097,-0.026,0.0098,0.0187,-0.0622,-0.0835,0.0341,-0.0465,0.0687,0.0344,-0.0349,-0.0168,-0.0095,-0.0127,-0.045,-0.0157,0.0391,0.0346,0.0225,0.1595,-0.0477,0.0385,-0.0012,0.0097,0.0288,-0.0428,-0.0167,-0.0383,-0.0311,0.0356,0.0084,-0.0255,0.0429,-0.0197,0.0104,0.0049,0.0601,0.0023,-0.0116,-0.0262,-0.0268,0.0367,0.0226,0.0013,0.0436,-0.073,0.0055,0.1551,0.0262,0.025,0.0429,-0.0781,-0.0703,-0.0193,0.02,0.0048,0.0522,0.0252,0.068,0.0154,-0.0671,-0.0721,0.0147,-0.0398,-0.0475,0.1414,0.0103,0.0527,-0.0179,-0.034,0.0243,0.0444,0.0017,-0.0652,0.0272,-0.0173,0.0444,0.051,-0.0523,0.0339,0.0178,-0.0137,0.0052,0.1191,0.0114,-0.0913,-0.0051,0.0419,0.0338,-0.0191,0.0536,0.0332,-0.0293,0.0455,0.045,-0.0166,-0.0752,-0.004,-0.0093,0.0134,-0.0045,-0.0237,-0.0051,0.0186,-0.0026,-0.0349,-0.0209,-0.0022,0.0173,0.088,-0.0086,0.0127,-0.0406,0.0269,-0.0509,-0.0333,-0.0185,0.0099,0.0073,-0.0376,-0.0113,0.0142,-0.0574,0.0547,-0.0401,0.0256,-0.0118,-0.0201,0.0367,0.0181,-0.0154,-0.0091,-0.0272,-0.047,-0.0229,0.0217,0.0163,0.0191,0.0073,0.0026,0.0012,-0.0343,-0.026,-0.2526,-0.0333,0.0002,0.0188,0.0569,-0.0912,0.0631,0.0043,0.0384,0.0989,0.0338,-0.0185,-0.0435,0.0465,-0.0289,0.0496,0.0267,0.0206,-0.0209,-0.0134,-0.0195,0.0352,-0.0383,-0.091,0.0321,-0.0103,0.2383,-0.0063,0.0318,-0.0172,0.0381,0.0116,0.0166,-0.1332,0.0426,0.0357,-0.0089,0.003,-0.0197,-0.0511,0.0072,0.01,-0.0059,-0.1095,-0.0429,-0.0272,-0.0372,-0.0165,-0.0674,0.0158,-0.0002,0.0219,0.0455,-0.0097,0.0205,-0.0678,-0.0705,-0.0048,-0.0305,0.0899,-0.0271,-0.0056,0.0333,-0.0697,0.0301,-0.0001,0.0085,-0.0607,0.0207,-0.034,-0.0165,0.0369,0.0161,0.0171,0.0114,0.0746,0.0019,-0.0727,-0.0323,-0.0575,0.0794,-0.0181,0.0078,-0.0071,0.0136,0.0568,0.0901,0.0152,0.0026,-0.0265,0.004,-0.0083,-0.1166,-0.0614,0.021,-0.0159,-0.2873,-0.0055,-0.0343,-0.0135,-0.0163,0.0243,0.0352,0.0103,-0.0786,-0.0095,0.0158,0.0802,0.0266,0.0075,0.0516,0.0234,0.067,-0.0368,0.0171,-0.0453,0.0504,0.0169,0.2128,-0.0032,0.0344,0.0079,0.0089,-0.0165,-0.0027,-0.0245,-0.0152,0.0198,0.041,-0.0456,0.0308,0.0553,-0.0381,0.0372,0.0521,-0.0213,-0.0336,-0.0365,-0.0228,0.0179,0.1353,0.0015,-0.0312,-0.0626,0.0179,0.0047,-0.0171,-0.0336,0.0044,-0.0021,0.0404,0.0249,-0.0792,-0.0053,-0.0156,-0.035,0.0333,-0.0494,-0.0091,-0.0044,0.0162]}
{"key":"[Self-Supervised Learning for Fine-Grained Image Classification] Fine-grained image classification involves identifying different subcategories of a class which possess very subtle discriminatory features. Fine-grained datasets usually provide bounding box annotations along with class labels to aid the process of classification. However, building large scale datasets with such annotations is a mammoth task. Moreover, this extensive annotation is time-consuming and often requires expertise, which is a huge bottleneck in building large datasets. On the other hand, self-supervised learning (SSL) exploits the freely available data to generate supervisory signals which act as labels. The features learnt by performing some pretext tasks on huge unlabelled data proves to be very helpful for multiple downstream tasks. Our idea is to leverage self-supervision such that the model learns useful representations of fine-grained image classes. We experimented with 3 kinds of models: Jigsaw solving as pretext task, adversarial learning (SRGAN) and contrastive learning based (SimCLR) model. The learned features are used for downstream tasks such as fine-grained image classification. Our code is available at http://github.com/rush2406/Self-Supervised-Learning-for-Fine-grained-Image-Classification","layer":3,"vector":[-0.0204,-0.0391,0.0027,-0.0135,0.0505,0.0044,0.0324,-0.0207,-0.0234,0.0016,0.0105,-0.0618,0.0508,0.0586,0.008,0.036,0.033,0.04,-0.0429,-0.0306,0.0144,-0.0054,-0.0271,-0.0696,0.0174,-0.0164,-0.0127,-0.0561,-0.0321,-0.2322,0.024,-0.0564,-0.0021,0.0351,0.0009,-0.0916,-0.0216,0.0412,-0.0398,-0.002,0.0493,0.0155,-0.0268,-0.0264,-0.024,-0.0635,-0.028,-0.0528,-0.025,-0.0398,0.0268,-0.0616,-0.0053,0.0532,-0.0419,0.0233,0.0312,0.0447,0.0642,0.0079,0.0092,0.089,-0.1551,0.0617,0.0383,0.0398,-0.0248,-0.0083,0.0328,0.055,0.0201,0.056,0.0451,0.0259,-0.0151,0.0016,0.0231,0.0047,-0.0198,0.0134,0.0016,0.0155,-0.0211,-0.0252,-0.0102,-0.0218,0.0016,-0.0745,0.0895,-0.0015,-0.0107,-0.0031,-0.0189,0.0363,-0.0584,-0.0033,0.0355,0.0291,-0.0666,0.2008,-0.065,0.0263,0.0373,-0.0277,0.0262,-0.0258,-0.0382,-0.0263,-0.0576,-0.0318,0.0126,-0.0062,0.018,-0.0344,0.0424,-0.0577,0.0632,-0.0075,0.0237,0.0184,0.0214,0.0096,0.0322,-0.0001,0.0202,-0.0256,0.0236,0.1394,0.0047,0.0098,0.0279,-0.0132,-0.0356,0.0254,-0.0,0.038,0.0645,0.0173,0.0217,-0.0368,-0.0357,-0.05,0.0458,-0.0593,-0.0604,0.0741,-0.0444,0.0532,-0.04,-0.0358,-0.0436,0.009,-0.0467,0.0092,0.0504,0.0647,0.0213,0.0535,-0.0588,-0.0197,-0.0489,-0.0861,-0.034,0.0938,0.0335,-0.0855,-0.0139,-0.0108,0.0215,-0.014,0.0353,0.0401,-0.0322,0.0422,0.0596,0.0462,-0.0894,0.0007,-0.0143,0.0466,0.0146,-0.0168,-0.0395,0.0307,0.0382,-0.0282,0.0041,-0.0394,0.0367,0.0183,-0.0179,0.0682,-0.02,-0.0606,-0.026,-0.034,0.0205,-0.0392,-0.0123,-0.016,0.0117,0.0161,-0.0031,0.032,-0.0134,-0.0365,-0.027,-0.011,0.0532,0.0482,-0.0511,0.0634,0.0327,0.0215,-0.0419,-0.0168,0.0218,0.0827,-0.0286,0.0564,0.0385,-0.0605,-0.0423,-0.235,-0.0048,0.0235,-0.031,0.0127,-0.0887,0.0403,-0.0026,0.0507,0.0451,0.0673,0.0054,-0.0524,-0.0122,-0.0034,0.0145,0.0186,0.0371,-0.0365,0.0029,-0.0053,0.0681,0.0226,-0.0833,0.0298,-0.0059,0.2433,0.0304,0.0353,-0.0497,0.0118,0.0227,-0.0764,-0.0992,0.0604,0.0034,0.0689,0.001,-0.0408,-0.0357,-0.0217,0.0526,-0.0073,-0.1222,-0.0366,-0.0196,-0.0256,0.0038,-0.0515,0.0105,0.0617,-0.0296,0.0489,0.0217,-0.022,-0.0144,-0.078,0.0467,-0.0818,0.0264,0.0038,-0.0742,0.0167,-0.0541,0.0513,0.0261,-0.0595,-0.0279,0.0107,-0.0377,0.0117,0.1041,-0.0403,-0.022,0.0518,0.0242,0.0105,-0.0126,-0.0501,-0.0345,0.0366,0.0276,0.0394,0.0333,0.0221,0.0168,0.0675,0.0225,0.0283,0.003,-0.0126,0.0516,-0.0686,-0.0134,0.0419,0.0187,-0.3242,0.053,0.0244,0.071,-0.0217,0.0522,0.0516,0.0353,-0.0193,0.0113,-0.0271,0.0242,0.0392,-0.0126,0.0287,0.0112,0.0674,-0.0355,0.0868,-0.0218,0.0023,0.0139,0.1984,-0.0332,0.0214,-0.0257,-0.0615,-0.0021,0.0162,-0.0297,0.0477,-0.0187,0.1128,-0.0592,0.0092,0.0821,-0.0402,-0.0077,0.0017,0.0107,-0.0164,-0.0001,-0.04,-0.0165,0.0416,0.0064,0.006,0.0245,-0.0569,0.0024,-0.0327,0.0029,-0.0175,0.01,0.0437,0.0351,-0.0635,-0.018,-0.0488,-0.0126,0.0187,-0.0326,-0.0463,-0.0321,-0.0074]}
{"key":"[Meta-data Study in Autism Spectrum Disorder Classification Based on Structural MRI] Accurate diagnosis of autism spectrum disorder (ASD) based on neuroimaging data has significant implications, as extracting useful information from neuroimaging data for ASD detection is challenging. Even though machine learning techniques have been leveraged to improve the information extraction from neuroimaging data, the varying data quality caused by different meta-data conditions (i.e., data collection strategies) limits the effective information that can be extracted, thus leading to data-dependent predictive accuracies in ASD detection, which can be worse than random guess in some cases. In this work, we systematically investigate the impact of three kinds of meta-data on the predictive accuracy of classifying ASD based on structural MRI collected from 20 different sites, where meta-data conditions vary.","layer":0,"vector":[0.0121,-0.0037,0.0226,0.0007,0.0276,0.0578,0.0529,0.0255,0.0251,-0.0125,0.0276,-0.0365,0.0186,0.0595,0.019,0.0127,0.0223,-0.0173,-0.0356,0.0512,0.004,-0.038,0.0142,-0.0418,0.0256,0.072,-0.0207,-0.0382,-0.079,-0.2505,0.0201,-0.0707,0.0612,-0.0482,0.0165,-0.039,0.0058,0.0804,-0.0554,0.017,0.0187,0.0351,-0.0103,-0.0647,-0.0791,0.0002,-0.0413,-0.0419,0.0049,0.0225,-0.0344,0.0023,0.0489,0.0648,0.023,0.0806,0.0459,0.0055,0.0575,0.0619,0.0366,0.0519,-0.1228,0.0735,0.0948,0.0515,-0.0551,-0.046,0.013,0.0206,0.0167,0.0015,-0.0316,0.0292,0.0265,-0.0,-0.031,-0.0281,0.0135,-0.0148,0.0004,-0.0273,-0.0708,-0.0144,-0.0015,-0.023,0.0107,-0.066,0.025,0.0051,-0.0138,0.0023,-0.0452,0.0265,-0.096,-0.0371,0.0134,0.0114,-0.0312,0.1649,-0.0636,0.0344,0.0057,-0.036,-0.0095,-0.055,-0.0574,-0.0376,-0.0801,-0.0244,-0.0074,0.0124,0.0318,-0.0691,-0.0035,-0.0094,0.0727,0.0086,-0.0001,0.0134,-0.003,0.007,0.063,-0.0268,0.0536,-0.0408,-0.0075,0.1066,0.0394,-0.0195,0.0417,-0.0381,-0.0254,-0.0152,0.0219,0.0071,0.0508,-0.0157,0.0204,0.0303,-0.0029,-0.0717,-0.0185,-0.0469,-0.0827,0.1138,-0.1065,0.0162,-0.0592,0.0007,-0.0101,0.0597,-0.0401,-0.0319,0.0083,0.0286,-0.0038,0.0733,-0.0577,-0.0512,0.0109,-0.0711,-0.0302,0.1126,0.0065,-0.0607,-0.0596,-0.0225,-0.0103,-0.0576,0.0366,0.045,0.0188,0.0022,0.0609,0.0118,-0.049,0.0169,-0.0155,0.0218,0.0499,-0.0747,-0.0246,0.0326,0.0536,-0.0416,0.0035,-0.0478,0.0544,0.0059,-0.0159,0.0152,-0.0451,-0.0533,-0.044,-0.0052,-0.0007,-0.0279,-0.0339,-0.0311,0.005,0.0352,-0.0156,0.0144,-0.0052,0.0631,-0.038,0.0127,0.0418,0.0283,-0.0361,-0.0129,0.0227,-0.0038,-0.0403,-0.0099,0.0319,0.0577,0.0004,0.0854,0.0613,-0.036,-0.077,-0.2202,-0.0393,0.0668,0.0273,0.0001,-0.0662,0.0267,-0.0032,0.0753,0.1093,0.0451,0.0529,-0.0237,0.0232,-0.0288,0.0406,0.0477,-0.0015,-0.0354,0.0136,-0.0293,0.0497,0.0109,-0.071,0.042,0.0164,0.1731,0.0098,0.0328,-0.0302,-0.0029,0.0143,-0.0456,-0.0958,0.0765,0.0421,0.0223,-0.0576,-0.0702,-0.0591,-0.0443,0.0166,0.0395,-0.0862,-0.088,0.0009,-0.0355,-0.0204,-0.0401,0.0337,0.061,-0.0458,0.0429,-0.0305,-0.0062,0.0007,-0.0657,0.008,-0.0195,0.0334,0.0742,-0.0408,0.0272,-0.0757,0.0433,0.0084,-0.0939,-0.0485,0.0021,-0.0329,-0.0087,0.1213,0.0089,-0.0049,0.06,0.0205,0.0034,-0.032,0.0087,-0.0121,0.0686,-0.0097,0.0573,0.0107,0.0548,0.005,0.0855,-0.0186,-0.0233,-0.0566,0.0409,0.0375,-0.0252,-0.0285,0.0595,0.0205,-0.2783,0.0346,0.0283,0.0119,0.0151,0.0133,0.0557,0.0047,0.0059,0.0179,-0.0015,0.0197,0.0467,-0.0643,-0.0297,0.0323,0.0667,-0.0319,0.0488,-0.0831,0.0031,0.0514,0.2254,-0.0133,0.0041,0.0334,-0.0294,-0.0088,-0.0121,-0.0157,-0.0177,-0.0038,0.0387,-0.0242,0.0826,0.0663,-0.0871,0.031,0.0149,-0.048,0.0448,0.0286,-0.0295,0.0035,0.1023,-0.0105,-0.031,-0.0157,0.0027,0.0306,-0.057,-0.0342,-0.0306,-0.0383,0.0292,0.0471,-0.011,-0.0067,-0.0444,-0.0422,0.0027,0.0052,-0.0195,0.0537,0.0326]}
{"key":"[Deep Learning for Predicting Asset Returns] Deep learning searches for nonlinear factors for predicting asset returns. Predictability is achieved via multiple layers of composite factors as opposed to additive ones. Viewed in this way, asset pricing studies can be revisited using multi-layer deep learners, such as rectified linear units (ReLU) or long-short-term-memory (LSTM) for time-series effects. State-of-the-art algorithms including stochastic gradient descent (SGD), TensorFlow and dropout design provide imple- mentation and efficient factor exploration. To illustrate our methodology, we revisit the equity market risk premium dataset of Welch and Goyal (2008). We find the existence of nonlinear factors which explain predictability of returns, in particular at the extremes of the characteristic space. Finally, we conclude with directions for future research.","layer":1,"vector":[-0.0162,-0.044,-0.0067,0.0205,0.0306,0.021,0.013,0.0184,0.0459,-0.0382,0.0165,-0.0425,0.045,0.0472,0.0283,0.0117,-0.0099,0.0099,-0.0874,0.0201,0.0492,-0.0419,0.0026,-0.0753,0.0402,-0.0273,-0.0303,-0.0504,-0.0486,-0.237,0.0319,-0.0573,0.0454,-0.0528,0.0342,0.0011,-0.0573,0.0531,-0.0467,0.0668,-0.0332,0.0311,-0.0377,-0.052,0.0083,-0.0656,-0.0251,-0.0562,-0.0459,0.0228,0.0151,-0.0543,0.0337,0.0607,0.0288,0.0393,0.0341,0.0088,0.0465,0.0157,0.0244,0.0217,-0.1885,0.0501,0.0312,0.0167,-0.0314,-0.0083,0.0153,0.0521,0.017,0.0624,0.0062,0.0029,0.0232,0.0102,0.029,-0.0197,0.0143,0.0432,0.0568,-0.0007,-0.0458,-0.0412,-0.0405,-0.0509,0.0373,-0.0133,0.0389,-0.0073,-0.0166,0.0112,-0.0435,0.0497,-0.045,0.0179,0.0444,0.0193,-0.0494,0.1968,-0.055,0.0375,0.0119,-0.0153,-0.0155,-0.0064,-0.0211,0.0074,-0.0158,0.0156,0.0121,-0.0001,0.0554,-0.0576,-0.0193,-0.0167,0.0149,0.0177,0.0009,-0.0332,-0.0096,0.0035,0.0287,0.0006,-0.0157,-0.0664,0.0267,0.1463,0.0294,0.0178,0.0118,-0.0323,-0.0813,-0.0299,0.0705,0.0201,0.007,-0.0049,0.011,-0.0061,-0.0523,-0.009,0.006,-0.0633,-0.0662,0.1203,-0.0229,-0.0197,-0.0475,0.0032,-0.037,0.0196,-0.0121,-0.0511,0.0424,0.0355,0.0156,0.0217,-0.0373,0.0133,-0.0541,-0.0904,-0.065,0.1326,-0.0343,-0.045,-0.0375,0.0077,0.0025,0.0214,0.0605,0.0451,-0.0313,-0.0276,0.0809,0.0436,-0.0921,-0.0259,-0.0094,0.0259,0.0075,-0.0396,-0.03,0.0477,0.0287,-0.0382,-0.0244,-0.0607,-0.0232,0.0433,-0.035,0.0214,-0.0616,-0.0046,-0.0171,0.0271,-0.0153,-0.0246,-0.0092,0.0047,0.0168,0.0015,-0.0233,0.0385,0.011,0.0287,-0.0205,-0.0148,0.0399,0.0288,0.0207,0.0037,0.0769,-0.0624,-0.0212,0.0028,0.0085,0.0413,-0.0014,0.0546,0.055,-0.0469,-0.0572,-0.2239,-0.0128,-0.0182,-0.064,0.0541,-0.1005,0.0228,-0.0072,0.0567,0.0766,0.0612,-0.0217,-0.0348,0.0045,0.0285,0.0552,0.0131,0.0115,-0.0551,0.0273,-0.0181,0.0251,-0.0063,-0.0585,0.0916,0.0164,0.1745,-0.02,0.0398,-0.0143,0.0472,0.0432,0.0354,-0.0606,0.1104,0.004,0.1134,-0.0116,-0.0352,-0.0692,-0.0092,0.0189,0.0192,-0.0553,-0.0357,-0.0322,-0.014,0.0424,-0.0478,0.0452,0.0502,-0.0777,0.0759,0.0036,0.0479,-0.0511,-0.1005,0.0498,-0.0043,0.0278,0.0345,-0.0588,0.0478,-0.0658,0.0564,-0.0477,-0.0059,-0.0382,-0.0159,-0.0392,-0.0424,0.0602,-0.0152,-0.0268,0.0473,-0.0106,0.0539,-0.0206,-0.0309,0.0196,0.0631,-0.0315,0.0368,0.0449,0.0237,-0.0151,0.0931,-0.0394,0.0392,-0.0393,-0.0306,-0.0216,-0.0721,-0.0157,0.0415,-0.043,-0.2718,0.0651,0.0134,0.0739,0.0007,-0.0155,0.0364,0.0066,-0.0157,0.0265,-0.0009,0.0505,0.0882,-0.0247,-0.0127,0.0036,0.1125,-0.025,0.0601,-0.0102,0.016,0.0753,0.2288,-0.0109,0.0047,0.0099,-0.0425,-0.0226,0.0524,-0.0179,0.0282,-0.0202,0.0635,-0.0466,0.0117,0.0726,-0.021,0.0108,0.0357,-0.0054,0.0297,0.0116,-0.0379,-0.0375,0.0814,-0.0544,0.0091,-0.0482,0.0088,0.0194,-0.0127,-0.0056,-0.0342,-0.0196,0.0314,-0.0036,-0.0817,-0.0356,-0.002,-0.0522,0.0226,-0.0667,-0.0368,-0.0323,-0.0175]}
{"key":"[An objective function for order preserving hierarchical clustering] We present an objective function for similarity based hierarchical clustering of partially ordered data that preserves the partial order. That is, if $x \\le y$, and if $[x]$ and $[y]$ are the respective clusters of $x$ and $y$, then there is an order relation $\\le'$ on the clusters for which $[x] \\le' |y]$. The theory distinguishes itself from existing theories for clustering of ordered data in that the order relation and the similarity are combined into a bi-objective optimisation problem to obtain a hierarchical clustering seeking to satisfy both. In particular, the order relation is weighted in the range $[0,1]$, and if the similarity and the order relation are not aligned, then order preservation may have to yield in favor of clustering. Finding an optimal solution is NP-hard, so we provide a polynomial time approximation algorithm, with a relative performance guarantee of $O\\!\\left(\\log^{3/2} \\!\\!\\, n \\right)$, based on successive applications of directed sparsest cut. We provide a demonstration on a benchmark dataset, showing that our method outperforms existing methods for order preserving hierarchical clustering with significant margin. The theory is an extension of the Dasgupta cost function for divisive hierarchical clustering.","layer":0,"vector":[-0.0327,-0.0225,0.0123,-0.0302,0.0145,0.0221,-0.0032,0.0071,0.0438,-0.005,0.0098,-0.0893,0.0478,0.0658,0.0142,-0.022,0.0116,0.0447,-0.0468,-0.0233,-0.0142,-0.0199,-0.0354,-0.0516,0.0544,0.0244,-0.0039,-0.0012,-0.0448,-0.2527,-0.0081,-0.0293,0.0839,-0.0465,0.036,0.0154,-0.013,0.0753,-0.0749,0.0203,0.0424,0.011,-0.045,-0.0177,-0.0443,-0.0146,-0.021,0.0163,-0.0266,-0.0547,0.0361,-0.0377,-0.033,0.0108,0.0602,0.0233,0.0319,0.0134,0.0151,0.0293,0.0578,0.0339,-0.1336,0.0529,0.0736,-0.0042,0.0068,-0.032,0.0345,0.0551,-0.033,0.0684,0.0094,0.0658,0.0092,-0.0273,-0.0055,-0.0598,-0.0612,-0.0059,-0.0155,-0.0107,-0.006,0.0207,-0.0166,-0.0449,0.0294,-0.0842,0.025,-0.0084,-0.0624,-0.0093,-0.0033,0.0238,-0.0813,-0.0359,0.0119,0.0277,-0.0176,0.2047,-0.0602,0.0686,0.052,-0.0345,-0.0089,-0.0677,0.0097,-0.0586,-0.0271,-0.0116,-0.0098,0.011,0.0248,-0.0448,0.0572,-0.0267,0.1057,0.018,0.0134,-0.0297,0.0109,0.0451,0.0579,-0.0088,0.0489,-0.0634,0.0081,0.1202,0.0522,0.026,0.0347,0.0006,-0.044,-0.0007,-0.0154,0.0451,0.0056,-0.0158,-0.0055,-0.0505,0.0004,-0.0883,0.0111,-0.0617,-0.0267,0.1463,-0.0456,0.029,-0.0725,-0.0128,-0.0269,-0.0553,-0.0271,-0.0857,-0.0182,0.0203,0.0545,0.0082,-0.0169,0.013,0.0036,-0.0498,-0.0278,0.127,0.0163,-0.1087,-0.0085,0.0124,0.0252,-0.0242,0.0363,0.0598,0.0042,0.0774,0.0837,-0.0008,-0.0449,0.0057,0.0445,0.0195,0.0467,-0.0313,-0.0061,0.0547,0.0442,-0.001,-0.011,-0.0017,0.0154,0.0169,-0.0896,0.0019,0.0172,-0.023,0.0058,-0.0378,-0.0072,-0.0153,0.0131,-0.0077,0.002,-0.0133,-0.0573,0.0718,-0.0031,0.0535,0.0187,-0.061,0.0412,0.0022,-0.0142,-0.0092,0.0424,-0.0426,-0.0144,0.0072,0.0466,0.0615,0.0097,0.0381,0.0761,-0.0644,-0.0689,-0.2256,-0.0282,-0.0263,-0.0002,0.0135,-0.0561,0.044,-0.0002,0.0489,0.0744,0.0399,-0.0122,-0.0388,0.0652,-0.0346,0.0364,0.0729,0.0008,-0.0053,-0.0018,0.0115,-0.0071,-0.0494,-0.0818,0.0484,-0.007,0.2274,0.0351,0.0013,0.0041,0.0169,0.0309,-0.0367,-0.067,0.0122,0.0256,0.0311,-0.047,-0.0151,-0.0123,-0.0324,0.0374,0.0164,-0.0636,-0.0352,-0.0341,-0.0354,0.027,-0.0166,0.0013,0.0261,-0.0368,0.016,-0.0225,-0.0133,-0.0197,-0.0627,-0.0179,-0.063,0.0091,0.029,-0.056,-0.0109,-0.0622,0.0527,-0.0285,-0.0298,-0.0114,-0.01,-0.0283,-0.0253,0.0519,-0.0336,0.0152,0.0512,0.0421,0.0409,0.0241,-0.0256,-0.0144,0.0967,-0.0323,0.0315,0.0252,0.0462,0.0274,0.1066,0.0002,-0.0054,0.0164,0.0249,-0.0027,-0.0958,0.01,0.0372,-0.0093,-0.2755,0.0364,-0.0048,0.0144,-0.0161,-0.0037,0.0337,0.023,0.0075,0.001,0.0022,0.0793,0.0603,-0.0542,-0.0184,0.0395,0.0124,-0.0653,0.0345,-0.0499,-0.0074,0.042,0.2183,-0.0269,0.0276,0.0138,-0.0214,0.0198,0.0084,-0.0577,-0.0061,-0.0054,0.1335,-0.0308,0.034,0.0556,0.0107,0.082,0.0275,-0.0152,-0.0171,0.0138,-0.1051,-0.0231,0.1087,-0.0163,-0.0443,-0.0477,0.0159,0.0287,-0.0397,0.0118,-0.0164,-0.0195,0.0047,0.0266,-0.0534,-0.0519,-0.0401,-0.0567,-0.0179,-0.0306,-0.033,0.0121,0.0258]}
{"key":"[A Federated Deep Learning Framework for Privacy Preservation and Communication Efficiency] Deep learning has achieved great success in many applications. However, its deployment in practice has been hurdled by two issues: the privacy of data that has to be aggregated centrally for model training and high communication overhead due to transmission of a large amount of data usually geographically distributed. Addressing both issues is challenging and most existing works could not provide an efficient solution. In this paper, we develop FedPC, a Federated Deep Learning Framework for Privacy Preservation and Communication Efficiency. The framework allows a model to be learned on multiple private datasets while not revealing any information of training data, even with intermediate data. The framework also minimizes the amount of data exchanged to update the model. We formally prove the convergence of the learning model when training with FedPC and its privacy-preserving property. We perform extensive experiments to evaluate the performance of FedPC in terms of the approximation to the upper-bound performance (when training centrally) and communication overhead. The results show that FedPC maintains the performance approximation of the models within $8.5\\%$ of the centrally-trained models when data is distributed to 10 computing nodes. FedPC also reduces the communication overhead by up to $42.20\\%$ compared to existing works.","layer":2,"vector":[-0.0125,-0.0406,0.0173,-0.0197,0.026,0.0453,0.0302,0.0148,0.0644,-0.0411,-0.0052,-0.0314,0.0374,0.0462,0.0195,0.0501,0.0093,0.0241,-0.0582,-0.018,0.0274,-0.0558,-0.0591,-0.0503,0.0024,0.0337,-0.0542,-0.0235,-0.072,-0.2134,0.055,-0.0565,0.0213,-0.0198,0.0551,-0.0264,-0.0183,0.0299,-0.0468,0.0766,-0.0069,0.0247,-0.044,-0.0101,-0.0158,-0.0195,-0.0148,-0.0316,-0.028,-0.0415,0.073,-0.0485,-0.0225,0.0681,0.0151,0.0324,0.0946,0.0213,0.0457,-0.0155,0.0212,0.0622,-0.1609,0.0618,0.029,0.0634,-0.0319,-0.0161,0.0265,0.0146,-0.0068,0.057,-0.0072,0.0029,-0.0015,0.0164,-0.0106,0.0148,-0.0326,0.0148,0.0046,-0.0125,0.0023,-0.0206,-0.0299,-0.066,0.0313,-0.0578,-0.0049,-0.0314,-0.0726,-0.0001,0.0198,0.022,-0.0458,-0.0108,0.0004,0.0367,-0.067,0.1903,-0.0346,0.0651,0.018,-0.0648,0.0315,-0.0285,-0.0152,-0.0111,-0.0153,0.0301,-0.0205,-0.0608,0.0098,-0.0256,0.0334,0.0306,0.1051,0.0677,-0.0347,-0.0213,0.0307,0.002,0.0801,0.0135,0.0377,-0.0947,-0.0027,0.1416,0.0254,0.0294,0.0435,-0.0337,-0.0227,-0.0359,0.051,0.0495,0.0119,0.0005,0.0054,-0.013,-0.0325,-0.0333,0.0053,-0.0538,-0.0213,0.1394,0.0164,0.0626,-0.0663,-0.0374,0.0088,0.0311,0.0043,-0.0271,0.0653,-0.0038,0.0286,0.0649,-0.0764,0.0119,-0.016,-0.0095,-0.0122,0.1154,0.0339,-0.136,-0.0145,0.0374,0.0028,-0.0623,0.0637,0.032,0.0036,0.0355,0.0377,0.0409,-0.0595,0.0002,-0.0075,0.0058,-0.0168,-0.0613,-0.0096,0.0222,0.0014,-0.0479,0.0201,-0.0605,-0.0092,0.0432,-0.0677,0.0458,-0.0444,-0.0071,-0.0222,-0.0482,-0.0078,-0.0167,0.007,-0.0027,-0.0241,0.0012,-0.0061,0.0171,-0.0313,0.01,-0.0033,-0.0086,0.0262,0.0136,-0.0685,-0.0133,0.0332,-0.0597,-0.0337,-0.0209,-0.0116,0.0179,-0.0173,0.0271,0.0546,-0.017,-0.0581,-0.193,-0.0127,0.0006,-0.0331,0.0862,-0.0781,0.083,0.0056,0.0352,0.0633,0.0622,-0.0346,-0.0519,0.0489,-0.0593,0.0553,0.0682,0.0572,-0.0051,-0.0033,0.0077,0.0259,0.0145,-0.0878,0.0598,0.0486,0.2148,-0.0216,0.0186,-0.0421,-0.0166,0.04,-0.0129,-0.1637,0.0396,-0.0166,-0.0032,0.0089,-0.0466,-0.0149,0.0018,0.0108,0.0204,-0.1609,-0.0233,-0.0459,-0.06,0.0244,-0.0844,0.0206,0.0472,-0.024,0.0558,0.027,-0.0077,-0.0388,-0.0603,0.0337,-0.0408,0.0836,-0.0101,-0.0233,-0.0016,-0.0447,0.112,-0.027,-0.0087,-0.0452,0.0319,-0.0378,-0.0107,0.0451,0.0055,-0.0048,0.0397,0.023,0.0238,-0.0048,-0.0923,-0.0313,0.1306,0.0182,0.0463,0.0396,-0.0109,0.0206,0.0964,0.0473,0.0296,-0.033,0.0109,-0.0085,-0.0469,-0.0444,0.0391,0.0006,-0.2937,-0.028,-0.035,0.0269,-0.0598,-0.0009,0.0493,0.0509,-0.0444,0.006,0.0282,0.0836,0.0493,-0.0039,0.023,0.024,0.0409,-0.0377,0.006,-0.0339,0.0335,0.0134,0.1816,-0.0346,0.0174,0.0173,-0.024,0.0348,0.0362,-0.0071,-0.0299,-0.0116,0.0613,-0.0577,0.0141,0.079,0.0132,0.0037,0.0214,0.0,-0.0093,-0.0178,0.0025,0.0164,0.0663,-0.0058,-0.0561,-0.012,-0.0058,0.0022,0.0009,-0.017,0.0028,-0.0287,0.0268,-0.0132,-0.0433,-0.0073,-0.0286,-0.0328,0.0031,-0.0418,-0.0344,-0.0103,-0.0281]}
{"key":"[OmniXAI: A Library for Explainable AI] We introduce OmniXAI (short for Omni eXplainable AI), an open-source Python library of eXplainable AI (XAI), which offers omni-way explainable AI capabilities and various interpretable machine learning techniques to address the pain points of understanding and interpreting the decisions made by machine learning (ML) in practice. OmniXAI aims to be a one-stop comprehensive library that makes explainable AI easy for data scientists, ML researchers and practitioners who need explanation for various types of data, models and explanation methods at different stages of ML process (data exploration, feature engineering, model development, evaluation, and decision-making, etc). In particular, our library includes a rich family of explanation methods integrated in a unified interface, which supports multiple data types (tabular data, images, texts, time-series), multiple types of ML models (traditional ML in Scikit-learn and deep learning models in PyTorch/TensorFlow), and a range of diverse explanation methods including \"model-specific\" and \"model-agnostic\" ones (such as feature-attribution explanation, counterfactual explanation, gradient-based explanation, etc). For practitioners, the library provides an easy-to-use unified interface to generate the explanations for their applications by only writing a few lines of codes, and also a GUI dashboard for visualization of different explanations for more insights about decisions. In this technical report, we present OmniXAI's design principles, system architectures, and major functionalities, and also demonstrate several example use cases across different types of data, tasks, and models.","layer":1,"vector":[-0.0446,0.0028,0.0086,0.0197,0.0553,0.0126,0.0419,0.0509,0.0585,0.0178,0.0121,-0.096,0.033,0.0559,0.0021,-0.0348,-0.043,0.034,-0.0584,0.0098,0.0647,-0.0148,-0.0537,-0.0358,0.0178,0.0488,-0.0426,-0.0315,-0.0358,-0.2551,0.022,-0.0443,0.0712,-0.0606,0.0194,0.0094,-0.0047,0.0173,-0.035,0.0147,0.0318,-0.0144,-0.0109,-0.0131,-0.0125,-0.0602,-0.0244,-0.0051,-0.0287,-0.0329,0.0253,-0.0142,-0.0058,-0.0029,0.0585,0.0357,0.0497,0.0918,0.0335,0.0402,0.0602,0.0601,-0.1307,0.0787,0.0656,0.0402,-0.0152,0.0057,0.0397,0.0545,-0.0291,0.0062,0.0135,0.0538,-0.0,-0.0104,0.0036,-0.039,0.0086,0.0089,-0.0018,-0.0129,-0.0416,-0.0199,-0.0128,-0.0221,0.0473,-0.0277,0.0449,-0.0125,-0.0067,0.0112,-0.0477,-0.0203,-0.0279,-0.0041,0.0516,0.025,-0.089,0.223,-0.0525,0.0061,0.0176,-0.006,0.0316,-0.0301,-0.0578,-0.075,-0.0154,-0.0067,-0.0057,-0.0097,0.0361,-0.025,0.0171,0.0245,0.0212,-0.0077,-0.0087,-0.0012,-0.0277,-0.0327,0.0095,0.0225,0.0195,-0.031,0.0476,0.1272,-0.0197,-0.0328,0.0725,-0.0443,-0.0696,-0.0332,0.0535,0.0577,0.03,0.0206,0.0163,0.0044,-0.0303,-0.0684,-0.0022,-0.1151,-0.082,0.0935,-0.0219,-0.0018,-0.0301,0.0095,-0.0369,0.0144,-0.0498,-0.0296,0.0045,0.0623,0.0284,0.0336,-0.0753,0.0661,-0.0114,-0.0744,-0.0348,0.0615,0.0364,-0.1228,-0.0244,0.0024,-0.0172,0.0248,0.0709,0.046,-0.0488,0.0379,0.055,0.0055,-0.0496,-0.014,0.0085,-0.0183,0.01,-0.0416,-0.0139,0.0633,0.0159,-0.0621,-0.0067,-0.0637,0.0002,0.0454,-0.016,0.0341,-0.0038,0.0117,-0.0378,-0.0077,0.0057,-0.0116,0.0038,-0.0168,-0.0042,-0.0045,-0.0345,0.0368,-0.0612,0.025,-0.0157,-0.0122,0.0573,0.0152,-0.003,0.01,0.0145,-0.001,-0.0366,-0.0167,0.0497,0.0262,0.0014,0.0225,0.0186,-0.0412,-0.063,-0.2012,0.005,0.0205,-0.0069,-0.0016,-0.0853,0.0482,-0.0315,0.0281,0.0775,0.088,-0.0259,-0.0261,-0.0307,-0.0544,0.0448,0.0044,0.0188,-0.0582,0.0079,0.0236,0.0446,0.0138,-0.1168,-0.0019,0.0207,0.2282,0.0232,0.0748,0.0025,0.0302,0.0276,-0.0408,-0.1425,0.0301,0.0321,0.0679,-0.034,-0.0184,-0.0195,-0.0121,0.0821,0.027,-0.0748,-0.0173,-0.0153,-0.0083,0.0372,-0.0219,0.0375,-0.0091,-0.005,0.0715,0.0148,-0.0258,-0.056,-0.083,0.0442,-0.0059,0.0082,0.0037,-0.0374,0.0029,-0.0757,0.0481,-0.0361,-0.0313,-0.0612,-0.0096,-0.0288,-0.0265,0.1274,-0.0476,-0.0366,0.0661,0.0373,0.027,-0.0301,-0.0329,0.0337,0.0566,0.0035,0.016,0.0016,0.0369,-0.0171,0.0539,-0.0179,0.041,-0.0236,0.0025,0.037,-0.0263,-0.0188,0.0498,-0.013,-0.3199,0.0448,-0.0161,0.0132,-0.0439,0.0049,0.0291,0.0441,-0.0226,0.0035,-0.0101,0.0344,0.0671,0.0055,-0.0317,0.0305,0.106,-0.037,0.047,-0.0084,0.0277,0.0404,0.2072,-0.0429,0.008,-0.0029,-0.0341,-0.0213,0.0579,-0.0217,0.027,-0.0434,0.0532,-0.0435,0.0483,0.0564,-0.0578,0.0345,0.0269,-0.0473,0.0126,-0.0101,-0.0389,-0.0203,0.0637,0.0117,-0.0466,-0.0367,-0.005,0.0555,0.0243,-0.0024,-0.0754,-0.0026,0.0113,0.009,-0.0241,-0.0385,-0.0138,-0.0221,0.0119,-0.0715,0.0453,0.0114,-0.0093]}
{"key":"[Supervised learning on heterogeneous, attributed entities interacting over time] Most physical or social phenomena can be represented by ontologies where the constituent entities are interacting in various ways with each other and with their environment. Furthermore, those entities are likely heterogeneous and attributed with features that evolve dynamically in time as a response to their successive interactions. In order to apply machine learning on such entities, e.g., for classification purposes, one therefore needs to integrate the interactions into the feature engineering in a systematic way. This proposal shows how, to this end, the current state of graph machine learning remains inadequate and needs to be be augmented with a comprehensive feature engineering paradigm in space and time.","layer":4,"vector":[-0.0415,-0.0096,0.0305,-0.0405,0.0364,-0.0082,0.0417,0.0172,-0.0029,-0.002,0.0382,-0.049,0.0295,0.0471,0.0276,0.049,0.0095,0.0649,-0.0407,-0.033,0.0206,-0.0402,-0.0103,0.019,0.0048,0.0283,-0.0431,-0.0356,-0.0501,-0.2233,0.038,-0.0419,0.0544,-0.005,-0.0129,-0.0177,0.0395,0.0325,-0.0452,0.0643,0.0239,-0.0176,-0.0134,-0.0517,-0.0248,-0.0368,0.0024,-0.0256,-0.075,-0.0472,0.0356,-0.048,0.0191,0.0385,0.0123,0.0831,0.0795,0.0314,0.0351,0.0398,0.0367,0.0554,-0.134,0.0539,0.047,0.0405,-0.0557,0.0153,0.0591,0.0552,0.0348,0.0836,-0.0056,0.0084,0.0103,0.0071,-0.0076,-0.0171,-0.0033,0.0361,-0.0116,-0.0099,-0.0598,-0.0004,-0.0256,-0.0259,0.0263,-0.0681,0.037,0.0287,-0.0421,-0.007,0.0153,0.0232,-0.0349,0.0009,0.071,-0.0116,-0.0147,0.2064,-0.0658,0.0443,0.0064,-0.0015,0.0279,-0.0582,-0.0179,-0.0957,-0.0145,-0.019,-0.0331,0.0163,-0.0243,-0.0459,0.0278,-0.0066,0.0516,0.0046,-0.0021,-0.0228,0.006,0.0157,0.0378,-0.0642,0.0234,-0.0822,0.0196,0.0927,0.0083,0.0163,0.0383,0.0356,-0.0392,-0.0402,0.0005,0.044,0.0241,-0.0137,0.0255,-0.018,-0.0367,-0.0772,0.04,-0.08,-0.0257,0.1384,-0.0341,0.0167,-0.0131,-0.0019,-0.03,0.017,-0.0547,-0.0105,-0.0145,0.0506,0.037,0.0254,-0.042,-0.015,-0.0132,-0.0296,-0.0562,0.1344,0.0058,-0.1224,-0.0392,0.0181,-0.0224,0.0063,0.0467,0.0991,0.0081,0.019,0.0771,0.0394,-0.083,0.01,0.0073,-0.0139,0.0341,-0.0169,-0.0582,0.0541,0.0071,-0.0311,-0.0036,-0.0387,0.0085,0.0391,0.0184,0.0152,0.0425,0.0132,0.0002,-0.0315,-0.0254,-0.0185,-0.003,-0.0498,-0.0112,0.0108,-0.0387,0.0106,-0.0193,0.0084,-0.0294,0.0424,0.0134,-0.016,-0.0178,-0.0331,0.0083,-0.0122,-0.0231,0.017,0.0243,0.0063,0.0294,0.0648,0.0358,-0.0493,-0.0438,-0.2138,-0.0059,0.029,-0.0252,0.0309,-0.0546,-0.0065,-0.0324,0.061,0.0687,0.0674,-0.0029,-0.0611,-0.0185,-0.0261,0.0442,0.0547,0.011,-0.0508,0.0052,-0.017,0.0166,-0.003,-0.0855,0.0155,0.0047,0.2337,0.0507,-0.0071,-0.0433,0.0179,0.0242,-0.0579,-0.1164,0.0812,0.0438,0.0553,0.0061,-0.0082,-0.0392,-0.0493,0.0277,0.0169,-0.0803,-0.0442,-0.041,-0.0003,0.0004,-0.0746,0.0327,0.0393,-0.028,0.0236,0.0184,-0.0168,-0.0367,-0.0487,0.0501,-0.0403,0.0156,0.0233,-0.0669,0.001,-0.0838,0.0614,-0.0103,-0.0424,-0.0416,-0.0275,-0.044,-0.0205,0.1252,-0.0107,-0.0639,0.0752,-0.0111,0.0225,0.0035,-0.042,-0.0123,0.0392,-0.0907,0.0372,0.0443,0.002,-0.015,0.0608,-0.0454,0.0443,-0.0158,0.0047,0.0099,-0.0189,-0.0229,0.0335,-0.024,-0.3013,0.0393,0.0322,0.0737,-0.0327,0.0109,0.0223,0.0116,-0.0418,-0.017,0.0183,0.0237,0.032,-0.0115,0.0026,0.0542,0.0782,-0.0552,0.0196,-0.0875,0.0321,0.0409,0.2503,0.0012,0.0818,0.029,-0.0718,0.0039,-0.0101,-0.012,0.0171,0.0148,0.0814,-0.0161,0.0327,0.0521,0.0165,0.0051,-0.0276,-0.0031,0.0176,-0.0005,-0.0379,-0.0092,0.1013,0.0273,-0.0214,-0.0812,-0.0004,0.0318,-0.0015,-0.0369,-0.0563,0.0477,0.0494,0.032,-0.0074,-0.0386,-0.0164,-0.0408,-0.0053,-0.0365,-0.0318,-0.0282,-0.036]}
{"key":"[Multi-level conformal clustering: A distribution-free technique for clustering and anomaly detection] In this work we present a clustering technique called \\textit{multi-level conformal clustering (MLCC)}. The technique is hierarchical in nature because it can be performed at multiple significance levels which yields greater insight into the data than performing it at just one level. We describe the theoretical underpinnings of MLCC, compare and contrast it with the hierarchical clustering algorithm, and then apply it to real world datasets to assess its performance. There are several advantages to using MLCC over more classical clustering techniques: Once a significance level has been set, MLCC is able to automatically select the number of clusters. Furthermore, thanks to the conformal prediction framework the resulting clustering model has a clear statistical meaning without any assumptions about the distribution of the data. This statistical robustness also allows us to perform clustering and anomaly detection simultaneously. Moreover, due to the flexibility of the conformal prediction framework, our algorithm can be used on top of many other machine learning algorithms.","layer":1,"vector":[-0.0227,-0.0387,0.0212,0.0065,0.0358,0.0253,0.0734,-0.0053,0.0024,-0.0336,0.0207,-0.0795,-0.0128,0.0392,0.0176,0.0089,0.0056,0.0116,-0.0137,-0.0053,0.016,0.0026,-0.0133,-0.0593,0.0438,0.0745,-0.0059,-0.0261,-0.0853,-0.2322,-0.0149,-0.0361,0.0806,-0.0006,0.014,-0.0193,-0.0851,0.0654,0.0002,0.0384,0.0365,-0.0017,-0.0405,-0.0396,-0.0659,-0.0592,-0.0181,0.0172,-0.0219,-0.0476,0.0236,-0.0225,0.0027,0.0404,0.02,0.0403,0.0681,0.0437,0.0423,0.0811,0.0743,0.0624,-0.1477,0.0345,0.0688,0.0148,-0.0234,-0.0422,0.0363,0.0187,-0.0222,0.0209,-0.0192,0.0354,0.0181,0.0246,0.0131,-0.0365,-0.0494,0.0058,-0.0303,-0.0519,0.0008,-0.0175,-0.0457,-0.0378,0.0037,-0.0436,0.067,-0.017,-0.048,-0.0061,-0.0253,0.0192,-0.0776,-0.0404,0.0159,0.0096,-0.0062,0.1819,-0.0549,0.0074,0.0434,-0.0034,0.0186,-0.0474,0.0018,-0.0208,-0.0013,-0.0516,-0.0101,-0.005,0.0402,-0.0562,-0.0044,-0.0052,0.071,0.0314,-0.0287,-0.022,0.0034,0.0036,0.0772,-0.0302,0.0425,-0.0366,0.0218,0.1204,0.0472,-0.0351,0.0246,0.0193,-0.054,-0.0217,0.0441,0.0425,-0.023,0.0228,0.0327,0.0025,-0.0297,-0.0487,0.007,-0.0787,-0.0443,0.1342,-0.0847,0.0005,-0.0299,-0.0286,-0.0382,-0.035,-0.0808,-0.0783,0.0271,0.0064,0.0521,0.0219,-0.0522,0.033,-0.0051,-0.0145,-0.0417,0.1087,0.0156,-0.1065,0.0109,-0.0116,0.018,0.0023,0.0213,0.0507,-0.0145,0.0254,0.0495,0.0338,-0.0736,0.01,-0.0167,0.0254,0.0163,-0.0141,-0.0314,0.0447,0.0511,-0.03,-0.0273,0.0216,0.0695,0.0278,-0.0735,0.0179,0.0045,-0.0279,0.0228,-0.0446,-0.0334,-0.0272,0.0449,-0.0224,0.0381,0.0145,-0.0664,0.0672,-0.0312,0.0729,-0.0144,-0.0372,0.0482,0.0193,-0.0088,0.0069,0.0492,-0.0373,-0.011,0.0307,0.0425,0.0764,-0.0036,0.0463,0.0675,-0.0474,-0.0686,-0.2038,-0.0387,0.0484,-0.043,0.0072,-0.0464,0.0237,-0.0038,0.071,0.0795,0.0605,0.0193,-0.0392,0.0576,-0.0237,0.0651,0.0067,0.0143,-0.0281,0.0347,-0.0239,0.0129,-0.0011,-0.0757,0.0293,0.0231,0.2095,0.0368,0.0028,0.02,0.0515,-0.018,-0.0178,-0.0472,0.0629,0.0129,0.0428,-0.0379,-0.0228,-0.0469,-0.0688,0.0439,0.005,-0.0675,-0.0505,-0.0513,-0.0005,0.0323,-0.0149,-0.0004,0.0219,-0.0183,0.0718,0.0066,-0.006,-0.0471,-0.0304,-0.0128,-0.042,0.0151,0.021,-0.0472,0.0133,-0.1076,0.0645,-0.0311,-0.051,-0.0224,0.0396,-0.083,-0.0707,0.1117,-0.0187,-0.0213,0.0817,0.0071,0.0251,-0.0236,-0.0383,-0.001,0.056,-0.0133,0.044,0.0049,0.0085,-0.0213,0.0475,0.0045,0.0819,0.0026,0.0404,-0.004,-0.0237,-0.0047,0.0188,0.0322,-0.2962,0.0075,-0.0165,0.0069,-0.0121,-0.008,0.0283,0.0097,-0.0082,0.0152,0.0477,0.001,0.0416,-0.0481,0.0398,0.0629,0.0491,-0.0631,0.0644,-0.0368,0.0053,0.0639,0.2554,-0.0503,-0.0132,0.0029,-0.0295,0.0194,0.0465,-0.0538,-0.0155,-0.0011,0.0918,-0.0555,0.0094,0.0459,0.0099,0.0299,0.009,-0.0595,0.0241,0.0226,-0.0515,-0.076,0.0606,-0.0252,-0.0037,-0.0758,0.0165,0.0358,-0.0207,0.012,-0.0732,0.0105,0.0028,0.0485,-0.0259,-0.0494,-0.0223,-0.0595,0.0399,-0.0388,-0.036,0.0199,-0.0091]}
{"key":"[Double Ramp Loss Based Reject Option Classifier] We consider the problem of learning reject option classifiers. The goodness of a reject option classifier is quantified using $0-d-1$ loss function wherein a loss $d \\in (0,.5)$ is assigned for rejection. In this paper, we propose {\\em double ramp loss} function which gives a continuous upper bound for $(0-d-1)$ loss. Our approach is based on minimizing regularized risk under the double ramp loss using {\\em difference of convex (DC) programming}. We show the effectiveness of our approach through experiments on synthetic and benchmark datasets. Our approach performs better than the state of the art reject option classification approaches.","layer":10,"vector":[-0.059,-0.0108,0.0308,-0.0074,-0.003,0.07,0.0581,0.0411,0.0392,-0.0275,0.0353,-0.0056,0.0067,0.0353,0.0205,0.0388,0.0256,0.0267,-0.0523,0.0366,0.0442,-0.0141,-0.0413,-0.0712,0.0364,0.0119,-0.0064,-0.029,-0.0371,-0.2508,0.032,-0.0268,0.0119,-0.0748,-0.014,-0.0189,-0.0066,0.0434,-0.0776,0.032,0.0183,0.0163,-0.0282,-0.058,-0.0328,-0.0364,0.0076,-0.0103,-0.0321,-0.0198,0.0269,-0.0301,0.0616,0.0191,0.0163,0.0357,0.0375,0.0542,0.0178,0.0885,0.0087,0.0298,-0.1347,0.0153,0.0155,0.0383,-0.065,-0.0239,-0.0023,0.0863,-0.0046,0.0082,0.0086,0.0492,-0.0432,0.0151,0.0179,-0.046,-0.0138,0.0159,0.0216,-0.0544,-0.0488,-0.0168,-0.0222,-0.0367,0.0302,-0.0101,0.0548,0.0136,0.0203,-0.0201,-0.0475,0.0391,-0.092,0.0257,0.0403,0.0012,-0.056,0.2104,-0.078,0.0059,-0.0162,-0.043,0.0689,-0.0351,-0.024,-0.0631,0.0121,-0.0019,-0.0064,0.0095,0.0415,-0.0348,-0.0284,0.0407,0.0538,0.0258,0.0096,-0.011,-0.0477,-0.016,0.0684,-0.0021,0.0273,-0.0853,0.0447,0.1698,0.0278,0.0279,0.0252,-0.0799,-0.0437,-0.0259,0.0405,-0.0024,-0.0071,0.0329,0.0114,-0.0276,-0.016,-0.0573,0.0056,-0.0479,-0.0419,0.1291,-0.0254,0.0401,-0.015,-0.0346,0.0222,-0.0107,-0.0185,-0.0072,0.0416,0.061,0.0571,0.0499,-0.0481,-0.0371,-0.0151,-0.048,-0.0432,0.1223,0.008,-0.0573,-0.0537,-0.0176,0.0344,-0.0081,0.0124,0.0472,-0.0077,0.0139,0.056,0.035,-0.0858,-0.0555,0.0289,-0.0239,0.002,-0.0524,-0.0353,0.0253,0.0586,0.0013,-0.0062,-0.0798,0.0429,0.0353,-0.0457,-0.0228,-0.0561,-0.043,-0.0085,-0.0739,-0.0312,-0.01,0.037,-0.0547,0.0331,0.035,0.0093,0.0739,0.0296,0.0261,-0.0195,-0.0073,0.0436,0.0209,-0.0403,-0.0066,0.0573,-0.0026,-0.0019,-0.0131,0.0472,0.0376,-0.0369,0.0245,0.0312,-0.0119,-0.0341,-0.2393,-0.0293,0.0088,-0.0197,0.0276,-0.0975,0.0519,-0.0346,0.0192,0.0959,0.0744,-0.0192,-0.0391,0.0184,0.0104,0.0293,0.0139,0.0255,-0.0418,0.018,-0.0169,0.0644,-0.0265,-0.0691,0.0554,0.0217,0.2161,-0.0087,0.0595,-0.0363,0.027,0.0494,0.0247,-0.0427,0.0506,0.048,0.0204,-0.0524,-0.0522,-0.0258,-0.0149,0.0067,0.0212,-0.1039,-0.0067,-0.0363,-0.0579,0.0587,-0.0684,0.0425,0.0672,-0.007,0.0815,0.0233,0.0324,-0.001,-0.0929,0.0288,-0.0046,0.0066,0.0122,-0.0815,0.0221,-0.0593,0.0431,-0.0081,-0.0429,-0.0178,-0.0068,-0.05,-0.0312,0.074,-0.0092,-0.0105,0.055,0.0345,0.0368,-0.0024,-0.0508,-0.0308,0.0632,-0.0572,0.0008,0.0227,0.0443,0.0417,0.1091,-0.0107,0.0143,0.025,-0.0199,0.0072,-0.0612,0.0007,0.0315,-0.024,-0.2693,-0.0221,0.0231,0.0076,-0.02,0.0072,0.0297,0.0062,-0.0476,-0.0184,-0.0001,0.0412,0.006,-0.008,0.0509,0.0233,0.0452,-0.0523,0.0447,-0.0317,0.0432,0.0247,0.1993,-0.0225,-0.0314,-0.0091,-0.05,-0.0268,0.0269,-0.0232,0.0086,-0.0059,0.0803,-0.1086,0.0243,0.0675,-0.0298,0.0075,0.0056,-0.0562,0.0143,0.0117,-0.0567,-0.0136,0.1233,-0.0399,-0.0027,-0.0315,-0.0035,0.0463,-0.012,0.0233,-0.0053,-0.0148,0.0187,0.0493,-0.0541,-0.0706,-0.0047,-0.0596,0.0142,-0.0505,0.0026,0.0023,0.0197]}
{"key":"[Autoregressive Text Generation Beyond Feedback Loops] Autoregressive state transitions, where predictions are conditioned on past predictions, are the predominant choice for both deterministic and stochastic sequential models. However, autoregressive feedback exposes the evolution of the hidden state trajectory to potential biases from well-known train-test discrepancies. In this paper, we combine a latent state space model with a CRF observation model. We argue that such autoregressive observation models form an interesting middle ground that expresses local correlations on the word level but keeps the state evolution non-autoregressive. On unconditional sentence generation we show performance improvements compared to RNN and GAN baselines while avoiding some prototypical failure modes of autoregressive models.","layer":0,"vector":[-0.0336,-0.0244,0.0226,0.013,0.0086,0.0486,0.0117,0.0466,0.0063,-0.0533,0.0082,-0.0066,0.05,0.0652,-0.0083,-0.0055,-0.0109,-0.0066,-0.0119,-0.0106,0.0336,-0.0282,0.0393,-0.0007,0.0027,0.0269,-0.0265,-0.0724,-0.0518,-0.2579,-0.0082,-0.0501,0.033,0.0169,0.0183,-0.0231,-0.0633,0.0565,0.0214,0.0418,-0.0009,0.0286,-0.0146,-0.0375,-0.0239,-0.044,0.0141,0.0055,-0.0254,-0.014,-0.0102,-0.0229,0.0258,0.013,0.0419,0.0353,0.0517,0.0288,0.0434,0.0493,0.0333,0.0485,-0.2156,0.0377,0.0357,0.003,-0.0646,0.018,0.003,0.0531,0.0114,0.0394,0.0122,0.0898,-0.0016,-0.0034,-0.0168,-0.0257,0.0061,0.0181,0.0632,-0.0127,-0.0943,-0.0092,-0.0079,-0.0648,0.0145,-0.0324,0.0346,0.0181,-0.0166,-0.0536,-0.009,0.0343,-0.053,0.0092,0.0325,0.0285,-0.0133,0.1835,-0.0373,0.0491,0.065,-0.0138,0.0438,-0.0182,0.0008,-0.0074,-0.0293,-0.0229,-0.0125,0.0033,0.0287,-0.0442,0.0735,0.0019,0.0855,0.0338,-0.0055,-0.0014,-0.0039,0.0533,0.013,-0.0691,0.0246,-0.067,0.0803,0.1425,0.0493,-0.0154,0.0254,-0.0251,-0.0545,-0.0258,-0.0224,-0.0009,0.0092,-0.022,0.0217,-0.0221,-0.0083,-0.0354,-0.0205,-0.0899,-0.0754,0.1249,-0.0314,0.0167,-0.0513,-0.0006,-0.029,0.0093,0.0145,-0.0602,0.0121,0.0538,0.0117,-0.0034,-0.0474,0.0513,-0.0121,-0.0215,-0.0567,0.041,0.0173,-0.0876,-0.0492,0.0334,0.0252,-0.016,0.0722,0.0253,-0.0727,0.0282,0.0515,0.0202,-0.0618,-0.0168,0.0194,0.0415,0.0075,-0.0535,-0.057,0.0984,-0.0003,-0.0515,0.0033,-0.0521,0.001,0.0345,0.0003,0.018,-0.0053,0.0262,-0.0651,-0.0493,-0.0279,0.0071,0.0033,-0.0675,-0.0105,0.0116,-0.054,-0.0391,0.0326,0.0128,-0.0265,-0.0128,0.0632,-0.0067,-0.0106,-0.025,0.0677,-0.0172,-0.0298,-0.0234,-0.0581,0.0094,0.0005,0.021,0.0059,0.0003,-0.0017,-0.2226,0.0084,0.0176,-0.0336,0.0992,-0.0671,0.0092,-0.0035,0.0882,0.0522,0.0017,-0.0228,-0.0005,0.0147,0.0201,0.0498,-0.0094,0.0422,0.001,0.0006,-0.0114,-0.0309,-0.0364,-0.1032,0.0519,-0.0069,0.194,0.03,0.0389,-0.0416,0.0639,0.0175,-0.0312,-0.0928,0.0747,0.0318,0.0661,0.0184,-0.0582,-0.0469,0.003,0.0542,0.0082,-0.0765,-0.0425,-0.0506,-0.0156,-0.0187,-0.0923,0.041,0.0173,-0.0499,0.0372,-0.0201,0.0108,-0.0435,-0.1309,0.0028,-0.055,0.0015,0.0167,-0.0076,-0.0038,-0.0831,0.059,0.0079,-0.0021,-0.0255,0.0284,0.0066,-0.0266,0.0891,-0.0518,-0.0032,0.0503,-0.0082,-0.0053,-0.0596,-0.0786,-0.0297,0.0997,-0.0532,0.0682,0.0355,0.0094,0.0148,0.0997,-0.0256,0.0427,-0.0105,0.0242,0.0214,-0.037,-0.0116,0.0579,-0.0377,-0.2926,-0.0059,0.0148,0.0227,0.0097,0.0044,0.0374,0.039,-0.0611,0.0103,-0.0221,0.0762,0.0541,-0.0692,-0.013,0.0417,0.0901,-0.0413,0.0524,-0.0622,0.026,0.0523,0.1961,-0.0005,0.0385,0.0042,-0.0139,0.0139,0.0566,0.0175,0.0047,-0.0075,0.0647,0.0203,0.0207,0.0461,-0.02,0.0419,0.0217,-0.0192,-0.0372,0.0469,-0.0455,-0.0144,0.091,0.0132,0.0174,-0.0309,0.0095,0.026,-0.009,-0.0128,-0.047,-0.0005,0.0299,0.0428,-0.0467,-0.05,-0.0219,-0.0376,-0.0007,-0.0683,0.005,0.0284,-0.0293]}
{"key":"[Job Dispatching Policies for Queueing Systems with Unknown Service Rates] In multi-server queueing systems where there is no central queue holding all incoming jobs, job dispatching policies are used to assign incoming jobs to the queue at one of the servers. Classic job dispatching policies such as join-the-shortest-queue and shortest expected delay assume that the service rates and queue lengths of the servers are known to the dispatcher. In this work, we tackle the problem of job dispatching without the knowledge of service rates and queue lengths, where the dispatcher can only obtain noisy estimates of the service rates by observing job departures. This problem presents a novel exploration-exploitation trade-off between sending jobs to all the servers to estimate their service rates, and exploiting the currently known fastest servers to minimize the expected queueing delay. We propose a bandit-based exploration policy that learns the service rates from observed job departures. Unlike the standard multi-armed bandit problem where only one out of a finite set of actions is optimal, here the optimal policy requires identifying the optimal fraction of incoming jobs to be sent to each server. We present a regret analysis and simulations to demonstrate the effectiveness of the proposed bandit-based exploration policy.","layer":7,"vector":[-0.104,-0.0204,-0.0014,-0.0259,0.008,0.0066,0.0745,0.0073,0.0209,0.0077,0.0307,-0.0173,0.0045,0.0501,-0.0148,-0.0004,-0.0183,0.0275,-0.0153,-0.018,0.0385,-0.0798,-0.0551,-0.1008,0.0578,0.0081,-0.0354,-0.0635,-0.0486,-0.1951,0.0323,-0.0526,0.0228,-0.0479,0.0276,-0.0261,0.0207,0.0483,0.0007,0.0709,0.0398,0.0124,-0.0462,-0.0505,-0.0402,-0.0575,0.0085,-0.0243,-0.0258,-0.0231,0.0054,0.0135,0.0219,0.0349,0.0145,0.0164,0.0353,0.0534,0.0579,0.0355,0.0353,-0.0274,-0.1476,0.0275,0.0284,0.0074,-0.0442,-0.0084,-0.0007,0.0301,-0.0196,0.0554,-0.0081,0.1109,0.0161,0.0066,0.0048,-0.0325,-0.0066,0.0227,-0.0086,-0.0655,-0.02,0.0078,-0.0271,-0.0551,0.021,-0.0151,0.0635,0.0241,-0.0211,0.0047,-0.0408,0.01,-0.0688,0.0088,0.0057,0.0322,-0.0451,0.2079,-0.0021,0.058,0.0233,-0.0068,0.0179,-0.05,-0.0528,0.0256,-0.0236,0.0303,-0.0194,0.0041,0.0646,-0.0238,-0.0077,0.0358,0.0314,0.0724,-0.003,0.0367,-0.0532,0.0187,0.0546,-0.0343,0.0191,-0.0601,0.0343,0.1527,0.0558,0.0144,0.0294,-0.0738,-0.0088,-0.0084,0.0129,0.0213,0.0079,0.0353,-0.003,-0.0278,-0.0203,-0.0647,0.0163,-0.1272,-0.011,0.0933,0.0327,0.0168,-0.0484,-0.0559,-0.0426,0.0284,-0.0396,-0.0194,-0.0206,0.0376,0.0706,0.0633,-0.0464,0.0505,-0.0521,-0.0463,0.0111,0.0833,-0.0041,-0.0615,-0.0075,0.0029,-0.0156,0.0003,-0.0151,0.0008,-0.1073,0.0063,0.0752,-0.0006,-0.0652,0.0282,0.0017,-0.0113,0.0452,-0.0347,0.0121,0.0378,0.06,-0.0409,0.0175,-0.0065,0.0111,0.0239,-0.0282,0.0023,0.0036,-0.0183,-0.0066,-0.0553,0.0128,0.0212,0.034,-0.0297,0.0339,0.0035,-0.0514,0.018,0.0079,-0.0072,-0.0036,-0.0021,0.0128,0.0198,-0.016,-0.0059,0.077,-0.0092,-0.0373,0.0421,0.0358,0.0649,0.0054,0.0107,0.0217,-0.0113,-0.0182,-0.2629,-0.0303,0.0049,-0.0069,0.0621,0.0007,0.077,-0.0216,0.0063,0.0563,0.0669,-0.0711,0.0116,0.0318,0.0043,0.0807,0.0144,0.0265,-0.0147,-0.0025,0.0203,0.0347,-0.0157,-0.0849,0.0474,0.0179,0.2263,0.0057,0.0089,-0.0583,0.0018,0.0177,-0.0119,-0.0842,0.0224,0.0544,0.0831,-0.0408,0.0119,-0.0195,-0.0313,0.0209,-0.0417,-0.0782,-0.0496,-0.055,-0.0413,0.0268,-0.0866,-0.0411,0.0116,-0.0589,0.0387,-0.0226,0.0252,-0.0526,-0.0384,0.0318,-0.0883,0.0325,0.0249,-0.0435,-0.0032,-0.0674,0.0472,0.0082,-0.0009,-0.0108,0.0184,-0.0411,-0.0446,0.0304,0.0077,0.0029,-0.0481,0.0286,-0.014,-0.0743,-0.0347,-0.0369,0.0608,-0.0771,0.0133,0.0236,0.003,0.0214,0.0703,0.0449,0.0113,-0.0049,-0.0234,-0.0247,-0.054,-0.0041,0.0582,0.0087,-0.2987,0.0803,0.0231,0.0147,-0.0139,0.0067,0.0047,0.0063,-0.0089,0.0003,0.0278,0.0502,0.0094,-0.0097,0.0362,0.074,0.0908,-0.0285,0.0028,-0.0621,0.0509,0.0301,0.2112,-0.0353,0.0572,0.025,-0.0305,0.0208,-0.0042,-0.0352,-0.0072,-0.0045,0.1081,-0.0708,0.0293,0.0608,-0.0221,0.0705,0.0012,0.0152,-0.0812,0.0101,0.0175,0.0453,0.1049,0.0094,-0.067,-0.0579,0.018,0.0199,-0.0278,0.0378,-0.021,-0.0007,0.0635,0.0177,-0.0841,-0.0521,-0.0581,-0.0136,0.0261,-0.0593,0.0279,-0.0291,-0.0041]}
{"key":"[Rethinking Exponential Averaging of the Fisher] In optimization for Machine learning (ML), it is typical that curvature-matrix (CM) estimates rely on an exponential average (EA) of local estimates (giving EA-CM algorithms). This approach has little principled justification, but is very often used in practice. In this paper, we draw a connection between EA-CM algorithms and what we call a \"Wake of Quadratic regularized models\". The outlined connection allows us to understand what EA-CM algorithms are doing from an optimization perspective. Generalizing from the established connection, we propose a new family of algorithms, \"KL-Divergence Wake-Regularized Models\" (KLD-WRM). We give three different practical instantiations of KLD-WRM, and show numerical results where we outperform K-FAC.","layer":0,"vector":[-0.0777,-0.027,0.0551,0.0375,0.0355,0.0741,-0.012,0.0597,0.024,-0.018,0.0059,-0.0473,-0.0281,0.0307,0.0247,0.0252,0.0237,0.0365,-0.0744,0.0169,0.0275,-0.0221,-0.0136,-0.0686,0.0163,-0.0186,-0.0331,-0.0154,-0.017,-0.2601,0.0161,-0.0442,0.0854,0.005,0.0078,-0.0104,-0.005,0.0228,-0.0155,0.0417,0.0079,0.0669,0.0051,-0.021,-0.0115,-0.0331,-0.0519,0.0206,-0.0437,-0.0379,0.0276,-0.0269,0.0019,0.0234,0.0203,0.0379,0.0141,0.0025,0.0462,0.0611,0.0001,0.0047,-0.1813,0.0355,0.0517,0.0149,-0.027,-0.0781,0.0325,0.0791,-0.0184,0.0218,0.0164,0.0085,0.0324,-0.0108,0.0493,-0.0348,-0.0287,0.0076,0.051,-0.0239,-0.0415,-0.0318,-0.0354,0.0214,0.0215,-0.0477,0.0429,0.0139,-0.0423,-0.0156,-0.0312,0.0123,-0.0578,-0.0487,0.0462,0.0302,-0.0444,0.1958,-0.0405,0.0676,0.0158,0.0181,0.0367,-0.0536,-0.0447,-0.0032,0.0069,-0.0082,0.0002,0.0012,0.0151,-0.0303,0.01,0.0058,0.0485,0.0775,-0.0265,-0.009,-0.0926,0.0037,0.038,-0.0034,0.0246,-0.0368,0.0192,0.1194,0.0134,0.0392,0.0365,-0.0124,-0.0562,-0.0276,0.0177,0.0049,0.0062,0.0156,0.0317,0.0215,-0.0467,-0.0828,-0.0043,-0.1075,-0.0174,0.1491,-0.0536,-0.0008,-0.0325,-0.0347,0.046,0.0206,-0.0402,0.0004,0.0856,0.0232,0.0373,0.0479,-0.0853,-0.0022,-0.0521,-0.0428,-0.0237,0.1186,0.0044,-0.039,-0.0426,-0.0095,0.0087,-0.0499,0.1007,0.05,-0.03,0.0414,0.0707,0.0252,-0.0715,-0.0195,-0.0046,-0.0428,-0.0016,-0.019,-0.044,0.0225,0.0113,-0.0529,0.0227,-0.0231,0.0148,0.0192,-0.0326,-0.0164,-0.0247,0.0025,-0.0319,-0.0216,-0.004,-0.0069,0.0482,-0.027,-0.004,0.0222,-0.0181,0.0522,0.0009,0.0321,0.0126,-0.0079,0.031,0.047,-0.065,-0.068,0.0633,-0.0331,-0.0478,0.019,0.0427,0.0247,-0.0224,0.0508,0.0344,-0.0466,-0.0585,-0.2225,0.0119,-0.0027,-0.0358,0.0528,-0.1095,0.0748,-0.0074,0.063,0.0739,0.0245,-0.0378,-0.0177,0.0504,0.004,0.0696,0.0187,0.0507,-0.0101,-0.0049,-0.0396,0.021,-0.0118,-0.0565,0.0105,-0.0148,0.2224,0.0456,0.0224,-0.0077,-0.0094,0.0238,0.0153,-0.0601,0.034,0.021,0.0731,-0.0349,-0.0406,-0.0022,-0.0233,0.0018,0.0003,-0.102,-0.0317,-0.0273,-0.0772,0.0192,-0.0308,-0.0016,0.0482,-0.0407,0.076,-0.0726,-0.0144,-0.0501,-0.0798,0.0123,-0.0869,0.012,0.0338,-0.0155,0.008,-0.0773,0.0678,0.002,-0.0321,-0.0635,-0.0024,-0.0499,-0.0233,0.0729,0.0305,-0.0181,0.0477,0.0037,0.0491,-0.0099,-0.0307,0.0087,0.0505,-0.0646,0.0394,0.0043,0.0343,0.0189,0.0667,-0.004,-0.0041,-0.0413,-0.0153,-0.0303,-0.0645,0.0239,0.0459,0.0319,-0.269,-0.0106,-0.0201,-0.0006,0.0067,-0.0233,0.0243,-0.0088,-0.0488,0.0157,-0.0037,0.0811,0.0137,-0.017,0.0222,0.0279,0.0767,-0.0493,0.0583,-0.0514,-0.0337,0.0297,0.2237,-0.0443,0.0295,0.0252,-0.0315,-0.0262,0.0408,-0.0235,0.029,0.0122,0.085,-0.0599,0.0965,0.0766,-0.0183,0.0306,0.0163,0.0043,0.0369,0.0147,-0.0278,0.0047,0.0856,-0.0047,0.0039,-0.0664,0.0151,0.0173,0.0113,0.0001,-0.0389,0.0503,0.0064,0.0105,-0.0411,-0.0477,-0.0362,-0.018,0.0283,-0.0724,-0.0317,-0.0155,0.0399]}
{"key":"[Which graphical models are difficult to learn?] We consider the problem of learning the structure of Ising models (pairwise binary Markov random fields) from i.i.d. samples. While several methods have been proposed to accomplish this task, their relative merits and limitations remain somewhat obscure. By analyzing a number of concrete examples, we show that low-complexity algorithms systematically fail when the Markov random field develops long-range correlations. More precisely, this phenomenon appears to be related to the Ising model phase transition (although it does not coincide with it).","layer":1,"vector":[-0.0696,0.002,0.0025,-0.0103,0.0361,0.0488,0.0246,0.0075,0.0457,-0.053,0.0509,-0.0889,0.033,0.0705,0.0302,0.0357,-0.0082,0.0386,-0.0368,0.0119,0.0234,-0.06,-0.0031,-0.0942,0.0338,0.0358,0.0041,-0.0569,-0.0085,-0.2574,0.0004,-0.0379,0.0465,-0.0123,0.0493,-0.0215,-0.0093,0.0595,0.0021,0.0324,0.0458,0.0274,-0.0206,-0.0644,-0.0012,-0.034,0.0043,-0.0212,-0.0175,-0.0542,-0.0209,-0.023,0.0122,0.007,0.0617,0.0619,0.0872,0.0109,0.0345,0.0286,-0.0067,0.0507,-0.1519,0.0862,0.0765,0.0264,-0.0215,0.0006,0.0372,0.0294,0.0146,0.0332,0.0118,0.014,-0.0059,-0.0172,-0.0125,-0.0078,-0.0178,0.0101,-0.0044,-0.059,-0.0396,0.0012,-0.0561,-0.0186,0.0136,-0.0482,-0.0037,0.0035,-0.0432,0.0012,-0.0458,0.0,-0.0771,0.0144,0.0525,0.0112,-0.0033,0.1632,-0.0553,0.0209,0.0371,-0.0343,0.0206,-0.0332,0.0117,-0.0483,0.01,-0.0109,-0.0165,-0.0015,0.045,-0.1082,0.0165,-0.0198,0.0649,0.0403,-0.0303,0.0231,-0.0399,0.0187,0.026,-0.0103,0.0068,-0.0595,-0.0296,0.113,0.0429,0.0296,0.0562,0.0169,-0.0436,-0.0252,0.0188,-0.0046,0.01,-0.0004,0.0098,0.0226,-0.0208,-0.0601,0.0315,-0.1012,-0.0388,0.0928,-0.0734,0.0288,-0.0768,-0.0066,-0.018,-0.0135,-0.0271,-0.0226,0.0497,-0.0045,0.0288,0.0388,-0.0404,0.0075,-0.051,-0.0902,-0.0178,0.1139,0.0291,-0.0601,-0.0202,-0.0053,-0.0008,-0.002,0.0352,0.0895,-0.0387,0.0544,0.0625,0.0323,-0.0794,-0.0208,0.0501,0.0083,0.0179,-0.0394,-0.0178,0.0476,0.0564,-0.0209,0.0155,-0.0432,0.0062,0.0291,0.0196,0.0211,-0.013,0.0322,-0.0456,-0.025,-0.0291,-0.012,0.0186,-0.038,0.0074,0.0052,-0.0701,0.018,-0.0173,-0.0005,0.0282,0.0371,0.0561,0.0033,-0.0102,-0.0051,0.0186,-0.0342,0.0172,0.0142,0.0316,0.0529,0.0027,0.0291,0.0294,-0.0683,-0.0771,-0.2529,-0.0271,0.0076,-0.0152,0.0979,-0.0962,0.0413,-0.0019,0.0661,0.0809,-0.0054,0.0001,0.0002,-0.0312,-0.038,0.0288,0.0209,0.0478,-0.0268,0.039,-0.0253,0.0198,-0.0163,-0.063,0.0267,-0.0038,0.2447,0.0016,-0.0065,-0.019,-0.0086,0.0233,-0.0652,-0.0039,0.0789,0.0147,0.0575,-0.0248,-0.0587,-0.052,-0.0457,-0.0002,0.0276,-0.0379,-0.0662,0.0071,-0.0274,0.0021,-0.0398,0.0234,0.0288,-0.0133,0.051,0.0042,-0.0122,-0.0305,-0.087,-0.001,-0.0386,0.0034,0.0281,-0.0595,0.0129,-0.0803,0.0173,0.0084,-0.0293,-0.0733,0.0477,-0.0251,-0.0015,0.0831,-0.0197,-0.0311,0.0374,0.0056,0.0183,-0.0384,-0.0702,-0.0459,0.0249,-0.0349,0.0334,0.0483,0.0312,-0.0102,0.0576,-0.013,0.0482,-0.0258,-0.0115,-0.0163,-0.0383,0.0173,0.0467,-0.0477,-0.2772,0.0158,0.0001,0.1063,-0.014,0.0187,0.0569,0.0831,-0.0682,-0.0306,-0.0245,0.0293,0.0631,-0.0395,-0.0291,0.0435,0.06,-0.0514,0.0685,-0.0624,-0.005,0.0083,0.2214,-0.0077,0.0732,0.0129,0.0076,0.0143,0.0329,-0.0003,0.0752,0.0065,0.076,-0.0275,0.0739,0.0686,0.0002,0.0343,0.0172,-0.0189,-0.0043,0.0221,-0.0587,-0.0555,0.1101,-0.0404,0.0161,-0.0331,0.0056,0.0485,-0.0532,-0.0051,-0.0259,0.0165,-0.0,0.0611,-0.0445,-0.0466,-0.0098,-0.0343,0.0054,-0.0618,-0.0209,-0.0344,-0.0041]}
{"key":"[GFCN: A New Graph Convolutional Network Based on Parallel Flows] In view of the huge success of convolution neural networks (CNN) for image classification and object recognition, there have been attempts to generalize the method to general graph-structured data. One major direction is based on spectral graph theory and graph signal processing. In this paper, we study the problem from a completely different perspective, by introducing parallel flow decomposition of graphs. The essential idea is to decompose a graph into families of non-intersecting one dimensional (1D) paths, after which, we may apply a 1D CNN along each family of paths. We demonstrate that the our method, which we call GraphFlow, is able to transfer CNN architectures to general graphs. To show the effectiveness of our approach, we test our method on the classical MNIST dataset, synthetic datasets on network information propagation and a news article classification dataset.","layer":2,"vector":[0.0345,-0.0079,-0.0124,-0.0334,0.0457,0.0365,0.0216,-0.001,0.0255,-0.0212,0.0153,-0.0484,0.0651,0.0792,0.0302,0.0295,0.0221,0.0667,-0.0426,-0.0175,0.0165,-0.0332,0.0276,-0.0429,0.0797,0.0272,0.0215,-0.0576,-0.0647,-0.234,0.0015,-0.0185,0.0709,-0.0468,0.0017,-0.0742,0.0204,0.0254,-0.0749,0.044,0.0152,-0.0282,-0.0513,-0.0143,-0.0141,0.017,-0.0492,0.0001,0.0032,-0.059,0.0364,-0.025,0.0275,-0.0017,0.032,0.0238,0.0615,0.0322,0.0497,0.0559,0.0358,0.0481,-0.1429,0.0783,0.0303,-0.0035,-0.0464,-0.0109,0.0179,0.0659,0.0049,0.0258,0.0169,-0.0257,0.0198,-0.0074,0.04,-0.0221,-0.0246,-0.0104,0.0021,-0.0145,-0.0503,-0.026,0.0286,-0.0169,0.0094,-0.0293,-0.0099,0.0258,-0.0789,-0.0251,-0.0155,0.0363,-0.0678,-0.0239,0.0055,0.0125,-0.0111,0.1597,-0.0855,0.0209,0.0609,-0.0042,0.0351,-0.0098,-0.0009,-0.0414,-0.046,-0.005,-0.0051,-0.0589,0.0329,-0.0436,0.0345,0.0028,0.0571,0.0686,-0.0587,0.0194,-0.0591,0.0166,0.0367,-0.0283,-0.0018,-0.0432,0.0339,0.1016,0.0519,0.0247,0.0518,0.0279,-0.0342,-0.0243,-0.01,0.0326,0.0345,-0.0168,-0.0205,-0.0349,-0.0509,0.0204,-0.0076,-0.0636,-0.0462,0.0991,-0.0604,-0.0158,-0.0355,-0.0397,-0.061,0.0301,-0.0195,-0.0482,-0.0158,0.0274,0.0235,0.0319,-0.0729,0.0167,-0.0115,-0.0336,-0.0817,0.0997,0.0422,-0.1234,-0.018,-0.0583,0.0076,-0.0172,0.0193,0.0348,-0.0224,0.0106,0.0663,0.0544,-0.0814,-0.0045,0.0277,0.0061,-0.0067,-0.0514,-0.002,0.0115,0.0495,-0.017,0.0008,-0.0374,0.0169,0.074,-0.0714,0.0573,-0.0146,-0.0054,-0.0272,0.0042,-0.0225,-0.0439,-0.0305,-0.0176,-0.016,-0.0055,-0.0312,0.03,-0.0648,0.0191,-0.0525,0.0203,-0.0264,0.0039,-0.043,0.014,0.0256,-0.0111,0.0173,-0.0152,0.0505,0.0005,0.0197,0.0537,0.0027,-0.0626,-0.0722,-0.2017,-0.027,0.0005,-0.0453,0.0786,-0.0682,0.0325,0.0176,0.0754,0.0947,0.0531,-0.0044,-0.024,-0.0344,0.0325,0.0821,0.0383,0.0742,-0.0213,0.0017,-0.0046,0.0354,0.0164,-0.1147,0.0401,0.0312,0.2491,0.0043,0.0544,-0.012,0.0218,0.0578,-0.0375,-0.0799,0.0448,0.0419,0.0366,0.0009,-0.0607,-0.0047,-0.0353,-0.0081,0.0117,-0.0963,-0.0296,0.0039,-0.0196,0.02,-0.0396,0.0206,0.0234,-0.0332,0.0465,0.0199,0.0281,-0.0493,-0.093,0.0603,-0.0499,0.0193,0.0044,-0.0476,0.0324,-0.0572,0.0933,0.033,-0.044,-0.0446,0.0285,-0.0089,-0.0028,0.0685,0.0638,0.0107,0.1101,0.0443,0.0322,-0.0111,-0.0324,-0.0216,0.0585,-0.0248,0.0301,0.0293,0.046,0.0352,0.0765,-0.0242,0.0169,-0.0541,0.0501,0.0033,-0.0375,-0.007,0.0094,-0.0104,-0.3003,0.0221,0.0338,0.0599,-0.0189,0.0268,0.0569,0.0396,-0.0337,0.0131,0.0175,0.0336,0.0295,-0.0084,0.0098,0.056,0.058,-0.0467,0.0487,-0.047,0.0205,0.0169,0.2257,-0.0349,0.0101,0.0253,-0.049,-0.0312,0.0069,-0.0471,0.0205,0.005,0.0762,-0.0799,0.0468,0.0646,-0.035,0.0608,0.0128,-0.0483,-0.0031,-0.0101,-0.0226,-0.0471,0.0639,-0.0031,-0.0246,-0.0652,0.0089,0.0061,-0.0054,-0.0231,0.02,0.0055,0.0087,0.0203,-0.0479,-0.0311,-0.0528,-0.0392,-0.0134,-0.1049,-0.0186,-0.0147,-0.0708]}
{"key":"[The Many Faces of Link Fraud] Most past work on social network link fraud detection tries to separate genuine users from fraudsters, implicitly assuming that there is only one type of fraudulent behavior. But is this assumption true? And, in either case, what are the characteristics of such fraudulent behaviors? In this work, we set up honeypots (\"dummy\" social network accounts), and buy fake followers (after careful IRB approval). We report the signs of such behaviors including oddities in local network connectivity, account attributes, and similarities and differences across fraud providers. Most valuably, we discover and characterize several types of fraud behaviors. We discuss how to leverage our insights in practice by engineering strongly performing entropy-based features and demonstrating high classification accuracy. Our contributions are (a) instrumentation: we detail our experimental setup and carefully engineered data collection process to scrape Twitter data while respecting API rate-limits, (b) observations on fraud multimodality: we analyze our honeypot fraudster ecosystem and give surprising insights into the multifaceted behaviors of these fraudster types, and (c) features: we propose novel features that give strong (>0.95 precision/recall) discriminative power on ground-truth Twitter data.","layer":2,"vector":[-0.0338,-0.03,-0.0,-0.0271,0.0617,-0.008,0.0991,0.0359,0.0488,-0.0271,0.0354,-0.0247,0.0112,0.0234,0.0182,0.0009,0.0278,0.001,-0.0298,0.0266,0.034,-0.0598,0.0273,-0.0769,0.0844,0.0316,-0.0061,-0.0519,-0.0569,-0.2149,0.0382,-0.0653,0.0225,-0.032,0.015,-0.0411,-0.0323,0.0124,-0.0086,0.0078,0.0259,0.0119,-0.0342,-0.049,-0.0429,-0.028,-0.0077,0.0465,-0.0356,-0.018,-0.0277,-0.0342,0.009,0.0427,0.0348,0.0555,0.0597,0.0578,0.0586,0.0432,0.0672,0.0527,-0.1025,0.0298,0.0293,0.046,-0.0356,-0.0086,-0.0066,0.0026,0.028,0.0483,0.0099,0.0307,-0.0071,0.029,-0.0591,0.0178,-0.0386,0.0417,-0.0251,-0.0538,-0.0341,-0.02,0.011,-0.0429,0.0438,-0.0102,0.0283,-0.0222,-0.0087,-0.0127,0.0018,0.003,-0.0338,-0.0147,0.0066,0.026,-0.0514,0.2175,-0.0672,0.0329,0.0191,-0.023,0.04,-0.0674,-0.0048,-0.0522,0.0007,0.0083,-0.0314,-0.0276,0.0186,-0.0192,0.031,0.0174,0.0399,0.0344,-0.0087,-0.0154,-0.0439,0.0392,0.0756,-0.0114,-0.0032,-0.0461,-0.0026,0.1443,0.041,0.0565,-0.0191,0.0157,-0.0637,-0.0108,0.0163,0.0204,-0.0584,0.0199,0.0413,-0.0137,-0.0881,-0.0382,0.012,-0.0771,-0.0643,0.1048,-0.0368,0.0175,-0.0112,0.0056,-0.015,0.0286,-0.0628,-0.0668,0.028,0.0151,0.032,0.0871,-0.0645,0.0243,0.0182,-0.0449,-0.0258,0.1037,0.0414,-0.1218,0.0356,0.0128,-0.019,-0.0363,0.0499,0.0177,-0.0823,0.0082,0.0203,-0.0126,-0.0698,-0.0194,0.0208,0.0361,0.0248,-0.0567,-0.0577,0.0319,0.0342,-0.0211,0.0131,-0.011,0.0247,0.0406,-0.0428,0.0172,-0.0516,-0.037,-0.0158,-0.0218,-0.015,-0.003,0.0011,-0.0254,0.0113,-0.0057,-0.0417,0.0025,0.0011,0.0476,-0.0217,-0.0076,0.0434,-0.0023,-0.0186,0.021,0.0132,-0.0106,0.0088,-0.0505,0.0247,0.0338,0.0218,0.026,0.0269,-0.0564,-0.045,-0.2281,-0.0381,-0.0059,0.0185,0.042,-0.0984,0.0131,-0.0345,0.0602,0.0851,0.0818,-0.0372,-0.0219,0.0012,0.024,0.0756,0.0167,0.045,-0.0042,0.039,-0.0526,0.0316,-0.0209,-0.087,0.062,0.0134,0.2293,0.0787,-0.0092,-0.0381,0.0248,0.0159,-0.0392,-0.1108,0.0516,0.0376,0.0612,-0.0529,-0.0232,-0.0086,-0.0621,0.05,-0.0016,-0.0756,-0.0186,-0.0217,-0.0666,-0.0339,-0.0483,0.0611,0.0554,-0.0062,0.0721,0.0118,0.0106,-0.0702,-0.0703,0.0143,-0.0584,0.0482,0.0205,-0.0467,0.0443,-0.0464,0.0673,0.0127,-0.0442,-0.0059,0.0376,-0.0247,-0.0059,0.1048,0.0324,-0.017,0.0298,-0.0348,0.0363,-0.0963,-0.0555,-0.0085,0.0784,-0.0167,0.0458,0.0192,-0.0143,0.0096,0.0739,0.0095,0.0654,-0.039,0.0184,0.0263,-0.0487,-0.0579,0.0277,0.0307,-0.2964,-0.0038,-0.0063,0.0622,-0.0459,-0.0017,0.0635,0.0256,-0.0189,-0.0433,0.0361,0.0169,0.0451,-0.0173,-0.0112,0.071,0.0176,-0.0653,0.0202,-0.016,0.0314,0.0047,0.2268,0.0113,0.0071,0.0326,0.0063,0.0408,0.0205,-0.0211,0.0361,0.0078,0.0353,-0.0497,0.0454,0.0238,-0.0428,0.0335,0.0224,-0.0204,-0.0598,0.0182,-0.0771,-0.0077,0.0894,-0.0554,-0.0295,-0.0463,0.0339,0.0712,-0.0193,-0.0127,-0.0277,-0.0081,0.0312,0.0522,-0.0367,-0.0153,-0.0239,-0.0476,-0.0497,-0.0233,-0.0212,-0.0142,-0.0152]}
{"key":"[Asynchronous Adaptation and Learning over Networks --- Part I: Modeling and Stability Analysis] In this work and the supporting Parts II [2] and III [3], we provide a rather detailed analysis of the stability and performance of asynchronous strategies for solving distributed optimization and adaptation problems over networks. We examine asynchronous networks that are subject to fairly general sources of uncertainties, such as changing topologies, random link failures, random data arrival times, and agents turning on and off randomly. Under this model, agents in the network may stop updating their solutions or may stop sending or receiving information in a random manner and without coordination with other agents. We establish in Part I conditions on the first and second-order moments of the relevant parameter distributions to ensure mean-square stable behavior. We derive in Part II expressions that reveal how the various parameters of the asynchronous behavior influence network performance. We compare in Part III the performance of asynchronous networks to the performance of both centralized solutions and synchronous networks. One notable conclusion is that the mean-square-error performance of asynchronous networks shows a degradation only of the order of $O(\\nu)$, where $\\nu$ is a small step-size parameter, while the convergence rate remains largely unaltered. The results provide a solid justification for the remarkable resilience of cooperative networks in the face of random failures at multiple levels: agents, links, data arrivals, and topology.","layer":1,"vector":[-0.0802,-0.0348,-0.0288,0.0341,0.0241,0.0368,0.0142,0.028,0.0345,-0.0132,0.0689,-0.0165,0.0752,0.0552,-0.0055,0.0377,-0.0243,-0.003,-0.0111,0.0201,-0.0137,-0.0645,-0.0015,-0.0426,0.0131,-0.0018,-0.0222,-0.0257,-0.0426,-0.252,0.0246,-0.0537,0.0145,-0.0291,-0.0194,-0.0321,-0.0062,0.0181,0.0061,0.0487,0.0264,0.0246,-0.0649,-0.1075,-0.0401,0.0025,-0.0124,-0.0358,-0.0022,-0.0629,-0.0038,-0.0172,0.025,0.011,0.0295,0.0607,0.0578,0.0987,0.023,0.0572,0.0184,0.0559,-0.1574,0.0622,0.0503,0.0087,-0.0429,-0.0224,0.0417,0.0309,0.0154,-0.0053,0.0282,0.0414,0.064,0.0513,-0.0066,-0.0224,0.0226,0.0271,-0.006,-0.0395,-0.0403,-0.0129,-0.0209,-0.0551,0.0403,-0.0334,0.0171,0.0204,-0.0527,0.0374,-0.0165,-0.0009,-0.0395,-0.0207,0.013,0.019,-0.0293,0.1798,-0.0209,0.0059,0.0695,-0.0043,0.0342,-0.0346,0.003,-0.0467,0.0034,0.0142,-0.0153,0.0023,0.0148,-0.0054,0.0373,0.0154,0.0477,0.0037,0.0045,-0.0397,-0.0442,0.0419,0.0725,-0.0423,0.0629,-0.0447,-0.0087,0.1378,0.0071,0.0441,0.0394,0.0288,0.0169,-0.0585,0.0044,0.0585,0.0161,-0.0408,0.0004,-0.0077,0.0001,-0.0734,0.053,-0.0944,-0.0478,0.1136,0.0241,0.057,-0.0255,-0.0528,-0.0598,-0.0388,-0.0025,-0.0461,-0.0314,0.0141,0.053,0.0354,-0.0391,0.0046,-0.0545,-0.0325,-0.0012,0.113,-0.0069,-0.0718,-0.0551,-0.0019,-0.0042,-0.0386,-0.0015,0.0378,-0.0194,0.0037,0.0767,0.0278,-0.0642,-0.0463,-0.014,0.0175,0.0109,-0.0591,0.0033,0.0224,0.0132,-0.0124,-0.0422,0.0095,0.0266,0.0481,-0.0487,-0.0271,0.0085,0.0126,-0.0291,-0.0557,0.0112,-0.0424,0.0212,-0.0075,-0.0263,0.0249,-0.0566,-0.004,-0.0243,0.0085,-0.0466,0.0201,0.0167,0.0184,-0.0284,-0.0121,0.0232,-0.0175,-0.0574,0.014,0.0172,0.0775,0.0273,0.0229,0.067,0.0257,-0.0475,-0.1899,-0.0002,0.0124,-0.0153,0.0656,-0.0344,0.0333,-0.0567,-0.0048,0.0455,0.069,0.0107,-0.0241,0.0171,-0.0188,0.0386,0.0232,0.0469,0.0188,-0.0185,-0.028,0.016,0.0082,-0.1013,0.0486,0.0205,0.1998,-0.0428,0.0344,-0.0298,0.06,0.0666,-0.0483,-0.0709,0.0628,0.0669,0.0727,-0.0213,-0.0419,-0.0075,0.0173,0.0141,0.0278,-0.101,-0.049,-0.0045,-0.0448,0.0227,-0.0644,-0.0363,0.0267,-0.0153,0.0523,-0.0193,-0.0191,-0.0391,-0.0273,0.0374,-0.0331,0.0183,0.009,-0.0432,-0.0088,-0.0368,0.1005,0.0098,-0.0155,-0.0682,0.0252,-0.0149,0.0008,0.0823,-0.027,0.0204,0.0089,0.0151,-0.0066,-0.0441,-0.0391,-0.0378,0.0594,-0.0975,0.0583,0.0093,0.022,-0.0126,0.0641,0.0065,0.001,0.0027,0.0035,0.0329,-0.0509,-0.0314,0.0439,-0.0496,-0.3111,0.0378,0.0148,0.0194,-0.0198,-0.0043,0.0332,0.0534,-0.0866,0.0165,0.0436,0.0946,0.0328,0.0155,0.042,0.0559,0.0675,-0.0492,0.0308,-0.0814,0.0321,0.0384,0.2118,-0.0404,0.0488,0.0463,0.0025,0.032,-0.0003,-0.0286,-0.002,0.0484,0.0647,-0.075,0.0733,0.0514,-0.0677,0.0441,0.0086,-0.0047,-0.0206,0.0381,0.0073,-0.0174,0.1336,-0.0035,-0.0555,-0.0963,-0.0233,0.0296,-0.0685,-0.0587,0.0237,-0.0271,-0.0136,0.026,-0.0339,-0.0562,-0.0717,-0.0355,0.0475,-0.0793,-0.019,-0.0009,-0.0149]}
{"key":"[A bag-to-class divergence approach to multiple-instance learning] In multi-instance (MI) learning, each object (bag) consists of multiple feature vectors (instances), and is most commonly regarded as a set of points in a multidimensional space. A different viewpoint is that the instances are realisations of random vectors with corresponding probability distribution, and that a bag is the distribution, not the realisations. In MI classification, each bag in the training set has a class label, but the instances are unlabelled. By introducing the probability distribution space to bag-level classification problems, dissimilarities between probability distributions (divergences) can be applied. The bag-to-bag Kullback-Leibler information is asymptotically the best classifier, but the typical sparseness of MI training sets is an obstacle. We introduce bag-to-class divergence to MI learning, emphasising the hierarchical nature of the random vectors that makes bags from the same class different. We propose two properties for bag-to-class divergences, and an additional property for sparse training sets.","layer":1,"vector":[-0.0289,-0.0426,-0.0029,-0.0027,0.0305,0.0063,0.0698,0.0093,0.0187,0.0021,-0.0034,-0.0585,-0.0135,0.0854,0.0465,0.0215,0.0246,-0.0017,-0.0746,-0.0232,0.0362,-0.0422,-0.0232,-0.0255,0.0275,0.0158,-0.0011,-0.0358,-0.0202,-0.2531,0.039,-0.0271,0.0343,-0.0003,0.0426,0.0017,-0.0047,0.0565,-0.0716,0.0509,0.0149,0.021,-0.0378,-0.0731,0.0007,-0.057,-0.0372,-0.0317,-0.01,-0.0353,0.0586,-0.0186,-0.0353,0.0476,-0.0052,0.0178,0.0248,0.0625,0.0165,0.0573,-0.0032,0.0308,-0.1568,0.0432,0.0359,0.0278,-0.0595,-0.0505,0.034,0.0679,-0.0024,0.0454,0.0168,0.0858,-0.0093,-0.0183,0.045,-0.0248,-0.0161,0.016,0.016,-0.0108,-0.0643,-0.0107,-0.0458,-0.0186,0.015,-0.0645,0.0598,-0.0096,-0.0916,-0.0105,-0.018,0.0286,-0.0719,-0.0291,0.0295,-0.0014,-0.0174,0.1927,-0.0282,0.045,0.0775,0.0064,0.0429,-0.0304,-0.0073,-0.0584,-0.0193,-0.0307,-0.024,-0.0143,0.0058,-0.0284,-0.0161,-0.006,0.0748,0.0037,-0.0127,0.0011,-0.0175,0.0196,0.0996,-0.0296,0.0075,-0.0515,0.0216,0.1382,0.0447,-0.0041,0.017,-0.0064,-0.0536,-0.0287,0.0148,0.0101,0.0186,0.0433,0.0486,-0.0258,-0.015,-0.0704,0.0498,-0.0573,-0.0746,0.1064,-0.0617,0.0387,-0.049,-0.0424,-0.0334,0.0076,-0.0095,-0.0524,0.0251,0.0295,0.0674,0.0517,-0.0034,-0.0151,-0.0178,-0.0617,-0.0154,0.0804,0.0179,-0.0766,-0.049,0.0236,0.0299,-0.017,0.0082,0.0146,-0.0268,0.0427,0.0956,0.0308,-0.0987,-0.0062,0.0469,-0.0057,-0.012,-0.0629,-0.0587,0.0051,0.0173,-0.03,0.0153,-0.0448,0.0389,0.0395,-0.0382,0.0225,0.0076,-0.0148,-0.0143,-0.0234,-0.0041,-0.0122,0.0159,-0.053,-0.0243,-0.0098,-0.0287,0.0446,0.0006,0.0419,-0.0003,-0.0073,0.0531,0.0102,-0.0235,0.0012,-0.0009,-0.0246,-0.0293,-0.0232,0.046,0.075,-0.0104,0.0175,0.0217,-0.0764,-0.0478,-0.2302,0.0065,0.0279,-0.0172,0.0146,-0.0567,0.0543,0.0209,0.0491,0.0705,0.0647,-0.0249,-0.0243,0.0355,0.0078,0.038,0.03,0.0513,-0.0239,0.0069,-0.0077,0.0375,0.0281,-0.0728,0.0554,0.0066,0.2093,0.0252,0.0092,0.0116,0.0252,0.0722,-0.016,-0.071,0.052,-0.0022,0.0558,-0.0311,-0.0384,-0.0134,-0.0012,0.0315,0.0145,-0.0985,-0.0597,-0.0657,-0.02,0.0488,-0.0752,0.0207,0.0356,-0.0262,0.0535,-0.0571,0.0047,-0.0113,-0.0532,0.0216,-0.0435,0.0216,0.0088,-0.0354,0.0575,-0.1048,0.093,-0.0159,-0.0397,-0.0525,0.0295,-0.0535,-0.0504,0.0774,-0.0099,-0.0105,0.0481,0.0083,0.0119,0.0437,-0.0668,-0.0223,0.051,-0.011,0.0139,-0.0024,0.0639,0.0221,0.0961,0.0304,0.0121,0.0003,0.0319,0.0529,-0.022,0.0341,0.0044,-0.0109,-0.2913,-0.0036,-0.0141,0.0087,-0.0168,0.0432,0.0843,-0.0053,-0.0308,-0.0172,0.0446,0.029,0.0391,-0.0206,0.0256,0.0358,0.088,-0.0463,0.0417,-0.0888,0.0197,0.067,0.1817,-0.0105,-0.0051,-0.005,-0.06,-0.0132,0.0003,0.001,0.0339,-0.014,0.1034,-0.0692,0.0121,0.0904,-0.04,0.0274,-0.0104,-0.0421,0.0203,0.0085,-0.102,-0.0517,0.0864,0.0418,0.0025,-0.045,-0.0288,-0.0134,0.0051,0.0176,-0.0207,0.0337,-0.0057,0.0025,-0.0155,-0.0519,-0.0375,-0.0945,0.0559,-0.0682,-0.0463,-0.0212,0.0217]}
{"key":"[A deep learning framework for morphologic detail beyond the diffraction limit in infrared spectroscopic imaging] Infrared (IR) microscopes measure spectral information that quantifies molecular content to assign the identity of biomedical cells but lack the spatial quality of optical microscopy to appreciate morphologic features. Here, we propose a method to utilize the semantic information of cellular identity from IR imaging with the morphologic detail of pathology images in a deep learning-based approach to image super-resolution. Using Generative Adversarial Networks (GANs), we enhance the spatial detail in IR imaging beyond the diffraction limit while retaining their spectral contrast. This technique can be rapidly integrated with modern IR microscopes to provide a framework useful for routine pathology.","layer":1,"vector":[-0.0151,0.0051,0.0733,-0.0044,0.0354,0.062,0.0412,0.0239,0.0171,-0.0026,0.0103,-0.0599,0.0646,0.095,0.0202,0.0078,0.0086,-0.0067,-0.0679,0.0359,0.043,-0.0107,0.013,-0.0572,-0.0374,-0.0145,-0.0451,-0.0267,-0.0555,-0.2095,0.0113,-0.0263,0.031,-0.0023,0.0035,-0.04,-0.0638,0.0633,-0.0458,0.0222,0.0189,0.0169,-0.0566,0.0011,0.0102,-0.0541,-0.0258,-0.0025,0.0044,-0.0675,0.0251,-0.0771,0.021,0.033,0.015,0.0102,0.0746,0.0262,0.0729,0.0395,0.0346,0.0664,-0.1439,0.057,0.081,0.0063,-0.0328,-0.0247,0.0138,0.0086,-0.0336,0.0577,0.0279,0.0482,-0.0033,-0.016,-0.0234,-0.0699,-0.0419,0.0199,0.0487,0.0298,-0.0214,-0.0125,0.015,-0.0077,-0.0248,-0.0617,0.0206,0.0368,-0.0178,-0.0616,-0.0675,0.0748,-0.0774,-0.0251,-0.0054,-0.0267,-0.012,0.2058,-0.0665,-0.0455,0.044,-0.0693,-0.0226,0.0046,-0.0888,0.0121,-0.0327,0.0409,-0.0069,-0.0116,0.0305,-0.0101,0.0075,-0.0155,0.0608,0.015,0.0165,-0.0363,-0.0298,0.0027,0.0598,-0.0317,0.0484,-0.0457,0.0327,0.119,0.0535,0.0271,0.018,0.001,-0.0419,0.0014,0.0047,0.0391,0.0329,-0.0021,0.002,0.0141,-0.0617,-0.0508,-0.0225,-0.0291,-0.0235,0.1258,-0.0893,0.0353,-0.0665,-0.0288,-0.0325,0.0523,-0.0275,-0.0346,0.0105,0.0692,-0.0023,0.0669,-0.0265,0.0039,-0.0262,-0.0622,-0.0349,0.1304,0.0045,-0.0835,-0.0216,0.0004,0.0222,0.001,-0.0018,0.0066,-0.0299,0.0273,0.088,0.0287,-0.0614,-0.0124,-0.0131,0.0079,0.0396,-0.0688,-0.0177,0.0295,0.0438,-0.0351,0.0054,-0.0385,-0.0197,0.0674,-0.0449,0.0534,-0.0515,-0.0565,-0.0358,-0.0274,-0.0177,0.0027,-0.0514,-0.0146,0.0455,-0.0172,-0.037,0.0537,0.0061,0.0222,0.009,-0.0014,0.0141,0.0277,-0.0692,0.0119,0.0202,-0.0001,-0.0523,-0.0186,-0.0133,-0.0056,-0.0113,0.0478,0.0536,-0.0557,-0.1132,-0.2117,0.0479,-0.0225,-0.0443,0.0438,-0.0703,0.0283,-0.0054,0.0514,0.0554,0.0222,0.0189,0.0136,-0.0202,-0.0117,0.0536,0.0317,0.0329,0.0023,0.0125,0.0048,0.0489,0.0142,-0.0714,0.0586,-0.0171,0.2254,0.0543,0.0358,-0.0004,0.0107,0.0107,-0.059,-0.0884,0.0326,0.0187,0.0829,-0.007,-0.0514,-0.014,-0.026,0.0299,-0.0242,-0.1063,-0.0158,-0.0123,-0.0088,0.0205,0.0055,0.0344,0.037,-0.0342,0.0448,-0.0285,0.0054,-0.0125,-0.0857,0.0498,-0.0506,-0.049,-0.0035,-0.0485,0.0334,-0.0462,0.0757,-0.0024,-0.0591,-0.0506,0.0928,-0.0531,0.0153,0.0574,0.0182,0.0403,0.0356,0.0133,0.0334,-0.0229,-0.0393,-0.0334,0.0963,-0.0044,0.0547,0.0198,0.0068,0.0523,0.0805,-0.0257,0.0025,-0.0341,-0.0214,0.05,-0.086,-0.0172,0.0045,-0.0271,-0.2688,0.0618,0.0373,0.0629,-0.0467,-0.0166,0.0175,0.0436,-0.0617,0.0225,0.0159,-0.008,0.0692,-0.0269,0.0324,0.0212,0.0788,-0.0776,0.0776,-0.0612,0.0215,0.0107,0.2157,-0.081,0.0156,0.024,-0.0289,0.036,-0.0178,-0.0474,0.0435,0.0283,0.0552,-0.0244,0.0269,0.0662,-0.0426,0.0404,0.0036,0.0173,-0.0179,0.0053,-0.0501,0.019,0.0899,-0.0284,-0.0224,0.0281,0.0018,-0.0086,-0.044,0.0097,-0.0217,0.0178,0.045,0.003,-0.0318,-0.056,0.0055,-0.0169,-0.023,-0.0129,-0.0757,0.0343,-0.0241]}
{"key":"[Relationship between Variants of One-Class Nearest Neighbours and Creating their Accurate Ensembles] In one-class classification problems, only the data for the target class is available, whereas the data for the non-target class may be completely absent. In this paper, we study one-class nearest neighbour (OCNN) classifiers and their different variants. We present a theoretical analysis to show the relationships among different variants of OCNN that may use different neighbours or thresholds to identify unseen examples of the non-target class. We also present a method based on inter-quartile range for optimising parameters used in OCNN in the absence of non-target data during training. Then, we propose two ensemble approaches based on random subspace and random projection methods to create accurate OCNN ensembles. We tested the proposed methods on 15 benchmark and real world domain-specific datasets and show that random-projection ensembles of OCNN perform best.","layer":0,"vector":[-0.0046,-0.023,0.0423,-0.0371,0.0504,0.0118,0.0487,0.0178,0.0121,-0.0317,0.0174,-0.0575,-0.0186,0.0562,0.0196,0.0047,0.045,0.0487,-0.0307,-0.0277,-0.0101,-0.0134,0.0017,-0.0285,0.0194,0.008,0.0339,-0.0797,-0.0377,-0.2619,0.008,-0.0622,0.0599,-0.0461,-0.0051,-0.0268,-0.0124,0.019,-0.0312,0.0327,-0.0085,0.0355,0.0,-0.0772,-0.0288,-0.0484,-0.0108,-0.0208,-0.0113,-0.0068,0.0355,-0.0461,-0.004,0.0197,0.0089,0.0087,0.0054,0.0447,0.0494,0.0373,0.0425,0.0471,-0.1137,0.0359,0.0481,0.018,-0.009,-0.0187,-0.0038,0.0737,0.009,0.0158,0.025,0.0392,-0.0135,-0.0304,0.012,0.0008,-0.0141,0.0096,0.0078,-0.0397,-0.0863,-0.024,0.0099,-0.0548,0.0256,-0.0412,0.0632,0.0059,-0.072,0.0034,-0.0275,0.0248,-0.0365,-0.0364,0.0796,0.005,-0.0461,0.2136,-0.0756,0.0183,0.0387,-0.0235,0.0437,-0.0569,-0.045,-0.0549,-0.0181,-0.0204,0.0298,-0.03,-0.0401,-0.0328,0.0225,0.0206,0.0738,0.0567,-0.0096,-0.0055,-0.0521,-0.0481,0.0572,-0.0387,0.0272,-0.0288,0.0548,0.1426,0.0031,0.0402,0.0384,-0.0265,-0.0443,-0.0449,0.0124,0.0547,0.0338,0.0322,-0.0052,-0.0363,-0.0088,-0.0618,0.0549,-0.058,-0.063,0.1017,-0.0433,0.0386,-0.0261,-0.0451,0.0129,0.0257,-0.0529,0.0007,0.0247,-0.0075,0.0424,0.028,-0.0225,-0.0021,0.0192,-0.0586,-0.017,0.1029,0.0335,-0.1348,-0.0451,-0.0028,-0.0024,-0.0443,0.0412,0.0552,-0.0298,0.0685,0.0692,0.0305,-0.096,-0.0073,-0.0085,0.0146,-0.0082,-0.0139,-0.0427,0.0147,0.0456,0.004,0.0232,-0.0618,0.0023,0.0441,-0.0763,0.0025,-0.0206,-0.0241,-0.0445,-0.0292,-0.0222,-0.0246,0.0517,-0.0818,0.0143,0.046,-0.0132,0.0398,-0.0081,0.0103,0.0221,-0.0267,0.0342,0.0334,-0.0475,-0.0016,0.0171,-0.0276,-0.0352,-0.0004,0.0114,0.0424,0.0185,0.0631,0.0435,-0.0189,-0.0801,-0.238,-0.0005,0.0328,-0.0441,0.0511,-0.0581,0.0382,0.0285,0.046,0.0737,0.0655,0.0157,0.0057,0.0165,-0.0032,0.0599,0.0613,0.0347,0.0226,-0.0219,0.0271,0.0366,-0.0121,-0.096,0.0564,-0.0121,0.1731,0.0001,0.0673,-0.0074,0.0341,0.05,-0.0345,-0.0917,0.0635,0.0361,0.0777,-0.0488,-0.0574,-0.0049,0.0072,0.0411,0.0283,-0.1139,-0.0667,-0.0464,-0.0309,0.0065,-0.0521,0.0298,0.06,-0.0247,0.069,0.01,0.0006,-0.0439,-0.091,0.0649,-0.0061,0.0147,0.0043,-0.0772,0.0657,-0.0736,0.0597,0.0021,-0.0794,-0.0413,0.0369,-0.0186,-0.0524,0.0892,-0.0156,0.0054,0.0474,-0.0707,0.0596,0.0134,-0.0355,-0.0159,0.0651,-0.0105,0.0311,-0.0085,0.0356,0.0176,0.1162,0.0065,0.0086,0.0079,0.0015,0.0267,-0.0327,0.0162,0.0412,-0.0015,-0.2686,-0.004,0.0248,0.0521,0.0132,-0.0013,0.0042,-0.0055,-0.0529,0.0022,0.0194,0.0245,0.0739,-0.0131,-0.0011,-0.0103,0.0709,-0.0536,0.0587,-0.0612,0.049,0.015,0.2177,-0.0034,0.007,0.0164,-0.0338,-0.0024,-0.0188,-0.0563,0.0152,0.0017,0.0664,-0.0688,0.0052,0.0781,-0.0156,0.0132,0.0067,-0.0372,0.0012,-0.0245,-0.0744,-0.0088,0.1056,0.0138,-0.039,-0.0305,-0.0053,0.0195,-0.0257,-0.0071,-0.0071,-0.0266,0.0234,0.0658,-0.0529,-0.0435,-0.0646,-0.0269,0.0072,-0.0407,-0.0127,-0.0026,0.0093]}
{"key":"[Banker Online Mirror Descent] We propose Banker-OMD, a novel framework generalizing the classical Online Mirror Descent (OMD) technique in online learning algorithm design. Banker-OMD allows algorithms to robustly handle delayed feedback, and offers a general methodology for achieving $\\tilde{O}(\\sqrt{T} + \\sqrt{D})$-style regret bounds in various delayed-feedback online learning tasks, where $T$ is the time horizon length and $D$ is the total feedback delay. We demonstrate the power of Banker-OMD with applications to three important bandit scenarios with delayed feedback, including delayed adversarial Multi-armed bandits (MAB), delayed adversarial linear bandits, and a novel delayed best-of-both-worlds MAB setting. Banker-OMD achieves nearly-optimal performance in all the three settings. In particular, it leads to the first delayed adversarial linear bandit algorithm achieving $\\tilde{O}(\\text{poly}(n)(\\sqrt{T} + \\sqrt{D}))$ regret.","layer":0,"vector":[-0.0622,-0.0117,0.0597,-0.0281,0.0049,0.0267,0.0378,0.0201,0.0419,-0.0157,0.0459,-0.0146,0.0288,0.0352,-0.0108,0.0206,0.0006,0.0141,-0.0434,0.0293,0.0255,-0.0609,0.0082,-0.0646,0.036,-0.0125,-0.0245,-0.0829,-0.0469,-0.2131,0.0187,-0.0391,0.0184,-0.0506,-0.0119,-0.0375,-0.0477,0.0498,0.0174,0.0672,-0.0167,0.0461,-0.0321,-0.0733,0.0115,-0.017,-0.0028,0.0255,-0.0043,-0.0742,0.0021,-0.0395,0.0308,-0.0001,0.0348,0.0097,0.0203,0.0863,0.035,0.0746,0.044,0.0073,-0.1484,0.0369,0.0048,0.0291,-0.0162,-0.007,0.0053,0.0426,0.0156,0.0034,0.0148,0.063,0.0208,0.0171,0.0182,-0.0491,0.0001,-0.0101,0.0455,0.0034,-0.0589,-0.0081,-0.0276,-0.0574,0.0219,-0.0259,0.0611,0.0284,-0.0056,-0.042,-0.036,0.0263,-0.0642,-0.0468,0.0478,0.0248,-0.0654,0.2232,-0.0073,0.0964,0.0122,-0.0313,0.0217,-0.0247,-0.0059,-0.0245,-0.0022,-0.0205,-0.0353,-0.0093,0.1068,-0.0256,0.0076,0.0315,0.0311,0.0147,-0.0052,0.0287,-0.0197,0.02,0.0937,-0.0281,0.041,-0.0742,-0.006,0.1147,0.0286,0.0643,0.0183,-0.0937,-0.0353,-0.0514,0.0126,0.0006,-0.0111,0.0133,0.0483,-0.0422,-0.0395,-0.0256,-0.0003,-0.0859,0.0014,0.0977,0.0067,0.0308,-0.0326,-0.0215,-0.0225,0.0074,-0.0269,-0.0225,-0.0131,0.0425,0.0469,0.072,-0.0415,0.0071,-0.0414,-0.0438,-0.0022,0.0714,0.0082,-0.0874,-0.0439,-0.015,-0.003,0.0204,0.0487,0.0012,-0.0429,-0.007,0.0909,0.0095,-0.1005,0.0016,0.0163,-0.0316,-0.0034,-0.0227,-0.0029,0.0454,0.043,0.0062,0.0191,-0.0618,0.0133,0.0675,-0.0223,-0.0117,-0.0617,-0.034,-0.059,-0.079,-0.0385,-0.0242,-0.0017,0.0042,0.0102,0.006,-0.0471,-0.0062,0.0352,0.0269,0.0204,-0.0401,0.0479,0.0158,-0.0758,-0.0148,0.0294,-0.0005,-0.0139,0.0014,0.0634,0.031,-0.0277,0.0472,0.0213,-0.0348,-0.0203,-0.2218,-0.0116,-0.0492,-0.0086,0.0484,-0.086,0.058,-0.0363,0.0511,0.0457,0.0701,-0.0224,-0.0047,0.0139,-0.0196,0.0665,0.0216,0.0196,-0.0301,0.0198,-0.0482,0.0756,0.026,-0.0392,0.0454,-0.0139,0.2458,0.0441,0.0121,-0.0166,0.0585,0.0362,0.0161,-0.0851,0.0413,0.0282,0.1155,-0.0167,-0.0621,-0.0558,-0.0048,0.0306,0.0396,-0.083,-0.0608,-0.0151,-0.0357,0.0142,-0.018,0.0034,0.0223,-0.026,0.0417,-0.0299,-0.0199,-0.0603,-0.0931,0.0822,-0.0627,0.0431,0.053,-0.037,-0.0186,-0.0575,0.0405,0.0019,-0.0253,-0.0464,0.0475,-0.0286,-0.0309,0.0515,0.0075,0.0148,0.0054,-0.0046,0.0398,-0.047,-0.0721,-0.0099,0.0534,-0.0684,0.0256,0.0084,0.0185,0.0056,0.0738,0.0184,-0.0053,-0.0111,-0.0158,0.0255,-0.0744,0.002,0.0631,0.0157,-0.3182,0.0659,0.0237,0.0474,-0.0227,0.0203,0.0456,-0.0118,-0.0542,0.0162,0.0124,0.0446,0.0251,0.0013,0.0386,0.0412,0.0694,-0.0456,0.0293,-0.078,0.0019,0.0718,0.1941,-0.0264,0.0434,-0.0183,-0.0318,0.0186,0.0425,-0.0465,0.0235,0.0333,0.0642,-0.0396,0.0136,0.0753,-0.0391,0.0414,0.0145,-0.0184,-0.0443,0.0152,-0.0436,0.024,0.0998,0.0253,-0.0429,-0.0306,-0.0356,0.038,-0.024,0.0072,-0.0288,-0.0118,0.0154,0.0078,-0.0351,-0.0452,-0.0494,-0.0345,0.0081,-0.0148,-0.035,-0.0548,-0.005]}
{"key":"[Adding more data does not always help: A study in medical conversation summarization with PEGASUS] Medical conversation summarization is integral in capturing information gathered during interactions between patients and physicians. Summarized conversations are used to facilitate patient hand-offs between physicians, and as part of providing care in the future. Summaries, however, can be time-consuming to produce and require domain expertise. Modern pre-trained NLP models such as PEGASUS have emerged as capable alternatives to human summarization, reaching state-of-the-art performance on many summarization benchmarks. However, many downstream tasks still require at least moderately sized datasets to achieve satisfactory performance. In this work we (1) explore the effect of dataset size on transfer learning medical conversation summarization using PEGASUS and (2) evaluate various iterative labeling strategies in the low-data regime, following their success in the classification setting. We find that model performance saturates with increase in dataset size and that the various active-learning strategies evaluated all show equivalent performance consistent with simple dataset size increase. We also find that naive iterative pseudo-labeling is on-par or slightly worse than no pseudo-labeling. Our work sheds light on the successes and challenges of translating low-data regime techniques in classification to medical conversation summarization and helps guides future work in this space. Relevant code available at \\url{https://github.com/curai/curai-research/tree/main/medical-summarization-ML4H-2021}.","layer":0,"vector":[-0.0161,0.0219,-0.001,-0.015,0.0159,-0.0191,0.0416,0.0585,0.0154,-0.0674,-0.0221,-0.0316,0.0178,0.0495,0.0178,0.0291,0.0247,-0.0034,-0.0534,0.0602,-0.0046,-0.0186,-0.0035,-0.0027,0.045,-0.0103,-0.0736,-0.0873,-0.0629,-0.2014,-0.0257,-0.0436,0.0573,-0.0012,-0.0182,0.0253,-0.0101,0.0769,-0.0387,0.0577,0.045,-0.0008,-0.0259,-0.0476,-0.012,-0.05,-0.0412,-0.0431,-0.0061,-0.0385,0.0172,-0.0361,-0.0127,-0.0019,0.0188,0.0447,0.0322,0.0067,0.0272,0.051,0.0144,0.0632,-0.1715,0.082,0.0348,0.0288,-0.0509,0.008,0.0079,0.0632,0.0091,0.0335,0.0435,0.0793,-0.043,0.0088,-0.0141,-0.0179,0.0262,0.0372,-0.0181,-0.0324,-0.0847,-0.0294,-0.0242,-0.0608,0.059,-0.058,0.0222,0.0099,-0.0514,-0.0211,0.0133,0.06,-0.0521,-0.0191,0.0252,-0.0111,-0.0386,0.1982,-0.084,0.0376,0.0425,-0.0223,0.0077,-0.0181,-0.0188,-0.0514,-0.0085,-0.0106,-0.0501,0.0101,0.0465,-0.0385,0.0686,0.035,0.0944,0.0081,-0.02,0.0159,-0.0157,0.0019,0.0145,-0.0121,0.0225,-0.0565,0.0726,0.1264,0.0564,0.0221,0.0654,0.0101,-0.0429,-0.026,-0.0081,0.0246,0.055,0.0097,0.0242,-0.0032,-0.0281,-0.0634,0.005,-0.0784,-0.0513,0.1719,-0.0611,-0.0417,-0.0351,-0.0478,-0.003,0.0431,-0.0034,-0.0185,-0.013,0.0232,0.0948,0.08,-0.0608,-0.0016,0.0302,-0.0809,-0.0382,0.098,0.0429,-0.092,-0.0637,-0.0219,0.0369,-0.0321,0.0647,0.0482,-0.0072,0.0346,0.0496,0.0511,-0.0353,-0.0243,0.0143,0.0044,0.0322,-0.0098,-0.0177,0.0693,-0.0193,-0.057,-0.0133,-0.0359,0.0497,0.048,-0.0237,0.0224,-0.0146,0.0145,-0.0799,-0.0215,-0.021,-0.0061,0.0084,-0.0011,-0.0111,-0.0251,-0.0221,0.0425,0.0023,-0.0144,-0.0248,0.0125,0.0452,0.0376,-0.0196,0.004,0.0359,-0.0184,-0.0735,-0.0254,0.0075,0.0212,-0.0388,0.0344,0.0272,-0.0099,-0.0179,-0.2407,0.0124,0.0323,-0.0615,0.0382,-0.0548,0.044,0.0159,0.0473,0.0876,0.0521,-0.016,-0.0438,-0.0306,-0.0575,0.0515,0.0382,-0.0006,-0.0162,-0.0159,-0.0,0.0541,0.0453,-0.1032,0.0195,-0.0449,0.2193,0.0349,0.0163,-0.0275,0.0346,0.0118,-0.0521,-0.1409,0.0505,-0.014,0.0617,-0.0292,-0.0493,0.0161,-0.0371,0.0224,-0.0215,-0.0887,-0.0456,-0.0446,-0.0393,-0.0087,-0.0513,0.0299,-0.0039,0.0028,0.0609,-0.0071,-0.0162,-0.0348,-0.1135,-0.0003,-0.0435,0.0148,-0.0077,-0.0045,0.0306,-0.0372,0.0185,0.0042,-0.0196,-0.0051,0.0208,-0.0429,-0.0279,0.0953,-0.0304,0.0161,-0.0002,0.0469,0.0137,-0.0563,-0.063,-0.0406,0.0656,-0.0412,0.0508,0.0355,0.0302,0.0086,0.0743,-0.0048,0.009,-0.0468,0.0173,0.0557,-0.0515,0.0032,0.0125,-0.0304,-0.27,0.0488,0.012,0.0463,-0.0396,0.0531,0.0456,0.0246,-0.0555,0.0183,0.0063,0.0203,0.0569,-0.0153,-0.0251,0.0494,0.0867,-0.0586,0.0116,-0.0239,0.0325,0.005,0.1785,-0.0556,0.0138,0.0112,-0.0144,-0.0019,0.0518,-0.0015,-0.0256,-0.0049,0.108,-0.0254,0.0535,0.0409,-0.013,0.02,0.0381,0.0662,0.0263,0.0111,-0.0327,0.0061,0.0662,0.0027,0.0005,-0.0913,-0.0043,0.0189,-0.0199,0.0021,0.0452,0.0127,0.0441,0.0321,0.0105,-0.0366,-0.0258,-0.036,-0.0275,-0.1025,-0.0249,0.0215,0.0017]}
{"key":"[Transfer Learning in 4D for Breast Cancer Diagnosis using Dynamic Contrast-Enhanced Magnetic Resonance Imaging] Deep transfer learning using dynamic contrast-enhanced magnetic resonance imaging (DCE-MRI) has shown strong predictive power in characterization of breast lesions. However, pretrained convolutional neural networks (CNNs) require 2D inputs, limiting the ability to exploit the rich 4D (volumetric and temporal) image information inherent in DCE-MRI that is clinically valuable for lesion assessment. Training 3D CNNs from scratch, a common method to utilize high-dimensional information in medical images, is computationally expensive and is not best suited for moderately sized healthcare datasets. Therefore, we propose a novel approach using transfer learning that incorporates the 4D information from DCE-MRI, where volumetric information is collapsed at feature level by max pooling along the projection perpendicular to the transverse slices and the temporal information is contained either in second-post contrast subtraction images. Our methodology yielded an area under the receiver operating characteristic curve of 0.89+/-0.01 on a dataset of 1161 breast lesions, significantly outperforming a previous approach that incorporates the 4D information in DCE-MRI by the use of maximum intensity projection (MIP) images.","layer":2,"vector":[-0.033,-0.0228,0.0286,-0.0142,0.032,0.0386,0.0318,0.0126,0.0394,-0.0037,0.0231,-0.0837,0.0194,0.0186,-0.0035,0.0261,0.0061,0.017,-0.0505,0.0294,0.0088,-0.0205,0.0167,-0.0117,0.0256,-0.0226,-0.0057,-0.0179,-0.0955,-0.2538,0.0149,-0.0227,-0.0237,-0.0395,-0.0006,-0.0528,-0.0186,0.0447,-0.0216,0.0421,0.0279,0.0422,-0.0193,-0.0592,-0.0425,-0.0509,-0.0691,-0.0079,0.0495,-0.0365,0.0351,-0.017,0.0122,0.0509,0.0346,0.038,0.0902,0.0292,0.0275,0.0488,0.0442,0.083,-0.1557,0.0454,0.0112,0.057,-0.0686,-0.018,0.0224,0.0354,-0.0177,0.0414,0.015,0.0085,0.0247,-0.035,0.0286,-0.0188,-0.0104,-0.0136,0.0121,0.0228,-0.0322,-0.0646,-0.0226,-0.0014,0.0092,-0.0931,0.0252,-0.0013,-0.0413,-0.0137,-0.0547,0.0428,-0.0622,-0.0109,0.0916,0.0503,-0.0313,0.1814,-0.0502,0.0114,0.0354,-0.0263,0.0163,-0.0549,0.0257,-0.0034,-0.037,-0.003,-0.0233,-0.0391,0.0528,0.0039,0.0195,0.0331,0.068,0.0017,-0.034,-0.0054,-0.0293,-0.0078,0.0579,-0.0516,-0.0082,-0.0339,0.0171,0.1396,-0.0047,0.0021,0.0089,0.005,-0.0531,-0.0074,-0.0268,-0.0217,0.0335,-0.0799,-0.0177,0.0185,-0.0155,-0.0771,0.023,-0.0873,-0.0059,0.1297,-0.0698,0.0413,-0.0489,0.011,0.0268,0.0103,-0.0521,-0.0425,-0.0172,0.0395,0.0009,0.0364,-0.0875,-0.0348,-0.0072,-0.0854,-0.0424,0.1408,0.0155,-0.0882,-0.0341,-0.0241,0.0456,-0.032,0.0331,0.0395,-0.0371,-0.0013,0.0872,0.0212,-0.0866,-0.0025,0.036,-0.0016,0.0394,-0.0616,-0.0113,0.0218,0.0408,-0.0311,0.0345,-0.0468,0.0251,0.0556,-0.0435,0.0306,-0.0668,0.0189,-0.0406,-0.0055,-0.0028,0.0079,-0.0014,-0.0013,0.0079,-0.0093,-0.0262,0.0607,0.0244,-0.0033,0.0027,0.0418,0.0176,0.0596,-0.0265,0.0253,0.0814,-0.0382,-0.0326,0.0065,0.0143,0.0333,0.0004,0.0812,0.046,-0.0329,-0.0383,-0.2431,0.0138,0.0121,-0.0563,0.0417,-0.0996,0.0037,-0.0201,0.063,0.0436,0.0611,-0.0021,-0.0307,0.0031,0.0091,0.0376,0.0345,0.0255,-0.0682,-0.0369,-0.0113,0.0367,-0.0253,-0.0556,0.051,0.0202,0.2291,0.0122,0.0214,0.0097,0.0266,0.0097,-0.0475,-0.0951,0.0428,-0.025,0.0356,-0.0004,-0.069,0.0026,-0.0164,-0.0412,0.0073,-0.0907,-0.0166,-0.0155,-0.0136,0.0504,-0.0359,0.0163,0.0175,-0.0395,-0.0014,-0.0206,0.0412,0.0145,-0.1108,0.0594,-0.0523,-0.0188,0.0255,-0.0613,-0.0473,-0.0498,0.0127,0.0296,-0.0436,-0.0488,-0.0002,-0.0156,0.0062,0.0765,0.0369,0.0318,0.0964,0.0009,0.019,-0.0149,-0.0423,-0.0238,0.0945,-0.011,0.0394,0.0475,0.0475,0.0953,0.0879,-0.0339,-0.0406,-0.0333,-0.0039,-0.0036,-0.06,-0.0397,-0.0275,-0.0206,-0.2776,0.031,0.032,0.0545,-0.0264,0.0463,-0.0151,-0.0214,-0.0066,-0.0304,-0.0071,0.0285,0.0485,-0.0335,0.0524,0.0135,0.0743,-0.0369,0.0461,-0.0744,-0.0115,0.0559,0.1615,-0.0114,0.0009,0.0325,-0.0412,0.0105,0.0194,-0.0104,0.0257,0.0382,0.0697,-0.0203,0.0362,0.1408,-0.0267,0.0571,-0.0039,-0.0136,0.0208,-0.0105,-0.0149,0.0151,0.0797,0.0012,-0.021,-0.0083,-0.0262,0.0372,-0.0005,0.021,-0.0131,-0.0146,0.0362,0.0705,-0.0179,-0.0631,-0.0302,-0.0193,0.0145,-0.0397,-0.015,0.0121,-0.0005]}
{"key":"[Variance-Reduced Splitting Schemes for Monotone Stochastic Generalized Equations] We consider monotone inclusion problems where the operators may be expectation-valued, a class of problems that subsumes convex stochastic optimization problems as well as subclasses of stochastic variational inequality and equilibrium problems. A direct application of splitting schemes is complicated by the need to resolve problems with expectation-valued maps at each step, a concern that is addressed by using sampling. Accordingly, we propose an avenue for addressing uncertainty in the mapping: Variance-reduced stochastic modified forward-backward splitting scheme (vr-SMFBS). In constrained settings, we consider structured settings when the map can be decomposed into an expectation-valued map A and a maximal monotone map B with a tractable resolvent. We show that the proposed schemes are equipped with a.s. convergence guarantees, linear (strongly monotone A) and O(1/k) (monotone A) rates of convergence while achieving optimal oracle complexity bounds. The rate statements in monotone regimes appear to be amongst the first and rely on leveraging the Fitzpatrick gap function for monotone inclusions. Furthermore, the schemes rely on weaker moment requirements on noise and allow for weakening unbiasedness requirements on oracles in strongly monotone regimes. Preliminary numerics on a class of two-stage stochastic variational inequality problems reflect these findings and show that the variance-reduced schemes outperform stochastic approximation schemes and sample-average approximation approaches. The benefits of attaining deterministic rates of convergence become even more salient when resolvent computation is expensive.","layer":4,"vector":[-0.0167,-0.0355,0.0764,-0.014,-0.0015,0.0348,0.0065,0.0306,0.06,0.0119,-0.0039,-0.0239,0.0682,0.091,0.0295,0.0461,-0.0126,-0.0074,-0.0408,0.0302,0.0166,-0.038,0.0067,-0.0348,0.0668,-0.0347,-0.0413,-0.0605,-0.0062,-0.2597,0.0312,-0.0233,0.0189,-0.092,0.0043,-0.0254,-0.0308,0.066,-0.0391,0.0553,0.0079,0.0135,-0.0325,-0.0645,-0.0357,-0.0727,-0.0381,-0.0167,-0.0379,-0.0479,0.0072,-0.0288,-0.0168,0.0004,-0.0062,0.049,0.0153,0.063,0.0272,0.0577,-0.0192,0.0488,-0.1378,0.0515,0.096,0.0435,0.0146,-0.0265,0.0216,0.0798,-0.0308,0.0395,0.0151,0.066,0.0545,0.0048,0.0385,-0.0441,-0.0448,0.0428,-0.0006,-0.0489,-0.0196,0.015,-0.0151,-0.0739,0.0152,-0.0532,0.0415,0.0227,0.0129,-0.0471,-0.0067,0.0464,-0.065,0.0278,-0.0168,0.028,0.0032,0.1985,-0.0185,0.098,0.0601,-0.0275,0.0278,-0.0315,-0.0593,-0.0447,-0.0126,0.0059,0.0006,-0.0382,0.0534,-0.0414,-0.0205,-0.0038,0.0197,-0.0231,-0.0237,-0.0136,-0.0645,-0.0142,0.0246,0.0094,0.0246,-0.0514,-0.0163,0.1526,0.0118,0.0293,0.028,-0.033,-0.0179,-0.047,0.0091,-0.0277,-0.0475,0.0144,0.0461,0.0141,-0.0238,-0.0878,-0.0045,-0.1235,-0.0588,0.1454,-0.0126,0.0631,-0.0462,-0.0445,0.0082,0.0012,-0.0158,-0.003,0.045,0.0374,0.018,0.0357,-0.06,0.0234,-0.017,-0.0752,0.0134,0.1334,-0.0284,-0.0576,-0.0397,0.0222,0.0223,0.0036,0.0096,-0.0009,0.0124,0.0126,0.0691,0.0506,-0.066,0.0023,0.017,-0.0123,-0.0185,-0.0269,-0.0495,0.0315,-0.0143,-0.0469,0.0283,-0.0169,0.0084,0.0128,-0.0556,-0.0207,-0.0264,-0.0192,-0.0207,-0.0138,-0.0232,-0.0354,0.0631,-0.0101,0.0292,-0.0081,-0.061,0.0632,-0.0045,0.0078,-0.0252,0.0124,0.015,0.0674,-0.0242,-0.0369,0.0922,-0.0067,-0.0159,0.0391,0.0709,0.0157,-0.0026,0.0284,0.0324,0.0162,-0.0691,-0.2342,-0.0089,-0.0156,0.0115,0.0283,-0.0165,-0.0032,-0.0353,0.052,0.0698,0.0325,-0.011,-0.0205,0.0635,0.0118,0.0225,0.007,-0.0107,-0.0219,0.041,0.0113,0.0013,-0.0498,-0.0819,0.0926,0.0115,0.1969,-0.0039,0.0417,-0.0135,0.0206,-0.002,-0.0186,-0.0351,0.0317,0.0215,0.0794,-0.0422,-0.0209,-0.058,0.0291,0.0434,-0.0141,-0.0831,-0.0107,-0.031,0.0021,0.0316,-0.0527,0.0174,0.053,-0.0115,0.0391,-0.0148,0.0335,-0.0079,-0.0907,0.0175,-0.0287,0.0024,-0.0294,-0.0675,-0.0172,-0.0038,0.0629,0.0298,0.0191,-0.0242,-0.0101,-0.0434,-0.0422,0.0396,-0.0288,0.032,0.0146,0.0444,0.0189,0.0119,-0.0591,-0.0235,0.054,-0.0541,0.0328,0.0463,0.0354,0.026,0.054,-0.023,-0.0503,-0.0144,-0.0176,0.0199,-0.0312,0.0014,-0.0055,-0.0357,-0.2605,0.0524,0.0026,-0.0292,-0.0758,-0.0073,0.0396,-0.0275,-0.0879,-0.0059,0.0269,0.0719,0.0175,0.0302,0.0351,0.0119,0.0622,-0.0621,0.0826,-0.083,0.0239,0.0346,0.2201,-0.0363,0.0078,0.0151,0.0007,-0.0122,0.0272,-0.039,0.0383,0.0047,0.0664,-0.077,0.0729,0.0829,-0.04,0.0625,0.0496,0.0097,-0.0443,-0.0483,-0.0163,-0.0032,0.0987,-0.0132,-0.0327,-0.0349,0.0478,0.0366,-0.0242,0.0447,0.0282,-0.0236,-0.0098,0.0048,-0.0743,-0.0681,-0.0056,-0.0193,0.0169,-0.0567,-0.0661,0.0257,-0.0044]}
{"key":"[To Federate or Not To Federate: Incentivizing Client Participation in Federated Learning] Federated learning (FL) facilitates collaboration between a group of clients who seek to train a common machine learning model without directly sharing their local data. Although there is an abundance of research on improving the speed, efficiency, and accuracy of federated training, most works implicitly assume that all clients are willing to participate in the FL framework. Due to data heterogeneity, however, the global model may not work well for some clients, and they may instead choose to use their own local model. Such disincentivization of clients can be problematic from the server's perspective because having more participating clients yields a better global model, and offers better privacy guarantees to the participating clients. In this paper, we propose an algorithm called IncFL that explicitly maximizes the fraction of clients who are incentivized to use the global model by dynamically adjusting the aggregation weights assigned to their updates. Our experiments show that IncFL increases the number of incentivized clients by 30-55% compared to standard federated training algorithms, and can also improve the generalization performance of the global model on unseen clients.","layer":2,"vector":[0.007,-0.0675,0.0115,-0.0169,0.0325,0.0001,0.0244,0.0014,0.0422,-0.0417,0.0351,-0.0485,-0.0036,0.0851,0.0012,0.0142,0.0109,0.0338,-0.0528,-0.0144,-0.0081,-0.0604,-0.0183,-0.0433,-0.0018,0.0538,-0.0493,-0.0443,-0.067,-0.1978,0.0417,-0.0686,0.0359,0.0056,0.0178,-0.0205,-0.009,0.0663,-0.0493,0.0506,0.0105,0.0161,-0.0487,-0.0215,-0.0358,-0.0503,-0.0269,-0.0172,-0.0467,-0.0077,0.0894,-0.0513,0.0042,0.0583,-0.0213,0.0786,0.0457,0.0444,0.0493,0.0051,-0.0105,0.058,-0.1482,0.0886,0.0177,0.0495,-0.0416,-0.006,-0.0053,0.0357,0.0208,0.0725,-0.0119,0.0225,0.0343,-0.0036,0.0052,-0.0212,-0.0002,0.0393,0.0027,-0.0377,-0.0386,0.0025,-0.0276,-0.0385,0.0196,-0.0532,0.0319,0.0102,-0.0639,0.015,-0.0065,0.0094,-0.0343,0.0002,0.0313,0.0451,-0.0851,0.1923,-0.0487,0.0619,0.0209,-0.0548,0.0227,-0.0387,-0.0344,-0.0303,-0.0055,0.0287,-0.0322,-0.0265,0.0161,-0.0588,-0.0078,0.0458,0.0637,0.0637,-0.0244,-0.0121,-0.0039,-0.0071,0.0696,-0.0103,0.0223,-0.0431,0.01,0.1454,0.0293,0.015,0.0209,-0.0349,-0.0432,-0.0473,0.0186,0.0723,0.0268,-0.0174,0.0051,0.0395,-0.0173,-0.0655,-0.0213,-0.0853,-0.0265,0.1565,0.0162,0.0625,-0.0906,-0.053,-0.0176,0.0123,-0.0257,-0.0229,0.0529,0.0486,0.0652,0.0502,-0.0656,-0.0229,-0.0307,-0.0365,0.0104,0.1083,0.033,-0.1164,-0.027,0.0268,0.006,-0.0442,0.0525,0.0362,-0.009,0.0438,0.0773,0.0512,-0.0537,-0.0061,0.0169,0.0042,0.0168,-0.029,-0.0406,0.0466,-0.0066,-0.029,0.0571,-0.0434,-0.0052,0.0248,-0.0589,0.0469,-0.0632,-0.0069,-0.0195,-0.0136,0.0035,-0.0229,0.0281,-0.059,-0.0127,0.025,0.0003,-0.0097,-0.0194,0.0157,0.0162,-0.0248,0.0378,-0.0251,0.0021,0.0126,0.0107,-0.0242,-0.0308,-0.0115,0.0083,0.0148,0.0008,0.0152,0.0443,0.0042,-0.0386,-0.1952,-0.0217,0.0401,-0.0212,0.0504,-0.0424,0.0675,-0.0114,-0.0108,0.0796,0.0829,-0.0394,-0.0643,0.0373,-0.0254,0.052,0.0285,0.0433,-0.0173,-0.0019,-0.0008,0.0313,0.0069,-0.0695,0.047,0.061,0.1899,0.0294,-0.0139,-0.0499,-0.0134,0.0272,-0.0271,-0.1313,0.0303,-0.0083,0.0452,-0.018,-0.0212,-0.0321,0.0144,0.0021,0.0284,-0.1695,-0.0265,-0.0489,-0.056,0.0222,-0.0944,0.0292,0.0083,-0.0259,0.0503,0.0188,-0.0191,-0.0447,-0.0722,0.0553,-0.035,0.083,0.0309,-0.0512,-0.0112,-0.07,0.0595,-0.0511,-0.0343,-0.0266,0.0759,-0.0237,-0.0157,0.0655,0.014,-0.008,0.0295,0.0506,0.0433,-0.041,-0.1132,-0.0133,0.0886,-0.0425,0.0442,0.0249,-0.0078,0.0186,0.0534,0.0571,0.0125,-0.0477,0.0106,0.0287,-0.0482,-0.0059,0.0559,0.0018,-0.3185,0.0106,0.0036,0.0243,-0.049,0.0071,0.0708,0.0361,-0.0249,-0.0033,0.0576,0.0591,0.0288,0.0165,0.0332,0.0398,0.0467,-0.022,0.0628,-0.0838,0.0145,0.0419,0.218,-0.0293,0.0153,0.0368,-0.0229,0.0173,0.0565,-0.0315,-0.0305,-0.0315,0.0671,-0.0341,0.0199,0.0446,-0.0096,0.0014,0.0046,-0.038,-0.0109,-0.035,0.009,-0.0074,0.0526,0.0207,-0.027,-0.0391,-0.0244,0.0265,-0.0061,-0.0145,-0.0161,0.0006,0.0362,-0.01,-0.0373,-0.0139,-0.0453,-0.0227,-0.0029,-0.0612,-0.0197,0.0008,-0.0106]}
{"key":"[Human Parity on CommonsenseQA: Augmenting Self-Attention with External Attention] Most of today's AI systems focus on using self-attention mechanisms and transformer architectures on large amounts of diverse data to achieve impressive performance gains. In this paper, we propose to augment the transformer architecture with an external attention mechanism to bring external knowledge and context to bear. By integrating external information into the prediction process, we hope to reduce the need for ever-larger models and increase the democratization of AI systems. We find that the proposed external attention mechanism can significantly improve the performance of existing AI systems, allowing practitioners to easily customize foundation AI models to many diverse downstream applications. In particular, we focus on the task of Commonsense Reasoning, demonstrating that the proposed external attention mechanism can augment existing transformer models and significantly improve the model's reasoning capabilities. The proposed system, Knowledgeable External Attention for commonsense Reasoning (KEAR), reaches human parity on the open CommonsenseQA research benchmark with an accuracy of 89.4\\% in comparison to the human accuracy of 88.9\\%.","layer":1,"vector":[-0.0579,0.0064,0.0203,-0.062,0.0237,-0.0097,0.0452,0.0603,0.034,-0.0055,-0.0261,-0.0668,0.0358,0.0626,0.0291,0.0385,-0.0345,0.0452,-0.0185,-0.0152,0.0297,-0.0593,-0.0267,-0.046,-0.0356,0.0259,-0.0639,-0.0184,-0.073,-0.2208,0.0058,-0.0916,0.0452,0.0118,0.0085,-0.0463,-0.0187,0.0353,0.0032,0.0262,0.0018,0.0302,0.0248,0.0013,-0.0467,-0.069,-0.0068,-0.0023,-0.057,-0.0456,0.0159,-0.037,0.0008,0.0395,0.0638,0.035,0.0309,0.0589,0.0529,-0.018,0.0485,0.0657,-0.1437,0.0949,0.051,0.0438,-0.0408,-0.0284,-0.0152,0.0226,0.0075,0.0003,0.0269,0.0409,0.0273,-0.0142,-0.042,-0.0238,0.0231,-0.012,-0.0067,-0.0168,-0.0473,-0.0028,-0.0136,-0.0052,-0.0177,-0.0307,0.0526,-0.0211,-0.0563,0.0143,-0.0379,0.0178,-0.0103,0.0078,0.019,0.0044,-0.0757,0.1982,-0.0767,0.0125,0.0299,-0.0633,0.0429,-0.0225,-0.044,-0.052,-0.0265,-0.0058,-0.0111,-0.0213,0.0575,-0.0144,0.0354,0.0068,0.1064,0.019,-0.0267,-0.0224,0.0125,0.022,0.0019,-0.0118,0.0398,-0.0768,0.0558,0.1444,-0.0007,0.0174,0.0546,-0.0502,-0.0429,-0.0404,0.0232,0.022,0.0243,-0.0283,0.0075,-0.0274,-0.0224,-0.0066,-0.0452,-0.0779,-0.0681,0.101,0.0186,0.052,-0.0447,0.0162,-0.0473,0.0193,0.0054,-0.0617,0.03,-0.0036,0.0394,0.0199,-0.0539,0.0006,0.0201,-0.0265,-0.0621,0.0999,-0.0014,-0.1016,-0.0393,-0.0211,0.0315,0.0028,0.0776,0.0153,-0.0369,0.0478,0.1064,0.0448,-0.0547,0.0473,-0.0192,0.0261,0.0501,-0.045,-0.0199,0.0505,0.0094,-0.0764,0.0382,-0.0715,0.0295,0.064,-0.0374,0.0561,-0.0028,0.0146,-0.0641,-0.0159,0.0084,-0.0037,0.0026,-0.0186,-0.0479,0.0351,-0.0289,0.0208,0.0116,0.0249,-0.003,0.0001,0.0171,0.0007,-0.0439,0.0307,0.029,-0.028,-0.0116,-0.0211,0.0129,0.0142,-0.0097,0.053,0.0468,-0.0411,-0.0384,-0.2245,-0.024,0.0178,-0.0461,0.0482,-0.0737,0.0432,0.0197,0.0282,0.079,0.066,-0.047,-0.0044,0.0455,-0.0033,0.025,0.0089,0.0399,-0.0478,0.0176,-0.0008,0.018,0.0159,-0.087,0.0139,0.0148,0.2371,0.0437,0.041,-0.0343,0.0212,0.0623,-0.0569,-0.1569,0.0329,0.0065,0.0386,-0.0304,-0.0424,0.012,0.0127,0.0191,-0.0202,-0.1057,-0.0408,0.0097,0.003,0.0683,-0.0452,0.0596,0.0572,-0.083,0.0013,0.0002,0.0119,-0.0983,-0.0646,-0.0086,-0.035,0.0387,-0.0101,-0.04,0.012,-0.0524,0.0419,-0.0192,-0.0394,-0.0169,0.0068,-0.0027,-0.0589,0.0857,0.0301,0.0119,0.0393,0.0012,0.0422,-0.0228,-0.0346,-0.0064,0.0473,-0.034,0.0173,-0.013,0.0492,0.0089,0.0634,-0.0054,0.0611,-0.0247,0.026,0.0273,-0.0032,-0.0036,0.0332,-0.0095,-0.328,0.0465,0.0161,0.0109,-0.0196,0.0217,0.0752,0.0345,-0.0142,0.0026,-0.0614,0.0439,0.019,0.0109,-0.0299,0.0361,0.0524,-0.0253,0.0109,-0.054,0.015,0.0326,0.217,-0.032,0.0166,0.0366,-0.0261,-0.0228,0.0453,0.0011,0.0256,-0.0029,0.032,-0.0574,0.0165,0.0638,-0.0386,0.0159,0.0339,-0.016,-0.0033,0.0075,-0.0434,-0.0066,0.1018,0.0273,-0.0041,-0.0438,-0.0245,-0.0094,-0.0154,-0.0022,-0.0401,0.0123,0.0292,0.0169,-0.0176,0.0165,-0.0475,-0.0299,0.0133,-0.0563,0.0166,0.0297,0.0297]}
{"key":"[SEREN: Knowing When to Explore and When to Exploit] Efficient reinforcement learning (RL) involves a trade-off between \"exploitative\" actions that maximise expected reward and \"explorative'\" ones that sample unvisited states. To encourage exploration, recent approaches proposed adding stochasticity to actions, separating exploration and exploitation phases, or equating reduction in uncertainty with reward. However, these techniques do not necessarily offer entirely systematic approaches making this trade-off. Here we introduce SElective Reinforcement Exploration Network (SEREN) that poses the exploration-exploitation trade-off as a game between an RL agent -- \\exploiter, which purely exploits known rewards, and another RL agent -- \\switcher, which chooses at which states to activate a pure exploration policy that is trained to minimise system uncertainty and override Exploiter. Using a form of policies known as impulse control, \\switcher is able to determine the best set of states to switch to the exploration policy while Exploiter is free to execute its actions everywhere else. We prove that SEREN converges quickly and induces a natural schedule towards pure exploitation. Through extensive empirical studies in both discrete (MiniGrid) and continuous (MuJoCo) control benchmarks, we show that SEREN can be readily combined with existing RL algorithms to yield significant improvement in performance relative to state-of-the-art algorithms.","layer":0,"vector":[-0.0907,0.0074,0.0298,-0.025,0.0014,0.0139,0.0363,0.0454,0.0348,0.0008,0.0122,-0.0506,0.0343,0.0957,0.005,0.0023,-0.0546,0.0342,-0.0028,0.0009,0.0356,-0.047,0.0001,-0.0936,0.0278,0.0348,-0.041,-0.0445,-0.0516,-0.224,0.0091,-0.0652,0.0206,-0.0134,-0.0254,-0.0295,-0.0516,0.0654,-0.0201,0.0388,0.0597,0.0341,-0.0135,-0.0753,-0.0233,-0.0683,0.0049,-0.0397,0.0023,-0.047,0.0281,-0.0099,0.0401,-0.0136,0.0327,0.0379,0.0402,0.0738,0.0288,0.0049,0.0124,0.0451,-0.1431,0.0376,0.051,0.037,-0.0419,-0.0419,0.0356,0.0548,-0.0108,0.0725,-0.0227,0.0631,0.0459,0.0031,-0.0386,-0.0552,-0.0035,-0.0009,0.0455,-0.0795,-0.021,0.0109,-0.0295,-0.0769,-0.002,-0.0388,0.0847,0.0207,-0.0243,0.0195,-0.0238,0.0238,-0.0176,0.0335,0.0307,0.0285,-0.0726,0.2034,-0.0038,0.0366,-0.0098,0.0243,0.0432,-0.0346,-0.0315,0.0294,0.016,-0.0033,-0.0069,-0.0297,0.0314,-0.0144,-0.0069,0.0268,0.0091,0.0279,0.0153,-0.0018,-0.0477,-0.0053,0.0799,0.0254,0.021,-0.0666,0.0405,0.1362,0.0035,-0.015,0.0344,-0.0782,0.0157,-0.0247,0.0354,0.01,0.0133,0.0112,0.0318,0.0163,-0.0513,0.0118,0.0286,-0.1359,-0.0661,0.0627,0.0584,0.0174,0.0098,-0.0351,-0.0377,0.008,0.0266,-0.05,-0.0275,0.0482,0.0576,0.0258,-0.0505,0.0022,-0.0418,-0.0378,-0.043,0.1043,-0.0149,-0.0593,-0.0396,-0.0086,-0.0176,-0.028,-0.0205,0.024,-0.0693,0.0503,0.0845,0.0107,-0.0909,-0.001,-0.0122,-0.0022,0.0395,-0.0575,0.011,0.0251,0.0432,-0.0391,0.0236,-0.0241,-0.0082,0.0055,-0.0293,0.0257,0.0154,-0.0018,-0.0471,-0.0538,0.0018,-0.0227,-0.0021,0.0182,0.0295,-0.0423,-0.0501,-0.0204,0.0025,0.0024,-0.0193,-0.0054,0.0626,-0.0039,-0.0418,0.0486,0.0602,-0.0215,-0.0701,-0.0172,0.0298,0.0314,-0.0238,0.0355,0.0334,-0.0013,-0.0138,-0.2456,-0.0253,0.0013,-0.0217,0.0596,-0.0564,0.0267,-0.0781,0.0226,0.0295,0.0344,-0.0582,-0.0495,0.0671,0.0198,0.0542,0.0213,-0.0242,0.028,0.0061,0.0331,0.0019,-0.01,-0.0844,0.0309,0.0209,0.2338,0.0433,0.0264,-0.0226,0.0314,0.0183,-0.051,-0.0987,0.0187,0.0496,0.1149,-0.0348,-0.0335,-0.0466,0.0052,0.0153,0.0011,-0.1411,-0.012,-0.043,-0.0328,0.0397,-0.0303,-0.0499,0.0544,-0.032,0.046,-0.0244,-0.0191,-0.0276,-0.0523,0.035,-0.0407,0.0464,0.0038,-0.0231,0.0038,-0.0549,0.0549,-0.0053,0.0077,-0.0506,0.0374,-0.0159,-0.0295,0.0376,0.0008,0.0041,-0.0304,0.0116,0.006,-0.0102,-0.0386,-0.0154,0.0505,-0.0421,0.0244,0.0294,0.013,-0.0282,0.063,-0.0224,0.0324,-0.0084,-0.0255,0.0303,-0.0571,-0.0447,0.0535,-0.0125,-0.288,0.0566,0.0456,0.0184,-0.0229,-0.0079,0.0418,-0.0148,-0.0097,0.0009,0.0216,0.0564,0.0436,0.0318,0.021,0.0348,0.1145,-0.0172,0.0433,-0.0914,0.0244,0.0344,0.2239,-0.0185,0.0535,0.0004,-0.0365,-0.0007,0.0224,-0.0033,0.0195,0.0136,0.0643,-0.0735,0.0619,0.1034,-0.0053,0.0361,0.0113,-0.0113,-0.0533,0.0025,0.0169,-0.0102,0.0995,-0.0273,-0.045,-0.0356,-0.0303,0.0399,-0.0156,0.0295,-0.0443,-0.0036,0.0396,0.0464,-0.0524,-0.0338,-0.0164,-0.0481,0.0382,-0.0407,0.0405,-0.0057,-0.0148]}
{"key":"[Autonomous exploration for navigating in non-stationary CMPs] We consider a setting in which the objective is to learn to navigate in a controlled Markov process (CMP) where transition probabilities may abruptly change. For this setting, we propose a performance measure called exploration steps which counts the time steps at which the learner lacks sufficient knowledge to navigate its environment efficiently. We devise a learning meta-algorithm, MNM and prove an upper bound on the exploration steps in terms of the number of changes.","layer":3,"vector":[-0.0744,-0.0283,0.0463,-0.0138,-0.0317,0.0462,0.0412,0.0273,0.0141,0.0167,0.0545,-0.0344,0.0425,0.0793,-0.0115,0.0131,-0.0682,0.0697,0.0028,0.0037,0.0391,-0.0333,-0.0003,-0.0475,-0.0192,0.0719,-0.026,-0.0561,-0.0494,-0.209,0.0093,-0.0447,0.0365,-0.0235,-0.0074,-0.0115,-0.0381,0.0657,0.0098,0.0273,0.0566,0.0232,-0.0095,-0.0537,-0.0297,-0.0514,0.0181,-0.0499,0.0041,-0.0418,-0.0466,-0.0142,0.0263,-0.0192,0.0562,0.0378,0.0568,0.0302,0.0537,0.0307,-0.0168,0.0205,-0.1832,0.0643,0.0481,0.036,-0.023,-0.0481,0.0379,0.038,-0.0337,0.0395,-0.013,0.0512,0.0301,-0.0323,-0.0066,-0.0219,0.0256,0.0016,0.0088,-0.0336,-0.0181,-0.0108,-0.0392,-0.0394,0.0127,-0.0538,0.0469,0.021,0.0084,0.0054,-0.0268,0.0348,-0.0395,0.0011,0.0681,0.0448,-0.0064,0.1705,-0.0069,0.0509,0.0152,-0.0068,0.0467,-0.0415,0.0056,0.0056,-0.0022,-0.0065,-0.0053,-0.0444,0.048,-0.0609,-0.0321,0.0352,0.0475,0.0369,0.0007,0.0142,-0.0214,0.0026,0.0665,-0.0339,0.0205,-0.0764,0.0333,0.1792,0.0115,0.0025,0.0601,-0.0686,-0.0177,0.025,0.0334,0.0172,-0.0101,-0.0291,0.0092,-0.016,-0.0608,-0.0216,0.015,-0.1147,-0.0537,0.1347,0.0049,0.0313,-0.0365,-0.0558,-0.0149,0.0075,0.01,-0.0367,-0.0112,0.0347,0.06,0.0335,-0.0434,0.0244,-0.0735,-0.0538,-0.0192,0.1223,0.0071,-0.0499,-0.0261,-0.0221,0.0068,-0.0173,0.0187,0.0491,-0.051,0.027,0.0884,-0.0083,-0.0922,0.0152,-0.0011,-0.0057,0.0177,-0.0268,0.0102,0.0366,0.0673,-0.0259,0.0312,-0.0292,0.0345,0.0213,-0.0143,0.0247,0.0186,-0.0048,-0.0842,-0.0427,0.0101,-0.0152,0.0441,-0.0372,-0.0034,-0.0404,-0.0489,0.0023,0.0003,0.0245,-0.0392,0.0139,0.0474,0.0082,-0.0486,-0.0193,0.0371,-0.0201,-0.0194,-0.0218,-0.0115,0.0298,0.0057,0.0349,0.0094,-0.0037,-0.0326,-0.2198,-0.0242,0.0285,0.0344,0.0471,-0.0475,0.0007,-0.0377,0.0603,0.0151,0.0403,-0.0727,-0.0402,0.0302,-0.0157,0.0302,0.0239,0.0457,-0.0181,0.0313,0.0018,-0.0094,-0.0307,-0.0936,0.0229,-0.0143,0.2325,0.0401,0.044,-0.0559,0.0178,0.0308,-0.0398,-0.1076,0.0248,0.0248,0.0633,-0.0198,-0.0138,-0.063,-0.0111,0.0371,-0.0198,-0.0822,-0.0412,-0.0399,0.0013,0.0329,-0.0079,-0.0271,-0.0002,-0.0303,0.0122,-0.0442,-0.0661,-0.0482,-0.037,0.0162,-0.0617,0.0511,0.0112,-0.0155,0.0331,-0.0377,0.0685,-0.0226,0.0268,-0.053,0.0399,-0.0395,-0.0125,0.0593,0.0011,0.004,0.0002,-0.0037,0.0018,-0.0384,-0.0587,-0.0611,0.0422,-0.0408,0.0442,0.0225,0.0124,-0.025,0.1016,-0.0464,-0.0051,-0.0376,0.037,0.0104,-0.056,-0.0079,0.0697,-0.0084,-0.2953,0.0726,0.0211,0.0354,-0.0337,0.0171,0.036,0.0128,-0.0778,0.0166,-0.0033,0.051,0.0515,0.0132,0.0455,0.0317,0.0744,-0.0162,0.0324,-0.0994,0.028,0.06,0.2214,-0.0176,0.059,0.0148,-0.0623,0.0217,0.0476,-0.0077,0.001,-0.0053,0.0809,-0.0309,0.101,0.0665,-0.0361,0.0683,-0.0052,-0.0245,-0.0267,-0.0029,0.0144,-0.0048,0.1133,-0.0099,-0.0078,-0.0478,-0.0328,0.0499,-0.0345,0.0291,-0.0194,0.0085,0.0105,0.0215,-0.0527,-0.0693,-0.0451,-0.0788,0.0323,-0.0783,0.04,-0.0265,0.0112]}
{"key":"[Deep Gaussian Markov Random Fields] Gaussian Markov random fields (GMRFs) are probabilistic graphical models widely used in spatial statistics and related fields to model dependencies over spatial structures. We establish a formal connection between GMRFs and convolutional neural networks (CNNs). Common GMRFs are special cases of a generative model where the inverse mapping from data to latent variables is given by a 1-layer linear CNN. This connection allows us to generalize GMRFs to multi-layer CNN architectures, effectively increasing the order of the corresponding GMRF in a way which has favorable computational scaling. We describe how well-established tools, such as autodiff and variational inference, can be used for simple and efficient inference and learning of the deep GMRF. We demonstrate the flexibility of the proposed model and show that it outperforms the state-of-the-art on a dataset of satellite temperatures, in terms of prediction and predictive uncertainty.","layer":5,"vector":[-0.0312,-0.0034,0.0222,-0.0514,0.0698,0.0374,0.0272,-0.011,0.0122,-0.0159,0.0087,-0.0858,0.0541,0.0963,0.0205,0.0245,-0.0305,0.053,-0.0345,-0.0178,0.0485,-0.0511,0.0123,-0.0392,0.0386,0.0172,-0.018,-0.0208,-0.0599,-0.2366,0.0072,-0.0357,0.0372,-0.0293,0.0065,-0.0568,-0.0351,0.0431,-0.0081,0.0364,0.0525,-0.0266,-0.0347,-0.0697,0.0055,-0.0377,-0.0474,-0.0224,-0.0313,-0.0257,0.007,-0.0196,0.0032,0.0549,0.046,0.0433,0.0709,0.0443,0.0307,0.0141,0.0112,0.048,-0.1761,0.0759,0.0405,0.0325,-0.0432,-0.0179,-0.0026,-0.0082,-0.0265,0.0679,-0.0072,0.0605,0.0307,-0.0263,-0.0302,-0.002,-0.0194,-0.0021,0.0181,-0.01,-0.0288,-0.0083,0.0048,-0.0492,0.0133,-0.0157,0.0124,0.0126,-0.0331,-0.0217,-0.0266,0.0298,-0.0736,0.0687,0.0206,0.005,-0.0105,0.1894,-0.0454,0.0104,0.018,0.0202,0.0604,0.0021,-0.0005,-0.0229,-0.0181,-0.008,-0.0005,-0.065,0.0059,-0.0303,0.0014,-0.0198,0.0609,0.0427,-0.0256,-0.0299,-0.059,-0.007,0.0416,-0.0128,0.0142,-0.0752,-0.0264,0.1551,0.0803,0.0249,0.0533,-0.0272,-0.0535,-0.0366,0.0567,-0.002,0.0339,-0.0345,0.0108,-0.007,-0.0416,-0.0364,0.0049,-0.0706,-0.0341,0.1074,-0.0712,0.0045,-0.0777,-0.0206,-0.0068,0.0003,-0.0302,-0.0052,0.0499,0.033,-0.0364,0.022,-0.04,0.0025,-0.0135,-0.0603,-0.04,0.1206,0.0225,-0.0761,-0.0522,0.0153,-0.0031,0.0155,0.0603,0.0348,-0.0199,0.0007,0.0544,0.034,-0.0654,0.0494,0.0409,0.012,-0.0172,-0.0172,0.0138,0.0473,0.0322,-0.0726,-0.0156,-0.027,-0.0046,0.0244,-0.0202,0.031,-0.0105,-0.0245,-0.0247,-0.0109,-0.0319,-0.0211,0.0483,-0.058,0.0062,-0.0088,-0.0706,0.0197,-0.0084,-0.0093,0.001,0.0418,0.0224,0.0084,-0.0001,-0.0187,0.0662,-0.0276,-0.0044,0.0057,0.0094,0.0087,-0.0261,0.0036,0.0299,-0.0834,-0.075,-0.241,0.0016,0.0084,-0.0552,0.0617,-0.0754,0.0373,-0.0264,0.0732,0.0546,0.0663,-0.0608,-0.0052,-0.0292,-0.0067,0.0366,-0.013,0.0758,-0.0201,0.016,0.0214,-0.0006,-0.0271,-0.0945,0.0775,0.0096,0.2324,0.0116,0.0452,-0.0285,0.0417,0.0165,-0.0102,-0.0718,0.0518,0.0236,0.0744,0.0239,-0.0493,-0.0516,-0.0183,0.032,0.0092,-0.0935,-0.0777,-0.0419,-0.0204,0.0181,-0.0583,-0.0013,-0.0116,-0.0592,0.0338,-0.0414,0.0135,-0.0361,-0.0919,0.0385,-0.0239,0.0059,0.0523,-0.0135,-0.0117,-0.0353,0.0323,-0.023,-0.0253,-0.0391,0.0309,-0.0139,0.0063,0.0792,-0.017,0.0124,0.0672,-0.027,0.0607,-0.002,-0.0553,-0.0452,0.0791,-0.0058,0.023,0.0363,0.057,0.0173,0.0528,-0.0161,0.0598,-0.0363,0.0083,-0.0014,-0.0104,0.0233,0.0355,-0.0129,-0.2869,0.0457,0.0101,0.027,-0.0197,-0.0061,0.0461,0.0639,-0.0357,-0.0036,-0.0299,0.0393,0.0976,-0.0059,0.0021,0.0128,0.054,-0.0552,0.0862,-0.0448,0.0072,0.0296,0.2124,-0.0148,0.0679,0.0293,0.0005,0.0212,0.0517,0.017,0.0283,0.0265,0.072,-0.0593,0.1028,0.0898,0.0244,0.0654,0.001,-0.0229,-0.0104,-0.0076,-0.0222,-0.0113,0.0769,-0.0282,0.0043,-0.0256,-0.0284,0.0419,-0.0104,-0.0124,-0.0268,-0.0498,-0.0145,0.0201,-0.0235,-0.0596,-0.0185,-0.021,0.0176,-0.0887,-0.013,-0.0562,-0.0367]}
{"key":"[Boosting EfficientNets Ensemble Performance via Pseudo-Labels and Synthetic Images by pix2pixHD for Infection and Ischaemia Classification in Diabetic Foot Ulcers] Diabetic foot ulcers are a common manifestation of lesions on the diabetic foot, a syndrome acquired as a long-term complication of diabetes mellitus. Accompanying neuropathy and vascular damage promote acquisition of pressure injuries and tissue death due to ischaemia. Affected areas are prone to infections, hindering the healing progress. The research at hand investigates an approach on classification of infection and ischaemia, conducted as part of the Diabetic Foot Ulcer Challenge (DFUC) 2021. Different models of the EfficientNet family are utilized in ensembles. An extension strategy for the training data is applied, involving pseudo-labeling for unlabeled images, and extensive generation of synthetic images via pix2pixHD to cope with severe class imbalances. The resulting extended training dataset features $8.68$ times the size of the baseline and shows a real to synthetic image ratio of $1:3$. Performances of models and ensembles trained on the baseline and extended training dataset are compared. Synthetic images featured a broad qualitative variety. Results show that models trained on the extended training dataset as well as their ensemble benefit from the large extension. F1-Scores for rare classes receive outstanding boosts, while those for common classes are either not harmed or boosted moderately. A critical discussion concretizes benefits and identifies limitations, suggesting improvements. The work concludes that classification performance of individual models as well as that of ensembles can be boosted utilizing synthetic images. Especially performance for rare classes benefits notably.","layer":10,"vector":[-0.052,0.0067,0.052,0.0109,0.0755,0.0175,0.0124,0.0406,-0.0241,-0.0047,0.02,-0.0608,0.0158,0.0571,-0.0206,0.0037,0.0377,0.0507,-0.0481,0.0116,0.0063,-0.0228,-0.0288,-0.0266,0.0221,0.0204,0.0095,-0.0224,-0.0655,-0.2342,-0.0318,-0.04,0.0233,-0.0213,0.0241,-0.0183,-0.0466,0.0548,-0.0387,0.043,-0.0073,0.0136,-0.0216,-0.0466,0.0085,-0.0808,-0.0512,-0.0049,-0.0179,-0.0223,0.019,-0.0066,0.0432,0.0502,-0.0031,0.0049,0.0299,0.0267,0.0415,0.0209,0.0676,0.0476,-0.1431,0.0556,0.0307,0.0037,-0.0344,-0.0282,0.0334,0.0267,-0.0221,0.0539,0.0372,0.0322,-0.0085,-0.0057,0.0127,-0.018,-0.0305,0.0337,0.0205,-0.0357,-0.0343,-0.021,-0.0066,-0.0634,0.0384,-0.0412,0.0356,0.008,-0.0473,0.0023,-0.0461,0.0319,-0.02,-0.0295,0.0289,-0.0011,-0.0433,0.217,-0.0693,-0.0199,0.0666,-0.0208,0.0332,-0.0845,0.0214,-0.0169,-0.0654,-0.0351,0.0074,-0.006,-0.0072,-0.0111,-0.0005,0.0149,0.0746,0.0718,0.0063,0.0388,-0.0105,-0.0233,0.0411,-0.0329,-0.009,-0.0469,0.0328,0.1564,0.0513,0.033,0.0338,-0.0221,-0.0564,-0.0177,0.024,0.0234,-0.0056,0.0339,-0.0276,-0.0105,-0.0098,-0.0321,0.0095,-0.0682,-0.0341,0.1405,-0.0776,0.0741,-0.0554,-0.0348,0.0148,-0.003,-0.048,-0.03,0.0011,0.0244,0.0369,0.0193,-0.0106,-0.0001,-0.0026,-0.0614,-0.0027,0.0851,0.0344,-0.0414,-0.0055,-0.0336,0.0023,-0.0033,0.0307,0.006,0.0029,0.0743,0.0344,0.0308,-0.0801,-0.0585,-0.0013,-0.0164,0.0328,-0.0272,-0.0419,0.028,0.0229,-0.0165,-0.0357,-0.0395,0.0221,0.0612,-0.0256,0.0135,-0.0509,0.0082,0.0052,0.0026,-0.0299,0.0067,-0.0015,-0.0552,0.0135,0.0224,-0.015,-0.0345,0.0035,0.0171,-0.0094,0.0068,0.0234,0.0157,-0.0523,0.0331,0.0545,-0.0302,-0.0381,-0.016,0.0161,0.0615,-0.0121,0.0753,0.0505,-0.0212,-0.0851,-0.2276,0.0225,0.0543,-0.0439,0.0505,-0.0522,0.026,0.0099,0.0361,0.1045,0.0489,0.0063,-0.0272,0.0265,-0.0563,0.0471,0.0233,-0.0158,-0.0258,0.0031,0.0156,0.0334,0.0548,-0.0876,0.0648,-0.0417,0.2187,0.038,0.0255,-0.0822,0.0063,0.0741,-0.0845,-0.1015,0.065,-0.0052,0.0567,-0.0296,-0.0687,-0.0056,-0.0352,0.0099,0.0312,-0.1049,-0.0029,0.0234,-0.0762,0.0315,-0.0577,0.0349,0.0752,-0.065,0.0332,-0.01,-0.0012,-0.027,-0.1229,0.044,-0.0624,-0.0052,0.0191,-0.0616,0.057,-0.0702,0.0937,-0.0079,-0.0611,-0.046,0.0118,-0.0128,-0.0329,0.0813,0.0252,-0.0268,0.0742,0.0265,0.0536,-0.0271,-0.0425,-0.0427,0.0689,-0.0289,0.036,0.0281,0.0635,0.0208,0.0754,0.0201,0.0274,-0.0186,0.038,0.0295,-0.0551,-0.0083,0.036,-0.0421,-0.2899,0.0348,-0.0084,0.0202,-0.018,0.0075,0.046,0.0339,-0.017,-0.0072,0.0056,0.0321,0.0697,-0.0357,-0.0195,0.0448,0.0538,-0.088,0.0772,-0.0803,-0.0068,-0.011,0.2023,-0.0492,0.0096,0.0421,-0.0317,-0.0047,-0.0057,-0.0615,0.0517,-0.0167,0.0346,-0.055,0.0717,0.0838,-0.013,-0.0021,-0.0232,-0.0419,0.0109,0.0173,-0.0182,0.0144,0.0674,-0.004,-0.0067,-0.0365,0.0164,0.0264,-0.0027,0.0187,-0.0231,0.0281,0.0319,0.0095,-0.0485,-0.0526,-0.0171,-0.0134,0.0228,-0.0265,-0.0385,-0.0097,-0.0294]}
{"key":"[Memory via Temporal Delays in weightless Spiking Neural Network] A common view in the neuroscience community is that memory is encoded in the connection strength between neurons. This perception led artificial neural network models to focus on connection weights as the key variables to modulate learning. In this paper, we present a prototype for weightless spiking neural networks that can perform a simple classification task. The memory in this network is stored in the timing between neurons, rather than the strength of the connection, and is trained using a Hebbian Spike Timing Dependent Plasticity (STDP), which modulates the delays of the connection.","layer":4,"vector":[-0.0293,0.0231,0.0755,-0.0578,-0.0012,0.0472,0.0597,0.0207,0.0339,-0.0268,0.0047,-0.0305,0.0682,0.0909,-0.021,0.0115,-0.036,0.0485,-0.0114,-0.001,0.0262,-0.0586,-0.0335,0.0161,0.0009,-0.0138,-0.0081,-0.0025,-0.0476,-0.2344,0.0282,-0.0523,0.0432,-0.0351,-0.0016,-0.054,-0.022,0.0515,-0.0157,0.0294,0.0113,0.026,0.0088,-0.0896,-0.0257,-0.0497,0.017,-0.0355,0.0168,-0.0634,0.038,-0.0117,0.0602,-0.003,-0.0119,0.064,0.0199,0.037,0.0356,0.0043,-0.001,0.0692,-0.1599,0.0413,0.0553,0.0043,-0.0676,-0.0317,0.027,0.0312,-0.0462,0.0546,0.0159,0.034,0.0288,0.0053,-0.0148,-0.044,0.0295,0.0457,0.0118,-0.0148,-0.0679,-0.0433,-0.0148,-0.0326,0.0072,-0.0513,-0.0304,0.031,-0.0172,0.0021,-0.0576,0.0042,-0.0813,-0.0187,0.0139,0.0074,-0.0769,0.2102,-0.061,0.0469,0.0434,-0.0131,0.0333,-0.0352,-0.0271,-0.0482,-0.044,0.0163,0.0129,-0.055,0.016,-0.0351,0.051,0.0285,0.0489,0.0537,0.0201,0.0151,-0.0299,0.0037,0.0408,-0.0278,0.0303,-0.0648,-0.01,0.1377,0.0419,0.0193,0.001,-0.0393,-0.0655,-0.0025,0.0099,0.0175,0.016,-0.043,0.0136,-0.0742,-0.0269,-0.0046,0.0181,-0.0826,-0.0619,0.0664,0.0006,0.0513,-0.0588,0.0472,-0.0412,-0.0253,-0.0113,-0.0935,0.0293,0.043,0.0287,0.0294,-0.0344,0.0253,-0.0685,-0.0576,0.0134,0.058,0.0255,-0.0479,-0.0219,0.0154,-0.0081,-0.0482,0.0349,0.0164,-0.017,0.0045,0.0759,0.0126,-0.047,-0.0435,0.0123,0.022,0.0237,-0.0609,0.0033,0.0429,0.0483,-0.0294,0.0117,-0.043,0.0077,0.0568,-0.0286,-0.006,-0.016,-0.0389,-0.0298,-0.0288,-0.0086,-0.0177,0.0281,-0.0171,0.0141,-0.0009,-0.014,0.0353,0.0264,0.0131,-0.0429,0.0209,0.033,0.0307,-0.0295,0.0002,0.0834,-0.0465,-0.0272,-0.0178,-0.0183,0.0544,-0.0193,0.0141,0.0487,-0.0357,-0.0639,-0.2207,0.0127,-0.0031,-0.0434,0.0894,-0.0742,0.0243,0.0189,0.0243,0.0155,0.0416,0.0166,-0.0097,0.0071,-0.0407,0.0585,0.0526,-0.0113,0.0026,0.0142,-0.0271,0.0326,-0.0034,-0.0555,0.0618,-0.006,0.2186,0.0305,0.062,-0.0544,-0.0069,0.0312,-0.0396,-0.0798,0.0376,0.0145,0.054,-0.0097,0.0009,-0.0386,-0.0627,0.0363,0.0266,-0.1089,-0.0197,-0.0403,0.027,0.0154,-0.0398,-0.0047,0.0343,-0.0498,0.018,0.027,0.037,-0.0506,-0.0851,0.0565,-0.0809,0.0536,-0.0046,-0.0576,0.0225,-0.0293,0.0683,0.0244,-0.0494,-0.0612,0.0173,-0.01,-0.0279,0.1111,-0.0107,0.0223,0.0575,-0.0106,0.0188,-0.0178,-0.0311,0.0057,0.0489,-0.0013,0.0907,0.0638,0.0106,0.0257,0.0818,-0.0228,0.0443,0.0221,0.0469,0.0323,-0.0286,-0.0154,0.0198,0.0001,-0.3108,0.074,0.0089,0.0361,-0.017,0.0019,0.0216,0.0039,-0.0626,-0.008,-0.0469,0.0178,0.0302,0.0147,-0.0063,0.0465,0.0809,-0.0254,0.077,-0.0527,-0.0098,0.0618,0.2206,-0.0672,0.0481,-0.0381,-0.0262,0.032,0.0682,0.0068,0.02,0.025,0.0809,-0.0381,0.0009,0.0885,-0.034,-0.006,0.0159,-0.0611,-0.0062,0.0353,-0.0432,-0.0106,0.107,0.0097,-0.0173,-0.0472,-0.0273,0.0422,-0.0658,0.0078,0.024,-0.0074,-0.0058,0.0116,-0.0175,-0.0618,-0.0373,-0.0376,0.0297,-0.0921,0.0031,-0.0065,-0.001]}
{"key":"[A survey on independence-based Markov networks learning] This work reports the most relevant technical aspects in the problem of learning the \\emph{Markov network structure} from data. Such problem has become increasingly important in machine learning, and many other application fields of machine learning. Markov networks, together with Bayesian networks, are probabilistic graphical models, a widely used formalism for handling probability distributions in intelligent systems. Learning graphical models from data have been extensively applied for the case of Bayesian networks, but for Markov networks learning it is not tractable in practice. However, this situation is changing with time, given the exponential growth of computers capacity, the plethora of available digital data, and the researching on new learning technologies. This work stresses on a technology called independence-based learning, which allows the learning of the independence structure of those networks from data in an efficient and sound manner, whenever the dataset is sufficiently large, and data is a representative sampling of the target distribution. In the analysis of such technology, this work surveys the current state-of-the-art algorithms for learning Markov networks structure, discussing its current limitations, and proposing a series of open problems where future works may produce some advances in the area in terms of quality and efficiency. The paper concludes by opening a discussion about how to develop a general formalism for improving the quality of the structures learned, when data is scarce.","layer":0,"vector":[-0.0544,0.0294,-0.0197,-0.0111,0.048,0.0754,0.0519,0.0436,0.0587,-0.0091,0.0148,-0.0637,0.0474,0.0773,0.0466,0.0349,0.0006,0.0486,-0.0126,-0.0615,0.0269,-0.0817,-0.0327,-0.0475,0.0498,0.0705,0.0146,-0.0121,-0.0438,-0.2244,0.0156,-0.0959,0.0404,-0.0194,-0.0024,-0.0307,-0.0186,0.0577,-0.0128,0.0446,0.0247,0.0337,-0.0292,-0.0847,-0.0042,-0.0392,-0.0316,-0.0359,-0.0175,-0.0433,0.023,-0.0093,0.0519,0.0323,0.0696,0.0425,0.0531,0.0347,0.003,0.0148,0.0073,0.0728,-0.1533,0.0591,0.0571,0.0415,-0.0518,0.0099,0.0019,0.045,-0.0172,0.0243,-0.0148,0.0415,0.0193,-0.0282,0.0226,-0.0474,-0.0164,0.0042,-0.0164,-0.0264,-0.0353,-0.0215,-0.0468,-0.0072,0.0034,-0.0638,0.0219,0.0196,-0.0493,0.0611,-0.0086,0.012,-0.0613,-0.0095,0.0497,0.0048,-0.0268,0.1785,-0.0338,0.0122,0.0268,-0.003,0.0348,-0.0308,0.0143,-0.0313,-0.0215,-0.0129,0.0207,-0.0382,-0.0024,-0.0709,-0.0109,0.0252,0.0319,0.04,-0.0191,-0.0257,-0.0424,-0.0249,0.0942,0.0071,0.0155,-0.0627,-0.0218,0.1251,0.0604,0.0227,0.0876,-0.0433,-0.0334,-0.0228,0.0305,0.0115,0.0499,-0.0455,0.0303,-0.0118,-0.0395,-0.1014,0.0158,-0.0591,-0.0774,0.1069,-0.0381,0.033,-0.0281,-0.04,-0.0426,0.0262,0.0212,-0.0213,0.0228,0.0386,0.0549,0.0382,-0.036,-0.0014,-0.0318,-0.054,-0.0327,0.1306,0.0033,-0.0642,-0.0254,0.0212,0.009,-0.0262,0.0478,0.0693,-0.0126,0.0621,0.0749,0.0326,-0.0601,-0.0044,0.0437,0.0212,0.0192,-0.0183,-0.0366,0.0054,0.0153,-0.058,0.0198,-0.0484,0.0341,0.0398,-0.0102,0.0354,0.0232,-0.0031,-0.0271,-0.0595,-0.0264,0.0052,-0.0024,-0.0481,-0.0087,-0.0313,-0.0467,0.0058,-0.0599,0.0471,-0.0021,0.0598,0.0451,0.0017,-0.0291,0.0015,0.0519,-0.0539,-0.031,-0.0129,0.0385,0.0651,0.0175,0.0152,0.0081,-0.0397,-0.0612,-0.1741,-0.0085,0.0421,-0.0254,0.0441,-0.0752,-0.0118,-0.0029,0.056,0.0676,0.0715,-0.0188,-0.024,0.019,-0.0051,0.0382,-0.0133,0.0423,-0.037,0.0051,-0.0005,-0.0177,-0.0027,-0.0917,0.0597,0.0061,0.1932,-0.0418,0.0271,-0.0166,0.0097,0.0498,-0.0492,-0.0841,0.0589,0.0225,0.0431,-0.0037,-0.0339,-0.051,-0.007,0.015,-0.0118,-0.101,-0.06,-0.0193,-0.0128,0.0396,-0.0492,0.0197,0.0212,-0.0244,0.0467,-0.0466,0.007,-0.0429,-0.0425,0.0358,-0.0203,0.0228,0.0095,-0.0484,-0.005,-0.0866,0.0473,-0.0412,-0.0293,-0.0385,0.0346,-0.0244,-0.0332,0.1199,0.0108,-0.0095,0.0494,0.0016,0.0021,-0.0344,-0.0565,-0.0648,0.0511,-0.0515,0.0337,0.0365,0.0213,-0.0194,0.0883,0.0245,0.0356,-0.0089,0.0347,-0.0014,0.0071,0.0057,0.0527,0.0247,-0.3075,0.053,0.0078,0.053,-0.0071,-0.0091,0.0555,0.0076,-0.0433,-0.0194,0.0191,0.0306,0.0504,-0.0393,-0.0294,0.0471,0.0574,-0.0464,0.0636,-0.0747,0.0258,0.042,0.2199,-0.0307,0.0683,0.0355,0.001,0.0247,0.0194,-0.0291,0.0224,-0.012,0.0864,-0.0623,0.0845,0.0833,-0.033,0.0151,-0.0191,-0.04,-0.0056,-0.0361,-0.0588,-0.0496,0.1127,0.0185,0.0012,-0.0873,-0.026,0.0678,-0.0296,-0.0343,-0.0053,-0.0037,0.0013,-0.0299,-0.0128,-0.0533,-0.0207,-0.0667,0.0239,-0.0948,0.0152,-0.0509,-0.0201]}
{"key":"[On the Generalization Properties of Differential Privacy] A new line of work, started with Dwork et al., studies the task of answering statistical queries using a sample and relates the problem to the concept of differential privacy. By the Hoeffding bound, a sample of size $O(\\log k/\\alpha^2)$ suffices to answer $k$ non-adaptive queries within error $\\alpha$, where the answers are computed by evaluating the statistical queries on the sample. This argument fails when the queries are chosen adaptively (and can hence depend on the sample). Dwork et al. showed that if the answers are computed with $(\\epsilon,\\delta)$-differential privacy then $O(\\epsilon)$ accuracy is guaranteed with probability $1-O(\\delta^\\epsilon)$. Using the Private Multiplicative Weights mechanism, they concluded that the sample size can still grow polylogarithmically with the $k$. Very recently, Bassily et al. presented an improved bound and showed that (a variant of) the private multiplicative weights algorithm can answer $k$ adaptively chosen statistical queries using sample complexity that grows logarithmically in $k$. However, their results no longer hold for every differentially private algorithm, and require modifying the private multiplicative weights algorithm in order to obtain their high probability bounds. We greatly simplify the results of Dwork et al. and improve on the bound by showing that differential privacy guarantees $O(\\epsilon)$ accuracy with probability $1-O(\\delta\\log(1/\\epsilon)/\\epsilon)$. It would be tempting to guess that an $(\\epsilon,\\delta)$-differentially private computation should guarantee $O(\\epsilon)$ accuracy with probability $1-O(\\delta)$. However, we show that this is not the case, and that our bound is tight (up to logarithmic factors).","layer":0,"vector":[-0.021,-0.004,-0.0019,-0.0498,-0.0176,0.0018,0.0665,-0.001,0.0338,0.0061,0.0458,-0.0197,0.0542,0.0335,0.0314,0.0397,-0.0195,-0.0052,-0.0662,0.0663,0.0488,-0.0662,-0.019,-0.0819,-0.004,-0.0219,-0.084,-0.0398,-0.0285,-0.2528,0.0009,-0.0844,0.0597,-0.0507,0.0301,-0.0117,-0.04,0.0227,-0.0266,0.0459,0.0201,0.0426,-0.0458,-0.0281,-0.0327,-0.0436,-0.0262,0.0123,-0.0558,-0.0236,-0.0102,-0.014,-0.0001,0.063,0.0547,0.0239,0.0263,-0.0041,0.0033,0.072,0.02,0.0812,-0.1523,0.0699,0.0247,0.0277,-0.0415,-0.0779,0.0165,0.0443,0.0038,0.084,0.012,0.023,0.0311,0.0209,-0.0105,-0.0083,-0.0408,0.046,-0.0011,-0.0395,-0.0047,-0.0087,-0.0785,-0.0551,0.0102,-0.0361,0.0677,-0.0123,-0.0175,-0.0089,-0.0282,-0.006,-0.0384,-0.0143,0.0349,0.0263,-0.0105,0.1617,-0.0345,0.035,0.0105,-0.0132,0.0293,-0.0366,0.0046,-0.0423,-0.0442,0.0126,0.047,-0.0173,0.0514,-0.0261,-0.0114,0.0384,0.0553,0.0138,-0.0261,-0.0053,-0.0026,0.0406,0.0121,0.0171,0.0508,-0.0502,-0.0026,0.1413,0.0314,0.0353,0.0265,-0.0886,-0.0449,0.003,0.0068,0.026,0.0032,0.029,0.0635,-0.0014,-0.0693,-0.0778,0.0115,-0.0676,-0.036,0.1563,-0.0032,0.0789,-0.0202,-0.0619,0.0153,0.0232,0.0083,-0.0483,0.0369,-0.0373,0.0303,0.0678,-0.0519,0.052,0.0213,-0.0282,0.0119,0.142,0.0044,-0.0913,0.0028,0.0617,0.0196,-0.0347,0.0868,0.031,-0.0494,0.0383,0.0465,0.0031,-0.0849,0.0095,-0.0194,0.0159,-0.03,-0.0026,-0.0301,0.0057,0.0018,-0.0565,-0.0112,-0.0342,0.0438,0.0784,-0.0366,-0.0056,-0.039,0.0127,-0.0481,-0.0415,-0.0104,-0.0031,0.0127,-0.0332,-0.0117,-0.0013,-0.0436,0.041,-0.0065,0.0452,-0.0091,-0.0236,0.0222,0.017,-0.0319,0.0044,-0.0089,-0.032,-0.035,0.0282,0.0259,0.0322,-0.009,0.026,-0.0167,-0.0657,-0.0399,-0.2227,-0.0673,-0.0237,0.0155,0.0724,-0.0638,0.0471,0.0082,0.0211,0.0871,0.0361,-0.0069,-0.0337,0.0688,-0.0002,0.0657,0.0394,0.0244,0.0138,-0.0297,-0.029,0.0292,-0.0162,-0.0726,0.0458,0.0066,0.2293,-0.0122,0.0281,-0.0457,0.0629,0.0143,-0.004,-0.0986,0.0417,0.0438,-0.0453,-0.014,-0.0007,-0.0294,0.014,0.0372,-0.0311,-0.1034,-0.0523,-0.0441,-0.0185,-0.025,-0.0704,0.0279,0.0414,0.0362,0.0404,0.0243,0.0225,-0.0878,-0.0544,-0.0136,-0.0669,0.0782,-0.0171,-0.0339,-0.0073,-0.0668,0.0377,-0.0012,-0.0223,-0.0604,0.0312,-0.0378,-0.0084,0.0423,0.0278,0.0039,0.0479,0.0464,0.0421,-0.0485,-0.0229,-0.0507,0.0977,-0.025,0.0183,-0.0149,-0.0154,0.0335,0.0839,0.0333,0.0125,-0.0062,-0.002,0.0212,-0.0475,-0.0594,0.018,-0.0183,-0.296,0.0019,-0.0131,0.0103,-0.0229,0.0311,0.0372,0.0303,-0.0877,0.0023,0.0111,0.123,-0.0051,-0.0004,0.0302,0.018,0.0377,-0.0166,0.0478,-0.0136,0.0484,0.0142,0.1898,-0.0036,0.0327,0.0239,0.0197,0.0118,-0.0064,-0.0297,0.0008,0.0108,0.0608,-0.0629,0.0391,0.025,-0.0173,0.0159,0.0334,-0.0304,-0.0251,-0.0669,-0.0102,0.0207,0.1257,0.0282,-0.0457,-0.0407,0.0124,0.0157,-0.0423,0.0053,-0.016,0.005,0.0367,0.0314,-0.0612,-0.0179,-0.0398,-0.0254,0.0338,-0.0283,-0.0256,0.018,0.0154]}
{"key":"[Exact Indexing of Time Series under Dynamic Time Warping] Dynamic time warping (DTW) is a robust similarity measure of time series. However, it does not satisfy triangular inequality and has high computational complexity, severely limiting its applications in similarity search on large-scale datasets. Usually, we resort to lower bounding distances to speed up similarity search under DTW. Unfortunately, there is still a lack of an effective lower bounding distance that can measure unequal-length time series and has desirable tightness. In the paper, we propose a novel lower bounding distance LB_Keogh+, which is a seamless combination of sequence extension and LB_Keogh. It can be used for unequal-length sequences and has low computational complexity. Besides, LB_Keogh+ can extend sequences to an arbitrary suitable length, without significantly reducing tightness. Next, based on LB_Keogh+, an exact index of time series under DTW is devised. Then, we introduce several theorems and complete the relevant proofs to guarantee no false dismissals in our similarity search. Finally, extensive experiments are conducted on real-world datasets. Experimental results indicate that our proposed method can perform similarity search of unequal-length sequences with high tightness and good pruning power.","layer":3,"vector":[-0.0352,-0.011,0.0198,-0.0154,0.0645,0.0026,-0.0079,0.0109,0.0508,-0.0218,0.0163,-0.044,0.0026,0.0491,-0.0016,-0.0397,-0.0225,0.0501,-0.058,-0.0018,0.071,-0.0284,-0.019,-0.0359,0.0525,0.0224,0.0012,-0.0,-0.0198,-0.2411,-0.0187,-0.06,0.045,-0.005,-0.0096,-0.0251,-0.0585,0.0694,-0.0412,0.0455,0.0355,0.0432,-0.0075,-0.0374,-0.0044,-0.0694,-0.0317,0.0282,0.0054,-0.02,-0.007,-0.0333,0.0051,0.043,0.0035,0.0339,0.0727,0.0291,0.0579,0.0041,0.0603,0.0054,-0.1832,0.0431,0.0213,-0.0147,-0.0029,-0.0371,0.0147,0.0159,-0.0265,0.0498,0.0371,0.0328,0.0081,0.008,-0.03,-0.0671,-0.0564,-0.0319,0.0291,-0.0413,-0.0064,-0.051,-0.0278,-0.0208,0.0041,-0.078,0.0444,0.0117,-0.0558,-0.0112,0.0103,0.02,-0.1122,-0.0724,0.0587,0.0618,-0.0565,0.188,-0.1048,0.0952,0.0303,-0.0527,0.0605,-0.004,0.0169,-0.0229,-0.0197,-0.005,-0.016,-0.0158,0.0469,-0.0533,0.0486,-0.0036,0.0567,0.0132,0.016,0.0104,-0.0025,0.0193,0.0551,-0.0411,0.0205,-0.0624,0.0631,0.096,0.0108,-0.0052,0.0407,-0.0111,-0.0418,0.0028,0.0125,0.0229,-0.0183,0.0329,0.0388,-0.0365,-0.0189,-0.0683,0.0191,-0.0444,-0.0501,0.1164,-0.0391,0.0315,-0.0598,-0.0026,-0.0536,-0.0135,-0.0291,-0.0519,0.0157,0.0625,0.1003,0.0307,-0.0523,0.0009,-0.0686,-0.0386,0.0168,0.0924,0.0227,-0.1187,-0.0258,0.051,0.0182,-0.0255,0.0306,-0.0163,-0.0364,0.0666,0.1033,0.0599,-0.0235,0.011,0.024,0.0061,0.0405,-0.0522,-0.0069,0.0396,0.0067,-0.0273,0.0114,0.0091,0.0334,0.0256,-0.0353,-0.0424,-0.0155,0.0506,-0.0073,-0.0164,-0.0044,-0.0132,0.0328,-0.0589,0.0179,-0.0438,-0.0306,0.0187,0.0124,0.0201,-0.0538,-0.007,0.0287,0.0129,0.0171,-0.022,-0.0005,-0.0324,-0.0616,-0.0137,0.0333,0.053,0.0043,0.059,0.0237,-0.0534,-0.0371,-0.2705,-0.028,-0.0183,0.0364,0.0331,-0.035,-0.0246,-0.0241,0.0929,0.0339,0.028,0.0018,-0.0275,0.0187,-0.0178,0.0877,0.0395,0.0533,-0.0245,-0.0233,0.0058,0.0421,-0.0249,-0.0896,0.0688,-0.0305,0.1728,-0.0015,0.0316,-0.0734,0.0027,-0.0117,-0.0121,-0.076,0.0183,0.0273,0.0403,-0.0035,-0.04,-0.0508,-0.0746,0.0005,0.0155,-0.0529,-0.0687,-0.0085,0.0008,-0.0305,-0.0535,0.0299,0.0348,-0.0095,0.0529,0.008,-0.0222,-0.0501,-0.065,-0.006,-0.016,0.0247,-0.0194,-0.0361,0.0228,-0.0247,0.0208,-0.0052,-0.0176,0.0027,-0.0417,-0.0107,-0.0161,0.0736,-0.0568,0.0238,0.0442,0.0135,0.0315,0.0108,-0.0127,-0.0367,0.0976,-0.039,0.0782,0.0281,0.0427,0.0032,0.0911,0.0057,0.0418,-0.0269,0.0302,-0.0024,-0.0471,-0.0353,0.0086,0.0128,-0.269,0.0823,-0.0218,-0.0,-0.056,-0.0077,-0.0041,0.031,0.0115,-0.0248,0.0029,0.0854,0.0324,-0.0257,-0.0265,0.0514,0.0712,-0.0533,0.0434,-0.0514,0.0289,0.0194,0.2458,-0.0024,0.024,0.0047,0.008,-0.0092,0.0206,-0.0218,0.0055,0.0087,0.0626,-0.0209,0.0114,0.0643,0.0177,0.0709,0.0359,-0.0177,0.0099,0.0466,-0.0343,0.0007,0.1094,0.0293,-0.0326,-0.0712,-0.0151,0.0709,-0.0744,-0.0047,0.0014,0.0451,0.0293,0.0581,-0.0517,-0.0248,-0.0126,-0.0838,-0.0353,-0.0501,-0.0264,-0.0147,0.0168]}
{"key":"[Stance Detection with Bidirectional Conditional Encoding] Stance detection is the task of classifying the attitude expressed in a text towards a target such as Hillary Clinton to be \"positive\", negative\" or \"neutral\". Previous work has assumed that either the target is mentioned in the text or that training data for every target is given. This paper considers the more challenging version of this task, where targets are not always mentioned and no training data is available for the test targets. We experiment with conditional LSTM encoding, which builds a representation of the tweet that is dependent on the target, and demonstrate that it outperforms encoding the tweet and the target independently. Performance is improved further when the conditional model is augmented with bidirectional encoding. We evaluate our approach on the SemEval 2016 Task 6 Twitter Stance Detection corpus achieving performance second best only to a system trained on semi-automatically labelled tweets for the test target. When such weak supervision is added, our approach achieves state-of-the-art results.","layer":0,"vector":[-0.0579,0.0449,0.0017,-0.0005,0.0293,0.037,0.0502,0.035,-0.0037,-0.0229,0.0268,-0.0301,-0.016,0.0236,-0.0046,0.0111,0.046,0.0576,-0.0438,0.009,0.0242,-0.0267,0.0058,-0.0125,0.0177,0.0353,-0.0224,-0.0229,-0.0395,-0.1951,0.0136,-0.0685,0.045,-0.026,-0.0015,-0.0126,-0.0959,-0.0147,-0.0408,0.0259,-0.0186,-0.0308,0.0,-0.0656,-0.0138,-0.0585,-0.0089,-0.0231,-0.0653,-0.0296,0.0217,-0.027,0.0134,-0.0213,0.0238,0.0679,0.0251,0.0472,0.0172,0.0378,0.0555,0.0492,-0.1689,0.0534,0.0148,0.0281,-0.0621,0.0202,0.0152,0.0671,-0.008,0.0487,0.0236,0.0404,0.0448,0.0269,0.0567,-0.0127,-0.0368,-0.0354,0.0351,0.0136,-0.0333,0.0271,-0.0029,-0.0691,0.0,-0.0374,0.0061,-0.0007,-0.0703,0.0127,0.0143,0.0433,-0.0681,-0.046,0.0182,0.0563,-0.0593,0.2072,-0.0385,0.0437,-0.0037,-0.0311,0.0432,-0.0463,-0.0059,-0.0511,-0.048,-0.046,-0.0272,-0.0281,0.0054,-0.09,0.0483,-0.0062,0.0882,0.0527,-0.0349,-0.0057,-0.0536,0.0235,0.0901,-0.0344,0.0336,-0.049,0.0622,0.1411,0.082,0.0361,0.03,-0.034,-0.0456,-0.0402,0.0168,0.019,-0.0163,0.0094,0.0151,0.0085,-0.0355,-0.0818,-0.0204,-0.0872,-0.0502,0.1384,-0.0755,0.0284,-0.0473,0.0164,-0.0008,0.018,-0.042,-0.021,0.0452,0.0516,0.0408,0.0364,-0.049,0.0109,0.0267,-0.0546,-0.0242,0.0497,0.0175,-0.0988,-0.0131,-0.0074,0.0256,-0.0179,0.081,0.0373,-0.0319,0.0064,0.0821,0.0366,-0.0415,0.041,0.0091,0.0393,0.0699,-0.0547,-0.0351,0.076,0.0236,-0.0046,-0.0012,-0.0515,0.0317,-0.0013,-0.0113,-0.0076,-0.0561,-0.0618,-0.0282,-0.0376,0.0191,0.012,0.0139,-0.0421,0.0042,0.0096,-0.0494,0.0254,-0.012,0.0031,0.0018,0.0135,0.0551,0.0221,-0.0139,0.0154,0.0593,-0.0588,-0.0285,-0.0383,0.04,0.0509,0.0078,0.0258,-0.0116,-0.0223,0.0055,-0.2525,0.002,0.03,-0.0144,0.0447,-0.0732,0.0039,-0.0141,0.0811,0.0507,0.0517,-0.0448,-0.0473,0.0402,-0.0003,0.0374,0.0305,0.0367,0.0109,0.0077,-0.0323,-0.0289,-0.0454,-0.0826,0.0761,0.0142,0.2161,0.0836,0.0226,-0.0448,0.0177,0.0199,-0.038,-0.1164,0.0956,0.0189,0.0258,-0.0291,-0.0362,0.012,-0.0167,0.0216,0.0349,-0.0955,-0.0153,-0.0157,-0.0372,-0.0262,-0.0454,0.052,0.033,-0.0767,0.042,0.0028,-0.0068,-0.0336,-0.1121,-0.0282,-0.0551,0.0074,0.0151,-0.0386,0.0239,-0.0561,0.0432,0.0204,-0.0549,-0.0631,0.0155,0.0036,-0.0137,0.0521,0.0119,0.032,0.053,0.0216,0.0284,0.001,-0.0638,-0.0324,0.1009,-0.039,0.035,0.0021,-0.006,-0.0149,0.062,-0.0312,0.0866,0.0107,0.039,0.0219,-0.0602,0.0154,0.0112,-0.0229,-0.2684,0.0417,-0.0102,0.0375,0.0059,0.0039,0.04,-0.0344,-0.0728,0.0026,-0.031,0.0782,0.0165,-0.044,-0.0285,0.0457,0.0677,-0.0502,0.0343,-0.07,0.0287,0.0217,0.1982,-0.0099,0.0209,0.0047,-0.0552,0.0266,0.0164,-0.0242,0.0186,-0.0117,0.0876,-0.0349,0.0156,0.0678,-0.0424,-0.0056,0.0241,0.0034,-0.0222,0.0454,-0.0242,-0.0249,0.0878,0.0047,0.0119,-0.0021,-0.0161,0.0281,-0.0287,0.0374,-0.0161,0.0519,0.0438,0.0117,-0.0664,-0.0505,-0.0443,-0.0285,0.004,-0.0704,-0.0006,0.0386,-0.0019]}
{"key":"[Q-Learning in enormous action spaces via amortized approximate maximization] Applying Q-learning to high-dimensional or continuous action spaces can be difficult due to the required maximization over the set of possible actions. Motivated by techniques from amortized inference, we replace the expensive maximization over all actions with a maximization over a small subset of possible actions sampled from a learned proposal distribution. The resulting approach, which we dub Amortized Q-learning (AQL), is able to handle discrete, continuous, or hybrid action spaces while maintaining the benefits of Q-learning. Our experiments on continuous control tasks with up to 21 dimensional actions show that AQL outperforms D3PG (Barth-Maron et al, 2018) and QT-Opt (Kalashnikov et al, 2018). Experiments on structured discrete action spaces demonstrate that AQL can efficiently learn good policies in spaces with thousands of discrete actions.","layer":2,"vector":[-0.0833,-0.0042,0.0439,-0.0096,0.011,0.0207,0.0123,0.0575,0.0184,0.0202,0.0285,-0.0566,-0.0111,0.0813,0.049,0.0374,-0.0216,0.0253,-0.0481,-0.0099,0.0458,-0.0217,0.0013,-0.0622,-0.0208,-0.0203,-0.0074,-0.0693,-0.0251,-0.2393,0.0207,-0.0093,0.0335,-0.0326,-0.0215,0.0056,-0.066,0.0418,-0.0219,0.0385,0.0558,0.0288,-0.0184,-0.0493,-0.043,-0.0562,-0.0184,-0.0647,-0.0413,-0.0251,-0.0038,-0.0149,0.013,0.0396,0.0613,0.0334,0.042,0.0271,0.0411,0.0066,0.0217,-0.018,-0.1279,0.0395,0.0179,0.041,-0.0063,0.0129,0.0292,0.0446,-0.028,0.0633,0.0168,0.0666,0.0403,-0.0492,-0.0159,-0.0362,0.0156,0.004,0.0042,-0.0683,-0.0628,0.0138,-0.0876,-0.0368,0.0186,-0.066,0.0383,0.0043,-0.0459,0.0045,-0.0361,0.0037,-0.0792,-0.0013,0.0292,0.0177,-0.0532,0.2084,0.0107,0.02,0.0063,-0.0219,0.0444,-0.0123,-0.0247,-0.0271,-0.03,0.0029,-0.0415,-0.0217,0.0389,-0.0289,0.0148,0.0355,0.0565,0.064,-0.0219,-0.0324,0.0093,0.0083,0.047,-0.0282,0.0203,-0.0679,0.0072,0.1404,0.0256,0.0098,0.059,-0.0458,-0.016,-0.0385,0.0099,0.0115,0.0407,-0.0081,0.0497,0.0155,-0.032,-0.0048,0.0225,-0.0903,-0.0556,0.0925,-0.0486,-0.0042,-0.0345,0.0012,0.0045,0.0367,-0.0198,-0.0491,0.0326,0.0228,0.0508,0.0357,-0.0424,-0.032,-0.0375,-0.0331,-0.0016,0.1075,-0.0112,-0.0722,-0.0317,-0.0441,0.0228,0.0132,0.0281,0.0228,-0.0507,0.0573,0.0866,0.0122,-0.07,0.0049,0.0056,0.0153,0.0468,-0.0499,-0.0209,0.0597,0.0811,0.004,0.0386,-0.0273,0.0377,0.0299,-0.0072,0.0258,-0.0411,0.0211,-0.031,-0.0303,-0.0015,-0.0104,0.0084,-0.0277,0.0176,0.0249,-0.1016,0.0277,0.0179,-0.0416,-0.0397,0.0108,0.0468,0.0366,-0.0321,0.0191,0.0649,-0.0209,-0.0234,0.0157,0.0284,-0.012,-0.0349,0.0227,0.0301,-0.0219,-0.0271,-0.2558,-0.0118,-0.0159,-0.0418,0.0308,-0.0707,0.0616,-0.0156,0.0067,0.0719,0.0032,-0.0755,-0.0157,0.0309,0.0056,0.0619,0.0508,0.0466,-0.0593,0.0078,-0.0083,-0.0147,-0.0483,-0.0738,0.0604,-0.0352,0.2369,0.0278,0.0838,0.0201,0.0387,0.027,-0.0297,-0.0862,0.071,0.0278,0.0659,-0.0055,-0.024,-0.0289,-0.0504,0.0255,-0.0696,-0.1239,-0.0123,-0.0487,-0.0512,0.0732,-0.0628,-0.025,0.0563,-0.0177,0.0324,-0.0256,-0.0022,-0.0185,-0.0688,0.0365,-0.0655,0.0521,0.0265,-0.037,-0.0157,-0.013,0.0639,0.0027,0.0017,-0.0524,0.0255,-0.0013,-0.0425,0.0532,0.0167,-0.0415,0.0787,0.0144,0.0372,-0.0325,-0.0653,-0.059,0.0745,-0.0105,0.0318,-0.0079,0.0229,-0.0155,0.0705,-0.0101,0.0498,-0.0162,0.0001,0.0061,-0.0768,0.0178,0.0641,0.0009,-0.2858,0.0082,0.0119,0.0343,-0.0553,0.0079,0.0582,-0.0177,-0.019,0.0474,-0.0081,0.0913,0.0052,0.0339,0.0041,0.0232,0.0879,-0.0227,0.0377,-0.0671,0.0477,0.0134,0.2012,-0.0058,0.0349,0.0027,-0.054,-0.0057,0.0283,-0.0339,0.0146,0.0183,0.0791,-0.0847,0.0573,0.0612,-0.0183,0.0507,-0.009,0.033,-0.027,-0.0066,-0.0415,-0.024,0.0872,-0.0017,-0.0014,-0.0544,-0.0328,0.0373,-0.016,0.0445,-0.0162,0.0065,-0.005,0.0129,-0.0556,-0.0412,-0.0503,-0.0417,0.0289,-0.0323,0.0254,-0.0202,0.0143]}
{"key":"[Multi-Resolution Fully Convolutional Neural Networks for Monaural Audio Source Separation] In deep neural networks with convolutional layers, each layer typically has fixed-size/single-resolution receptive field (RF). Convolutional layers with a large RF capture global information from the input features, while layers with small RF size capture local details with high resolution from the input features. In this work, we introduce novel deep multi-resolution fully convolutional neural networks (MR-FCNN), where each layer has different RF sizes to extract multi-resolution features that capture the global and local details information from its input features. The proposed MR-FCNN is applied to separate a target audio source from a mixture of many audio sources. Experimental results show that using MR-FCNN improves the performance compared to feedforward deep neural networks (DNNs) and single resolution deep fully convolutional neural networks (FCNNs) on the audio source separation problem.","layer":1,"vector":[0.0014,-0.0194,0.0213,-0.0617,0.0115,0.0291,0.0089,0.0036,0.0372,-0.0193,-0.0059,-0.0776,0.063,0.0746,0.0651,0.0407,0.0609,0.0459,-0.0582,0.0164,0.017,-0.0122,0.0047,-0.0206,0.0079,-0.0121,-0.0291,-0.0931,-0.0521,-0.2484,0.0046,-0.0231,0.0799,-0.0645,0.0111,-0.0329,0.0014,0.0346,-0.0318,0.0409,0.0358,0.0107,0.0221,-0.1063,-0.0483,-0.0024,-0.072,-0.0341,0.0414,-0.0223,0.0461,-0.0424,-0.0264,0.0238,-0.0083,0.0571,0.07,0.0656,0.0626,0.0478,0.0065,0.041,-0.1644,0.0307,0.0349,0.0548,0.0075,0.0105,0.0118,0.0325,-0.0412,0.0357,-0.0049,0.0019,-0.0167,-0.031,0.0065,0.0,-0.0222,0.0453,0.021,0.0408,-0.0251,-0.0211,-0.0001,-0.0064,0.0084,-0.0338,-0.0267,-0.0259,-0.0494,0.0008,-0.0136,0.0466,-0.0506,-0.0194,-0.0022,0.0033,-0.0061,0.1952,-0.0282,0.0278,0.0485,-0.0587,0.048,-0.0041,-0.014,0.0137,-0.0256,0.0328,-0.0243,-0.0533,0.0356,-0.018,0.0391,0.033,0.0821,0.0193,0.006,0.0154,-0.0128,0.0097,0.0569,0.0162,0.0465,-0.0636,0.02,0.1133,0.0191,0.033,0.0117,-0.0365,-0.0409,-0.0613,0.0353,0.022,0.0205,-0.0039,0.0413,0.0219,-0.0311,-0.0719,-0.0186,-0.0603,-0.0682,0.1214,-0.0596,0.0151,-0.0731,-0.0332,-0.0147,0.0607,-0.0451,-0.0201,0.0663,0.0371,0.0203,0.0154,-0.0747,0.0347,0.0169,-0.0606,-0.0108,0.1188,-0.0052,-0.0969,-0.0722,-0.013,0.0186,-0.0187,-0.0322,-0.0069,-0.0522,0.0172,0.0657,0.0505,-0.0945,0.0409,0.0482,0.0029,0.0017,-0.0533,-0.0338,-0.0,0.016,-0.0327,0.0369,-0.0588,-0.0162,0.0604,-0.0517,0.034,-0.0306,-0.0072,-0.0171,-0.0244,-0.0029,-0.0018,-0.0027,-0.0008,-0.0235,-0.0325,-0.0314,0.0274,0.0225,0.0332,-0.0296,-0.0167,-0.0131,0.0587,-0.0602,-0.0024,0.0973,-0.029,0.0051,-0.0525,0.0293,0.0025,-0.0026,0.034,0.0281,-0.0865,-0.0751,-0.2403,0.0381,0.021,-0.0121,0.0233,-0.0915,0.0183,-0.0198,0.1056,0.0478,0.0838,0.023,-0.0054,0.0305,-0.0087,0.0332,0.0219,0.0527,-0.0197,0.0083,0.0204,0.0199,-0.0367,-0.0647,0.0657,0.0011,0.2057,0.0069,0.0517,-0.0001,0.0367,0.0363,-0.0085,-0.1117,0.0288,0.0113,0.0835,-0.0128,-0.0381,-0.0278,-0.0433,0.0116,-0.0427,-0.0917,-0.0521,-0.0408,-0.0456,-0.0025,-0.0449,-0.0266,0.0503,0.0065,0.036,0.0017,0.0032,-0.0001,-0.1244,0.0353,-0.0463,-0.0035,0.0303,-0.0161,0.016,-0.069,0.0413,0.0458,-0.0485,0.0075,0.031,-0.006,0.0096,0.0468,0.0285,-0.002,0.0336,-0.036,0.0432,-0.0788,-0.0571,-0.015,0.0847,0.0283,0.0008,0.0205,0.0283,0.0599,0.0677,-0.0182,0.0341,-0.0454,0.0048,0.0185,-0.0438,0.0011,0.0124,0.008,-0.2653,0.0449,0.0487,0.0202,-0.0233,0.0305,0.0164,0.0409,-0.0415,0.0107,-0.0329,0.0492,0.0125,-0.0209,0.0442,0.0327,0.0678,-0.0188,0.055,-0.0305,-0.0061,0.0918,0.1721,0.0029,-0.0118,-0.0057,-0.0257,-0.0344,0.0271,-0.0677,0.006,0.0129,0.0834,-0.0669,-0.0004,0.1095,-0.004,0.0506,0.0424,-0.0005,-0.033,0.0014,-0.0725,-0.0465,0.0797,-0.0691,-0.0313,-0.0338,-0.0148,0.0243,-0.0327,0.009,0.0181,-0.0243,0.0072,0.0225,-0.068,-0.064,-0.0176,0.0299,0.017,-0.0604,-0.0249,0.0174,-0.0363]}
{"key":"[Learning Dynamics from Noisy Measurements using Deep Learning with a Runge-Kutta Constraint] Measurement noise is an integral part while collecting data of a physical process. Thus, noise removal is a necessary step to draw conclusions from these data, and it often becomes quite essential to construct dynamical models using these data. We discuss a methodology to learn differential equation(s) using noisy and sparsely sampled measurements. In our methodology, the main innovation can be seen in of integration of deep neural networks with a classical numerical integration method. Precisely, we aim at learning a neural network that implicitly represents the data and an additional neural network that models the vector fields of the dependent variables. We combine these two networks by enforcing the constraint that the data at the next time-steps can be given by following a numerical integration scheme such as the fourth-order Runge-Kutta scheme. The proposed framework to learn a model predicting the vector field is highly effective under noisy measurements. The approach can handle scenarios where dependent variables are not available at the same temporal grid. We demonstrate the effectiveness of the proposed method to learning models using data obtained from various differential equations. The proposed approach provides a promising methodology to learn dynamic models, where the first-principle understanding remains opaque.","layer":1,"vector":[-0.056,0.0126,0.0383,0.0005,0.0043,0.0146,0.0141,-0.0101,0.0608,-0.0332,0.008,-0.0784,0.0372,0.0397,0.0165,-0.0219,-0.0324,0.0611,-0.0496,0.0084,0.0036,-0.0236,0.0199,-0.0314,0.007,0.0126,-0.0283,-0.0203,-0.0544,-0.2609,0.0274,-0.0523,0.013,-0.0633,0.0765,-0.0246,-0.0356,0.0745,-0.0115,0.0543,0.0179,0.021,0.0181,-0.0906,0.0003,-0.0747,-0.0016,-0.0138,-0.0124,-0.025,0.0008,-0.0238,-0.0073,0.0122,0.0335,0.0379,0.1165,0.044,0.0682,0.0478,0.0101,0.0574,-0.1533,0.0748,0.035,0.0171,-0.0259,0.0039,0.0586,0.0454,-0.0211,0.0516,-0.0111,0.0418,-0.0014,0.0123,-0.0028,-0.0436,-0.033,0.0424,0.041,-0.0107,-0.0088,-0.0362,-0.0343,-0.0457,0.0365,-0.0458,0.0627,0.0351,-0.0299,-0.0221,-0.0402,0.0138,-0.0483,0.0093,0.0377,0.0194,-0.0185,0.1785,-0.0601,0.0221,0.0377,-0.0156,-0.0187,-0.0322,-0.0702,-0.0151,0.0073,0.0007,-0.0301,-0.0027,0.0279,-0.0624,0.0215,-0.0185,0.0145,-0.0051,-0.0011,-0.0157,-0.0285,0.0018,0.0281,-0.0147,0.0463,-0.0603,-0.019,0.1389,0.018,0.0434,0.0539,-0.0295,-0.0489,-0.0588,-0.0011,-0.0004,0.0141,-0.0176,0.0088,0.0128,-0.0819,-0.0698,0.0232,-0.1172,-0.0769,0.1148,-0.0035,0.0398,-0.0247,-0.043,-0.0343,0.0425,-0.0188,0.0014,0.0722,0.0147,-0.0108,0.0488,-0.0671,0.0503,-0.0622,-0.0593,-0.0323,0.1043,-0.0303,-0.0461,-0.0271,0.0306,0.003,-0.0541,0.0232,0.0396,-0.0736,0.012,0.0676,0.0326,0.0032,0.037,-0.0266,0.0242,0.0028,-0.0237,-0.0243,0.0446,0.0505,-0.0427,0.0148,-0.0311,0.0248,0.0473,-0.0093,0.0061,-0.0295,0.0363,-0.0117,-0.0079,-0.0375,-0.0006,-0.0053,-0.054,-0.0237,-0.0201,-0.028,0.0439,-0.0312,0.0083,0.0173,0.0347,-0.0019,0.0296,-0.0041,-0.0238,0.0433,-0.0779,-0.0323,0.039,0.0101,0.0293,-0.0299,0.0162,0.0473,-0.0791,-0.0571,-0.2492,0.0046,0.0117,-0.0027,0.0911,-0.0877,0.0405,-0.0648,0.0626,0.0502,0.0509,0.0377,0.0082,-0.0105,0.0157,0.0445,0.0175,0.0662,-0.0434,-0.0071,-0.0069,-0.0462,-0.0078,-0.0941,0.0443,-0.0172,0.2031,-0.0112,0.0809,-0.0316,0.0025,0.0279,-0.0118,-0.0691,0.0544,-0.0157,0.0771,0.0069,-0.0259,-0.062,-0.0194,-0.0198,-0.023,-0.068,-0.0046,-0.0235,0.0124,0.0061,-0.0901,0.0123,0.0351,-0.0217,0.0624,-0.0224,-0.0103,-0.0461,-0.0731,0.0483,-0.0413,-0.0212,-0.0099,-0.0111,-0.0169,-0.036,0.0362,0.0413,0.0156,-0.0467,0.0278,-0.0364,0.0311,0.0877,-0.0061,0.0302,0.0599,0.0003,-0.011,-0.0258,-0.0657,-0.009,0.0687,-0.0031,0.06,0.0387,0.059,0.0018,0.0656,-0.0314,-0.0012,-0.0359,0.0066,0.0383,-0.0298,-0.0053,0.036,0.0032,-0.2842,0.006,0.0164,0.0213,-0.0,-0.0227,0.0386,-0.0043,-0.0495,-0.017,-0.0773,0.0613,0.061,0.0168,0.0191,0.0513,0.0522,-0.0933,0.0797,-0.0698,0.0388,0.0643,0.2148,-0.0437,0.0564,0.0191,-0.0204,-0.0018,0.0426,-0.0121,0.0144,0.01,0.0679,-0.0693,0.0383,0.0905,-0.0105,0.0098,0.02,0.0044,0.0469,0.0054,-0.0082,-0.0005,0.0648,-0.0175,-0.0027,-0.0616,0.0056,0.0544,-0.0244,0.0253,0.0193,0.0109,0.0054,0.0517,-0.0409,-0.0245,-0.0091,-0.0501,0.0108,-0.0876,-0.0077,-0.0473,-0.0095]}
{"key":"[AutoQML: Automated Quantum Machine Learning for Wi-Fi Integrated Sensing and Communications] Commercial Wi-Fi devices can be used for integrated sensing and communications (ISAC) to jointly exchange data and monitor indoor environment. In this paper, we investigate a proof-of-concept approach using automated quantum machine learning (AutoQML) framework called AutoAnsatz to recognize human gesture. We address how to efficiently design quantum circuits to configure quantum neural networks (QNN). The effectiveness of AutoQML is validated by an in-house experiment for human pose recognition, achieving state-of-the-art performance greater than 80% accuracy for a limited data size with a significantly small number of trainable parameters.","layer":3,"vector":[-0.0823,0.0095,0.0384,0.0005,-0.0167,0.0596,0.0758,0.0118,-0.0165,-0.0086,0.0311,-0.0404,0.0238,0.0691,0.0459,-0.0074,-0.0022,0.0247,-0.0659,0.0242,0.0209,-0.0095,-0.0038,-0.0544,0.0051,0.0154,0.0113,-0.0442,-0.0144,-0.1761,0.0069,-0.0315,0.0311,0.0114,0.0093,-0.0428,-0.0436,-0.004,-0.0159,0.0197,0.0262,0.0234,-0.0059,0.0215,-0.0047,-0.0689,-0.0123,-0.0257,-0.0582,-0.0396,0.0309,0.0417,0.0249,0.0635,0.0483,0.0138,0.0678,0.0507,0.0404,0.0204,0.0509,0.0551,-0.1602,0.091,0.044,0.0206,-0.0243,-0.0394,0.0374,0.0096,-0.0452,0.0497,0.0057,0.0148,0.0236,-0.013,0.0209,-0.0432,-0.0132,0.0079,0.029,-0.0458,-0.0529,0.0072,-0.0641,-0.0105,0.0022,-0.0592,-0.0018,0.0122,-0.0692,-0.0285,-0.0623,0.0146,-0.0476,0.0029,0.0187,0.0087,-0.0906,0.2149,-0.0486,0.0388,0.0072,-0.0366,0.0529,-0.0695,-0.0752,-0.0358,-0.0686,-0.0173,-0.0309,-0.0564,0.021,-0.0334,0.0247,0.0104,0.0551,0.0483,0.0329,-0.0229,-0.0097,0.0087,0.0654,-0.0184,0.0084,-0.0669,0.024,0.1509,0.0278,0.0738,0.0278,-0.0409,0.0185,-0.067,0.0201,0.049,0.0189,-0.0006,0.0593,0.0202,0.0109,-0.0688,0.0258,-0.1169,-0.0443,0.0704,-0.0535,0.0226,-0.0072,-0.0251,0.0256,0.0561,-0.0186,-0.0064,0.0603,0.0916,0.0212,0.0553,-0.0403,0.0387,-0.0512,-0.0637,-0.0247,0.0506,0.0117,-0.1106,-0.043,-0.0306,0.011,-0.008,0.0756,0.0637,-0.078,0.0473,0.0725,0.0294,-0.0429,0.0431,-0.0333,0.0189,-0.0383,-0.046,-0.0236,0.0129,0.0582,-0.0249,-0.0043,-0.0476,0.0061,0.0142,-0.0177,0.0357,-0.0524,-0.0059,0.0004,-0.0187,-0.0399,0.037,-0.0018,-0.0119,0.0592,-0.0011,-0.0221,-0.0069,-0.0058,-0.0198,0.0069,0.0074,0.0691,0.0197,0.0132,-0.03,0.0378,-0.039,-0.065,-0.0199,0.0115,0.0206,-0.0188,0.0541,0.0233,-0.0437,-0.0985,-0.2335,-0.0043,0.0185,-0.0325,0.0265,-0.0711,0.0115,-0.0097,0.0493,0.0666,0.0552,0.0192,-0.0006,0.0136,-0.0338,0.0667,0.0331,0.0615,-0.0564,-0.0125,-0.0208,0.0132,-0.0512,-0.0816,0.0112,-0.0065,0.217,0.0429,0.0514,0.0031,0.0587,0.0409,-0.0647,-0.0835,0.0282,0.0434,0.0399,0.0401,-0.0705,-0.0207,-0.0443,0.0293,0.0305,-0.0909,0.0081,-0.0176,-0.0454,-0.0123,-0.0578,0.0053,0.0785,-0.041,0.0105,-0.0281,-0.0618,-0.0167,-0.0557,0.0051,-0.0251,0.0382,-0.0078,-0.0565,0.0194,-0.0587,0.0512,-0.0012,-0.0529,-0.0187,0.0653,-0.0308,-0.022,0.0978,0.0189,0.0066,0.0846,-0.0405,0.0749,-0.0267,-0.0185,-0.0197,0.0826,0.0029,0.0557,-0.0048,-0.0083,0.0117,0.0712,0.0059,0.0189,-0.0225,0.0055,0.0497,-0.011,0.0179,0.0178,-0.0103,-0.3003,0.0155,0.0163,0.041,-0.0602,-0.0001,0.0438,0.0209,-0.0702,0.018,-0.0491,-0.008,0.0296,-0.0112,0.0251,0.0507,0.0661,-0.0217,-0.0044,-0.0736,-0.006,0.059,0.2175,-0.0361,0.0479,0.0604,-0.0256,0.0095,0.0388,-0.0513,-0.0074,-0.0303,0.0436,-0.0751,0.063,0.0237,-0.0329,0.0017,-0.0199,0.0113,0.0131,0.0168,-0.0462,-0.046,0.1028,0.0079,-0.0264,-0.0501,0.0193,0.0318,-0.008,0.019,-0.0246,0.0219,0.0018,-0.004,-0.0131,-0.0453,-0.059,-0.0147,0.0501,-0.0336,-0.0186,-0.0072,-0.0014]}
{"key":"[Using Laplacian Spectrum as Graph Feature Representation] Graphs possess exotic features like variable size and absence of natural ordering of the nodes that make them difficult to analyze and compare. To circumvent this problem and learn on graphs, graph feature representation is required. A good graph representation must satisfy the preservation of structural information, with two particular key attributes: consistency under deformation and invariance under isomorphism. While state-of-the-art methods seek such properties with powerful graph neural-networks, we propose to leverage a simple graph feature: the graph Laplacian spectrum (GLS). We first remind and show that GLS satisfies the aforementioned key attributes, using a graph perturbation approach. In particular, we derive bounds for the distance between two GLS that are related to the \\textit{divergence to isomorphism}, a standard computationally expensive graph divergence. We finally experiment GLS as graph representation through consistency tests and classification tasks, and show that it is a strong graph feature representation baseline.","layer":0,"vector":[-0.0296,-0.0469,-0.0187,0.0033,0.0228,0.0379,0.0127,0.0351,0.0324,-0.0176,0.027,-0.0341,0.0681,0.0751,0.0203,0.0579,-0.0068,0.0836,-0.0484,-0.0123,0.0159,-0.0377,-0.0378,-0.0513,0.0413,0.0131,-0.038,0.0013,-0.0521,-0.2607,0.0414,-0.0434,0.04,-0.0535,0.0309,-0.0543,0.0327,0.0501,0.0018,0.0373,0.0184,-0.0006,-0.0126,-0.0493,-0.0309,-0.0363,0.006,-0.0046,-0.0251,-0.034,0.0205,-0.0467,0.003,0.0265,0.0369,0.0669,0.0623,0.0136,0.0552,0.096,0.0322,0.0513,-0.1313,0.0752,0.0405,0.042,-0.0706,-0.0149,0.0151,0.0953,0.0298,0.0162,0.0191,0.0201,0.0191,0.0193,0.0228,-0.0293,-0.0524,-0.0061,0.0295,-0.0225,-0.0683,0.0011,0.0046,-0.0075,0.0,-0.031,0.0268,0.0098,-0.0583,-0.0123,-0.0164,0.0376,-0.0706,-0.0336,0.0367,-0.0104,-0.0415,0.1787,-0.0416,0.0335,0.025,-0.0306,0.0325,-0.0463,-0.0226,-0.0245,-0.0459,-0.0109,-0.0243,-0.0099,-0.0209,-0.0531,-0.0103,-0.0283,0.0603,0.0281,-0.0146,-0.0191,-0.029,0.0502,0.0346,-0.0429,0.0493,-0.0947,-0.0264,0.1065,0.0612,0.0524,0.0271,0.0026,0.0298,0.0007,-0.0371,0.007,0.011,0.0024,0.0116,0.0316,-0.021,-0.0108,0.0184,-0.0349,-0.0872,0.1477,-0.0826,-0.0305,-0.0484,-0.0104,-0.0422,0.0044,-0.0235,-0.0146,0.0111,0.0232,0.0161,0.049,-0.0456,0.0005,0.0066,-0.0231,-0.0817,0.1153,0.0469,-0.1008,-0.016,-0.0247,-0.0391,-0.0108,0.0531,0.0531,-0.0073,0.0145,0.1073,0.0421,-0.0692,-0.01,0.0319,0.0006,0.0313,-0.0217,-0.0539,0.0226,0.0179,-0.0185,-0.0268,-0.0424,0.046,0.0477,0.0007,0.0099,0.0016,-0.0038,-0.0363,-0.0113,-0.0481,-0.0292,-0.0534,-0.0276,0.0184,-0.002,-0.0402,0.0117,-0.0501,0.0256,-0.0033,-0.0053,-0.024,0.001,-0.0483,-0.0135,-0.0135,-0.0017,-0.0174,-0.0022,0.0483,0.0155,0.0223,0.0831,0.0258,-0.0387,-0.073,-0.2155,-0.0475,-0.0424,-0.0095,0.0471,-0.08,0.0022,0.0338,0.0947,0.0295,0.0479,0.033,-0.0538,-0.023,-0.0094,0.0661,0.0474,0.049,-0.0051,-0.003,-0.0322,0.0217,-0.0279,-0.0704,0.0586,-0.0077,0.2242,0.0047,0.0385,-0.0249,-0.0217,0.017,-0.0701,-0.0575,0.0675,0.0487,0.0296,0.0068,-0.0337,-0.0054,-0.0263,0.0011,-0.0184,-0.0837,-0.023,-0.0139,-0.0211,0.0089,-0.0784,0.0107,0.0586,-0.0086,0.0622,0.0028,0.0032,-0.0553,-0.0737,-0.0015,-0.0265,-0.0144,0.0032,-0.101,0.0331,-0.0521,0.0814,0.0286,-0.0149,-0.0262,0.0321,0.0006,-0.0231,0.0618,0.0148,-0.0295,0.0742,0.0283,0.0025,0.0106,-0.0231,0.0048,0.0274,-0.0534,0.0178,-0.0047,0.0531,0.016,0.0784,-0.0183,0.0338,-0.025,0.023,0.0218,-0.0262,-0.0201,0.0519,-0.0343,-0.2844,0.0269,0.0174,0.0456,-0.0204,0.007,0.0652,0.0384,-0.0788,-0.0109,0.0335,0.0342,0.0327,-0.0316,-0.0174,0.0439,0.0638,-0.0536,0.0687,-0.0418,0.0335,0.04,0.2484,-0.0181,0.0601,0.0331,0.0017,-0.0072,0.0063,-0.0272,0.0025,-0.0144,0.0978,-0.0415,0.0384,0.0797,-0.0571,0.0528,0.0235,-0.0112,0.0345,-0.0471,-0.052,-0.0387,0.104,-0.0078,-0.0489,-0.0328,0.0287,0.0392,-0.0262,-0.003,-0.0328,0.0261,-0.0016,0.0205,-0.0233,-0.0341,-0.0303,-0.0984,0.0061,-0.0436,-0.011,-0.041,-0.0043]}
{"key":"[MESS: Manifold Embedding Motivated Super Sampling] Many approaches in the field of machine learning and data analysis rely on the assumption that the observed data lies on lower-dimensional manifolds. This assumption has been verified empirically for many real data sets. To make use of this manifold assumption one generally requires the manifold to be locally sampled to a certain density such that features of the manifold can be observed. However, for increasing intrinsic dimensionality of a data set the required data density introduces the need for very large data sets, resulting in one of the many faces of the curse of dimensionality. To combat the increased requirement for local data density we propose a framework to generate virtual data points that faithful to an approximate embedding function underlying the manifold observable in the data.","layer":1,"vector":[-0.0323,-0.043,0.0524,-0.0211,0.0079,0.0352,-0.0049,0.0753,-0.0093,0.0225,-0.0135,-0.0779,0.0101,0.0415,-0.0017,0.0498,-0.0012,0.0555,-0.0291,0.0342,0.0536,-0.0345,-0.0237,-0.0575,-0.0205,0.0328,0.0015,-0.0685,-0.022,-0.2495,0.0364,-0.0563,0.086,-0.0027,0.0014,-0.0681,-0.0117,0.0575,-0.0133,0.0485,0.0259,0.0411,-0.0431,-0.0221,-0.0525,-0.0529,-0.0154,-0.0029,0.0027,0.0038,0.0412,-0.0549,-0.009,0.0375,0.0649,0.0158,0.0809,0.0079,0.0461,0.0614,0.0399,0.0214,-0.1128,0.0408,0.0497,0.0171,-0.0392,-0.0136,0.0217,0.0389,-0.0267,0.0262,-0.0416,0.0555,0.0251,-0.0025,0.0289,-0.0386,-0.0506,0.0161,0.0075,-0.0103,-0.0284,0.0309,-0.053,-0.031,0.0558,-0.0562,0.0296,-0.0097,-0.0367,-0.0362,-0.0727,0.0346,-0.0507,-0.0362,0.041,0.0319,-0.0062,0.1933,-0.0873,0.0409,0.0225,-0.01,0.0014,-0.0709,-0.0522,-0.0232,0.0026,-0.008,-0.0055,-0.0203,-0.023,-0.0575,0.0298,-0.0138,0.0757,0.0537,-0.0197,-0.037,-0.0092,0.006,0.0759,-0.0169,0.0715,-0.0617,0.0069,0.1225,0.0567,0.0333,0.0035,0.0007,-0.0563,-0.0292,-0.0395,-0.0125,-0.0082,0.0333,0.027,0.0337,-0.0493,-0.0322,0.0047,-0.0698,-0.066,0.1361,-0.0517,0.0241,-0.0454,-0.008,0.0151,0.048,-0.0138,-0.0168,0.0165,0.0382,-0.0012,0.0205,-0.0525,0.0424,-0.0446,-0.0646,-0.0135,0.1271,-0.0146,-0.0775,-0.0376,0.0225,0.0223,-0.0323,0.0309,0.0422,-0.0603,0.0652,0.1076,0.0255,-0.1038,-0.0195,0.0223,-0.001,0.0268,-0.0154,-0.0442,0.0096,-0.0083,-0.0667,-0.008,-0.0178,0.0347,0.0289,0.0053,0.0238,-0.0261,-0.0389,-0.0154,-0.0495,0.027,0.0213,0.0122,-0.0439,0.0099,0.0101,-0.0329,0.0517,-0.0143,0.0044,0.0213,-0.0059,0.02,0.0212,0.0009,-0.0237,0.008,-0.0138,-0.0174,0.0015,-0.0038,0.0146,-0.0059,0.0169,0.0056,-0.0494,-0.0641,-0.2377,-0.036,-0.0127,-0.0062,0.0289,-0.059,0.0682,-0.016,0.0253,0.0692,0.0391,-0.001,0.0123,0.0369,-0.0013,0.0604,0.0383,0.0179,-0.0653,-0.034,-0.0218,0.0455,-0.0453,-0.0653,0.0502,-0.0521,0.2813,0.0239,0.0437,-0.0206,0.0517,-0.0006,-0.0249,-0.1425,0.0671,0.0077,0.029,0.026,-0.0142,-0.0196,-0.0507,0.0015,0.0284,-0.1049,0.0061,-0.0243,-0.0565,0.0226,-0.0642,0.0085,0.0624,-0.04,0.0687,-0.0032,-0.0453,-0.0338,-0.0577,-0.011,-0.0449,0.051,-0.0115,-0.048,0.0265,-0.0484,0.0475,-0.0091,-0.0007,-0.0364,0.0287,-0.0217,-0.0486,0.0349,-0.0128,-0.0017,0.0767,0.0009,0.0141,0.0189,-0.0177,-0.0237,0.0482,-0.0159,0.0134,0.0567,0.0547,0.0188,0.0967,0.0005,0.0078,-0.0264,-0.0086,0.0098,-0.0279,-0.0089,0.0556,0.0401,-0.2629,0.0436,0.0006,0.0271,-0.0102,-0.0228,0.0081,0.0123,-0.0295,-0.0296,0.0524,0.0466,0.0797,0.0114,0.0132,0.0563,0.0459,-0.0238,0.0656,-0.0663,0.0014,0.0178,0.2331,-0.0294,-0.0017,0.0205,-0.0267,-0.0051,0.0311,-0.0503,-0.0199,0.0194,0.086,-0.0239,-0.0101,0.0747,-0.0374,0.0064,0.0377,-0.065,0.0299,0.0094,-0.0377,-0.0234,0.083,0.0103,-0.0035,-0.0371,0.0134,0.0453,-0.0109,-0.0131,0.0039,0.0217,0.0078,0.0261,-0.0582,-0.0386,-0.0079,-0.0145,-0.0094,-0.0562,-0.0438,-0.0489,-0.0055]}
{"key":"[An Enhanced Randomly Initialized Convolutional Neural Network for Columnar Cactus Recognition in Unmanned Aerial Vehicle Imagery] Recently, Convolutional Neural Networks (CNNs) have made a great performance for remote sensing image classification. Plant recognition using CNNs is one of the active deep learning research topics due to its added-value in different related fields, especially environmental conservation and natural areas preservation. Automatic recognition of plants in protected areas helps in the surveillance process of these zones and ensures the sustainability of their ecosystems. In this work, we propose an Enhanced Randomly Initialized Convolutional Neural Network (ERI-CNN) for the recognition of columnar cactus, which is an endemic plant that exists in the Tehuac\\'an-Cuicatl\\'an Valley in southeastern Mexico. We used a public dataset created by a group of researchers that consists of more than 20000 remote sensing images. The experimental results confirm the effectiveness of the proposed model compared to other models reported in the literature like InceptionV3 and the modified LeNet-5 CNN. Our ERI-CNN provides 98% of accuracy, 97% of precision, 97% of recall, 97.5% as f1-score, and 0.056 loss.","layer":1,"vector":[0.0036,-0.0342,0.0411,-0.0214,0.0809,0.0525,0.0487,0.0293,0.0269,0.0232,0.0221,-0.0889,0.0618,0.0553,0.0288,0.0372,0.0027,0.0566,-0.0451,-0.0055,0.0395,-0.0071,0.0041,-0.0129,-0.0487,-0.003,-0.043,0.0058,-0.0442,-0.226,0.0019,-0.0171,0.0627,-0.0456,-0.0128,-0.0218,-0.036,-0.0163,-0.0394,0.0144,0.0149,-0.0145,-0.0313,-0.0588,-0.0408,-0.0556,-0.0155,-0.049,0.0097,-0.0547,0.0024,-0.0479,0.0123,0.0191,0.0012,0.0278,0.068,0.0576,0.0272,0.0419,0.0664,0.0467,-0.2102,0.0523,-0.0035,0.0212,-0.0658,-0.0062,0.048,-0.032,0.0036,0.0623,-0.004,0.0172,0.0132,0.0043,-0.0059,-0.0136,-0.0098,0.0137,0.0332,0.0094,-0.0487,-0.0049,0.0099,-0.0264,0.0125,-0.0162,0.044,0.0249,-0.0186,-0.0239,-0.0602,0.0166,-0.0864,0.0059,0.0554,-0.0032,-0.0742,0.2201,-0.0572,-0.0161,0.0404,-0.0341,0.0294,-0.0152,-0.0279,-0.0247,-0.0352,0.0265,-0.0003,-0.0406,0.0134,0.0065,0.0032,-0.0408,0.0963,0.0699,0.0001,-0.0204,-0.032,0.0118,0.0179,-0.0134,0.0552,-0.0326,0.0237,0.1366,0.0106,0.0744,0.0844,-0.0304,-0.0276,-0.0468,-0.0182,0.0587,0.0556,0.022,-0.0062,-0.0547,-0.0407,-0.0374,0.0334,-0.1011,-0.044,0.0648,-0.0114,0.0493,-0.0364,-0.0484,-0.0226,0.0207,-0.0409,0.0046,0.0421,0.0301,0.0293,0.0228,-0.0243,0.0173,-0.0274,-0.0635,-0.0025,0.1021,0.0398,-0.0796,-0.0277,-0.0102,-0.0131,0.0192,0.0316,0.0134,-0.0306,0.0232,0.102,0.047,-0.0767,-0.0347,-0.0357,-0.0101,-0.0231,-0.0427,0.0009,0.0353,0.0383,-0.024,0.0107,-0.0333,-0.0029,0.0425,-0.0377,0.023,-0.0619,0.0163,0.0017,-0.0214,-0.0453,-0.0364,0.0148,-0.0256,0.0147,-0.0336,0.0024,0.0471,-0.0117,0.0013,0.0231,0.0524,0.0014,0.0447,-0.0276,0.0305,0.056,-0.007,-0.0721,0.0038,0.0061,0.0327,0.0111,0.0688,0.0453,-0.0727,-0.027,-0.223,-0.0062,0.0251,0.0322,0.0858,-0.0662,0.0366,0.0037,0.0618,0.027,0.0343,-0.0101,0.0586,0.0448,-0.0149,0.051,0.049,0.0261,-0.0391,-0.0187,0.0036,0.0204,0.014,-0.0612,0.0315,-0.008,0.1973,0.0201,0.0314,-0.0098,0.0171,0.0261,-0.0569,-0.1224,0.0603,-0.0241,0.028,0.0056,-0.0567,-0.024,0.0179,0.0221,0.03,-0.0846,-0.0392,-0.0226,-0.0811,0.0618,0.0027,0.0051,-0.0145,0.0017,0.0383,0.0051,0.0221,0.0051,-0.1366,0.0593,-0.0289,-0.0124,0.0036,-0.039,0.006,-0.084,0.0898,0.0022,-0.064,-0.0566,0.0109,0.0034,-0.0063,0.101,0.0411,-0.0397,0.0779,-0.0188,0.0496,-0.0252,-0.0396,-0.0489,0.0522,-0.0283,0.0393,0.0407,0.0114,0.035,0.0551,-0.027,0.0335,0.0117,0.0511,0.0136,-0.0714,0.006,0.0566,-0.0176,-0.3007,0.058,-0.0101,-0.0067,-0.0,0.037,0.0286,0.0272,-0.0195,-0.0161,-0.0067,0.0203,0.0764,-0.0497,-0.0036,0.0511,0.0044,-0.0416,0.0614,-0.061,0.0077,-0.0055,0.1891,-0.0571,-0.019,0.0083,-0.0376,-0.0489,-0.0066,-0.0374,0.0527,0.0387,0.0766,-0.0844,0.0404,0.0795,0.015,0.0282,0.0121,-0.015,-0.0485,0.0345,-0.0293,-0.0366,0.0351,-0.0505,0.0128,-0.0344,-0.0019,0.0202,-0.0163,-0.0434,-0.0571,-0.02,0.0202,0.0298,-0.0366,-0.0201,-0.078,-0.0166,0.0367,-0.0602,-0.0062,-0.0339,0.0071]}
{"key":"[Primal-Dual Block Frank-Wolfe] We propose a variant of the Frank-Wolfe algorithm for solving a class of sparse/low-rank optimization problems. Our formulation includes Elastic Net, regularized SVMs and phase retrieval as special cases. The proposed Primal-Dual Block Frank-Wolfe algorithm reduces the per-iteration cost while maintaining linear convergence rate. The per iteration cost of our method depends on the structural complexity of the solution (i.e. sparsity/low-rank) instead of the ambient dimension. We empirically show that our algorithm outperforms the state-of-the-art methods on (multi-class) classification tasks.","layer":27,"vector":[-0.0479,0.0194,0.0564,0.0014,0.0327,0.0494,0.0044,0.0316,0.0494,-0.0471,0.0236,-0.0381,-0.0116,0.0282,0.0309,0.0333,0.0777,0.0714,-0.0036,0.0263,0.0547,-0.0564,-0.0179,-0.1158,0.0283,0.0077,-0.0033,-0.0527,-0.041,-0.2639,0.0544,-0.0392,0.0762,-0.0235,0.0163,-0.0278,-0.0268,0.0666,-0.0718,0.0584,0.0238,0.003,-0.0313,-0.032,0.0179,-0.0482,-0.031,-0.034,-0.0247,-0.0518,0.0067,-0.064,0.0219,0.0397,0.0341,0.0343,0.0193,0.0489,0.0307,0.0429,0.0097,0.0249,-0.1513,0.0525,0.091,0.0382,-0.0218,-0.0187,0.0176,0.0748,-0.0157,0.0322,0.0527,0.0413,-0.0124,-0.0467,0.0051,-0.008,-0.0181,0.0263,0.011,-0.0117,-0.0059,-0.0424,-0.0622,-0.0224,0.0143,-0.0442,0.0116,0.011,-0.0498,-0.0435,0.0206,0.0301,-0.0603,-0.0423,0.0184,0.0438,-0.0571,0.2168,-0.0491,0.0471,0.0392,-0.0576,0.0062,-0.0503,-0.0535,-0.044,-0.025,-0.0205,-0.0146,0.0105,0.07,-0.0532,0.0118,-0.0313,0.0201,0.0238,-0.0038,0.0466,-0.0218,-0.012,0.059,-0.003,0.0477,-0.046,0.037,0.1117,0.0526,0.0559,0.0194,-0.038,-0.0283,-0.057,0.0136,0.0207,0.0124,0.0295,-0.0134,-0.0034,-0.0148,-0.0762,0.0173,-0.0742,-0.0434,0.1212,-0.0535,0.0095,-0.0435,-0.0536,-0.033,0.0017,-0.011,-0.0366,0.0309,0.0136,0.0132,0.0135,-0.0534,-0.0041,-0.0182,-0.0276,-0.0368,0.1102,0.0202,-0.0826,-0.003,-0.0196,-0.0149,-0.0237,0.047,0.0588,0.0157,0.0562,0.0901,0.0587,-0.0646,-0.0123,-0.0275,-0.0059,0.0025,-0.0454,-0.0161,0.0189,0.0596,-0.0456,0.0189,-0.0361,-0.0219,0.0162,-0.0454,-0.0255,-0.0205,0.0036,-0.0214,-0.0332,0.0098,0.0069,0.0237,-0.0185,0.0526,-0.0037,-0.0387,0.0309,0.0291,0.018,-0.0133,-0.0367,0.0148,0.0217,-0.0296,-0.008,0.0586,-0.0456,-0.0494,-0.0427,0.0235,0.0638,-0.0314,0.0724,0.046,-0.0536,-0.0765,-0.2139,-0.0173,0.0111,0.0181,-0.0073,-0.062,0.0839,-0.0303,0.016,0.0502,0.0424,0.0311,-0.0289,0.0223,-0.0076,0.0412,0.0606,0.04,-0.014,-0.01,0.0016,0.0362,0.0162,-0.0438,0.0832,-0.004,0.1968,0.0296,0.0392,-0.0165,-0.0143,0.0056,-0.0316,-0.0794,0.0525,0.002,0.1018,-0.0149,-0.0407,-0.0327,-0.022,-0.0047,0.0082,-0.0669,-0.0379,-0.0197,-0.0253,-0.0315,-0.0638,0.0229,0.0288,-0.0345,0.0674,-0.031,-0.0016,-0.0289,-0.1135,0.0009,-0.0242,0.0343,0.0057,-0.0832,0.0226,-0.0371,0.0709,0.0386,-0.0136,-0.0071,0.001,-0.0606,-0.0424,0.0353,0.0263,-0.0147,0.0389,-0.0044,0.0084,0.0102,-0.0415,-0.0209,0.1068,-0.0235,0.0453,-0.014,0.0034,0.0327,0.1295,0.0107,-0.0072,0.0101,-0.0229,0.0444,-0.0923,-0.0259,0.0369,-0.0307,-0.2767,-0.0038,0.043,-0.0133,-0.0435,0.0022,0.0557,0.012,-0.0384,0.0157,-0.0223,0.0342,0.0488,-0.0205,0.0336,0.0243,0.0351,-0.0345,0.0501,-0.0903,-0.0296,0.0648,0.2034,-0.0631,0.0052,0.0223,-0.0047,0.0292,-0.0111,-0.0119,0.0525,-0.0268,0.0994,-0.0609,0.0309,0.0727,-0.0435,0.0481,-0.0014,-0.009,0.0232,0.0234,-0.0686,-0.0393,0.1023,0.0152,0.0021,-0.0333,-0.0107,0.0108,-0.0454,0.0493,0.0329,-0.0015,0.0229,0.0197,-0.0711,-0.0135,-0.0433,-0.0239,0.0444,-0.0337,-0.0649,-0.0054,-0.0102]}
{"key":"[Teach me how to Label: Labeling Functions from Natural Language with Text-to-text Transformers] Annotated data has become the most important bottleneck in training accurate machine learning models, especially for areas that require domain expertise. A recent approach to deal with the above issue proposes using natural language explanations instead of labeling individual data points, thereby increasing human annotators' efficiency as well as decreasing costs substantially. This paper focuses on the task of turning these natural language descriptions into Python labeling functions by following a novel approach to semantic parsing with pre-trained text-to-text Transformers. In a series of experiments our approach achieves a new state of the art on the semantic parsing benchmark CoNaLa, surpassing the previous best approach by 3.7 BLEU points. Furthermore, on a manually constructed dataset of natural language descriptions-labeling functions pairs we achieve a BLEU of 0.39. Our approach can be regarded as a stepping stone towards models that are taught how to label in natural language, instead of being provided specific labeled samples. Our code, constructed dataset and models are available at https://github.com/ypapanik/t5-for-code-generation.","layer":0,"vector":[-0.039,-0.033,-0.0219,-0.0014,0.0385,-0.0327,0.0234,0.0634,-0.0031,-0.0136,-0.0206,-0.105,0.0498,0.0745,0.0315,0.0125,0.0041,0.0372,-0.0357,-0.0036,0.0799,-0.0062,-0.0124,-0.0293,0.0332,0.0549,-0.0037,-0.0079,-0.0319,-0.2105,0.03,-0.0556,0.0447,0.0178,-0.0034,-0.0111,-0.0344,0.0172,0.0156,0.0248,0.0367,-0.0251,-0.0285,-0.0373,-0.0392,-0.0343,-0.047,-0.0517,-0.0793,-0.0186,-0.0123,-0.0147,0.0018,0.0341,0.0153,0.0431,0.0551,0.0262,0.0749,0.0254,0.0061,0.038,-0.1596,0.0727,0.0179,0.009,-0.0871,-0.0076,-0.0234,0.0634,-0.0059,0.0405,0.0196,0.053,-0.015,0.0015,-0.0104,-0.0227,-0.0129,0.0025,-0.0085,0.0056,-0.0539,-0.0125,-0.0216,-0.0422,0.0314,-0.0191,0.0446,0.0164,0.0031,-0.0359,-0.063,0.0739,-0.0467,0.0243,0.0464,-0.0045,-0.078,0.1903,-0.0742,0.0501,-0.0132,-0.063,0.0476,-0.0421,-0.0349,-0.0274,-0.0079,-0.0471,-0.0437,0.0086,0.0119,-0.0621,0.0689,0.018,0.0999,-0.0041,-0.0261,-0.0105,-0.0116,-0.0073,0.0172,-0.0232,0.0474,0.0194,0.061,0.1239,0.0719,0.0479,0.0431,0.0148,-0.0611,0.0312,0.0274,0.0347,0.0104,0.0009,0.0563,-0.0076,-0.0103,-0.0755,-0.0294,-0.0674,-0.0389,0.126,-0.0217,0.0266,-0.0505,0.0054,-0.0243,0.0587,-0.0024,-0.0346,0.0261,0.0508,0.0735,0.0346,-0.0348,-0.0129,0.0149,-0.0609,-0.056,0.0729,-0.0003,-0.113,-0.034,-0.0118,-0.0179,-0.0231,0.0697,0.0322,-0.0314,0.0435,0.05,0.0268,-0.0755,-0.023,0.0139,0.039,0.0204,-0.059,-0.0459,0.0196,0.0205,-0.0576,-0.029,-0.0774,0.0301,0.0313,0.0011,0.0491,-0.0058,-0.044,-0.0337,-0.0147,0.0269,-0.0005,-0.0018,-0.0252,-0.0346,0.0333,-0.0402,-0.001,-0.0253,-0.0316,-0.0027,0.0093,0.0372,0.0098,-0.034,-0.0148,0.0592,0.0028,-0.0259,-0.0378,0.0177,0.0171,0.023,0.0511,-0.0165,-0.0379,-0.0032,-0.2216,0.0163,0.0407,-0.06,0.0319,-0.0515,-0.0079,0.0246,0.0247,0.0835,0.0482,-0.0564,-0.0218,-0.0167,-0.0331,0.0341,0.0661,0.0239,-0.0078,0.0362,0.0175,-0.0256,0.003,-0.1106,0.0346,-0.0288,0.2736,0.0403,0.0719,-0.0104,0.0554,0.0126,-0.0249,-0.1125,0.0647,0.0314,0.0267,0.0072,-0.0113,-0.0365,0.0006,0.0313,0.0168,-0.0893,-0.0358,-0.03,-0.0365,-0.0324,-0.0287,0.0654,0.0147,0.0114,0.064,-0.0095,-0.0432,-0.0356,-0.1019,0.0113,-0.0444,-0.0144,-0.024,-0.0404,0.0455,-0.0732,0.0347,0.0195,-0.0277,-0.0247,0.0093,-0.0368,-0.0593,0.0965,-0.0038,-0.0193,0.059,0.0515,0.0026,-0.0357,-0.0437,-0.0461,0.0524,-0.0019,0.0499,-0.036,0.0326,-0.0154,0.0929,-0.005,0.0592,-0.0141,0.0279,0.01,-0.0306,0.026,0.0691,0.0094,-0.2805,0.0484,0.0546,0.0305,-0.0105,0.0582,0.0888,0.0012,-0.0545,0.0055,-0.0406,-0.0108,0.0337,-0.0272,-0.0002,0.0394,0.0713,-0.0561,0.0183,-0.0236,0.0083,0.014,0.1867,-0.0109,0.0343,0.0171,-0.0437,-0.0062,0.0695,0.0186,0.0182,-0.0053,0.0702,-0.0344,0.019,0.0669,-0.0238,-0.011,0.0251,0.0049,0.0085,0.0063,-0.0672,-0.0493,0.0527,0.0061,-0.0124,-0.11,-0.0134,-0.0336,-0.0203,0.0482,-0.0303,0.0046,0.0572,0.0142,-0.0155,-0.0323,-0.0506,-0.0141,-0.0125,-0.0948,0.0262,0.0388,-0.0264]}
{"key":"[Introducing ECAPA-TDNN and Wav2Vec2.0 Embeddings to Stuttering Detection] The adoption of advanced deep learning (DL) architecture in stuttering detection (SD) tasks is challenging due to the limited size of the available datasets. To this end, this work introduces the application of speech embeddings extracted with pre-trained deep models trained on massive audio datasets for different tasks. In particular, we explore audio representations obtained using emphasized channel attention, propagation, and aggregation-time-delay neural network (ECAPA-TDNN) and Wav2Vec2.0 model trained on VoxCeleb and LibriSpeech datasets respectively. After extracting the embeddings, we benchmark with several traditional classifiers, such as a k-nearest neighbor, Gaussian naive Bayes, and neural network, for the stuttering detection tasks. In comparison to the standard SD system trained only on the limited SEP-28k dataset, we obtain a relative improvement of 16.74% in terms of overall accuracy over baseline. Finally, we have shown that combining two embeddings and concatenating multiple layers of Wav2Vec2.0 can further improve SD performance up to 1% and 2.64% respectively.","layer":3,"vector":[-0.0649,-0.0149,0.0327,-0.0083,0.0473,0.0736,0.0001,-0.0008,0.0487,-0.0594,-0.0067,-0.042,0.0644,0.031,0.0296,0.0216,0.0651,0.0327,-0.046,-0.0066,0.0148,0.0102,-0.0036,-0.0646,0.0101,0.0289,-0.0559,-0.0747,-0.0618,-0.2289,0.0301,-0.0693,0.0634,-0.0617,-0.0113,-0.0541,-0.0122,0.0437,-0.0739,0.0256,-0.019,0.0357,-0.0114,-0.1351,-0.0153,-0.0711,-0.027,-0.0306,0.0186,-0.0368,0.0102,-0.0839,0.0609,-0.0126,0.0322,0.0272,0.0322,0.0583,0.0364,0.0601,0.0243,0.0091,-0.1818,0.0687,0.0423,0.0092,0.0098,-0.0204,0.0058,0.0403,-0.0307,0.0003,0.0307,0.0671,0.0226,-0.0204,0.0315,-0.0231,0.038,0.0036,0.0222,-0.0014,-0.0287,-0.0673,0.0089,-0.0633,0.0241,-0.0332,-0.0276,-0.024,-0.0643,0.0342,-0.0306,0.0252,-0.0498,-0.0307,0.0476,0.0285,-0.0177,0.173,0.0099,0.0108,0.0519,-0.0596,0.0501,-0.0556,-0.0226,-0.0321,-0.0443,0.0019,-0.0186,-0.0215,0.0305,-0.0258,0.0593,0.0051,0.0617,0.0225,-0.0206,0.0356,-0.033,0.0106,0.0355,-0.0576,0.054,-0.0628,0.0524,0.1069,0.0324,0.0173,0.031,-0.0165,-0.0395,0.0497,0.0016,0.0206,0.0123,0.0186,0.0145,-0.0144,-0.0282,-0.0555,-0.0096,-0.0668,-0.0661,0.0744,-0.0523,-0.0023,-0.0437,-0.0329,0.0062,-0.0059,-0.0168,-0.0402,0.0276,0.0155,0.0474,0.0749,-0.0461,0.0247,0.0405,-0.0267,-0.0608,0.0912,-0.0058,-0.1119,-0.0379,-0.0226,-0.0025,-0.0232,0.0362,-0.0024,-0.052,0.0141,0.0826,0.0115,-0.0486,0.0023,0.0072,-0.0168,0.009,-0.0418,-0.0464,0.0239,0.0654,-0.0619,0.0013,-0.0839,0.0175,0.0467,-0.0501,-0.0141,-0.0416,0.0052,-0.0285,-0.0011,0.0159,0.0197,0.0366,0.0059,0.0058,0.0048,-0.0351,0.0215,-0.0073,0.0421,-0.081,0.0342,0.0537,0.0247,-0.0173,-0.024,0.0589,-0.0371,-0.0127,0.0101,-0.0066,0.0475,-0.0013,0.0366,0.0336,-0.0686,-0.0706,-0.2141,-0.0011,0.0441,-0.0372,0.0244,-0.0614,0.0303,0.0121,0.1017,0.0574,0.0411,-0.0045,-0.0193,-0.0153,0.0062,0.0475,0.0048,0.0252,-0.0254,-0.0031,0.0355,0.0333,-0.0333,-0.0552,0.0383,0.0113,0.2033,-0.0116,0.0374,-0.0437,-0.0102,0.0314,0.002,-0.1552,0.0109,0.0294,0.0735,0.01,-0.0433,-0.0279,-0.0657,0.0142,-0.0265,-0.1107,-0.056,0.0167,-0.0311,-0.0102,-0.0589,0.0093,0.0831,0.0188,0.0274,0.0026,-0.0242,-0.0163,-0.1155,0.0373,-0.0484,0.0263,0.016,0.0047,0.0408,-0.0455,0.019,-0.0019,-0.0442,-0.0229,0.031,0.0017,0.0105,0.0915,-0.0263,0.0165,0.0763,-0.0106,0.0531,-0.0332,-0.0304,-0.0263,0.0811,-0.0024,0.0575,0.0224,0.0417,0.0115,0.0955,0.0247,0.0449,0.0144,0.0246,-0.018,-0.0123,-0.0114,0.0238,0.0192,-0.2839,0.0156,0.0365,0.0079,-0.0478,0.0219,0.0139,-0.0233,-0.0525,0.0346,-0.0206,0.0746,0.026,-0.0288,-0.0081,0.0534,0.1152,-0.0506,0.0857,-0.0478,-0.0166,0.0127,0.2335,-0.0087,0.0085,-0.0377,-0.0069,-0.0034,0.0339,-0.0403,0.0175,0.0153,0.0856,-0.0445,-0.0185,0.0668,-0.0729,0.0313,0.0402,0.04,0.0159,-0.0053,-0.0262,-0.0361,0.039,-0.0149,0.0233,-0.0118,-0.0075,0.069,-0.0066,-0.0173,0.0212,0.008,0.0233,0.0557,-0.0534,-0.0306,-0.0236,-0.0331,0.0477,-0.0386,-0.0234,0.0173,0.0124]}
{"key":"[Risk-Averse Explore-Then-Commit Algorithms for Finite-Time Bandits] In this paper, we study multi-armed bandit problems in explore-then-commit setting. In our proposed explore-then-commit setting, the goal is to identify the best arm after a pure experimentation (exploration) phase and exploit it once or for a given finite number of times. We identify that although the arm with the highest expected reward is the most desirable objective for infinite exploitations, it is not necessarily the one that is most probable to have the highest reward in a single or finite-time exploitations. Alternatively, we advocate the idea of risk-aversion where the objective is to compete against the arm with the best risk-return trade-off. Then, we propose two algorithms whose objectives are to select the arm that is most probable to reward the most. Using a new notion of finite-time exploitation regret, we find an upper bound for the minimum number of experiments before commitment, to guarantee an upper bound for the regret. As compared to existing risk-averse bandit algorithms, our algorithms do not rely on hyper-parameters, resulting in a more robust behavior in practice, which is verified by the numerical evaluation.","layer":3,"vector":[-0.0663,0.0018,0.0146,-0.0324,0.0114,0.0007,0.0429,0.0119,0.0358,-0.0405,0.0288,-0.0437,0.0279,0.0481,-0.0163,0.0273,-0.055,0.0184,-0.029,0.0115,0.0455,-0.0712,0.0227,-0.1041,0.0428,0.0006,-0.0461,-0.0589,-0.0352,-0.2204,0.0036,-0.0238,0.0153,-0.0476,-0.015,-0.016,-0.0297,0.0641,-0.0189,0.0781,0.0485,0.0214,-0.058,-0.0616,-0.0233,-0.0897,0.0211,-0.0458,0.0177,0.0118,0.0001,-0.0469,0.0431,0.0247,0.0257,0.0187,0.0339,0.0671,0.0082,0.0357,0.0254,0.0024,-0.1311,0.0088,0.0555,0.0285,-0.0383,-0.0027,0.0327,0.0606,0.0035,0.0455,-0.0178,0.0464,0.0319,0.0176,-0.0099,-0.0525,0.0157,0.0052,-0.0137,-0.0395,-0.0129,0.0335,-0.0543,-0.0569,0.0281,-0.0304,0.0702,0.0562,-0.0158,-0.0271,-0.0192,0.0241,-0.0584,-0.0029,0.0687,0.0398,-0.0333,0.2082,-0.0602,0.0652,-0.0092,-0.0255,0.038,-0.046,-0.0398,-0.0348,-0.0103,-0.0139,0.0113,0.0091,0.0705,-0.0221,-0.0167,0.0674,0.0226,0.0532,0.0062,-0.0076,-0.0291,0.0191,0.1087,-0.0062,-0.0065,-0.0699,0.0378,0.13,0.0242,0.0068,0.0218,-0.0824,-0.0294,-0.029,0.0331,0.0139,-0.0274,0.0167,0.0481,-0.0257,-0.0242,-0.0015,0.0495,-0.1193,-0.0348,0.0938,0.0421,0.0535,-0.051,-0.0736,-0.0358,-0.0246,0.0172,-0.0521,0.0054,0.0302,0.0581,0.0175,-0.0078,0.0307,-0.061,-0.0707,0.0273,0.1037,0.0038,-0.0833,-0.0176,-0.0077,-0.0127,0.0032,-0.0036,0.0267,-0.0815,0.0168,0.0814,-0.0274,-0.0644,-0.0057,-0.0046,-0.0069,0.0005,-0.0153,-0.0355,0.0515,0.0417,-0.0034,0.0024,-0.0388,0.037,0.048,-0.0306,0.0174,0.0129,0.0192,-0.0466,-0.0773,-0.0159,-0.0121,0.0169,-0.0062,0.0133,-0.059,-0.0518,0.0006,0.059,0.0288,0.0091,-0.0498,0.0174,0.0157,-0.0213,0.0228,0.0402,-0.0256,-0.0426,0.0036,0.0361,0.0864,-0.0007,0.0665,0.0141,-0.011,0.031,-0.2531,-0.0171,-0.0235,0.0253,0.0576,-0.0522,0.021,-0.0814,0.0387,0.0519,0.0539,-0.0747,-0.0287,0.0559,-0.0027,0.0483,-0.0093,-0.0018,0.0001,0.006,-0.0018,0.0083,-0.0634,-0.0562,0.0096,-0.0064,0.248,0.0563,0.0102,-0.0541,0.0602,0.0143,0.0049,-0.0919,0.0103,0.0536,0.0409,-0.0177,-0.0194,-0.0632,0.0014,0.0131,-0.0185,-0.055,-0.0615,-0.0219,-0.0611,0.0187,-0.0333,0.0003,0.0322,-0.0025,0.048,-0.0386,-0.0262,-0.0319,-0.0701,0.0159,-0.0374,0.0548,0.0125,-0.0243,-0.0054,-0.0577,0.0937,-0.0284,0.0067,-0.0084,0.0385,-0.069,-0.0075,0.0404,-0.0109,-0.0031,0.0114,0.04,0.0139,-0.0356,-0.0402,-0.0301,0.038,-0.0649,0.0171,-0.0054,-0.026,-0.0001,0.084,-0.0189,0.0153,0.0097,-0.0017,0.0004,-0.0706,0.0198,0.0796,-0.0162,-0.2949,0.0588,0.0375,0.0176,-0.0268,0.045,0.0281,-0.0015,-0.0401,0.012,0.0324,0.0955,0.0321,0.0065,0.032,0.0229,0.073,-0.0219,0.0224,-0.0625,0.0255,0.0458,0.2016,-0.0422,0.012,0.0144,-0.0486,0.0152,0.0106,-0.0048,0.022,-0.0082,0.0696,-0.0804,0.0362,0.1188,-0.052,0.0248,0.0085,0.0236,-0.0226,0.0274,-0.0485,0.0185,0.1174,0.0036,-0.039,-0.0463,-0.0078,0.0234,-0.0183,-0.0084,-0.0265,-0.0124,0.04,-0.0193,-0.0296,-0.0416,-0.0081,-0.0165,0.0424,-0.0076,0.0152,-0.0202,0.0044]}
{"key":"[Unconditional Audio Generation with Generative Adversarial Networks and Cycle Regularization] In a recent paper, we have presented a generative adversarial network (GAN)-based model for unconditional generation of the mel-spectrograms of singing voices. As the generator of the model is designed to take a variable-length sequence of noise vectors as input, it can generate mel-spectrograms of variable length. However, our previous listening test shows that the quality of the generated audio leaves room for improvement. The present paper extends and expands that previous work in the following aspects. First, we employ a hierarchical architecture in the generator to induce some structure in the temporal dimension. Second, we introduce a cycle regularization mechanism to the generator to avoid mode collapse. Third, we evaluate the performance of the new model not only for generating singing voices, but also for generating speech voices. Evaluation result shows that new model outperforms the prior one both objectively and subjectively. We also employ the model to unconditionally generate sequences of piano and violin music and find the result promising. Audio examples, as well as the code for implementing our model, will be publicly available online upon paper publication.","layer":2,"vector":[-0.0366,-0.0316,0.0293,-0.0546,-0.0006,0.0262,-0.0353,-0.024,0.0364,-0.0151,0.0109,-0.0016,0.0446,0.047,0.043,0.0151,0.0497,0.0277,-0.0591,0.023,0.0264,0.0131,0.0258,-0.0565,0.0088,-0.0157,-0.0278,-0.076,-0.0283,-0.2284,0.0045,-0.0515,0.0824,-0.0507,-0.0307,-0.0356,-0.0428,0.028,-0.0082,0.0602,0.0271,0.0302,-0.0366,-0.0986,0.0122,-0.0254,-0.0494,0.0021,-0.0111,-0.0243,-0.0051,-0.035,0.0269,0.0456,0.015,0.011,0.0419,0.0396,0.0222,0.0763,-0.0023,0.0284,-0.1579,0.0416,0.0146,0.0646,-0.0271,-0.0152,0.0145,0.0498,-0.0209,0.0227,0.0295,0.0179,0.0044,-0.0241,0.0017,-0.0668,-0.0814,0.0391,0.0181,-0.0299,-0.0232,-0.0119,0.0298,-0.035,-0.023,-0.0465,-0.0017,0.0066,-0.0699,0.0272,-0.0469,0.056,-0.0155,-0.0347,0.0514,0.0203,-0.0448,0.2246,-0.0629,0.0382,0.0693,-0.0412,-0.0201,0.0182,-0.0932,-0.0322,-0.0372,-0.0055,0.007,-0.0091,0.0292,-0.0429,0.0371,0.0276,0.0368,0.0231,-0.0164,-0.056,-0.0465,0.0319,0.0157,-0.0324,0.0911,-0.0424,0.0185,0.0972,0.0683,0.0539,0.0713,-0.002,-0.0728,-0.004,0.0193,-0.0138,0.0124,-0.0122,0.025,0.0042,-0.0324,-0.0617,-0.0578,-0.0405,-0.0011,0.1015,-0.0166,0.028,-0.0245,0.0021,-0.0101,-0.0059,-0.0021,-0.0414,0.0414,0.0667,0.0066,0.0424,-0.0445,0.0274,-0.0244,-0.0499,-0.0543,0.1135,0.0097,-0.0914,-0.0597,-0.0179,0.0047,0.0075,-0.0185,-0.003,-0.0546,0.0467,0.0917,0.0339,-0.0727,0.0199,-0.002,0.0243,0.0235,-0.0632,-0.0169,0.0011,0.0296,-0.0593,0.031,-0.0563,0.0062,0.0613,-0.0051,0.0239,-0.0086,0.0131,-0.0625,-0.0625,-0.001,0.0092,-0.0028,-0.0224,0.0027,0.0101,-0.012,0.0226,-0.0067,0.0646,-0.0041,-0.0065,0.0509,0.0533,-0.0432,-0.0067,0.0839,0.0044,-0.0483,0.0161,-0.02,0.0532,-0.0292,0.0351,-0.008,-0.0696,-0.0591,-0.2206,0.0461,0.008,0.0039,0.0513,-0.0934,0.0157,-0.0158,0.0626,0.0049,0.043,0.0229,-0.0039,0.0445,-0.0038,0.0209,-0.0001,0.0379,0.0163,0.0295,0.0234,0.0264,-0.0106,-0.1115,0.048,-0.0092,0.2034,0.0414,0.0709,-0.0204,0.0369,0.0045,-0.0317,-0.0942,0.0325,0.0541,0.0783,0.0252,-0.0529,-0.0479,-0.0201,0.0462,-0.0302,-0.0559,-0.0437,-0.0127,-0.0271,0.0167,-0.0381,0.0183,0.0249,-0.011,0.0654,-0.0109,0.0148,-0.0992,-0.1633,0.025,-0.0396,0.0236,0.0295,-0.025,0.0402,-0.0643,0.016,0.0507,-0.0152,-0.0572,0.0279,-0.0342,0.0016,0.0973,0.0261,0.0173,0.0782,-0.007,0.0005,-0.0587,-0.0569,-0.0062,0.0368,-0.022,0.0265,0.0187,0.0068,0.0175,0.0515,0.0307,0.0228,-0.0439,-0.0143,0.052,-0.0518,0.031,-0.0092,0.0008,-0.3007,0.002,0.0119,0.0523,-0.0337,-0.0044,0.024,0.0556,-0.0915,0.0212,-0.0355,0.0229,0.037,-0.0397,0.027,0.0326,0.1147,-0.0431,0.0407,-0.0435,0.0177,0.0498,0.2057,-0.0473,-0.0109,-0.0104,-0.0411,0.017,0.0213,-0.0432,-0.0168,0.0307,0.1253,-0.0401,-0.0059,0.061,-0.0199,0.0186,-0.0038,-0.0371,-0.0244,0.0223,-0.033,0.0013,0.0741,-0.016,0.0159,-0.0163,0.0127,0.0377,-0.0156,-0.0078,-0.0073,0.0447,0.0172,0.0337,-0.033,-0.0318,0.0516,-0.0037,0.019,-0.0388,-0.0156,0.0135,-0.0409]}
{"key":"[Towards Robust Classification with Deep Generative Forests] Decision Trees and Random Forests are among the most widely used machine learning models, and often achieve state-of-the-art performance in tabular, domain-agnostic datasets. Nonetheless, being primarily discriminative models they lack principled methods to manipulate the uncertainty of predictions. In this paper, we exploit Generative Forests (GeFs), a recent class of deep probabilistic models that addresses these issues by extending Random Forests to generative models representing the full joint distribution over the feature space. We demonstrate that GeFs are uncertainty-aware classifiers, capable of measuring the robustness of each prediction as well as detecting out-of-distribution samples.","layer":0,"vector":[-0.0057,-0.0086,0.0156,-0.0525,0.0566,0.0479,0.0252,-0.014,0.0543,0.0129,-0.0025,-0.0663,0.0244,0.0531,0.0405,0.0363,0.0305,0.0689,-0.051,-0.0329,0.0506,-0.0182,0.0234,-0.0447,0.0047,-0.0114,-0.0106,-0.0474,-0.0723,-0.2329,0.0206,-0.0577,0.0303,-0.0545,0.0273,-0.022,-0.0567,0.0567,-0.0269,0.0072,0.0119,0.013,-0.0644,-0.0638,0.0057,-0.0627,0.0003,-0.0296,-0.0545,-0.0245,0.0099,-0.037,0.0312,0.0474,0.0248,0.0259,0.0715,0.0326,0.0158,0.0814,0.0282,0.0614,-0.1166,0.0449,0.0388,0.0409,-0.0579,-0.0243,0.0069,0.0427,-0.0099,0.0282,-0.0159,0.0332,-0.001,-0.0315,-0.0095,-0.0285,-0.0214,0.037,0.0055,-0.0254,-0.0656,-0.0036,-0.0197,-0.0084,0.0164,-0.0469,0.0544,0.0498,-0.029,0.0,-0.0786,0.0369,-0.0614,0.0115,0.0574,0.0003,-0.0592,0.1977,-0.0691,0.0415,0.0444,-0.0427,0.0158,-0.017,-0.0348,-0.0269,-0.0588,-0.0355,0.0196,-0.0337,0.0117,-0.0081,-0.0445,-0.0391,0.1111,0.0302,-0.021,-0.0248,-0.0563,0.0107,0.0602,-0.0383,0.0691,-0.0398,0.044,0.1369,0.0467,0.0211,0.0154,-0.0217,-0.0506,-0.0423,0.0259,0.0302,0.0077,0.0209,0.0336,0.0015,0.0044,-0.0444,0.0021,-0.0475,-0.0731,0.0845,-0.0634,0.0284,-0.0522,-0.0458,-0.0082,-0.0223,-0.0005,-0.0569,0.0177,0.0201,0.0526,0.0453,-0.0264,-0.0058,-0.0059,-0.0604,0.0194,0.0874,-0.0212,-0.0879,-0.0451,0.0172,0.0001,0.0157,0.0529,0.0178,0.0373,0.0722,0.0644,0.0409,-0.0517,-0.0126,0.0102,0.0089,-0.0205,-0.0405,0.0036,0.0301,0.034,-0.0391,-0.0067,-0.062,0.0609,0.0405,0.0132,-0.001,-0.0013,-0.0032,0.0211,-0.0556,-0.0454,-0.0243,0.0395,-0.0314,0.0004,0.005,-0.0262,0.0038,-0.0518,0.0389,-0.0072,0.045,0.0609,0.0066,-0.0322,0.0331,0.0755,-0.0297,-0.0534,0.0084,0.0094,0.0424,-0.0242,0.0446,0.0495,-0.0469,-0.0339,-0.221,0.0008,0.0181,-0.0265,0.0079,-0.0812,-0.0036,-0.0261,-0.0179,0.0677,0.0437,0.0069,-0.0055,0.0327,-0.0138,0.0465,0.0091,-0.0076,-0.0371,0.0155,-0.0113,0.0476,-0.0051,-0.1227,0.0128,0.0419,0.2215,0.0169,0.0518,-0.0034,0.0628,0.053,-0.0347,-0.0821,0.0618,0.0478,0.0452,-0.0249,-0.0298,-0.0081,0.0245,0.0396,0.0033,-0.1278,-0.0049,-0.0439,-0.0653,0.0277,-0.0695,0.0081,0.0469,-0.0227,0.0973,-0.021,0.015,-0.0411,-0.0843,0.0558,-0.0101,0.0061,0.0302,-0.0596,0.0267,-0.0688,0.0026,-0.0173,-0.048,-0.0318,0.0551,-0.0549,0.0079,0.0896,-0.0046,0.0007,0.0879,0.0081,0.0197,-0.0011,-0.0284,-0.0214,0.0751,-0.0094,0.0382,0.0074,0.022,0.0021,0.0948,-0.0108,0.0575,-0.0297,-0.0006,0.0256,-0.0456,-0.0498,0.071,0.0175,-0.2845,0.0108,-0.0177,0.0419,-0.0068,-0.0002,0.0572,0.0017,-0.0185,0.0107,0.016,0.0463,0.0541,-0.0521,-0.0244,0.0171,0.0744,-0.052,0.0734,-0.0692,0.0405,0.0022,0.2243,-0.0152,0.0133,0.0125,-0.0119,-0.0041,0.025,-0.0287,-0.0266,0.0222,0.1016,-0.0631,0.0332,0.1177,-0.0494,0.0443,-0.0131,-0.0322,-0.0015,-0.0274,-0.022,-0.0468,0.0631,-0.0349,-0.0179,0.0053,-0.0014,0.0337,-0.0416,-0.0048,-0.0488,-0.0039,0.0194,0.0222,-0.0047,-0.0447,-0.0522,-0.039,0.0337,-0.0557,-0.0509,-0.0255,-0.0382]}
{"key":"[Learning to Reason: Leveraging Neural Networks for Approximate DNF Counting] Weighted model counting (WMC) has emerged as a prevalent approach for probabilistic inference. In its most general form, WMC is #P-hard. Weighted DNF counting (weighted #DNF) is a special case, where approximations with probabilistic guarantees are obtained in O(nm), where n denotes the number of variables, and m the number of clauses of the input DNF, but this is not scalable in practice. In this paper, we propose a neural model counting approach for weighted #DNF that combines approximate model counting with deep learning, and accurately approximates model counts in linear time when width is bounded. We conduct experiments to validate our method, and show that our model learns and generalizes very well to large-scale #DNF instances.","layer":0,"vector":[-0.0645,-0.0039,0.0284,-0.0366,0.0186,0.0641,0.0591,0.0359,0.054,-0.0229,0.0062,-0.0783,0.0395,0.0807,0.0105,0.0172,-0.0441,0.0511,-0.0482,-0.0039,0.0468,-0.0148,-0.0085,0.0011,0.0047,0.0383,-0.0226,0.0001,-0.0303,-0.259,0.0,-0.062,0.067,-0.0126,0.0134,-0.0565,-0.0715,-0.0156,-0.0351,0.0404,0.0385,0.0099,-0.0347,-0.0423,0.0076,-0.0461,0.006,-0.0036,-0.0416,-0.0422,0.0455,-0.0565,0.0432,0.0271,0.0387,0.0368,0.0522,0.01,0.0391,0.0354,0.028,0.0139,-0.1773,0.0445,0.0513,-0.0007,-0.0549,-0.0564,-0.0022,0.0508,-0.0153,0.0505,-0.0066,0.0868,-0.0157,0.0022,0.0079,-0.0344,0.0076,0.0214,0.032,-0.0171,-0.041,-0.0029,-0.0285,-0.0394,0.0086,-0.0154,-0.0022,-0.0159,0.0035,0.0226,-0.0542,0.0277,-0.062,-0.0233,0.033,0.0047,-0.0703,0.2088,-0.0464,0.0134,-0.0165,-0.0034,0.0252,0.0051,-0.0229,-0.0503,-0.0199,-0.0037,0.0314,-0.0044,0.0513,-0.0761,0.0519,0.021,0.1244,0.0117,-0.0146,-0.0177,-0.0144,-0.0394,0.0414,-0.0034,0.0259,-0.0662,0.0128,0.1598,0.0337,0.0159,0.0034,-0.0317,-0.0615,-0.0401,0.0414,0.0058,0.0326,0.0187,-0.0035,-0.0169,-0.041,-0.0195,-0.0005,-0.0877,-0.0818,0.1249,-0.0405,0.0188,-0.0506,-0.0446,0.0043,0.053,0.0022,-0.0597,0.0421,0.0471,0.0444,0.0195,-0.0554,0.0307,0.028,-0.0432,0.0039,0.0963,0.0031,-0.0639,-0.0278,0.0035,-0.0451,-0.0676,0.0386,-0.0124,-0.0462,0.0339,0.0662,0.0145,-0.0613,-0.0184,0.0441,0.0575,0.0062,-0.0527,-0.0542,0.0297,0.0274,-0.0485,-0.0056,-0.0835,0.0083,0.0389,0.0105,0.0369,-0.0185,0.0392,-0.0218,-0.0404,-0.0291,-0.0024,0.0478,-0.0161,-0.0024,-0.0376,-0.0573,0.0004,0.0285,0.022,-0.0551,0.0473,0.0401,0.0257,-0.0327,0.0457,0.0322,-0.031,-0.0063,0.0103,0.0182,0.0153,-0.0129,0.0263,0.0588,-0.0751,-0.0237,-0.2242,-0.0314,0.0438,-0.0142,0.061,-0.0498,0.0467,-0.0203,0.0243,0.0954,0.0091,-0.0094,-0.0586,-0.0146,-0.0204,0.0452,0.0118,0.0038,-0.0689,0.0033,-0.0014,0.0052,-0.0411,-0.0743,0.0388,0.0183,0.2325,0.048,0.0539,-0.0258,0.0369,0.0449,-0.0263,-0.0677,0.0882,-0.0172,0.0137,-0.0002,-0.0033,0.0105,-0.0049,0.0297,0.0035,-0.1194,-0.0654,-0.0135,-0.0186,-0.0067,-0.0161,-0.0012,0.0208,0.001,0.0657,0.011,-0.0216,-0.0305,-0.0951,-0.0244,-0.0496,0.0292,0.0079,-0.0381,0.0092,-0.0414,0.0259,-0.0197,-0.0383,-0.043,-0.0279,-0.0011,-0.0237,0.0781,0.0047,0.0171,0.0783,-0.0043,0.0291,-0.0485,-0.0471,-0.0386,0.0984,-0.0467,0.0057,0.0036,0.0348,0.045,0.067,-0.0158,0.0268,0.0097,0.0157,0.0235,-0.0337,0.0099,0.0245,0.0245,-0.3011,0.0415,-0.0207,0.033,-0.0311,0.0491,0.0286,0.058,-0.0294,-0.0316,0.0226,0.0567,0.0371,-0.0437,-0.041,0.0383,0.0351,-0.0248,0.094,-0.057,-0.001,0.0271,0.1667,-0.0118,0.038,0.0466,-0.0185,0.0154,0.0481,0.0275,-0.0045,-0.0198,0.0775,-0.0734,0.0564,0.0892,-0.0239,0.0137,0.0241,-0.0453,-0.0179,-0.0011,-0.03,-0.0023,0.1024,-0.0122,-0.0092,-0.0405,0.0298,0.0221,-0.0206,0.0178,-0.0004,-0.0026,0.052,0.0064,0.0019,-0.0182,-0.064,-0.0369,-0.019,-0.0638,0.0344,-0.0369,-0.0191]}
{"key":"[Self-Supervised Learning of Face Representations for Video Face Clustering] Analyzing the story behind TV series and movies often requires understanding who the characters are and what they are doing. With improving deep face models, this may seem like a solved problem. However, as face detectors get better, clustering/identification needs to be revisited to address increasing diversity in facial appearance. In this paper, we address video face clustering using unsupervised methods. Our emphasis is on distilling the essential information, identity, from the representations obtained using deep pre-trained face networks. We propose a self-supervised Siamese network that can be trained without the need for video/track based supervision, and thus can also be applied to image collections. We evaluate our proposed method on three video face clustering datasets. The experiments show that our methods outperform current state-of-the-art methods on all datasets. Video face clustering is lacking a common benchmark as current works are often evaluated with different metrics and/or different sets of face tracks.","layer":4,"vector":[-0.0262,-0.0193,0.0142,0.0137,0.0318,0.0612,0.0395,-0.034,-0.0025,0.0026,0.0688,-0.1322,0.0226,0.0627,0.0283,-0.01,0.0403,0.0708,-0.0424,-0.0181,0.0016,-0.0538,-0.0283,-0.0246,0.0358,0.0083,-0.0064,-0.0196,-0.0339,-0.2451,-0.0116,-0.0381,0.066,0.0308,0.019,-0.0706,-0.0231,0.0735,-0.0374,0.024,-0.0183,0.0055,-0.0245,-0.0336,-0.0457,-0.0188,-0.0326,0.0086,-0.013,-0.0639,0.0389,-0.0141,0.0265,0.0329,-0.0533,0.005,0.0437,0.015,0.0592,0.0645,0.04,0.0637,-0.1348,0.0504,0.0012,0.0366,-0.0072,0.0031,0.038,0.0328,-0.0538,0.0605,-0.0014,-0.0051,-0.0107,0.0225,-0.0051,-0.0104,-0.0224,0.014,-0.0178,-0.0079,-0.0344,-0.0157,-0.0129,-0.0272,0.0183,-0.0637,-0.0138,0.0336,-0.0439,0.0238,-0.0383,0.0163,-0.0748,-0.0558,0.0204,0.0046,-0.0484,0.2267,-0.0364,0.0145,0.084,-0.034,0.0235,-0.0316,-0.0177,-0.0424,-0.0376,-0.0017,-0.0166,-0.0095,-0.0202,-0.088,0.0566,-0.0075,0.0387,0.0686,-0.0239,-0.0662,0.0396,0.0263,0.0351,-0.0707,0.0279,-0.0606,0.0422,0.1173,0.0369,-0.0047,0.0509,-0.0035,-0.0229,0.0345,-0.0081,0.0275,0.0128,0.0092,0.0108,-0.0695,-0.0107,-0.0682,0.035,-0.0638,-0.0567,0.0928,-0.0405,0.0452,-0.019,-0.023,-0.0244,0.0293,-0.0952,-0.0019,-0.0211,-0.0134,0.0582,0.0229,-0.0727,0.0648,-0.0413,-0.0585,0.0096,0.0594,0.0252,-0.1048,-0.018,0.0112,0.0236,-0.0167,0.0408,0.0223,0.014,0.0582,0.0923,0.068,-0.0732,-0.0126,-0.0031,0.0168,-0.0001,-0.0189,-0.0531,0.0309,0.0375,-0.0453,-0.0032,-0.0402,0.0164,0.0367,-0.0116,0.0118,-0.0448,-0.0418,0.0071,-0.0093,-0.0118,-0.0319,0.0152,0.0113,-0.0376,0.0152,-0.0405,0.0525,-0.0045,0.0585,-0.0456,-0.0168,0.0649,0.0028,-0.0687,0.0152,0.0309,-0.0379,-0.039,-0.0398,0.0673,0.072,0.0132,0.0076,0.0546,-0.0367,-0.0659,-0.2024,0.0229,0.0185,-0.0297,0.0135,-0.0834,0.0098,0.0393,0.1019,0.0969,0.0444,-0.0167,-0.0605,0.0065,-0.0004,0.0408,0.0506,0.0751,-0.0314,0.0082,0.0191,0.0187,-0.0146,-0.085,0.0215,0.0276,0.2163,0.0489,0.0142,-0.0395,0.0457,0.0307,-0.0965,-0.0977,0.0565,-0.0326,0.0591,0.0026,-0.0434,-0.0522,-0.0509,-0.0258,0.007,-0.0782,-0.0008,0.0146,-0.0218,0.0286,-0.0507,-0.0095,0.0604,-0.0735,0.0294,0.0187,-0.0085,-0.0562,-0.0534,-0.0137,-0.0395,0.0406,-0.0164,-0.0577,-0.0061,-0.0799,0.091,-0.0047,-0.0573,0.0091,0.0345,0.0004,-0.0275,0.0754,-0.0215,-0.0057,0.0343,-0.0049,0.0623,-0.0153,-0.0552,-0.0016,0.0564,-0.0169,0.014,0.0131,0.0196,0.0357,0.0363,-0.0026,0.0732,-0.0395,-0.0083,0.0093,-0.0489,-0.0091,0.0269,0.0225,-0.2787,0.0332,-0.0334,0.0521,-0.0131,-0.0318,0.0296,0.0458,-0.0104,-0.0097,0.0101,0.0321,0.0885,-0.0035,-0.0069,0.0368,0.0337,-0.0282,0.0713,-0.0024,0.0229,0.0098,0.2205,-0.0468,-0.0038,0.016,-0.0147,0.0021,0.0565,-0.04,-0.0272,-0.0205,0.1135,-0.0028,0.0272,0.0815,-0.0327,0.0133,0.0147,0.0166,-0.0073,-0.0006,-0.0151,-0.0332,0.118,-0.016,0.0241,-0.0539,0.0052,0.0482,-0.0397,0.0153,-0.0182,-0.0222,0.0302,0.0258,-0.0536,0.001,-0.0338,-0.029,0.0152,-0.0356,-0.0448,0.0196,0.0003]}
{"key":"[HTTP2vec: Embedding of HTTP Requests for Detection of Anomalous Traffic] Hypertext transfer protocol (HTTP) is one of the most widely used protocols on the Internet. As a consequence, most attacks (i.e., SQL injection, XSS) use HTTP as the transport mechanism. Therefore, it is crucial to develop an intelligent solution that would allow to effectively detect and filter out anomalies in HTTP traffic. Currently, most of the anomaly detection systems are either rule-based or trained using manually selected features. We propose utilizing modern unsupervised language representation model for embedding HTTP requests and then using it to classify anomalies in the traffic. The solution is motivated by methods used in Natural Language Processing (NLP) such as Doc2Vec which could potentially capture the true understanding of HTTP messages, and therefore improve the efficiency of Intrusion Detection System. In our work, we not only aim at generating a suitable embedding space, but also at the interpretability of the proposed model. We decided to use the current state-of-the-art RoBERTa, which, as far as we know, has never been used in a similar problem. To verify how the solution would work in real word conditions, we train the model using only legitimate traffic. We also try to explain the results based on clusters that occur in the vectorized requests space and a simple logistic regression classifier. We compared our approach with the similar, previously proposed methods. We evaluate the feasibility of our method on three different datasets: CSIC2010, CSE-CIC-IDS2018 and one that we prepared ourselves. The results we show are comparable to others or better, and most importantly - interpretable.","layer":1,"vector":[-0.0557,-0.0436,-0.0113,0.0115,0.0241,0.0136,0.0628,0.0117,0.0119,-0.0416,0.0396,-0.0313,0.0232,0.0574,0.0364,0.0062,-0.0114,0.0071,-0.042,0.0174,0.0359,-0.013,-0.008,-0.0539,0.007,0.0387,-0.0208,-0.0137,-0.0815,-0.21,0.0572,-0.1131,0.0248,-0.0323,0.0091,-0.0279,-0.0103,0.0251,-0.0081,0.059,-0.0248,-0.0025,-0.0131,-0.0358,-0.0268,-0.0789,0.0139,0.0097,-0.0357,-0.0522,0.0072,-0.0153,0.0446,0.0382,0.0421,-0.009,0.0602,0.0322,0.0569,0.0554,0.0144,0.0448,-0.1429,0.0488,0.0389,0.0439,-0.0462,0.0045,0.0427,0.0317,-0.0241,0.0356,-0.0606,0.0773,-0.0024,0.0408,-0.0022,-0.0004,0.001,0.0234,-0.0012,-0.0344,-0.0117,-0.0051,-0.0054,-0.0628,0.0221,-0.0247,0.0721,-0.005,-0.0328,0.0069,-0.0194,0.0272,-0.0391,-0.03,-0.027,0.0387,-0.035,0.1976,-0.0253,0.0005,0.0264,-0.0262,0.0124,-0.0297,-0.0009,-0.0533,-0.0066,-0.0207,-0.0243,-0.0197,0.0014,-0.0372,0.0401,0.0293,0.0972,0.0541,-0.0457,0.0131,-0.0236,-0.0168,0.0609,-0.0449,0.052,-0.0573,0.063,0.143,0.0354,0.0378,-0.0054,0.021,-0.0524,0.0252,0.0058,0.0366,-0.0237,-0.0008,-0.0278,-0.0477,-0.0439,-0.0819,0.0498,-0.074,-0.0487,0.1252,-0.0302,-0.0006,-0.0321,-0.022,0.0226,0.0248,-0.0281,-0.0391,0.0317,0.0353,0.0465,0.0258,-0.0415,-0.005,0.0172,-0.0745,-0.0443,0.1167,0.037,-0.1168,-0.0169,0.0133,0.0046,-0.034,0.0462,0.0332,-0.0627,0.0326,0.0183,0.0089,-0.0704,-0.0062,-0.0056,-0.0189,0.0235,-0.042,-0.0676,0.0742,0.0622,-0.0667,-0.0445,0.0024,0.0744,0.0154,-0.0767,0.014,-0.0329,-0.0285,-0.0069,-0.0295,0.0203,0.0177,0.0123,-0.0823,0.063,0.0394,-0.0558,0.0185,-0.0367,-0.0007,0.0203,-0.0017,-0.0011,0.0126,-0.0042,-0.0137,0.0238,-0.0174,-0.0147,-0.0349,0.0248,0.0913,0.0418,0.0407,-0.0068,-0.0124,-0.0614,-0.2454,-0.0394,0.0242,-0.0743,0.0344,-0.053,0.0005,0.0097,0.0502,0.0662,0.0465,0.015,-0.0277,-0.0059,0.0033,0.0684,0.0453,0.0526,-0.0307,0.0245,-0.0062,-0.0185,-0.0205,-0.0468,0.0148,-0.0147,0.182,0.0857,0.0367,-0.121,0.0451,-0.0206,-0.0082,-0.1361,0.0611,0.0414,0.0356,0.0463,0.0065,-0.0166,-0.0201,0.0352,-0.0158,-0.0734,0.0009,-0.0214,-0.0333,-0.0138,-0.0647,0.0228,0.0338,0.0119,0.0298,0.0495,-0.0011,0.0026,-0.0818,0.0197,-0.0057,0.0289,0.0091,-0.0141,0.0292,-0.0862,0.071,-0.0121,-0.0279,-0.0441,0.0172,-0.0083,-0.0561,0.1354,-0.0161,0.0158,0.0287,0.013,0.0132,-0.0565,-0.0356,-0.0437,0.1022,-0.0146,0.0513,0.0123,0.0374,0.0133,0.049,-0.0003,0.0325,0.0164,0.0225,-0.033,-0.0363,-0.0277,-0.0038,-0.0018,-0.313,0.0145,0.0076,0.0428,-0.02,0.0141,0.0374,0.0261,-0.0546,0.0104,0.0197,0.0389,0.0402,-0.0342,-0.0011,0.0161,0.0665,-0.0603,-0.0226,-0.0287,0.0227,0.0693,0.2364,-0.02,0.0158,0.0021,-0.0071,0.0288,0.0382,-0.0099,0.0145,0.005,0.1112,-0.081,0.046,-0.0063,-0.0036,-0.0174,0.0082,-0.005,-0.0154,0.0062,-0.0343,0.0005,0.0366,0.0093,0.0011,-0.0499,0.0495,0.0452,-0.0234,-0.0081,-0.0407,0.0061,0.0398,-0.0006,-0.0706,-0.0559,-0.049,-0.0308,0.0157,-0.0542,0.0153,0.01,-0.0148]}
{"key":"[Deep Unitary Convolutional Neural Networks] Deep neural networks can suffer from the exploding and vanishing activation problem, in which the networks fail to train properly because the neural signals either amplify or attenuate across the layers and become saturated. While other normalization methods aim to fix the stated problem, most of them have inference speed penalties in those applications that require running averages of the neural activations. Here we extend the unitary framework based on Lie algebra to neural networks of any dimensionalities, overcoming the major constraints of the prior arts that limit synaptic weights to be square matrices. Our proposed unitary convolutional neural networks deliver up to 32% faster inference speeds and up to 50% reduction in permanent hard disk space while maintaining competitive prediction accuracy.","layer":0,"vector":[-0.0628,-0.011,0.025,0.0073,0.0007,0.0444,0.0264,0.0182,0.0502,-0.038,-0.002,-0.0623,0.054,0.0661,0.0379,-0.0034,0.0063,0.0627,-0.0758,0.0167,0.0437,-0.0496,-0.0077,-0.0482,0.0029,-0.0174,-0.0244,-0.0688,-0.0495,-0.2382,0.0338,-0.0134,0.0468,0.0034,0.0035,-0.0565,-0.0201,0.005,-0.029,0.0154,0.0283,0.0127,-0.0003,-0.0108,-0.0267,-0.0122,-0.0609,-0.0014,-0.0195,-0.0193,0.0249,0.0198,0.0035,0.0656,0.04,-0.0066,0.1048,0.0536,0.0525,0.0397,0.0004,0.0243,-0.1472,0.048,0.0298,-0.0212,-0.0229,-0.0563,0.0148,0.0681,-0.0415,0.0551,0.0019,0.0216,0.0096,-0.0013,0.0212,-0.031,0.0164,0.0439,0.0097,-0.0087,-0.054,-0.0268,-0.0073,-0.029,-0.0105,-0.0247,0.0234,-0.0212,-0.0477,-0.0052,-0.0602,0.023,-0.0343,0.036,-0.0067,0.0434,-0.0607,0.1921,-0.038,0.0446,0.0634,-0.055,0.0426,-0.0142,-0.0168,-0.0131,-0.0564,-0.005,-0.0148,-0.0325,0.0033,-0.0061,0.0579,0.0071,0.0742,0.0125,-0.0714,0.0013,-0.0506,0.0359,0.0473,-0.0179,-0.0045,-0.0611,-0.002,0.1249,0.0251,0.0491,0.0331,-0.0065,-0.0553,-0.0156,0.0153,0.0525,0.0575,0.0114,-0.01,-0.0123,-0.0343,-0.0255,0.0313,-0.044,-0.0445,0.0639,-0.0598,-0.0148,-0.0466,0.0141,0.0048,0.0357,0.0006,-0.0466,0.0535,-0.0194,-0.0026,0.0054,-0.0675,0.0066,-0.0269,-0.0618,-0.0344,0.1201,-0.0028,-0.071,0.0088,-0.0213,0.0726,-0.003,0.0165,0.0346,-0.0322,0.0217,0.086,0.0217,-0.0702,-0.0019,0.0018,0.0349,-0.0087,-0.0343,-0.0093,0.0551,0.0344,-0.0092,0.0189,-0.0307,-0.0138,0.0087,-0.0684,0.0511,-0.1255,0.0099,-0.0598,-0.0446,-0.0123,0.0191,-0.013,-0.0205,0.0103,-0.0025,-0.0257,0.0034,0.0386,0.0054,0.0106,0.0113,0.027,0.0585,-0.0199,-0.0342,0.0697,-0.0321,-0.0422,-0.0009,-0.0318,0.0104,-0.0121,0.0309,0.0337,-0.058,-0.0858,-0.2551,0.0338,0.0048,-0.0681,0.0999,-0.0939,0.0185,0.0203,0.0301,0.0494,0.0442,0.024,-0.0013,-0.0179,0.013,0.065,0.0556,0.0487,-0.0149,0.0109,-0.0074,0.0281,-0.0015,-0.0663,0.0596,0.0418,0.2374,0.0254,0.0655,0.0029,0.0175,0.0464,-0.0438,-0.0768,0.0389,0.0226,0.0642,0.0169,-0.0262,-0.0548,-0.024,0.0207,0.0096,-0.1138,0.0022,-0.0098,0.0086,0.0389,-0.0554,-0.0266,0.0111,-0.0765,0.0432,0.0139,-0.0259,-0.0449,-0.0709,0.0158,-0.079,0.0248,-0.012,-0.0789,0.0004,-0.0633,0.0379,-0.0087,-0.0455,-0.0073,0.0521,-0.0187,0.0029,0.0578,-0.0045,-0.0042,0.0657,-0.0525,0.0557,0.0077,-0.0449,0.0033,0.0456,0.0208,0.0383,0.0096,0.0378,0.0407,0.1065,0.0173,0.039,-0.0061,0.0067,0.0347,-0.0596,-0.0101,-0.0029,-0.0167,-0.3056,0.0413,0.006,0.0183,-0.0501,0.0046,0.049,-0.0016,-0.0319,-0.0404,-0.0269,0.0498,0.0396,-0.0067,0.0002,0.0253,0.079,-0.0466,0.0794,-0.0595,0.0273,0.0061,0.2419,-0.035,0.0143,0.033,-0.0265,0.0228,0.0173,-0.0327,-0.0086,0.0063,0.0481,-0.0292,0.0368,0.045,-0.0407,0.046,0.0367,0.0208,-0.0392,-0.0013,-0.0313,-0.0004,0.0932,-0.026,-0.0236,-0.0316,-0.0294,0.0393,0.0167,-0.0029,-0.0034,-0.0269,0.0325,0.0297,-0.0644,-0.0561,0.0025,-0.0342,0.0236,-0.0794,-0.024,-0.0087,-0.0177]}
{"key":"[The politics of deceptive borders: 'biomarkers of deceit' and the case of iBorderCtrl] This paper critically examines a recently developed proposal for a border control system called iBorderCtrl, designed to detect deception based on facial recognition technology and the measurement of micro-expressions, termed 'biomarkers of deceit'. Funded under the European Commission's Horizon 2020 programme, we situate our analysis in the wider political economy of 'emotional AI' and the history of deception detection technologies. We then move on to interrogate the design of iBorderCtrl using publicly available documents and assess the assumptions and scientific validation underpinning the project design. Finally, drawing on a Bayesian analysis we outline statistical fallacies in the foundational premise of mass screening and argue that it is very unlikely that the model that iBorderCtrl provides for deception detection would work in practice. By interrogating actual systems in this way, we argue that we can begin to question the very premise of the development of data-driven systems, and emotional AI and deception detection in particular, pushing back on the assumption that these systems are fulfilling the tasks they claim to be attending to and instead ask what function such projects carry out in the creation of subjects and management of populations. This function is not merely technical but, rather, we argue, distinctly political and forms part of a mode of governance increasingly shaping life opportunities and fundamental rights.","layer":2,"vector":[-0.0297,0.0263,0.066,0.0027,0.042,0.0149,0.0816,0.004,0.035,-0.0091,0.0384,-0.0646,0.0093,0.0262,0.0351,0.0018,-0.0061,-0.005,0.0072,0.0559,0.0384,-0.0242,-0.016,-0.0436,-0.0117,0.0383,-0.0414,-0.0436,-0.0675,-0.1968,0.0604,-0.0813,0.0381,-0.0343,0.0451,-0.0743,-0.0341,0.0139,-0.0304,0.0062,0.0359,-0.0203,-0.0095,-0.059,-0.0092,-0.013,-0.026,0.0407,-0.0743,-0.0652,0.0102,-0.0076,0.0313,0.0265,0.0328,0.0104,0.0721,0.0463,0.0463,0.0422,0.0422,0.0399,-0.1516,0.0534,0.0148,0.0431,-0.0616,-0.043,0.0282,-0.0062,-0.0298,0.0288,0.0338,0.0317,0.0153,-0.039,-0.0289,0.0023,0.0325,-0.0219,0.048,0.0152,-0.0369,0.0127,-0.0184,-0.079,0.0152,-0.0324,0.0569,0.0013,-0.0465,-0.0063,-0.0263,0.0187,-0.0677,-0.0163,0.0122,0.0712,-0.0487,0.2134,-0.0404,0.0143,0.0215,-0.0282,0.0528,-0.0374,-0.0361,-0.0433,-0.0161,0.0191,-0.03,-0.0508,0.0173,-0.0258,0.0313,0.0306,0.0435,0.0161,-0.0011,-0.028,-0.0206,0.0377,0.0466,-0.0062,0.022,-0.0428,0.0643,0.1295,0.0188,-0.0114,0.0192,-0.0276,-0.0368,-0.0083,0.0038,0.0258,-0.0275,0.0394,0.0298,-0.0308,-0.0732,-0.0464,-0.0133,-0.0397,-0.0522,0.0778,0.013,0.064,-0.0508,0.0425,-0.028,0.0296,-0.0648,-0.0254,0.0118,0.0102,0.0046,0.0717,-0.0922,0.0221,0.0258,-0.0249,-0.0385,0.1491,0.0129,-0.0806,-0.0096,0.0101,0.028,-0.0122,0.04,0.0102,-0.0148,0.0257,-0.0018,-0.0191,-0.0368,0.034,-0.0166,0.0264,0.0499,-0.0529,-0.0425,0.04,0.0363,-0.0313,-0.0141,-0.0125,0.0129,0.0265,-0.0494,0.0762,-0.0266,-0.0038,-0.0358,-0.0323,-0.0121,-0.0123,-0.0334,0.0027,0.0015,-0.0006,-0.0311,0.0182,0.0371,0.0212,-0.0127,-0.0237,0.0722,0.0359,-0.0062,0.0325,0.0535,-0.0359,-0.0229,-0.0339,0.0038,0.0034,-0.0083,0.019,0.0019,-0.0428,-0.0487,-0.2747,-0.017,-0.0251,-0.0085,0.0521,-0.0643,0.0515,-0.01,0.0444,0.1183,0.0887,-0.0279,-0.0397,0.0999,0.0147,0.027,-0.0069,0.0046,-0.0507,0.0394,-0.0049,-0.0127,-0.0331,-0.0719,-0.0076,-0.0187,0.2352,0.081,-0.0127,-0.0106,-0.0006,0.0432,-0.0461,-0.1382,0.0477,0.0371,0.0018,-0.022,-0.0327,0.0027,-0.0096,0.01,0.023,-0.0756,-0.039,-0.0132,-0.0128,-0.0002,-0.047,-0.0008,0.0167,-0.0299,0.0537,0.0249,-0.019,-0.0968,-0.0908,0.0333,-0.0301,0.0508,0.0112,-0.03,-0.003,-0.0381,0.0731,0.0633,-0.0399,-0.0264,0.0451,-0.0268,0.0126,0.128,0.0261,-0.0895,0.0474,0.0008,0.0472,-0.0439,-0.053,0.005,0.0515,0.0147,-0.0253,0.0037,0.0217,-0.0124,0.0407,-0.0061,0.0602,-0.0421,-0.0057,0.0367,-0.0621,-0.0376,0.0395,0.0036,-0.2908,0.0297,-0.0096,0.0152,-0.0583,-0.0031,0.0372,0.0525,-0.0591,-0.061,0.0152,0.0527,0.0563,0.0229,-0.0367,0.0238,0.0644,-0.0725,0.0418,-0.0077,0.0291,0.0065,0.2015,-0.0205,0.0179,0.0448,0.0391,0.0039,0.0254,-0.0554,0.0284,-0.0194,0.0312,-0.0481,0.0399,0.023,-0.071,0.0246,0.0032,-0.0169,-0.0319,-0.0003,0.0241,-0.0403,0.1148,-0.0216,-0.0443,-0.0046,0.0386,0.0203,-0.0304,-0.002,-0.0325,-0.0353,0.051,0.0605,-0.0406,-0.0094,-0.0338,-0.0403,0.0046,-0.0141,0.0024,0.056,-0.0265]}
{"key":"[Personalized Automatic Sleep Staging with Single-Night Data: a Pilot Study with KL-Divergence Regularization] Brain waves vary between people. An obvious way to improve automatic sleep staging for longitudinal sleep monitoring is personalization of algorithms based on individual characteristics extracted from the first night of data. As a single night is a very small amount of data to train a sleep staging model, we propose a Kullback-Leibler (KL) divergence regularized transfer learning approach to address this problem. We employ the pretrained SeqSleepNet (i.e. the subject independent model) as a starting point and finetune it with the single-night personalization data to derive the personalized model. This is done by adding the KL divergence between the output of the subject independent model and the output of the personalized model to the loss function during finetuning. In effect, KL-divergence regularization prevents the personalized model from overfitting to the single-night data and straying too far away from the subject independent model. Experimental results on the Sleep-EDF Expanded database with 75 subjects show that sleep staging personalization with a single-night data is possible with help of the proposed KL-divergence regularization. On average, we achieve a personalized sleep staging accuracy of 79.6%, a Cohen's kappa of 0.706, a macro F1-score of 73.0%, a sensitivity of 71.8%, and a specificity of 94.2%. We find both that the approach is robust against overfitting and that it improves the accuracy by 4.5 percentage points compared to non-personalization and 2.2 percentage points compared to personalization without regularization.","layer":1,"vector":[-0.024,-0.0047,0.016,-0.0105,0.0152,0.0309,0.0338,-0.0039,0.0357,-0.0178,0.01,-0.0127,0.0099,0.0383,0.0189,0.0201,0.0372,0.0428,-0.0263,-0.0258,0.0033,-0.0051,0.0279,0.0082,0.0146,-0.0233,-0.002,-0.0462,-0.0385,-0.2536,-0.0045,-0.0737,0.0117,-0.0076,0.0255,-0.0144,-0.0216,0.0501,-0.0011,0.0622,0.0746,0.0088,0.0358,-0.0475,0.0151,-0.043,-0.0556,-0.0135,-0.0116,0.0063,0.0319,-0.0136,-0.0096,0.0267,0.0582,0.0318,0.0545,0.0295,0.0826,0.0631,-0.0047,0.0759,-0.174,0.0488,0.0279,0.0471,-0.0541,-0.0423,0.0126,0.0432,-0.0326,0.0181,0.0178,0.0486,0.0286,-0.0282,0.0531,-0.0133,-0.0266,0.0403,0.0183,-0.0241,-0.0344,-0.0426,-0.0119,-0.0156,0.006,-0.0755,0.0175,-0.0052,-0.0463,-0.0013,-0.0175,0.034,-0.0166,-0.0478,0.0457,0.0219,-0.0351,0.203,0.0024,0.0559,0.0358,0.0297,0.0083,-0.0363,-0.0641,-0.0353,0.0019,0.0047,-0.034,-0.005,0.0669,-0.0626,0.056,0.0327,0.1003,0.0549,0.0551,0.0035,-0.0069,-0.018,0.0711,-0.0208,0.0538,-0.0403,0.0425,0.1181,0.0565,-0.0111,0.0558,0.0085,-0.0412,-0.0141,-0.012,0.0192,-0.0054,-0.0281,0.0127,-0.0108,-0.0434,-0.0368,0.0436,-0.0736,-0.088,0.1469,-0.0455,0.018,-0.047,-0.036,-0.0407,-0.0071,-0.0269,-0.0265,0.0541,0.052,0.0718,0.0519,-0.0713,0.0027,0.0239,-0.0322,-0.0278,0.0768,0.0189,-0.0654,-0.0271,-0.0017,0.0082,-0.0716,0.0696,0.0661,-0.0347,0.0441,0.0813,0.064,-0.0227,0.0072,0.0184,-0.0339,0.0257,-0.0812,-0.0383,0.0165,-0.0105,-0.0529,0.0024,0.005,0.004,0.0277,-0.0364,-0.0127,-0.0315,-0.0188,-0.0146,-0.0372,-0.0514,-0.0164,0.005,-0.0468,0.0023,0.0356,-0.0337,0.0314,0.0074,0.0715,-0.0184,0.0091,0.0986,0.0197,-0.0195,-0.0238,0.0578,-0.0211,-0.0546,0.0196,-0.0167,0.0121,0.0377,0.0775,0.0245,-0.0267,-0.0751,-0.245,-0.0106,-0.0007,-0.0443,0.0146,-0.0781,-0.0199,0.0291,0.0566,0.08,0.0592,-0.022,-0.0397,0.0569,-0.0299,0.0551,0.0173,0.0451,-0.0286,-0.0495,-0.0068,0.0207,-0.0156,-0.0814,0.0373,-0.0141,0.2196,0.0056,0.0669,-0.0093,0.022,0.0095,0.0033,-0.1246,0.0447,0.0027,0.0813,0.0014,-0.0617,-0.0402,-0.0259,0.0073,-0.0045,-0.0755,-0.058,-0.0476,-0.0487,-0.0009,-0.0694,0.0296,0.0642,-0.0293,0.0546,-0.0662,0.0171,-0.04,-0.0906,0.0019,-0.0332,0.0133,0.0107,-0.016,-0.004,-0.0663,0.0489,-0.0208,-0.043,-0.0729,0.0139,-0.0176,-0.0386,0.053,-0.0206,-0.0255,0.0702,0.0292,0.0242,-0.0172,-0.0459,-0.0351,0.0378,-0.0484,-0.0093,0.0224,0.0494,0.0198,0.0779,0.0092,-0.0249,-0.0358,0.0075,0.0229,-0.0464,-0.0474,0.0365,-0.0211,-0.2787,-0.0047,-0.0451,0.0226,-0.0063,-0.0025,0.0381,0.0128,-0.0729,-0.006,-0.0469,0.0467,0.0019,-0.0159,0.0276,0.0499,0.0709,-0.0582,0.0366,-0.0298,0.0006,0.0256,0.1764,-0.0131,0.0388,0.0325,-0.0026,-0.0115,0.0399,-0.0188,0.0378,0.0422,0.0589,-0.0152,0.0485,0.0674,-0.0586,-0.0142,-0.0014,-0.0355,0.0264,-0.0065,-0.0376,-0.0332,0.1176,0.0167,-0.0168,-0.0718,-0.0206,-0.0336,-0.0175,0.0094,-0.0185,0.0644,0.0383,0.0438,0.0011,-0.0199,-0.012,-0.0731,-0.0152,-0.0603,-0.07,-0.0037,0.0195]}
{"key":"[MIcro-Surgical Anastomose Workflow recognition challenge report] The \"MIcro-Surgical Anastomose Workflow recognition on training sessions\" (MISAW) challenge provided a data set of 27 sequences of micro-surgical anastomosis on artificial blood vessels. This data set was composed of videos, kinematics, and workflow annotations described at three different granularity levels: phase, step, and activity. The participants were given the option to use kinematic data and videos to develop workflow recognition models. Four tasks were proposed to the participants: three of them were related to the recognition of surgical workflow at three different granularity levels, while the last one addressed the recognition of all granularity levels in the same model. One ranking was made for each task. We used the average application-dependent balanced accuracy (AD-Accuracy) as the evaluation metric. This takes unbalanced classes into account and it is more clinically relevant than a frame-by-frame score. Six teams, including a non-competing team, participated in at least one task. All models employed deep learning models, such as CNN or RNN. The best models achieved more than 95% AD-Accuracy for phase recognition, 80% for step recognition, 60% for activity recognition, and 75% for all granularity levels. For high levels of granularity (i.e., phases and steps), the best models had a recognition rate that may be sufficient for applications such as prediction of remaining surgical time or resource management. However, for activities, the recognition rate was still low for applications that can be employed clinically. The MISAW data set is publicly available to encourage further research in surgical workflow recognition. It can be found at www.synapse.org/MISAW","layer":2,"vector":[-0.0469,-0.0559,0.0262,-0.0578,0.0056,0.0185,0.0191,0.0346,0.023,-0.0201,0.0019,-0.0585,-0.0179,0.0502,-0.0368,-0.0216,-0.0275,0.0608,-0.0579,-0.0028,0.0094,-0.0043,-0.0352,-0.0201,0.0248,0.014,-0.0403,-0.0365,-0.0271,-0.2442,-0.0063,-0.0234,0.0441,0.0099,0.0648,-0.0155,-0.0391,0.0675,-0.0746,0.0201,0.0289,0.0251,-0.0449,-0.0304,-0.0352,-0.0137,-0.0101,-0.0705,-0.0168,-0.0044,0.0039,-0.0653,0.0102,0.0941,0.0494,0.0028,0.0743,0.0373,0.0287,0.0068,0.0596,0.0359,-0.1869,0.0847,0.0051,-0.0024,-0.0049,-0.0589,0.0427,0.0151,-0.0219,0.0227,0.032,0.055,-0.0007,-0.0107,0.0217,-0.0125,-0.0311,0.0081,0.01,0.0092,-0.0607,0.0073,-0.0579,-0.0257,-0.0117,-0.0353,0.0317,-0.0027,-0.0505,0.0064,-0.0283,-0.0036,-0.0339,-0.0234,0.0014,0.0217,-0.0209,0.2064,-0.0376,0.0246,0.0759,-0.0484,0.0188,-0.0165,-0.0265,-0.0319,-0.031,0.0232,0.0182,0.0059,0.0656,-0.0019,0.0413,0.0304,0.0419,0.0532,-0.0132,-0.0141,0.0126,-0.0028,0.0411,-0.0259,-0.0003,-0.0573,0.0785,0.1092,0.0405,-0.0101,0.0455,-0.0124,-0.0328,-0.0078,-0.0044,0.029,0.038,-0.0019,-0.0159,-0.0598,-0.0158,-0.0133,0.0275,-0.0965,-0.0569,0.1587,-0.0235,0.0313,-0.0421,-0.0535,-0.0629,0.0352,-0.0395,0.0036,0.0074,0.0602,0.0383,0.0181,-0.0627,0.0122,-0.0176,-0.1087,-0.0495,0.0828,0.0583,-0.088,-0.0219,-0.009,-0.0082,0.0059,0.0149,0.0402,-0.0437,0.0087,0.0606,0.048,-0.0269,-0.0466,0.0356,0.0295,0.0507,-0.0498,-0.0138,0.043,0.033,0.0046,0.0412,-0.0339,0.0585,0.0327,-0.0312,0.0331,-0.0234,0.0154,0.0178,-0.0421,0.0417,-0.0199,-0.0289,-0.0146,0.0003,0.0362,-0.0078,0.0209,0.0207,-0.0122,-0.0563,0.0226,0.0575,0.0549,-0.0522,0.0419,0.0521,-0.0087,-0.0168,-0.0144,0.0304,-0.0077,-0.0093,0.0277,0.0323,0.0068,-0.0513,-0.2608,0.0111,0.0199,0.0018,0.071,-0.0248,0.0375,-0.0164,-0.0079,0.0419,0.0819,-0.0182,-0.013,-0.0067,-0.0095,0.0462,0.0054,0.0387,-0.0344,0.0302,0.0252,0.0057,-0.0017,-0.0777,-0.0041,0.0243,0.2201,0.0009,0.0297,-0.0457,0.0187,0.0337,-0.0522,-0.1095,0.0505,-0.0157,0.0576,-0.0354,0.0038,-0.0007,-0.0664,0.0136,0.0294,-0.1151,-0.0248,-0.0268,-0.0855,0.0254,-0.1012,0.0225,0.0122,-0.0521,0.0573,-0.0092,-0.0018,-0.0215,-0.076,0.0535,-0.0745,0.0326,0.0002,-0.0285,0.0133,-0.0271,0.0484,0.0264,-0.0339,-0.0408,0.015,-0.0699,0.016,0.1137,0.0466,-0.0181,0.0832,0.007,0.0343,-0.0217,-0.0318,-0.0577,0.048,-0.0402,0.0157,0.0165,0.0391,0.0494,0.0474,-0.0304,0.0204,-0.0364,0.0571,0.0219,-0.0711,-0.0076,0.0323,0.0053,-0.2723,0.0514,0.0181,0.0459,-0.0584,-0.0359,0.0181,-0.0088,-0.0005,-0.0032,-0.0133,0.01,0.0356,0.0257,-0.0209,0.0756,0.0889,-0.0195,0.0355,-0.0802,0.0467,-0.0206,0.2005,-0.0432,0.0217,-0.01,-0.02,-0.0061,0.0429,-0.0075,0.0228,-0.023,0.0857,-0.0254,0.0693,0.0525,-0.0495,0.0299,0.0176,0.0204,0.0109,0.0415,-0.0424,-0.0253,0.107,-0.0239,-0.0187,-0.0476,-0.0076,0.0063,-0.0254,0.0037,0.0041,0.0378,0.0283,0.0129,0.0005,-0.0526,-0.0665,-0.0454,-0.0136,-0.0529,-0.0014,0.0033,-0.0642]}
{"key":"[Improving Subgraph Recognition with Variational Graph Information Bottleneck] Subgraph recognition aims at discovering a compressed substructure of a graph that is most informative to the graph property. It can be formulated by optimizing Graph Information Bottleneck (GIB) with a mutual information estimator. However, GIB suffers from training instability and degenerated results due to its intrinsic optimization process. To tackle these issues, we reformulate the subgraph recognition problem into two steps: graph perturbation and subgraph selection, leading to a novel Variational Graph Information Bottleneck (VGIB) framework. VGIB first employs the noise injection to modulate the information flow from the input graph to the perturbed graph. Then, the perturbed graph is encouraged to be informative to the graph property. VGIB further obtains the desired subgraph by filtering out the noise in the perturbed graph. With the customized noise prior for each input, the VGIB objective is endowed with a tractable variational upper bound, leading to a superior empirical performance as well as theoretical properties. Extensive experiments on graph interpretation, explainability of Graph Neural Networks, and graph classification show that VGIB finds better subgraphs than existing methods. Code is avaliable at https://github.com/Samyu0304/VGIB","layer":4,"vector":[-0.0001,-0.0531,0.0104,-0.036,0.0527,0.0304,0.0184,0.0189,0.0174,0.0078,0.0112,-0.0694,0.0754,0.0526,-0.0064,0.0063,-0.0101,0.0435,-0.0242,-0.0141,0.0157,-0.0332,-0.0162,-0.0895,0.0606,0.0143,-0.0317,-0.0166,-0.0351,-0.2568,0.0429,-0.0267,0.042,-0.0144,0.0232,-0.0935,-0.0145,0.0697,-0.0584,0.0535,0.0183,0.0097,-0.0203,-0.0011,-0.0291,-0.0261,0.034,0.0028,-0.0144,-0.0549,0.001,-0.0323,0.0655,-0.0022,0.0413,0.0686,0.0368,0.0339,0.0374,0.0814,0.0142,0.0684,-0.1352,0.0702,0.0619,0.0053,-0.0773,-0.0018,0.0379,0.0616,0.019,0.0088,0.0399,0.0184,0.0221,0.0159,-0.0118,-0.0494,-0.0148,-0.0263,-0.0022,-0.02,-0.0547,-0.0285,-0.0297,-0.0399,0.0201,-0.0505,0.0386,-0.0039,-0.0162,-0.0304,0.0236,0.0353,-0.0789,-0.0199,0.0306,0.0289,-0.0472,0.1861,-0.0457,0.0246,0.0091,-0.0286,0.0443,-0.0431,0.0111,-0.0344,-0.0265,-0.007,-0.0097,-0.0254,0.0278,-0.0716,0.0391,-0.0065,0.0595,0.0502,-0.0244,-0.0104,-0.0518,0.0225,0.0024,-0.0354,-0.0,-0.0428,-0.0133,0.1125,0.0228,0.0539,0.0513,0.0044,0.0092,-0.0281,-0.0057,-0.0252,0.0368,0.0256,0.0424,-0.0332,-0.0263,-0.0202,0.0265,-0.0528,-0.1191,0.1292,-0.0701,0.0046,-0.0135,-0.0418,-0.0198,0.0291,-0.0268,0.008,0.0279,0.0255,0.0323,0.0385,-0.069,0.0336,-0.0169,-0.0314,-0.0433,0.0773,0.0329,-0.0987,-0.028,-0.0225,-0.0024,-0.0484,0.0234,0.0833,0.0071,0.0525,0.049,0.0489,-0.0802,-0.0207,0.0038,-0.0252,0.0241,-0.0577,-0.0389,0.0041,0.032,0.0062,-0.0174,-0.0388,0.0376,0.0182,-0.0398,0.0261,-0.0078,0.0067,-0.0356,0.0003,-0.0164,-0.0533,-0.0291,-0.0433,0.0374,-0.0177,-0.0296,0.0409,-0.0523,0.0304,-0.009,-0.0486,-0.0263,-0.0145,-0.0241,-0.0125,0.0628,-0.0151,-0.0145,0.0067,0.0636,0.0205,0.0138,0.0306,0.0403,-0.0542,-0.0613,-0.2186,-0.0144,-0.0179,-0.0096,0.0735,-0.0489,0.0309,-0.0112,0.0605,0.0711,-0.0034,0.0013,-0.0539,-0.0046,0.0023,0.1033,0.0296,0.0416,-0.0156,-0.0272,-0.031,0.0383,-0.008,-0.0546,0.0309,0.0221,0.2372,0.0129,0.0269,-0.021,-0.0083,0.0316,-0.0686,-0.0981,0.0644,0.0571,0.0096,-0.0195,-0.0364,0.0022,-0.0743,-0.0056,0.0327,-0.0578,-0.0428,-0.0109,-0.0198,-0.0011,-0.0873,0.0057,0.0688,0.0191,0.0501,0.0523,0.0323,-0.0583,-0.0797,0.022,-0.03,-0.0161,0.0068,-0.0863,-0.0621,-0.0085,0.052,0.0295,-0.0553,-0.0474,0.0268,-0.0193,-0.042,0.0448,0.0563,-0.008,0.0725,0.0385,0.0602,-0.0206,-0.0631,-0.0095,0.0225,-0.0365,0.0126,0.008,0.0218,0.0263,0.084,-0.0529,0.0278,-0.0009,0.0493,0.0277,-0.0226,-0.0081,0.0487,-0.0399,-0.3144,0.0208,0.0722,0.0489,-0.0227,0.0278,0.0531,0.0506,-0.0291,-0.0175,0.0099,0.06,-0.0106,-0.0034,-0.0257,0.0693,0.0438,-0.0529,0.0363,-0.0474,0.0261,0.0101,0.2117,-0.0168,0.0613,0.0181,-0.0223,0.0209,0.003,-0.0263,-0.0042,-0.0186,0.0951,-0.0463,0.0633,0.0647,-0.0516,0.0538,0.0177,0.0014,0.0106,-0.0238,-0.0269,-0.0359,0.0808,-0.0258,-0.0185,-0.0227,0.0303,0.039,0.0155,-0.0162,-0.0128,0.0227,0.0253,0.0555,-0.0217,-0.0548,-0.0235,-0.0452,0.0034,-0.0815,0.0008,-0.0017,-0.0547]}
{"key":"[Improving Graph Attention Networks with Large Margin-based Constraints] Graph Attention Networks (GATs) are the state-of-the-art neural architecture for representation learning with graphs. GATs learn attention functions that assign weights to nodes so that different nodes have different influences in the feature aggregation steps. In practice, however, induced attention functions are prone to over-fitting due to the increasing number of parameters and the lack of direct supervision on attention weights. GATs also suffer from over-smoothing at the decision boundary of nodes. Here we propose a framework to address their weaknesses via margin-based constraints on attention during training. We first theoretically demonstrate the over-smoothing behavior of GATs and then develop an approach using constraint on the attention weights according to the class boundary and feature aggregation pattern. Furthermore, to alleviate the over-fitting problem, we propose additional constraints on the graph structure. Extensive experiments and ablation studies on common benchmark datasets demonstrate the effectiveness of our method, which leads to significant improvements over the previous state-of-the-art graph attention methods on all datasets.","layer":0,"vector":[-0.0265,-0.0093,0.0432,-0.0378,0.043,0.0214,0.0402,0.0411,0.0305,-0.0343,0.0145,-0.0565,0.0371,0.0986,0.0247,0.052,0.0303,0.0746,-0.0518,-0.004,-0.034,0.0079,-0.0021,-0.0477,0.0312,-0.0156,-0.0556,-0.0265,-0.0698,-0.2411,0.0211,-0.0565,0.0636,-0.0101,0.0067,-0.042,0.0233,0.0198,-0.0368,0.047,0.0069,0.0305,-0.0412,-0.0503,-0.0635,-0.0302,-0.016,-0.0428,-0.0387,-0.0545,0.0456,-0.0633,0.0332,0.003,0.0653,0.065,0.0328,0.0136,0.049,0.0481,0.0263,0.0494,-0.1086,0.0459,0.0603,0.0305,-0.0642,0.0082,-0.002,0.0749,0.0222,0.0074,0.0051,0.0221,-0.0057,0.0124,0.0234,-0.0193,-0.0078,-0.0441,0.0143,-0.0147,-0.0446,0.0037,0.0563,-0.0225,0.0321,-0.0495,0.0219,-0.0087,-0.032,-0.0596,-0.0074,-0.0118,-0.0653,-0.0117,0.0562,0.0042,-0.0646,0.1715,-0.0551,0.0313,0.0258,-0.0253,0.0418,-0.0375,0.0026,-0.0183,-0.0358,0.0148,-0.015,-0.0266,0.0003,-0.0118,0.0031,0.0373,0.0829,0.0278,-0.0178,-0.0283,-0.0157,0.0393,0.0217,-0.0466,0.027,-0.0472,-0.0129,0.112,0.0439,0.0314,0.0448,-0.0073,-0.0452,-0.0308,0.0243,-0.009,0.0303,0.0201,0.0124,-0.0511,-0.0223,-0.0196,0.0054,-0.0786,-0.0894,0.1354,-0.094,0.0032,-0.0099,-0.0183,-0.0317,-0.0024,-0.0186,-0.0591,-0.0197,0.039,0.0211,0.057,-0.091,0.0296,-0.0107,-0.0012,-0.0724,0.0955,0.0491,-0.0892,-0.0125,-0.0162,-0.0476,-0.02,0.0412,0.0717,-0.0372,0.0836,0.1002,0.0626,-0.0668,-0.0111,0.0086,-0.0151,0.0199,-0.0471,-0.0341,-0.0006,0.0048,-0.0576,-0.0148,-0.0248,0.0127,0.0381,-0.0437,0.0412,0.0032,0.0285,-0.0402,-0.0009,-0.0176,-0.0351,-0.006,-0.02,0.034,-0.0168,-0.0432,0.0216,-0.0152,0.0384,-0.0254,-0.0034,0.0306,-0.0059,-0.0951,0.0229,0.0252,-0.029,-0.0591,-0.0132,0.0206,0.0213,0.0149,0.0593,0.0487,-0.0213,-0.0316,-0.2429,0.0144,0.015,-0.0135,0.0558,-0.0693,0.0135,0.0325,0.0983,0.0899,0.0622,0.0162,-0.0672,-0.0155,-0.0196,0.0602,0.0167,0.0072,-0.0421,-0.0311,-0.0029,0.0372,-0.0037,-0.0708,0.0375,0.0361,0.2434,0.0003,0.0188,-0.0459,0.0057,0.0383,-0.0326,-0.0856,0.0207,0.0225,0.0112,-0.0013,-0.0471,-0.0032,-0.0098,-0.0044,-0.0151,-0.0866,-0.0175,0.0303,-0.0224,0.0222,-0.0643,0.0069,0.0712,-0.0549,0.0592,0.0082,-0.0337,-0.051,-0.0914,0.038,-0.0549,0.0147,0.0093,-0.0985,0.0202,-0.0369,0.0458,0.0419,-0.0525,-0.0039,-0.0005,-0.005,-0.0118,0.0624,0.0107,-0.0355,0.0649,0.0291,0.0601,-0.0076,-0.0367,-0.0213,0.0437,-0.0215,0.0207,-0.0278,0.0572,0.0177,0.0978,-0.0414,0.0525,-0.0193,0.0026,0.0081,-0.027,-0.0361,0.062,-0.0059,-0.2924,0.0473,0.0221,0.0412,0.0185,0.0131,0.0581,0.0332,-0.0438,0.0156,0.0041,0.0431,0.0269,-0.0193,-0.0324,0.0541,0.0182,-0.0244,0.0578,-0.0295,0.0302,0.0297,0.223,-0.0242,0.0565,0.0072,-0.0249,-0.048,0.0291,-0.0135,0.0047,0.0059,0.0674,-0.0732,0.0221,0.0811,-0.0466,0.0569,0.0205,0.0086,0.0351,-0.0172,-0.0546,-0.051,0.0879,-0.0006,-0.0182,-0.0395,0.0188,0.0266,-0.0138,-0.0004,0.0077,0.037,0.0179,0.0432,-0.015,-0.0191,-0.05,-0.0468,0.0049,-0.0916,-0.028,0.0029,-0.0256]}
{"key":"[I call BS: Fraud Detection in Crowdfunding Campaigns] Donations to charity-based crowdfunding environments have been on the rise in the last few years. Unsurprisingly, deception and fraud in such platforms have also increased, but have not been thoroughly studied to understand what characteristics can expose such behavior and allow its automatic detection and blocking. Indeed, crowdfunding platforms are the only ones typically performing oversight for the campaigns launched in each service. However, they are not properly incentivized to combat fraud among users and the campaigns they launch: on the one hand, a platform's revenue is directly proportional to the number of transactions performed (since the platform charges a fixed amount per donation); on the other hand, if a platform is transparent with respect to how much fraud it has, it may discourage potential donors from participating. In this paper, we take the first step in studying fraud in crowdfunding campaigns. We analyze data collected from different crowdfunding platforms, and annotate 700 campaigns as fraud or not. We compute various textual and image-based features and study their distributions and how they associate with campaign fraud. Using these attributes, we build machine learning classifiers, and show that it is possible to automatically classify such fraudulent behavior with up to 90.14% accuracy and 96.01% AUC, only using features available from the campaign's description at the moment of publication (i.e., with no user or money activity), making our method applicable for real-time operation on a user browser.","layer":0,"vector":[-0.0066,-0.0191,-0.017,0.0033,0.0779,0.0268,0.0338,0.0152,0.021,-0.0097,0.0198,-0.001,-0.0185,0.0341,0.0428,-0.0257,0.0064,-0.0036,-0.0305,0.0461,0.0592,-0.0476,-0.0027,-0.0216,0.0287,0.019,0.0022,-0.0223,-0.0831,-0.2275,0.0601,-0.0663,0.0613,-0.0484,0.0571,-0.0333,-0.0211,0.0445,-0.066,0.0495,0.0105,0.0048,-0.0373,-0.0485,-0.012,-0.0856,-0.0356,-0.0175,-0.0244,-0.0242,0.0167,-0.0383,0.0025,0.05,0.0044,-0.017,0.0232,0.0727,0.0695,0.0439,0.0611,0.0521,-0.14,0.0461,0.0328,0.028,-0.0231,-0.0109,-0.007,0.0317,0.049,0.0427,-0.0264,0.0379,0.0116,-0.0082,-0.0053,-0.0012,0.0038,0.0177,-0.0182,-0.0137,-0.0148,0.0054,-0.0072,-0.0055,0.0487,-0.0259,0.0403,0.0001,0.0081,0.0314,-0.0193,0.015,-0.086,0.0024,0.0154,-0.0009,-0.0429,0.2233,-0.0399,0.0484,0.0034,-0.0357,0.0601,-0.0641,-0.0141,-0.007,-0.0233,0.0041,-0.0208,0.0037,0.0581,-0.0271,-0.0024,0.0336,0.0164,0.0155,-0.0312,-0.0061,-0.0325,0.0236,0.0393,-0.0064,-0.0087,-0.0517,0.0462,0.1535,0.0597,0.0191,0.0225,0.0211,-0.0851,-0.028,-0.0109,0.0187,-0.0506,0.039,0.0359,0.0003,-0.0455,-0.0326,-0.0133,-0.0723,-0.0824,0.0739,-0.0389,0.0311,-0.0063,-0.0193,-0.0182,0.0411,-0.0578,-0.0328,0.0408,0.0149,0.0481,0.0925,-0.0354,-0.0098,0.0136,-0.0248,-0.0164,0.1055,0.0127,-0.1253,-0.0133,0.0109,-0.0017,-0.029,0.0107,0.0283,-0.0375,0.0286,0.0672,-0.0073,-0.0934,0.0084,0.0109,0.0056,0.0313,-0.0451,-0.0819,0.0462,0.0395,-0.0608,-0.0178,-0.0594,0.0526,0.0452,-0.0263,0.0301,-0.0607,0.0006,-0.018,-0.0341,-0.0291,-0.0275,0.0299,-0.0331,0.0164,0.0187,-0.0429,0.013,-0.0077,0.0283,-0.0234,0.0005,0.0876,0.0089,-0.032,0.0456,-0.0135,-0.0291,-0.0231,0.0057,0.0041,0.0227,-0.0021,0.0449,0.0537,-0.0435,-0.0634,-0.2206,-0.0454,0.0108,0.013,0.0186,-0.0413,0.0094,-0.0113,0.0143,0.0862,0.0888,-0.0771,-0.0307,0.0568,0.0111,0.05,-0.0111,0.0206,-0.0177,0.0179,-0.0374,0.0223,-0.0158,-0.0681,0.0023,0.0378,0.2116,0.0778,-0.0285,-0.0349,0.0179,0.0208,-0.0434,-0.1286,0.0433,0.0129,0.0374,-0.0394,-0.0349,-0.0011,-0.0358,0.0073,0.0093,-0.0583,0.002,0.0124,-0.064,-0.0063,-0.0386,0.0482,0.0486,-0.0394,0.073,0.0422,0.035,-0.0606,-0.0579,0.0391,-0.0192,0.0122,0.0356,-0.0602,0.0494,-0.0599,0.078,-0.0051,-0.0555,-0.0122,0.0454,-0.0248,-0.0218,0.1279,-0.0085,-0.026,0.0737,-0.0094,0.0582,-0.0977,-0.0498,0.0162,0.0604,0.0065,0.0297,0.0051,0.0062,-0.0186,0.0522,0.0289,0.0459,-0.0448,0.0033,0.0501,-0.1212,-0.0064,0.0065,0.0145,-0.2998,-0.0074,0.0149,0.0716,0.0063,0.0003,0.0353,0.0017,-0.0443,-0.0028,-0.0012,0.0596,0.0527,-0.0552,0.0087,0.0514,0.023,-0.0807,0.0538,-0.0624,0.0414,0.0269,0.22,-0.0436,-0.0259,0.0357,0.0033,0.028,0.0532,-0.0223,0.0334,-0.0097,0.07,-0.0391,0.0121,0.0149,-0.0437,0.0132,-0.008,-0.0315,-0.0285,0.04,-0.0443,-0.0197,0.0507,-0.0138,-0.0068,-0.0663,0.0267,0.0839,-0.0319,-0.0305,-0.0575,0.0048,0.0296,0.0416,-0.0736,-0.029,-0.0123,-0.0314,-0.0093,-0.0364,-0.0138,0.004,-0.0248]}
{"key":"[Causal Machine Learning for Healthcare and Precision Medicine] Causal machine learning (CML) has experienced increasing popularity in healthcare. Beyond the inherent capabilities of adding domain knowledge into learning systems, CML provides a complete toolset for investigating how a system would react to an intervention (e.g.\\ outcome given a treatment). Quantifying effects of interventions allows actionable decisions to be made whilst maintaining robustness in the presence of confounders. Here, we explore how causal inference can be incorporated into different aspects of clinical decision support (CDS) systems by using recent advances in machine learning. Throughout this paper, we use Alzheimer's disease (AD) to create examples for illustrating how CML can be advantageous in clinical scenarios. Furthermore, we discuss important challenges present in healthcare applications such as processing high-dimensional and unstructured data, generalisation to out-of-distribution samples, and temporal relationships, that despite the great effort from the research community remain to be solved. Finally, we review lines of research within causal representation learning, causal discovery and causal reasoning which offer the potential towards addressing the aforementioned challenges.","layer":4,"vector":[-0.0475,0.0041,0.0131,-0.0087,0.0266,0.0023,0.0792,0.0479,0.0605,-0.0457,0.0417,-0.0453,0.0185,0.0792,-0.0151,0.0239,-0.0266,0.0487,-0.0441,0.0436,-0.0029,-0.0113,-0.008,-0.0281,0.0004,0.046,-0.0451,-0.0292,-0.042,-0.2182,0.0325,-0.0783,0.0162,-0.035,-0.0164,-0.0194,-0.0263,0.0684,-0.0099,0.0176,0.0087,0.0172,-0.0208,-0.0761,0.0039,-0.0526,-0.0208,-0.019,-0.0433,-0.0016,-0.0145,0.0164,0.0275,0.0506,0.0391,0.0315,0.0185,0.0782,0.0582,0.0583,0.0298,0.0272,-0.1677,0.0584,0.0546,0.0017,-0.0154,0.0014,0.0263,0.0843,0.0024,0.0171,0.0077,0.0466,0.0035,0.0223,0.0439,-0.0015,-0.0047,-0.0051,0.0588,0.0289,-0.0249,0.0118,-0.0068,-0.0685,-0.0073,-0.0831,0.0185,-0.0014,-0.0401,-0.0099,-0.0262,0.0254,-0.072,-0.0169,0.0766,0.0168,-0.048,0.2058,-0.0373,0.0244,-0.0319,-0.0331,0.0443,-0.0459,-0.0188,-0.0567,-0.0218,-0.0169,0.0058,-0.0197,0.0566,-0.0319,0.0217,0.0341,0.0532,0.0148,-0.0169,-0.0224,-0.0047,0.0081,0.0377,0.0,0.0224,-0.0329,0.0035,0.1707,-0.0078,-0.0469,0.0473,-0.0373,-0.0408,0.0155,0.0417,0.0192,0.0396,-0.0139,0.0155,0.0197,-0.0189,-0.0219,0.0158,-0.0641,-0.0993,0.1276,-0.0264,0.0139,-0.0444,-0.0007,-0.02,0.0547,-0.0362,-0.0175,0.0004,0.0272,0.0333,0.0233,-0.0503,0.0448,-0.0416,-0.0786,-0.0382,0.1194,-0.0052,-0.1077,-0.0394,-0.0218,0.0326,0.0122,0.0523,0.0407,-0.0095,0.0211,0.0354,0.0392,-0.0405,-0.0146,0.0042,-0.0004,0.0242,-0.0045,-0.0034,0.07,0.0635,-0.0207,0.0133,0.022,0.044,0.0034,0.0175,-0.0034,-0.0581,-0.0283,-0.0329,-0.0182,-0.0411,-0.0009,0.007,-0.0005,-0.0144,-0.0086,-0.0576,0.0156,-0.0318,0.0484,-0.0544,0.0341,0.0683,0.0496,-0.0077,0.0364,0.076,0.0153,-0.0472,0.0495,-0.014,0.0016,0.0167,0.0515,0.0642,-0.0197,-0.0623,-0.2125,-0.0457,0.0114,-0.027,-0.0396,-0.07,0.0269,-0.0084,0.0071,0.0877,0.0501,-0.0125,-0.0629,-0.0129,-0.0327,0.0109,0.0244,0.0286,-0.0588,-0.001,-0.0379,-0.0158,0.0335,-0.0735,0.0513,0.0362,0.2469,0.0341,-0.0031,-0.0297,-0.0065,-0.0301,0.0004,-0.1184,0.0819,-0.0268,0.0278,0.0289,-0.0439,-0.037,-0.0687,0.0444,-0.0473,-0.0858,-0.0573,0.0053,-0.0219,0.0189,-0.0183,0.0592,-0.003,-0.0243,0.0727,0.0137,0.0019,-0.0166,-0.0811,0.0434,-0.0474,0.0152,0.0221,-0.043,0.0256,-0.0557,0.0375,-0.0404,-0.0067,-0.0592,0.009,-0.0271,0.0003,0.0854,-0.0556,-0.0224,0.067,0.0566,0.0376,-0.0306,-0.0497,0.0254,0.0615,-0.0567,0.0234,0.0293,0.0121,-0.0439,0.0618,-0.001,0.0187,-0.021,-0.0114,0.0056,-0.0603,0.0019,0.0065,-0.0132,-0.2769,0.0694,0.0219,0.0308,-0.0229,0.0031,0.013,0.009,-0.0457,-0.0069,-0.0087,-0.0036,0.0636,-0.0111,-0.0277,0.013,0.0506,-0.0937,0.0608,-0.0771,0.0156,0.06,0.2119,-0.0325,0.0351,0.0412,-0.0069,0.0209,0.0488,-0.017,0.0388,-0.0098,0.0741,-0.0496,0.0597,0.0465,-0.0881,-0.0081,0.0314,-0.057,0.0413,-0.0051,-0.0609,-0.0436,0.1174,0.03,-0.0121,-0.0503,-0.0245,0.0035,-0.0202,-0.0037,-0.0378,0.0156,0.0211,0.0192,-0.0223,-0.0536,-0.0166,-0.0815,0.0139,-0.0296,-0.021,0.0137,-0.0292]}
{"key":"[Deep learning method for identifying mass composition of ultra-high-energy cosmic rays] We introduce a novel method for identifying the mass composition of ultra-high-energy cosmic rays using deep learning. The key idea of the method is to use a chain of two neural networks. The first network predicts the type of a primary particle for individual events, while the second infers the mass composition of an ensemble of events. We apply this method to the Monte-Carlo data for the Telescope Array Surface Detectors readings, on which it yields an unprecedented low error of 7% for 4-component approximation. We also discuss the problems of applying the developed method to the experimental data, and the way they can be resolved.","layer":0,"vector":[-0.0678,-0.0005,0.0111,-0.0164,0.0394,0.0505,0.0257,-0.0054,0.025,-0.0477,0.0082,-0.0792,0.0231,0.0362,0.0258,-0.0157,0.0163,0.0085,-0.0531,0.0397,0.0478,-0.02,-0.017,-0.0479,0.0478,0.042,-0.0488,-0.047,-0.0557,-0.2427,0.0408,-0.0675,0.0526,-0.0371,0.0469,-0.0036,-0.0116,0.0029,-0.0616,0.0557,0.0172,0.0056,-0.0388,-0.0739,0.0104,-0.0696,-0.0091,-0.0292,0.0339,0.0053,0.0387,-0.0278,-0.0012,0.0724,0.0165,0.0089,0.0369,-0.0459,0.0491,0.0569,-0.0208,0.0412,-0.1808,0.0209,0.0611,0.0574,-0.0173,0.0217,0.0491,0.0233,-0.0185,0.0677,0.0041,0.03,0.0042,0.037,0.0109,-0.0605,-0.0037,0.0328,-0.0148,-0.0022,-0.0377,-0.0327,-0.0117,-0.024,-0.0058,0.0053,0.0488,0.005,-0.0472,0.01,-0.0103,0.0322,-0.067,-0.0219,0.0342,0.0115,0.0031,0.1976,-0.0584,0.0144,0.0484,-0.0234,0.0213,0.001,-0.0427,-0.0162,0.0201,-0.0106,0.0352,-0.0352,-0.0151,-0.0708,0.0165,0.0319,0.0554,0.0287,-0.05,-0.0261,0.0045,0.0163,0.0741,0.0237,-0.0046,-0.0648,0.0119,0.1055,0.0122,0.0543,0.0631,-0.0274,-0.0694,-0.0139,-0.0151,0.0585,0.0741,-0.036,-0.0176,0.0422,-0.0303,-0.0241,-0.0064,-0.1124,-0.0619,0.1069,-0.0759,-0.0041,-0.0354,-0.0527,-0.0372,0.0334,-0.0304,-0.0399,0.0446,0.0069,0.0082,0.0763,-0.0483,0.0169,-0.0195,-0.0266,0.0121,0.1419,-0.008,-0.0406,-0.0325,-0.0195,0.042,-0.0428,0.0522,0.0642,-0.0678,0.0129,0.142,-0.0213,-0.0808,0.013,0.0123,0.0292,0.002,-0.0234,-0.0725,0.0671,0.0332,-0.0659,0.0299,-0.0747,0.0372,-0.0197,-0.0577,0.0499,-0.0306,-0.0054,-0.0148,-0.0301,-0.0202,0.0015,0.013,-0.0394,0.006,0.0144,-0.0382,0.0163,-0.0106,0.0156,-0.0328,0.0111,0.0452,0.0229,-0.0317,-0.0419,0.0262,-0.0034,-0.0123,-0.0246,0.0465,-0.0076,-0.0148,0.0334,0.0222,-0.0666,-0.0427,-0.214,0.0099,0.0316,-0.0478,0.0631,-0.0606,0.058,0.0379,0.0037,0.0129,0.0741,-0.0074,-0.0044,-0.0343,-0.0308,0.069,-0.0152,0.0267,-0.0233,0.001,0.0109,0.0328,-0.0271,-0.0427,0.0287,0.0506,0.1676,0.0953,0.0007,-0.0048,0.0032,0.002,0.0231,-0.0359,0.0292,0.0269,0.0716,0.0548,-0.0622,-0.0069,-0.0386,-0.0064,0.0107,-0.0944,-0.009,-0.049,0.0237,0.0479,-0.0253,-0.017,0.0964,0.0081,-0.0211,-0.0046,-0.0109,-0.0423,-0.1172,0.0352,0.0077,0.0045,-0.0261,-0.0287,-0.0026,-0.059,0.0274,0.0032,-0.0389,-0.01,0.0166,-0.0339,-0.0198,0.1126,-0.0321,-0.0,0.088,-0.0133,0.0248,-0.0397,-0.0513,-0.0079,0.0548,0.0088,0.0069,0.0073,0.0348,0.0204,0.0647,-0.0333,0.024,-0.0386,-0.0236,0.0197,-0.0343,-0.0088,0.0143,0.0155,-0.2986,0.0666,-0.0171,0.0196,-0.0462,-0.0083,0.0374,0.0209,-0.052,0.0133,0.0006,0.019,0.0617,-0.0244,-0.0466,0.0463,0.0185,-0.0046,0.0275,-0.0094,0.0251,0.0778,0.2552,-0.0119,-0.0154,0.0398,-0.0502,0.0428,0.0348,0.0013,0.0181,-0.0019,0.0549,-0.0466,0.0195,0.113,0.0018,0.006,0.0168,0.0223,-0.0133,0.0407,-0.0093,-0.0582,0.0981,-0.0747,-0.03,-0.0771,-0.0099,0.0006,-0.0433,0.0178,-0.0279,0.0078,-0.0052,-0.0182,-0.0589,-0.0386,-0.0441,-0.0354,0.0645,-0.0051,0.01,-0.014,0.006]}
{"key":"[Multiple-Play Stochastic Bandits with Shareable Finite-Capacity Arms] We generalize the multiple-play multi-armed bandits (MP-MAB) problem with a shareable arm setting, in which several plays can share the same arm. Furthermore, each shareable arm has a finite reward capacity and a ''per-load'' reward distribution, both of which are unknown to the learner. The reward from a shareable arm is load-dependent, which is the \"per-load\" reward multiplying either the number of plays pulling the arm, or its reward capacity when the number of plays exceeds the capacity limit. When the \"per-load\" reward follows a Gaussian distribution, we prove a sample complexity lower bound of learning the capacity from load-dependent rewards and also a regret lower bound of this new MP-MAB problem. We devise a capacity estimator whose sample complexity upper bound matches the lower bound in terms of reward means and capacities. We also propose an online learning algorithm to address the problem and prove its regret upper bound. This regret upper bound's first term is the same as regret lower bound's, and its second and third terms also evidently correspond to lower bound's. Extensive experiments validate our algorithm's performance and also its gain in 5G & 4G base station selection.","layer":0,"vector":[-0.079,-0.0327,0.0493,-0.0447,0.0283,0.0212,0.0529,0.0141,0.0226,-0.037,0.0285,-0.0037,-0.0033,0.0672,0.0171,0.0395,-0.0298,-0.0187,-0.0533,-0.0096,0.037,-0.0858,-0.0058,-0.0837,0.0501,-0.0254,-0.049,-0.0812,-0.0251,-0.2091,0.0213,-0.0378,0.0139,-0.0285,-0.0311,-0.0426,-0.0185,0.0791,-0.0439,0.0726,0.0032,0.0448,-0.0316,-0.0732,-0.0376,-0.0535,0.0044,0.0041,0.0064,-0.0312,0.0178,-0.0089,0.0103,0.0254,0.0223,0.0422,0.0131,0.0897,0.0167,0.0361,0.0165,0.042,-0.1372,0.036,0.0381,0.052,-0.0641,0.0233,0.0149,0.043,-0.0026,0.0883,0.0287,0.0549,0.024,0.0104,-0.0074,-0.0229,-0.0138,0.0161,-0.0266,-0.0407,-0.0761,0.0202,-0.0467,-0.0697,0.0044,-0.0081,0.0438,0.0537,-0.0515,0.0403,-0.0246,0.0192,-0.0597,0.0022,0.0325,-0.0118,-0.0655,0.2036,-0.0106,0.0109,0.0168,-0.0264,0.0153,-0.0282,-0.0354,0.015,-0.015,-0.0077,-0.0156,-0.0317,0.0432,-0.0169,0.0073,0.0537,0.0166,0.0441,0.0028,-0.0129,-0.0192,0.0044,0.0908,-0.002,0.0189,-0.0647,0.0006,0.1672,0.0409,-0.0044,0.0023,-0.05,-0.0332,-0.0401,0.0379,0.0223,-0.0143,0.0001,0.0241,0.0112,-0.0169,-0.0354,0.0523,-0.13,-0.0297,0.1026,0.0342,0.063,-0.037,-0.0417,-0.0153,-0.0165,0.0121,-0.0139,-0.0129,0.0228,0.0682,0.0575,-0.0532,0.0411,-0.0426,-0.0518,0.0386,0.0698,0.0085,-0.055,-0.0098,-0.0282,-0.0111,-0.0268,0.0227,0.0415,-0.1105,0.0116,0.058,-0.0249,-0.1048,0.0131,0.0005,-0.0034,-0.0105,0.0045,-0.0236,0.0296,0.0358,0.0115,0.0267,-0.0336,0.0467,0.0394,0.0035,0.0242,-0.018,-0.0062,-0.0407,-0.0405,-0.0248,-0.0148,0.0406,-0.0033,0.0127,-0.0172,-0.0861,0.0172,0.0333,0.0057,0.0312,-0.021,0.0415,0.0199,-0.0023,0.0089,0.0358,-0.0209,-0.0425,-0.0107,0.0431,0.0549,-0.0298,0.0449,-0.0024,-0.0316,-0.005,-0.2502,0.0072,-0.016,-0.0122,0.0159,-0.0131,0.0568,-0.0513,0.0489,0.0589,0.0973,-0.0519,-0.0421,0.0448,0.0014,0.056,0.0207,-0.0041,-0.016,0.0208,0.0081,0.0096,-0.0415,0.0002,0.0798,0.0336,0.2355,0.0132,0.0089,-0.0555,0.0698,0.0608,-0.0299,-0.0609,0.0098,0.0564,0.0529,-0.0281,-0.0077,-0.0321,-0.0224,0.0209,0.002,-0.0987,-0.055,-0.0067,-0.056,0.018,-0.0548,0.0077,0.0498,-0.0192,0.0268,-0.0468,-0.0081,-0.0224,-0.095,0.0843,-0.0198,0.0777,0.028,-0.0372,-0.011,-0.1125,0.0684,-0.0072,-0.038,-0.0599,0.0262,-0.024,0.0123,0.0008,0.0002,0.0264,0.0126,-0.013,0.0531,-0.0489,-0.0374,-0.0369,0.0475,-0.0403,0.005,0.0553,-0.0047,-0.0073,0.0491,0.0293,0.0237,0.001,0.0035,0.024,-0.0347,-0.0188,0.0348,-0.0278,-0.3039,0.0552,0.0415,0.0351,-0.029,0.0316,0.0111,0.0075,-0.0364,0.0012,0.0423,0.0735,0.002,0.0054,0.0249,0.0365,0.0668,-0.049,0.0379,-0.0612,0.0365,0.0326,0.1937,-0.0569,0.0313,0.0356,-0.0036,0.0514,-0.0104,-0.019,0.0146,-0.0229,0.077,-0.0877,0.0391,0.0736,-0.0421,0.0312,0.013,0.0012,-0.0525,-0.0213,-0.0721,0.0068,0.1174,0.0036,-0.0484,-0.0482,-0.0145,0.0166,-0.0252,0.0189,0.0047,-0.0705,0.0102,-0.0217,-0.0184,-0.0809,-0.0255,-0.0362,0.0157,-0.0141,-0.0146,-0.0155,0.0401]}
{"key":"[Weakly Supervised Domain Detection] In this paper we introduce domain detection as a new natural language processing task. We argue that the ability to detect textual segments which are domain-heavy, i.e., sentences or phrases which are representative of and provide evidence for a given domain could enhance the robustness and portability of various text classification applications. We propose an encoder-detector framework for domain detection and bootstrap classifiers with multiple instance learning (MIL). The model is hierarchically organized and suited to multilabel classification. We demonstrate that despite learning with minimal supervision, our model can be applied to text spans of different granularities, languages, and genres. We also showcase the potential of domain detection for text summarization.","layer":3,"vector":[-0.0249,0.0076,-0.0045,-0.0385,0.0117,0.0111,0.0152,0.0314,-0.0044,-0.0807,-0.0275,-0.0518,0.0253,0.0678,0.0254,0.0204,0.0357,0.0489,-0.054,-0.0325,0.069,0.0107,-0.006,0.0049,0.0121,0.0425,-0.0259,-0.0178,-0.0416,-0.1982,0.022,-0.0484,0.0214,-0.023,0.0623,-0.0045,-0.0538,0.0254,-0.0131,0.0379,0.0107,-0.0174,-0.0268,-0.0531,-0.0164,-0.0887,-0.007,-0.029,-0.0252,-0.0327,0.0426,-0.0142,0.0505,-0.0138,0.0245,0.0401,0.057,0.0613,0.0568,0.0498,0.0311,0.0564,-0.1701,0.0539,0.0723,0.0421,-0.0938,-0.0066,-0.0257,0.0763,0.008,0.0707,0.0112,0.0701,-0.0312,0.0034,-0.0082,-0.0355,0.0024,0.0221,0.0053,-0.004,-0.0492,-0.0179,-0.0427,-0.0364,-0.0005,-0.0466,0.0656,0.0245,-0.0445,-0.0538,0.024,0.0336,-0.0691,-0.0576,0.0078,0.0266,-0.0268,0.1962,-0.02,-0.0019,0.0365,-0.0555,0.0265,0.0062,-0.0226,-0.0144,-0.0262,-0.009,-0.0306,-0.0112,-0.0077,-0.0488,0.0283,-0.0214,0.127,0.0227,-0.0261,-0.0237,-0.0169,-0.0355,0.0449,-0.0531,0.0153,-0.0392,0.0729,0.1483,0.065,-0.0227,0.0311,-0.0147,-0.0636,-0.0385,0.0038,0.0351,0.0336,0.0227,0.0308,-0.0117,-0.0868,-0.0681,0.0066,-0.0639,-0.0661,0.1289,-0.0482,0.0093,-0.0729,-0.0251,-0.0029,0.0217,-0.0337,-0.0606,0.0154,0.0273,0.1045,0.0016,-0.0274,-0.0123,0.0176,-0.0635,-0.0016,0.1167,0.0355,-0.1011,-0.0331,0.0047,0.0047,-0.0684,0.0588,0.013,-0.0268,0.0311,0.0515,-0.0124,-0.0814,0.0235,0.0339,0.007,0.0128,-0.0364,-0.0505,0.0757,0.0552,-0.0381,-0.0026,-0.0654,0.0517,0.0297,-0.0722,0.0056,0.0043,-0.0014,-0.0324,-0.0185,-0.0019,-0.0154,0.0145,-0.0147,0.0082,0.0104,-0.0284,0.0182,-0.011,-0.0111,-0.0006,0.0108,0.0257,-0.0361,0.008,0.043,0.0378,-0.036,0.0015,-0.0469,0.0266,0.0662,0.0064,0.0216,0.0332,-0.0457,-0.0264,-0.2401,0.0004,0.0389,-0.0346,0.0446,-0.042,0.0471,0.0217,0.0616,0.0608,0.0276,-0.055,-0.039,0.0199,-0.0415,0.0471,0.0063,0.0008,0.0049,0.0203,0.0313,0.0462,-0.0036,-0.0868,0.043,-0.0156,0.2049,0.0748,0.0545,-0.0341,0.0502,0.0273,-0.0242,-0.0757,0.0466,0.014,0.0525,-0.0462,-0.0517,-0.0245,-0.0118,0.0499,0.0024,-0.1038,-0.0291,-0.0457,-0.0164,-0.014,-0.0549,0.0704,0.0457,-0.0355,0.052,0.0317,-0.0415,-0.0226,-0.0994,0.0075,-0.0127,0.0265,0.0327,-0.0097,0.0744,-0.0715,0.0825,0.0021,-0.0703,-0.0153,0.0356,-0.029,-0.0256,0.0555,-0.0046,-0.0317,0.0241,0.0168,0.0065,-0.0188,-0.0872,-0.0107,0.0579,-0.0508,0.0374,0.0002,0.0085,0.0168,0.0794,0.017,0.022,0.0146,0.0216,0.0524,-0.0455,-0.0283,0.0134,-0.0093,-0.2922,0.0231,-0.0016,0.0203,-0.0021,0.0535,0.0336,0.0513,-0.0016,0.0242,0.0188,0.0599,-0.0214,-0.0325,-0.032,0.074,0.0566,-0.0372,0.0078,-0.0733,0.0178,0.0177,0.1975,-0.0166,0.0122,-0.0173,-0.0297,-0.0116,0.0004,0.0074,-0.0072,-0.0185,0.0958,-0.0223,0.0255,0.0882,-0.0064,0.0076,0.0265,-0.0128,0.0244,0.0277,-0.0685,-0.0621,0.0526,-0.0038,-0.0001,-0.0498,-0.0137,0.0358,-0.0123,0.0138,-0.0099,0.0389,0.0077,0.0154,-0.0227,-0.0168,-0.0768,-0.0744,-0.0263,-0.0699,-0.0081,0.0633,-0.0]}
{"key":"[Neural View-Interpolation for Sparse Light Field Video] We suggest representing light field (LF) videos as \"one-off\" neural networks (NN), i.e., a learned mapping from view-plus-time coordinates to high-resolution color values, trained on sparse views. Initially, this sounds like a bad idea for three main reasons: First, a NN LF will likely have less quality than a same-sized pixel basis representation. Second, only few training data, e.g., 9 exemplars per frame are available for sparse LF videos. Third, there is no generalization across LFs, but across view and time instead. Consequently, a network needs to be trained for each LF video. Surprisingly, these problems can turn into substantial advantages: Other than the linear pixel basis, a NN has to come up with a compact, non-linear i.e., more intelligent, explanation of color, conditioned on the sparse view and time coordinates. As observed for many NN however, this representation now is interpolatable: if the image output for sparse view coordinates is plausible, it is for all intermediate, continuous coordinates as well. Our specific network architecture involves a differentiable occlusion-aware warping step, which leads to a compact set of trainable parameters and consequently fast learning and fast execution.","layer":3,"vector":[-0.0181,-0.0262,-0.0035,-0.0064,0.0437,0.0578,0.0118,-0.0033,0.0247,-0.0169,0.002,-0.0667,0.0358,0.1034,-0.0178,0.014,-0.0029,0.0449,-0.0391,-0.0189,0.0511,-0.058,-0.0283,-0.0494,-0.0009,-0.0196,-0.046,-0.0767,-0.0251,-0.2483,0.0099,-0.0173,0.0784,-0.0113,0.0147,-0.044,-0.0315,0.0737,-0.0668,0.0585,0.0598,-0.0308,-0.0028,-0.0878,-0.0227,-0.0482,-0.0178,-0.0111,-0.0037,-0.0499,0.0323,-0.0337,-0.0114,0.01,0.005,0.0137,0.0467,0.0646,0.0303,0.0,0.0222,0.053,-0.1713,0.0554,0.0412,0.0382,0.0025,-0.0139,0.0333,0.0072,-0.0323,0.0212,0.0098,0.037,-0.0073,-0.0323,0.0009,-0.0694,-0.0251,-0.021,0.0302,-0.0195,-0.0528,0.0019,0.04,-0.0174,-0.0036,-0.0392,0.0073,0.0252,-0.0631,-0.0049,-0.0743,-0.0207,-0.068,-0.0089,0.0331,0.0385,-0.0301,0.2118,-0.0458,0.0273,0.1014,-0.012,0.0661,-0.0161,-0.0336,0.0037,-0.0498,0.0348,0.0001,-0.0239,-0.0253,-0.0372,0.0174,0.0135,0.004,0.0376,0.0256,0.013,-0.0087,0.0151,0.0224,-0.0408,0.0404,-0.1046,0.0506,0.1412,0.0662,0.0537,0.0437,-0.0409,0.0023,-0.0281,0.0305,0.0253,0.0428,0.0378,0.004,-0.0097,-0.0266,-0.0567,0.0307,-0.063,-0.0435,0.0654,-0.0219,0.0445,-0.0292,0.0097,-0.0504,0.0442,-0.0645,-0.0029,-0.0316,0.0214,0.0339,0.0451,-0.0836,0.0608,-0.0325,-0.0861,-0.0185,0.0721,0.0204,-0.0831,-0.0192,0.0233,-0.0373,-0.0156,-0.0088,0.0103,0.0233,0.0099,0.0893,0.0392,-0.0822,-0.0052,0.0312,0.0109,0.0014,-0.057,-0.0026,0.0571,0.0479,-0.0238,0.0214,-0.0058,-0.0296,0.0245,-0.0462,0.023,-0.0493,0.0261,-0.0427,0.0389,-0.025,-0.007,0.03,-0.0632,0.0239,-0.062,-0.0544,0.0146,0.0174,-0.0387,-0.0054,0.0137,0.0185,0.0604,-0.0523,-0.015,0.0371,-0.0419,-0.0273,0.0208,0.0113,-0.0066,-0.0029,0.0312,-0.0228,-0.1131,-0.0666,-0.2414,0.0249,-0.0389,-0.0365,0.0276,-0.0716,0.0598,-0.0001,0.0374,0.0722,0.068,-0.028,0.0077,-0.0084,-0.0005,0.0193,0.0572,0.043,-0.01,-0.0181,-0.0033,0.0062,0.0039,-0.0866,0.0487,0.0171,0.2475,0.0195,0.0357,-0.0556,0.0323,0.0388,-0.0359,-0.0687,0.0329,-0.0035,0.0706,0.0258,-0.0288,-0.0437,-0.008,0.0127,-0.0106,-0.1159,-0.0157,-0.0328,-0.0038,0.0261,-0.034,0.0231,0.0074,-0.0934,0.031,-0.0011,0.0034,-0.0229,-0.0835,0.0424,-0.0209,0.0292,-0.0034,-0.0607,0.0351,-0.0405,0.0817,0.0161,-0.013,-0.0495,0.0475,-0.004,-0.0196,0.0696,-0.0603,0.0099,0.0821,0.0474,0.0544,0.0123,-0.0416,0.0086,0.0628,0.0044,0.0442,0.017,0.0469,0.0278,0.0239,-0.0351,0.0436,-0.0162,-0.0068,0.0053,-0.0486,-0.0094,0.0376,-0.0192,-0.2732,0.0372,0.0305,0.0246,-0.0112,0.0556,0.0638,0.0429,-0.0052,-0.0152,-0.0451,-0.0142,0.0516,0.0071,-0.0209,0.0117,0.0842,-0.0229,0.0813,-0.0757,0.0193,0.0228,0.1958,-0.0498,-0.0194,0.03,-0.0015,-0.0272,0.0008,-0.0269,0.0438,0.0416,0.0607,-0.065,0.0064,0.0707,-0.0192,0.0498,0.0353,0.0195,0.018,-0.003,-0.0425,-0.0343,0.0876,-0.0153,-0.0107,0.02,-0.0471,-0.0129,-0.0224,0.0182,0.0337,0.0223,-0.0018,0.041,-0.0463,0.008,-0.0282,-0.029,0.0081,-0.0751,-0.0107,-0.0021,0.0325]}
{"key":"[Deep Multimodal Image-Text Embeddings for Automatic Cross-Media Retrieval] This paper considers the task of matching images and sentences by learning a visual-textual embedding space for cross-modal retrieval. Finding such a space is a challenging task since the features and representations of text and image are not comparable. In this work, we introduce an end-to-end deep multimodal convolutional-recurrent network for learning both vision and language representations simultaneously to infer image-text similarity. The model learns which pairs are a match (positive) and which ones are a mismatch (negative) using a hinge-based triplet ranking. To learn about the joint representations, we leverage our newly extracted collection of tweets from Twitter. The main characteristic of our dataset is that the images and tweets are not standardized the same as the benchmarks. Furthermore, there can be a higher semantic correlation between the pictures and tweets contrary to benchmarks in which the descriptions are well-organized. Experimental results on MS-COCO benchmark dataset show that our model outperforms certain methods presented previously and has competitive performance compared to the state-of-the-art. The code and dataset have been made available publicly.","layer":4,"vector":[-0.0249,-0.0098,0.0047,0.0138,0.0113,0.0353,0.0352,0.0345,-0.0091,-0.0021,0.0361,-0.0968,0.0471,0.0551,0.0336,0.0078,0.026,0.0327,-0.0698,-0.0217,0.0473,-0.0165,0.0144,-0.0735,0.0268,-0.0381,-0.0184,-0.0296,-0.0533,-0.2394,0.005,-0.0335,0.024,-0.0087,0.0142,-0.0103,-0.0449,0.0534,-0.036,0.0107,-0.0042,-0.0165,-0.0119,-0.037,-0.026,-0.0605,-0.0212,-0.0177,-0.0181,-0.0522,0.0526,-0.0302,0.036,0.0197,-0.0134,0.0847,0.0768,0.0892,-0.008,0.0156,0.0669,0.0645,-0.1618,0.0978,0.0278,-0.0078,-0.0256,0.0357,0.0192,0.0632,0.0013,0.017,0.0315,0.0442,-0.0198,0.002,-0.0176,-0.0173,-0.073,-0.0585,0.0287,0.0229,-0.0083,-0.0319,-0.0033,-0.0841,-0.0208,-0.011,0.0081,-0.0156,-0.0847,-0.0119,-0.0431,0.0089,-0.0713,-0.0179,0.0294,0.0405,-0.018,0.213,-0.0374,0.0338,0.048,-0.0569,0.0315,-0.0367,-0.0035,-0.0015,-0.0193,-0.015,-0.0471,-0.0204,0.0712,-0.0344,0.0994,0.0019,0.0847,0.0039,0.0112,-0.0218,-0.0502,0.0133,0.0102,-0.0443,0.0064,-0.0304,0.0427,0.1119,0.0311,-0.0266,0.0332,0.038,-0.033,-0.0149,-0.0103,0.0771,-0.0353,-0.0119,0.0033,-0.0234,-0.0058,-0.1164,0.0362,-0.0487,-0.0704,0.1156,-0.0564,0.0136,-0.0396,-0.0126,-0.0048,0.0119,-0.0211,-0.0087,0.0206,-0.0048,0.0565,0.0382,-0.0255,0.0362,0.0169,-0.0219,-0.0282,0.0843,-0.0014,-0.0982,0.0033,-0.0349,0.0009,-0.0315,0.0593,0.0124,-0.0269,-0.0045,0.0773,0.0549,-0.0869,-0.0333,0.0205,0.0194,-0.0133,-0.0803,-0.0426,0.0745,0.0466,-0.0098,-0.0001,-0.0799,0.0364,0.0236,0.0361,0.0439,-0.0282,0.0332,-0.009,-0.0106,-0.0156,-0.0225,0.0611,-0.0127,0.0243,-0.0241,-0.0408,0.0018,-0.0171,-0.0117,-0.0193,0.03,0.0587,0.0193,-0.0339,-0.0168,0.0203,-0.0298,-0.0018,-0.0668,0.0406,0.0545,0.0084,0.0416,-0.0062,-0.0622,-0.0024,-0.2333,-0.0103,0.0013,0.0145,0.0326,-0.0635,0.0071,-0.0328,0.0787,0.0759,0.0753,-0.0438,-0.0334,0.0214,0.0234,0.0243,0.0487,0.063,0.0103,-0.0395,0.0003,0.0091,-0.012,-0.084,0.0805,-0.0141,0.2078,0.0749,0.005,-0.0434,0.0341,0.0667,-0.0658,-0.1085,0.0489,0.0087,0.0787,0.0136,-0.0349,-0.0173,-0.0508,0.0587,0.0307,-0.0856,-0.0334,-0.0385,-0.0485,-0.0208,-0.0392,0.0501,0.0284,-0.0522,0.0088,0.0119,-0.0503,0.0068,-0.1096,-0.0234,-0.0507,0.0115,0.0467,0.0043,0.0094,-0.0777,0.0407,0.009,-0.0344,-0.0484,0.0064,0.0097,-0.0241,0.0524,-0.028,0.0555,0.0496,0.0263,0.0575,-0.0162,-0.061,-0.0108,0.0788,0.0288,0.0529,-0.0237,0.0166,0.0096,0.0703,0.0011,0.0767,-0.0304,0.0445,0.0023,-0.0474,-0.0475,0.0108,0.0053,-0.2825,0.0338,0.0247,0.0222,-0.003,0.0241,0.0341,0.0306,-0.0031,-0.031,0.0052,0.072,0.0405,-0.0379,-0.0143,0.0236,0.0725,-0.0327,0.0552,-0.0243,-0.0119,-0.0035,0.2123,-0.0495,-0.008,-0.014,-0.0392,-0.0387,0.0019,-0.0056,0.0161,0.01,0.0968,-0.0375,0.0054,0.0683,-0.026,0.0415,0.033,0.0174,0.0109,0.0308,-0.0408,-0.0624,0.0392,-0.0042,0.0391,0.0051,-0.0275,-0.0153,-0.0292,-0.0026,-0.0322,0.0086,0.0281,0.0089,-0.043,-0.0156,-0.0193,-0.029,0.0193,-0.0322,-0.0717,0.0139,0.0014]}
{"key":"[An iterative clustering algorithm for the Contextual Stochastic Block Model with optimality guarantees] Real-world networks often come with side information that can help to improve the performance of network analysis tasks such as clustering. Despite a large number of empirical and theoretical studies conducted on network clustering methods during the past decade, the added value of side information and the methods used to incorporate it optimally in clustering algorithms are relatively less understood. We propose a new iterative algorithm to cluster networks with side information for nodes (in the form of covariates) and show that our algorithm is optimal under the Contextual Symmetric Stochastic Block Model. Our algorithm can be applied to general Contextual Stochastic Block Models and avoids hyperparameter tuning in contrast to previously proposed methods. We confirm our theoretical results on synthetic data experiments where our algorithm significantly outperforms other methods, and show that it can also be applied to signed graphs. Finally we demonstrate the practical interest of our method on real data.","layer":0,"vector":[-0.0324,0.0102,0.0296,0.0087,0.0657,0.0313,0.0675,0.0249,0.0242,-0.0048,0.0133,-0.053,0.0157,0.0502,0.0193,0.0328,-0.0079,0.0206,-0.0134,-0.0119,-0.0025,-0.0731,0.0006,-0.0464,0.074,0.0417,-0.0516,-0.0203,-0.0704,-0.2325,-0.0168,-0.0276,0.0681,-0.0261,-0.0135,0.01,0.0249,0.0667,-0.0039,0.0725,0.002,0.0321,-0.0344,-0.056,-0.0431,-0.0258,-0.0281,0.0147,-0.0276,-0.0609,0.0186,-0.0108,0.0235,0.0515,0.0253,0.0261,0.0418,0.0035,0.023,0.0493,0.0272,0.0805,-0.1624,0.02,0.0641,-0.0194,-0.0428,0.0133,0.0335,0.076,0.0199,0.0537,-0.0233,0.042,0.0539,0.0336,-0.0333,-0.0202,-0.0368,0.019,-0.065,-0.0558,-0.0229,0.0416,-0.0039,-0.0554,0.0314,-0.0615,0.0274,0.0084,-0.0314,0.0136,0.0094,0.0178,-0.081,-0.0492,-0.0,-0.0019,-0.016,0.1744,-0.0632,0.0268,0.0626,-0.0043,0.0419,-0.0326,-0.0092,-0.0784,0.0074,0.0022,0.0284,-0.0326,-0.0048,-0.0949,0.0048,-0.0615,0.0986,0.0521,0.0142,-0.0431,-0.0525,0.0323,0.0468,-0.0125,0.0358,-0.0812,0.0219,0.1136,0.0622,0.0359,0.0372,0.0227,-0.0241,0.0006,0.0115,-0.0119,0.0104,0.0095,0.0313,-0.0759,-0.0053,-0.0479,0.0269,-0.077,-0.069,0.1387,-0.0275,0.026,-0.0453,-0.0166,-0.0001,0.0104,-0.0554,-0.0515,-0.0026,0.0217,0.0284,0.0404,-0.0473,-0.0042,-0.0421,-0.0298,-0.0252,0.1279,0.0368,-0.0915,-0.0495,0.0455,-0.0008,-0.0331,0.0552,0.0132,-0.0072,0.0542,0.0709,0.0357,-0.0884,-0.0403,-0.0032,-0.0105,0.0082,-0.0223,-0.0488,-0.0174,0.0126,-0.0185,0.006,-0.0309,-0.0147,0.0192,-0.0402,-0.0077,-0.0081,-0.0058,-0.05,-0.047,0.0056,-0.0167,-0.013,0.0122,0.0061,0.0227,-0.0667,0.011,-0.0242,0.0563,-0.0336,-0.0097,0.04,-0.0099,-0.0298,-0.0035,0.058,-0.0424,0.0029,-0.0272,0.0433,0.0444,-0.0249,0.0656,0.06,-0.0067,-0.0627,-0.2307,-0.0046,-0.0084,0.0154,0.0375,-0.0607,0.0021,0.0157,0.0803,0.0972,0.0556,0.0064,-0.0463,0.0042,0.0161,0.0332,0.0463,0.0299,0.0086,0.028,-0.0041,-0.0174,-0.0247,-0.05,0.0709,-0.0069,0.2098,0.0208,0.0221,-0.0022,0.0269,0.0164,-0.0813,-0.085,0.0438,0.0818,0.0955,-0.0032,-0.0252,-0.0264,-0.0211,-0.0006,0.0159,-0.0875,-0.0642,-0.0358,0.0144,0.0221,-0.0403,-0.0419,0.0365,0.0144,0.0656,0.0146,0.0065,-0.0382,-0.0614,-0.0281,-0.036,0.0196,0.0079,-0.0615,-0.0082,-0.0707,0.0607,-0.0192,-0.042,-0.0207,-0.0287,-0.029,-0.0371,0.1,-0.0006,0.0026,0.0519,-0.0087,0.0391,-0.03,-0.0371,0.0165,0.084,-0.0883,0.0628,0.0521,0.0224,0.0156,0.0706,0.015,0.0306,0.0185,0.0014,-0.0063,-0.0725,0.0061,0.0074,-0.0304,-0.292,0.055,-0.0037,0.0463,-0.0389,0.0077,0.0425,0.0314,-0.0401,0.0062,0.0179,0.0373,0.0506,-0.0248,-0.0054,0.0465,0.0155,-0.0515,0.059,-0.0486,0.0242,0.0283,0.2028,-0.0527,0.021,0.054,-0.0318,0.025,0.0404,-0.0184,-0.0302,0.0048,0.079,-0.0447,0.0595,0.0614,-0.0469,0.0108,0.0139,-0.044,-0.0087,0.0027,-0.0437,0.0098,0.0882,-0.0076,-0.037,-0.0679,0.0343,0.0481,-0.0594,0.0393,-0.0184,-0.0152,-0.001,0.0324,-0.0217,-0.0358,-0.0296,-0.0646,-0.0264,-0.0799,-0.0161,0.006,-0.0249]}
{"key":"[Quantifying Membership Inference Vulnerability via Generalization Gap and Other Model Metrics] We demonstrate how a target model's generalization gap leads directly to an effective deterministic black box membership inference attack (MIA). This provides an upper bound on how secure a model can be to MIA based on a simple metric. Moreover, this attack is shown to be optimal in the expected sense given access to only certain likely obtainable metrics regarding the network's training and performance. Experimentally, this attack is shown to be comparable in accuracy to state-of-art MIAs in many cases.","layer":1,"vector":[-0.0051,-0.0123,-0.0391,-0.017,0.0399,0.0036,0.0572,0.0129,0.0225,-0.0219,0.0461,-0.0681,0.0544,0.065,-0.0106,-0.0175,-0.0158,0.0407,-0.0077,0.0383,0.012,-0.063,0.0315,-0.0281,0.0168,0.0284,-0.0547,0.0337,-0.0588,-0.2628,-0.0003,-0.063,0.0677,-0.0085,0.0074,-0.0458,-0.047,0.0303,-0.003,0.0301,0.0415,0.0455,-0.0284,-0.0225,-0.0284,-0.0211,-0.0372,0.0021,-0.0422,-0.0587,0.02,-0.0025,-0.0062,0.0594,0.0514,0.0112,0.0557,0.0457,0.0253,0.0424,-0.0073,0.0365,-0.1469,0.0246,0.0416,0.0488,-0.039,-0.0285,-0.0104,0.0329,-0.0156,0.0263,0.0045,0.0484,0.0241,0.0179,-0.0114,0.0097,-0.0083,0.0275,-0.0106,-0.0517,-0.0305,0.0117,-0.0098,-0.0449,0.0134,-0.0474,-0.0035,0.0165,-0.0518,-0.0203,0.0134,0.0377,-0.0251,-0.0276,0.0361,0.0072,-0.1011,0.2064,-0.0448,0.0157,0.0157,-0.0327,0.0388,0.0028,-0.0202,-0.058,-0.0143,-0.0131,0.0077,-0.0044,-0.0093,-0.0307,0.0419,0.0002,0.0856,0.0138,-0.0115,0.0074,-0.0353,0.0192,0.0605,-0.0007,-0.0023,-0.0573,0.0259,0.1623,0.0571,0.0459,0.0059,-0.0281,-0.0039,-0.0413,0.0444,0.0249,0.0097,0.0259,0.0294,-0.0003,-0.0595,-0.0365,0.0505,-0.0461,-0.0366,0.1343,-0.0313,0.0207,-0.0127,-0.0136,-0.0055,-0.0013,-0.0236,-0.0493,0.0284,0.0107,0.0269,0.0314,-0.046,-0.026,-0.0171,-0.0426,-0.0284,0.1011,0.0664,-0.0765,-0.0317,-0.0102,0.0231,-0.0438,0.0189,-0.0169,-0.0246,0.03,0.0211,0.0022,-0.0522,-0.0159,-0.0208,0.0304,-0.0129,-0.0517,-0.0469,0.0103,0.0374,-0.0294,-0.0097,-0.0179,0.0287,0.0359,-0.0611,0.0477,-0.0982,-0.011,-0.0169,-0.0456,-0.0182,-0.009,0.019,-0.068,-0.0054,0.0064,-0.0824,-0.0058,-0.0329,0.0344,-0.0132,-0.0053,0.0144,0.0341,-0.021,-0.0332,0.0374,-0.0371,0.0058,0.0208,0.0396,0.0442,-0.0014,0.0428,0.0376,-0.0009,-0.0239,-0.2218,-0.0212,-0.0139,-0.0402,0.0766,-0.1052,0.0644,-0.038,0.0234,0.0564,0.0527,-0.0482,-0.0093,0.0281,-0.026,0.0911,-0.0182,0.0153,-0.0188,0.0315,-0.0612,0.0447,-0.0418,-0.0727,0.0265,0.0435,0.2309,0.0647,0.0667,-0.0315,0.0196,0.0322,-0.0351,-0.0805,0.043,0.0413,0.0241,0.0055,-0.0058,-0.0225,-0.0506,0.0294,0.0243,-0.1066,-0.0102,-0.0467,-0.0169,0.0691,-0.0753,0.0337,0.0697,0.0057,0.0546,0.0413,0.032,-0.0614,-0.0594,0.0183,-0.0377,0.0987,0.0122,-0.0214,-0.0248,-0.1026,0.0606,-0.051,-0.0129,-0.0585,0.0661,-0.0333,0.0188,0.0727,0.0178,-0.0042,0.0331,0.0126,0.0211,-0.0732,-0.0508,-0.0464,0.0645,-0.0238,0.0109,-0.0075,0.0101,0.021,0.1006,0.0384,0.0693,-0.0386,-0.0219,-0.0232,-0.0972,-0.0376,0.0316,0.0171,-0.298,-0.0045,0.0061,0.0758,-0.0392,0.0049,0.0883,-0.0175,-0.0738,-0.0149,0.0451,0.052,0.0272,-0.0017,0.0158,0.043,0.056,-0.0763,0.0332,-0.0343,0.0146,0.0433,0.206,-0.0218,0.028,0.0298,0.0056,0.0081,0.0248,0.0095,0.0154,-0.0074,0.0691,-0.0454,0.0373,0.0361,0.001,0.0455,0.0584,-0.0276,0.003,-0.0604,-0.0349,0.0084,0.0795,-0.0125,-0.0226,-0.0406,0.0172,-0.008,-0.0048,0.0056,-0.0147,0.0041,0.0377,0.0297,-0.0353,-0.037,-0.0547,-0.0297,-0.0154,-0.0558,-0.0174,0.006,-0.014]}
{"key":"[MAREO: Memory- and Attention- based visual REasOning] Humans continue to vastly outperform modern AI systems in their ability to parse and understand complex visual scenes flexibly. Attention and memory are two systems known to play a critical role in our ability to selectively maintain and manipulate behaviorally-relevant visual information to solve some of the most challenging visual reasoning tasks. Here, we present a novel architecture for visual reasoning inspired by the cognitive-science literature on visual reasoning, the Memory- and Attention-based (visual) REasOning (MAREO) architecture. MAREO instantiates an active-vision theory, which posits that the brain solves complex visual reasoning problems compositionally by learning to combine previously-learned elementary visual operations to form more complex visual routines. MAREO learns to solve visual reasoning tasks via sequences of attention shifts to route and maintain task-relevant visual information into a memory bank via a multi-head transformer module. Visual routines are then deployed by a dedicated reasoning module trained to judge various relations between objects in the scenes. Experiments on four types of reasoning tasks demonstrate MAREO's ability to learn visual routines in a robust and sample-efficient manner.","layer":2,"vector":[-0.0256,0.0036,0.026,-0.0074,-0.0051,0.0259,0.061,0.012,0.0148,-0.0082,-0.0049,-0.1051,0.05,0.0868,-0.0004,0.0139,-0.0224,0.023,-0.0112,0.0125,0.0258,-0.0294,-0.0533,-0.0562,-0.0276,0.0489,-0.0382,0.0053,-0.0533,-0.232,0.0059,-0.0409,0.0537,-0.0056,-0.0202,-0.0334,-0.0211,0.0381,-0.0213,0.016,0.0101,0.0411,-0.0409,-0.0516,-0.053,-0.0612,-0.0061,-0.0192,0.0042,-0.0513,-0.007,-0.0726,0.0068,0.0051,0.0002,0.0701,0.0498,0.0484,0.0572,0.0432,0.0423,0.0536,-0.1463,0.0893,0.0316,0.0134,-0.0434,0.0009,0.0013,0.0281,-0.0216,0.0113,-0.0012,0.0282,-0.0091,-0.0123,-0.0334,-0.0149,-0.0015,-0.0379,0.0285,-0.0134,-0.0345,0.0085,0.0216,-0.0072,-0.013,-0.031,0.0424,-0.0171,-0.0389,-0.0118,-0.0458,0.0124,-0.0094,0.0059,0.0394,0.0156,-0.0459,0.1975,-0.0514,0.0195,0.0371,-0.0657,0.0322,-0.024,-0.0322,-0.0285,-0.0514,0.021,-0.0259,-0.0001,0.023,0.0072,0.0614,0.0396,0.0665,-0.007,-0.0367,0.003,0.0109,-0.0074,0.0278,0.0038,-0.0116,-0.0923,0.0288,0.1671,0.0165,0.0185,0.0931,-0.0289,-0.0086,-0.0595,0.0151,0.0601,0.0558,-0.0209,-0.02,-0.0271,-0.0263,0.013,0.0372,-0.0637,-0.0385,0.0859,-0.0165,0.047,-0.0101,0.0078,-0.0324,-0.0002,-0.0221,-0.0302,-0.0493,-0.0306,0.0176,0.0477,-0.0569,0.0366,-0.0335,-0.0524,-0.0285,0.0745,-0.022,-0.0735,-0.0212,-0.0431,0.0157,-0.0366,0.0441,0.0243,-0.0089,0.0147,0.0868,0.0473,-0.0663,0.0306,-0.0354,0.0016,0.0511,-0.0821,-0.0178,0.0217,0.0554,-0.0498,0.0223,-0.0499,0.0099,0.035,0.0096,0.0547,-0.0048,-0.0188,-0.0293,-0.0074,-0.026,0.0009,-0.0045,0.0184,0.0173,-0.02,-0.0467,-0.0093,0.0293,0.0011,-0.0503,0.0137,0.0554,0.0493,-0.0391,0.0275,0.0236,-0.0184,-0.0287,-0.0138,0.011,0.0214,-0.0078,-0.0024,0.0549,-0.0559,-0.0553,-0.2343,-0.0381,0.0052,-0.0151,0.0305,-0.0555,0.0382,0.0156,0.0086,0.0381,0.0585,-0.0427,-0.0311,-0.0005,-0.0299,0.0541,0.0054,0.0272,-0.0511,-0.0402,-0.0362,0.0357,0.0358,-0.0924,0.0494,0.0051,0.2753,0.0366,0.0504,-0.0079,0.0361,0.0353,-0.0237,-0.0789,0.0527,0.0115,0.043,-0.0366,0.0065,-0.0095,-0.0164,0.0256,-0.0348,-0.0846,-0.0506,-0.0089,0.0075,0.1031,0.0068,0.0058,0.0479,-0.065,0.0023,0.0048,-0.0147,-0.0114,-0.0768,-0.0047,-0.0441,0.0514,0.0167,-0.0338,-0.0141,-0.0472,0.0786,0.0175,-0.017,-0.078,0.0561,-0.0231,-0.02,0.0707,-0.0066,-0.0358,0.0532,0.0378,0.0874,-0.0469,-0.0139,-0.0077,0.0548,-0.0237,0.0027,-0.0285,0.0572,0.0271,0.0721,-0.0523,0.0449,-0.0248,0.0064,0.0239,-0.0576,-0.0242,0.0334,-0.0186,-0.3177,0.044,-0.0198,0.0322,-0.0297,0.0583,0.0587,0.0141,-0.0101,-0.0154,-0.0424,0.0628,0.0517,0.0393,-0.0548,0.034,0.0837,-0.0213,0.0693,-0.0355,0.0081,0.0398,0.2075,-0.0364,0.0349,0.0048,0.0179,-0.071,0.0537,-0.012,-0.0112,0.0321,0.0612,-0.0675,0.0254,0.0832,-0.0652,0.0368,0.058,-0.0093,-0.0069,0.0294,-0.0451,-0.0118,0.0992,0.0176,-0.0093,-0.0202,-0.0179,0.0017,-0.0375,-0.0384,-0.0125,-0.0028,0.053,0.0074,-0.0089,0.0024,-0.0595,-0.0148,-0.0082,-0.093,0.0345,0.0247,-0.028]}
{"key":"[Multivariate Probabilistic Forecasting of Intraday Electricity Prices using Normalizing Flows] Electricity is traded on various markets with different time horizons and regulations. Short-term trading becomes increasingly important due to higher penetration of renewables. In Germany, the intraday electricity price typically fluctuates around the day-ahead price of the EPEX spot markets in a distinct hourly pattern. This work proposes a probabilistic modeling approach that models the intraday price difference to the day-ahead contracts. The model captures the emerging hourly pattern by considering the four 15 min intervals in each day-ahead price interval as a four-dimensional joint distribution. The resulting nontrivial, multivariate price difference distribution is learned using a normalizing flow, i.e., a deep generative model that combines conditional multivariate density estimation and probabilistic regression. The normalizing flow is compared to a selection of historical data, a Gaussian copula, and a Gaussian regression model. Among the different models, the normalizing flow identifies the trends most accurately and has the narrowest prediction intervals. Notably, the normalizing flow is the only approach that identifies rare price peaks. Finally, this work discusses the influence of different external impact factors and finds that, individually, most of these factors have negligible impact. Only the immediate history of the price difference realization and the combination of all input factors lead to notable improvements in the forecasts.","layer":10,"vector":[-0.0122,-0.0131,0.0111,-0.0017,0.0641,0.0423,0.0452,-0.0375,0.0645,-0.0216,-0.0052,-0.0463,-0.0184,0.0433,0.0139,0.0395,-0.0074,0.0273,-0.0564,0.0198,0.0479,-0.0486,-0.0343,-0.0662,0.0521,0.003,0.0219,-0.0121,-0.0362,-0.2646,0.0331,-0.0319,0.0329,-0.0212,0.0071,-0.055,-0.0585,0.0187,0.0166,0.0593,-0.0071,0.0009,-0.0322,-0.0755,-0.0119,-0.0412,-0.0411,-0.0094,-0.0505,-0.0038,0.0386,-0.0309,0.0388,0.0568,0.0478,0.0298,0.039,0.0028,0.0746,0.0664,0.0072,0.0501,-0.2045,0.0445,0.0445,-0.0187,-0.0175,0.0315,-0.0032,0.0432,-0.0032,0.0071,-0.0078,0.0238,0.0036,-0.0117,0.0071,-0.0133,-0.0379,0.0006,0.022,-0.0416,-0.0225,-0.0149,-0.0366,-0.0052,0.0675,-0.0272,0.0451,0.0148,-0.0403,-0.0043,-0.087,0.0219,-0.0607,-0.0046,0.0498,-0.0062,-0.0262,0.1674,-0.0487,0.0997,0.036,-0.0328,0.0089,-0.0338,-0.0806,-0.0027,-0.0047,-0.0514,-0.0288,-0.005,0.0552,-0.0529,0.0266,-0.0044,0.0633,0.015,0.0204,0.022,-0.0684,0.0342,0.028,0.0219,0.0188,-0.0259,0.058,0.1303,-0.005,0.0141,0.0268,-0.0145,-0.0887,0.0068,0.0834,0.0173,-0.0008,0.0015,-0.026,0.0285,-0.0966,-0.031,-0.0386,-0.113,-0.0668,0.1038,-0.0378,0.0042,-0.0468,-0.0449,-0.0228,0.0041,0.02,-0.0383,0.0487,0.0181,0.0593,0.01,-0.0159,-0.0168,-0.0431,-0.0107,-0.0482,0.091,0.0067,-0.0767,-0.0006,-0.0308,0.0211,-0.0104,0.0012,0.0804,-0.0166,0.0151,0.1117,0.0248,-0.0263,0.0141,-0.0084,0.0025,0.0312,-0.0292,0.011,0.0126,0.0185,-0.0582,-0.0002,0.0045,-0.0159,0.0632,-0.0357,-0.0117,-0.0168,0.0513,0.0083,-0.0151,-0.0684,-0.011,0.0163,-0.0688,0.0243,0.0328,-0.0365,-0.0129,-0.0426,0.0713,0.0005,-0.0078,0.0704,0.0107,0.0262,-0.0114,0.1105,-0.0187,-0.0512,0.0473,-0.0165,0.0659,0.033,0.0483,0.01,-0.0281,-0.0303,-0.2136,0.0219,-0.0011,-0.0107,0.0943,-0.0531,0.0572,-0.0695,0.0146,0.0868,0.0693,-0.0248,-0.0304,0.0039,-0.0041,0.044,0.04,0.0476,-0.0516,0.0118,-0.0421,0.0176,-0.0327,-0.0821,0.0391,-0.0103,0.168,-0.0167,0.0474,-0.0476,0.0372,0.0086,-0.0069,-0.0581,0.0696,0.0247,0.1011,-0.016,-0.0592,-0.0414,-0.0146,0.0029,-0.0025,-0.0828,-0.0388,-0.028,0.0098,0.0251,-0.0514,0.0103,0.0456,-0.0552,0.0819,-0.043,0.0497,-0.0658,-0.0884,0.0735,-0.0338,0.0079,0.0044,-0.0444,0.0357,-0.0541,0.0306,-0.0146,0.01,-0.044,-0.0037,-0.0063,-0.0536,0.1149,0.0049,-0.0061,0.037,0.0186,0.0402,0.0019,-0.0244,0.005,0.0579,-0.0428,0.0834,0.0568,0.0242,0.0129,0.0449,-0.0365,0.0277,-0.0008,0.0051,-0.0105,-0.0185,0.0032,0.0403,-0.0002,-0.2848,0.0258,-0.0007,-0.0102,-0.0216,0.0113,0.0028,0.0227,-0.012,-0.0369,0.0077,0.0476,0.0439,-0.0123,-0.0556,0.0054,0.0736,-0.0616,0.0217,-0.008,0.0014,0.0455,0.2331,-0.0439,0.0232,-0.0027,-0.0344,0.0415,0.0365,-0.0307,-0.0005,0.0012,0.1012,-0.0421,0.0537,0.0521,-0.0209,0.0385,0.0285,-0.0339,-0.0666,0.0332,-0.0558,-0.0255,0.0735,0.0124,-0.0231,-0.0514,-0.0078,0.0048,-0.0347,0.0258,-0.0378,0.0076,0.0341,0.0019,-0.0046,-0.0329,-0.0211,-0.0658,-0.0047,-0.0618,-0.028,-0.058,-0.001]}
{"key":"[Sparse Nonlinear Regression: Parameter Estimation and Asymptotic Inference] We study parameter estimation and asymptotic inference for sparse nonlinear regression. More specifically, we assume the data are given by $y = f( x^\\top \\beta^* ) + \\epsilon$, where $f$ is nonlinear. To recover $\\beta^*$, we propose an $\\ell_1$-regularized least-squares estimator. Unlike classical linear regression, the corresponding optimization problem is nonconvex because of the nonlinearity of $f$. In spite of the nonconvexity, we prove that under mild conditions, every stationary point of the objective enjoys an optimal statistical rate of convergence. In addition, we provide an efficient algorithm that provably converges to a stationary point. We also access the uncertainty of the obtained estimator. Specifically, based on any stationary point of the objective, we construct valid hypothesis tests and confidence intervals for the low dimensional components of the high-dimensional parameter $\\beta^*$. Detailed numerical results are provided to back up our theory.","layer":0,"vector":[-0.0132,0.0127,0.04,0.0136,0.0061,0.0434,0.0098,0.0488,0.0425,-0.0491,0.027,-0.0344,0.037,0.0535,-0.0026,0.0081,0.0209,0.0708,-0.0689,0.0547,0.069,-0.077,0.0063,-0.0355,0.0503,-0.0206,-0.0206,-0.0683,-0.0238,-0.244,0.0218,-0.0435,0.063,-0.0063,0.0604,0.0063,-0.023,0.0398,-0.0018,0.0646,-0.0139,0.0206,-0.0426,-0.0429,-0.0315,-0.0716,-0.0149,-0.0183,-0.0436,-0.0288,-0.0099,0.0166,0.0445,0.019,0.0233,-0.0058,0.0037,0.0095,0.0675,0.0421,0.0221,0.0062,-0.2004,0.0505,0.0495,0.0466,-0.0421,-0.05,0.0205,0.0735,0.0159,0.0348,-0.0027,0.0707,0.0084,-0.0341,0.0161,-0.0187,0.0126,0.0283,0.0621,-0.0319,-0.0377,0.0173,-0.0205,-0.0749,0.0576,-0.0708,0.0184,0.0064,-0.0636,0.0115,-0.0421,-0.0095,-0.0678,-0.0082,0.0459,0.0679,-0.0158,0.1916,-0.059,0.0606,0.0259,0.007,0.0209,-0.039,-0.0245,-0.0066,-0.0021,0.001,-0.0277,-0.0315,0.0452,-0.02,0.0044,-0.0389,0.0259,-0.0077,-0.0165,0.0111,0.0078,-0.002,0.0262,-0.008,0.0363,-0.0658,-0.0064,0.1436,0.0304,0.0261,0.0224,-0.0218,-0.0399,-0.0361,-0.0034,0.0104,0.0359,0.0364,0.0181,0.0083,-0.0491,-0.0976,0.0345,-0.0693,-0.0717,0.1123,-0.0262,-0.0229,-0.0779,-0.065,-0.006,0.0391,-0.0045,0.0047,0.0492,0.0328,0.0284,0.0203,-0.0679,0.0196,-0.0426,-0.0402,-0.0044,0.0929,-0.0268,-0.0358,-0.0246,0.0171,0.028,0.0312,0.058,0.0125,-0.0263,-0.0017,0.0885,0.0334,-0.0303,0.0254,0.0142,0.0385,-0.0021,-0.0689,-0.0362,0.0186,0.0437,-0.0392,-0.0172,-0.0602,0.0161,0.0265,-0.0654,-0.0079,-0.0431,-0.0167,-0.009,-0.0112,0.0361,-0.0152,0.0149,-0.0558,0.0226,-0.0366,-0.0381,0.0376,0.0442,0.0358,-0.0041,-0.0173,-0.0056,0.0277,-0.0304,-0.0155,0.0699,-0.0201,-0.0301,0.0556,0.0102,0.0262,0.006,0.0453,0.0292,-0.038,-0.0716,-0.2234,-0.0482,0.0107,-0.0194,0.0619,-0.0715,0.0614,-0.0045,0.0992,0.0837,0.0198,-0.0107,-0.0115,0.072,-0.0118,0.0463,0.0512,0.0162,-0.0279,-0.0012,-0.0559,-0.0382,-0.0403,-0.0488,0.0351,-0.0046,0.1567,0.0022,0.0169,-0.0427,0.0095,0.0009,0.0108,-0.0786,0.05,0.0266,0.0589,-0.011,-0.0551,0.009,-0.0184,-0.0149,-0.0055,-0.0558,-0.0647,-0.0308,-0.0153,0.0415,-0.0781,0.0274,0.0333,-0.0315,0.0863,-0.0618,0.0122,-0.0215,-0.1098,-0.0098,-0.0074,0.0352,0.0125,-0.0467,0.0262,-0.0382,0.0942,0.0231,-0.0405,-0.0517,0.0213,-0.0199,-0.0181,0.0581,-0.0047,0.06,0.0688,0.0353,0.0168,-0.0124,-0.0679,-0.0284,0.0759,-0.0561,0.0118,0.0074,0.0031,0.0004,0.0934,-0.0079,0.054,-0.0233,-0.0254,-0.036,-0.0722,-0.0265,0.0308,0.0231,-0.3034,0.0181,0.0239,0.0082,-0.032,0.0153,0.0903,-0.0019,-0.0731,0.0293,-0.0095,0.0428,0.067,-0.0056,0.0404,0.0194,0.0616,-0.073,0.0271,-0.111,-0.008,0.0149,0.1958,-0.0626,-0.0202,0.0551,-0.0189,0.0109,0.0237,-0.0533,0.0287,0.0071,0.07,-0.0305,0.0436,0.0702,-0.0531,0.0562,0.0077,-0.0468,0.0469,0.0173,-0.0627,-0.0302,0.0834,-0.048,-0.0298,-0.0337,0.0024,0.0202,-0.0138,0.007,0.0465,0.0161,0.023,0.0071,-0.0467,0.0067,-0.0393,-0.0642,-0.0075,-0.0241,-0.0515,-0.0021,0.0319]}
{"key":"[LATTE: LSTM Self-Attention based Anomaly Detection in Embedded Automotive Platforms] Modern vehicles can be thought of as complex distributed embedded systems that run a variety of automotive applications with real-time constraints. Recent advances in the automotive industry towards greater autonomy are driving vehicles to be increasingly connected with various external systems (e.g., roadside beacons, other vehicles), which makes emerging vehicles highly vulnerable to cyber-attacks. Additionally, the increased complexity of automotive applications and the in-vehicle networks results in poor attack visibility, which makes detecting such attacks particularly challenging in automotive systems. In this work, we present a novel anomaly detection framework called LATTE to detect cyber-attacks in Controller Area Network (CAN) based networks within automotive platforms. Our proposed LATTE framework uses a stacked Long Short Term Memory (LSTM) predictor network with novel attention mechanisms to learn the normal operating behavior at design time. Subsequently, a novel detection scheme (also trained at design time) is used to detect various cyber-attacks (as anomalies) at runtime. We evaluate our proposed LATTE framework under different automotive attack scenarios and present a detailed comparison with the best-known prior works in this area, to demonstrate the potential of our approach.","layer":4,"vector":[-0.0352,0.0151,0.0391,0.0036,0.0446,0.0302,0.0665,0.0537,0.0198,-0.0472,0.0346,-0.019,0.0401,0.0423,0.0142,-0.0025,-0.0161,0.0265,0.0315,-0.052,0.0078,-0.0589,0.0039,-0.0512,-0.0169,0.0425,0.0089,-0.0078,-0.1084,-0.2182,0.0035,-0.0615,0.0645,-0.0083,0.0006,-0.0427,-0.0603,0.0714,-0.014,0.0176,0.0174,0.0494,-0.0052,-0.0555,-0.0437,-0.0915,-0.0027,0.0002,-0.0169,-0.0811,0.0062,-0.0274,0.0631,-0.0008,0.0196,0.0025,0.0416,0.0254,0.0439,0.0115,0.0522,0.0053,-0.1569,0.0401,0.0704,0.0341,-0.0469,-0.0022,0.0644,0.0205,-0.009,-0.001,-0.0181,0.0601,0.039,0.064,-0.0221,0.0102,-0.0097,-0.0133,-0.0075,-0.0264,-0.0626,-0.0206,-0.0419,-0.0739,0.0045,-0.0155,0.0687,-0.0057,-0.0553,0.0077,0.0372,0.0028,-0.0257,0.0059,-0.0218,-0.016,-0.0814,0.2328,-0.0637,0.0108,0.0061,-0.0061,0.0316,0.0083,-0.0132,-0.0168,-0.0383,0.0144,0.0181,-0.022,0.0208,-0.0332,0.0592,-0.0007,0.0673,0.0553,0.0178,0.0051,0.0147,0.0037,0.0299,-0.0688,0.0636,-0.0799,0.0282,0.1579,0.0176,0.0181,-0.0207,-0.0183,-0.0522,-0.0456,0.0203,0.0114,-0.0267,0.0111,0.0179,-0.003,-0.0641,-0.0336,0.0531,-0.0727,-0.0523,0.0843,-0.0146,0.0375,-0.0359,-0.0044,-0.027,0.0155,-0.0227,-0.0289,0.0361,0.0215,0.038,0.0283,-0.0674,0.043,-0.0194,-0.0018,-0.0091,0.1013,-0.0083,-0.0866,0.0114,-0.0117,0.0089,-0.0144,0.0001,0.0367,-0.0391,-0.0014,0.0689,0.045,-0.0526,0.0356,-0.047,-0.0207,-0.0002,-0.0295,-0.0589,0.0375,0.0665,-0.058,0.0213,-0.0087,0.0065,0.0578,-0.0464,0.0285,-0.0523,0.0123,-0.0461,-0.0153,0.014,-0.0174,0.0163,-0.0421,0.0341,0.0045,-0.0338,0.0333,-0.0358,-0.0135,-0.0516,-0.0195,0.0292,0.0428,-0.0622,0.0404,0.0867,-0.0448,-0.028,-0.0146,-0.0259,0.0883,-0.0124,0.0045,0.0291,-0.0031,-0.0629,-0.2418,-0.0223,-0.0024,-0.0619,0.0241,-0.0719,0.0332,-0.0115,0.0623,0.0418,0.0842,-0.0074,-0.0059,0.0092,0.0094,0.0967,-0.0061,0.01,-0.0367,0.0266,-0.0433,0.0015,-0.0068,-0.0555,0.03,0.0008,0.2064,0.0161,0.0459,-0.0634,0.0158,0.0428,0.0069,-0.025,0.0622,-0.0213,0.0556,-0.01,-0.0233,-0.0146,-0.0662,0.0137,-0.0128,-0.0892,-0.0452,-0.0464,-0.0318,0.0199,-0.037,0.0185,0.0887,-0.0294,0.062,0.0366,0.0106,-0.0589,-0.0824,0.0054,-0.0178,0.0281,0.0068,-0.0317,0.0277,-0.0994,0.051,-0.0308,-0.0134,-0.0573,0.0265,-0.02,-0.0573,0.1191,0.0321,-0.0183,0.0583,-0.0239,0.0336,-0.011,-0.0196,-0.0202,0.061,-0.047,0.0486,0.0218,0.0273,0.0188,0.0394,0.0164,0.0142,-0.0235,0.0253,0.0022,-0.0262,-0.0374,0.0424,0.0124,-0.2811,0.0531,0.0067,0.064,-0.0161,-0.0034,0.0368,0.0192,-0.0694,0.039,-0.0312,0.044,0.0408,-0.0047,-0.0145,0.0471,0.0414,-0.0365,0.0475,-0.0466,-0.0022,0.0488,0.2298,-0.0296,0.0186,0.0431,-0.042,-0.0247,0.0435,0.004,0.0245,0.0016,0.0639,-0.0618,-0.0052,0.02,-0.0452,0.0458,0.0703,0.0083,-0.001,0.0238,-0.0326,-0.044,0.0998,-0.0552,-0.0128,-0.0796,0.0174,0.0651,0.0063,-0.0336,-0.037,0.0014,0.0427,-0.0036,-0.085,-0.0365,-0.0424,-0.0227,0.0518,-0.0374,0.0012,0.0264,0.0198]}
{"key":"[Resilience of Supervised Learning Algorithms to Discriminatory Data Perturbations] Discrimination is a focal concern in supervised learning algorithms augmenting human decision-making. These systems are trained using historical data, which may have been tainted by discrimination, and may learn biases against the protected groups. An important question is how to train models without propagating discrimination. In this study, we i) define and model discrimination as perturbations of a data-generating process and show how discrimination can be induced via attributes correlated with the protected attributes; ii) introduce a measure of resilience of a supervised learning algorithm to potentially discriminatory data perturbations, iii) propose a novel supervised learning algorithm that inhibits discrimination, and iv) show that it is more resilient to discriminatory perturbations in synthetic and real-world datasets than state-of-the-art learning algorithms. The proposed method can be used with general supervised learning algorithms and avoids inducement of discrimination, while maximizing model accuracy.","layer":5,"vector":[-0.0258,-0.0039,-0.0353,-0.0098,0.0483,0.0087,0.0591,0.0236,0.0239,0.0026,0.0057,-0.0152,0.0245,0.0971,-0.01,0.037,-0.0186,0.0712,-0.0607,0.0097,-0.0072,0.0007,-0.0575,-0.0626,0.0179,0.0112,-0.0178,-0.0101,-0.045,-0.2526,0.0177,-0.0437,0.0106,-0.0556,0.0309,-0.0055,-0.0427,0.0523,-0.0569,0.038,0.0141,-0.0182,-0.047,-0.0637,-0.0544,-0.007,-0.0272,-0.0167,-0.0601,-0.053,0.0302,-0.041,0.0382,0.0412,0.0441,0.0743,0.0743,0.0397,0.0582,0.0554,0.0062,0.0632,-0.1717,0.0423,0.0052,0.0492,-0.0684,-0.026,0.0151,0.0366,-0.016,0.0794,0.0317,0.0219,0.0037,0.0051,0.0139,-0.0445,0.0323,0.0266,0.0373,-0.0531,-0.0835,-0.0111,-0.0297,-0.0857,0.0371,-0.0546,0.0304,0.033,-0.0278,-0.0059,-0.0035,0.0074,-0.0768,-0.0194,0.0553,0.0135,-0.0351,0.202,-0.0422,0.0216,0.0076,-0.0239,0.023,-0.0278,-0.0427,-0.0432,-0.0245,-0.0023,-0.0012,-0.0164,-0.0141,-0.0391,-0.0055,0.0379,0.0514,0.0187,-0.0247,-0.0087,0.0059,0.0055,0.0661,-0.0057,0.0328,-0.0865,0.0402,0.1659,-0.0025,0.0346,0.037,-0.056,-0.0554,-0.0141,-0.0038,0.0319,0.0096,0.017,0.0283,-0.0206,-0.0491,-0.04,-0.0178,-0.0987,-0.0524,0.1181,-0.0195,0.0699,-0.0134,-0.0283,-0.0394,-0.0299,-0.054,-0.0387,0.0026,0.0741,0.022,0.0465,-0.0197,-0.001,0.0235,-0.1002,-0.0286,0.1172,-0.0049,-0.0356,-0.0302,0.0236,0.0314,-0.0047,0.0262,0.0349,-0.0253,0.0275,0.058,0.0146,-0.048,-0.0121,-0.0111,0.0118,0.0681,-0.0516,-0.0461,0.0414,0.0261,-0.0088,-0.0205,-0.016,0.0459,0.0161,-0.0098,-0.0124,-0.0247,-0.0079,-0.0517,-0.0288,-0.0407,-0.0104,-0.0057,-0.0167,-0.0326,0.024,-0.0192,0.0015,0.0082,0.0463,0.0321,-0.0291,0.0522,0.0116,-0.0324,0.001,-0.0018,-0.0381,-0.0349,-0.0153,0.0292,0.0445,0.0104,0.0426,0.066,0.0034,-0.044,-0.2346,-0.022,-0.0238,0.0065,0.0673,-0.0788,0.028,-0.0106,0.0217,0.0757,0.015,-0.0028,-0.0533,0.0315,-0.0013,0.0351,0.0178,0.0224,-0.0161,-0.0192,-0.034,0.0412,-0.0242,-0.1015,0.0435,0.012,0.2259,-0.006,0.0361,-0.0188,0.0044,0.0275,-0.0615,-0.0795,0.0942,0.0095,-0.0054,-0.0284,-0.0289,-0.0085,0.0047,0.0427,0.0341,-0.0813,-0.0472,-0.0275,-0.0402,0.0472,-0.065,0.0276,-0.0017,-0.0066,0.0946,0.0049,-0.0102,-0.0071,-0.0754,0.0473,-0.0499,0.0037,0.0514,-0.0751,-0.0244,-0.0867,0.0379,0.0013,-0.0525,-0.0624,0.0529,0.0187,-0.0159,0.1278,0.0246,-0.0364,0.0416,-0.0084,-0.0215,-0.0538,-0.0564,-0.0057,0.0646,-0.0144,0.0331,0.0535,0.0467,0.0132,0.1081,0.0003,0.0609,0.0045,0.0015,-0.0116,-0.0577,-0.0004,0.0627,0.0123,-0.2501,0.0493,0.0154,0.032,-0.0218,0.0053,0.0145,0.032,-0.0326,-0.0016,-0.0213,0.054,0.0777,-0.0193,0.0021,-0.0087,0.0737,-0.022,0.0337,-0.0303,0.0329,0.0421,0.221,-0.0085,0.0414,0.0239,-0.016,-0.021,0.0055,-0.0161,-0.0064,-0.0118,0.0892,-0.0432,0.0319,0.05,0.0016,0.0141,0.0039,-0.0128,-0.0116,-0.0075,-0.0354,-0.0284,0.1248,0.0228,0.0037,-0.0328,-0.0334,0.0168,-0.0382,-0.01,-0.0285,-0.018,0.0508,0.0625,-0.0317,-0.0363,-0.0354,-0.0644,0.022,-0.0307,-0.0143,-0.0143,-0.0157]}
{"key":"[NEUROSPF: A tool for the Symbolic Analysis of Neural Networks] This paper presents NEUROSPF, a tool for the symbolic analysis of neural networks. Given a trained neural network model, the tool extracts the architecture and model parameters and translates them into a Java representation that is amenable for analysis using the Symbolic PathFinder symbolic execution tool. Notably, NEUROSPF encodes specialized peer classes for parsing the model's parameters, thereby enabling efficient analysis. With NEUROSPF the user has the flexibility to specify either the inputs or the network internal parameters as symbolic, promoting the application of program analysis and testing approaches from software engineering to the field of machine learning. For instance, NEUROSPF can be used for coverage-based testing and test generation, finding adversarial examples and also constraint-based repair of neural networks, thus improving the reliability of neural networks and of the applications that use them. Video URL: https://youtu.be/seal8fG78LI","layer":2,"vector":[-0.0624,-0.0122,0.0309,-0.0343,0.0234,0.0593,0.0553,0.0456,0.0218,-0.0485,-0.025,-0.0646,0.0542,0.0553,0.0217,-0.0249,-0.0699,0.0613,-0.0305,0.0113,0.0288,-0.0144,-0.0202,-0.0679,0.0166,0.0297,-0.0202,-0.0277,-0.0502,-0.2327,0.0195,-0.024,0.0549,-0.0277,-0.0245,0.016,-0.0487,0.0306,-0.0023,0.0115,0.0238,0.0222,0.0107,-0.0461,-0.0141,-0.0459,-0.0052,-0.0527,-0.0129,-0.027,-0.0028,-0.0261,0.0422,0.0236,0.028,-0.0115,0.0745,0.0449,0.0189,0.0447,0.0067,0.0339,-0.1947,0.0619,0.0525,0.0395,-0.0341,-0.0536,0.0317,0.064,-0.0289,0.0521,0.0132,0.023,-0.0046,0.0293,0.0132,-0.0656,0.0052,0.0273,0.0491,-0.0012,-0.0262,0.0063,0.0114,0.0028,-0.0021,-0.0094,0.0268,-0.0404,-0.0235,0.0071,0.0188,0.0179,-0.0219,-0.0069,0.035,-0.0031,-0.0808,0.2337,-0.0318,-0.015,0.0317,-0.012,0.0042,-0.0124,-0.037,-0.0557,-0.0292,-0.024,-0.0438,-0.0457,0.0218,-0.0252,0.0153,0.0092,0.0207,-0.0031,-0.0383,-0.0212,-0.0178,-0.0011,0.031,0.0018,0.048,-0.0795,-0.018,0.1305,0.0193,0.0075,0.0443,-0.0233,-0.0316,-0.0153,0.0207,0.0223,-0.0121,0.0342,-0.0147,0.0075,-0.0506,-0.052,0.0374,-0.0602,-0.0438,0.0952,-0.0183,0.0352,-0.0562,-0.0203,-0.0418,0.0844,-0.0505,-0.0291,0.0214,0.037,0.0078,0.0567,-0.0769,0.0324,-0.0316,-0.0486,-0.0344,0.1138,0.0347,-0.0778,-0.0054,-0.0149,0.0158,-0.0518,0.0252,0.0408,-0.0338,-0.0031,0.0104,0.0214,-0.0415,-0.0413,-0.0182,0.0523,0.0372,-0.0737,-0.0695,0.0497,0.0065,-0.058,0.0085,-0.0447,0.0149,0.0314,-0.0564,0.0312,-0.0392,-0.0222,-0.0336,-0.0291,0.0182,0.0226,0.0122,-0.0269,0.0269,0.0108,-0.0082,0.0013,-0.0107,0.0321,-0.0362,-0.0017,0.0253,0.0521,-0.0318,-0.025,0.0532,-0.0218,-0.0148,0.0199,-0.0217,0.0104,0.047,0.066,0.0271,-0.0437,-0.0669,-0.2171,-0.0104,0.0056,-0.0131,0.0662,-0.0974,0.04,-0.0063,0.0358,0.0407,0.08,-0.0046,-0.0461,0.018,-0.0436,0.0844,0.0347,0.0117,-0.0655,0.026,-0.0138,-0.005,0.0367,-0.082,0.0252,0.0007,0.1813,0.0306,0.0623,-0.0386,0.0117,0.0188,-0.013,-0.0844,0.0976,-0.0059,0.0613,0.0039,-0.0357,-0.0219,-0.0032,0.0632,0.0009,-0.1057,-0.0005,-0.027,0.0193,-0.0404,-0.0462,0.0359,0.003,-0.0385,0.0326,0.0346,0.0308,-0.0641,-0.0757,-0.0002,-0.012,0.0351,0.0249,-0.0727,0.0219,-0.0704,0.0767,-0.0089,-0.0382,-0.0274,0.0404,0.0089,-0.0147,0.0947,0.0436,-0.0067,0.0785,0.0279,0.0376,-0.0492,0.0016,-0.0126,0.0612,-0.028,0.0425,-0.023,0.001,0.0036,0.0775,-0.038,0.0721,-0.0384,-0.0298,0.0089,-0.0541,0.0001,0.0745,0.0116,-0.2882,0.0575,0.0527,0.0369,-0.0252,0.0127,0.0438,0.0184,-0.0726,0.008,0.0156,0.0384,0.0529,-0.0282,0.0217,0.0404,0.0536,-0.0732,0.0893,-0.0634,0.0071,0.0589,0.2083,-0.0412,0.0248,0.014,-0.012,0.0178,0.0776,-0.0063,0.0643,-0.0096,0.0924,-0.0729,0.0409,0.0774,-0.0453,-0.0112,0.0498,-0.0313,0.015,0.0123,-0.0396,-0.0338,0.0869,-0.0231,-0.0343,-0.0199,-0.0064,0.0579,-0.0459,-0.0015,0.0104,-0.0203,0.0235,0.0097,-0.056,-0.0774,-0.0658,-0.0655,0.0145,-0.056,0.0352,0.0059,-0.0422]}
{"key":"[Prediction of Occurrence of Extreme Events using Machine Learning] Machine learning models play a vital role in the prediction task in several fields of study. In this work, we utilize the ability of machine learning algorithms to predict the occurrence of extreme events in a nonlinear mechanical system. Extreme events are rare events that occur ubiquitously in nature. We consider four machine learning models, namely Logistic Regression, Support Vector Machine, Random Forest and Multi-Layer Perceptron in our prediction task. We train these four machine learning models using training set data and compute the performance of each model using the test set data. We show that the Multi-Layer Perceptron model performs better among the four models in the prediction of extreme events in the considered system. The persistent behaviour of the considered machine learning models is cross-checked with randomly shuffled training set and test set data.","layer":0,"vector":[-0.0701,0.0009,0.0418,-0.0189,0.0391,0.0414,0.0142,0.0489,0.0246,-0.0177,0.0198,-0.0327,-0.0012,0.051,0.029,0.0168,-0.0026,0.0566,-0.0428,0.0132,0.0496,-0.0059,-0.0095,-0.0785,0.0078,0.0089,-0.0633,0.0145,-0.0325,-0.2287,0.0132,-0.0729,0.0319,-0.0262,0.0227,0.0041,-0.0353,0.0851,0.0021,0.0739,-0.0044,0.0084,-0.0087,-0.084,-0.0017,-0.0443,0.0351,-0.0615,0.0164,-0.0337,-0.017,-0.0648,0.0448,-0.0168,0.0415,0.0307,0.0277,0.0143,0.0441,0.0543,0.0181,0.029,-0.1881,0.0394,0.0389,0.0772,-0.0422,-0.006,0.056,0.081,-0.0378,0.0299,0.0171,0.0652,0.0128,0.0069,0.0118,-0.0293,-0.0165,0.0539,0.0252,-0.0752,-0.0486,-0.068,-0.0335,-0.0517,0.0189,-0.0339,0.0454,0.0105,-0.0619,0.0346,-0.0026,0.0357,-0.0315,0.0025,0.0629,0.0103,-0.0513,0.1939,-0.0625,0.0202,0.0236,-0.0252,-0.0048,-0.0298,-0.0244,-0.0382,-0.0539,-0.0508,0.0294,-0.0414,0.0192,0.0113,-0.0056,0.0136,0.0602,0.04,-0.0378,-0.0122,-0.0185,0.0178,0.0276,-0.0193,0.0232,-0.0525,0.0254,0.1326,0.005,-0.0101,0.0105,-0.0416,-0.0779,-0.0202,0.0223,0.0126,0.0433,0.0282,0.0034,-0.0141,-0.0204,-0.0782,0.0384,-0.101,-0.0805,0.0797,-0.0493,-0.0095,-0.0648,-0.0428,-0.0256,0.0272,-0.0166,-0.0084,0.0636,0.0212,0.0368,0.0399,-0.048,-0.0111,-0.0185,-0.0419,-0.0627,0.1005,0.0205,-0.076,0.0109,-0.0123,0.0094,-0.0251,0.037,0.0385,-0.0018,0.0149,0.0563,0.0493,-0.0306,-0.0263,-0.0354,0.0525,0.0541,-0.0357,-0.0573,0.0737,0.0708,-0.0491,-0.0074,-0.0673,0.0271,0.0116,-0.0287,0.0015,-0.0128,0.0198,-0.0145,-0.0031,-0.0096,-0.0113,-0.0012,-0.0641,0.0002,0.0293,-0.0642,0.0117,-0.0424,0.0468,0.0022,0.0202,0.0231,0.0356,-0.0126,-0.0181,0.1126,-0.0524,-0.0295,-0.003,-0.032,0.1127,0.0042,0.0532,0.0948,-0.0247,-0.0466,-0.2286,-0.039,0.0024,-0.0153,0.0702,-0.0434,0.0271,-0.0394,0.0414,0.0131,0.0956,0.0178,-0.0458,-0.0134,0.0024,0.0549,0.0203,0.0273,-0.0758,0.0426,-0.0098,0.0044,-0.0105,-0.101,0.0175,-0.0332,0.1616,-0.0003,0.0269,-0.0359,0.0229,0.0147,-0.0326,-0.0818,0.0943,0.0347,0.0443,-0.0235,-0.0223,-0.0224,-0.0321,0.0262,0.0147,-0.0927,-0.0435,-0.0801,-0.0363,0.0427,-0.0934,0.0447,0.0468,-0.0062,0.077,-0.0123,0.0403,-0.0228,-0.087,0.0428,-0.0155,0.0022,0.0325,-0.0685,0.0072,-0.0613,0.038,-0.0175,-0.0165,-0.0398,0.0161,-0.0447,-0.0318,0.1501,0.0149,-0.0194,0.0684,-0.0124,0.0193,-0.0269,-0.0265,-0.0069,0.0415,-0.0264,0.0428,0.0228,0.0295,-0.0117,0.0606,-0.0289,0.0473,-0.0032,-0.001,0.0002,-0.0241,-0.009,0.0323,0.0087,-0.2888,0.0382,0.001,0.0525,-0.0067,-0.0308,0.0041,0.015,-0.0026,0.0166,-0.0252,0.0355,0.0675,-0.004,0.0344,0.0271,0.0321,-0.0193,0.0331,-0.061,-0.0014,0.0303,0.2156,-0.0008,0.0442,0.0209,-0.0486,-0.0097,0.0191,-0.0483,0.0723,-0.0069,0.0745,-0.0443,0.0335,0.0758,-0.0083,0.0273,0.0511,-0.0051,0.0196,0.0021,-0.027,-0.0401,0.0948,-0.0111,-0.0218,-0.0739,-0.016,0.0526,-0.0088,-0.01,-0.0221,-0.0015,0.0035,0.0295,-0.018,-0.0267,-0.0354,-0.0481,0.0426,-0.0418,-0.0069,-0.0577,-0.0185]}
{"key":"[Ordered Subgraph Aggregation Networks] Numerous subgraph-enhanced graph neural networks (GNNs) have emerged recently, provably boosting the expressive power of standard (message-passing) GNNs. However, there is a limited understanding of how these approaches relate to each other and to the Weisfeiler--Leman hierarchy. Moreover, current approaches either use all subgraphs of a given size, sample them uniformly at random, or use hand-crafted heuristics instead of learning to select subgraphs in a data-driven manner. Here, we offer a unified way to study such architectures by introducing a theoretical framework and extending the known expressivity results of subgraph-enhanced GNNs. Concretely, we show that increasing subgraph size always increases the expressive power and develop a better understanding of their limitations by relating them to the established $k\\text{-}\\mathsf{WL}$ hierarchy. In addition, we explore different approaches for learning to sample subgraphs using recent methods for backpropagating through complex discrete probability distributions. Empirically, we study the predictive performance of different subgraph-enhanced GNNs, showing that our data-driven architectures increase prediction accuracy on standard benchmark datasets compared to non-data-driven subgraph-enhanced graph neural networks while reducing computation time.","layer":2,"vector":[-0.0296,-0.051,0.026,-0.0314,0.0272,0.0469,0.0068,0.0026,0.0464,0.0,0.0219,-0.0367,0.0652,0.0516,-0.0006,0.0061,-0.0066,0.0332,-0.0291,-0.0281,0.0164,-0.0376,-0.031,-0.0965,0.057,-0.0017,-0.0365,-0.0165,-0.0682,-0.2808,0.0232,-0.0402,0.0625,-0.0351,-0.0239,-0.0586,0.0036,0.047,-0.0328,0.0265,0.0172,0.0102,-0.0127,-0.0055,-0.0129,-0.0378,0.0324,0.0075,-0.0372,-0.024,0.0479,-0.043,0.0434,0.022,0.0537,0.0366,0.0201,-0.0051,0.0503,0.0455,-0.0018,0.0549,-0.1258,0.0547,0.0422,0.0266,-0.0703,0.0168,-0.001,0.0945,0.0374,0.0056,0.0601,0.0333,0.0355,0.0465,-0.0101,-0.0776,-0.0366,0.0144,-0.014,-0.0458,-0.058,-0.0117,-0.0027,-0.0282,0.0087,-0.076,0.0229,0.0133,-0.0041,-0.0226,-0.0171,0.024,-0.0705,-0.0525,0.0623,0.0182,-0.0082,0.1928,-0.0364,0.0002,0.0216,-0.0134,0.0029,-0.0773,-0.0276,-0.0366,-0.0512,-0.0484,-0.029,-0.0374,0.0166,-0.0499,0.0219,-0.0181,0.0968,0.0346,-0.0294,-0.0297,-0.0596,0.0356,0.0227,-0.0544,0.0283,-0.0557,-0.0307,0.1131,0.0377,0.018,0.0371,0.0058,-0.0251,-0.0302,0.0072,-0.0185,0.0472,0.0108,0.0153,0.0258,-0.0343,-0.0079,0.0021,-0.0406,-0.0787,0.1437,-0.0418,-0.0031,0.0134,-0.0424,-0.0561,0.0136,0.0042,-0.0743,-0.0086,0.041,0.0144,0.0474,-0.093,0.0302,-0.007,-0.0279,-0.0557,0.0937,0.0048,-0.1185,0.0003,-0.0308,0.0119,-0.0428,0.0327,0.0331,-0.0056,0.0708,0.0619,0.0452,-0.069,-0.0551,-0.0134,-0.011,0.0335,-0.0263,-0.0488,0.0426,0.009,0.0079,0.0032,-0.0118,0.0005,0.0549,-0.0674,0.0627,-0.02,0.0002,-0.0229,-0.0165,0.0082,-0.0276,0.0023,-0.0279,0.0164,-0.016,-0.0229,0.0366,-0.0393,-0.0025,-0.009,-0.0064,0.0056,-0.0371,-0.0399,-0.0091,0.0897,-0.0465,-0.0071,0.0196,0.0396,0.048,0.0166,0.0178,0.0404,-0.0504,-0.0453,-0.2063,-0.0017,0.0095,-0.0224,0.0774,-0.0783,0.0177,0.0018,0.0608,0.1149,0.0342,0.0189,-0.0437,0.0055,0.0121,0.08,0.0055,0.0154,0.0021,-0.0129,-0.0115,0.0266,-0.0009,-0.0698,0.0173,0.0534,0.226,0.0054,0.0514,-0.0297,0.0138,0.033,-0.0532,-0.0838,0.0781,0.063,0.0397,-0.0267,-0.0263,-0.0438,-0.046,0.033,0.0106,-0.1096,-0.051,0.0051,0.0043,-0.0255,-0.0513,-0.0016,0.0697,-0.0231,0.0743,0.0174,-0.0128,-0.0312,-0.0944,0.0223,-0.0233,0.0101,0.0207,-0.0752,-0.017,-0.0152,0.0491,0.0084,-0.0062,-0.0149,0.0147,-0.0374,-0.0375,0.0638,0.03,-0.005,0.0668,0.0443,0.0229,-0.0449,-0.0436,0.0024,0.0511,-0.0811,0.0227,0.0415,0.0618,0.0255,0.0652,-0.0334,0.0434,0.0117,0.027,0.0296,-0.0588,-0.0245,0.0383,-0.0274,-0.281,0.0446,0.0141,0.0546,-0.0439,0.0159,0.0423,0.0327,-0.0255,0.0014,0.0372,0.0635,0.0235,-0.0148,-0.0307,0.0316,0.053,-0.0412,0.0271,-0.0391,0.0362,0.0303,0.2317,0.0119,0.0433,0.0134,-0.012,-0.001,0.0045,-0.01,0.0073,-0.0089,0.1076,-0.0591,0.0546,0.0953,-0.0532,0.0473,0.0186,-0.0006,-0.0018,-0.023,-0.0321,-0.0293,0.0618,0.0054,-0.0436,-0.0475,0.0315,0.0543,-0.0182,-0.012,-0.0271,0.0014,-0.0082,0.0304,-0.0165,-0.0547,-0.0493,-0.0143,0.0204,-0.053,0.0279,0.0143,-0.0232]}
{"key":"[Turnover Prediction Of Shares using Data Mining techniques : A Case Study] Predicting the turnover of a company in the ever fluctuating Stock market has always proved to be a precarious situation and most certainly a difficult task in hand. Data mining is a well-known sphere of Computer Science that aims on extracting meaningful information from large databases. However, despite the existence of many algorithms for the purpose of predicting the future trends, their efficiency is questionable as their predictions suffer from a high error rate. The objective of this paper is to investigate various classification algorithms to predict the turnover of different companies based on the Stock price. The authorized dataset for predicting the turnover was taken from www.bsc.com and included the stock market values of various companies over the past 10 years. The algorithms were investigated using the \"R\" tool. The feature selection algorithm, Boruta, was run on this dataset to extract the important and influential features for classification. With these extracted features, the Total Turnover of the company was predicted using various classification algorithms like Random Forest, Decision Tree, SVM and Multinomial Regression. This prediction mechanism was implemented to predict the turnover of a company on an everyday basis and hence could help navigate through dubious stock market trades. An accuracy rate of 95% was achieved by the above prediction process. Moreover, the importance of stock market attributes was established as well.","layer":1,"vector":[-0.0442,-0.0114,0.0149,-0.0197,0.0254,0.0043,0.0738,0.0209,0.0382,-0.0449,0.0201,-0.0232,0.0029,-0.0014,0.0123,0.0198,0.0006,0.0153,-0.0315,0.0058,-0.0133,-0.0034,0.0019,-0.0429,0.066,0.0041,-0.0545,-0.074,-0.0923,-0.1796,0.0089,-0.0941,0.0649,-0.0324,-0.0046,0.0036,-0.0372,0.0711,-0.0277,0.0595,-0.0353,0.0216,0.024,-0.0502,-0.0609,-0.0391,0.0171,-0.0207,0.0003,0.0279,-0.0277,-0.0562,-0.0037,0.0197,-0.0075,0.0439,0.0277,-0.0004,0.0468,0.0403,0.0629,0.0489,-0.1916,0.0422,0.0266,0.0261,-0.0419,-0.0475,0.036,0.0741,-0.0268,0.0279,0.0003,0.0168,0.0204,-0.0119,0.0238,-0.0242,-0.0159,0.067,-0.0125,-0.02,-0.0265,0.0132,-0.041,0.0145,0.069,-0.0635,0.0773,0.0197,0.0104,0.0399,0.0057,0.0013,-0.0788,0.0189,0.0739,0.0387,-0.0741,0.1992,-0.0356,0.019,0.0222,-0.0726,-0.0181,-0.0417,-0.0502,-0.0552,-0.0042,-0.0019,0.0031,-0.0434,0.0038,-0.0073,0.0128,-0.0129,0.0367,0.0347,-0.0106,0.0098,-0.014,0.0123,0.0451,-0.0159,0.0116,-0.0428,0.0278,0.0994,0.0337,0.0272,0.0409,-0.0084,-0.0896,-0.0503,0.0065,0.0151,0.0668,0.0257,0.008,-0.0293,-0.0638,-0.0691,0.0469,-0.1077,-0.0289,0.1228,0.0017,0.0302,-0.0321,-0.0166,-0.058,0.0373,-0.0415,-0.0467,-0.0011,0.0246,0.0325,0.0538,-0.0478,-0.034,-0.0316,-0.0667,-0.0505,0.1111,-0.0088,-0.0829,-0.0203,-0.0047,-0.0173,-0.0101,0.0348,0.0499,-0.0376,0.0367,0.0521,0.0079,0.0028,-0.049,-0.0009,0.0045,0.0761,-0.0327,-0.0632,0.0534,0.0322,-0.0481,-0.0119,-0.066,0.0366,0.0624,0.0048,0.0374,-0.0664,-0.0138,-0.0333,-0.0002,-0.0138,-0.0167,0.0287,-0.046,0.0238,0.0231,-0.0671,0.0183,-0.021,-0.0005,0.0249,-0.0081,0.0782,0.0651,0.0245,0.0146,0.0715,-0.0191,-0.0769,0.0003,0.0369,0.0463,0.0276,0.0704,0.058,-0.0397,-0.0455,-0.2016,0.0081,-0.0052,-0.007,0.0355,-0.0518,0.0058,-0.0161,0.0653,0.0338,0.097,0.0027,-0.0031,-0.0205,-0.015,0.0078,0.0181,0.0275,-0.0497,-0.0235,0.0022,-0.0009,0.0426,-0.0747,0.0824,0.0324,0.1959,-0.0033,0.0084,-0.0413,0.0427,0.0433,-0.0527,-0.0828,0.0814,0.0249,0.0112,-0.0231,-0.0318,-0.0197,0.0069,0.0286,-0.0095,-0.0453,-0.0165,-0.0301,-0.0389,0.0024,-0.0423,0.0659,0.0722,0.0007,0.0928,0.0239,0.0198,-0.0326,-0.0534,0.0435,-0.0018,-0.0153,0.0164,-0.0683,0.0,-0.0654,0.0387,-0.0381,-0.0178,0.0003,-0.0253,-0.0143,-0.0205,0.1205,-0.0143,-0.0528,0.0461,-0.0262,-0.0027,-0.0013,-0.0295,0.0058,0.0417,-0.0506,0.0461,0.0799,0.0341,0.0176,0.0853,-0.048,0.0402,-0.0097,0.0016,-0.0023,-0.0353,-0.0092,0.0506,0.0145,-0.2763,0.0065,-0.0314,0.0328,-0.0104,-0.0157,0.0069,0.0236,-0.0375,0.0295,0.0362,-0.0172,0.057,-0.0918,0.0064,0.0386,0.079,-0.0575,0.0119,-0.0042,0.0402,0.0436,0.2352,-0.0378,0.0441,0.0165,-0.0072,-0.0175,-0.0053,-0.0262,0.0362,-0.0001,0.1177,-0.0736,0.0118,0.0772,-0.033,0.0689,0.0252,-0.0313,-0.0029,-0.0102,-0.0206,-0.0483,0.1386,-0.0418,-0.01,-0.0741,0.0413,0.0263,-0.0758,0.0021,-0.0505,-0.0104,-0.0107,0.0021,-0.05,-0.0492,-0.013,-0.0494,-0.0031,-0.0837,-0.03,-0.0172,0.0315]}
{"key":"[Online Neural Networks for Change-Point Detection] Moments when a time series changes its behaviour are called change points. Detection of such points is a well-known problem, which can be found in many applications: quality monitoring of industrial processes, failure detection in complex systems, health monitoring, speech recognition and video analysis. Occurrence of change point implies that the state of the system is altered and its timely detection might help to prevent unwanted consequences. In this paper, we present two online change-point detection approaches based on neural networks. These algorithms demonstrate linear computational complexity and are suitable for change-point detection in large time series. We compare them with the best known algorithms on various synthetic and real world data sets. Experiments show that the proposed methods outperform known approaches.","layer":2,"vector":[-0.0705,-0.018,0.0636,0.0017,0.0622,0.0473,0.05,0.0251,0.0561,-0.0414,0.0131,-0.0294,0.0004,0.0574,-0.026,-0.018,-0.0173,0.0319,-0.015,0.0143,0.0157,0.0114,-0.0025,-0.0183,0.0114,0.0181,-0.0009,-0.0167,-0.0719,-0.2109,0.0021,-0.0717,0.0924,0.0064,0.0171,-0.006,-0.0553,0.0377,-0.0349,0.0318,0.0361,0.022,-0.0012,-0.0716,-0.0504,-0.0809,0.0408,0.0157,0.0132,-0.0537,0.0012,-0.0411,0.0746,0.012,0.0414,0.0289,0.0466,0.0607,0.0636,0.024,0.0579,0.0469,-0.1886,0.0122,0.0269,0.0358,0.0009,-0.0351,0.0001,0.0572,-0.0493,0.0271,-0.0189,0.0227,0.0278,0.0203,0.0004,-0.0105,-0.0271,0.0105,0.0405,-0.0376,-0.0424,-0.0411,-0.0164,-0.0625,0.0149,-0.0475,0.0574,-0.0281,-0.0415,0.0041,0.0015,0.0345,-0.0469,-0.0562,0.0344,0.0297,-0.0259,0.1741,-0.0737,0.0304,0.0304,-0.056,0.0392,-0.0527,-0.0328,-0.0467,0.0105,0.0029,-0.0068,0.0012,0.034,-0.0553,0.014,-0.0014,0.0166,0.0466,0.0103,0.0103,-0.0108,0.0453,0.0813,-0.0278,0.0702,-0.031,0.0255,0.1336,-0.0035,0.0133,0.0392,0.0023,-0.0966,-0.008,0.0206,0.0441,0.0178,0.0045,0.0355,0.0031,-0.0676,-0.0601,0.0264,-0.0788,-0.0638,0.1307,-0.0263,0.0119,-0.0486,-0.0622,-0.0486,0.031,-0.0594,-0.0235,0.0059,0.0236,0.0599,0.0043,-0.0508,-0.023,-0.0357,-0.0423,-0.0012,0.0971,0.049,-0.1013,-0.027,-0.0363,0.0229,0.0112,0.0467,0.0514,-0.0128,-0.002,0.1003,0.0149,-0.0249,0.0042,0.0222,0.0161,0.0344,-0.0442,-0.0417,0.0686,0.0314,-0.0198,0.0313,-0.0575,0.0304,0.0702,-0.0384,-0.0001,-0.0244,0.0262,0.0108,-0.0407,-0.0223,0.0105,0.0312,-0.049,-0.0094,-0.0208,-0.0385,-0.0063,-0.0135,0.0291,-0.0661,0.0056,0.0545,0.0495,-0.02,0.0327,0.0632,-0.0338,-0.0382,-0.0279,-0.0179,-0.0019,-0.0245,0.0249,0.0593,-0.0189,-0.043,-0.2131,-0.0516,0.0194,-0.0032,0.0709,-0.0441,0.0069,-0.0155,0.0646,0.0618,0.0873,-0.0076,0.005,-0.0143,0.0348,0.0531,0.0569,0.029,-0.052,-0.0097,-0.0618,0.0172,-0.0059,-0.1059,0.0526,-0.0135,0.1975,-0.0145,0.067,-0.0363,0.0259,-0.0104,0.0027,-0.0765,0.1228,0.0542,0.0477,-0.0073,-0.0667,-0.0626,0.0014,0.0241,-0.0019,-0.0383,-0.0289,-0.0214,-0.0117,-0.0078,-0.0722,-0.0316,0.0437,-0.0168,0.067,-0.0206,0.0022,-0.0614,-0.0503,0.0353,-0.0344,-0.0183,0.0252,-0.0089,0.0325,-0.0711,0.0535,0.0199,-0.0474,-0.0708,0.0118,-0.002,-0.0082,0.1322,0.0249,-0.026,0.047,-0.0212,-0.0148,-0.0269,-0.0711,-0.028,0.0391,-0.0508,0.0243,0.0146,0.006,-0.0333,0.0633,-0.0096,0.0253,0.005,0.0243,0.0066,-0.0477,-0.0112,0.0544,0.001,-0.2854,0.0485,-0.0013,0.0261,-0.0298,0.0154,-0.0474,0.0581,-0.0222,0.0319,-0.0377,0.0345,0.061,-0.012,0.0086,0.0761,0.0149,-0.038,0.0536,-0.0529,0.0151,0.0491,0.2077,-0.0273,0.0461,-0.0298,0.0102,-0.0045,0.0558,-0.0004,0.0252,0.0289,0.0602,-0.0496,0.0127,0.096,-0.0638,0.0345,0.0168,-0.0291,0.0075,0.0105,-0.0425,-0.0646,0.0888,0.0204,-0.0483,-0.0391,-0.0105,0.043,-0.0879,0.0014,0.0007,0.0356,-0.0502,0.0224,-0.0724,-0.0296,-0.0369,-0.0607,0.0233,-0.0734,-0.0113,-0.0228,-0.0002]}
{"key":"[Deep Reinforcement Learning of Marked Temporal Point Processes] In a wide variety of applications, humans interact with a complex environment by means of asynchronous stochastic discrete events in continuous time. Can we design online interventions that will help humans achieve certain goals in such asynchronous setting? In this paper, we address the above problem from the perspective of deep reinforcement learning of marked temporal point processes, where both the actions taken by an agent and the feedback it receives from the environment are asynchronous stochastic discrete events characterized using marked temporal point processes. In doing so, we define the agent's policy using the intensity and mark distribution of the corresponding process and then derive a flexible policy gradient method, which embeds the agent's actions and the feedback it receives into real-valued vectors using deep recurrent neural networks. Our method does not make any assumptions on the functional form of the intensity and mark distribution of the feedback and it allows for arbitrarily complex reward functions. We apply our methodology to two different applications in personalized teaching and viral marketing and, using data gathered from Duolingo and Twitter, we show that it may be able to find interventions to help learners and marketers achieve their goals more effectively than alternatives.","layer":4,"vector":[-0.0743,-0.01,0.0273,-0.0246,-0.007,0.0183,0.0548,0.0321,0.0783,-0.017,0.0475,-0.0237,0.0185,0.0784,0.023,0.0138,-0.0373,0.0079,-0.0447,0.0058,0.0315,-0.0496,-0.0068,-0.07,0.0264,-0.0317,-0.0267,-0.0589,-0.0311,-0.1928,0.0601,-0.0369,-0.0077,-0.0135,-0.0144,-0.0249,-0.0718,0.0082,-0.0172,0.0196,0.0409,0.0393,-0.0527,-0.0586,-0.0052,-0.0836,-0.0334,-0.0411,-0.0267,-0.0198,0.018,-0.0398,0.0235,0.0165,0.0342,0.0425,0.0845,0.0528,0.0455,0.0123,0.0397,0.0145,-0.1879,0.0805,-0.0023,0.0514,-0.026,0.0391,-0.0109,0.0489,-0.0436,0.0264,0.0086,0.048,0.0268,0.012,0.0112,-0.023,-0.0322,-0.0236,-0.0142,-0.0226,-0.0052,-0.0434,-0.0202,-0.0685,0.0269,-0.0388,0.0663,0.015,-0.0169,0.0218,-0.0402,0.0308,-0.0593,-0.0179,0.0318,0.0154,-0.0563,0.1911,-0.0194,0.0585,0.0522,-0.0029,0.0287,-0.0405,-0.058,-0.0392,-0.0001,0.0332,-0.0637,0.0155,0.0495,-0.0501,0.0345,0.0287,0.086,0.0324,0.0151,-0.0191,0.0134,0.0322,0.0504,-0.036,0.0231,-0.0518,0.0191,0.1559,0.0102,0.0295,0.0525,-0.0163,-0.0368,-0.0027,0.0231,0.0348,-0.039,-0.041,0.028,-0.0073,-0.0453,0.031,-0.0099,-0.105,-0.0349,0.128,-0.007,0.0645,-0.0225,-0.0109,-0.0387,0.0159,-0.0139,-0.0295,0.0079,0.0243,0.056,0.0121,-0.0482,0.0075,-0.0505,-0.0548,-0.0011,0.097,0.0127,-0.1101,-0.0429,-0.0231,0.0131,-0.0262,0.0439,0.0388,-0.0312,-0.0081,0.0809,0.0507,-0.0966,0.0002,-0.0081,0.0164,0.04,-0.0663,-0.0066,0.0216,0.0225,-0.0384,0.0122,-0.066,-0.0049,0.0405,-0.011,0.014,-0.0065,-0.0207,0.0134,-0.062,0.0063,0.0058,0.0238,-0.0199,-0.0373,-0.0466,-0.0532,-0.0018,0.0303,0.0076,-0.0316,0.0172,0.0806,0.0203,-0.0262,0.0396,0.0352,-0.0538,-0.002,0.0077,0.0092,0.0273,-0.0119,0.0332,0.0112,0.013,-0.0337,-0.209,-0.0344,0.0001,-0.0387,0.0263,-0.0485,-0.0117,-0.0362,0.0622,0.056,0.0829,-0.0512,-0.0045,0.0067,0.02,0.0552,0.0224,0.0362,0.042,-0.0014,0.0033,-0.0212,-0.0243,-0.0968,0.0537,-0.0313,0.2192,0.0721,0.0435,-0.0022,0.06,0.0336,-0.0509,-0.1188,0.0617,0.0099,0.1017,0.0031,-0.0131,-0.0473,0.0176,0.0372,0.0093,-0.0721,-0.0364,-0.0308,0.018,0.0357,-0.0597,-0.0117,0.042,-0.04,0.0463,-0.0017,-0.0571,-0.0141,-0.0759,0.0457,-0.0476,0.0228,0.0007,-0.0461,0.0329,-0.0558,0.0493,0.0207,0.0212,-0.0755,-0.0026,0.0379,-0.0154,0.0827,-0.0313,0.0369,0.0583,0.0017,0.0154,-0.0484,-0.0639,-0.0192,0.0593,-0.0381,0.0342,0.0409,-0.0076,-0.0085,0.0664,-0.0063,0.0197,0.0234,0.0235,0.0064,-0.0629,-0.0136,0.0314,-0.0304,-0.2956,0.0509,-0.018,0.081,-0.0203,0.0093,0.0192,-0.0068,-0.0794,0.0059,0.012,0.0352,0.07,-0.0249,-0.004,0.0238,0.1085,-0.0374,0.0658,-0.0631,0.0113,0.046,0.2358,-0.0225,0.0469,-0.0069,-0.0437,-0.0076,0.0637,-0.029,0.0051,0.0247,0.0567,-0.0363,0.0053,0.0565,-0.0194,0.0189,0.0207,0.0083,-0.0119,0.0333,-0.0116,-0.0419,0.0587,-0.0018,-0.0421,-0.0769,-0.0268,0.0401,-0.0467,0.0054,-0.0246,0.0368,0.0217,0.0178,-0.0553,-0.0732,-0.0294,-0.0468,-0.0107,-0.0538,-0.0095,-0.0221,-0.0051]}
{"key":"[Incident Detection on Junctions Using Image Processing] In traffic management, it is a very important issue to shorten the response time by detecting the incidents (accident, vehicle breakdown, an object falling on the road, etc.) and informing the corresponding personnel. In this study, an anomaly detection framework for road junctions is proposed. The final judgment is based on the trajectories followed by the vehicles. Trajectory information is provided by vehicle detection and tracking algorithms on visual data streamed from a fisheye camera. Deep learning algorithms are used for vehicle detection, and Kalman Filter is used for tracking. To observe the trajectories more accurately, the detected vehicle coordinates are transferred to the bird's eye view coordinates using the lens distortion model prediction algorithm. The system determines whether there is an abnormality in trajectories by comparing historical trajectory data and instantaneous incoming data. The proposed system has achieved 84.6% success in vehicle detection and 96.8% success in abnormality detection on synthetic data. The system also works with a 97.3% success rate in detecting abnormalities on real data.","layer":3,"vector":[-0.0184,-0.0353,0.0611,-0.0115,0.0397,0.0428,0.0768,0.0327,0.0005,-0.0102,0.0161,-0.0701,0.013,0.0854,-0.009,-0.0292,0.0115,0.0216,0.0003,-0.0198,0.0123,-0.0314,-0.0161,-0.059,0.0001,0.0474,0.0059,-0.0095,-0.0612,-0.1789,0.0098,-0.0709,0.0771,-0.0331,-0.0027,-0.0652,-0.0676,0.0955,-0.0072,0.0371,0.0512,0.0059,-0.0002,-0.0809,-0.0143,-0.0413,0.0136,-0.0326,0.0103,-0.1014,0.0119,-0.0531,0.0175,0.0221,-0.0108,0.0336,0.0392,0.0441,0.0839,0.0531,0.0576,0.0185,-0.2069,0.0273,0.0156,0.0414,-0.0158,-0.029,0.0174,-0.0128,-0.0532,0.0075,-0.0332,0.0091,-0.0035,-0.0178,-0.002,-0.0179,-0.0238,0.0043,-0.0145,-0.0287,0.02,-0.0277,-0.0187,-0.0563,-0.0193,-0.0551,0.0477,-0.0405,-0.0384,0.0096,-0.0311,0.0025,-0.0096,0.0176,-0.009,0.0058,-0.0033,0.2033,-0.0633,0.05,0.0316,-0.0245,0.0103,0.003,-0.0696,-0.0542,-0.0142,-0.0103,0.0073,-0.0503,0.0298,0.0011,0.0016,0.0281,0.0359,0.0582,-0.0216,0.0222,-0.0107,0.0149,0.0598,-0.0335,0.0103,-0.0714,0.0301,0.127,0.0227,0.0325,0.0045,-0.0096,-0.086,-0.0303,0.0032,0.0327,0.0446,-0.0085,-0.0193,-0.0199,-0.0271,-0.0698,0.0232,-0.0418,-0.02,0.1058,-0.0393,0.0267,-0.0648,-0.052,0.0109,0.0052,-0.0629,-0.0175,0.0165,0.0433,0.051,0.0425,-0.0632,0.0555,-0.0321,-0.0382,-0.0424,0.0829,-0.0026,-0.1011,0.0133,-0.0218,0.033,-0.0102,0.0009,0.0296,-0.008,0.0105,0.0536,0.0599,-0.0607,0.0293,-0.0348,-0.0358,0.0466,-0.033,-0.0772,0.0202,0.0797,-0.0656,-0.0237,-0.0312,0.0189,0.0377,-0.0548,0.0176,-0.0295,-0.013,-0.0047,-0.0226,-0.0288,-0.036,-0.0028,-0.0747,0.0263,-0.0351,-0.0186,0.015,-0.0392,0.0054,-0.0243,-0.0158,-0.002,0.0672,-0.031,-0.0267,0.0507,-0.0176,-0.0067,-0.0179,0.0347,0.0652,0.0031,0.0234,0.0237,-0.0213,-0.0346,-0.2457,0.0173,-0.0184,-0.0169,0.0118,-0.0431,-0.0045,0.0336,0.0589,0.086,0.0833,-0.0443,0.0051,0.0037,0.0134,0.0637,-0.0253,0.079,-0.0645,-0.0066,-0.0354,0.0321,-0.0235,-0.0671,0.0232,0.0045,0.1929,0.0069,0.0473,-0.0223,0.0108,0.0443,-0.0027,-0.0543,0.0701,0.0083,0.0435,0.0018,-0.0674,-0.0448,-0.0604,0.0141,-0.0142,-0.0351,0.0287,-0.018,-0.0153,0.0345,-0.0281,0.0141,0.0228,-0.0434,0.0466,0.0237,0.0208,-0.0243,-0.0471,0.04,-0.0501,-0.0098,-0.0095,-0.0314,0.0214,-0.0462,0.0865,0.0175,-0.0415,-0.075,-0.003,0.0181,-0.0233,0.1574,0.0222,-0.0034,0.0544,0.0573,0.0222,-0.0178,-0.0284,-0.0438,0.066,-0.0112,0.031,0.0338,0.051,0.0139,0.0717,-0.0062,0.0069,-0.0163,0.0738,-0.0232,-0.0432,-0.0278,0.0366,0.0568,-0.3194,0.0122,-0.0129,0.0177,-0.0292,0.011,0.0673,0.0039,0.0016,-0.008,-0.0173,0.0066,0.0465,-0.0177,0.0317,0.0428,0.0469,-0.0363,0.0612,-0.0369,0.0048,0.0586,0.2406,-0.0429,0.0106,0.0063,-0.0262,-0.046,0.0629,-0.0254,0.017,0.0257,0.0753,-0.0657,0.0323,0.0632,-0.0222,0.0734,-0.015,-0.0015,0.0169,0.064,0.0154,-0.0228,0.1122,-0.0094,-0.0149,-0.026,0.0279,0.0493,-0.0264,-0.0413,-0.0231,-0.0,0.0481,-0.0062,-0.0741,-0.0156,-0.0263,-0.0454,0.0483,-0.0744,0.0184,-0.0078,0.0179]}
{"key":"[Enhanced Lasso Recovery on Graph] This work aims at recovering signals that are sparse on graphs. Compressed sensing offers techniques for signal recovery from a few linear measurements and graph Fourier analysis provides a signal representation on graph. In this paper, we leverage these two frameworks to introduce a new Lasso recovery algorithm on graphs. More precisely, we present a non-convex, non-smooth algorithm that outperforms the standard convex Lasso technique. We carry out numerical experiments on three benchmark graph datasets.","layer":4,"vector":[-0.0209,-0.0076,0.0282,-0.0279,0.0163,0.0456,0.0032,0.0519,0.0417,-0.0277,0.0564,-0.0331,0.0814,0.0214,0.0011,0.0316,0.0276,0.0641,-0.027,0.0109,0.0124,-0.0556,-0.0189,-0.063,0.0685,-0.0035,0.0069,-0.0427,-0.0716,-0.2346,-0.001,-0.0518,0.0801,-0.0309,0.0329,-0.001,0.0153,0.0386,-0.0503,0.1072,0.0198,0.0005,-0.0638,-0.0162,-0.0032,-0.0605,-0.0105,0.0042,-0.0137,-0.0597,-0.001,-0.0455,0.0348,0.0233,0.0657,0.013,0.0379,0.0231,0.051,0.0491,0.0066,0.037,-0.1796,0.0524,0.085,0.0347,-0.0295,-0.0228,0.0366,0.087,0.004,0.0288,0.0371,-0.0101,0.0155,0.007,0.0048,-0.0485,-0.0178,-0.0054,0.0131,-0.0528,-0.0131,0.0206,-0.0033,-0.0532,0.0264,-0.0646,0.0375,0.0438,-0.0631,-0.0074,-0.0146,0.0183,-0.1144,-0.0352,0.0211,0.0424,0.0015,0.2054,-0.0521,0.0413,0.0313,-0.0041,0.0183,-0.102,0.0068,-0.0467,-0.0047,0.0036,-0.0022,-0.0173,0.0361,-0.0602,0.0559,-0.0394,0.0379,0.0362,-0.0201,-0.011,-0.0179,0.0276,0.0503,-0.0495,0.0479,-0.0474,0.0143,0.1021,0.0526,0.0427,0.0253,-0.0165,-0.0019,-0.0043,-0.0018,-0.0238,0.0225,0.0267,-0.0033,0.0284,-0.0122,-0.0513,0.039,-0.0632,-0.0721,0.126,-0.0543,-0.0243,-0.0575,-0.0378,-0.0445,0.0258,-0.0083,0.0034,0.0056,0.0261,0.0366,0.0418,-0.0214,0.0358,-0.0345,-0.0399,-0.043,0.0743,0.029,-0.0907,-0.0181,0.0073,0.0061,-0.0206,0.0319,0.0028,-0.0283,0.013,0.072,0.041,-0.057,-0.0036,0.0246,0.0009,0.0135,-0.0223,-0.0212,0.0237,0.0223,-0.0173,-0.0197,-0.0111,-0.0052,0.0158,-0.0407,-0.0162,-0.0141,-0.0152,-0.0283,0.0071,0.0063,-0.0372,-0.0436,0.0054,0.0478,-0.0572,-0.0207,0.039,0.0104,0.0485,-0.0129,-0.0291,-0.0029,0.0192,-0.0429,-0.0271,0.0905,-0.0261,-0.0422,0.0142,0.0579,0.0104,-0.0338,0.0519,0.0193,-0.1027,-0.0752,-0.1977,-0.0478,-0.001,0.0124,0.0249,-0.0751,0.0249,-0.0134,0.1113,0.0773,0.018,-0.0124,-0.0643,0.026,0.0068,0.0716,0.0319,0.0434,-0.0314,-0.0012,-0.0209,0.0057,-0.0167,-0.0353,0.0481,0.0059,0.2251,0.0258,0.0287,-0.0424,0.0085,0.0234,-0.0409,-0.0648,0.0433,0.0612,0.0382,0.013,-0.0617,-0.0328,-0.0486,-0.0634,0.0056,-0.0495,-0.0225,-0.0272,-0.0223,-0.0102,-0.0652,0.0319,0.0275,0.007,0.0659,-0.0335,0.0076,-0.0397,-0.1053,0.0364,-0.0369,0.0199,0.0228,-0.0857,0.0127,-0.0806,0.0985,0.0402,0.0155,-0.0429,0.0164,-0.013,0.0219,0.0417,0.0085,0.0185,0.0633,0.0216,0.0249,-0.046,-0.0401,-0.0337,0.0762,-0.0388,0.0269,0.0006,0.0258,0.025,0.0969,0.0184,0.0312,-0.0047,0.0018,-0.0115,-0.0462,-0.0695,0.0296,-0.0198,-0.3004,-0.0141,0.0511,0.0172,-0.0255,0.0199,0.0662,0.0162,-0.074,0.0137,-0.0424,0.0306,-0.014,-0.0008,0.0101,0.095,0.044,-0.0313,0.0121,-0.0938,0.0266,0.0321,0.2098,-0.0446,0.0451,0.0516,-0.0145,-0.0061,-0.0327,-0.0635,0.0121,-0.0089,0.0564,-0.0556,0.0497,0.0757,-0.0358,0.0737,0.0036,0.0012,0.0491,-0.0074,-0.0467,-0.002,0.0964,-0.0457,-0.0438,-0.0339,0.0492,0.0029,-0.0134,-0.0022,0.0144,0.017,0.002,0.0216,-0.0591,-0.0201,-0.0284,-0.0466,0.0005,-0.0387,-0.046,0.0004,-0.0172]}
{"key":"[Adaptive Cut Selection in Mixed-Integer Linear Programming] Cut selection is a subroutine used in all modern mixed-integer linear programming solvers with the goal of selecting a subset of generated cuts that induce optimal solver performance. These solvers have millions of parameter combinations, and so are excellent candidates for parameter tuning. Cut selection scoring rules are usually weighted sums of different measurements, where the weights are parameters. We present a parametric family of mixed-integer linear programs together with infinitely many family-wide valid cuts. Some of these cuts can induce integer optimal solutions directly after being applied, while others fail to do so even if an infinite amount are applied. We show for a specific cut selection rule, that any finite grid search of the parameter space will always miss all parameter values, which select integer optimal inducing cuts in an infinite amount of our problems. We propose a variation on the design of existing graph convolutional neural networks, adapting them to learn cut selection rule parameters. We present a reinforcement learning framework for selecting cuts, and train our design using said framework over MIPLIB 2017. Our framework and design show that adaptive cut selection does substantially improve performance over a diverse set of instances, but that finding a single function describing such a rule is difficult. Code for reproducing all experiments is available at https://github.com/Opt-Mucca/Adaptive-Cutsel-MILP.","layer":1,"vector":[-0.0602,-0.0195,0.0145,0.0051,0.0057,0.038,-0.0019,0.0828,0.0125,-0.0074,0.0354,-0.0326,0.0142,0.0924,0.0159,0.0133,-0.004,0.0421,-0.053,0.0011,0.0353,-0.0527,-0.0417,-0.0736,0.0844,-0.0086,-0.0112,-0.0625,-0.0621,-0.2652,0.024,0.0115,0.0763,-0.0488,0.0033,0.0051,0.0221,0.0334,-0.0279,0.0074,0.0094,-0.0062,-0.0157,-0.0596,-0.0136,-0.0488,-0.0168,-0.0112,0.0119,-0.0102,0.017,-0.0363,0.0343,0.0348,0.0283,0.0177,0.0382,0.0402,0.0203,0.0557,0.0184,0.0387,-0.1596,0.0756,0.0131,0.0192,-0.0461,-0.0137,0.0279,0.0676,-0.0124,0.0604,0.0164,-0.0176,-0.0114,0.016,-0.0163,0.0053,-0.0154,0.0259,-0.0348,-0.0309,-0.0661,-0.0215,-0.0141,-0.041,0.0197,-0.0175,0.0575,0.0,-0.0488,-0.0081,-0.0229,0.0181,-0.0686,-0.0084,0.0573,0.018,-0.079,0.2218,-0.0215,0.0237,-0.0028,-0.0027,0.019,-0.0559,-0.0249,-0.0083,-0.0418,-0.0113,-0.029,-0.0403,0.0512,0.0076,-0.0105,0.0266,0.0436,0.0287,0.031,0.0073,-0.0381,0.0035,0.0493,-0.0279,0.0108,-0.0405,0.0093,0.1413,0.0114,-0.0145,0.0648,-0.0321,-0.0287,-0.0172,0.0036,0.0003,0.0063,-0.0162,-0.0432,-0.0321,-0.034,0.0196,0.0188,-0.0888,-0.0547,0.1051,-0.0407,0.0137,-0.0222,-0.065,-0.0345,0.0022,-0.0116,-0.0217,-0.0036,0.0451,0.0015,0.0019,-0.0162,0.0187,-0.0256,-0.0244,-0.09,0.095,-0.0197,-0.091,-0.0287,-0.0123,0.0052,-0.0207,0.0047,0.0426,-0.0224,0.0205,0.0778,0.036,-0.1057,0.0104,0.0293,0.0191,0.0328,-0.0435,-0.0252,0.0099,0.0397,-0.0518,0.054,-0.0614,0.0011,-0.0025,-0.0746,0.073,-0.0136,0.0178,-0.0391,-0.0176,0.0183,-0.0341,0.003,-0.0188,-0.0161,-0.0092,-0.0157,0.0129,-0.0176,-0.0007,-0.0394,-0.0044,0.0636,0.0632,-0.0534,0.0232,0.0767,-0.0075,-0.0108,0.0446,0.0233,0.0175,0.0149,0.055,0.0554,-0.0379,-0.024,-0.263,0.0025,-0.0167,-0.033,0.0697,-0.0628,0.0449,-0.0238,0.0052,0.0996,0.0273,-0.0265,-0.041,0.0048,0.015,0.0067,0.0217,-0.0032,-0.0275,-0.0039,-0.016,-0.0059,-0.0286,-0.0846,0.0265,0.0253,0.224,0.0086,0.0611,-0.0141,0.0072,0.0222,-0.007,-0.0716,0.0466,0.0241,0.091,-0.0102,-0.0212,-0.0243,-0.0045,0.0378,-0.0402,-0.0803,-0.0242,-0.0042,-0.0413,0.0402,-0.0454,-0.0109,0.0423,-0.0268,0.0399,-0.0268,0.0136,-0.0184,-0.1002,0.0478,-0.0211,-0.0086,0.0036,-0.0583,0.0173,-0.0326,0.0866,0.0317,0.0029,-0.0225,-0.0081,0.0083,0.0191,0.0213,0.0331,0.0217,0.0361,0.0094,0.0096,-0.0022,-0.0304,-0.0278,0.0889,-0.0461,0.0035,0.0304,0.0318,-0.0016,0.0441,0.0268,0.0604,-0.0355,-0.0125,-0.0107,-0.0889,-0.0018,0.048,-0.0162,-0.3114,0.0685,0.0459,-0.0085,0.0021,0.0428,0.0533,0.0172,-0.0019,-0.0271,0.0069,0.0504,0.04,-0.0039,0.0454,0.0135,0.0835,-0.0101,0.0519,-0.0643,0.0259,0.002,0.2305,-0.038,0.0682,0.0425,-0.0209,-0.0275,0.0351,-0.0134,-0.0051,0.014,0.0914,-0.0884,0.0578,0.0857,-0.0267,0.0573,0.0156,0.0057,-0.0466,0.0163,-0.0341,-0.0099,0.0461,-0.0325,-0.0218,-0.0272,0.0355,0.0226,-0.0154,0.0076,-0.0078,-0.0494,0.0328,0.0068,-0.064,-0.0537,-0.0645,-0.0079,0.0369,-0.0259,0.0098,-0.0368,-0.0328]}
{"key":"[An energy-based model for neuro-symbolic reasoning on knowledge graphs] Machine learning on graph-structured data has recently become a major topic in industry and research, finding many exciting applications such as recommender systems and automated theorem proving. We propose an energy-based graph embedding algorithm to characterize industrial automation systems, integrating knowledge from different domains like industrial automation, communications and cybersecurity. By combining knowledge from multiple domains, the learned model is capable of making context-aware predictions regarding novel system events and can be used to evaluate the severity of anomalies that might be indicative of, e.g., cybersecurity breaches. The presented model is mappable to a biologically-inspired neural architecture, serving as a first bridge between graph embedding methods and neuromorphic computing - uncovering a promising edge application for this upcoming technology.","layer":0,"vector":[-0.0674,-0.0242,0.0383,-0.0221,0.0146,0.0427,0.0623,0.0194,0.023,-0.0433,0.0025,-0.0338,0.0539,0.0624,0.0004,0.0238,-0.0277,0.0662,-0.0056,0.0234,0.0706,-0.0614,-0.0273,-0.0697,0.013,0.028,-0.0227,0.0111,-0.0607,-0.2196,0.0037,-0.0555,0.065,-0.005,-0.0052,-0.0332,0.001,0.0509,0.0146,0.0492,0.0318,-0.0093,0.0087,-0.0579,-0.0146,-0.0546,0.0093,0.0218,-0.024,-0.0907,0.0236,-0.0407,0.0418,0.0504,0.0499,0.0312,0.0564,0.0592,0.0434,0.0469,0.0388,0.0546,-0.1546,0.0515,0.0655,0.0141,-0.0635,0.0067,0.004,0.0308,0.0245,0.0026,-0.0034,0.041,0.0029,0.051,0.0054,-0.0213,-0.0378,-0.016,-0.0225,-0.0376,-0.0322,-0.0111,-0.0137,-0.0136,-0.0119,-0.0266,0.0489,0.0215,-0.044,-0.0027,0.0035,-0.0029,-0.0358,-0.0324,0.0397,0.0166,-0.0879,0.1751,-0.031,-0.0114,0.0325,-0.0184,0.0156,-0.0264,-0.0074,-0.0515,-0.0349,-0.0365,-0.006,0.0036,0.0153,-0.0515,0.0743,0.0367,0.0825,0.0217,-0.0025,-0.0175,-0.026,0.0618,0.0441,-0.033,0.0151,-0.0745,-0.0096,0.1195,0.0038,0.0186,0.0214,-0.0019,-0.0039,0.0129,0.0576,0.0252,0.024,-0.0109,-0.0107,0.015,-0.0254,-0.0006,0.0392,-0.0924,-0.0454,0.0827,-0.0452,-0.0344,-0.0772,-0.0116,-0.0268,0.0472,-0.0243,-0.0088,0.0101,0.0417,0.0234,0.0158,-0.0609,0.01,-0.0215,-0.0065,-0.0442,0.121,0.0532,-0.0843,-0.0101,-0.0156,0.02,-0.0625,0.0254,0.0357,-0.0313,0.0256,0.0457,0.03,-0.0322,-0.0188,0.0064,0.0298,0.0595,-0.0564,-0.0444,0.0343,0.0156,-0.0541,-0.0175,-0.033,-0.0036,0.0423,-0.0424,0.0502,-0.0237,0.0034,-0.0141,0.0133,-0.0168,-0.0119,-0.0059,-0.0409,-0.005,-0.017,-0.0327,0.024,-0.0422,0.028,-0.0357,0.0208,0.0122,0.0227,-0.0245,-0.027,0.0702,-0.007,-0.0198,0.0246,0.0181,0.0335,0.0443,0.0425,0.0439,0.0187,-0.0622,-0.2203,-0.0464,-0.0072,-0.0338,0.0344,-0.0689,0.0319,-0.0083,0.0358,0.0286,0.0842,-0.0158,-0.0344,0.0,-0.03,0.0454,0.0608,0.0151,-0.0797,0.0277,-0.0175,0.0366,-0.0106,-0.0984,0.0071,-0.0194,0.2412,0.0207,0.0542,-0.0309,-0.0029,0.0038,-0.0644,-0.1124,0.0737,0.0074,0.0509,0.0335,-0.0314,-0.0157,-0.0613,0.0144,-0.0461,-0.0797,-0.027,-0.0737,-0.0102,-0.0164,-0.0412,0.0383,0.0324,-0.02,0.0289,0.0267,-0.047,-0.0363,-0.0655,0.0441,-0.0181,0.0122,0.0396,-0.0538,-0.0128,-0.0433,0.0794,-0.0225,-0.0313,-0.0069,0.0174,-0.0248,-0.0472,0.1367,0.0581,-0.023,0.0691,0.0201,0.0584,-0.0295,-0.0316,0.0255,0.0432,-0.058,0.0698,0.0205,0.0393,-0.0123,0.0847,-0.0178,0.0644,-0.0178,-0.0118,0.0138,-0.0627,-0.0367,0.0619,-0.0089,-0.3099,0.0541,0.0202,0.0538,-0.0733,-0.0108,0.0288,0.0119,-0.0466,0.0146,-0.0094,0.0107,0.0153,0.0129,-0.012,0.0544,0.047,-0.0863,0.0044,-0.0396,0.0138,0.0697,0.2413,0.0107,0.0747,0.0162,-0.0153,-0.0366,0.0408,0.0185,0.0221,-0.0075,0.0727,-0.059,0.0664,0.0459,-0.0491,-0.0079,0.0522,-0.0614,0.0165,-0.0178,-0.0711,-0.0359,0.0649,0.0037,-0.047,-0.0506,0.0246,0.0291,-0.0033,0.0161,-0.0393,-0.0075,0.0322,0.02,-0.014,-0.0465,-0.0512,-0.0703,-0.0136,-0.0389,-0.0051,0.0137,-0.0351]}
{"key":"[Type I Attack for Generative Models] Generative models are popular tools with a wide range of applications. Nevertheless, it is as vulnerable to adversarial samples as classifiers. The existing attack methods mainly focus on generating adversarial examples by adding imperceptible perturbations to input, which leads to wrong result. However, we focus on another aspect of attack, i.e., cheating models by significant changes. The former induces Type II error and the latter causes Type I error. In this paper, we propose Type I attack to generative models such as VAE and GAN. One example given in VAE is that we can change an original image significantly to a meaningless one but their reconstruction results are similar. To implement the Type I attack, we destroy the original one by increasing the distance in input space while keeping the output similar because different inputs may correspond to similar features for the property of deep neural network. Experimental results show that our attack method is effective to generate Type I adversarial examples for generative models on large-scale image datasets.","layer":0,"vector":[-0.0395,-0.041,0.0292,-0.0265,0.0199,0.0363,0.0298,-0.0409,0.0042,0.0161,0.0003,-0.0421,0.0501,0.0557,0.0106,0.0126,-0.0085,0.0133,-0.0552,0.0116,0.0436,-0.0214,0.0244,-0.0384,-0.0114,-0.0047,0.004,-0.0485,-0.0227,-0.2549,0.0399,-0.0364,0.0394,-0.0189,-0.0067,-0.0353,-0.0825,0.0303,-0.0425,0.0723,0.0379,0.0477,-0.0283,-0.0825,-0.0016,-0.024,-0.06,-0.0047,0.0165,-0.0657,0.0523,-0.0282,0.0036,0.0526,0.0544,-0.0415,0.1007,0.0613,0.0265,0.0523,0.0487,0.0488,-0.1511,0.0502,0.0408,0.0585,-0.0476,-0.0355,0.0089,0.0336,-0.0341,0.019,-0.0132,0.0092,0.0269,0.0214,-0.0439,-0.0089,-0.0329,0.0253,0.0204,-0.0482,-0.0273,-0.002,-0.0175,-0.0271,0.0192,0.0085,0.0857,0.0046,-0.0027,-0.0413,-0.0229,0.0451,-0.0265,-0.0218,-0.003,0.0157,-0.068,0.219,-0.0601,-0.0209,0.0458,-0.0213,0.0453,0.0036,-0.0682,-0.0719,-0.029,0.0011,0.0055,-0.0053,-0.0112,-0.0201,-0.0197,-0.0292,0.0624,0.0308,-0.0394,-0.0372,-0.045,0.0432,0.0349,-0.0155,0.0367,-0.0775,0.0336,0.1529,0.0524,0.0384,-0.0138,-0.0068,-0.0444,-0.0063,0.0069,0.0165,-0.0233,0.0273,0.0351,-0.0444,-0.0269,-0.0287,0.0212,-0.0352,-0.0016,0.0769,-0.0268,0.0426,-0.0306,-0.0265,0.0037,0.0213,-0.0596,-0.0175,0.0436,0.0455,-0.0108,0.0472,-0.0337,-0.0155,-0.0028,-0.0412,-0.0547,0.0997,0.0358,-0.0763,-0.0115,0.0316,0.0317,0.0018,-0.0175,0.0097,-0.0078,0.0515,0.0673,0.0064,-0.0561,-0.0275,-0.0248,0.0675,-0.0046,-0.0732,-0.028,0.0301,0.0496,-0.0372,0.0294,-0.0387,0.0679,0.0485,-0.0297,0.0443,-0.06,-0.0445,-0.0243,-0.0368,-0.0735,-0.0217,0.018,-0.0563,0.0119,0.0024,-0.0433,-0.035,-0.0484,0.0157,0.0092,-0.0309,0.0233,0.0214,-0.0366,0.0116,0.043,0.0017,-0.0378,-0.0094,0.015,0.0489,0.0034,0.0485,0.0029,-0.0662,-0.0144,-0.2271,-0.0443,-0.0335,-0.057,0.0217,-0.1022,0.0466,-0.0232,0.0051,0.0345,0.0383,0.0026,-0.0328,0.0229,-0.0012,0.0636,0.0039,0.0352,0.0111,-0.0384,-0.0047,0.0261,-0.002,-0.0984,0.0128,-0.0111,0.2295,0.0554,0.0386,0.0027,0.0468,0.0429,-0.0239,-0.0793,0.0711,0.0116,0.0691,-0.0003,-0.0273,-0.0033,-0.0067,0.0191,-0.0039,-0.1125,0.0116,-0.0396,-0.031,0.0185,-0.0525,0.059,0.0631,-0.0097,0.0688,-0.017,0.0204,-0.0762,-0.1482,0.0604,-0.0389,0.035,0.0178,-0.0422,0.0237,-0.0968,0.0554,0.0261,-0.03,-0.057,0.0553,-0.0206,0.0212,0.0909,-0.0019,-0.0114,0.0564,0.0277,-0.0163,-0.0222,-0.0568,-0.0233,0.0106,0.0048,0.0311,0.0457,0.0136,-0.0081,0.0687,-0.0253,0.0426,-0.0536,0.0147,0.0187,-0.0645,-0.0106,0.0317,0.0093,-0.2852,0.0264,0.0169,0.0961,-0.0393,0.0485,0.0181,0.0067,-0.0578,-0.0138,0.0161,0.0261,0.0648,0.0091,0.0002,0.0232,0.0652,-0.0759,0.0562,-0.0414,0.0229,0.0286,0.2301,-0.0324,-0.006,-0.0213,-0.0316,0.0347,0.0163,0.0064,-0.0263,0.0109,0.0659,0.0072,0.0408,0.0647,-0.0346,0.0132,0.0111,-0.0381,-0.0156,0.0255,-0.0609,0.0348,0.0509,0.0209,0.0312,-0.0187,0.0016,0.0196,-0.0308,0.0052,-0.0002,0.0137,0.0499,0.0318,-0.0405,-0.0403,-0.0188,0.026,0.042,0.0013,-0.0398,-0.0016,-0.0371]}
{"key":"[Low-Pass Filtering SGD for Recovering Flat Optima in the Deep Learning Optimization Landscape] In this paper, we study the sharpness of a deep learning (DL) loss landscape around local minima in order to reveal systematic mechanisms underlying the generalization abilities of DL models. Our analysis is performed across varying network and optimizer hyper-parameters, and involves a rich family of different sharpness measures. We compare these measures and show that the low-pass filter-based measure exhibits the highest correlation with the generalization abilities of DL models, has high robustness to both data and label noise, and furthermore can track the double descent behavior for neural networks. We next derive the optimization algorithm, relying on the low-pass filter (LPF), that actively searches the flat regions in the DL optimization landscape using SGD-like procedure. The update of the proposed algorithm, that we call LPF-SGD, is determined by the gradient of the convolution of the filter kernel with the loss function and can be efficiently computed using MC sampling. We empirically show that our algorithm achieves superior generalization performance compared to the common DL training strategies. On the theoretical front, we prove that LPF-SGD converges to a better optimal point with smaller generalization error than SGD.","layer":0,"vector":[-0.0035,-0.0273,0.0345,0.0029,0.0234,0.051,0.0,0.0356,0.0381,-0.0214,0.056,-0.0399,0.0752,0.0426,0.0055,0.0045,0.007,0.0339,-0.0488,-0.0308,0.0663,-0.0227,-0.0096,-0.0714,0.0069,-0.0362,0.0088,-0.0034,-0.058,-0.2942,0.0226,-0.0034,0.0638,-0.0542,0.0202,-0.0359,-0.0159,0.0486,-0.0599,0.0286,0.0254,0.0179,-0.0446,-0.0341,-0.015,-0.0279,-0.006,-0.0538,-0.0351,-0.039,-0.0024,0.003,0.0253,0.0358,0.0113,0.0308,0.0935,0.0449,0.0354,0.0471,-0.0253,0.0551,-0.1631,0.0197,0.0206,0.015,-0.0195,-0.0369,-0.0261,0.0483,-0.0176,0.0458,0.0186,0.007,-0.0309,-0.0051,-0.0083,0.007,-0.0117,0.0268,0.0577,-0.0333,-0.0155,-0.0184,-0.0065,-0.0309,0.029,-0.0449,0.0585,0.0201,-0.0428,-0.0295,-0.0329,0.0063,-0.0719,0.0293,0.0381,0.0465,-0.065,0.2155,-0.0395,0.0411,0.0407,-0.0167,0.0166,-0.0287,-0.0433,0.0028,-0.0074,0.0101,-0.0101,-0.0335,0.0008,-0.0098,0.023,0.0114,0.0502,0.0614,-0.0157,0.0253,-0.0596,-0.0058,0.0497,-0.0302,0.0411,-0.0433,0.002,0.1103,0.0474,0.0414,0.0231,-0.0317,-0.0362,-0.0452,0.0558,0.0537,0.0045,-0.0252,0.0111,0.0146,-0.0726,0.0036,-0.0062,-0.0638,-0.0299,0.1165,-0.0537,0.0116,-0.0528,-0.0457,0.0077,-0.0012,0.0055,-0.037,0.0201,0.0372,0.0242,0.0393,-0.0671,0.0277,-0.0034,-0.0004,-0.0641,0.1129,-0.0163,-0.0709,-0.0218,0.0023,-0.0108,0.0168,0.0737,0.0499,-0.0393,-0.0358,0.0494,0.0374,-0.1076,0.0049,-0.0513,-0.0106,0.052,-0.0338,-0.0102,0.0066,0.0474,-0.0297,0.0382,-0.0452,0.009,0.0012,-0.0437,-0.0028,-0.0509,-0.0186,-0.0229,-0.0295,-0.0194,-0.0442,0.006,0.0118,0.0147,0.0036,-0.0361,0.048,-0.0011,0.0168,-0.0184,-0.0171,0.0397,0.0572,-0.062,-0.0124,0.0696,-0.0335,0.003,-0.0377,-0.0062,0.0062,-0.0623,0.0323,0.0304,-0.0454,-0.0666,-0.2364,-0.0052,0.0069,-0.0378,0.0796,-0.0992,0.0416,-0.0125,0.0641,0.0869,0.034,-0.0326,-0.0337,0.021,-0.007,0.0997,0.0273,0.0271,-0.0336,0.006,0.0163,0.0385,0.0022,-0.0883,0.0424,-0.0219,0.2156,-0.0089,0.0573,-0.0286,-0.003,0.0479,0.0291,-0.0477,0.0357,0.0088,0.0671,-0.0376,-0.0584,-0.075,-0.021,-0.002,0.0067,-0.1329,-0.0375,-0.0167,-0.0598,0.0515,-0.0379,0.0368,0.0403,-0.0614,0.0516,-0.0224,0.0249,-0.0303,-0.0975,0.0129,-0.0294,0.0439,-0.0264,-0.0695,0.0262,-0.0754,0.0403,0.0108,-0.0223,-0.0357,0.0347,-0.0296,0.0002,0.0553,0.0008,0.0185,0.0569,-0.0003,0.0381,-0.0127,-0.0446,-0.0377,0.1027,0.0095,0.0543,0.001,0.0164,0.048,0.0747,-0.0158,0.0392,-0.0552,-0.0032,-0.0021,-0.0553,-0.0385,0.0396,-0.0177,-0.2812,0.0338,0.0201,0.0337,-0.0333,0.0194,0.0403,0.0116,-0.0131,0.001,0.0039,0.0355,0.0496,-0.0136,0.0113,0.0293,0.0433,-0.035,0.1075,-0.04,0.0029,0.059,0.1923,-0.0465,-0.0028,-0.0272,-0.0298,-0.0285,0.0142,-0.0096,-0.0095,0.0328,0.076,-0.0437,0.037,0.1058,-0.0422,0.0004,0.0288,0.0184,0.024,0.0303,-0.0254,-0.0154,0.0751,-0.0592,-0.0263,-0.0008,0.0034,0.0257,-0.0052,0.0305,0.0507,-0.0286,0.0494,0.0019,-0.0457,-0.0309,-0.0067,-0.011,0.0268,-0.0631,-0.0369,-0.0369,0.0243]}
{"key":"[Projective Inference in High-dimensional Problems: Prediction and Feature Selection] This paper discusses predictive inference and feature selection for generalized linear models with scarce but high-dimensional data. We argue that in many cases one can benefit from a decision theoretically justified two-stage approach: first, construct a possibly non-sparse model that predicts well, and then find a minimal subset of features that characterize the predictions. The model built in the first step is referred to as the \\emph{reference model} and the operation during the latter step as predictive \\emph{projection}. The key characteristic of this approach is that it finds an excellent tradeoff between sparsity and predictive accuracy, and the gain comes from utilizing all available information including prior and that coming from the left out features. We review several methods that follow this principle and provide novel methodological contributions. We present a new projection technique that unifies two existing techniques and is both accurate and fast to compute. We also propose a way of evaluating the feature selection process using fast leave-one-out cross-validation that allows for easy and intuitive model size selection. Furthermore, we prove a theorem that helps to understand the conditions under which the projective approach could be beneficial. The benefits are illustrated via several simulated and real world examples.","layer":5,"vector":[-0.048,0.0063,0.0231,0.0069,0.0137,0.0048,0.0496,0.0415,-0.0032,-0.0151,0.003,-0.0324,0.0192,0.0489,0.0338,0.0163,0.0428,0.0494,-0.0527,0.0506,0.0538,-0.0354,-0.0315,-0.0348,0.0354,-0.0018,0.0034,-0.0138,-0.0549,-0.2506,-0.0165,-0.0076,0.0658,-0.009,0.0283,-0.0532,-0.0268,0.0634,-0.0258,0.0364,-0.0026,0.0259,-0.0278,-0.0379,-0.001,-0.0601,-0.0049,-0.0025,-0.0578,-0.0524,0.0102,-0.0328,-0.0182,0.0203,0.0235,0.0086,0.0456,0.02,0.0163,0.0267,0.0151,0.0517,-0.1591,0.0558,0.0559,0.0631,-0.0489,-0.0181,0.03,0.0485,-0.0254,0.0541,-0.0332,0.046,0.0337,-0.0612,0.0163,0.0084,0.0041,0.051,0.0538,0.0197,-0.0781,-0.0164,-0.0477,-0.0263,0.0154,-0.0364,0.037,-0.0035,-0.0539,0.0043,-0.0395,0.0504,-0.0816,-0.0414,0.0903,0.0309,-0.0424,0.2232,-0.0489,0.0184,0.0348,0.001,0.0206,-0.0701,-0.0817,-0.0212,-0.0037,-0.0138,0.0181,-0.0205,0.0031,-0.0115,0.0378,-0.0506,0.1115,0.0348,-0.0233,0.0195,-0.0023,-0.0086,0.0555,-0.0387,-0.023,-0.0873,0.0454,0.1397,0.0188,0.0142,0.0388,-0.0235,-0.0833,-0.0238,0.0075,0.0087,0.0851,0.043,-0.0093,-0.0104,-0.0378,-0.0482,0.0395,-0.0444,-0.0546,0.1338,-0.0702,0.0223,-0.0348,-0.0577,0.0181,0.0158,-0.0334,-0.0261,-0.0007,0.0262,-0.0312,-0.0001,-0.0586,0.0419,-0.0289,-0.01,-0.0217,0.0799,-0.0335,-0.0644,-0.0587,-0.0159,0.0053,0.0035,0.0424,0.0366,-0.0467,0.0115,0.087,0.0204,-0.0575,0.0333,0.023,0.0166,0.0282,-0.052,-0.0697,0.066,0.0436,-0.0368,0.019,-0.0576,-0.0035,0.0242,-0.0234,0.0084,-0.0008,-0.002,-0.0204,0.0067,-0.0238,0.0328,0.0284,-0.0422,0.015,0.0603,-0.0639,0.0063,0.0072,0.026,0.0026,0.0123,0.0239,0.0622,-0.0151,-0.0091,0.069,-0.0235,-0.0342,0.0399,0.0155,0.0343,0.0111,0.0682,0.0303,-0.0349,-0.0405,-0.2569,0.0189,0.009,-0.0133,0.0442,-0.1089,0.0537,-0.0222,0.0543,0.0603,0.0342,-0.0159,-0.0478,0.0638,-0.022,0.0494,-0.0113,-0.0204,-0.0409,0.0149,-0.0406,0.0245,-0.0394,-0.0714,0.0458,0.0163,0.2074,0.0323,0.0103,-0.0196,0.0148,0.0357,-0.0495,-0.0889,0.0636,0.044,0.0629,-0.027,-0.0342,-0.0277,-0.0037,0.043,-0.0057,-0.0677,-0.0473,-0.0151,-0.003,0.0658,-0.0523,0.0291,0.0714,-0.0159,0.0262,-0.0538,0.0351,-0.0222,-0.1089,0.0215,-0.0136,0.0501,0.0001,-0.0462,-0.0005,-0.0512,0.0567,-0.0118,-0.0348,-0.0554,-0.0153,-0.0235,-0.0162,0.063,0.0079,0.0114,0.0652,-0.0017,0.0205,-0.0488,-0.0565,-0.0476,0.0846,-0.0319,0.0004,0.0343,0.0451,-0.0406,0.0834,-0.0371,0.0117,-0.0288,-0.0014,-0.029,-0.0236,-0.0029,0.0325,0.0041,-0.2687,0.0075,0.0075,0.0289,-0.0482,-0.0036,0.0492,0.0007,-0.0165,0.0366,0.0522,0.0131,0.0896,-0.0073,0.0012,0.0144,0.1123,-0.0677,0.0159,-0.065,0.0016,0.0263,0.1829,-0.0566,0.0428,-0.0092,-0.0022,-0.0374,-0.0079,-0.0188,0.0368,0.0487,0.097,-0.0465,0.0164,0.0952,-0.0312,0.0152,0.0023,-0.0323,0.0146,-0.0219,-0.0533,-0.0408,0.1013,-0.0181,-0.0179,0.0182,-0.011,0.0179,-0.0332,0.01,-0.0152,0.0429,-0.0094,-0.0039,-0.0267,-0.0335,-0.0324,-0.0373,-0.0063,-0.0329,-0.0414,-0.0219,-0.0093]}
{"key":"[Interpolating between sampling and variational inference with infinite stochastic mixtures] Sampling and Variational Inference (VI) are two large families of methods for approximate inference that have complementary strengths. Sampling methods excel at approximating arbitrary probability distributions, but can be inefficient. VI methods are efficient, but may misrepresent the true distribution. Here, we develop a general framework where approximations are stochastic mixtures of simple component distributions. Both sampling and VI can be seen as special cases: in sampling, each mixture component is a delta-function and is chosen stochastically, while in standard VI a single component is chosen to minimize divergence. We derive a practical method that interpolates between sampling and VI by solving an optimization problem over a mixing distribution. Intermediate inference methods then arise by varying a single parameter. Our method provably improves on sampling (reducing variance) and on VI (reducing bias+variance despite increasing variance). We demonstrate our method's bias/variance trade-off in practice on reference problems, and we compare outcomes to commonly used sampling and VI methods. This work takes a step towards a highly flexible yet simple family of inference methods that combines the complementary strengths of sampling and VI.","layer":1,"vector":[-0.0254,0.0067,0.019,-0.0113,0.0458,0.0291,0.0154,0.0446,0.049,0.0076,0.0121,-0.0687,0.0401,0.0679,0.0478,0.0378,-0.0101,0.0311,-0.0432,-0.0065,0.0104,-0.0357,-0.0024,-0.0613,0.0356,-0.0216,0.0058,-0.0556,-0.0178,-0.2639,0.0306,-0.0095,0.0361,-0.0631,0.0199,-0.0348,-0.0497,0.0246,-0.0104,0.0665,0.0326,0.0105,-0.0353,-0.0267,-0.0691,-0.0569,-0.0365,0.0083,-0.0496,-0.0024,0.017,-0.0356,0.0157,0.0305,0.0366,0.061,0.0681,0.0363,0.0576,0.0431,-0.0148,0.0406,-0.1501,0.0889,0.0409,0.0497,-0.0351,0.019,0.0296,0.0557,-0.0385,0.0759,-0.0455,0.0697,0.0224,-0.0295,0.023,-0.053,-0.0343,0.033,-0.0014,-0.0317,-0.0232,0.0105,-0.0308,-0.0263,0.0338,-0.0367,0.0579,0.0226,-0.0089,-0.0319,-0.036,0.0299,-0.0301,0.0214,0.035,0.02,-0.005,0.1949,-0.0464,0.0411,0.0229,-0.0001,-0.0079,-0.0537,-0.0249,-0.0397,-0.0053,0.0414,0.0159,-0.0133,0.0291,-0.0651,0.0037,-0.0113,0.0764,-0.0116,0.0058,-0.0239,-0.0293,0.0063,0.04,0.0159,0.0127,-0.0826,0.0046,0.1644,0.0262,0.0177,0.0379,-0.0338,-0.0281,-0.0693,0.0131,0.0053,-0.0063,0.0013,0.0453,0.0145,-0.0554,-0.0754,0.0185,-0.0934,-0.048,0.1368,-0.0349,-0.0036,-0.0866,-0.006,0.0477,0.0264,-0.0111,0.0006,0.0575,0.0258,0.041,0.0356,-0.058,0.0384,-0.006,-0.0661,0.0049,0.0947,-0.02,-0.024,-0.0541,0.0027,-0.0112,-0.0095,0.0383,0.0323,-0.0254,0.0347,0.0569,0.0184,-0.0868,0.0314,0.0198,0.0196,-0.0016,-0.0416,-0.0666,0.0569,-0.002,-0.0558,-0.0077,-0.0497,0.0062,0.0588,-0.0066,-0.0008,-0.0107,-0.0028,0.0018,-0.0221,-0.0225,0.0052,0.0187,-0.0296,0.0116,-0.0109,-0.1084,0.0388,0.0093,0.0249,-0.0039,0.0015,0.0606,0.0511,-0.0134,-0.0288,0.035,0.0252,-0.0035,0.0383,0.023,-0.0117,0.0223,0.0274,0.0372,-0.0784,-0.0267,-0.2191,-0.0099,0.0298,0.0109,0.0642,-0.0439,0.0562,0.0141,0.0544,0.0848,-0.0185,-0.002,-0.0176,0.0289,-0.0139,0.0503,0.0,-0.0099,-0.0152,0.0462,-0.0371,0.0175,-0.0476,-0.0778,0.0707,-0.0019,0.235,0.0157,0.0336,0.0111,-0.0029,0.0459,0.0072,-0.0796,0.0719,0.0384,0.0594,-0.0414,-0.0166,-0.0153,-0.0238,0.0269,-0.025,-0.0956,-0.0567,-0.0626,-0.0586,0.0429,-0.0523,-0.0167,0.026,-0.014,0.0322,-0.053,0.0043,-0.0342,-0.0824,0.0121,-0.0305,-0.0059,0.025,-0.0275,0.0118,-0.0547,0.04,-0.0081,0.0051,-0.0714,-0.0096,0.0115,-0.0241,0.0638,-0.0417,0.0374,0.0635,0.0134,0.012,-0.0349,-0.0893,-0.0222,0.046,-0.0526,-0.0103,0.0399,0.0075,0.0214,0.0833,-0.0395,0.0258,-0.0116,-0.0041,0.005,-0.0509,0.0008,-0.0028,-0.025,-0.2857,0.018,0.0083,0.0491,-0.024,-0.0003,0.0595,0.0184,-0.0345,-0.0287,-0.0019,0.0184,0.0466,0.0021,0.0197,0.0282,0.0901,-0.0842,0.0594,-0.0931,0.0048,0.0242,0.2253,-0.0307,0.0136,0.0235,0.0111,0.0132,0.0254,-0.0529,-0.0214,-0.031,0.0665,-0.0582,0.0348,0.0832,-0.0264,0.0705,0.0203,-0.0641,0.0049,-0.041,-0.0121,-0.0176,0.092,-0.0525,-0.0176,-0.0358,-0.0082,0.0148,-0.0513,0.0486,0.0113,-0.033,0.0174,0.0098,-0.0292,-0.0645,-0.0137,-0.0207,0.0251,-0.0688,-0.0229,-0.0235,0.0134]}
{"key":"[Predicting Periodicity with Temporal Difference Learning] Temporal difference (TD) learning is an important approach in reinforcement learning, as it combines ideas from dynamic programming and Monte Carlo methods in a way that allows for online and incremental model-free learning. A key idea of TD learning is that it is learning predictive knowledge about the environment in the form of value functions, from which it can derive its behavior to address long-term sequential decision making problems. The agent's horizon of interest, that is, how immediate or long-term a TD learning agent predicts into the future, is adjusted through a discount rate parameter. In this paper, we introduce an alternative view on the discount rate, with insight from digital signal processing, to include complex-valued discounting. Our results show that setting the discount rate to appropriately chosen complex numbers allows for online and incremental estimation of the Discrete Fourier Transform (DFT) of a signal of interest with TD learning. We thereby extend the types of knowledge representable by value functions, which we show are particularly useful for identifying periodic effects in the reward sequence.","layer":1,"vector":[-0.0625,0.0095,0.0589,-0.0064,0.0084,0.0074,0.018,0.0556,0.0642,-0.0452,0.0215,-0.0005,0.0414,0.0688,0.0287,0.0215,-0.0698,0.0327,-0.0065,-0.0046,0.0512,-0.065,-0.0301,-0.0471,0.0185,0.0089,-0.0277,-0.0459,-0.041,-0.2028,0.0464,-0.0304,0.0368,-0.0587,-0.0078,-0.0415,-0.0736,0.0574,-0.0332,0.0741,0.0253,0.0165,-0.0034,-0.0492,-0.0639,-0.083,-0.0155,-0.0259,-0.0128,-0.0351,0.0056,-0.0282,0.0139,0.0259,0.0346,0.0287,0.0628,0.0998,0.0777,0.033,0.0235,0.0182,-0.1651,0.0771,0.0375,0.0625,-0.0417,-0.0047,0.0399,0.0049,-0.031,0.0454,0.0155,-0.0047,0.0816,-0.035,-0.0324,-0.0548,-0.0372,-0.004,0.0207,-0.0596,-0.045,-0.0353,-0.0218,-0.0707,0.0098,-0.0733,0.0444,0.0172,-0.0411,0.0183,-0.0573,0.026,-0.0807,0.0143,0.0508,0.0346,-0.0472,0.1916,0.0022,0.0268,0.0315,-0.0484,0.0339,-0.0588,-0.0462,-0.0433,-0.0431,-0.0168,-0.0437,-0.0008,0.0607,-0.0372,-0.0198,0.0405,0.0533,0.0289,0.0409,0.0044,-0.005,0.0238,0.052,-0.0286,0.023,-0.0388,0.0274,0.1437,-0.0145,0.0088,0.0504,-0.0541,-0.0201,-0.0101,0.0262,0.0575,-0.0189,0.0022,-0.001,0.0225,-0.0654,-0.0203,-0.0083,-0.1232,-0.0474,0.1002,0.0321,0.036,-0.0128,-0.013,-0.0069,0.0118,0.0055,-0.0531,0.0272,0.0785,0.0324,0.0149,-0.0199,-0.0106,-0.046,-0.0309,-0.0002,0.1361,-0.0016,-0.0518,-0.0052,-0.0364,0.0326,-0.032,0.0206,0.0214,-0.0538,-0.0075,0.1373,0.0101,-0.069,-0.0,0.0161,-0.009,0.0412,-0.0346,-0.015,0.0445,0.0475,-0.0346,-0.007,-0.0259,0.0164,0.0288,0.0059,-0.0151,-0.0325,0.021,-0.061,-0.0396,-0.0079,-0.0251,0.0221,-0.0197,0.0033,-0.0368,-0.0195,-0.0196,0.0194,0.0342,-0.0213,-0.0026,0.0707,0.0084,-0.0476,0.002,0.0652,-0.0064,-0.0132,0.0027,0.0428,-0.0044,-0.0344,0.0544,0.0233,0.0097,-0.0561,-0.213,-0.0235,0.0141,-0.0117,0.0619,-0.057,0.0143,-0.0719,0.0253,0.0432,0.0322,-0.0608,-0.01,-0.0215,-0.0133,0.0254,0.0744,0.0108,0.0195,0.0127,-0.0153,-0.0055,-0.0283,-0.0867,0.0566,-0.0263,0.1934,0.0116,0.0593,-0.0381,0.0135,0.0217,-0.0358,-0.0313,0.0291,0.0059,0.0622,-0.0509,-0.0,-0.0627,-0.0163,0.0296,-0.0285,-0.0572,-0.039,-0.0112,-0.025,0.0505,-0.0719,0.0299,0.0434,-0.0189,0.026,-0.0044,-0.0069,-0.0638,-0.071,0.045,-0.022,-0.0017,0.0471,-0.0493,-0.0032,-0.0384,0.0999,0.0038,0.0109,-0.0789,0.0282,0.0196,0.0301,0.0591,-0.0084,-0.0216,0.0313,-0.0224,0.003,-0.02,-0.0394,0.016,0.0768,-0.048,0.0321,0.0392,0.0041,-0.0132,0.0673,-0.0096,0.0066,-0.0071,-0.0034,0.0188,-0.0369,-0.017,0.0166,-0.0024,-0.3092,0.0573,0.0239,0.0209,-0.0194,0.0284,0.0241,-0.0078,-0.0691,0.0054,-0.0113,0.0589,0.0296,0.0179,0.0357,0.062,0.0858,-0.0111,0.0109,-0.0564,0.0263,0.0494,0.2695,-0.0249,0.0404,-0.0082,-0.0207,0.0246,0.0518,-0.0355,0.0016,0.0067,0.0581,-0.0529,0.0043,0.0651,-0.0457,0.069,-0.0027,-0.0145,0.0163,0.0263,-0.0304,-0.0159,0.1282,0.0194,-0.0264,-0.058,-0.0436,0.0289,-0.0516,0.0434,-0.0035,-0.0194,0.0086,0.0191,-0.0549,-0.0487,-0.0027,-0.0399,-0.0172,-0.0756,0.0185,-0.0126,-0.0121]}
{"key":"[Temporal Interpolation via Motion Field Prediction] Navigated 2D multi-slice dynamic Magnetic Resonance (MR) imaging enables high contrast 4D MR imaging during free breathing and provides in-vivo observations for treatment planning and guidance. Navigator slices are vital for retrospective stacking of 2D data slices in this method. However, they also prolong the acquisition sessions. Temporal interpolation of navigator slices an be used to reduce the number of navigator acquisitions without degrading specificity in stacking. In this work, we propose a convolutional neural network (CNN) based method for temporal interpolation via motion field prediction. The proposed formulation incorporates the prior knowledge that a motion field underlies changes in the image intensities over time. Previous approaches that interpolate directly in the intensity space are prone to produce blurry images or even remove structures in the images. Our method avoids such problems and faithfully preserves the information in the image. Further, an important advantage of our formulation is that it provides an unsupervised estimation of bi-directional motion fields. We show that these motion fields can be used to halve the number of registrations required during 4D reconstruction, thus substantially reducing the reconstruction time.","layer":3,"vector":[-0.0447,-0.0106,0.0098,-0.0288,0.0039,0.0334,0.016,0.0012,0.026,-0.0022,0.0263,-0.0569,0.0144,0.062,0.0108,0.0104,-0.052,0.0598,-0.0169,0.0246,0.0226,-0.0455,0.0427,-0.0373,0.0028,0.0289,-0.0262,-0.0217,-0.0455,-0.2079,0.0314,-0.0502,0.0166,-0.0271,-0.0036,-0.04,-0.0264,0.0821,-0.0398,0.0499,0.0292,0.0194,0.0018,-0.0719,-0.053,-0.0486,-0.0499,-0.0413,0.0487,0.0109,0.0304,-0.0102,-0.0097,0.0748,0.0033,0.0253,0.0665,0.0384,0.032,0.0151,0.047,0.0573,-0.1879,0.0501,0.0217,0.0425,-0.0458,-0.023,0.0311,0.0206,-0.0369,0.0527,0.0178,0.0231,0.0074,-0.0217,-0.0117,-0.0274,-0.0267,-0.0041,0.0375,0.012,-0.046,-0.0455,-0.0191,-0.0203,0.0261,-0.0755,0.0072,0.0166,-0.0547,-0.0284,-0.0757,0.0249,-0.0959,0.0264,0.0255,0.0523,-0.033,0.197,-0.0531,0.0555,0.0636,-0.0166,0.0177,-0.0348,-0.0273,-0.0016,-0.0427,0.0267,-0.0297,-0.0202,0.084,-0.0222,0.0075,0.0492,0.0509,0.0371,0.0116,0.0064,-0.0447,0.0087,0.0633,-0.0496,0.0092,-0.058,0.025,0.134,0.0373,0.0196,0.0272,0.0225,-0.0318,-0.0334,0.0124,-0.0145,0.0159,-0.0349,0.0131,-0.0058,-0.0246,-0.0645,0.0071,-0.0663,-0.0341,0.1117,-0.026,0.0162,-0.0836,-0.0253,-0.0285,0.0424,-0.074,-0.0015,0.0032,0.0085,0.0087,0.0498,-0.0495,-0.0187,-0.0494,-0.1189,-0.0652,0.1299,0.0047,-0.0761,-0.0135,-0.0003,-0.0021,-0.0163,0.0204,-0.007,-0.0053,-0.0274,0.1026,0.0417,-0.0723,0.0423,0.012,-0.0015,0.0213,-0.0538,-0.0269,0.0436,0.0279,-0.0231,0.0151,-0.0095,0.0032,0.0545,-0.0302,0.0282,-0.0702,0.0287,-0.052,0.0223,-0.0141,0.0193,0.0068,-0.0516,0.0009,-0.0567,-0.0279,0.0199,0.0241,-0.0175,-0.0195,0.0455,0.0121,0.0579,-0.0264,-0.0179,0.073,-0.0714,-0.016,-0.0115,-0.0102,-0.0005,-0.0317,0.0737,0.0236,-0.0676,-0.0213,-0.2328,0.0156,-0.0198,0.0458,0.0319,-0.0972,0.0347,-0.0023,0.0929,0.0584,0.0436,-0.027,0.0046,-0.0087,0.0008,0.0303,0.0634,0.0188,-0.0111,-0.0339,0.0423,-0.0074,-0.0301,-0.0673,0.0619,0.0334,0.2277,0.0181,0.057,0.0143,0.0286,0.0137,-0.0268,-0.0796,0.0645,0.0038,0.0729,0.0041,-0.0355,-0.074,-0.0795,-0.0475,0.0333,-0.0674,-0.0333,0.0123,-0.063,0.0138,-0.0341,-0.0105,0.0453,-0.0645,0.007,-0.033,0.0206,-0.023,-0.0819,0.0214,-0.0229,-0.0006,-0.0313,-0.0361,0.0146,-0.0389,0.0171,0.0355,-0.0398,-0.0637,-0.0265,-0.0122,0.0095,0.0536,0.0082,0.0111,0.1042,-0.0076,0.0627,0.0033,-0.0775,-0.0072,0.0755,0.002,0.0272,0.0845,0.0106,0.0153,0.023,-0.0077,-0.0116,-0.0606,-0.0111,0.0147,-0.0484,-0.0241,-0.0191,-0.0075,-0.2953,0.0409,0.0374,0.045,0.0025,0.0673,0.0213,0.0131,-0.013,-0.0225,-0.0443,0.0349,0.0707,-0.0152,0.0229,0.0209,0.0853,-0.0342,0.0532,-0.0996,0.0294,0.0318,0.1858,0.0045,0.0295,0.0386,-0.0469,-0.0062,0.021,0.0042,0.0606,-0.01,0.0674,-0.0332,0.0666,0.0873,-0.0156,0.0563,-0.0165,0.0072,-0.0163,0.0467,0.0036,-0.0385,0.0831,-0.0197,-0.0446,0.008,-0.0161,0.0236,-0.0289,0.0294,0.004,0.026,0.0566,0.04,-0.0006,-0.075,-0.0364,-0.0009,0.0312,-0.05,-0.0227,-0.0183,-0.0091]}
{"key":"[On the Equivalence between Kernel Quadrature Rules and Random Feature Expansions] We show that kernel-based quadrature rules for computing integrals can be seen as a special case of random feature expansions for positive definite kernels, for a particular decomposition that always exists for such kernels. We provide a theoretical analysis of the number of required samples for a given approximation error, leading to both upper and lower bounds that are based solely on the eigenvalues of the associated integral operator and match up to logarithmic terms. In particular, we show that the upper bound may be obtained from independent and identically distributed samples from a specific non-uniform distribution, while the lower bound if valid for any set of points. Applying our results to kernel-based quadrature, while our results are fairly general, we recover known upper and lower bounds for the special cases of Sobolev spaces. Moreover, our results extend to the more general problem of full function approximations (beyond simply computing an integral), with results in L2- and L$\\infty$-norm that match known results for special cases. Applying our results to random features, we show an improvement of the number of random features needed to preserve the generalization guarantees for learning with Lipschitz-continuous losses.","layer":5,"vector":[-0.0559,-0.028,0.0509,-0.0127,0.0256,0.0085,0.0505,0.0593,0.029,-0.0484,-0.0047,-0.0672,0.03,0.0207,0.0308,0.0352,0.03,0.0345,-0.0951,-0.0108,0.0267,-0.0265,-0.0054,-0.0506,0.0016,-0.0031,-0.0313,-0.06,-0.0037,-0.2557,0.03,-0.0145,0.0501,-0.0689,0.0079,-0.0126,-0.038,0.0645,-0.0536,0.0436,0.0237,0.0114,-0.0501,-0.0437,-0.0255,-0.0784,-0.0209,-0.029,-0.0188,0.0038,0.0216,-0.0149,0.0482,0.0099,-0.0014,0.0325,0.052,0.0441,0.0673,0.0853,-0.0091,0.0468,-0.13,0.0615,0.0191,-0.0074,-0.0387,-0.039,0.0236,0.0709,-0.0097,0.0279,-0.0208,0.0228,-0.0157,-0.0209,0.0102,-0.0344,-0.0461,0.0614,0.007,-0.0413,-0.0304,-0.0023,-0.0245,-0.0143,0.0409,-0.046,0.0214,0.0295,0.007,-0.0412,-0.0421,0.0036,-0.0338,-0.029,0.0429,-0.008,-0.0333,0.2109,-0.0402,-0.0011,0.0192,-0.0079,0.0166,-0.037,-0.0527,-0.0438,-0.028,-0.0444,-0.0046,-0.0253,-0.0009,-0.0455,0.0297,-0.0278,0.0634,0.0523,-0.0019,-0.0317,-0.041,0.0418,0.0422,0.0147,0.0387,-0.055,-0.0271,0.111,0.04,0.0333,0.0274,-0.0016,-0.0493,-0.041,-0.022,0.0355,-0.0243,0.0585,0.0089,0.0191,-0.0429,-0.0741,0.0131,-0.0728,-0.0006,0.1427,-0.0405,0.0482,-0.0289,-0.0446,0.0553,0.0642,-0.0367,-0.037,0.0554,-0.012,0.0253,-0.0031,-0.0208,0.0207,-0.0259,-0.0625,0.0084,0.1075,-0.055,-0.0478,0.0043,0.0538,0.0223,-0.0248,0.0326,0.0214,0.0143,0.0504,0.1161,0.0462,-0.0783,0.0287,0.0258,0.0148,0.0125,-0.0563,-0.058,0.0098,0.0397,-0.0319,-0.0226,-0.024,0.0255,0.0454,-0.0206,-0.0211,-0.0426,-0.0119,-0.0099,-0.017,0.0007,-0.0101,0.0446,-0.0439,0.0002,0.0077,-0.0508,0.0208,-0.0427,-0.0195,0.0057,0.0052,0.0036,0.0229,-0.0106,-0.0101,0.0547,-0.0298,-0.0229,0.0342,0.0302,0.0263,-0.0084,0.0234,0.0251,-0.036,-0.1052,-0.2502,-0.0525,-0.005,0.0115,0.0487,-0.0622,0.0687,-0.0062,0.0542,0.0656,0.0316,-0.0049,-0.0424,0.0285,-0.022,0.0044,0.0242,-0.0212,-0.0562,-0.0283,-0.0584,0.0196,-0.0428,-0.0415,0.0548,-0.0242,0.215,0.0018,0.0405,-0.0422,0.0161,0.0606,0.0061,-0.0507,0.0999,0.0815,0.0736,-0.0035,-0.0264,-0.016,-0.0022,0.0144,0.0266,-0.0511,-0.0526,-0.048,-0.0363,0.0216,-0.056,0.0145,0.0675,-0.0385,0.0944,-0.0449,0.0478,-0.031,-0.0821,0.0072,-0.0072,0.0377,-0.0136,-0.0698,0.0227,-0.0431,0.0767,0.0098,0.028,-0.0236,0.0308,-0.0483,-0.0034,0.0657,-0.0159,0.0258,0.0559,-0.0072,0.0349,0.0365,-0.0205,-0.0597,0.0957,-0.0626,0.0045,-0.0148,0.0506,0.0143,0.0708,-0.0191,0.001,-0.0299,-0.0662,0.0366,-0.0667,-0.0071,0.0304,0.0018,-0.2851,0.0103,-0.0018,-0.0066,-0.0267,-0.0049,0.0598,-0.0232,-0.0791,-0.0098,-0.0036,0.0864,0.0406,-0.0085,0.009,0.0661,0.0442,-0.0496,0.0517,-0.0552,0.011,0.0536,0.2006,-0.0583,0.013,0.0213,-0.0354,0.0055,0.0078,-0.021,0.0567,-0.0107,0.0787,-0.0642,0.0339,0.0916,-0.0169,0.0686,-0.012,-0.0702,0.0257,-0.0361,-0.0531,0.0159,0.0592,0.0156,-0.0016,-0.0677,0.0148,0.051,-0.0166,0.0248,0.0196,-0.0182,0.0472,0.0576,-0.0316,-0.0234,-0.0023,-0.0392,0.0236,-0.0267,-0.017,-0.0051,-0.0084]}
{"key":"[Translation and Rotation Equivariant Normalizing Flow (TRENF) for Optimal Cosmological Analysis] Our universe is homogeneous and isotropic, and its perturbations obey translation and rotation symmetry. In this work we develop Translation and Rotation Equivariant Normalizing Flow (TRENF), a generative Normalizing Flow (NF) model which explicitly incorporates these symmetries, defining the data likelihood via a sequence of Fourier space-based convolutions and pixel-wise nonlinear transforms. TRENF gives direct access to the high dimensional data likelihood p(x|y) as a function of the labels y, such as cosmological parameters. In contrast to traditional analyses based on summary statistics, the NF approach has no loss of information since it preserves the full dimensionality of the data. On Gaussian random fields, the TRENF likelihood agrees well with the analytical expression and saturates the Fisher information content in the labels y. On nonlinear cosmological overdensity fields from N-body simulations, TRENF leads to significant improvements in constraining power over the standard power spectrum summary statistic. TRENF is also a generative model of the data, and we show that TRENF samples agree well with the N-body simulations it trained on, and that the inverse mapping of the data agrees well with a Gaussian white noise both visually and on various summary statistics: when this is perfectly achieved the resulting p(x|y) likelihood analysis becomes optimal. Finally, we develop a generalization of this model that can handle effects that break the symmetry of the data, such as the survey mask, which enables likelihood analysis on data without periodic boundaries.","layer":4,"vector":[-0.0258,-0.0336,-0.0163,-0.0207,0.0584,0.0342,0.0007,0.0223,0.0313,-0.0217,-0.0111,-0.069,0.0156,0.0481,0.0308,-0.0055,0.0034,0.053,-0.0675,0.0108,0.0501,-0.0458,-0.0078,-0.0329,0.0512,0.0321,-0.0616,-0.0291,-0.0465,-0.261,0.0024,-0.0395,0.0766,-0.0299,0.0313,-0.018,-0.0605,0.04,-0.0512,0.0828,0.026,0.0465,-0.0339,-0.0633,-0.0233,-0.0403,-0.0687,0.006,-0.0303,-0.017,0.017,-0.0536,0.0174,0.0584,0.0433,0.0671,0.0558,-0.0179,0.0841,0.0471,-0.0042,0.0243,-0.165,0.0834,0.071,-0.0027,-0.027,0.0008,0.035,-0.0031,-0.038,0.0461,-0.0147,0.0536,0.0319,-0.0277,0.0126,-0.0451,-0.024,-0.0031,0.0385,-0.0001,-0.0091,-0.011,-0.0003,-0.0211,0.0168,-0.0326,-0.0078,0.0365,-0.0531,0.0028,-0.0558,0.0181,-0.0672,-0.0028,0.007,0.0058,0.0174,0.1787,-0.0326,0.0481,0.0536,0.0331,0.0674,-0.0421,-0.0442,0.0092,0.0442,-0.0033,-0.016,-0.0525,0.0308,-0.0612,0.0114,-0.0055,0.0586,0.0355,-0.058,0.012,-0.0519,0.048,0.0215,-0.0312,0.0434,-0.0644,0.0842,0.0829,0.0676,0.0216,0.0714,0.0225,-0.0965,-0.0107,0.0089,0.0227,0.0298,0.0285,-0.0117,0.0045,-0.0476,-0.0327,-0.0464,-0.0944,-0.0212,0.0939,-0.0667,-0.0045,-0.0595,-0.0153,0.0119,0.0234,-0.0404,-0.0024,0.0204,0.0581,0.0438,0.0265,-0.0689,-0.0175,-0.0031,-0.0421,-0.0161,0.1055,0.0162,-0.054,-0.0145,0.0131,0.0037,0.0267,0.0266,-0.0142,-0.056,0.0247,0.106,0.0155,-0.057,-0.0003,0.0208,0.0157,-0.0166,-0.0657,-0.0037,0.0246,0.0704,-0.0441,-0.0127,-0.0012,0.0182,0.0289,0.0157,0.0071,-0.0201,0.0326,-0.0206,-0.0125,-0.005,0.0109,-0.0299,-0.0555,0.0104,0.0356,-0.0377,-0.0144,-0.0394,0.0034,-0.012,0.033,0.0228,0.014,-0.0261,-0.0492,0.0483,-0.0207,-0.0281,0.0153,0.0129,0.0029,-0.0197,0.042,0.0184,-0.0514,-0.0557,-0.2557,-0.0091,0.0113,-0.0136,0.0999,-0.078,0.041,-0.0178,0.0679,0.0781,0.0575,0.024,-0.0247,0.009,-0.0102,0.0544,-0.0035,0.0356,-0.0461,-0.0238,0.0019,0.0606,-0.0688,-0.1174,0.0306,0.0116,0.1972,0.0524,0.0572,-0.0304,0.0429,0.0049,0.0071,-0.0391,0.0493,0.0456,0.0596,0.0064,-0.0484,-0.0371,-0.0172,-0.0199,0.0305,-0.0876,-0.0173,-0.0419,-0.0294,0.0329,-0.0278,0.0285,0.0324,-0.0409,0.0606,-0.0238,0.0065,-0.0438,-0.1091,0.0047,-0.056,0.038,0.005,-0.0202,0.0569,-0.0753,0.0248,-0.0032,-0.0522,-0.0486,0.0395,-0.0472,-0.0325,0.0602,-0.0435,0.0385,0.072,0.0595,0.0637,-0.0032,-0.0429,-0.0804,0.0658,-0.0224,0.0473,0.0329,0.0433,0.0293,0.0594,-0.0608,0.0063,-0.0414,0.0058,0.0051,-0.0252,0.0129,-0.0042,0.0275,-0.2732,0.0126,-0.0402,0.0175,0.003,0.0072,0.0177,0.029,-0.0374,-0.0154,-0.0193,0.0572,0.0383,-0.0045,0.009,0.0403,0.06,-0.0637,0.0473,-0.0745,-0.0215,0.0054,0.2125,-0.0016,-0.0155,0.0143,-0.0124,0.0399,0.0437,-0.0091,0.02,0.0185,0.0812,-0.0158,0.0912,0.0457,-0.029,0.0129,0.0156,-0.0455,-0.0112,0.0471,0.0084,-0.0546,0.0829,-0.0415,-0.0529,-0.0136,-0.002,0.0243,-0.0465,0.0419,0.0106,0.0157,0.025,0.0112,-0.0572,-0.0462,-0.0096,-0.0474,0.0049,-0.0914,-0.0178,-0.0087,-0.0569]}
{"key":"[Predicting Attributes of Nodes Using Network Structure] In many graphs such as social networks, nodes have associated attributes representing their behavior. Predicting node attributes in such graphs is an important problem with applications in many domains like recommendation systems, privacy preservation, and targeted advertisement. Attributes values can be predicted by analyzing patterns and correlations among attributes and employing classification/regression algorithms. However, these approaches do not utilize readily available network topology information. In this regard, interconnections between different attributes of nodes can be exploited to improve the prediction accuracy. In this paper, we propose an approach to represent a node by a feature map with respect to an attribute $a_i$ (which is used as input for machine learning algorithms) using all attributes of neighbors to predict attributes values for $a_i$. We perform extensive experimentation on ten real-world datasets and show that the proposed feature map significantly improves the prediction accuracy as compared to baseline approaches on these datasets.","layer":0,"vector":[-0.0195,-0.0226,-0.0053,-0.0036,0.0458,0.042,0.0784,0.0485,0.0023,-0.0424,0.0124,-0.0407,0.045,0.0144,0.0222,0.0158,-0.0098,0.0651,-0.0396,-0.0039,0.016,-0.0203,-0.0122,-0.0554,0.055,0.0323,-0.0169,-0.041,-0.0447,-0.2084,0.0298,-0.0698,0.0738,-0.0299,0.0074,-0.0701,0.0275,0.0352,-0.0472,0.0691,0.0038,0.006,-0.0404,-0.0261,-0.0371,-0.0044,0.0345,-0.0505,-0.0358,-0.0525,0.0334,-0.0489,0.005,0.0587,0.0404,0.0503,0.0581,0.0244,0.0097,0.0499,0.0367,0.0498,-0.1257,0.0437,0.0413,0.0613,-0.063,0.0267,-0.0121,0.0674,0.0427,0.0328,0.0252,0.0292,0.0293,0.0498,-0.0004,0.0219,-0.0407,0.0278,-0.0143,-0.0086,-0.0264,0.0033,-0.0002,-0.0286,0.0519,-0.0484,0.0121,-0.0148,-0.0379,-0.0165,-0.0209,0.027,-0.0606,-0.0275,0.0339,-0.0101,-0.0427,0.2081,-0.0631,0.0582,0.0382,-0.0474,0.012,-0.0509,0.0144,-0.0463,-0.051,0.0338,-0.0153,-0.0291,-0.0311,-0.0546,0.044,-0.0172,0.078,0.0998,0.0016,-0.0324,-0.0291,-0.0007,0.0729,-0.0217,0.031,-0.048,-0.0154,0.1022,0.0293,0.0502,-0.0392,0.0153,-0.0327,-0.0096,0.0043,0.0169,0.0301,-0.012,-0.0044,-0.018,-0.0124,-0.048,0.0148,-0.0626,-0.0736,0.1497,-0.0455,0.0168,0.0015,-0.0038,-0.0157,0.0271,-0.0422,0.0019,-0.0124,0.0228,0.0221,0.0415,-0.0656,0.0137,-0.0195,-0.0535,-0.0741,0.0826,0.0211,-0.1365,0.0067,0.0056,-0.0159,-0.0383,0.0256,0.0318,-0.0264,0.0311,0.0725,0.048,-0.0692,0.0073,-0.0094,-0.0234,0.029,-0.0181,-0.051,0.0304,-0.0011,0.0216,-0.0119,-0.0293,0.0105,0.0594,-0.0255,0.0039,-0.0171,-0.0008,-0.0526,-0.0217,-0.0576,0.0088,0.0217,-0.045,0.0246,0.0028,-0.0551,-0.0168,-0.0444,0.0262,-0.0013,0.015,0.0222,-0.0215,-0.0358,-0.0305,0.0266,-0.0315,-0.0447,0.013,0.0026,0.032,0.0384,0.09,0.0312,-0.0365,-0.0798,-0.2346,-0.0172,0.0093,0.0178,0.0453,-0.0706,0.0298,-0.0242,0.0647,0.076,0.0542,-0.0201,-0.022,0.0343,-0.0084,0.0501,0.0464,0.0136,-0.0052,0.0199,-0.0163,-0.0066,-0.0062,-0.0598,0.0243,0.027,0.195,0.013,0.0176,-0.0299,-0.0179,0.0357,-0.0557,-0.0923,0.0616,0.0594,0.0405,-0.0103,-0.0334,-0.0186,-0.0596,0.0401,0.0045,-0.0923,-0.0129,-0.0091,-0.0236,0.0554,-0.065,0.026,0.057,0.0305,0.0869,0.0027,0.0088,-0.0441,-0.049,0.0105,-0.0087,0.0442,0.0124,-0.0858,-0.0262,-0.0793,0.0602,-0.0068,-0.0305,-0.0281,-0.0152,-0.0136,-0.0393,0.076,0.0076,-0.0435,0.058,-0.0458,-0.022,-0.0031,-0.0479,0.0065,0.0666,-0.0733,0.0586,0.022,0.0229,0.0011,0.0612,0.0061,0.0623,-0.0149,-0.0093,-0.0266,-0.0157,-0.0581,0.0526,-0.0289,-0.3184,0.0265,-0.0476,0.0588,-0.0262,-0.0077,0.0708,0.0474,-0.0245,-0.0101,0.0787,0.0199,0.0498,-0.04,-0.0272,0.0233,0.0666,-0.0521,0.0433,-0.0211,0.0787,0.0145,0.2518,-0.0254,0.0689,0.0157,-0.0602,-0.0045,-0.0056,0.0166,0.0156,-0.0045,0.0827,-0.0385,0.0213,0.0328,-0.0113,-0.021,0.012,-0.0144,0.0068,-0.0197,-0.0399,-0.0088,0.1026,-0.0105,-0.0576,-0.0568,0.0244,0.0445,-0.055,-0.0063,-0.0416,0.0045,0.0187,0.0506,-0.0411,0.0022,-0.0558,-0.0465,-0.0001,-0.0548,0.0179,-0.0081,-0.0061]}
{"key":"[Developing Constrained Neural Units Over Time] In this paper we present a foundational study on a constrained method that defines learning problems with Neural Networks in the context of the principle of least cognitive action, which very much resembles the principle of least action in mechanics. Starting from a general approach to enforce constraints into the dynamical laws of learning, this work focuses on an alternative way of defining Neural Networks, that is different from the majority of existing approaches. In particular, the structure of the neural architecture is defined by means of a special class of constraints that are extended also to the interaction with data, leading to \"architectural\" and \"input-related\" constraints, respectively. The proposed theory is cast into the time domain, in which data are presented to the network in an ordered manner, that makes this study an important step toward alternative ways of processing continuous streams of data with Neural Networks. The connection with the classic Backpropagation-based update rule of the weights of networks is discussed, showing that there are conditions under which our approach degenerates to Backpropagation. Moreover, the theory is experimentally evaluated on a simple problem that allows us to deeply study several aspects of the theory itself and to show the soundness of the model.","layer":0,"vector":[-0.052,0.0036,0.0479,-0.0282,-0.0295,0.0407,0.0261,-0.0148,0.0563,-0.0342,0.0182,-0.0492,0.014,0.0772,-0.0093,-0.0104,-0.0255,0.0755,-0.0235,-0.0044,-0.0004,-0.0239,0.0,-0.0454,0.0062,0.0082,-0.0114,-0.0224,-0.0157,-0.2429,0.0399,-0.0382,0.0509,-0.0458,-0.0071,0.019,-0.043,0.0354,0.0033,0.0568,0.0226,0.0632,0.0364,-0.0899,-0.0059,-0.0028,-0.0133,-0.0069,-0.04,-0.0051,-0.003,0.009,0.0066,-0.0083,0.0149,0.0354,0.0744,0.021,0.07,0.0382,0.0394,0.0189,-0.1689,0.0351,0.0447,0.016,-0.0467,0.0059,0.023,0.0695,0.0032,0.0518,0.0393,0.0225,0.017,0.0266,-0.0214,-0.0306,0.0271,-0.0065,0.0261,-0.0192,-0.0282,0.0103,-0.0044,-0.0561,-0.0227,-0.0581,0.0245,0.0149,-0.0636,-0.0392,-0.0043,-0.0001,-0.039,-0.0587,0.0358,0.0513,-0.071,0.1829,-0.0297,0.0559,0.0734,-0.0314,0.0535,-0.0288,-0.0613,-0.0386,-0.0441,-0.005,-0.0274,0.0114,0.0196,-0.0088,0.0194,0.034,0.004,0.0113,0.003,-0.0334,-0.0317,0.0138,0.0327,-0.0104,0.0322,-0.0791,0.0372,0.1433,0.021,0.0638,0.0414,-0.0397,-0.0612,-0.0443,0.0364,0.0562,0.0332,-0.0335,0.0015,-0.0083,-0.0675,-0.0575,0.0404,-0.0873,-0.0722,0.1403,-0.0092,-0.008,-0.0054,-0.0015,-0.0531,0.0037,-0.0182,-0.0321,0.0037,0.0368,0.0052,0.011,-0.051,0.0153,-0.0212,-0.0438,-0.0175,0.1291,0.0295,-0.06,-0.0067,-0.0215,0.0234,-0.0182,0.0497,0.0585,-0.0218,0.0148,0.0778,0.0367,-0.0557,-0.0391,-0.0272,0.0459,0.0261,-0.0542,0.0176,0.0233,0.0365,0.0006,0.0246,-0.013,0.0332,0.0549,-0.0472,0.0284,0.0019,0.0173,-0.0468,-0.0393,0.0122,0.013,-0.0243,-0.0409,-0.0174,0.0038,-0.0301,0.0392,-0.0107,0.0003,-0.0243,0.0025,0.0243,0.0273,-0.0409,-0.004,0.0876,-0.1062,-0.0212,-0.002,0.0032,0.0049,-0.0102,0.0218,0.0542,-0.0137,-0.0154,-0.2528,0.0197,-0.0143,0.0004,0.0953,-0.0517,0.0361,-0.0071,-0.0002,0.0369,0.0586,-0.0109,-0.0477,-0.04,-0.0317,0.0108,0.068,-0.0042,-0.0551,-0.0032,0.0082,-0.0039,0.004,-0.0914,0.0445,0.0236,0.1987,0.0215,0.0911,-0.0304,0.014,0.0062,-0.0695,-0.0676,0.0901,-0.0037,0.0737,0.0007,-0.0218,-0.059,-0.0304,0.0121,0.0243,-0.0547,-0.0468,-0.0073,0.0161,-0.0117,-0.0746,-0.0046,0.0242,-0.0129,0.0208,-0.0068,-0.0298,-0.0237,-0.0667,0.0449,-0.018,0.0333,0.0076,-0.0562,-0.0015,-0.0466,0.0486,0.0296,-0.0413,-0.0538,0.0377,-0.0073,-0.0227,0.1226,-0.004,0.009,0.0618,0.005,0.0018,-0.0481,-0.0219,-0.0231,0.0347,-0.0524,0.0374,0.0164,0.0391,-0.0152,0.0623,-0.0681,0.0313,-0.0046,0.0093,0.0422,-0.0535,0.0543,0.0617,-0.0274,-0.2693,0.0339,-0.0008,0.0462,-0.0,0.02,0.0256,0.0158,-0.0685,-0.0315,-0.0156,0.0327,0.0206,0.0274,0.0246,0.0195,0.057,-0.0754,0.0585,-0.0788,0.0116,0.0966,0.2467,-0.0476,0.0423,0.0029,-0.0375,-0.0156,0.0168,-0.0203,0.0164,0.0093,0.1011,-0.0648,0.017,0.0707,-0.059,0.041,0.0301,-0.0179,-0.0033,0.0249,-0.0551,0.0061,0.0936,0.0027,-0.03,-0.0635,-0.0609,0.0491,-0.0264,0.0093,0.0118,0.0061,0.0034,-0.0145,-0.0513,-0.0528,-0.0364,-0.0743,0.0295,-0.0819,0.028,-0.0201,0.0013]}
{"key":"[Graph Multiview Canonical Correlation Analysis] Multiview canonical correlation analysis (MCCA) seeks latent low-dimensional representations encountered with multiview data of shared entities (a.k.a. common sources). However, existing MCCA approaches do not exploit the geometry of the common sources, which may be available \\emph{a priori}, or can be constructed using certain domain knowledge. This prior information about the common sources can be encoded by a graph, and be invoked as a regularizer to enrich the maximum variance MCCA framework. In this context, the present paper's novel graph-regularized (G) MCCA approach minimizes the distance between the wanted canonical variables and the common low-dimensional representations, while accounting for graph-induced knowledge of the common sources. Relying on a function capturing the extent low-dimensional representations of the multiple views are similar, a generalization bound of GMCCA is established based on Rademacher's complexity. Tailored for setups where the number of data pairs is smaller than the data vector dimensions, a graph-regularized dual MCCA approach is also developed. To further deal with nonlinearities present in the data, graph-regularized kernel MCCA variants are put forward too. Interestingly, solutions of the graph-regularized linear, dual, and kernel MCCA, are all provided in terms of generalized eigenvalue decomposition. Several corroborating numerical tests using real datasets are provided to showcase the merits of the graph-regularized MCCA variants relative to several competing alternatives including MCCA, Laplacian-regularized MCCA, and (graph-regularized) PCA.","layer":2,"vector":[-0.0116,-0.0257,0.0171,-0.0069,0.0385,0.0301,0.023,0.0209,0.0153,-0.0287,0.0101,-0.0774,0.0409,0.0745,0.0517,0.0305,0.0441,0.0418,-0.0507,-0.0208,0.0044,-0.0356,0.0209,-0.037,0.044,0.01,-0.0152,-0.0595,-0.0428,-0.2732,0.0196,-0.0177,0.1037,0.0119,0.0433,-0.0441,-0.0173,0.0665,-0.0138,0.056,-0.014,-0.0106,0.0012,-0.0243,-0.0378,-0.0678,-0.0152,0.0271,-0.0121,-0.0264,0.0009,-0.0158,0.0175,0.0074,0.0094,0.0592,0.035,-0.0119,0.0539,0.0697,0.0599,0.0686,-0.1763,0.0747,0.0524,0.0092,-0.0238,0.0026,0.031,0.0219,0.0008,-0.0098,0.0144,0.0169,0.0144,-0.015,0.0144,0.008,-0.0782,0.0261,0.0286,0.0348,-0.0297,-0.0185,0.0128,0.0021,-0.0391,-0.0336,0.0304,0.0193,-0.0481,-0.0695,-0.0013,-0.0033,-0.0846,0.0036,0.0679,0.0174,-0.0488,0.1736,-0.0722,0.0407,0.0599,-0.0199,0.0734,-0.0191,0.0037,-0.0204,0.0355,0.0212,0.0506,-0.0091,-0.0089,-0.0774,0.0223,-0.0189,0.0591,0.0623,-0.0061,-0.0165,-0.0183,0.0267,-0.0016,-0.0022,0.0355,-0.0942,0.0187,0.0971,0.0451,-0.0026,0.0538,0.0494,-0.042,-0.0051,0.0052,0.0094,0.0487,0.0379,0.0351,0.0362,-0.0057,-0.0165,-0.0062,-0.0354,-0.06,0.1661,-0.0539,-0.0129,-0.0433,0.015,-0.0079,0.0368,-0.0392,0.0255,0.0132,0.0383,0.0084,0.0172,-0.0425,0.0468,-0.0341,-0.0169,-0.0475,0.0997,0.0331,-0.1237,-0.0129,-0.0038,0.0285,-0.0191,0.0557,0.0467,-0.0294,0.0144,0.0893,0.0084,-0.0739,0.0498,0.0264,0.0003,0.0355,-0.064,-0.0424,0.0858,0.0302,-0.0312,-0.0081,0.0014,-0.0058,-0.015,-0.0192,-0.0268,-0.054,0.0465,-0.0433,0.0101,-0.0344,-0.0388,0.0035,-0.0305,0.0396,-0.0352,-0.0725,0.027,-0.0372,0.0099,0.0048,-0.0116,0.0119,-0.0027,-0.0444,-0.0185,0.0319,-0.0232,-0.0122,0.0076,0.0228,0.0488,0.0202,0.0167,0.0534,-0.0679,-0.0743,-0.234,-0.0259,-0.0551,0.0158,0.0451,-0.056,0.041,0.0123,0.0813,0.0884,0.025,0.0019,-0.0518,0.026,-0.008,0.0531,0.0233,0.0484,-0.0486,0.0011,-0.0566,0.03,0.011,-0.0697,0.0546,0.0101,0.2017,0.0137,-0.0153,-0.0425,0.0202,0.0596,-0.0495,-0.0564,0.0451,0.0238,0.049,-0.0022,-0.0441,-0.0424,-0.0234,0.0017,0.0123,-0.091,-0.0352,-0.0354,-0.0088,0.0149,-0.0258,-0.007,0.029,-0.0096,0.0264,-0.0126,0.0201,-0.0176,-0.0897,0.0062,-0.033,0.0221,0.0222,-0.0649,0.0262,-0.0963,0.0574,-0.0415,-0.0325,-0.0097,-0.0033,-0.0123,-0.0554,0.0711,0.0092,-0.0277,0.0473,0.0218,0.0246,-0.0151,-0.0288,0.0229,0.0811,-0.0327,-0.0124,-0.0224,0.0297,0.0064,0.055,0.0028,0.0143,-0.0763,0.0105,-0.0306,-0.0452,-0.0506,0.0656,-0.0172,-0.3161,0.0005,0.0008,0.0235,-0.0196,-0.0167,0.0327,0.0319,-0.06,-0.0017,0.01,0.032,0.0419,-0.0434,0.0089,0.0429,0.0398,-0.0523,0.0463,-0.0204,0.0067,0.0208,0.2084,-0.033,0.0357,-0.0028,-0.0214,-0.0129,-0.0125,-0.0092,0.0015,0.0241,0.0906,-0.0663,0.0356,0.0857,-0.0545,0.031,0.0251,-0.0313,0.0252,0.0076,-0.0277,-0.0798,0.1001,0.0045,0.0057,-0.0589,0.0265,0.0004,-0.0212,-0.0044,-0.0249,0.0094,0.0155,0.0159,-0.0136,-0.0178,-0.0643,-0.0639,-0.0495,-0.0112,-0.0706,0.0008,-0.0043]}
{"key":"[Efficient Algorithms for Estimating the Parameters of Mixed Linear Regression Models] Mixed linear regression (MLR) model is among the most exemplary statistical tools for modeling non-linear distributions using a mixture of linear models. When the additive noise in MLR model is Gaussian, Expectation-Maximization (EM) algorithm is a widely-used algorithm for maximum likelihood estimation of MLR parameters. However, when noise is non-Gaussian, the steps of EM algorithm may not have closed-form update rules, which makes EM algorithm impractical. In this work, we study the maximum likelihood estimation of the parameters of MLR model when the additive noise has non-Gaussian distribution. In particular, we consider the case that noise has Laplacian distribution and we first show that unlike the the Gaussian case, the resulting sub-problems of EM algorithm in this case does not have closed-form update rule, thus preventing us from using EM in this case. To overcome this issue, we propose a new algorithm based on combining the alternating direction method of multipliers (ADMM) with EM algorithm idea. Our numerical experiments show that our method outperforms the EM algorithm in statistical accuracy and computational time in non-Gaussian noise case.","layer":2,"vector":[-0.0108,-0.0139,0.069,0.0243,0.0034,-0.0132,0.0098,0.0182,0.0375,0.0016,0.0783,-0.0455,-0.0173,0.0064,0.0366,0.0379,0.0462,-0.0007,-0.0597,0.0099,0.0045,0.0028,-0.0345,-0.0344,0.077,0.0215,-0.0703,-0.0477,-0.0616,-0.2789,0.0187,-0.0288,0.0787,0.0213,-0.0222,-0.0453,-0.0172,0.0471,-0.0102,0.0421,-0.0209,0.0004,-0.0014,-0.0539,-0.0159,-0.0543,-0.0367,-0.006,0.0173,-0.013,0.039,-0.0479,0.0168,0.0217,0.0562,0.0419,0.0332,0.0115,0.0211,0.0588,0.0406,0.0814,-0.1936,0.0504,0.0267,-0.003,-0.0638,-0.0488,0.0086,0.0643,-0.0602,0.0537,0.013,0.0362,0.0234,-0.0015,0.0438,-0.0125,-0.0177,0.0069,0.0247,-0.0108,-0.0116,0.0141,-0.0122,-0.0286,0.0456,-0.0411,0.0397,-0.0468,-0.0367,-0.0072,-0.0471,0.0316,-0.0364,-0.0129,0.0073,0.0389,-0.0273,0.2167,-0.0514,0.025,0.0548,-0.0329,0.015,-0.0639,-0.0087,-0.0296,-0.0163,-0.0063,0.0196,-0.0285,0.0441,-0.0346,0.0016,0.0065,0.0425,0.0084,-0.0028,-0.0134,-0.0322,-0.0222,0.0181,0.0179,0.017,-0.0644,0.0096,0.1457,0.0421,0.0424,0.045,-0.042,-0.0471,-0.0343,-0.01,-0.0116,0.0109,0.038,0.0626,0.0007,-0.0323,-0.1147,0.0108,-0.1102,-0.0416,0.128,-0.014,0.0254,-0.0429,-0.0275,-0.0299,0.035,-0.0142,0.0068,0.0123,-0.0149,0.0042,0.0398,-0.0264,0.0076,-0.0365,-0.0457,-0.0167,0.0912,-0.0005,-0.066,-0.0177,0.0166,0.006,0.0102,0.079,0.0428,-0.0125,-0.001,0.0667,0.0366,-0.0408,0.017,-0.0066,0.0152,0.0196,-0.0481,-0.0307,0.0668,0.0252,-0.0723,0.0074,-0.0526,0.0185,0.0204,-0.0364,0.0216,0.0084,0.0028,-0.0285,-0.022,-0.0193,-0.0046,0.0303,-0.0326,0.0212,0.0232,-0.0424,-0.0236,-0.0114,0.0409,0.0177,-0.0255,0.0448,0.0184,0.0001,-0.0053,0.1017,0.0076,-0.0517,0.0324,0.0373,0.0225,0.0212,0.0796,0.0621,-0.0299,-0.1003,-0.2421,-0.0308,0.0139,0.0129,0.0578,-0.0647,0.0411,-0.0071,0.0973,0.1112,0.0518,-0.0004,-0.0128,0.0397,-0.0062,0.0844,0.0163,0.0212,-0.0361,0.0288,-0.0306,0.0037,-0.0443,-0.0166,0.0676,-0.0299,0.1212,-0.005,-0.0068,-0.0172,-0.0056,0.0092,0.0016,-0.0497,0.0489,0.0364,0.0643,-0.0426,-0.0339,-0.0045,-0.0449,0.0391,-0.0158,-0.0662,-0.0479,-0.0581,-0.0582,0.0145,-0.0979,0.0229,0.0241,-0.0135,0.0887,-0.0392,-0.001,-0.0325,-0.0865,0.0362,0.0424,0.0046,0.0086,-0.0546,0.0419,-0.0622,0.0403,-0.0176,-0.04,-0.0652,-0.007,-0.0128,-0.02,0.1071,0.0333,0.0191,0.0677,0.0211,0.0037,-0.057,-0.0483,-0.026,0.0401,-0.0429,0.0192,0.0292,0.0226,-0.0234,0.0447,-0.005,-0.0174,-0.0237,-0.0657,-0.065,-0.0378,0.012,0.0361,-0.0107,-0.289,0.02,0.0257,0.0005,-0.0462,0.0057,0.02,0.0138,-0.0559,0.0079,-0.0262,0.0488,0.0371,-0.0545,0.0553,-0.0139,0.0615,-0.0379,0.0015,-0.0463,-0.0311,0.0532,0.2042,-0.0271,0.0241,0.0353,-0.0217,0.0082,0.0129,-0.0769,0.0049,0.0504,0.1037,-0.0364,0.0814,0.0813,-0.0486,0.0758,0.0202,-0.0244,0.0169,-0.0154,-0.0411,0.0086,0.0782,-0.0113,-0.0093,-0.0093,0.0044,0.0469,-0.0535,-0.0183,-0.0007,0.0224,0.0036,0.0272,-0.0609,-0.0195,-0.0083,-0.018,0.0417,-0.0491,-0.0739,-0.0405,-0.0076]}
{"key":"[Test-time Collective Prediction] An increasingly common setting in machine learning involves multiple parties, each with their own data, who want to jointly make predictions on future test points. Agents wish to benefit from the collective expertise of the full set of agents to make better predictions than they would individually, but may not be willing to release their data or model parameters. In this work, we explore a decentralized mechanism to make collective predictions at test time, leveraging each agent's pre-trained model without relying on external validation, model retraining, or data pooling. Our approach takes inspiration from the literature in social science on human consensus-making. We analyze our mechanism theoretically, showing that it converges to inverse meansquared-error (MSE) weighting in the large-sample limit. To compute error bars on the collective predictions we propose a decentralized Jackknife procedure that evaluates the sensitivity of our mechanism to a single agent's prediction. Empirically, we demonstrate that our scheme effectively combines models with differing quality across the input space. The proposed consensus prediction achieves significant gains over classical model averaging, and even outperforms weighted averaging schemes that have access to additional validation data.","layer":1,"vector":[-0.0517,-0.0361,0.0127,0.008,0.0474,0.0134,0.0094,0.0409,0.0148,-0.006,0.0285,-0.045,0.0464,0.024,0.0337,-0.0083,-0.0238,0.0124,-0.0083,-0.0046,-0.0036,-0.0124,-0.0645,-0.0375,0.0219,0.0261,-0.0321,-0.0822,-0.062,-0.2535,0.0654,-0.0649,0.0448,-0.0049,0.0043,-0.0271,-0.0216,0.0219,-0.0169,0.0348,0.019,0.0242,0.008,-0.062,-0.0012,-0.0167,-0.0153,-0.007,-0.049,0.0204,0.0668,-0.0566,0.0077,0.0093,0.0706,0.0408,0.0343,0.0212,0.0652,0.0152,0.0368,0.0371,-0.1515,0.0408,0.0623,0.0502,-0.017,0.0039,-0.0075,0.017,0.0636,0.0639,0.0799,0.0468,0.0329,0.0159,0.0039,-0.0253,-0.0213,-0.0011,-0.024,-0.0181,-0.0263,-0.027,-0.027,-0.0232,0.0628,-0.0078,0.0455,0.0167,-0.0378,0.0059,-0.0104,0.0138,-0.072,0.0014,0.0082,0.0358,-0.0413,0.2195,-0.0519,0.0839,0.033,-0.06,0.0152,-0.0768,0.004,-0.0593,0.0001,-0.013,-0.0128,-0.014,0.0399,-0.0644,0.011,0.0121,0.0758,0.0354,-0.0142,-0.0247,-0.0006,0.0225,0.0634,-0.0585,0.0464,-0.0549,0.0046,0.1755,0.0025,-0.0026,0.0103,-0.0145,-0.0565,-0.0518,-0.0129,0.0153,0.0115,-0.0001,0.0427,-0.0026,-0.0185,-0.062,-0.0206,-0.0459,-0.0729,0.1257,0.0015,0.0366,-0.0201,-0.019,-0.0252,0.0025,-0.0123,-0.0069,0.0245,0.0207,0.023,0.0295,-0.0588,0.0084,-0.025,-0.0206,-0.0474,0.0733,-0.0013,-0.0653,-0.0465,0.0167,0.0371,-0.0225,0.0205,-0.0238,-0.0127,0.0436,0.0414,0.0229,-0.0528,0.0223,0.0149,-0.0214,0.0236,-0.0222,-0.0554,0.0572,0.0484,0.0004,-0.0275,-0.0412,0.032,0.0454,-0.0177,0.028,-0.0564,-0.0025,-0.0393,-0.0353,0.0022,-0.0366,0.0275,-0.0412,-0.0297,0.0331,-0.0087,0.0137,-0.0372,0.0402,0.0099,0.0101,0.0506,0.0103,0.0012,-0.0299,0.076,-0.0088,-0.0503,0.022,0.0209,0.0433,-0.0082,0.0559,0.0518,0.0032,-0.0406,-0.1972,-0.0238,-0.0026,0.011,0.0739,-0.0399,0.0404,-0.0329,0.0063,0.0702,0.0798,0.0075,-0.0173,0.0293,-0.0034,0.0551,-0.0131,0.029,-0.0042,0.0219,-0.0223,0.0047,-0.0353,-0.0824,0.0686,-0.009,0.2048,0.0288,0.0274,-0.0112,0.0445,0.0749,-0.0564,-0.1259,0.0444,0.0567,0.0459,-0.001,-0.0322,-0.0218,-0.0137,0.0155,0.0413,-0.1414,-0.088,-0.0228,-0.0296,0.0458,-0.0892,-0.0452,0.0596,-0.0585,0.0449,-0.0194,-0.0259,-0.0436,-0.0921,0.0273,-0.0269,0.0328,0.0448,-0.0107,-0.0094,-0.0376,0.057,-0.0451,-0.0262,-0.0523,0.0384,-0.0134,-0.0504,0.0639,-0.0331,0.0051,0.0272,0.0101,0.0145,-0.0525,-0.0179,0.0232,0.0632,-0.0603,0.0322,0.0487,0.0379,0.0162,0.0321,0.0206,0.0748,-0.0211,-0.01,-0.0056,-0.0567,0.0255,0.0065,-0.015,-0.2869,0.0084,-0.0193,0.0543,-0.0377,0.0069,0.0647,-0.0067,-0.0639,0.0286,0.028,0.11,0.0338,0.0051,0.0316,0.0581,0.0756,-0.0633,0.0101,-0.1004,0.0181,0.0198,0.241,-0.048,-0.0133,0.0696,-0.011,-0.0702,0.023,-0.0034,0.0159,-0.0125,0.0419,-0.0788,0.0286,0.0536,-0.0567,-0.0106,0.0398,-0.017,-0.0179,-0.0164,0.022,-0.0385,0.0815,0.0154,-0.0304,-0.0822,-0.0308,0.0279,-0.0289,0.0094,-0.0164,0.003,-0.0061,0.0056,-0.0262,-0.0424,-0.038,-0.025,-0.003,-0.0556,0.0088,-0.008,-0.0326]}
{"key":"[On Generalizing Beyond Domains in Cross-Domain Continual Learning] Humans have the ability to accumulate knowledge of new tasks in varying conditions, but deep neural networks often suffer from catastrophic forgetting of previously learned knowledge after learning a new task. Many recent methods focus on preventing catastrophic forgetting under the assumption of train and test data following similar distributions. In this work, we consider a more realistic scenario of continual learning under domain shifts where the model must generalize its inference to an unseen domain. To this end, we encourage learning semantically meaningful features by equipping the classifier with class similarity metrics as learning parameters which are obtained through Mahalanobis similarity computations. Learning of the backbone representation along with these extra parameters is done seamlessly in an end-to-end manner. In addition, we propose an approach based on the exponential moving average of the parameters for better knowledge distillation. We demonstrate that, to a great extent, existing continual learning algorithms fail to handle the forgetting issue under multiple distributions, while our proposed approach learns new tasks under domain shift with accuracy boosts up to 10% on challenging datasets such as DomainNet and OfficeHome.","layer":0,"vector":[-0.0273,-0.0055,0.0471,-0.0219,0.0209,0.0171,0.0128,0.0252,0.0283,-0.0192,0.0021,-0.016,0.0685,0.0955,0.0186,0.0615,0.0037,0.0574,-0.0418,0.0182,0.0508,0.0092,0.0017,0.015,-0.0238,0.0209,-0.0279,-0.0234,-0.015,-0.2358,0.0038,-0.0453,0.0058,-0.015,-0.007,-0.0297,-0.0579,0.0586,-0.0079,0.0537,0.0171,0.0035,-0.0176,-0.0442,-0.0263,-0.0542,-0.0027,-0.0078,0.0166,-0.0343,0.0125,-0.0694,0.0284,0.022,0.0366,0.0598,0.0549,0.0415,0.0641,0.0338,0.0521,0.0172,-0.1225,0.0423,0.036,0.0219,-0.0397,-0.0328,-0.0365,0.0675,-0.023,0.0671,-0.0071,0.1029,0.0044,-0.0132,-0.0393,-0.0168,-0.0597,0.0289,0.0634,0.0013,0.0052,-0.0422,-0.0333,-0.0421,0.0344,-0.061,0.0393,0.0147,-0.0839,-0.0311,-0.0252,0.0279,-0.0616,-0.0259,0.0419,0.0271,-0.0625,0.2191,-0.0244,-0.0098,0.025,-0.0442,0.0184,-0.0157,-0.0109,0.0031,-0.0265,0.0059,-0.0183,-0.0294,0.0111,-0.0398,0.0459,0.0386,0.0579,0.0095,-0.0388,0.0184,-0.0194,0.0286,0.0349,-0.049,0.0365,-0.0324,0.0235,0.1155,0.0183,0.0352,0.0376,-0.0297,-0.0701,-0.0142,0.0374,0.0819,0.0027,-0.0237,0.0101,-0.0111,-0.0155,-0.0161,0.0451,-0.0858,-0.0723,0.1254,-0.0335,0.0194,-0.0153,0.0072,-0.0152,0.0124,-0.0293,-0.0393,0.0211,0.0222,0.0695,0.0239,-0.0092,-0.019,-0.0224,-0.0572,-0.0157,0.1236,0.0499,-0.0719,-0.0242,-0.0163,-0.0174,-0.0252,0.0823,0.0555,-0.0411,0.0151,0.103,0.0217,-0.0569,-0.0318,0.0751,0.0134,0.0388,-0.0788,-0.0424,0.0596,0.0547,-0.0311,0.0124,-0.0312,0.0366,0.0339,-0.0462,0.0081,-0.0188,-0.0124,-0.0416,0.0085,-0.0301,-0.0326,0.0335,-0.0306,-0.0031,0.0035,-0.0196,-0.0199,0.006,0.0175,-0.0267,0.0145,0.0533,0.0194,-0.0197,-0.0105,0.0253,-0.0177,0.0002,0.0077,0.0038,0.0224,-0.0182,0.012,0.0761,0.0001,-0.007,-0.2192,-0.015,0.0029,-0.042,0.0799,-0.0719,0.0184,0.0261,0.0534,0.0327,0.0126,-0.0384,-0.0132,-0.006,-0.0492,0.0359,0.0608,0.0055,-0.0506,0.0279,-0.0396,0.0345,-0.0273,-0.149,0.0676,-0.0342,0.2237,0.0197,0.0704,-0.0499,-0.0325,0.0583,0.011,-0.1105,0.0243,0.0077,0.0767,-0.0134,-0.0132,-0.0376,-0.0365,0.0604,0.0182,-0.0843,-0.0368,-0.034,-0.0623,0.0046,-0.0321,0.0447,0.0348,-0.0325,0.0434,-0.0165,-0.0272,-0.0155,-0.0826,0.0097,-0.0559,-0.0011,0.009,-0.0271,-0.0095,-0.0419,0.0704,-0.0318,-0.0382,-0.0484,0.0666,-0.0115,-0.0376,0.0442,-0.0114,-0.0102,0.0154,-0.0458,0.04,-0.0492,-0.093,0.007,0.0541,-0.0435,0.0201,0.0146,0.0395,0.044,0.101,0.0189,0.0513,-0.0053,0.0206,-0.0071,-0.0668,-0.0218,0.0228,-0.0514,-0.2755,0.0313,-0.0286,0.052,-0.0048,0.0607,0.0304,0.024,-0.0252,-0.0102,-0.0177,0.0727,0.0529,0.0282,-0.0023,0.0395,0.0566,-0.0412,0.0064,-0.0636,0.0156,0.0237,0.2162,-0.0391,0.0511,-0.034,-0.0291,-0.0138,0.0193,-0.0181,-0.0408,0.0135,0.0858,-0.0731,0.0221,0.0888,-0.0371,0.07,0.0295,-0.0275,0.0123,-0.0295,-0.0773,-0.0014,0.0859,0.0188,0.0031,-0.0743,-0.0181,0.0148,-0.0133,-0.0187,0.0021,0.0177,0.0131,0.0046,-0.0538,-0.0542,-0.0578,-0.0822,-0.0207,-0.0424,-0.0024,-0.0107,-0.0203]}
{"key":"[Modeling Massive Spatial Datasets Using a Conjugate Bayesian Linear Regression Framework] Geographic Information Systems (GIS) and related technologies have generated substantial interest among statisticians with regard to scalable methodologies for analyzing large spatial datasets. A variety of scalable spatial process models have been proposed that can be easily embedded within a hierarchical modeling framework to carry out Bayesian inference. While the focus of statistical research has mostly been directed toward innovative and more complex model development, relatively limited attention has been accorded to approaches for easily implementable scalable hierarchical models for the practicing scientist or spatial analyst. This article discusses how point-referenced spatial process models can be cast as a conjugate Bayesian linear regression that can rapidly deliver inference on spatial processes. The approach allows exact sampling directly (avoids iterative algorithms such as Markov chain Monte Carlo) from the joint posterior distribution of regression parameters, the latent process and the predictive random variables, and can be easily implemented on statistical programming environments such as R.","layer":4,"vector":[-0.0226,-0.0437,0.0397,-0.0256,0.0295,0.0193,0.0413,0.0283,0.0269,0.005,0.0294,-0.063,0.0149,0.0361,0.0103,0.0203,-0.0034,0.0576,0.0052,0.0087,0.0384,-0.0148,-0.0235,-0.0478,0.0533,0.042,-0.0272,-0.051,-0.0786,-0.2399,-0.0259,-0.0613,0.0319,0.0089,-0.0242,-0.0168,-0.0133,0.0452,0.0052,0.0653,0.0373,0.0187,-0.0148,-0.0035,-0.0459,-0.0548,-0.0339,-0.0153,-0.0355,-0.0271,0.0127,-0.0348,0.0095,0.0558,0.0369,0.0532,0.0449,0.0221,0.0346,0.0198,0.0484,0.0333,-0.2263,0.0747,0.0276,0.0504,-0.0437,-0.0165,-0.0092,0.0138,-0.0546,0.0527,-0.0291,0.0846,0.0706,-0.0452,-0.0204,0.0476,-0.0203,-0.008,0.0126,0.0013,-0.0113,0.0102,-0.0262,-0.0686,0.0051,-0.0188,0.0909,-0.0004,0.0029,-0.0423,-0.0304,0.0117,-0.0782,-0.003,0.0692,0.0257,0.0043,0.1755,-0.0554,0.0145,0.0433,0.0196,0.0126,-0.0162,-0.0263,-0.049,0.0136,0.0354,-0.0082,-0.0435,-0.0056,-0.023,0.0192,-0.001,0.0755,0.0221,-0.0464,-0.0127,-0.0035,0.0427,0.0457,-0.0033,0.0364,-0.0795,0.0005,0.1406,0.0518,0.001,0.0497,-0.0228,-0.096,-0.0056,-0.0211,0.0106,0.0523,0.0048,-0.0112,-0.0026,-0.0447,-0.0142,0.0176,-0.0913,-0.0484,0.1534,-0.0274,0.0151,-0.0585,-0.0073,0.0163,0.0299,-0.0458,-0.0212,-0.013,0.025,-0.0377,0.042,-0.0749,0.0163,-0.0242,-0.0084,-0.0206,0.0875,0.016,-0.0793,-0.0451,-0.004,0.0447,0.0304,-0.0009,0.009,0.0134,0.0091,0.0718,-0.0013,-0.0737,0.0446,0.0586,0.028,0.0214,-0.0356,-0.0234,0.0411,0.0459,-0.0749,-0.0193,0.0081,0.0164,0.0412,-0.0125,0.0028,-0.0034,-0.0395,0.0118,-0.0199,-0.0116,0.0049,0.097,-0.006,0.0132,-0.0165,-0.1183,0.0056,0.0167,0.0448,0.0001,0.0199,0.0032,0.0273,-0.0098,0.0177,0.0641,-0.0261,0.0059,0.0173,-0.0076,0.0272,0.0131,0.0494,0.0194,-0.0583,-0.0532,-0.2641,-0.0033,0.053,0.0076,0.0254,-0.0506,-0.0264,0.0063,0.0306,0.08,0.0768,-0.0219,-0.0137,0.0482,-0.0317,0.0222,-0.0044,0.0443,-0.029,-0.0181,-0.0077,-0.0421,-0.0564,-0.0697,0.0281,-0.0227,0.1684,0.0123,0.0201,-0.023,0.0544,0.0162,-0.0072,-0.0927,0.0453,0.0489,0.0354,0.0029,-0.048,-0.037,-0.0298,0.0442,-0.0143,-0.0618,-0.0508,0.0093,-0.0327,0.0219,-0.0095,-0.0122,0.0047,-0.0367,0.0819,-0.0394,-0.0047,-0.0158,-0.1137,0.067,-0.008,-0.0075,0.0401,-0.0427,0.0405,-0.0528,0.0532,-0.0195,-0.0434,-0.0437,-0.0226,-0.0049,-0.0144,0.1271,-0.0284,-0.0082,0.0581,0.0017,0.0155,-0.0187,-0.0332,-0.0534,0.0579,-0.0795,0.0209,0.0616,0.0268,-0.0199,0.0647,-0.019,0.0255,-0.0169,0.0043,-0.0314,-0.0484,-0.0122,0.0343,0.0197,-0.2913,0.0486,0.0189,0.0078,-0.0614,-0.0392,0.055,0.0506,-0.0152,0.0068,0.0445,0.0451,0.0678,-0.0184,-0.0275,-0.0234,0.0248,-0.0167,0.056,-0.0807,-0.0322,0.0213,0.1927,-0.007,0.0301,0.0631,0.0024,0.0147,0.0521,-0.0367,0.0169,0.0064,0.0722,-0.0711,0.0603,0.0632,-0.0356,0.0742,0.0011,-0.0276,0.0095,0.0217,-0.0262,-0.066,0.0852,-0.0235,-0.0285,-0.0585,-0.0204,0.0332,-0.0397,0.0307,-0.005,0.0077,-0.0152,-0.0123,-0.058,-0.0168,-0.0413,-0.0134,0.0168,-0.0956,-0.0126,-0.027,0.001]}
{"key":"[Causality matters in medical imaging] This article discusses how the language of causality can shed new light on the major challenges in machine learning for medical imaging: 1) data scarcity, which is the limited availability of high-quality annotations, and 2) data mismatch, whereby a trained algorithm may fail to generalize in clinical practice. Looking at these challenges through the lens of causality allows decisions about data collection, annotation procedures, and learning strategies to be made (and scrutinized) more transparently. We discuss how causal relationships between images and annotations can not only have profound effects on the performance of predictive models, but may even dictate which learning strategies should be considered in the first place. For example, we conclude that semi-supervision may be unsuitable for image segmentation---one of the possibly surprising insights from our causal analysis, which is illustrated with representative real-world examples of computer-aided diagnosis (skin lesion classification in dermatology) and radiotherapy (automated contouring of tumours). We highlight that being aware of and accounting for the causal relationships in medical imaging data is important for the safe development of machine learning and essential for regulation and responsible reporting. To facilitate this we provide step-by-step recommendations for future studies.","layer":4,"vector":[0.0011,0.0115,0.0196,-0.0112,0.0549,-0.0095,0.0702,0.0299,0.0245,-0.0133,0.0393,-0.0762,0.0215,0.0964,-0.0323,0.0406,-0.0055,0.0333,-0.0407,0.0269,-0.0038,-0.0094,0.0084,-0.0622,-0.0163,0.0477,-0.0243,-0.0278,-0.0548,-0.2529,-0.0028,-0.0536,0.0129,-0.0023,0.0172,-0.0251,-0.055,0.0589,-0.0146,0.0325,-0.0146,0.0304,-0.0517,-0.0678,-0.009,-0.0548,-0.0075,-0.017,-0.0362,-0.0127,-0.0066,-0.0127,0.0414,0.0619,0.0272,0.0336,0.0498,0.0627,0.0363,0.0528,0.0215,0.0398,-0.1734,0.0737,0.0893,-0.0131,-0.0215,0.0021,0.0174,0.0577,0.0242,0.0343,-0.0106,0.0422,0.025,-0.0388,0.0244,0.0151,-0.0267,-0.0187,0.0116,0.0346,-0.0333,-0.01,-0.0299,-0.07,0.0066,-0.0963,0.0293,0.001,-0.018,-0.0285,-0.014,0.0511,-0.0635,-0.046,0.0523,0.0163,-0.0242,0.1526,-0.0559,0.0119,-0.0064,-0.0152,0.0352,-0.0436,-0.0385,-0.0223,-0.0359,-0.0145,0.0158,-0.0131,0.0518,-0.0118,0.0311,-0.0074,0.0707,0.0449,-0.0061,-0.0383,0.0063,-0.0208,0.0283,-0.0068,0.0076,-0.05,0.0349,0.1409,0.0464,-0.0226,0.0264,-0.0288,-0.0427,0.0114,0.0139,-0.0062,0.0351,-0.0266,-0.022,0.0053,-0.0366,-0.0618,0.0155,-0.0655,-0.0799,0.1243,-0.0629,0.0494,-0.0505,-0.0165,-0.0228,0.0259,-0.0556,0.0018,0.0397,0.0407,-0.0025,0.0087,-0.0403,0.0534,0.0064,-0.088,-0.0761,0.1277,0.0038,-0.041,-0.0169,-0.0008,0.0288,0.0048,0.0463,0.0248,-0.0003,0.0193,0.0366,0.0383,-0.0711,-0.0221,0.0097,0.0034,0.0403,-0.0144,-0.0166,0.0714,0.0506,-0.0259,0.0085,0.0069,0.0652,0.0472,0.019,0.0036,-0.0329,-0.0074,-0.0191,-0.0309,-0.0021,-0.0164,-0.0188,0.0051,-0.0448,-0.0128,0.0052,0.0303,-0.0137,0.0023,-0.0425,0.0561,0.0307,0.0125,-0.0172,0.0206,0.0422,-0.007,-0.0491,0.0373,0.0488,0.0242,-0.0202,0.0594,0.0444,-0.026,-0.0631,-0.243,-0.015,-0.0081,0.0008,-0.0071,-0.0732,0.0357,0.0387,0.0182,0.0545,0.0323,-0.0157,-0.0471,-0.0443,-0.042,-0.0043,0.009,0.0593,-0.0363,-0.019,-0.0498,-0.0029,0.0055,-0.0926,0.0669,0.0335,0.2308,0.0634,-0.0019,-0.0205,0.0205,-0.0178,-0.048,-0.1237,0.0602,-0.0127,0.0419,0.0146,-0.08,-0.0221,-0.0472,0.001,-0.022,-0.0739,-0.0248,-0.0089,-0.0512,0.0316,-0.0308,0.0367,0.0281,-0.0559,0.032,0.0269,0.0046,-0.0528,-0.0935,0.0068,-0.0407,0.0172,0.0357,-0.0307,0.0125,-0.1135,0.0364,0.0022,0.0023,-0.0787,-0.0009,-0.0154,0.0054,0.0978,-0.0166,-0.0558,0.0678,0.0458,0.0498,-0.0056,-0.0623,-0.0072,0.0649,-0.0392,0.0276,0.0599,0.0171,0.019,0.0705,-0.0071,-0.0035,-0.0335,-0.015,0.0247,-0.0603,-0.009,0.0284,-0.0164,-0.2918,0.0673,0.0369,0.0174,-0.0269,0.0189,0.0345,0.0308,-0.0175,-0.0193,-0.0086,-0.0029,0.0592,-0.0097,0.001,0.045,0.0801,-0.0654,0.0608,-0.0418,-0.0107,0.0439,0.2333,-0.0657,-0.0247,0.0586,-0.0225,0.0252,0.053,0.0127,0.0441,0.0328,0.0388,-0.0444,0.0277,0.0726,-0.0565,0.0045,0.0073,-0.0653,0.0173,0.0155,-0.0199,-0.0311,0.1101,0.0347,-0.0191,-0.0371,-0.0098,0.0,-0.0013,0.0041,-0.0541,0.0173,0.0359,0.0137,-0.0272,-0.0334,-0.0293,-0.0232,0.0376,-0.0318,-0.037,0.0472,-0.0136]}
{"key":"[Equivalent Distance Geometry Error for Molecular Conformation Comparison] Straight-forward conformation generation models, which generate 3-D structures directly from input molecular graphs, play an important role in various molecular tasks with machine learning, such as 3D-QSAR and virtual screening in drug design. However, existing loss functions in these models either cost overmuch time or fail to guarantee the equivalence during optimization, which means treating different items unfairly, resulting in poor local geometry in generated conformation. So, we propose Equivalent Distance Geometry Error (EDGE) to calculate the differential discrepancy between conformations where the essential factors of three kinds in conformation geometry (i.e. bond lengths, bond angles and dihedral angles) are equivalently optimized with certain weights. And in the improved version of our method, the optimization features minimizing linear transformations of atom-pair distances within 3-hop. Extensive experiments show that, compared with existing loss functions, EDGE performs effectively and efficiently in two tasks under the same backbones.","layer":5,"vector":[-0.0512,-0.0567,0.0062,0.0421,0.0055,0.0274,-0.0129,0.0483,-0.0146,-0.0161,0.023,-0.0853,-0.0012,0.0235,0.0116,0.0104,-0.0352,0.0366,-0.0529,0.0336,0.037,-0.0129,-0.0045,-0.0484,0.0504,0.0951,0.001,0.0171,-0.031,-0.2791,0.0309,0.0101,0.0564,-0.0648,-0.051,-0.0607,-0.0589,0.0384,-0.0499,-0.0026,0.0543,0.0343,-0.001,0.0017,-0.0275,-0.0236,-0.0656,-0.0141,0.0255,-0.0552,0.0492,-0.0575,0.0037,0.0137,0.0512,0.0528,0.0343,0.0456,-0.0082,0.0478,0.0412,0.0396,-0.1489,0.0555,0.0765,0.0077,-0.0223,-0.0298,0.0098,0.1155,-0.0527,-0.0015,0.0356,0.0129,0.0038,0.0154,0.019,-0.0251,-0.0033,-0.0115,-0.0047,-0.0183,-0.0626,-0.0261,-0.0266,-0.022,0.0328,-0.0308,0.0742,0.0376,-0.0231,-0.0467,-0.0298,-0.0028,-0.1056,-0.0412,0.043,0.0053,-0.0612,0.172,-0.0563,0.0424,-0.0156,-0.0264,0.0275,-0.0269,-0.0429,-0.0502,0.0193,0.0023,-0.0055,-0.0063,0.0252,-0.0301,-0.0538,0.0135,0.0458,0.0704,-0.0282,-0.0197,-0.0383,0.0661,0.0518,0.0258,0.0117,-0.0346,-0.0547,0.0897,0.0451,0.0543,0.0646,0.0041,-0.0255,-0.0296,0.0205,-0.0178,-0.0132,0.0032,0.0214,0.0149,-0.0341,-0.0453,-0.0094,-0.053,-0.0403,0.1312,-0.0525,0.0144,-0.0497,-0.0226,0.04,0.0096,-0.0501,0.0032,0.0366,0.0248,0.0037,0.0298,-0.0192,0.03,-0.0343,-0.0157,-0.0435,0.1092,-0.022,-0.0644,-0.0051,-0.0098,0.0201,0.0025,0.032,0.0074,-0.0499,0.0507,0.0573,0.0202,-0.046,-0.0328,0.0327,0.0098,0.0333,-0.0173,-0.0558,0.0006,0.0175,-0.0556,-0.0134,-0.0034,0.0306,0.0598,-0.0235,0.0255,-0.0306,0.0185,-0.0267,-0.024,-0.0352,-0.0041,0.003,0.021,0.103,0.0172,-0.056,0.0573,-0.0395,0.021,0.0256,-0.0212,0.0351,0.0383,-0.041,0.0071,-0.0056,-0.0511,-0.0147,-0.0013,0.0561,0.0683,0.0073,0.0688,0.0126,-0.0436,-0.0584,-0.212,0.0123,-0.0014,-0.021,0.071,-0.0361,0.0279,-0.0061,0.0062,0.0494,0.0386,0.0318,-0.011,0.0022,-0.0652,0.0372,0.0472,0.0269,-0.0331,0.0045,-0.0264,0.0293,-0.0187,-0.0866,0.01,-0.0094,0.2296,-0.0014,0.0185,0.0364,0.0352,0.0012,-0.0292,-0.0931,0.053,0.0441,0.0666,-0.0622,-0.0343,-0.0654,-0.0631,0.0511,0.0496,-0.0861,-0.0419,-0.0224,-0.0407,0.0392,-0.0083,0.0264,0.0281,-0.0297,0.103,0.0039,-0.0082,-0.0435,-0.0603,0.0066,-0.0463,0.0375,-0.0042,-0.0618,0.008,-0.0213,0.0376,-0.0196,-0.003,-0.0279,0.05,-0.0172,-0.0127,0.0985,-0.0137,0.0255,0.0752,0.0013,0.032,-0.0007,-0.02,0.0061,-0.004,-0.0161,0.0297,-0.0237,0.0255,-0.0078,0.053,-0.0119,0.0421,-0.0212,0.0333,-0.0214,-0.0551,-0.0149,0.0408,0.0266,-0.2813,0.0233,-0.0068,0.0856,-0.0465,-0.0281,0.0878,-0.0013,-0.0598,-0.0511,0.0404,0.0323,0.0177,-0.0106,0.0137,0.0246,0.0898,-0.0885,0.0549,-0.0227,0.0384,0.0198,0.2556,-0.0421,-0.007,0.0479,-0.0433,-0.0097,0.0433,0.0126,-0.0033,0.0006,0.0881,-0.0595,0.0495,0.0877,-0.0524,0.0161,0.0161,-0.0251,0.0222,0.0106,-0.0563,-0.0112,0.0612,-0.0307,-0.0412,-0.0364,-0.0063,-0.0019,-0.0588,0.0231,-0.0693,-0.0097,0.0212,0.0253,-0.0054,-0.0502,-0.0333,-0.0434,0.0054,-0.0448,-0.053,0.0217,0.0157]}
{"key":"[Survey of Dropout Methods for Deep Neural Networks] Dropout methods are a family of stochastic techniques used in neural network training or inference that have generated significant research interest and are widely used in practice. They have been successfully applied in neural network regularization, model compression, and in measuring the uncertainty of neural network outputs. While original formulated for dense neural network layers, recent advances have made dropout methods also applicable to convolutional and recurrent neural network layers. This paper summarizes the history of dropout methods, their various applications, and current areas of research interest. Important proposed methods are described in additional detail.","layer":4,"vector":[-0.0336,-0.0105,0.0305,-0.0276,0.0241,0.0458,0.0026,0.0151,0.091,-0.0732,0.0275,-0.0219,0.0564,0.0622,0.0522,-0.0132,0.0377,0.0111,-0.0359,-0.0119,0.021,-0.0563,0.0097,-0.0432,0.039,-0.0045,-0.0356,-0.0562,-0.0506,-0.2358,0.0558,-0.0418,0.0594,-0.0294,0.018,-0.0454,-0.0383,0.0526,-0.0404,0.0389,0.0233,0.0204,-0.0368,-0.0847,-0.0029,0.006,-0.0349,-0.0646,0.0161,-0.0129,0.0107,-0.0536,0.0104,0.009,0.0267,0.0122,0.0712,0.0393,0.0462,0.0461,0.0078,0.0492,-0.181,0.0552,0.0594,-0.0007,-0.0628,-0.0121,0.0146,0.0471,-0.0166,0.0463,-0.0321,0.0642,0.0346,-0.0046,0.0311,-0.0509,-0.0045,0.0085,0.04,-0.0152,-0.0414,-0.0089,-0.015,-0.0547,0.0148,-0.0455,0.0106,-0.0263,0.0057,0.0347,-0.044,0.0293,-0.0583,0.0072,0.0426,-0.0082,-0.0484,0.2137,-0.0728,0.0177,0.05,-0.0274,0.0144,-0.0093,-0.0341,-0.0439,-0.0244,0.028,0.0053,-0.016,0.0522,-0.0336,0.0241,-0.007,0.0394,0.079,-0.0196,-0.013,-0.0581,-0.0102,0.0638,0.0083,0.0442,-0.0744,0.0324,0.1204,0.0502,0.0275,0.0604,-0.0514,-0.0642,-0.0547,0.0431,0.0211,0.0187,0.0078,-0.0158,-0.0115,-0.0027,-0.0512,0.0188,-0.0382,-0.0615,0.1079,-0.0316,0.0208,-0.0044,-0.0533,-0.0184,0.0157,-0.0211,-0.0185,0.0504,0.0289,0.0408,0.0774,-0.0867,0.0024,-0.0415,-0.0791,-0.038,0.1041,-0.0168,-0.0573,-0.0804,-0.0096,0.0295,-0.0353,0.0297,0.023,0.0001,-0.0045,0.0812,0.0311,-0.0809,-0.0054,0.0013,0.0419,0.0129,-0.0462,-0.0319,0.0383,0.0122,-0.0695,0.0344,-0.0782,-0.0043,0.052,-0.072,-0.0066,-0.0346,-0.0105,-0.0166,0.0082,-0.0368,-0.0221,-0.0047,-0.009,-0.0043,0.0032,-0.0179,-0.0044,0.0122,0.0565,-0.0184,0.0025,0.0455,0.0485,-0.0287,0.0013,0.068,-0.0462,0.0128,-0.002,0.0193,0.0218,-0.0164,0.003,0.0552,-0.0792,-0.0616,-0.222,0.0217,0.0207,-0.0439,0.0436,-0.1123,0.0101,-0.0188,0.0883,0.0818,0.0424,-0.017,-0.0196,0.0263,0.0153,0.049,0.0359,0.0192,-0.0637,0.0005,-0.007,0.0189,0.0158,-0.1107,0.0469,0.0207,0.1568,-0.0022,0.041,-0.0454,0.01,0.0668,0.0216,-0.0647,0.0695,0.0342,0.0686,0.0036,0.0311,-0.0221,-0.0091,0.0422,0.037,-0.1077,-0.0099,-0.0187,-0.015,0.0231,-0.0586,0.0023,0.0311,-0.0315,0.0232,-0.0384,0.0298,-0.0491,-0.1147,0.0289,-0.0271,0.0033,0.006,-0.0389,0.0144,-0.065,0.0357,0.0095,-0.0105,-0.0284,0.0542,-0.031,-0.0233,0.1138,0.0233,0.0155,0.0773,-0.0074,0.0217,-0.0478,-0.0475,-0.0349,0.0558,-0.019,0.0327,0.0607,0.0211,0.0412,0.0984,-0.0205,0.044,-0.0306,-0.0117,0.0052,-0.0446,0.0036,0.0328,-0.0261,-0.2578,0.0974,0.0312,0.0549,-0.0006,0.0089,0.0268,0.0186,-0.0149,0.0108,-0.0424,0.0121,0.0471,-0.0291,-0.0158,0.0387,0.0257,-0.0588,0.0801,-0.0637,0.0346,0.0077,0.1984,-0.0767,0.0244,0.0171,0.0077,-0.0026,0.0475,-0.0276,0.0127,0.0042,0.0826,-0.066,0.0257,0.0517,-0.0297,0.0362,0.0477,-0.0317,0.0157,-0.0154,-0.0374,-0.045,0.1103,-0.0439,-0.0312,-0.0566,0.0382,0.0348,-0.0428,-0.0312,-0.0198,0.012,0.0183,0.0435,-0.0505,-0.0045,-0.0303,-0.0373,0.0066,-0.1132,-0.0148,-0.0224,-0.0331]}
{"key":"[A Slice Sampler for Restricted Hierarchical Beta Process with Applications to Shared Subspace Learning] Hierarchical beta process has found interesting applications in recent years. In this paper we present a modified hierarchical beta process prior with applications to hierarchical modeling of multiple data sources. The novel use of the prior over a hierarchical factor model allows factors to be shared across different sources. We derive a slice sampler for this model, enabling tractable inference even when the likelihood and the prior over parameters are non-conjugate. This allows the application of the model in much wider contexts without restrictions. We present two different data generative models a linear GaussianGaussian model for real valued data and a linear Poisson-gamma model for count data. Encouraging transfer learning results are shown for two real world applications text modeling and content based image retrieval.","layer":1,"vector":[-0.0286,-0.0277,0.029,-0.0456,0.0015,0.026,0.0045,0.0104,0.0447,-0.0054,0.0194,-0.0547,-0.0042,0.042,0.0323,-0.0009,0.0083,0.0221,-0.0422,-0.0167,0.0302,-0.0455,-0.0228,-0.0147,0.0351,0.005,-0.0483,-0.0376,-0.0492,-0.2439,-0.0138,-0.0345,0.0948,-0.0023,0.0393,-0.0227,-0.0367,0.0421,-0.0269,0.0693,0.005,0.0059,-0.0722,-0.0122,-0.0472,-0.0651,-0.0418,-0.0447,-0.0283,-0.0126,0.0047,-0.0055,-0.0066,0.061,0.0521,0.0283,0.042,0.059,0.0362,0.0383,0.0058,0.0296,-0.1762,0.0709,0.0398,0.0392,-0.0824,0.0043,0.0089,0.0236,-0.0529,0.0462,-0.0108,0.0838,-0.0043,-0.0358,-0.0108,0.0078,-0.0146,-0.013,-0.0116,0.0239,-0.0202,-0.0171,-0.0324,-0.0421,0.0154,-0.0243,0.0137,0.0127,-0.0548,-0.0184,-0.0509,0.0325,-0.086,-0.0302,0.0167,0.0266,0.0058,0.2022,-0.064,0.0385,0.0792,-0.0148,0.0064,-0.0106,-0.0356,0.0048,0.0019,-0.0178,-0.0223,-0.0291,0.0261,-0.0207,0.0379,-0.0272,0.1063,-0.0164,-0.0221,-0.0517,-0.0102,0.0195,0.0673,-0.0393,0.0435,-0.0749,0.0518,0.1461,0.0625,0.0083,0.0408,-0.0316,-0.0589,-0.0286,-0.0206,-0.0099,0.0314,-0.0287,0.0058,-0.0002,-0.0108,-0.0182,0.0119,-0.0622,-0.0387,0.1216,-0.0206,0.0142,-0.0778,-0.043,-0.0318,0.0215,0.0122,-0.053,0.0089,0.0157,0.0103,-0.0133,-0.0531,0.029,-0.0158,-0.0323,-0.0335,0.0952,-0.0079,-0.0846,-0.0358,-0.0258,0.0329,0.024,0.0714,0.0196,-0.0219,0.0282,0.0623,0.025,-0.0546,0.0254,0.0565,0.0047,0.0204,-0.0755,-0.038,0.0541,0.1103,-0.0446,0.0303,-0.0385,0.0291,0.0112,-0.037,-0.0053,-0.019,-0.0061,0.0118,-0.0189,-0.0245,-0.0095,0.016,-0.003,0.0106,0.0189,-0.0813,0.0204,-0.0213,0.0387,0.0116,-0.0085,0.0416,0.0287,0.0033,0.0053,0.0824,-0.0284,-0.005,-0.0313,0.008,0.0484,-0.0031,0.0766,0.0254,-0.0556,-0.022,-0.2428,-0.0092,0.0274,0.0095,0.0316,-0.0344,0.0181,-0.0266,0.0377,0.0742,0.0504,0.0212,-0.0242,0.0095,-0.0075,0.0232,0.0326,0.0467,0.0061,-0.0019,-0.0297,-0.0319,-0.0178,-0.0538,0.0749,-0.017,0.1834,0.042,0.0155,0.0072,0.0566,0.0925,-0.0324,-0.056,0.0549,0.0064,0.0658,-0.001,-0.0431,-0.0273,-0.0187,-0.0005,0.0196,-0.0852,-0.0206,-0.0428,-0.0181,0.0206,-0.0321,0.0435,0.0463,-0.0659,0.0595,-0.0263,-0.0118,-0.0418,-0.1021,0.0363,-0.0604,0.0264,0.0285,-0.0603,0.0341,-0.0944,0.0207,-0.0041,-0.0198,-0.041,-0.0283,-0.0498,-0.0484,0.0512,-0.011,0.0342,0.0236,0.031,0.0275,-0.0671,-0.0661,-0.0443,0.0654,-0.0776,0.0076,0.031,0.0463,0.0255,0.0812,-0.0034,0.0477,-0.0602,-0.0248,0.0129,-0.0684,0.0048,0.0445,-0.013,-0.2872,0.0274,0.0123,0.0296,-0.0087,-0.0031,0.0608,0.0098,-0.0433,0.0088,0.0133,0.0483,0.0652,-0.055,-0.0098,0.0375,0.0606,0.0092,0.0364,-0.0814,0.0076,0.0587,0.2276,-0.0211,0.0155,0.0168,0.0034,0.0013,0.0189,-0.038,0.02,0.0076,0.1266,-0.0271,0.0504,0.0566,-0.037,0.0589,0.0065,-0.0306,-0.0146,0.0617,-0.0849,-0.0381,0.1048,-0.0149,0.0022,-0.0189,-0.0295,0.0299,-0.0022,0.0133,0.0467,-0.0165,0.0328,-0.0121,-0.06,-0.0561,0.0148,-0.0418,0.0154,-0.0317,-0.0348,0.0043,0.0274]}
{"key":"[Svadhyaya system for the Second Diagnosing COVID-19 using Acoustics Challenge 2021] This report describes the system used for detecting COVID-19 positives using three different acoustic modalities, namely speech, breathing, and cough in the second DiCOVA challenge. The proposed system is based on the combination of 4 different approaches, each focusing more on one aspect of the problem, and reaches the blind test AUCs of 86.41, 77.60, and 84.55, in the breathing, cough, and speech tracks, respectively, and the AUC of 85.37 in the fusion of these three tracks.","layer":5,"vector":[-0.0545,-0.0277,0.0497,0.0005,0.0267,0.0097,0.0561,0.0022,0.0251,-0.0029,0.0048,-0.0633,0.0093,0.0457,0.0459,0.0019,0.0337,0.0065,-0.0046,0.0219,0.0225,0.0377,-0.0192,-0.0911,0.0243,0.0272,-0.0263,-0.073,-0.0665,-0.1563,0.0029,-0.069,0.0305,-0.0355,0.0008,-0.0498,-0.0416,0.0499,-0.0489,0.0056,-0.0289,-0.0018,0.0218,-0.0926,-0.0469,-0.0871,-0.066,-0.0045,-0.0001,-0.0652,-0.0106,-0.0469,0.0213,-0.0195,0.0048,0.0244,0.027,0.028,0.0263,0.028,0.0137,0.0258,-0.2464,0.0789,0.0379,0.016,0.0099,-0.059,0.0686,0.0329,0.009,0.0341,0.0444,0.0067,0.0095,-0.0114,-0.0072,-0.047,0.0276,0.0378,0.0178,0.0077,0.0203,-0.0511,-0.0466,-0.0437,-0.002,-0.0414,0.0052,-0.0482,-0.061,-0.0074,-0.009,0.0438,-0.0951,-0.0418,0.0106,0.0258,-0.0109,0.2103,-0.0165,-0.0218,-0.0145,-0.0589,0.0179,-0.0456,-0.0566,-0.075,0.0325,0.0026,0.0033,-0.0449,0.0862,-0.0109,-0.0209,0.0333,0.0622,0.0299,-0.0151,0.0042,-0.0249,0.0039,0.0598,-0.0219,0.0668,-0.0546,0.0736,0.1065,0.0276,0.0603,0.0861,-0.0454,-0.0481,-0.0046,-0.0018,-0.0004,0.0299,0.0474,0.0158,0.0088,-0.0391,-0.092,-0.0062,-0.1175,-0.0579,0.1149,-0.077,0.0219,-0.0498,-0.0349,-0.0135,0.0102,-0.002,-0.0187,0.0279,0.0119,0.0468,0.0361,-0.0331,0.0266,0.0054,-0.0669,-0.0588,0.1139,0.0302,-0.0784,-0.0454,0.0321,0.0343,-0.0312,0.0183,0.0297,-0.049,-0.0132,0.0624,0.0052,-0.0699,0.023,-0.0204,0.053,0.0217,-0.0353,-0.0469,0.0357,0.0405,-0.0836,0.0268,-0.078,0.0397,0.0303,-0.0102,0.0124,-0.0456,-0.0088,-0.0306,-0.0206,-0.0036,0.0248,0.0184,-0.0221,0.0465,-0.0097,0.0106,0.0439,0.0125,-0.004,-0.0061,-0.007,0.0129,0.0453,-0.0002,-0.0181,0.0745,-0.0268,-0.0425,-0.0339,0.0029,0.058,-0.0327,0.081,0.0344,-0.0347,-0.0505,-0.2262,0.0214,0.0105,-0.0068,0.0319,-0.0268,0.0493,-0.0087,0.0864,0.0315,0.0695,0.0371,0.0076,0.0135,0.0179,0.0578,0.0353,-0.0142,-0.0297,-0.0323,0.0191,0.0132,0.0242,-0.0428,0.0483,-0.0087,0.2405,0.0128,-0.0001,-0.0065,-0.0251,0.004,-0.0161,-0.125,0.0722,0.0558,0.0023,0.0257,-0.0372,-0.0339,0.0014,0.0187,0.0197,-0.0196,-0.0149,-0.0195,-0.033,0.0133,-0.059,0.0253,0.0349,0.0187,0.0201,-0.0006,0.0112,-0.0068,-0.097,0.0117,-0.0376,0.0264,-0.0198,0.0029,0.0047,-0.0621,0.0307,0.0058,-0.0328,-0.048,0.0389,-0.0045,-0.0224,0.0643,-0.0169,0.0099,0.0549,-0.0121,0.0437,-0.0879,-0.0212,-0.0566,0.0777,-0.0123,0.0151,0.0548,0.0342,0.0061,0.0901,0.0166,0.014,-0.0617,-0.0087,0.0223,0.0165,-0.0273,-0.0021,-0.0048,-0.3012,0.0206,0.0004,-0.0035,-0.0287,-0.0202,-0.0098,0.0225,-0.0574,-0.0023,-0.0264,0.03,0.0109,-0.0181,0.024,0.0735,0.0629,-0.0662,0.1051,-0.0427,-0.0289,0.0575,0.2078,-0.0327,0.0267,-0.0202,0.0168,-0.0177,0.0069,0.0093,0.0314,0.0075,0.0828,-0.0544,-0.0143,0.0581,-0.0361,0.0717,0.013,0.0195,-0.0022,0.0612,-0.0208,-0.0221,0.0802,0.0044,-0.0271,-0.0442,0.0057,0.0092,-0.0101,0.0178,0.041,0.0042,0.0569,0.0462,-0.0266,-0.0329,0.0202,-0.0413,0.0108,-0.0376,-0.0057,0.0276,0.0302]}
{"key":"[Learning Synthetic Environments for Reinforcement Learning with Evolution Strategies] This work explores learning agent-agnostic synthetic environments (SEs) for Reinforcement Learning. SEs act as a proxy for target environments and allow agents to be trained more efficiently than when directly trained on the target environment. We formulate this as a bi-level optimization problem and represent an SE as a neural network. By using Natural Evolution Strategies and a population of SE parameter vectors, we train agents in the inner loop on evolving SEs while in the outer loop we use the performance on the target task as a score for meta-updating the SE population. We show empirically that our method is capable of learning SEs for two discrete-action-space tasks (CartPole-v0 and Acrobot-v1) that allow us to train agents more robustly and with up to 60% fewer steps. Not only do we show in experiments with 4000 evaluations that the SEs are robust against hyperparameter changes such as the learning rate, batch sizes and network sizes, we also show that SEs trained with DDQN agents transfer in limited ways to a discrete-action-space version of TD3 and very well to Dueling DDQN.","layer":2,"vector":[-0.0519,-0.0306,0.0227,0.0226,0.0065,-0.0013,-0.0027,0.0196,0.0293,-0.0036,0.019,-0.0421,0.0644,0.0535,-0.0026,-0.0075,-0.0378,0.0356,-0.0009,-0.0188,0.0108,-0.0169,-0.003,-0.0629,-0.0011,-0.007,-0.0185,-0.0313,-0.0427,-0.2417,0.0498,-0.0067,0.0105,-0.0029,0.0081,-0.0178,-0.0185,0.0434,-0.005,0.0514,0.0269,0.0153,-0.0182,-0.0827,-0.0338,-0.0236,0.0013,-0.0213,-0.0051,-0.0558,0.011,-0.0191,0.0151,0.0307,-0.0054,0.0315,0.0535,0.0583,0.0489,-0.0162,0.0059,0.0737,-0.1431,0.0521,0.0456,0.0592,-0.0677,-0.0413,0.037,0.0494,-0.0328,0.0221,0.0228,0.0108,0.0516,0.0254,-0.0003,-0.0452,0.0898,-0.0002,0.0218,-0.0849,-0.025,0.0012,-0.0384,-0.0619,0.0383,-0.0364,0.03,0.0394,-0.0477,0.0473,-0.0276,-0.0102,-0.0536,0.0124,0.0022,0.0129,-0.0797,0.2,-0.0157,0.0214,0.0077,0.0058,0.0224,-0.0399,-0.0274,-0.0366,-0.0155,-0.0358,-0.0295,0.0049,0.0295,-0.0241,-0.025,0.021,0.0625,0.0354,0.0087,-0.0145,-0.0142,0.0053,0.0416,-0.0167,0.0207,-0.0808,0.0099,0.157,-0.0153,0.05,0.0334,-0.0397,-0.0166,-0.0198,0.0153,0.0149,0.0274,0.0118,-0.0306,0.0086,-0.0304,0.0233,0.011,-0.0685,-0.071,0.0616,0.0122,0.0479,-0.0301,-0.0027,-0.0371,0.0166,-0.0123,-0.0121,0.0051,0.0363,0.0158,0.0622,-0.0813,-0.0012,-0.0323,-0.0146,-0.0018,0.1228,-0.0056,-0.0829,-0.0887,0.0245,0.0578,0.0014,0.0215,0.0211,-0.0482,0.0301,0.0889,0.0119,-0.0709,0.015,0.026,-0.0174,0.0245,-0.0786,-0.0136,0.0318,0.0431,-0.0343,-0.0148,-0.0311,0.062,0.0502,-0.009,0.0432,0.0175,0.0451,-0.0173,-0.0052,0.0163,-0.0174,0.0412,-0.005,-0.0033,0.0212,-0.0535,-0.0127,-0.0162,-0.0299,-0.0037,0.0001,0.0283,0.0355,-0.0568,-0.0072,0.0243,-0.0336,-0.0632,-0.0052,-0.0071,-0.0035,-0.0138,0.0244,0.0104,-0.0007,-0.0409,-0.2458,-0.0367,-0.004,-0.0579,0.0635,-0.0653,0.0631,-0.0188,0.0164,0.0306,0.0297,-0.0371,-0.018,0.0476,-0.0281,0.0408,0.0343,0.0145,-0.0064,-0.0138,0.0032,0.005,-0.0089,-0.0993,0.0705,-0.0085,0.2294,0.023,0.0484,0.0168,0.0244,0.0377,-0.0448,-0.132,0.062,-0.0105,0.0637,-0.0437,0.0026,-0.0518,-0.0129,0.0203,-0.0368,-0.1275,-0.0163,-0.0428,-0.0307,0.0701,-0.0604,0.0144,0.0434,-0.0043,0.0466,-0.0317,-0.0707,-0.0287,-0.066,0.0487,-0.0283,0.0422,0.0159,-0.0139,0.0428,-0.0108,0.0546,-0.0036,0.0493,-0.0698,0.0747,0.0091,-0.0403,0.0607,0.0256,0.0218,0.0232,0.0203,-0.0058,-0.0087,-0.0312,0.0053,0.0393,-0.0314,0.0752,0.0366,0.0157,-0.0031,0.0486,-0.0466,0.0784,0.0023,0.0163,0.0295,-0.0997,-0.0123,0.0634,-0.017,-0.2899,0.0439,0.0168,0.0158,-0.0487,-0.0078,0.0229,-0.0117,-0.0722,-0.0128,-0.0056,0.0338,0.0248,0.0616,0.0487,0.0257,0.1036,0.0033,0.0436,-0.0878,0.0145,0.0127,0.2212,-0.0293,0.0774,0.022,-0.0607,-0.007,-0.0056,-0.0388,0.0298,0.0375,0.0702,-0.0875,0.0765,0.0544,-0.0531,0.0243,-0.006,0.008,-0.0502,0.0064,-0.0279,0.0155,0.0761,-0.0095,-0.0181,-0.0629,-0.0415,0.0462,-0.0382,0.0275,-0.0116,-0.0233,0.0334,0.0145,-0.0285,-0.0931,-0.032,-0.037,0.0213,-0.0635,0.0579,-0.0142,-0.0289]}
{"key":"[Fooling OCR Systems with Adversarial Text Images] We demonstrate that state-of-the-art optical character recognition (OCR) based on deep learning is vulnerable to adversarial images. Minor modifications to images of printed text, which do not change the meaning of the text to a human reader, cause the OCR system to \"recognize\" a different text where certain words chosen by the adversary are replaced by their semantic opposites. This completely changes the meaning of the output produced by the OCR system and by the NLP applications that use OCR for preprocessing their inputs.","layer":0,"vector":[-0.056,-0.0302,-0.0062,-0.0148,0.0189,0.0156,0.0496,0.0113,-0.03,-0.0037,0.0101,-0.033,0.0113,0.0585,-0.0134,-0.0481,0.0149,0.0195,-0.0535,0.0275,0.068,-0.0292,-0.0009,-0.0369,-0.0163,0.0269,-0.0238,-0.054,-0.0331,-0.2366,0.0045,-0.0375,0.0341,-0.006,0.0124,-0.0261,-0.0652,0.0437,-0.0012,-0.0097,-0.0019,-0.0202,-0.0343,-0.044,0.0136,-0.0419,-0.04,-0.0158,-0.0038,-0.0372,0.0163,-0.0214,0.01,0.0248,0.0383,-0.0103,0.0938,0.0351,0.0245,0.0593,0.0085,0.0825,-0.1758,0.0963,0.0105,0.0158,-0.0081,-0.0254,0.0168,0.0203,0.0112,0.0654,-0.0184,0.0275,0.0026,0.0247,-0.0321,-0.0564,-0.0043,-0.0166,0.0698,0.0154,-0.0208,0.0293,-0.0285,-0.032,-0.0186,-0.0451,0.0315,-0.0373,-0.024,-0.0217,0.012,0.0237,-0.0646,-0.0389,0.0096,0.01,-0.0422,0.2156,-0.0827,-0.0034,0.0534,-0.082,0.064,0.0029,-0.0106,-0.0577,-0.0486,0.0273,-0.0199,-0.0171,0.0186,0.0124,0.0305,-0.0058,0.0227,0.0157,-0.045,0.0023,-0.0235,0.0323,-0.0051,0.0099,0.0101,-0.0484,0.0772,0.1346,0.0627,0.0341,0.0031,-0.0294,0.0024,-0.0153,0.0226,0.0597,-0.0375,0.0099,0.0382,-0.0164,-0.0163,-0.0674,0.0105,0.0111,-0.0022,0.0892,-0.0406,0.0343,-0.0437,-0.0153,0.0005,0.0424,-0.0123,0.0283,0.0004,0.0084,0.0658,0.0462,-0.0488,-0.0075,0.0184,-0.0216,-0.0629,0.1084,0.0111,-0.089,-0.0486,-0.0066,-0.0192,-0.0254,0.0697,0.003,-0.0161,0.0379,0.0504,0.0532,-0.1086,-0.0286,-0.0262,0.0146,-0.0069,-0.0453,-0.063,0.0221,0.0665,-0.0208,-0.0257,-0.0956,0.0372,0.0381,0.0044,0.0471,-0.0771,-0.0528,-0.0484,-0.0271,-0.0083,-0.017,-0.0441,-0.0669,0.0365,0.0179,-0.0159,-0.0323,0.0522,-0.0175,0.0497,-0.011,0.0205,0.0225,-0.0578,0.0005,0.0274,-0.0177,-0.0437,-0.0365,0.0039,0.0661,-0.0008,0.0442,-0.0084,-0.0472,-0.0704,-0.2376,0.0051,0.0001,-0.0638,0.075,-0.0688,-0.0033,-0.0191,0.0456,0.0426,0.0317,-0.0109,0.029,-0.0295,0.0078,0.0396,0.0377,0.0006,0.0012,0.0134,-0.0213,0.0421,-0.0138,-0.074,0.028,0.0147,0.2414,0.0584,0.0359,-0.0055,0.0306,0.0288,-0.0309,-0.1084,0.0714,0.0277,0.0365,0.0094,-0.013,0.01,-0.0071,0.0341,0.0032,-0.0983,-0.0193,-0.0351,-0.0217,0.0346,-0.0611,0.0834,0.0312,-0.0253,0.0638,0.0673,-0.0114,-0.0362,-0.0996,0.0528,-0.0612,-0.0336,0.0021,-0.0646,-0.0132,-0.0835,0.0739,0.0315,-0.0383,-0.0654,0.0535,0.0107,0.0039,0.0838,0.0241,0.0231,0.0443,0.0251,0.0099,-0.0515,-0.0466,-0.0249,0.0543,0.0206,0.0434,0.0187,0.0454,0.0099,0.0719,-0.0198,0.0144,-0.0159,-0.0089,0.022,-0.0376,-0.0354,0.0437,-0.0053,-0.2938,0.0597,0.0187,0.0458,-0.0043,0.0136,0.0619,0.0145,-0.0391,0.0294,-0.0356,0.0283,0.0007,-0.0413,0.0043,0.0697,0.0803,-0.0858,0.0222,-0.0313,0.0489,0.0138,0.2023,-0.0742,-0.0275,-0.0009,0.0395,0.0259,0.0006,-0.0252,0.0631,0.0349,0.077,0.0001,0.0229,0.0663,-0.0556,-0.002,0.0297,-0.0035,-0.0147,0.0427,-0.0571,-0.0261,0.0723,0.0121,-0.0067,0.0506,0.0044,0.0093,0.0014,0.0177,-0.0263,0.0109,0.0312,0.0117,-0.0803,-0.0482,0.0031,-0.0192,0.0088,-0.0307,-0.022,0.019,-0.0378]}
{"key":"[Adversarial Learning for Personalized Tag Recommendation] We have recently seen great progress in image classification due to the success of deep convolutional neural networks and the availability of large-scale datasets. Most of the existing work focuses on single-label image classification. However, there are usually multiple tags associated with an image. The existing works on multi-label classification are mainly based on lab curated labels. Humans assign tags to their images differently, which is mainly based on their interests and personal tagging behavior. In this paper, we address the problem of personalized tag recommendation and propose an end-to-end deep network which can be trained on large-scale datasets. The user-preference is learned within the network in an unsupervised way where the network performs joint optimization for user-preference and visual encoding. A joint training of user-preference and visual encoding allows the network to efficiently integrate the visual preference with tagging behavior for a better user recommendation. In addition, we propose the use of adversarial learning, which enforces the network to predict tags resembling user-generated tags. We demonstrate the effectiveness of the proposed model on two different large-scale and publicly available datasets, YFCC100M and NUS-WIDE. The proposed method achieves significantly better performance on both the datasets when compared to the baselines and other state-of-the-art methods. The code is publicly available at https://github.com/vyzuer/ALTReco.","layer":0,"vector":[-0.0109,-0.0436,-0.032,-0.0315,0.0501,0.0347,0.0565,0.0493,-0.0097,-0.0188,0.0142,-0.0297,0.0459,0.0925,0.0329,0.0083,0.0339,0.0348,-0.039,-0.0133,0.0246,-0.016,-0.0394,-0.0706,-0.0292,0.0356,-0.0026,-0.058,-0.0535,-0.1952,0.0364,-0.0468,0.0701,-0.0145,-0.0067,-0.0387,-0.024,0.0024,-0.0102,0.0345,0.0223,-0.0065,-0.0539,-0.0718,0.0085,0.0361,-0.0169,-0.058,-0.0211,-0.0217,0.0508,-0.0303,-0.0016,0.0396,0.0254,0.0366,0.0385,0.0113,0.0243,0.0412,0.0232,0.0568,-0.1601,0.0686,0.0252,-0.0089,-0.0246,0.0406,-0.0586,0.0603,0.0179,0.0929,-0.0092,0.0374,-0.0083,0.0289,-0.0196,-0.0179,-0.0394,0.0309,0.0022,0.0349,-0.0808,-0.0818,0.0328,-0.0272,0.0328,-0.0321,0.0339,-0.0059,-0.0245,-0.0224,-0.0285,0.0326,-0.0624,-0.038,0.0166,0.0227,-0.1058,0.217,-0.0856,0.0495,0.0517,-0.0434,0.0151,0.0196,-0.0437,-0.0274,-0.0158,-0.012,-0.0436,0.0281,0.0305,-0.0641,0.0716,0.0011,0.0603,0.0612,0.0091,-0.0402,-0.0364,-0.0265,0.0431,-0.0213,0.0282,-0.0118,0.0009,0.1396,0.04,0.0035,0.0239,-0.0082,-0.0667,0.0404,0.0068,0.0609,0.0251,-0.0285,0.0455,-0.0179,-0.0279,-0.0249,0.0007,-0.0486,-0.0291,0.0893,-0.0398,0.017,-0.061,-0.0465,-0.0107,0.0227,-0.0371,0.0089,0.0321,0.0362,0.0719,0.0622,-0.0577,0.0275,-0.0142,-0.0839,-0.0294,0.0818,-0.0196,-0.0991,-0.0342,0.0009,-0.0124,-0.0214,-0.012,0.0232,-0.0279,0.0279,0.083,0.0707,-0.0838,-0.0203,0.0001,0.0214,0.0117,-0.0284,-0.0285,0.0331,0.045,-0.02,0.0135,-0.0881,0.0181,0.0456,-0.0185,0.0057,-0.0076,-0.0169,0.0107,-0.0315,-0.0226,-0.0115,-0.0198,-0.0432,0.0251,0.0132,-0.0046,0.0218,0.0048,0.0067,0.005,-0.0276,0.0299,-0.0204,-0.0111,0.058,0.0257,-0.0202,-0.0666,-0.0265,0.0154,0.075,0.0557,0.041,0.0338,-0.0428,-0.0504,-0.235,0.0121,0.019,-0.0229,0.0057,-0.0979,0.0303,-0.0231,0.0687,0.0916,0.0568,-0.0109,-0.0419,0.0104,0.0353,0.0858,0.0154,0.0493,-0.0032,-0.0263,-0.0272,0.044,-0.0102,-0.0984,0.0643,0.0119,0.224,0.0751,-0.0424,0.0014,0.0519,0.0419,-0.0063,-0.1297,0.0425,0.0182,0.0694,-0.0285,-0.0352,-0.0274,-0.0145,0.0035,0.0392,-0.1268,0.0004,-0.0214,-0.0192,-0.0055,-0.0775,0.0322,0.0556,-0.0336,0.0273,-0.0036,-0.0301,-0.0121,-0.0923,0.0188,-0.0533,0.0193,-0.0321,-0.0908,0.0038,-0.0959,0.0515,0.0026,-0.0447,-0.0429,0.022,-0.0559,0.0042,0.0594,0.0071,0.0057,0.0679,0.0176,0.0165,0.0038,-0.0639,-0.0389,0.0477,0.0224,0.0353,0.0345,0.0355,-0.0068,0.053,0.0018,0.0182,-0.006,0.0066,0.0273,-0.0752,-0.0387,0.0843,-0.0307,-0.2947,0.0691,0.0246,0.0718,-0.0324,0.0047,0.0836,0.0352,-0.0152,0.0043,0.0204,-0.0022,0.0544,-0.0248,-0.0202,0.0142,0.0469,-0.0234,0.0311,-0.008,0.0466,-0.011,0.1731,-0.0281,0.0227,-0.0052,-0.0328,0.0212,0.0241,-0.0265,0.0052,0.0065,0.0926,-0.0548,0.0275,0.09,-0.05,-0.0172,-0.0054,0.0177,-0.0037,0.0054,-0.0472,-0.0286,0.0531,0.0044,-0.0134,-0.0016,-0.0108,-0.0121,-0.047,-0.0058,-0.0339,0.0222,0.0643,0.0353,-0.0397,-0.0382,-0.0258,-0.05,0.0299,-0.0086,-0.0342,-0.015,-0.0102]}
{"key":"[Test for non-negligible adverse shifts] Statistical tests for dataset shift are susceptible to false alarms: they are sensitive to minor differences when there is in fact adequate sample coverage and predictive performance. We propose instead a framework to detect adverse dataset shifts based on outlier scores, $\\texttt{D-SOS}$ for short. $\\texttt{D-SOS}$ holds that the new (test) sample is not substantively worse than the reference (training) sample, and not that the two are equal. The key idea is to reduce observations to outlier scores and compare contamination rates at varying weighted thresholds. Users can define what $\\it{worse}$ means in terms of relevant notions of outlyingness, including proxies for predictive performance. Compared to tests of equal distribution, our approach is uniquely tailored to serve as a robust metric for model monitoring and data validation. We show how versatile and practical $\\texttt{D-SOS}$ is on a wide range of real and simulated data.","layer":9,"vector":[-0.0372,-0.0146,0.0431,-0.0131,0.0691,0.0016,0.0323,0.0079,0.0059,-0.0232,0.033,-0.0482,0.0079,0.0381,-0.019,-0.0114,0.0007,0.0184,-0.049,0.0575,-0.0015,-0.015,-0.0157,-0.0207,0.0478,0.0264,-0.003,-0.0381,-0.0747,-0.2622,-0.0226,-0.0511,0.0265,-0.0507,0.0354,-0.0133,-0.071,0.001,-0.0016,0.0101,0.0062,0.0242,-0.0203,-0.0798,-0.0769,-0.0516,0.0183,0.0217,-0.0444,-0.0031,0.0102,-0.0715,0.0145,0.0538,0.054,0.0053,0.075,0.0342,0.026,0.0615,0.0268,0.0414,-0.1554,0.0241,0.0565,0.0472,-0.0647,-0.0603,0.0161,-0.0098,-0.0485,-0.0025,0.0036,0.0567,0.0023,0.023,0.0012,-0.0197,-0.0023,0.0268,0.0168,-0.0383,-0.0237,-0.0259,-0.0295,-0.0667,-0.0034,-0.0111,0.0252,-0.0071,-0.029,0.0133,0.0116,0.0223,-0.0517,-0.0131,0.0501,0.0239,-0.0233,0.2266,-0.0541,0.0243,-0.0203,0.0252,0.028,-0.0645,-0.001,-0.0688,-0.014,-0.0236,0.0207,-0.0072,0.0386,-0.0506,0.0059,0.0033,0.0832,0.0415,0.0111,0.0475,-0.0479,0.0129,0.0564,-0.0294,0.061,-0.0505,0.0408,0.1352,-0.0015,0.001,0.0006,-0.0198,-0.0585,0.0154,0.0052,0.0269,0.0081,0.0564,0.0548,-0.0076,-0.0371,-0.042,0.0117,-0.0783,-0.0319,0.1332,-0.0369,0.0347,-0.0345,-0.0453,-0.0095,0.0462,-0.0173,-0.0047,0.0419,0.0168,0.0049,0.0317,-0.0399,0.0038,-0.003,-0.0646,-0.0173,0.0813,-0.0054,-0.0076,-0.0394,-0.005,0.0275,-0.0119,0.0608,0.0108,-0.0216,0.0221,0.0378,-0.0455,-0.028,-0.0128,-0.0133,0.0081,0.0456,-0.0115,-0.0534,0.0749,0.0526,-0.0277,-0.0357,-0.0217,0.0545,0.0829,-0.0208,-0.0202,-0.0459,0.011,-0.0322,-0.0058,-0.0285,0.0143,-0.0017,-0.0395,0.0067,0.0416,-0.0127,0.0109,-0.0341,0.0345,0.0041,-0.0708,0.0583,0.0542,-0.0688,-0.006,0.0621,0.0073,-0.0014,0.0025,0.0348,0.0377,-0.022,0.06,0.0183,-0.0273,-0.0053,-0.2729,-0.0185,0.0117,0.004,0.059,-0.0443,0.0143,-0.0005,0.0746,0.0933,0.0472,0.0032,-0.0184,0.0213,0.0054,0.0381,0.0129,0.0055,-0.0396,-0.0065,-0.0405,0.0442,-0.02,-0.0886,0.0633,-0.0231,0.2019,-0.0011,0.0463,-0.0573,-0.0263,0.0028,-0.012,-0.0568,0.0892,0.0383,0.0257,-0.0033,-0.0508,-0.0381,-0.0199,0.0139,-0.0004,-0.0953,-0.0187,-0.0228,-0.0418,0.021,-0.1023,0.0694,0.0117,-0.0472,0.0705,0.0173,0.0449,-0.0358,-0.105,0.035,-0.038,-0.0034,0.0083,-0.0186,0.0494,-0.0846,0.029,-0.0272,-0.0441,-0.0988,0.0384,-0.0092,-0.0346,0.0964,-0.0346,0.0173,0.0281,-0.0033,-0.0429,-0.0445,-0.0675,-0.0644,0.0887,0.002,0.0128,0.0232,0.0052,0.0066,0.0697,0.0243,0.0591,-0.0386,0.0028,-0.0013,-0.0651,-0.0367,0.0645,0.0183,-0.2488,0.001,-0.0192,-0.0023,-0.016,-0.0202,0.0268,0.0412,-0.0099,-0.0044,0.0,0.048,0.0221,-0.0433,0.0192,0.0522,0.0799,-0.0534,0.0718,-0.0568,0.0193,0.0356,0.2027,-0.0169,-0.004,0.0453,-0.0109,0.0244,0.0222,-0.005,0.0535,0.0149,0.0504,-0.0334,0.0235,0.0932,-0.062,0.0351,0.0096,-0.024,0.0082,-0.0011,-0.0149,-0.0082,0.1532,-0.046,-0.0143,-0.0362,0.0493,0.0294,-0.0147,-0.0315,-0.0137,0.0448,0.0187,0.0154,-0.0553,-0.0206,-0.0321,-0.0623,0.0505,-0.0733,-0.0204,-0.006,0.0076]}
{"key":"[A comprehensive review on convolutional neural network in machine fault diagnosis] With the rapid development of manufacturing industry, machine fault diagnosis has become increasingly significant to ensure safe equipment operation and production. Consequently, multifarious approaches have been explored and developed in the past years, of which intelligent algorithms develop particularly rapidly. Convolutional neural network, as a typical representative of intelligent diagnostic models, has been extensively studied and applied in recent five years, and a large amount of literature has been published in academic journals and conference proceedings. However, there has not been a systematic review to cover these studies and make a prospect for the further research. To fill in this gap, this work attempts to review and summarize the development of the Convolutional Network based Fault Diagnosis (CNFD) approaches comprehensively. Generally, a typical CNFD framework is composed of the following steps, namely, data collection, model construction, and feature learning and decision making, thus this paper is organized by following this stream. Firstly, data collection process is described, in which several popular datasets are introduced. Then, the fundamental theory from the basic convolutional neural network to its variants is elaborated. After that, the applications of CNFD are reviewed in terms of three mainstream directions, i.e. classification, prediction and transfer diagnosis. Finally, conclusions and prospects are presented to point out the characteristics of current development, facing challenges and future trends. Last but not least, it is expected that this work would provide convenience and inspire further exploration for researchers in this field.","layer":0,"vector":[-0.0367,-0.0406,0.0109,-0.0248,0.0362,0.0236,0.0552,0.0604,0.0544,-0.0377,0.0096,-0.063,0.0603,0.0764,0.0358,0.0466,0.0266,0.065,-0.0069,-0.0265,0.0142,-0.058,-0.0302,-0.025,-0.0075,0.0196,0.0332,-0.0083,-0.0898,-0.2615,-0.0147,-0.0482,0.0699,-0.0541,-0.0013,-0.0271,-0.0276,0.0508,-0.0072,0.027,0.0334,-0.0286,-0.0369,-0.0789,-0.0235,-0.0389,0.0082,-0.034,0.0481,-0.0542,0.0299,-0.0291,0.0313,0.0084,0.0141,0.0236,0.0731,0.0429,0.0464,0.0231,0.0562,0.0228,-0.1934,0.0106,0.0479,0.0233,-0.0158,-0.0103,-0.0047,0.0404,-0.0004,0.0069,-0.0079,0.0294,-0.0317,0.021,0.0202,-0.0144,-0.0433,0.0081,0.0359,0.0128,-0.0481,-0.0509,-0.016,-0.0457,0.0363,-0.0223,0.0625,0.0136,-0.0497,-0.0023,-0.0445,0.0417,-0.0609,0.028,0.062,0.0129,-0.0879,0.1861,-0.0739,0.041,0.0472,-0.0232,0.0045,0.0158,-0.0417,-0.0279,-0.0532,-0.0353,0.0089,-0.0039,0.0459,-0.0058,0.0114,-0.0142,0.065,0.0195,0.0128,-0.0327,0.0015,0.0217,0.0738,-0.0738,0.0144,-0.0662,0.0308,0.1364,0.031,-0.0243,0.027,-0.0162,-0.0388,-0.0182,-0.0094,0.0414,0.0401,-0.0119,0.017,0.001,-0.0654,-0.0863,-0.0039,-0.0679,-0.0482,0.0643,-0.0653,-0.0425,-0.0436,-0.0698,-0.0133,0.0018,-0.0339,-0.0134,0.0284,0.0133,0.0274,0.0529,-0.0566,0.0437,-0.0087,-0.0531,-0.0845,0.1025,0.0014,-0.108,-0.0569,-0.0275,0.0021,-0.0076,-0.0006,0.0353,-0.0368,-0.0092,0.0621,0.0693,-0.0329,-0.0059,-0.0029,0.0514,0.0231,-0.0261,-0.006,0.0192,0.0792,-0.0435,0.0741,-0.047,-0.0041,0.0722,-0.0651,0.0326,-0.0481,0.0275,-0.0039,-0.0259,0.0002,0.0063,0.0156,-0.0383,-0.0252,0.0051,-0.0036,-0.0073,-0.0444,0.0299,-0.0419,0.0271,-0.0026,0.0668,-0.01,-0.0005,0.0872,-0.0326,-0.0355,-0.0042,0.0198,0.0747,-0.0141,0.0437,0.0548,-0.0203,-0.0573,-0.2107,-0.0181,0.0244,0.0001,0.0076,-0.052,-0.0123,0.0095,0.0603,0.0125,0.1197,0.0219,-0.0225,-0.042,0.0202,0.0446,0.0339,0.0381,-0.0624,-0.0189,-0.0227,0.0125,0.0426,-0.0918,0.0377,0.0017,0.1708,-0.0156,0.0591,0.006,0.03,0.0505,-0.0247,-0.0671,0.0852,0.0104,0.0123,-0.0068,-0.0424,-0.0214,-0.022,0.0235,0.0114,-0.0831,0.0199,-0.0226,-0.0438,0.0401,-0.0673,0.0242,0.0358,-0.0539,0.036,0.02,0.0156,-0.024,-0.1082,0.0402,-0.0403,-0.0355,0.0475,-0.0488,0.0294,-0.0936,0.0437,0.022,-0.0727,-0.0488,0.0337,-0.0098,-0.018,0.1514,0.0552,-0.0142,0.082,-0.0052,0.0479,-0.0395,-0.0299,-0.0386,0.0835,-0.0165,0.0158,0.027,0.0318,0.0067,0.0515,0.0171,0.0538,-0.0553,-0.0156,-0.0043,-0.032,0.007,0.0445,0.0393,-0.2414,0.0583,0.0167,0.0501,-0.0208,-0.0073,0.0098,0.0298,0.0223,-0.0154,0.0126,0.0025,0.0347,-0.074,0.0052,0.0297,0.0172,-0.0666,0.0896,-0.0804,0.0006,0.0697,0.225,-0.0588,0.032,0.0108,0.0178,-0.0189,0.0189,-0.0473,0.0074,-0.0001,0.0649,-0.0398,0.0425,0.0982,-0.043,0.0316,-0.0027,-0.0107,0.0177,0.0049,-0.0353,-0.0285,0.0459,0.0305,-0.0481,-0.0539,-0.0004,0.0223,-0.0267,-0.0104,-0.0044,0.0211,0.0314,-0.0004,-0.0207,-0.0248,-0.0379,-0.014,0.0378,-0.0677,-0.0169,-0.0307,-0.0268]}
{"key":"[Lightweight Compositional Embeddings for Incremental Streaming Recommendation] Most work in graph-based recommender systems considers a {\\em static} setting where all information about test nodes (i.e., users and items) is available upfront at training time. However, this static setting makes little sense for many real-world applications where data comes in continuously as a stream of new edges and nodes, and one has to update model predictions incrementally to reflect the latest state. To fully capitalize on the newly available data in the stream, recent graph-based recommendation models would need to be repeatedly retrained, which is infeasible in practice. In this paper, we study the graph-based streaming recommendation setting and propose a compositional recommendation model -- Lightweight Compositional Embedding (LCE) -- that supports incremental updates under low computational cost. Instead of learning explicit embeddings for the full set of nodes, LCE learns explicit embeddings for only a subset of nodes and represents the other nodes {\\em implicitly}, through a composition function based on their interactions in the graph. This provides an effective, yet efficient, means to leverage streaming graph data when one node type (e.g., items) is more amenable to static representation. We conduct an extensive empirical study to compare LCE to a set of competitive baselines on three large-scale user-item recommendation datasets with interactions under a streaming setting. The results demonstrate the superior performance of LCE, showing that it achieves nearly skyline performance with significantly fewer parameters than alternative graph-based models.","layer":0,"vector":[-0.0449,-0.0116,0.0149,-0.0356,-0.0009,0.0709,-0.0114,0.0254,-0.0005,-0.032,0.0157,-0.042,0.0158,0.0538,0.0747,0.0324,0.0529,0.0428,-0.0251,-0.0516,0.0329,-0.0396,-0.0601,-0.0904,0.0235,0.0299,-0.0649,-0.0207,-0.0925,-0.2325,-0.0093,-0.0853,0.0763,-0.0293,-0.0433,-0.0691,0.0482,0.0138,-0.0072,0.0542,0.0158,0.017,-0.0439,-0.036,-0.032,-0.0239,0.0032,-0.0144,-0.0333,-0.009,0.009,-0.0414,0.0644,0.0289,0.0354,0.06,0.035,0.06,0.0115,0.0429,0.0387,-0.0095,-0.1208,0.0432,0.0394,0.0431,-0.0612,0.0495,-0.0167,0.0345,0.0194,0.0473,-0.0142,0.0301,0.013,0.0353,0.0287,-0.0124,-0.0481,0.0266,-0.0403,-0.0393,-0.0351,-0.0386,0.0237,-0.0464,0.0236,-0.0224,0.0169,0.0136,-0.0566,-0.0204,-0.0081,0.0359,-0.0413,-0.0519,0.0527,0.0339,-0.0709,0.2073,-0.043,0.0569,0.039,-0.0223,0.021,-0.0476,0.0258,-0.0116,0.0122,0.0078,-0.0142,0.0132,0.0353,-0.0899,0.0271,0.0297,0.0881,0.0494,0.0008,0.0107,-0.0134,0.0319,0.0685,-0.036,0.0678,-0.084,0.016,0.1087,0.0217,0.0417,0.0423,0.0417,-0.0599,-0.0218,-0.0014,0.0592,0.0177,-0.0734,0.0351,0.0279,-0.0162,0.0085,-0.0203,-0.0843,-0.0301,0.1534,-0.0473,-0.0182,-0.0588,-0.0246,-0.0225,-0.0081,-0.0233,-0.038,0.0063,-0.0032,0.0572,0.0441,-0.0852,0.0398,-0.0382,-0.0755,-0.0268,0.0468,0.0066,-0.0899,-0.057,-0.0014,-0.003,0.0005,0.0036,0.0363,-0.0125,0.0112,0.0889,0.0289,-0.0727,-0.0343,0.0484,-0.0125,0.0424,-0.0333,-0.0241,0.0788,0.0065,0.0077,-0.0277,-0.0571,0.0116,0.039,-0.0644,0.0057,0.0511,-0.0159,-0.0136,0.0148,-0.0256,-0.0459,0.0099,-0.0568,0.0335,0.0154,-0.0164,0.0108,-0.0338,0.0361,-0.0311,-0.0159,0.0399,0.0023,-0.0544,-0.0117,0.0226,-0.0122,-0.0256,0.019,0.0478,-0.0009,0.0516,0.0428,0.0333,-0.0804,-0.0529,-0.2126,-0.0199,0.0254,-0.0189,0.0196,-0.0576,0.0065,-0.0192,0.0329,0.0878,0.0286,-0.02,-0.0242,0.0262,0.011,0.054,0.0194,0.0227,0.0146,0.0068,-0.0085,0.0444,-0.0243,-0.0553,0.062,0.0045,0.2354,0.0466,0.0163,-0.0574,0.019,0.0566,-0.044,-0.1237,0.0349,0.0656,0.0605,-0.0294,-0.0422,-0.0321,-0.0481,0.0106,0.0187,-0.0993,-0.024,-0.0215,-0.0279,0.0291,-0.0537,0.0621,0.0427,0.0032,0.0429,0.0044,-0.0231,-0.0645,-0.0803,0.0527,0.0005,0.0357,0.0074,-0.0251,-0.0134,-0.0276,0.0203,-0.0199,0.0013,-0.0015,-0.0017,-0.0196,-0.0318,0.0334,-0.019,-0.0179,0.0537,0.0344,0.0629,-0.0087,-0.0261,-0.0224,0.0617,-0.0778,0.029,-0.0309,0.0337,0.0113,0.0519,0.001,0.0398,-0.0236,-0.0029,-0.0063,-0.0981,-0.0465,0.0885,-0.0547,-0.31,0.0324,-0.0066,0.0329,-0.0473,-0.0226,0.0438,0.0515,-0.0464,0.0444,0.0464,0.0362,0.0228,-0.0048,0.0074,0.0405,0.0699,-0.0238,0.0106,-0.0188,0.0305,0.0125,0.2374,0.0018,0.0408,0.0015,0.0029,-0.0204,0.0163,-0.0188,0.0022,-0.0224,0.0941,-0.0441,0.0198,0.0526,-0.0287,0.0153,0.0375,-0.0188,0.0162,-0.0398,-0.0482,-0.0093,0.011,0.0261,0.0068,-0.0339,-0.0011,0.0215,-0.0147,-0.0037,0.0023,0.0015,0.0405,0.0379,-0.0703,-0.0149,-0.038,-0.0499,0.007,-0.04,0.0039,-0.0167,0.0135]}
{"key":"[Fibro-CoSANet: Pulmonary Fibrosis Prognosis Prediction using a Convolutional Self Attention Network] Idiopathic pulmonary fibrosis (IPF) is a restrictive interstitial lung disease that causes lung function decline by lung tissue scarring. Although lung function decline is assessed by the forced vital capacity (FVC), determining the accurate progression of IPF remains a challenge. To address this challenge, we proposed Fibro-CoSANet, a novel end-to-end multi-modal learning-based approach, to predict the FVC decline. Fibro-CoSANet utilized CT images and demographic information in convolutional neural network frameworks with a stacked attention layer. Extensive experiments on the OSIC Pulmonary Fibrosis Progression Dataset demonstrated the superiority of our proposed Fibro-CoSANet by achieving the new state-of-the-art modified Laplace Log-Likelihood score of -6.68. This network may benefit research areas concerned with designing networks to improve the prognostic accuracy of IPF. The source-code for Fibro-CoSANet is available at: \\url{https://github.com/zabir-nabil/Fibro-CoSANet}.","layer":0,"vector":[-0.0661,-0.0018,0.0373,-0.0243,0.0253,0.072,0.0364,0.0299,0.0142,0.0025,-0.0069,-0.061,0.0111,0.0878,0.0121,0.0505,0.0095,0.0673,-0.0897,0.015,0.0198,0.0264,0.0237,-0.0732,0.0451,0.0082,-0.0323,-0.086,-0.0537,-0.2218,0.0486,-0.0285,0.0432,-0.0125,-0.0012,-0.064,-0.0198,0.0368,-0.0444,0.0552,-0.0277,0.0093,-0.0126,-0.0362,-0.0169,-0.069,-0.0115,-0.0393,0.0138,-0.0286,0.0367,-0.0316,0.0006,0.052,0.0286,0.0233,0.0403,0.025,0.0686,0.0029,0.0501,0.0165,-0.1838,0.0581,0.045,0.0397,-0.0425,-0.0492,0.008,0.0309,-0.0001,0.0559,0.0271,0.0375,0.0088,0.039,-0.008,0.0222,0.0177,0.0058,0.0555,0.0102,-0.0578,-0.0402,-0.0129,-0.0369,-0.0144,-0.075,-0.0061,0.027,-0.0555,-0.0108,-0.0467,0.0581,-0.0864,0.0441,0.0213,0.0388,-0.0746,0.1893,-0.0284,0.003,0.0397,-0.025,0.0159,0.0035,-0.0321,-0.0115,-0.0349,0.0381,-0.0092,-0.0366,0.0496,-0.0226,0.0158,0.0244,0.0666,0.0554,0.0505,0.0055,-0.0153,0.0043,0.0556,0.012,0.0116,-0.0371,0.034,0.1213,0.015,-0.0091,0.045,0.0209,-0.0624,-0.0316,-0.0182,0.0053,0.0289,0.0003,-0.0116,-0.0129,-0.0128,-0.0754,0.0275,-0.0837,-0.0486,0.1072,-0.0284,0.0317,0.0031,-0.0426,-0.0256,-0.0091,-0.0173,-0.0551,0.0019,0.0108,0.005,0.0295,-0.0206,0.0069,-0.0184,-0.0774,-0.0846,0.1116,0.026,-0.0702,-0.0242,0.0232,0.0238,-0.0166,0.0762,-0.034,-0.0047,0.0224,0.0739,0.0666,-0.0657,-0.0405,0.0146,0.0214,0.0163,-0.0393,0.0045,0.0306,0.0132,-0.0806,0.0054,-0.0867,0.0615,0.0358,-0.0513,0.0315,-0.0496,0.0236,-0.0152,-0.0312,-0.0429,0.0218,0.0181,-0.0231,0.0011,-0.0048,-0.0229,0.0045,-0.0288,0.0095,-0.0365,0.0587,0.0523,-0.015,-0.0401,0.0187,0.0911,-0.0243,-0.0542,-0.0038,0.0003,0.0025,-0.0202,0.052,0.0587,-0.0375,-0.0576,-0.2465,0.007,0.0205,-0.0254,0.0564,-0.0646,0.06,0.0081,0.055,0.0839,0.0796,0.0295,-0.0094,0.0131,0.0241,0.0103,0.0336,-0.0119,-0.0506,-0.0501,-0.0046,0.003,0.0201,-0.0732,0.0236,0.0246,0.2282,0.0092,0.0104,-0.0073,0.023,0.0328,-0.0753,-0.0723,0.037,-0.017,0.0529,-0.0232,-0.0657,-0.0164,-0.0459,0.0066,0.0125,-0.0908,-0.06,-0.0174,-0.0158,0.0735,-0.1035,0.0288,0.0528,-0.0435,0.0279,-0.0327,0.0177,-0.0224,-0.0719,-0.0036,-0.0744,-0.0068,-0.0412,-0.0605,0.0396,-0.0585,0.0073,-0.005,-0.0455,-0.0431,0.0156,-0.0507,-0.0389,0.0699,0.0104,-0.007,0.0626,-0.0087,0.0635,-0.0029,-0.0143,-0.0305,0.0981,-0.0403,0.0256,0.0036,0.0158,0.035,0.073,0.0099,0.0435,-0.0505,-0.0205,0.0369,-0.004,-0.067,0.0295,-0.0015,-0.3074,0.0673,0.0166,0.0201,-0.0219,0.0387,0.0722,0.0484,-0.0157,0.0309,0.0178,0.02,0.0344,-0.0518,-0.0039,-0.0095,0.0155,-0.0679,0.0509,-0.0587,-0.0155,-0.0104,0.188,-0.0814,0.0439,0.0058,-0.0404,-0.0026,-0.0025,-0.0051,0.0721,0.0062,0.0638,-0.0233,0.0122,0.0331,-0.0241,0.0477,0.0141,-0.0062,0.0278,0.0251,-0.0071,-0.0126,0.098,-0.0169,-0.0121,0.0103,-0.0157,0.0378,-0.009,0.0344,0.0154,0.0018,0.0038,0.0153,-0.0021,-0.0236,-0.0073,-0.0223,0.0278,-0.0528,-0.0379,0.0376,-0.0007]}
{"key":"[Unified Interpretation of Softmax Cross-Entropy and Negative Sampling: With Case Study for Knowledge Graph Embedding] In knowledge graph embedding, the theoretical relationship between the softmax cross-entropy and negative sampling loss functions has not been investigated. This makes it difficult to fairly compare the results of the two different loss functions. We attempted to solve this problem by using the Bregman divergence to provide a unified interpretation of the softmax cross-entropy and negative sampling loss functions. Under this interpretation, we can derive theoretical findings for fair comparison. Experimental results on the FB15k-237 and WN18RR datasets show that the theoretical findings are valid in practical settings.","layer":6,"vector":[-0.0112,-0.0246,0.0272,-0.0173,0.0311,0.0378,0.0203,0.026,0.0315,0.0008,0.0084,-0.0418,0.0734,0.054,0.0226,0.0268,0.0158,0.0288,-0.0515,-0.0169,0.0896,-0.0806,0.0098,-0.0868,0.0177,0.0128,-0.035,-0.0366,-0.0293,-0.2526,0.0331,-0.0734,0.0458,-0.0054,0.0025,-0.0325,-0.0416,0.0238,-0.037,0.0654,0.0594,0.0036,0.0267,-0.0198,0.0153,-0.0302,-0.032,-0.0103,-0.0264,-0.0382,0.0188,-0.0094,-0.0089,0.0266,0.0763,0.0674,0.0675,0.0255,0.0205,0.0579,0.0422,0.0376,-0.1884,0.0208,0.0328,0.0551,-0.0803,-0.0241,0.031,0.0688,0.0247,0.0106,0.002,0.0696,0.0289,0.0408,-0.0184,-0.0267,-0.0436,-0.0472,0.0287,-0.0404,-0.0353,-0.0082,0.0308,-0.0449,0.015,-0.063,0.0268,0.019,0.0095,-0.0055,-0.0081,0.0249,-0.0502,-0.0376,0.0397,0.0267,-0.0409,0.154,-0.0221,0.0081,0.0648,-0.0353,0.0252,-0.0894,-0.034,-0.0212,-0.0368,-0.0086,-0.0595,-0.0326,0.0072,-0.0462,0.0623,0.0279,0.0925,0.0447,-0.0143,-0.0152,-0.0457,0.0388,0.0368,-0.0155,0.0423,-0.0345,-0.0408,0.0937,0.0401,-0.0041,0.0238,-0.036,-0.0228,-0.0088,0.0034,0.0085,-0.0098,0.0236,0.0361,0.0046,-0.0031,-0.0199,0.0192,-0.0836,-0.1039,0.1132,-0.0725,-0.0051,-0.0494,-0.0177,0.0126,0.0,-0.0098,-0.046,0.023,0.028,0.0335,0.0672,-0.061,0.0106,0.0221,-0.0443,-0.0323,0.1121,0.0273,-0.087,0.0127,-0.0166,0.0244,-0.0306,0.0271,-0.0001,-0.0365,0.0243,0.1055,0.0221,-0.0478,-0.0156,0.0525,-0.0356,0.0112,-0.0473,-0.0466,0.04,0.0048,-0.0537,0.0049,-0.0457,0.0384,0.069,-0.0042,-0.0166,-0.0045,0.043,-0.0465,0.0203,-0.0001,-0.005,-0.0175,-0.0396,0.0053,0.0233,-0.0613,0.0221,-0.0479,0.0565,-0.0049,-0.0151,0.0523,0.0233,-0.032,-0.0276,0.0293,-0.0045,0.0137,0.0221,0.044,0.0427,0.0365,0.0488,0.0473,-0.0682,-0.0506,-0.2289,-0.0283,-0.0461,0.0028,0.0499,-0.0425,0.0241,0.039,0.0587,0.0705,0.0727,-0.0125,-0.0538,-0.0066,-0.0089,0.0753,0.078,0.0229,-0.0448,0.0388,-0.0243,0.041,0.0209,-0.0667,0.0376,-0.0313,0.1971,0.0204,0.0406,-0.001,0.0047,0.0063,-0.0354,-0.1595,0.0581,0.0162,0.031,-0.0207,-0.0113,-0.0343,-0.0558,-0.0158,-0.0327,-0.063,-0.0444,-0.049,-0.0526,-0.0126,-0.0765,0.0278,0.0286,-0.036,0.0561,0.0322,-0.0247,-0.0523,-0.083,0.0125,-0.0437,0.0045,0.0411,-0.0548,0.017,-0.0677,0.0221,-0.017,0.0068,-0.003,0.0169,-0.0205,-0.0178,0.094,0.0153,-0.0083,0.0248,0.0321,0.038,-0.0197,-0.0771,-0.0239,0.0737,-0.061,0.032,-0.0195,0.0274,0.0382,0.0794,-0.0309,-0.0198,-0.0391,0.0306,0.0116,-0.0553,-0.0393,0.0452,-0.0137,-0.2856,0.0404,0.0152,0.0349,-0.0478,-0.0223,0.053,-0.0115,-0.0318,-0.0197,0.0346,0.056,0.0221,-0.0137,-0.0199,0.0456,0.027,-0.0602,0.0667,-0.0363,0.0111,0.0373,0.2348,0.0141,0.0399,0.0069,-0.018,0.0087,0.0087,-0.013,0.0032,0.0153,0.0912,-0.0142,0.05,0.0938,-0.0356,0.0301,0.0538,0.0034,0.0169,-0.0409,-0.067,-0.0329,0.0935,-0.006,-0.0271,-0.0408,0.003,-0.0081,-0.016,0.0174,-0.0175,0.0084,0.026,0.0246,-0.0077,-0.0483,-0.0372,-0.0627,-0.0145,-0.0496,-0.0219,-0.0027,0.0101]}
{"key":"[A comparison of cluster algorithms as applied to unsupervised surveys] When considering answering important questions with data, unsupervised data offers extensive insight opportunity and unique challenges. This study considers student survey data with a specific goal of clustering students into like groups with underlying concept of identifying different poverty levels. Fuzzy logic is considered during the data cleaning and organizing phase helping to create a logical dependent variable for analysis comparison. Using multiple data reduction techniques, the survey was reduced and cleaned. Finally, multiple clustering techniques (k-means, k-modes, and hierarchical clustering) are applied and compared. Though each method has strengths, the goal was to identify which was most viable when applied to survey data and specifically when trying to identify the most impoverished students.","layer":0,"vector":[-0.0142,-0.002,0.0223,-0.005,0.0534,0.0252,0.0386,0.0259,0.0045,0.0034,0.024,-0.0822,0.0141,0.0199,0.0244,-0.0147,-0.0033,0.0561,-0.0439,0.0173,0.0027,-0.0363,-0.0553,-0.0381,0.0528,0.0292,-0.0249,-0.0457,-0.0465,-0.203,0.0256,-0.0376,0.0977,0.0036,-0.0142,-0.0043,-0.0182,0.0581,-0.0316,0.0106,0.0274,-0.0278,-0.0015,-0.0518,-0.0426,-0.0598,-0.0498,-0.0103,-0.0206,-0.0682,0.0218,-0.0206,0.0275,-0.0062,0.0222,0.0009,0.0232,-0.0046,0.0426,0.0361,0.0179,0.0493,-0.1863,0.0668,0.0627,0.0322,-0.0136,-0.0424,0.0412,0.0179,-0.0108,0.0533,0.0019,0.0729,0.0235,-0.0144,-0.0309,-0.0438,-0.0474,0.0103,-0.0227,0.009,-0.0146,0.0265,-0.0038,-0.0261,-0.0024,-0.0972,0.0412,-0.0163,-0.0117,0.0216,-0.0096,0.0537,-0.0556,-0.0529,-0.001,0.0155,-0.0479,0.2172,-0.0743,0.0129,0.0805,-0.0828,-0.0187,-0.0688,-0.0075,-0.0564,-0.0227,0.0091,-0.0136,-0.0185,-0.0069,-0.04,0.014,-0.0015,0.0338,0.0206,0.0074,-0.0048,-0.0107,0.0018,0.055,-0.0187,0.0425,-0.0534,-0.0025,0.0947,-0.0114,0.0128,0.0607,-0.041,-0.0791,-0.0215,-0.0362,0.0021,0.0212,0.0453,0.0318,-0.0338,-0.0095,-0.0742,0.0316,-0.1355,-0.0512,0.1234,-0.0512,0.0202,-0.032,-0.0154,-0.0247,-0.0083,-0.0693,-0.0586,-0.0046,0.0542,0.0212,0.025,-0.0174,0.023,-0.0184,-0.067,-0.022,0.0993,0.0379,-0.0684,-0.026,0.0056,-0.0035,-0.0142,0.0516,0.0567,-0.0264,0.0775,0.1139,-0.009,0.0088,0.0017,0.0381,0.013,0.029,0.0001,-0.0458,0.0146,0.0199,-0.0358,-0.0452,-0.0155,0.0217,0.0188,-0.0656,-0.0299,-0.0434,0.0008,-0.0629,-0.0216,0.0075,-0.0528,0.0201,-0.0403,0.0189,0.0197,-0.0426,0.0631,-0.0104,0.0454,0.0722,0.0026,0.1219,0.0366,0.001,0.0204,0.0319,-0.0161,-0.036,-0.0076,0.0593,0.0327,-0.0058,0.0305,0.0468,-0.0579,-0.0621,-0.1936,-0.028,0.0048,0.0087,0.0528,-0.0108,-0.0019,-0.0116,0.045,0.1097,0.0782,0.0115,-0.0251,0.0797,-0.0112,0.0187,0.0515,0.012,-0.0577,-0.0125,-0.0063,0.0193,0.0039,-0.0753,0.0273,0.0017,0.1917,0.031,0.0251,0.0011,0.0521,0.0048,-0.0296,-0.1044,0.0797,0.0294,-0.0103,-0.0194,-0.0279,-0.062,-0.0274,0.0465,0.0102,-0.0419,-0.021,-0.0256,-0.0047,0.0169,-0.0131,0.013,-0.0179,-0.0379,0.022,0.0129,-0.0162,-0.0347,-0.1274,0.0107,-0.006,0.0021,0.0376,-0.0629,0.0198,-0.0685,0.0399,-0.0081,-0.0513,0.0444,0.0251,-0.0501,-0.0515,0.1269,-0.022,0.0216,0.0232,0.03,0.0177,-0.037,-0.0173,-0.0097,0.1008,0.0094,-0.0047,0.0216,0.0179,0.0217,0.0579,0.0284,0.0402,-0.0006,0.0326,-0.0015,-0.0517,-0.023,-0.0299,0.0118,-0.2511,0.0082,-0.0048,-0.0083,-0.0372,-0.0129,0.0242,0.0363,0.0292,-0.0087,0.0376,0.0816,0.0695,-0.0673,0.0199,-0.0008,0.0494,-0.0581,0.0617,-0.0466,0.0435,0.0471,0.2247,-0.0654,0.0488,0.0064,-0.0053,0.0083,0.0398,-0.0325,0.0108,0.0133,0.1352,-0.0485,0.0189,0.0323,-0.0103,0.0209,0.0312,-0.012,-0.0633,0.0258,-0.0483,-0.0107,0.1363,0.0008,-0.0307,-0.1059,0.0189,0.0293,-0.0634,-0.0335,-0.0629,0.0127,0.0542,0.0727,-0.0772,0.0145,-0.0421,-0.0585,0.0016,-0.0187,-0.0218,-0.0049,0.0431]}
{"key":"[Stop Wasting My Gradients: Practical SVRG] We present and analyze several strategies for improving the performance of stochastic variance-reduced gradient (SVRG) methods. We first show that the convergence rate of these methods can be preserved under a decreasing sequence of errors in the control variate, and use this to derive variants of SVRG that use growing-batch strategies to reduce the number of gradient calculations required in the early iterations. We further (i) show how to exploit support vectors to reduce the number of gradient computations in the later iterations, (ii) prove that the commonly-used regularized SVRG iteration is justified and improves the convergence rate, (iii) consider alternate mini-batch selection strategies, and (iv) consider the generalization error of the method.","layer":4,"vector":[-0.0456,0.0179,0.0354,0.0228,0.0376,-0.0113,-0.0429,0.023,0.025,0.0209,0.0347,-0.022,0.0406,0.0672,0.0096,0.0158,-0.0031,0.0272,-0.0318,0.0016,0.0029,-0.0175,-0.0146,-0.0752,0.0351,-0.0142,-0.0539,-0.0751,-0.0506,-0.2782,0.0508,-0.0788,0.0608,-0.0142,-0.0093,0.0089,-0.0547,0.0686,-0.0645,0.0238,0.0023,0.0129,-0.0711,-0.0435,-0.0299,-0.0593,-0.0389,-0.0034,-0.0303,0.01,0.0198,-0.0322,0.0208,0.0154,0.0156,0.0322,0.0507,0.0222,0.0496,0.0288,0.0031,0.046,-0.1737,0.0493,0.039,0.0285,0.0025,-0.0125,0.0191,0.0701,-0.0427,0.0632,0.024,0.0705,0.0144,-0.0025,0.0031,-0.0277,0.0034,0.0344,0.0126,-0.0109,-0.033,-0.0356,-0.0185,-0.0142,0.0238,-0.0446,0.0726,-0.0108,-0.0064,-0.0298,-0.0097,0.0126,-0.0649,0.0028,0.0311,0.0366,-0.0289,0.1886,-0.0323,0.0574,0.0259,-0.0373,0.0083,-0.0241,-0.0486,-0.0064,0.0023,0.0215,-0.012,-0.0304,0.0352,-0.0584,-0.02,-0.0009,0.0473,0.0659,-0.0208,-0.0028,-0.0316,0.0014,0.0412,0.0136,0.0261,-0.0633,0.0256,0.1504,0.0128,0.0402,0.0576,-0.0077,-0.0458,-0.0556,0.0378,0.0252,0.0092,-0.0039,0.0228,-0.001,-0.0483,-0.0306,-0.0013,-0.1155,-0.026,0.1424,-0.0391,0.0353,-0.0048,-0.0622,0.038,0.0156,-0.011,-0.0167,-0.0011,0.0175,0.0051,0.0417,-0.0408,0.023,-0.036,-0.0465,-0.031,0.0783,0.0054,-0.0664,-0.0747,0.0194,-0.007,-0.0297,0.0535,0.0119,-0.0491,-0.001,0.0753,0.0424,-0.0765,0.0082,0.0057,0.0225,0.0098,-0.04,-0.0322,0.0544,0.0383,-0.051,0.0163,-0.044,-0.0101,0.0427,-0.0497,-0.0256,0.0049,-0.0045,-0.0238,-0.007,-0.0352,-0.0406,0.0438,-0.0048,0.0051,-0.01,-0.0527,0.0426,-0.0157,0.0247,-0.0343,-0.0416,0.0604,0.0631,-0.0362,-0.049,0.0688,-0.0396,-0.0366,0.03,0.0303,0.0579,-0.0027,0.0476,0.0416,-0.0145,-0.0714,-0.2103,-0.0441,0.007,-0.0117,0.046,-0.0771,0.0162,-0.0229,0.0599,0.0422,0.0263,0.0071,-0.0104,-0.0062,0.0274,0.0494,0.0382,-0.0068,-0.0004,-0.035,0.0217,0.0034,0.0025,-0.0615,0.0843,0.0027,0.1731,0.0349,0.087,-0.0366,0.0273,0.0089,0.0247,-0.0515,0.0688,0.0663,0.0784,-0.0222,-0.014,-0.0274,-0.0183,0.0641,0.0158,-0.1111,-0.0774,-0.0504,-0.0244,0.0451,-0.0516,-0.0383,0.0435,0.027,0.0495,-0.0507,0.0195,-0.0301,-0.0975,0.0025,-0.0267,0.0315,0.0026,-0.0817,0.0296,-0.0626,0.0577,0.0136,0.0099,-0.0154,0.0259,-0.0311,-0.0074,0.0781,-0.0119,0.0142,0.0259,0.0054,-0.0156,-0.0054,-0.0888,-0.0069,0.0404,-0.0108,0.0255,0.0755,0.0108,0.0366,0.1048,-0.0106,0.0126,-0.0114,-0.0304,0.0046,-0.0579,0.0298,0.023,-0.0409,-0.2897,0.0389,0.0258,0.0211,-0.031,0.0002,0.0705,-0.0364,-0.0564,0.0091,-0.0318,0.0385,0.0445,0.0401,0.0411,0.0135,0.0557,-0.0374,0.0714,-0.0877,0.0263,0.0335,0.1928,-0.0361,-0.0045,-0.009,-0.0044,-0.0058,0.0292,-0.0405,-0.0055,-0.0051,0.0971,-0.0633,0.0551,0.0659,-0.0492,0.0375,0.0443,-0.0515,-0.0007,-0.0021,-0.0213,-0.0237,0.1283,-0.0616,-0.0195,-0.0132,0.0321,0.0414,-0.0497,0.045,-0.0432,-0.0468,0.0358,0.0326,-0.0525,-0.0477,-0.0319,-0.0181,0.0359,-0.091,-0.0432,0.0169,-0.0188]}
{"key":"[How To Not Drive: Learning Driving Constraints from Demonstration] We propose a new scheme to learn motion planning constraints from human driving trajectories. Behavioral and motion planning are the key components in an autonomous driving system. The behavioral planning is responsible for high-level decision making required to follow traffic rules and interact with other road participants. The motion planner role is to generate feasible, safe trajectories for a self-driving vehicle to follow. The trajectories are generated through an optimization scheme to optimize a cost function based on metrics related to smoothness, movability, and comfort, and subject to a set of constraints derived from the planned behavior, safety considerations, and feasibility. A common practice is to manually design the cost function and constraints. Recent work has investigated learning the cost function from human driving demonstrations. While effective, the practical application of such approaches is still questionable in autonomous driving. In contrast, this paper focuses on learning driving constraints, which can be used as an add-on module to existing autonomous driving solutions. To learn the constraint, the planning problem is formulated as a constrained Markov Decision Process, whose elements are assumed to be known except the constraints. The constraints are then learned by learning the distribution of expert trajectories and estimating the probability of optimal trajectories belonging to the learned distribution. The proposed scheme is evaluated using NGSIM dataset, yielding less than 1\\% collision rate and out of road maneuvers when the learned constraints is used in an optimization-based motion planner.","layer":6,"vector":[-0.0423,-0.0015,0.0472,-0.0204,0.0007,0.0876,0.054,0.0408,0.0155,-0.0013,0.0212,-0.0485,0.0224,0.0612,0.0031,0.0125,-0.0059,0.0521,-0.0044,-0.003,0.0152,-0.0514,-0.0129,-0.0632,0.0044,0.0564,0.0017,-0.0253,-0.0247,-0.1838,-0.0016,-0.0516,0.0224,-0.0322,-0.019,-0.0167,-0.0735,0.1055,0.0093,-0.0056,0.0313,0.0187,-0.0057,-0.0584,-0.025,-0.0316,-0.0093,-0.0398,-0.015,-0.0285,-0.019,-0.0205,0.0046,0.0033,0.0143,0.0038,0.0695,0.028,0.0355,0.0241,0.0107,0.03,-0.1698,0.0879,0.0386,0.0508,-0.0497,-0.013,0.0421,0.0319,-0.0264,0.0367,0.0041,0.0421,0.0142,-0.0242,0.0152,-0.0195,-0.0059,-0.0246,-0.0329,-0.0333,-0.0634,-0.0022,-0.0284,-0.0737,0.0229,-0.0581,0.0197,0.027,-0.0358,-0.0202,-0.038,-0.0143,-0.0565,-0.0026,0.0233,0.0014,-0.058,0.1975,-0.0301,0.0694,0.0242,-0.02,-0.0062,-0.0107,-0.038,-0.0148,-0.0325,-0.0007,-0.0301,0.0104,0.0486,-0.0081,0.0137,0.0599,0.0506,0.0454,0.0075,-0.0169,-0.0226,-0.0139,0.0836,-0.0189,0.0223,-0.1022,0.0137,0.1351,0.021,0.0036,0.0552,-0.0612,-0.0355,0.0047,0.0409,-0.0066,-0.0047,-0.0097,-0.0239,-0.0213,-0.0358,-0.0487,0.0098,-0.1088,-0.0498,0.1105,-0.027,0.0425,-0.008,-0.0174,-0.0006,-0.0297,-0.0418,-0.033,-0.023,0.0383,0.0427,0.0657,-0.0677,0.0285,-0.0314,-0.0679,-0.0013,0.1274,-0.0103,-0.0761,-0.0076,0.0328,0.0428,-0.015,0.008,0.0628,-0.0669,0.043,0.115,0.0463,-0.0581,0.0218,-0.0387,0.0289,0.0039,-0.0472,-0.0237,-0.0215,0.0582,-0.0011,-0.0391,-0.0227,-0.0056,0.032,0.005,0.0118,-0.0181,-0.0122,-0.0676,-0.0136,-0.0008,-0.0045,0.0339,-0.0229,-0.0163,-0.0064,-0.0313,0.0338,-0.0127,-0.0196,-0.0204,-0.0012,0.0565,0.0241,-0.069,-0.0085,0.049,-0.0049,-0.0505,0.0025,0.0023,0.0363,-0.0088,0.0248,0.0125,0.0101,-0.027,-0.2241,0.0137,0.0123,0.0194,0.0306,-0.0364,-0.0021,-0.018,0.0242,0.0635,0.0758,-0.0744,-0.0415,0.0059,-0.0094,0.0296,-0.0139,0.0411,-0.0456,0.0506,0.0113,-0.002,-0.0082,-0.0894,0.0374,0.0281,0.2309,-0.0048,0.073,-0.035,0.0503,0.0309,-0.0347,-0.0719,0.0431,0.0053,0.0522,-0.0289,-0.0406,-0.0571,0.006,-0.0205,-0.042,-0.0555,-0.0431,-0.0136,-0.0417,0.0855,-0.0286,0.0046,0.0622,-0.0418,0.0575,-0.0538,-0.0263,-0.0378,-0.0584,0.026,-0.0104,-0.0008,0.0139,-0.0442,0.0109,-0.0843,0.0666,-0.0066,0.0009,-0.0404,0.0554,0.0004,-0.0156,0.1022,0.0312,-0.0107,0.0411,0.0063,0.0223,-0.0183,-0.0675,-0.0704,0.0529,-0.0098,0.0649,0.0461,0.0211,-0.0161,0.0607,-0.0486,0.0149,-0.031,0.0444,0.0166,0.005,-0.002,0.1029,0.012,-0.2881,0.0295,0.0158,0.0493,-0.0223,0.0137,0.0856,-0.0122,-0.0502,-0.0171,-0.0103,0.0512,0.0147,0.0438,0.0589,0.0051,0.0648,-0.003,0.0522,-0.1026,0.0082,0.0606,0.2303,-0.0711,0.0508,0.0623,-0.039,-0.0271,0.0759,-0.0538,-0.0062,-0.0511,0.079,-0.081,0.0382,0.0783,-0.046,0.0462,0.0189,0.0367,-0.0063,0.0287,0.0147,-0.0248,0.0895,0.0016,-0.0236,-0.0604,0.0127,0.0355,-0.0224,0.0119,-0.0173,-0.0043,0.022,-0.0109,-0.0155,-0.0908,-0.0112,-0.0798,0.0488,-0.0584,0.0161,-0.0369,-0.0186]}
{"key":"[Causal Discovery in Heterogeneous Environments Under the Sparse Mechanism Shift Hypothesis] Machine learning approaches commonly rely on the assumption of independent and identically distributed (i.i.d.) data. In reality, however, this assumption is almost always violated due to distribution shifts between environments. Although valuable learning signals can be provided by heterogeneous data from changing distributions, it is also known that learning under arbitrary (adversarial) changes is impossible. Causality provides a useful framework for modeling distribution shifts, since causal models encode both observational and interventional distributions. In this work, we explore the sparse mechanism shift hypothesis, which posits that distribution shifts occur due to a small number of changing causal conditionals. Motivated by this idea, we apply it to learning causal structure from heterogeneous environments, where i.i.d. data only allows for learning an equivalence class of graphs without restrictive assumptions. We propose the Mechanism Shift Score (MSS), a score-based approach amenable to various empirical estimators, which provably identifies the entire causal structure with high probability if the sparse mechanism shift hypothesis holds. Empirically, we verify behavior predicted by the theory and compare multiple estimators and score functions to identify the best approaches in practice. Compared to other methods, we show how MSS bridges a gap by both being nonparametric as well as explicitly leveraging sparse changes.","layer":0,"vector":[-0.0081,-0.0061,0.0278,0.0041,0.0776,0.0194,0.0323,0.0175,0.064,-0.009,0.0577,-0.0119,0.026,0.0802,0.0132,0.033,-0.0312,0.0316,-0.0407,-0.0137,-0.003,-0.0623,0.042,-0.0175,0.0346,0.0565,0.0051,-0.0406,-0.0444,-0.2614,0.0157,-0.0582,0.031,-0.0232,0.0392,-0.0223,-0.0581,0.0221,-0.0038,0.0628,0.0299,0.0455,-0.0348,-0.0644,-0.0199,-0.0631,0.0139,0.0032,-0.0461,-0.0251,-0.0092,-0.0386,0.0346,0.0518,0.0445,0.0176,0.0348,0.0822,0.0503,0.0641,0.0145,0.0836,-0.1872,0.0496,0.1156,0.0457,-0.0439,-0.0128,0.0254,0.0492,0.021,0.0467,0.0049,-0.0101,0.0187,-0.0083,-0.0046,-0.0208,-0.0011,-0.0094,0.0531,-0.0061,-0.0389,0.0018,-0.0269,-0.0645,0.0393,-0.0784,0.0448,-0.0011,-0.0628,0.0163,-0.0054,0.0255,-0.0484,-0.013,0.04,0.0206,-0.026,0.1865,-0.0563,0.0478,-0.0279,-0.0055,0.0325,-0.055,-0.0046,-0.0449,-0.0228,0.0012,0.0017,0.0042,0.038,-0.0582,0.0006,-0.0197,0.0434,0.0419,-0.0084,-0.012,-0.0162,0.0199,0.0326,-0.0419,0.0278,-0.055,0.0065,0.1533,0.0406,-0.0254,0.0288,-0.06,-0.0422,0.0071,0.0155,0.027,0.0133,0.0143,0.0065,0.0076,-0.0383,-0.0102,0.0316,-0.0874,-0.0974,0.1651,0.0004,0.017,-0.0415,-0.0124,-0.0251,0.0383,-0.0394,-0.0547,-0.0075,0.0615,-0.0046,0.0282,-0.0236,0.043,-0.0496,-0.0632,-0.0519,0.0857,0.0073,-0.0717,-0.0259,0.0145,0.0036,-0.0211,0.0231,0.0707,-0.0183,0.0227,0.0415,0.016,-0.0698,0.017,-0.0158,0.001,0.0404,-0.0439,-0.024,0.0956,0.0615,0.0072,0.0119,-0.0374,0.0229,0.0205,0.0022,0.0165,-0.0313,-0.0012,-0.0087,-0.004,0.0124,0.0011,-0.011,0.0037,-0.0225,-0.0117,-0.0592,-0.0076,-0.0563,-0.0002,-0.0146,-0.0273,0.0375,0.0269,-0.0149,0.0181,0.0435,-0.0121,-0.005,0.017,0.0095,0.0316,-0.0054,0.0363,-0.0158,-0.0773,0.0008,-0.2241,-0.0635,0.0078,-0.0153,0.0184,-0.0484,0.0439,-0.0242,0.0781,0.0981,0.0587,-0.0081,-0.0365,0.0097,-0.0174,0.0293,0.013,0.0448,-0.0231,0.0148,-0.0203,-0.0105,-0.0021,-0.0997,0.0437,0.0392,0.1939,0.0435,-0.0094,0.0014,0.0091,0.0513,-0.0144,-0.1134,0.0727,0.0281,0.0343,-0.0072,-0.0501,-0.0743,-0.0154,0.0084,-0.0079,-0.1007,-0.0302,-0.0316,-0.0029,0.0326,-0.0399,0.0674,0.0094,-0.0434,0.0526,-0.0158,-0.0122,-0.039,-0.0631,0.0442,-0.0496,0.0031,-0.0143,-0.0355,0.0151,-0.0865,0.0529,0.001,0.0018,-0.0535,0.0102,-0.028,0.0132,0.0801,-0.0083,0.0065,0.0476,-0.0007,0.0294,-0.0354,-0.0741,-0.0511,0.0826,-0.0454,0.0082,0.014,-0.03,-0.0395,0.0834,-0.0132,0.0617,0.0072,0.018,0.0187,-0.0742,0.0003,0.0667,-0.0248,-0.2805,0.0431,0.0097,-0.001,-0.0069,0.0217,0.0416,0.0097,-0.0655,-0.0107,-0.0044,0.0315,0.0285,0.0022,-0.0218,0.0307,0.0604,-0.0679,0.0422,-0.1079,0.011,0.0383,0.2241,-0.0316,0.0119,0.0401,-0.022,0.0255,-0.0107,0.0059,0.01,0.0355,0.0371,-0.0649,0.0277,0.0512,-0.0412,0.0523,0.0438,-0.0594,0.0002,0.004,-0.0642,-0.0344,0.1146,0.0051,-0.0141,-0.0703,0.024,0.0264,-0.0205,-0.0063,-0.0231,0.025,-0.0079,-0.0112,-0.0393,-0.0185,-0.0142,-0.0733,-0.0084,-0.0091,-0.0419,-0.0041,-0.0251]}
{"key":"[An Ensemble Approach toward Automated Variable Selection for Network Anomaly Detection] While variable selection is essential to optimize the learning complexity by prioritizing features, automating the selection process is preferred since it requires laborious efforts with intensive analysis otherwise. However, it is not an easy task to enable the automation due to several reasons. First, selection techniques often need a condition to terminate the reduction process, for example, by using a threshold or the number of features to stop, and searching an adequate stopping condition is highly challenging. Second, it is uncertain that the reduced variable set would work well; our preliminary experimental result shows that well-known selection techniques produce different sets of variables as a result of reduction (even with the same termination condition), and it is hard to estimate which of them would work the best in future testing. In this paper, we demonstrate the potential power of our approach to the automation of selection process that incorporates well-known selection methods identifying important variables. Our experimental results with two public network traffic data (UNSW-NB15 and IDS2017) show that our proposed method identifies a small number of core variables, with which it is possible to approximate the performance to the one with the entire variables.","layer":1,"vector":[-0.0849,0.0132,0.0206,0.0104,0.0832,0.0123,0.069,0.0717,0.0331,-0.0535,0.0112,-0.0153,0.0147,0.061,-0.003,-0.0036,-0.0075,0.0101,-0.0382,0.0292,-0.0136,-0.0111,-0.0224,-0.0669,0.0337,0.0286,0.0077,-0.0266,-0.0455,-0.227,0.0053,-0.0576,0.0543,-0.0439,0.0343,-0.0219,-0.0571,0.0212,-0.0091,0.0713,0.0123,-0.0011,-0.0348,-0.0634,-0.0311,-0.0885,0.0283,0.0175,-0.0281,-0.0255,0.0412,-0.0464,0.0342,0.0553,0.0011,0.0142,0.0275,0.0505,0.0458,0.0452,0.0305,0.069,-0.1525,0.0311,0.0779,0.0243,-0.024,0.0014,0.0143,0.0487,0.0051,0.0896,-0.0353,0.0468,-0.0043,0.0264,0.017,-0.0144,-0.0045,0.0373,0.0065,-0.0154,-0.0289,0.0017,0.0024,-0.0541,-0.0208,-0.0462,0.0822,-0.0524,-0.0152,0.0156,0.02,0.0292,-0.0287,-0.0322,0.0043,0.0001,-0.0146,0.1832,-0.0431,-0.0091,-0.0119,-0.0178,0.0378,-0.042,-0.028,-0.0461,0.0014,0.0103,-0.0032,-0.0571,0.0107,-0.054,0.0231,0.0389,0.0556,0.0468,-0.0353,0.0079,0.006,-0.0139,0.099,-0.0465,0.0045,-0.0675,0.0236,0.1212,-0.0196,0.0094,0.0293,0.0094,-0.0679,-0.0451,0.0027,0.0268,0.0293,0.0615,-0.0541,-0.0209,-0.0397,-0.0454,0.0346,-0.0928,-0.0095,0.109,-0.0718,0.0153,-0.016,-0.0482,-0.0315,-0.0173,-0.0704,-0.0449,0.0058,0.0575,0.0359,0.0517,-0.0373,0.0276,-0.0259,-0.0632,-0.0218,0.0969,-0.0148,-0.1068,-0.0482,0.0038,-0.0166,-0.0076,-0.0212,0.0371,-0.072,0.0506,0.058,-0.011,-0.0452,0.0344,0.0087,0.0012,0.0039,-0.0577,-0.0253,0.041,0.0771,-0.0184,-0.0446,-0.0238,0.0431,0.0199,-0.0406,0.0023,0.0109,-0.0271,-0.0271,-0.0012,0.0052,-0.0185,0.0638,-0.0541,0.0271,0.0096,-0.0138,-0.0206,-0.0642,0.0536,-0.0294,-0.0043,0.0378,0.0507,-0.0346,-0.0073,0.0554,0.01,-0.0491,-0.0154,0.0051,0.0775,0.0419,0.0332,0.0319,0.0109,-0.0555,-0.2557,-0.0254,0.0317,-0.0194,0.0615,-0.0748,0.0416,-0.0017,0.038,0.0643,0.0434,0.0016,-0.0655,0.0391,-0.0329,0.0551,0.0099,0.0234,-0.0362,0.0366,-0.0247,-0.001,-0.054,-0.0628,0.038,0.0357,0.1618,0.0342,0.0383,-0.0699,0.0372,0.0054,-0.022,-0.0609,0.0636,0.0558,0.051,0.0085,-0.0494,-0.0308,-0.0166,0.0832,-0.0187,-0.0818,-0.0254,-0.0264,0.0094,0.0245,-0.0422,0.0035,0.0908,-0.0148,0.0615,-0.0071,0.0465,-0.0525,-0.0938,0.0326,-0.0185,0.0046,0.0102,-0.0538,0.0165,-0.0883,0.0408,0.0246,-0.0543,-0.0179,0.039,0.0039,-0.0388,0.0957,0.0258,-0.0167,0.0069,0.01,0.0022,-0.0428,-0.0237,-0.0501,0.0737,-0.035,0.0291,-0.0173,0.0137,0.0169,0.0863,0.0217,0.03,-0.0189,0.0021,0.0067,-0.0391,-0.0249,0.0303,-0.0119,-0.2922,0.0504,-0.0027,0.0092,0.0203,0.0051,0.0305,0.014,-0.0112,-0.0179,0.0139,-0.0169,0.0261,-0.0086,0.0177,0.0667,0.0545,-0.0404,0.0103,-0.0495,0.0203,0.0566,0.2415,-0.0185,0.0601,0.0393,-0.0576,0.0099,0.0094,-0.0336,0.0222,0.0122,0.0963,-0.0816,0.0302,0.0373,-0.0415,-0.0053,-0.0296,-0.0472,0.0088,0.0181,-0.0584,-0.017,0.1358,-0.0281,-0.0506,-0.0569,0.0423,0.0506,-0.0121,-0.0374,-0.0555,0.007,0.0478,0.0503,-0.0403,-0.0183,-0.0478,-0.0412,0.0107,-0.0372,0.0297,-0.0059,-0.0378]}
{"key":"[Variational Bayesian Sequence-to-Sequence Networks for Memory-Efficient Sign Language Translation] Memory-efficient continuous Sign Language Translation is a significant challenge for the development of assisted technologies with real-time applicability for the deaf. In this work, we introduce a paradigm of designing recurrent deep networks whereby the output of the recurrent layer is derived from appropriate arguments from nonparametric statistics. A novel variational Bayesian sequence-to-sequence network architecture is proposed that consists of a) a full Gaussian posterior distribution for data-driven memory compression and b) a nonparametric Indian Buffet Process prior for regularization applied on the Gated Recurrent Unit non-gate weights. We dub our approach Stick-Breaking Recurrent network and show that it can achieve a substantial weight compression without diminishing modeling performance.","layer":0,"vector":[-0.0749,-0.0355,-0.0034,-0.0227,-0.0447,0.0602,0.0141,0.0121,0.0583,-0.0351,-0.0188,-0.0494,0.0494,0.0445,0.0659,-0.0009,0.0045,0.022,-0.0034,0.016,0.057,-0.019,0.0148,-0.0176,0.0116,0.0254,-0.0145,-0.0072,-0.0435,-0.2441,-0.0187,-0.0517,0.0182,-0.025,0.0029,-0.01,-0.081,0.047,-0.0166,0.0641,0.0046,0.0443,-0.0071,-0.0587,0.03,-0.0516,-0.0177,-0.0238,-0.0121,-0.0561,0.0464,-0.0474,0.0218,0.0525,0.006,0.031,0.0499,0.037,0.0596,0.0482,0.0261,0.0372,-0.1791,0.0945,0.0205,0.0033,-0.0163,-0.0202,-0.002,0.0373,-0.0553,-0.0016,-0.0045,0.0396,0.0024,0.03,-0.0234,-0.0406,-0.0293,-0.0218,0.0217,-0.0271,-0.0235,-0.0224,-0.0597,-0.0405,-0.0181,-0.0811,-0.0202,-0.0011,-0.0715,0.009,-0.021,0.0168,-0.0758,-0.0227,0.0158,0.0442,-0.06,0.2042,0.0021,0.0177,0.0166,-0.0197,0.0511,-0.0294,-0.0051,-0.0122,-0.0579,0.0249,-0.0202,-0.0043,0.0336,-0.0379,0.0324,-0.0104,0.0692,0.0403,0.0169,0.0144,0.0134,0.0258,0.0152,-0.0348,0.0225,-0.0671,0.027,0.1229,0.0672,0.057,0.0886,-0.0413,-0.0685,-0.0251,0.0799,-0.0009,0.0262,-0.0148,0.023,-0.0152,-0.007,-0.0482,-0.0057,-0.0494,-0.0688,0.1201,-0.0579,0.0226,-0.0812,0.0004,-0.0044,0.0022,0.0174,-0.0307,0.0427,0.0171,0.0306,0.0282,-0.0733,0.0243,0.0034,-0.0553,-0.0135,0.0699,0.0367,-0.1056,-0.072,-0.0546,0.0276,-0.0346,0.0646,0.0208,0.0128,0.0363,0.051,0.0532,-0.0518,-0.0161,-0.0238,0.0379,-0.0305,-0.0264,-0.0228,0.0269,-0.0094,-0.004,0.024,-0.0336,0.0343,0.0282,0.0122,0.0138,-0.0396,-0.0263,-0.0051,-0.0449,-0.0152,-0.0127,0.0233,-0.0011,0.0063,-0.0145,-0.0529,0.0112,0.0387,0.0287,-0.0342,0.0079,0.0598,0.0531,-0.0157,0.004,0.054,0.0038,-0.0464,0.0238,0.0112,0.0422,0.0268,0.0378,0.0189,-0.0736,-0.0567,-0.2502,0.0395,0.0422,-0.0286,0.081,-0.0228,0.0108,0.0187,0.1113,0.0947,0.0344,-0.0425,-0.0402,0.0343,0.0056,0.0244,0.0039,0.0233,0.014,-0.0003,-0.0188,-0.0238,-0.0222,-0.0681,0.0411,-0.0029,0.2,0.0093,0.0574,-0.0631,0.0089,0.0441,-0.0158,-0.079,0.0636,0.0283,0.0559,0.0403,-0.0077,-0.0446,-0.0135,-0.0075,-0.0316,-0.0932,-0.0813,-0.0515,-0.0566,-0.0008,-0.0494,0.0106,0.0552,-0.0238,0.0319,0.0149,-0.0418,-0.0486,-0.0754,0.0122,-0.0631,0.0356,0.0192,-0.0239,0.0008,-0.0051,0.0322,0.0131,-0.0042,-0.0545,0.004,0.0045,-0.0565,0.0957,0.0156,0.0013,0.0709,0.0042,0.0185,-0.0657,-0.0856,-0.0434,0.0653,-0.0358,0.0418,0.0127,-0.0004,0.0465,0.1082,0.0122,0.0105,0.0295,0.018,0.01,-0.0119,-0.0157,0.0053,-0.0265,-0.294,0.0616,-0.0123,0.0337,-0.016,0.0282,0.0552,0.0137,-0.0593,0.0222,-0.0342,0.0222,0.068,0.0045,-0.0079,-0.0122,0.1037,-0.0363,0.0622,-0.0858,-0.0303,0.0268,0.1989,-0.0087,-0.0013,-0.0175,-0.0305,0.0105,0.0497,-0.0352,-0.0145,-0.0043,0.0786,-0.0064,0.0337,0.0883,-0.0399,0.0806,0.0021,-0.0139,0.0398,0.0105,-0.0065,-0.0394,0.1007,-0.0259,0.0152,-0.0342,-0.0011,0.0308,-0.0167,0.0178,0.0045,0.0022,-0.0338,0.024,-0.0366,-0.0475,-0.0174,-0.0398,0.0208,-0.0617,-0.0424,0.016,-0.0208]}
{"key":"[Kernel-Guided Training of Implicit Generative Models with Stability Guarantees] Modern implicit generative models such as generative adversarial networks (GANs) are generally known to suffer from issues such as instability, uninterpretability, and difficulty in assessing their performance. If we see these implicit models as dynamical systems, some of these issues are caused by being unable to control their behavior in a meaningful way during the course of training. In this work, we propose a theoretically grounded method to guide the training trajectories of GANs by augmenting the GAN loss function with a kernel-based regularization term that controls local and global discrepancies between the model and true distributions. This control signal allows us to inject prior knowledge into the model. We provide theoretical guarantees on the stability of the resulting dynamical system and demonstrate different aspects of it via a wide range of experiments.","layer":1,"vector":[-0.022,-0.0418,0.0487,-0.0224,-0.0176,0.0376,-0.0157,-0.043,0.0043,-0.0065,-0.0301,-0.0362,0.0687,0.0654,0.0022,0.0137,0.0326,0.032,-0.0403,0.0083,0.0492,-0.0446,0.0301,-0.0152,-0.0169,0.0076,-0.0007,-0.0538,-0.0197,-0.2562,0.0318,-0.0676,0.0292,-0.0344,-0.0095,-0.0075,-0.0737,0.0649,-0.0023,0.0608,0.0039,0.0303,-0.045,-0.0732,-0.0095,0.0138,-0.0469,-0.0012,-0.0145,-0.0472,0.0202,-0.0422,0.0145,0.0227,0.0305,0.0361,0.1107,0.0676,0.0743,0.0624,0.0134,0.0453,-0.1527,0.0561,0.0187,0.0158,-0.0239,-0.0238,0.0176,0.0608,-0.0257,0.028,0.0011,0.0308,0.0182,-0.0276,0.0298,-0.0458,-0.0439,0.031,0.044,0.0076,-0.026,-0.0231,0.0084,-0.0207,0.0244,-0.0409,0.0286,0.0282,-0.0352,-0.0323,-0.0439,0.0297,-0.0458,-0.0194,0.0369,0.0342,-0.0769,0.1998,-0.0616,0.0297,0.0599,0.0015,0.0037,-0.013,-0.0601,-0.02,-0.0449,-0.0099,-0.0121,0.0147,0.0265,-0.0469,0.0458,-0.0121,0.0476,0.0499,-0.0484,-0.0376,-0.029,0.0679,0.0364,-0.0192,0.0633,-0.0636,0.0148,0.1423,0.0758,0.0225,0.0405,0.0085,-0.0665,0.0054,-0.0201,0.0229,-0.0111,-0.0216,0.0075,0.0051,-0.0176,-0.0197,-0.0224,-0.044,-0.0185,0.1042,-0.0165,0.0289,-0.0198,-0.0203,-0.0162,0.0009,-0.0383,-0.0294,0.0084,0.0593,0.0135,0.0616,-0.0535,0.015,-0.0482,-0.0355,-0.0626,0.0788,-0.0468,-0.052,-0.0063,-0.0065,0.0243,-0.0019,0.0139,-0.003,-0.0171,0.003,0.0576,0.0442,-0.0994,0.0209,-0.0393,0.0329,0.0348,-0.0691,0.0193,0.0667,0.0112,-0.0241,-0.0045,-0.0588,0.0363,0.0341,-0.0113,0.0183,-0.0439,-0.0065,-0.0016,-0.0469,-0.034,-0.0193,0.0466,-0.0336,-0.0055,0.0343,-0.0277,-0.0326,-0.0547,0.0447,-0.0275,-0.0088,0.0322,0.0323,-0.0781,0.0129,0.058,-0.0333,-0.0434,0.0137,-0.0482,0.0266,-0.0253,0.0395,0.0496,-0.0296,0.001,-0.25,0.0093,-0.0026,-0.0312,0.0496,-0.0814,0.0579,-0.0265,0.0331,0.0473,0.0198,0.006,0.027,0.029,-0.0178,0.0617,0.0102,0.0387,0.0155,-0.0236,-0.0138,0.0109,-0.0094,-0.12,0.0434,0.0116,0.202,0.0213,0.0793,-0.046,0.0191,0.0659,-0.0395,-0.0791,0.0698,0.038,0.059,-0.0243,-0.0455,-0.0207,0.022,0.0218,0.0003,-0.1192,-0.0277,-0.0185,-0.0368,0.0373,-0.0178,0.0211,0.0444,-0.0392,0.0672,-0.025,-0.011,-0.0497,-0.1164,0.0458,-0.0455,0.038,0.0108,-0.0824,0.0319,-0.0591,0.0713,0.0315,-0.0122,-0.0462,0.0673,-0.0328,0.0122,0.0602,0.0087,0.0156,0.0696,0.0005,-0.017,-0.017,-0.0918,-0.0163,0.0497,0.0,0.0592,0.0266,0.0391,-0.0239,0.0812,-0.0155,0.0486,-0.0436,-0.0463,0.0503,-0.0796,-0.0098,0.064,-0.0321,-0.2957,0.0394,0.0166,0.0715,-0.032,0.0221,0.0479,0.0159,-0.0462,0.0037,-0.0132,0.0479,0.0483,-0.0285,0.0244,0.0159,0.081,-0.076,0.0304,-0.0541,0.0296,0.056,0.1967,-0.0518,-0.0121,-0.0411,-0.0077,-0.0005,0.0338,-0.0133,0.0012,0.03,0.0796,-0.0082,0.0295,0.0706,-0.0718,0.0275,0.0037,-0.0205,-0.0188,0.014,-0.012,0.0254,0.0322,-0.0054,-0.0199,0.0156,-0.0072,0.009,-0.022,0.0151,-0.0316,0.0134,0.0521,0.0198,-0.0401,-0.0668,-0.0398,-0.0225,0.0242,-0.0491,-0.0226,-0.0253,-0.0273]}
{"key":"[Multicalibrated Partitions for Importance Weights] The ratio between the probability that two distributions $R$ and $P$ give to points $x$ are known as importance weights or propensity scores and play a fundamental role in many different fields, most notably, statistics and machine learning. Among its applications, importance weights are central to domain adaptation, anomaly detection, and estimations of various divergences such as the KL divergence. We consider the common setting where $R$ and $P$ are only given through samples from each distribution. The vast literature on estimating importance weights is either heuristic, or makes strong assumptions about $R$ and $P$ or on the importance weights themselves. In this paper, we explore a computational perspective to the estimation of importance weights, which factors in the limitations and possibilities obtainable with bounded computational resources. We significantly strengthen previous work that use the MaxEntropy approach, that define the importance weights based on a distribution $Q$ closest to $P$, that looks the same as $R$ on every set $C \\in \\mathcal{C}$, where $\\mathcal{C}$ may be a huge collection of sets. We show that the MaxEntropy approach may fail to assign high average scores to sets $C \\in \\mathcal{C}$, even when the average of ground truth weights for the set is evidently large. We similarly show that it may overestimate the average scores to sets $C \\in \\mathcal{C}$. We therefore formulate Sandwiching bounds as a notion of set-wise accuracy for importance weights. We study these bounds to show that they capture natural completeness and soundness requirements from the weights. We present an efficient algorithm that under standard learnability assumptions computes weights which satisfy these bounds. Our techniques rely on a new notion of multicalibrated partitions of the domain of the distributions, which appear to be useful objects in their own right.","layer":8,"vector":[-0.0383,-0.0285,0.0473,-0.062,0.0195,0.0403,0.0487,0.0438,0.0619,0.0056,0.0089,-0.0396,0.0085,0.0604,0.025,0.0765,0.0085,0.0527,-0.0685,0.0099,0.0479,-0.026,-0.0229,-0.0397,0.043,-0.0073,-0.0789,-0.0335,-0.0398,-0.2535,0.0164,-0.0228,0.0477,-0.0416,0.0173,-0.0329,-0.0471,0.0282,-0.0062,0.0089,0.0527,-0.0108,-0.0209,-0.0721,-0.0374,-0.0697,0.0018,-0.0023,-0.0344,-0.0145,0.0059,-0.0344,0.0508,0.0405,0.0545,0.0269,0.0217,0.0207,0.0734,0.0605,0.001,0.0436,-0.1611,-0.0121,0.056,-0.0011,-0.0299,-0.0929,-0.004,0.0679,-0.0217,0.0515,0.0294,0.0963,0.0453,0.0069,0.0077,-0.0323,-0.0723,0.0425,-0.0186,-0.0422,-0.0278,0.013,-0.0376,-0.0087,0.0177,-0.0305,0.0166,0.0017,-0.0352,-0.0314,-0.0495,0.0286,-0.0585,-0.0091,0.0087,0.0146,-0.0257,0.2184,-0.0084,0.0157,0.0123,-0.013,0.0711,-0.0399,0.0265,-0.026,-0.0099,-0.0515,0.0033,-0.0111,0.0054,-0.027,0.0251,-0.0057,0.0495,0.0478,-0.0195,-0.027,-0.0272,-0.0006,0.07,-0.0047,0.0632,-0.0294,0.0056,0.1154,0.0234,0.0082,0.0183,-0.0224,-0.0505,-0.0358,0.0205,0.012,0.0115,-0.0062,0.0312,0.0095,-0.0402,-0.0306,0.0484,-0.0706,-0.0707,0.1763,-0.0536,0.0094,-0.066,-0.0568,-0.0156,0.0006,-0.0425,-0.047,0.041,0.0314,0.0151,-0.0014,-0.0517,0.016,-0.0115,-0.0689,0.0003,0.1064,-0.0027,-0.0242,-0.0063,-0.0058,0.0062,-0.0112,0.0413,0.0595,0.0271,0.0383,0.0763,-0.0286,-0.104,-0.0272,0.0514,0.0041,0.0112,-0.0412,-0.0604,0.0713,0.0068,-0.0227,-0.0138,-0.0558,0.031,0.0638,-0.0012,-0.0355,-0.0392,0.0015,-0.0242,0.0021,-0.0353,-0.0114,-0.0042,-0.0571,-0.0315,-0.0104,-0.024,0.0418,-0.0203,0.0075,-0.0175,0.0209,0.0285,0.0279,0.0033,-0.0042,0.0553,-0.0184,-0.0215,-0.03,0.0951,0.0538,0.0031,0.0361,0.0414,-0.0717,-0.0513,-0.2437,-0.0249,-0.0132,0.0045,0.0483,-0.06,0.0581,0.0084,0.0349,0.0825,0.0791,-0.0161,-0.0189,0.0567,0.0018,0.0416,0.0088,0.0039,-0.0248,0.0074,-0.0187,0.0164,-0.0405,-0.064,0.0416,-0.0034,0.2074,0.0238,0.0071,-0.0231,-0.0175,-0.0089,-0.0055,-0.0697,0.0449,0.0365,0.0302,-0.0077,0.001,0.0389,-0.0369,0.0225,-0.0205,-0.0843,-0.0285,-0.0372,-0.0409,0.0657,-0.0364,0.0296,0.0431,0.0142,0.0646,-0.0153,0.0219,-0.0376,-0.0741,0.0373,-0.0493,0.0631,0.0213,-0.0599,0.0629,-0.0422,0.0281,-0.0568,-0.0483,-0.055,0.0018,-0.0352,-0.0223,0.031,-0.0083,-0.0382,0.0316,-0.0148,0.0852,-0.0233,-0.0193,-0.0348,0.0508,-0.0676,0.01,0.006,0.0522,0.0122,0.0846,0.0226,-0.002,0.0269,-0.0067,0.0225,-0.0394,0.0024,0.0144,-0.0003,-0.3107,0.0453,-0.0313,-0.0096,-0.0267,0.0096,0.0483,0.018,-0.0616,0.0107,0.0404,0.0714,0.0328,-0.0383,-0.0201,0.0268,0.0443,0.0024,0.0649,-0.0308,-0.0054,0.033,0.2375,-0.0198,0.0026,0.0096,-0.0051,-0.0017,-0.0064,0.0057,0.0083,0.0016,0.0337,-0.0698,0.0304,0.1082,-0.0091,0.0241,-0.0053,-0.0192,0.0109,-0.0272,-0.0389,-0.0239,0.106,-0.0091,0.0105,-0.0613,0.0362,0.0123,-0.0373,0.0011,-0.0033,-0.0327,0.0136,-0.0048,-0.0154,-0.0126,-0.0495,-0.0482,0.0241,-0.027,-0.02,0.0097,0.0125]}
{"key":"[Conditional Noise-Contrastive Estimation of Unnormalised Models] Many parametric statistical models are not properly normalised and only specified up to an intractable partition function, which renders parameter estimation difficult. Examples of unnormalised models are Gibbs distributions, Markov random fields, and neural network models in unsupervised deep learning. In previous work, the estimation principle called noise-contrastive estimation (NCE) was introduced where unnormalised models are estimated by learning to distinguish between data and auxiliary noise. An open question is how to best choose the auxiliary noise distribution. We here propose a new method that addresses this issue. The proposed method shares with NCE the idea of formulating density estimation as a supervised learning problem but in contrast to NCE, the proposed method leverages the observed data when generating noise samples. The noise can thus be generated in a semi-automated manner. We first present the underlying theory of the new method, show that score matching emerges as a limiting case, validate the method on continuous and discrete valued synthetic data, and show that we can expect an improved performance compared to NCE when the data lie in a lower-dimensional manifold. Then we demonstrate its applicability in unsupervised deep learning by estimating a four-layer neural image model.","layer":0,"vector":[-0.0582,-0.0255,0.0075,-0.0032,0.053,0.0394,0.0509,0.0101,0.0617,0.0058,0.0319,-0.0733,0.0066,0.0572,0.0054,0.008,-0.0047,0.0538,-0.0312,0.0123,0.0172,-0.0405,0.0114,-0.037,0.0437,-0.0243,-0.0346,-0.0334,-0.0336,-0.2674,0.016,-0.0498,0.0535,-0.0195,0.0439,-0.0693,-0.0214,0.0533,0.0092,0.0361,0.0102,0.0072,-0.0072,-0.0882,-0.0222,-0.0772,-0.0249,-0.0563,-0.0059,-0.0412,0.0376,-0.0453,0.0433,0.0176,0.0255,0.0362,0.0863,0.0454,0.076,0.0689,-0.0065,0.0696,-0.1461,0.0167,0.0419,-0.0005,-0.0492,-0.0696,0.0185,0.0339,0.0209,0.0391,0.0282,0.0565,0.015,-0.0154,-0.0224,-0.0878,-0.0332,0.0229,0.012,0.001,-0.0226,-0.0393,-0.0327,-0.0246,0.0285,-0.0381,0.0084,-0.0428,-0.04,0.0085,-0.0882,0.0529,-0.0396,-0.0177,0.0121,0.0088,-0.0047,0.2064,-0.0743,0.0545,0.0202,-0.028,0.0058,-0.0245,-0.0395,0.0023,-0.0274,-0.0309,0.0004,-0.0334,0.0208,-0.0711,0.0135,0.0261,0.0612,0.0426,0.0342,-0.007,-0.0615,-0.0591,0.0454,-0.0335,0.0065,-0.0097,-0.0158,0.1203,0.0221,0.0064,0.0522,-0.0397,-0.0631,-0.0126,0.0125,-0.0193,0.0296,0.0202,0.0325,0.0224,-0.0341,-0.0994,0.0211,-0.0651,-0.0842,0.1344,-0.0572,0.0378,-0.0421,-0.0321,-0.0139,0.0062,-0.0378,-0.0003,0.0545,0.0521,0.005,0.0159,-0.0389,0.0152,-0.045,-0.0831,-0.0038,0.0915,-0.0245,-0.0379,-0.0542,0.0149,0.0371,0.0024,0.0128,0.0685,-0.0232,0.0618,0.0652,0.0215,-0.056,-0.0021,0.0076,0.0294,0.0118,-0.043,-0.067,0.0277,0.0151,-0.0427,0.0113,-0.0393,0.0576,0.0347,-0.0224,0.0167,-0.027,0.0023,-0.0359,-0.0111,-0.0188,-0.0168,0.0168,-0.0377,0.0157,0.0218,-0.0093,-0.004,-0.0142,-0.0072,-0.0197,-0.0109,0.0269,0.0397,-0.0067,-0.0416,0.0665,0.0086,-0.0211,-0.008,0.0463,0.0319,-0.012,0.0362,0.0023,-0.0475,-0.0527,-0.2515,0.0084,0.0128,-0.0229,0.0592,-0.0617,0.0112,-0.0056,0.0585,0.0921,0.0685,-0.0075,-0.0011,0.0688,0.0221,0.039,0.0379,0.0387,-0.0024,-0.012,-0.0029,0.0068,-0.0478,-0.069,0.0699,-0.0358,0.2169,0.0095,0.0238,-0.0126,0.031,-0.0043,-0.017,-0.1008,0.0345,0.0391,0.0594,0.0041,-0.0641,-0.0358,-0.028,0.0127,0.0417,-0.0684,-0.0278,-0.0557,-0.0447,-0.0064,-0.0734,0.0224,-0.0004,-0.0185,0.0699,-0.0267,-0.0222,-0.0604,-0.092,0.0061,-0.0492,0.0201,0.0316,-0.0606,0.0207,-0.0511,0.0215,-0.0085,-0.0098,-0.0702,0.0255,-0.0117,-0.0195,0.0816,-0.002,0.0267,0.0679,0.0236,0.0371,-0.0116,-0.0715,-0.0056,0.0748,-0.0028,0.058,0.0393,0.0386,0.0207,0.084,0.0232,0.0205,-0.0406,0.0318,0.0095,-0.0488,0.0106,0.0556,0.0165,-0.2663,-0.0124,0.0264,0.039,0.0275,-0.0244,0.0167,0.0265,-0.0552,-0.0143,-0.0151,0.0688,0.0508,-0.0079,-0.0156,0.0261,0.05,-0.0766,0.1069,-0.0353,0.0055,0.0255,0.1822,-0.0468,0.0125,0.0254,-0.041,0.0386,0.0026,-0.0432,0.0355,0.0625,0.0924,-0.0376,0.0344,0.1228,-0.0159,0.0006,0.0073,-0.0243,0.0079,-0.0062,-0.0258,-0.0098,0.1041,-0.0206,0.0068,0.0,-0.0303,0.0205,-0.0331,0.0123,0.0026,0.0322,0.0526,0.0226,-0.0693,-0.0345,0.0107,-0.029,-0.0105,-0.0613,-0.0175,0.0038,-0.05]}
{"key":"[Are Powerful Graph Neural Nets Necessary? A Dissection on Graph Classification] Graph Neural Nets (GNNs) have received increasing attentions, partially due to their superior performance in many node and graph classification tasks. However, there is a lack of understanding on what they are learning and how sophisticated the learned graph functions are. In this work, we propose a dissection of GNNs on graph classification into two parts: 1) the graph filtering, where graph-based neighbor aggregations are performed, and 2) the set function, where a set of hidden node features are composed for prediction. To study the importance of both parts, we propose to linearize them separately. We first linearize the graph filtering function, resulting Graph Feature Network (GFN), which is a simple lightweight neural net defined on a \\textit{set} of graph augmented features. Further linearization of GFN's set function results in Graph Linear Network (GLN), which is a linear function. Empirically we perform evaluations on common graph classification benchmarks. To our surprise, we find that, despite the simplification, GFN could match or exceed the best accuracies produced by recently proposed GNNs (with a fraction of computation cost), while GLN underperforms significantly. Our results demonstrate the importance of non-linear set function, and suggest that linear graph filtering with non-linear set function is an efficient and powerful scheme for modeling existing graph classification benchmarks.","layer":4,"vector":[0.0128,-0.0239,-0.0132,-0.0105,0.0421,0.0282,0.0333,0.0437,0.0253,-0.0268,0.035,-0.016,0.0854,0.0656,0.0243,0.0106,0.0365,0.0742,-0.0519,0.01,0.0224,-0.0033,-0.0159,-0.0641,0.0529,0.0249,-0.0193,-0.0084,-0.0628,-0.2375,0.0108,-0.0624,0.0558,-0.0347,0.0233,-0.0437,0.0262,0.0221,-0.0505,0.0342,0.018,-0.0126,-0.0212,-0.0559,-0.0161,-0.0141,0.0167,-0.048,-0.0443,-0.0356,0.0291,-0.0565,0.0456,0.026,0.0304,0.0564,0.0308,0.017,0.0332,0.0509,0.0293,0.031,-0.1455,0.0469,0.0375,0.0356,-0.0285,-0.0176,0.0276,0.096,0.0297,0.0493,0.0087,0.0034,-0.0013,-0.0042,-0.0108,-0.0267,-0.022,0.0018,0.0145,-0.0213,-0.0361,-0.0318,0.0568,-0.0002,0.0042,-0.0574,0.0147,0.0398,-0.0342,-0.0169,-0.0203,-0.0057,-0.0564,-0.0085,0.0655,0.0309,-0.0806,0.1894,-0.049,0.0121,0.0247,-0.034,0.0294,-0.0515,0.0123,-0.0517,-0.0511,0.0054,-0.0323,-0.0304,-0.0213,-0.0416,0.0176,-0.0216,0.0726,0.0787,-0.0166,-0.0117,-0.0366,0.0059,0.045,-0.0431,0.0522,-0.0316,0.0012,0.1032,0.0318,0.0377,0.0567,0.0194,-0.0271,-0.0069,-0.0149,0.0123,0.0503,0.0151,-0.0261,-0.0148,-0.0274,0.0066,0.0104,-0.0607,-0.0992,0.1078,-0.0456,-0.045,-0.0262,-0.0453,-0.0388,0.0171,-0.0249,-0.0208,-0.0196,0.0402,0.0416,0.0556,-0.0881,0.0123,-0.0119,-0.0133,-0.0679,0.0892,0.0419,-0.1167,-0.0346,-0.0356,-0.0496,-0.0314,0.032,0.0639,-0.0402,0.0099,0.0789,0.0363,-0.0705,-0.0547,-0.0095,-0.0195,0.0209,-0.007,-0.0615,0.023,0.0389,0.0027,0.0024,-0.0305,-0.0078,0.0568,-0.0623,0.0319,0.0164,0.0042,-0.0378,-0.0293,-0.0253,-0.0427,-0.0509,-0.0689,0.022,-0.0146,0.0272,0.0276,-0.0397,0.05,-0.0455,-0.0027,0.0174,0.0065,-0.0592,0.0107,0.0364,-0.0496,-0.0487,-0.0135,0.0247,0.0198,0.0082,0.0317,0.0621,-0.0414,-0.0609,-0.1969,-0.0266,0.0309,-0.0261,0.0908,-0.074,-0.0228,0.0317,0.0571,0.0838,0.059,0.0285,-0.0452,0.0063,0.0186,0.0803,0.0376,0.0401,-0.0457,-0.0212,-0.0034,0.0174,0.0001,-0.0771,0.0195,0.0518,0.213,-0.0282,0.0615,-0.0384,0.0176,0.0443,-0.0478,-0.0788,0.0581,0.0719,0.0364,-0.0144,-0.0556,-0.0294,-0.0167,-0.009,-0.0086,-0.1292,-0.0189,0.0104,-0.0231,0.0139,-0.0524,0.004,0.0611,-0.0121,0.0607,0.0288,0.0041,-0.0492,-0.0662,0.039,-0.0374,0.0274,0.0027,-0.0879,0.0127,-0.0567,0.0944,0.038,-0.045,-0.0295,-0.0095,0.0225,-0.0282,0.0781,0.0619,-0.0063,0.0239,-0.0042,0.0413,-0.0138,-0.0428,-0.0003,0.0557,-0.0466,0.032,-0.0024,0.0345,0.0257,0.0836,-0.0068,0.038,-0.0129,0.0016,0.0278,-0.0323,-0.0511,0.0781,-0.0395,-0.3003,0.0481,0.0582,0.0688,-0.0043,0.0049,0.0511,0.05,-0.0293,-0.0011,0.0204,0.0393,0.0587,-0.0521,-0.0282,0.0364,0.0291,-0.0326,0.0452,-0.0293,0.0604,0.0502,0.2168,-0.0227,0.0677,0.0384,-0.0027,-0.0477,0.007,-0.017,0.0107,0.0111,0.0791,-0.0776,0.0479,0.0672,-0.0438,0.0237,0.0014,-0.0295,0.0064,-0.038,-0.0633,-0.0148,0.0548,-0.02,-0.0476,-0.0455,0.023,0.0218,-0.0549,-0.0005,-0.0011,0.0201,0.0251,0.0109,-0.0348,-0.0253,-0.0697,-0.0725,0.0111,-0.0744,0.0213,0.011,-0.0268]}
{"key":"[Characterizing Speech Adversarial Examples Using Self-Attention U-Net Enhancement] Recent studies have highlighted adversarial examples as ubiquitous threats to the deep neural network (DNN) based speech recognition systems. In this work, we present a U-Net based attention model, U-Net$_{At}$, to enhance adversarial speech signals. Specifically, we evaluate the model performance by interpretable speech recognition metrics and discuss the model performance by the augmented adversarial training. Our experiments show that our proposed U-Net$_{At}$ improves the perceptual evaluation of speech quality (PESQ) from 1.13 to 2.78, speech transmission index (STI) from 0.65 to 0.75, short-term objective intelligibility (STOI) from 0.83 to 0.96 on the task of speech enhancement with adversarial speech examples. We conduct experiments on the automatic speech recognition (ASR) task with adversarial audio attacks. We find that (i) temporal features learned by the attention network are capable of enhancing the robustness of DNN based ASR models; (ii) the generalization power of DNN based ASR model could be enhanced by applying adversarial training with an additive adversarial data augmentation. The ASR metric on word-error-rates (WERs) shows that there is an absolute 2.22 $\\%$ decrease under gradient-based perturbation, and an absolute 2.03 $\\%$ decrease, under evolutionary-optimized perturbation, which suggests that our enhancement models with adversarial training can further secure a resilient ASR system.","layer":3,"vector":[-0.0697,-0.0372,0.0352,-0.0291,0.0113,0.0348,0.0314,0.0073,-0.0001,0.0049,-0.0006,-0.0263,0.0674,0.0754,0.0425,0.0523,0.0265,0.0231,-0.0821,0.0045,0.022,0.0158,0.0295,-0.0012,0.0047,-0.0027,-0.0659,-0.0248,-0.0351,-0.2478,0.0484,-0.0557,0.0193,-0.0193,-0.0026,-0.0325,-0.0294,0.0523,-0.0243,0.0278,0.03,0.0539,-0.0286,-0.0675,-0.0339,-0.0568,-0.0255,-0.0318,-0.0346,-0.061,0.0358,-0.042,0.0299,0.021,0.0145,-0.0225,0.0918,0.0256,0.0175,0.0323,0.0136,0.0519,-0.2,0.0617,0.0363,0.0686,-0.0165,-0.017,-0.0043,0.0161,-0.0028,-0.0038,0.0356,0.0129,-0.0234,0.0447,0.02,-0.0535,0.0628,0.0213,0.0559,0.0001,-0.0259,-0.034,0.0167,-0.0502,0.017,-0.0191,0.0233,-0.0224,-0.0552,0.0006,-0.038,0.0322,-0.0017,-0.0138,0.0265,-0.023,-0.0693,0.1855,-0.026,-0.0046,0.0063,-0.0565,0.0613,-0.0121,-0.0215,-0.0394,-0.0146,0.0534,-0.0213,-0.0,0.0178,-0.004,0.0711,0.0298,0.0584,0.0025,-0.0247,-0.0112,-0.0237,-0.0127,0.0108,-0.0262,0.0372,-0.0575,0.0599,0.1441,0.0272,0.0534,0.0615,-0.0381,-0.0058,0.0094,0.0254,0.0025,-0.0077,0.0057,0.0159,0.0058,-0.0342,-0.0649,-0.0214,-0.0806,-0.0374,0.1103,-0.0787,0.0206,-0.0365,-0.0339,-0.0217,0.0019,0.0043,-0.0426,0.0016,0.0611,0.0447,0.0058,-0.0263,-0.0021,-0.0232,-0.0455,-0.0192,0.0875,0.0018,-0.084,-0.048,-0.0166,0.0029,-0.0419,0.006,-0.007,-0.0438,0.0426,0.0506,0.0494,-0.084,-0.012,-0.0525,0.0186,0.0036,-0.0916,-0.0262,0.0316,0.0248,-0.0507,0.048,-0.0544,0.0231,0.0513,-0.0801,0.028,-0.0439,0.0147,-0.0581,-0.0433,0.0059,-0.0017,-0.0196,-0.0164,0.0094,-0.0095,-0.019,-0.004,0.0164,0.0476,-0.0272,0.0237,0.014,0.0066,-0.049,0.0126,0.0599,0.0065,-0.0572,-0.0268,-0.0158,0.0445,-0.0254,0.0197,0.0409,-0.0301,-0.0563,-0.2341,-0.0209,0.0133,-0.062,0.0596,-0.115,0.0404,0.014,0.0711,0.0556,0.0211,0.0228,0.0101,0.0269,0.0234,0.0469,-0.016,0.0137,-0.0013,0.0164,-0.016,0.0555,0.0099,-0.0513,0.0214,-0.0153,0.2026,-0.0022,0.0568,-0.0264,0.0115,0.058,-0.0014,-0.1277,0.0456,0.0171,0.0854,-0.0082,-0.0452,-0.0327,-0.0667,0.0219,0.0147,-0.1034,-0.0525,0.0003,-0.014,0.0126,-0.0711,0.035,0.0536,0.0061,0.0894,0.0536,0.0118,-0.054,-0.1137,0.0137,-0.0349,0.0391,-0.0288,-0.0645,0.0204,-0.0703,0.0213,0.0317,-0.0535,-0.0485,0.0139,0.0076,-0.041,0.0909,0.0399,0.0306,0.0066,-0.0176,0.0284,-0.0485,-0.0711,-0.0285,0.0299,0.0081,0.0362,-0.0274,0.07,0.0397,0.0747,0.0366,0.0181,0.0059,-0.0322,0.0259,-0.0041,-0.0641,0.0508,-0.003,-0.2911,0.0216,0.0303,0.0618,-0.0275,0.0013,0.0223,0.029,-0.0901,0.0177,-0.0208,0.0635,0.0226,-0.087,-0.014,0.054,0.0678,-0.0085,0.0596,-0.0298,0.0312,0.0538,0.1777,-0.012,0.0199,-0.0296,-0.0408,0.0438,0.018,-0.0681,0.0215,0.0397,0.0834,-0.0366,-0.0124,0.0676,-0.0241,0.0422,0.0109,0.0087,-0.0086,0.0092,-0.0432,0.01,0.0823,0.0029,0.0239,-0.05,-0.0067,0.0524,0.0159,0.0155,0.0381,0.0066,0.0599,0.0459,-0.0582,-0.0244,-0.0008,-0.012,0.0432,-0.0436,-0.03,-0.0072,-0.019]}
{"key":"[Implicit Maximum Likelihood Estimation] Implicit probabilistic models are models defined naturally in terms of a sampling procedure and often induces a likelihood function that cannot be expressed explicitly. We develop a simple method for estimating parameters in implicit models that does not require knowledge of the form of the likelihood function or any derived quantities, but can be shown to be equivalent to maximizing likelihood under some conditions. Our result holds in the non-asymptotic parametric setting, where both the capacity of the model and the number of data examples are finite. We also demonstrate encouraging experimental results.","layer":3,"vector":[-0.0233,0.0179,0.0412,-0.032,0.0178,0.0206,0.0144,0.0473,0.036,-0.0194,0.026,-0.0674,0.0176,0.0448,-0.0096,0.019,-0.0112,0.0588,-0.0637,0.0613,0.0525,-0.0483,-0.0386,-0.0253,0.0245,0.0557,-0.0076,-0.0055,-0.0146,-0.249,-0.0048,-0.0636,0.0468,-0.0284,-0.0102,-0.0273,-0.0666,0.0222,-0.0121,0.0645,-0.0021,0.0049,-0.0617,-0.0431,-0.0137,-0.0232,-0.0209,-0.0239,-0.01,-0.0426,0.0319,-0.0064,0.0374,-0.0157,0.0585,0.0354,0.0835,0.0048,0.0586,0.0332,0.0094,0.0292,-0.1541,0.0659,0.0303,0.0408,-0.0247,-0.0596,0.0334,0.0511,-0.0257,0.069,-0.009,0.0456,0.0005,-0.0595,0.012,-0.0185,0.0234,-0.0127,-0.017,-0.0412,-0.0346,0.0118,-0.0386,-0.058,0.0142,-0.0185,0.0468,0.0064,-0.048,0.0138,-0.0731,0.0101,-0.0749,-0.0185,0.0401,-0.0095,-0.0098,0.1907,-0.0324,0.0278,0.0166,-0.0173,0.0231,-0.0327,-0.0253,-0.0217,-0.0187,0.0003,-0.0116,0.0149,0.0134,-0.0758,0.022,-0.0024,0.0684,0.0035,0.0017,-0.0186,-0.0418,0.0076,0.0479,-0.0261,0.0066,-0.048,0.0305,0.1227,0.1049,0.0266,0.0458,-0.0405,-0.0637,0.0073,-0.0022,-0.0004,0.0604,0.0236,0.0205,0.0408,-0.0378,-0.102,-0.0003,-0.0932,-0.0662,0.1246,-0.0058,0.0394,-0.0987,-0.028,-0.0101,0.056,0.0247,-0.0405,0.0703,0.0083,-0.0004,0.0666,-0.0649,0.0391,-0.0579,-0.0429,0.0449,0.0785,-0.0239,-0.0036,-0.0276,0.0074,-0.026,0.0039,0.0557,0.0235,-0.0691,0.061,0.0565,0.0284,-0.0806,-0.012,0.0179,0.0656,-0.0076,-0.0623,-0.0003,0.0658,0.0069,-0.0174,0.0003,-0.0805,0.0501,0.0402,-0.0286,0.0204,-0.0093,-0.0223,-0.0032,-0.0143,0.0233,-0.0018,0.0292,-0.0584,0.0024,0.009,-0.0376,-0.0341,-0.0105,0.0591,-0.0406,0.0552,0.0477,0.0159,-0.0189,-0.0028,0.0585,0.0035,-0.0266,0.0043,-0.0043,0.0166,0.0237,0.084,0.0342,-0.0765,-0.0106,-0.2345,-0.0123,0.0565,-0.0372,0.0746,-0.0585,0.0163,0.0147,0.0425,0.1128,0.039,-0.0085,-0.0125,0.0597,-0.0087,0.0431,-0.0189,-0.0157,-0.0107,-0.0196,-0.0339,0.0082,-0.0738,-0.0557,0.0576,-0.0041,0.185,-0.0142,0.0333,-0.0468,-0.0101,-0.0157,-0.0049,-0.0789,0.055,0.0176,0.0415,0.0143,-0.0224,-0.0108,-0.0422,0.0097,-0.0187,-0.073,-0.0669,-0.0411,-0.0187,0.0332,-0.0391,0.0444,0.0136,-0.0107,0.0669,-0.0331,-0.0099,-0.0641,-0.0805,0.0351,-0.0267,0.0634,0.0101,-0.0386,0.0219,-0.0784,0.0566,-0.0234,-0.0552,-0.0436,0.0226,-0.0468,-0.0071,0.0982,0.008,0.0179,0.0546,0.0468,0.0218,-0.0425,-0.0737,-0.037,0.0541,-0.022,0.0005,0.0502,0.0185,0.0159,0.0596,-0.0144,0.0525,0.0037,-0.0024,-0.0189,-0.0333,0.0015,0.0362,-0.0154,-0.288,0.0249,0.0277,0.0153,-0.0276,0.0037,0.0152,-0.0037,-0.0654,-0.0337,0.0108,0.0655,0.039,0.0015,0.0163,0.046,0.0348,-0.0213,0.0182,-0.0741,0.0112,0.015,0.2286,-0.0511,0.0151,0.0303,-0.0321,0.0288,0.0527,-0.0244,0.0157,0.0127,0.0955,-0.0794,0.0405,0.0749,-0.0449,0.0444,0.005,-0.0344,-0.0205,-0.0274,-0.0319,-0.0412,0.1213,-0.0262,-0.0661,-0.0158,0.0008,0.0275,-0.0386,-0.0268,0.0062,0.0275,0.0779,-0.0106,-0.0628,-0.0261,-0.0282,-0.0307,0.0338,-0.0465,-0.0147,0.0064,0.0161]}
{"key":"[A fast and simple modification of Newton's method helping to avoid saddle points] We propose in this paper New Q-Newton's method. The update rule is very simple conceptually, for example $x_{n+1}=x_n-w_n$ where $w_n=pr_{A_n,+}(v_n)-pr_{A_n,-}(v_n)$, with $A_n=\\nabla ^2f(x_n)+\\delta _n||\\nabla f(x_n)||^2.Id$ and $v_n=A_n^{-1}.\\nabla f(x_n)$. Here $\\delta _n$ is an appropriate real number so that $A_n$ is invertible, and $pr_{A_n,\\pm}$ are projections to the vector subspaces generated by eigenvectors of positive (correspondingly negative) eigenvalues of $A_n$. The main result of this paper roughly says that if $f$ is $C^3$ (can be unbounded from below) and a sequence $\\{x_n\\}$, constructed by the New Q-Newton's method from a random initial point $x_0$, {\\bf converges}, then the limit point is a critical point and is not a saddle point, and the convergence rate is the same as that of Newton's method. The first author has recently been successful incorporating Backtracking line search to New Q-Newton's method, thus resolving the convergence guarantee issue observed for some (non-smooth) cost functions. An application to quickly finding zeros of a univariate meromorphic function will be discussed. Various experiments are performed, against well known algorithms such as BFGS and Adaptive Cubic Regularization are presented.","layer":3,"vector":[-0.1229,0.0209,0.0337,0.0375,-0.0238,0.0417,-0.0167,0.0134,0.0467,0.019,0.0085,-0.0759,0.0251,0.0079,0.0152,0.0147,0.062,0.056,-0.0159,0.0386,0.0275,-0.0167,-0.0154,-0.0446,0.0641,-0.0103,-0.0359,-0.0427,-0.0399,-0.2518,0.0172,-0.0415,0.0688,-0.0313,0.0238,-0.0154,-0.0061,0.0662,0.0103,0.0376,0.0488,0.0212,-0.0522,-0.0402,-0.0505,-0.0648,-0.0314,-0.0422,0.0069,-0.042,0.0032,0.0085,0.0297,0.0289,0.0338,0.0315,0.0098,-0.0095,0.0423,0.0347,0.0636,0.0035,-0.179,0.0547,0.0403,0.0034,0.0124,-0.0449,0.007,0.1125,-0.0358,0.035,0.0049,0.0164,0.0356,-0.0372,0.0011,-0.0062,-0.0089,-0.0013,0.0559,-0.0621,-0.0566,0.0334,-0.0206,-0.0012,0.0112,-0.065,0.0576,0.0218,-0.023,0.0179,-0.035,0.0214,-0.0865,-0.0608,-0.0013,0.0227,-0.0225,0.2004,-0.0189,0.0176,0.0496,-0.0642,-0.0021,-0.0044,-0.0716,-0.0779,-0.0111,0.0091,0.0048,-0.0311,0.0359,-0.0739,-0.0498,-0.0133,0.0274,0.0179,-0.0428,-0.0038,-0.0398,0.0338,0.0786,-0.0147,0.0599,-0.0488,0.0318,0.1069,0.0192,0.0732,0.0524,-0.0017,-0.0576,-0.0296,-0.0525,0.0389,-0.0084,0.0139,0.057,-0.0285,-0.0151,-0.103,-0.0201,-0.0807,-0.0512,0.0947,-0.0301,0.0357,-0.0166,-0.0487,0.0305,-0.0032,-0.0661,-0.0081,0.038,0.0191,0.0248,0.0396,-0.0671,0.028,-0.0341,-0.0478,-0.0231,0.1438,0.0093,-0.0247,0.0243,-0.0232,0.0652,-0.0272,0.0299,0.0311,-0.0331,0.0487,0.1145,-0.0076,-0.0209,-0.0022,0.0311,0.0234,0.0117,-0.0469,-0.0726,0.0248,0.0448,-0.0412,0.0062,-0.0094,0.0402,0.0144,-0.0677,-0.0343,-0.0642,0.0083,-0.0386,-0.0437,-0.0089,-0.0072,0.0395,-0.0482,0.0197,-0.0149,-0.0681,0.017,-0.0195,-0.0303,-0.0106,-0.0502,0.0363,0.0482,-0.0403,0.0111,0.0339,-0.0298,-0.0206,0.0209,0.034,0.0036,-0.0307,0.043,0.0238,-0.007,-0.0841,-0.2369,-0.0672,-0.0233,0.0007,0.0491,-0.0579,0.0246,-0.0174,0.0653,0.053,0.0598,0.0049,-0.0159,0.0723,0.0121,0.0365,0.0666,-0.0039,0.016,-0.0366,-0.0368,0.0077,-0.0288,-0.0518,0.0978,-0.0284,0.1747,0.0506,0.0636,0.0209,0.0756,-0.0073,0.0289,-0.0129,0.0556,0.047,0.0867,-0.0115,-0.011,-0.0425,0.0291,0.0008,-0.0098,-0.0218,-0.0103,-0.0269,-0.0603,0.0138,-0.0437,0.0158,0.0794,-0.0121,0.039,-0.0319,0.0091,-0.0511,-0.0431,-0.0167,-0.0179,0.0379,-0.0154,-0.065,0.009,-0.0764,0.0657,0.0069,0.0182,-0.0032,0.0439,-0.0139,-0.0278,0.0401,0.0289,-0.007,0.0599,-0.002,0.0444,-0.0129,-0.0688,-0.0542,0.0259,-0.0594,0.0258,-0.0132,0.0076,-0.012,0.0784,-0.001,0.037,-0.0089,-0.005,-0.0103,-0.0535,0.0315,0.0397,-0.0137,-0.2726,-0.0299,-0.0201,0.002,-0.0293,0.0149,0.0357,-0.015,-0.0587,0.0301,-0.0516,0.0724,0.0426,-0.0141,0.0415,-0.0045,0.0327,-0.067,0.0597,-0.0848,0.0364,-0.0033,0.2036,-0.0453,0.0175,0.0265,-0.002,-0.03,0.045,-0.016,0.0373,-0.0083,0.0658,-0.0617,0.0673,0.0821,-0.0486,0.068,0.0054,-0.0153,0.0203,0.0194,-0.0733,-0.0036,0.0713,-0.0079,-0.0151,-0.0435,0.0424,0.0591,-0.0367,0.0426,0.027,0.0181,0.023,0.0179,-0.0718,-0.0211,-0.0227,-0.0603,0.0082,-0.046,-0.0282,0.0093,0.0238]}
{"key":"[Large-scale neuromorphic optoelectronic computing with a reconfigurable diffractive processing unit] Application-specific optical processors have been considered disruptive technologies for modern computing that can fundamentally accelerate the development of artificial intelligence (AI) by offering substantially improved computing performance. Recent advancements in optical neural network architectures for neural information processing have been applied to perform various machine learning tasks. However, the existing architectures have limited complexity and performance; and each of them requires its own dedicated design that cannot be reconfigured to switch between different neural network models for different applications after deployment. Here, we propose an optoelectronic reconfigurable computing paradigm by constructing a diffractive processing unit (DPU) that can efficiently support different neural networks and achieve a high model complexity with millions of neurons. It allocates almost all of its computational operations optically and achieves extremely high speed of data modulation and large-scale network parameter updating by dynamically programming optical modulators and photodetectors. We demonstrated the reconfiguration of the DPU to implement various diffractive feedforward and recurrent neural networks and developed a novel adaptive training approach to circumvent the system imperfections. We applied the trained networks for high-speed classifying of handwritten digit images and human action videos over benchmark datasets, and the experimental results revealed a comparable classification accuracy to the electronic computing approaches. Furthermore, our prototype system built with off-the-shelf optoelectronic components surpasses the performance of state-of-the-art graphics processing units (GPUs) by several times on computing speed and more than an order of magnitude on system energy efficiency.","layer":1,"vector":[-0.0819,-0.0365,0.0511,-0.0244,0.0034,0.0668,0.0464,0.0116,0.0259,-0.0199,0.0421,-0.0242,0.0384,0.0417,0.0043,-0.0301,-0.0168,-0.012,-0.0007,0.0143,0.0524,-0.0301,-0.0466,-0.0875,-0.0046,-0.0266,-0.0034,-0.0083,-0.0496,-0.2442,0.0384,-0.0035,0.0358,-0.0308,0.0094,-0.0269,-0.0705,0.0114,-0.063,0.0325,-0.0177,-0.0155,-0.0018,-0.0761,0.0195,-0.0284,-0.0041,-0.0302,0.0028,-0.0342,0.009,-0.061,0.0088,0.0343,-0.0035,0.0309,0.0478,0.0442,0.0447,-0.0009,-0.0053,0.0881,-0.1754,0.0499,0.001,0.0148,-0.0065,-0.0873,0.0163,-0.0038,-0.0436,0.0513,0.0585,-0.0002,0.0028,0.0192,-0.0121,-0.0359,0.0036,0.0118,0.0221,-0.0386,-0.0222,-0.021,0.0063,0.0046,-0.0083,-0.0119,-0.0071,-0.0108,-0.0475,-0.0154,-0.0536,-0.0234,-0.0377,0.0006,0.0185,0.0022,-0.054,0.2109,-0.0578,-0.0327,0.0299,-0.063,0.0615,-0.0336,-0.0101,-0.0077,-0.0851,-0.0039,0.011,-0.0446,0.0284,0.0206,-0.0045,0.0142,-0.0079,-0.0098,0.028,0.0122,-0.0148,0.0301,0.0054,-0.0106,0.0289,-0.0982,-0.0036,0.1269,0.0191,0.0711,0.0329,0.0075,-0.011,0.0015,0.0617,0.0587,0.0385,-0.0596,0.0098,-0.0194,-0.0592,-0.0027,0.0662,-0.0576,-0.0407,0.1189,-0.0421,0.0516,-0.006,0.0016,-0.0144,0.0095,-0.0448,-0.0297,-0.0057,0.0409,0.026,0.0236,-0.0825,0.0329,-0.0069,-0.044,-0.0287,0.0907,0.0735,-0.0973,-0.0332,-0.0203,-0.0039,-0.0133,0.0283,0.0351,-0.033,0.011,0.0704,0.0397,-0.069,-0.0375,-0.0061,0.0262,0.0226,-0.0787,-0.0489,-0.0043,0.0775,-0.0177,0.0216,-0.039,-0.0051,0.0584,-0.057,0.0367,-0.0186,-0.0463,-0.0461,-0.0085,-0.0378,-0.0067,0.032,-0.05,0.046,-0.0209,-0.0161,0.0106,0.0429,-0.0191,-0.0034,0.0334,0.0029,0.0503,-0.0247,0.0043,0.0897,-0.0355,-0.0402,0.0291,-0.0158,0.008,0.0557,0.0442,0.0374,-0.0583,-0.0804,-0.2289,0.02,-0.0007,-0.0566,0.0517,-0.0881,0.0578,0.0298,0.0667,0.0855,0.0502,0.0285,0.0241,0.0258,-0.02,0.0379,0.0623,0.0431,-0.0525,-0.0281,0.0127,0.0294,0.0057,-0.0823,0.0517,0.0214,0.2119,0.014,0.0512,-0.0323,0.0232,0.0352,-0.0365,-0.1091,0.0477,0.025,0.0851,-0.0022,-0.0172,-0.0262,-0.0213,0.0256,-0.0216,-0.1249,-0.0205,0.0036,0.0092,0.0045,-0.0513,0.0273,0.0357,-0.0725,0.0567,-0.0002,-0.0294,-0.0272,-0.0561,0.027,-0.0488,-0.0055,0.0143,-0.0249,-0.0222,-0.0512,0.0549,0.0086,-0.0198,-0.045,0.0542,0.0004,0.0206,0.1186,0.0022,0.0332,0.1043,-0.0133,0.0327,-0.0203,0.0431,-0.0008,0.091,0.0259,0.0624,0.0565,0.031,-0.0319,0.0633,-0.0442,0.0041,0.0085,-0.0129,0.0002,-0.0698,0.0264,0.0202,-0.0024,-0.2964,0.0423,0.0199,0.0203,-0.0077,0.0026,0.0341,0.0264,-0.015,-0.0082,-0.0725,0.0213,0.0168,0.0147,0.0189,0.063,0.0602,-0.0601,0.0191,-0.0409,0.0389,0.061,0.1775,-0.045,0.0162,0.0116,-0.0175,0.0066,-0.0185,-0.0284,0.047,0.0017,0.0514,-0.0775,0.0204,0.0362,-0.0201,0.0227,0.0466,-0.0134,0.0579,0.0134,-0.0308,-0.0278,0.0933,-0.0105,-0.0398,-0.0233,-0.0615,0.0329,-0.0045,0.0436,0.002,0.0178,0.005,0.0223,-0.0758,-0.0724,-0.0436,-0.0361,0.0475,-0.0466,-0.008,0.0141,-0.003]}
{"key":"[Adaptive Statistical Learning with Bayesian Differential Privacy] In statistical learning, a dataset is often partitioned into two parts: the training set and the holdout (i.e., testing) set. For instance, the training set is used to learn a predictor, and then the holdout set is used for estimating the accuracy of the predictor on the true distribution. However, often in practice, the holdout dataset is reused and the estimates tested on the holdout dataset are chosen adaptively based on the results of prior estimates, leading to that the predictor may become dependent of the holdout set. Hence, overfitting may occur, and the learned models may not generalize well to the unseen datasets. Prior studies have established connections between the stability of a learning algorithm and its ability to generalize, but the traditional generalization is not robust to adaptive composition. Recently, Dwork et al. in NIPS, STOC, and Science 2015 show that the holdout dataset from i.i.d. data samples can be reused in adaptive statistical learning, if the estimates are perturbed and coordinated using techniques developed for differential privacy, which is a widely used notion to quantify privacy. Yet, the results of Dwork et al. are applicable to only the case of i.i.d. samples. In contrast, correlations between data samples exist because of various behavioral, social, and genetic relationships between users. Our results in adaptive statistical learning generalize the results of Dwork et al. for i.i.d. data samples to arbitrarily correlated data. Specifically, we show that the holdout dataset from correlated samples can be reused in adaptive statistical learning, if the estimates are perturbed and coordinated using techniques developed for Bayesian differential privacy, which is a privacy notion recently introduced by Yang et al. in SIGMOD 2015 to broaden the application scenarios of differential privacy when data records are correlated.","layer":6,"vector":[-0.0543,-0.0112,-0.014,-0.0215,0.0214,0.0476,0.0716,-0.0071,0.0557,-0.0313,0.0566,-0.0096,0.0287,0.0432,0.0388,0.0151,0.0016,0.0169,-0.0574,0.0028,-0.0081,-0.0167,-0.0003,-0.0617,0.0246,0.0114,-0.0772,-0.0401,-0.0608,-0.2356,0.0284,-0.0696,0.0288,-0.0232,0.032,-0.0204,-0.0703,0.0363,-0.0188,0.0605,-0.0309,0.0322,-0.0457,-0.0487,-0.0283,0.009,-0.0208,-0.0328,-0.0344,-0.0322,0.0145,-0.0541,0.0187,0.0835,0.04,0.0703,0.0708,0.0193,0.018,0.0767,0.012,0.0638,-0.1382,0.0768,0.0351,0.0512,-0.0379,-0.0112,0.0049,-0.0087,-0.0201,0.0802,-0.004,0.0403,0.0203,0.0463,0.0167,-0.0152,-0.0103,0.0466,0.0013,-0.0299,0.0244,0.0034,-0.0388,-0.0949,0.017,-0.045,0.0721,0.0165,-0.0654,0.0057,-0.006,-0.0023,-0.0702,-0.0026,0.0196,0.0356,-0.0323,0.1705,-0.0467,0.0374,0.0092,0.0202,0.0367,-0.0235,0.0013,-0.0452,-0.0418,-0.0105,0.0361,-0.0378,0.0288,-0.0388,0.0137,0.0186,0.0683,0.0078,-0.0234,-0.016,0.0042,0.0159,0.0526,-0.0034,0.0425,-0.0881,0.0071,0.1779,-0.0079,-0.0092,0.0333,-0.0595,-0.0742,-0.0202,0.0294,0.0229,0.0525,0.0109,0.0471,-0.0072,-0.0529,-0.0454,0.0233,-0.0626,-0.0425,0.153,-0.0168,0.0887,-0.0047,-0.0233,0.0181,0.0283,-0.0215,-0.0303,0.0283,-0.0146,0.0002,0.0732,-0.0223,0.034,-0.0006,-0.0386,-0.0295,0.0984,0.0168,-0.0887,-0.042,0.0202,0.0192,-0.0124,0.0418,0.0443,-0.0279,0.0272,0.0393,0.0133,-0.0404,0.0113,-0.0166,0.0279,-0.0075,-0.0437,-0.0369,0.0503,0.0207,-0.0438,-0.0012,-0.0346,-0.0033,0.0776,-0.0083,0.0035,-0.0139,-0.0232,-0.0386,-0.0194,-0.0349,-0.0294,-0.0026,-0.0321,-0.0169,0.0138,-0.0502,0.0301,-0.0263,0.034,-0.002,-0.0348,0.0573,-0.0114,-0.0247,-0.007,0.0199,-0.0521,-0.0263,0.0196,0.006,0.0158,-0.0129,0.0348,0.0283,-0.0289,-0.0428,-0.2153,-0.0212,0.0012,0.0019,0.0466,-0.0853,0.0593,0.0374,0.0379,0.0947,0.0268,-0.0163,-0.0278,0.0413,-0.0372,0.0282,0.0369,0.0176,-0.0021,0.0012,-0.0346,0.0192,-0.0053,-0.1044,0.0608,-0.0232,0.2103,-0.0149,0.0387,-0.0319,0.0177,0.0539,-0.001,-0.1189,0.0112,0.0273,-0.0064,-0.0283,-0.0442,-0.0207,0.0201,0.0227,0.0111,-0.1113,-0.0447,-0.0378,-0.0528,0.0456,-0.0869,0.0139,0.0158,0.02,0.0647,-0.0307,0.0038,-0.0544,-0.0717,0.0129,-0.0452,0.0603,0.0401,-0.0208,0.0416,-0.0704,0.0364,-0.0314,-0.0009,-0.0872,0.0461,-0.0261,-0.0106,0.08,0.0424,-0.0056,0.0291,0.0077,0.0084,-0.0548,-0.0562,-0.0535,0.0983,0.0046,0.0003,0.0481,0.0188,0.0306,0.1068,0.0079,0.0438,-0.0232,0.0048,-0.0119,-0.0569,-0.0332,0.0384,0.0277,-0.2936,-0.002,-0.0307,0.0115,-0.0328,0.0267,0.02,0.0318,-0.0842,-0.0174,-0.0026,0.085,0.037,-0.0045,0.0114,0.0116,0.0594,-0.0283,0.056,-0.0366,0.0295,0.0271,0.2367,-0.0205,0.0385,0.0192,0.0039,0.0073,0.0235,-0.0281,0.0034,0.0052,0.0521,-0.0551,0.0247,0.0813,-0.0307,-0.0149,0.01,-0.0086,-0.0204,-0.0434,-0.0671,-0.0363,0.1298,-0.007,-0.0152,-0.0486,0.0015,0.02,-0.0036,-0.0125,-0.0415,0.0117,0.0434,0.0329,-0.0589,-0.0086,-0.0325,-0.0614,0.0195,-0.0328,-0.0426,0.004,-0.0077]}
{"key":"[Tight Regret Bounds for Noisy Optimization of a Brownian Motion] We consider the problem of Bayesian optimization of a one-dimensional Brownian motion in which the $T$ adaptively chosen observations are corrupted by Gaussian noise. We show that as the smallest possible expected cumulative regret and the smallest possible expected simple regret scale as $\\Omega(\\sigma\\sqrt{T / \\log (T)}) \\cap \\mathcal{O}(\\sigma\\sqrt{T} \\cdot \\log T)$ and $\\Omega(\\sigma / \\sqrt{T \\log (T)}) \\cap \\mathcal{O}(\\sigma\\log T / \\sqrt{T})$ respectively, where $\\sigma^2$ is the noise variance. Thus, our upper and lower bounds are tight up to a factor of $\\mathcal{O}( (\\log T)^{1.5} )$. The upper bound uses an algorithm based on confidence bounds and the Markov property of Brownian motion (among other useful properties), and the lower bound is based on a reduction to binary hypothesis testing.","layer":7,"vector":[-0.0607,-0.03,0.0577,0.011,0.0288,0.0271,0.0448,0.035,0.0353,-0.0033,0.0623,-0.0342,-0.0121,0.0429,0.0043,0.0434,-0.0021,0.0044,-0.0713,0.0135,0.0203,-0.0963,0.0097,-0.0777,0.0486,-0.0377,-0.0382,-0.0787,-0.038,-0.2201,0.0122,-0.0392,0.0263,-0.0781,0.0113,-0.0201,-0.0129,0.0779,-0.0233,0.0476,0.0188,0.0783,-0.0465,-0.0703,0.0097,-0.0672,0.015,-0.017,-0.062,-0.0232,-0.0185,-0.0129,0.0167,0.017,0.0318,0.0363,0.0307,0.0552,0.0201,0.0452,-0.0032,0.0609,-0.1615,0.0447,0.0492,-0.0051,-0.0434,-0.0389,-0.0002,0.054,-0.0184,0.0234,0.0148,0.0838,0.0211,0.0074,-0.0049,-0.0203,-0.0438,0.0277,-0.0125,-0.0347,-0.0363,-0.0089,-0.0517,-0.111,0.0171,-0.0174,0.0489,0.019,-0.0013,-0.0072,-0.0376,-0.0073,-0.0772,0.0119,0.0518,0.0536,-0.0159,0.1911,-0.0441,0.0257,0.0059,-0.0041,0.0543,-0.0005,-0.0146,-0.0335,-0.0144,-0.0254,0.0198,-0.0294,0.0364,-0.0206,-0.0278,0.0085,0.0055,0.0389,0.0003,0.0164,-0.0708,0.0223,0.1053,-0.0242,-0.0082,-0.0768,0.0056,0.1548,0.0494,0.0243,0.0308,-0.0904,-0.0185,0.0149,0.0418,0.0153,-0.0413,0.0103,0.0304,-0.0236,-0.0402,-0.0977,0.0461,-0.0729,-0.0455,0.1329,-0.0316,0.0503,-0.0454,-0.0522,0.0042,0.0084,-0.0109,-0.0073,0.0455,-0.0018,0.0075,0.0379,-0.0631,0.029,-0.0418,-0.0223,0.0331,0.0889,0.0066,-0.0602,-0.0433,0.0012,-0.0165,0.0221,0.0359,0.043,-0.0432,0.0048,0.0515,0.0074,-0.0397,0.0269,-0.0156,0.0052,-0.0239,-0.0299,0.0084,-0.01,0.0431,-0.0337,-0.0375,-0.0257,0.0238,0.0505,-0.0077,-0.0092,0.0148,0.0047,-0.0543,-0.0478,-0.0108,-0.0024,-0.0073,-0.0241,0.0048,-0.0324,-0.0895,0.0448,0.0134,0.0184,0.0016,-0.0065,0.0357,0.0172,0.0009,-0.0137,0.0642,-0.0265,-0.0354,0.0431,0.0304,0.0466,0.0206,0.0146,0.0419,-0.0146,-0.0273,-0.22,0.0009,-0.0098,0.0088,0.0807,-0.07,0.042,-0.0374,0.0743,0.0682,0.0128,-0.0718,-0.0536,0.0061,-0.0042,0.0131,0.0088,0.018,-0.0068,0.0231,-0.016,0.0256,-0.0434,-0.0713,0.0616,0.0024,0.2353,-0.0072,0.0465,-0.0266,0.0086,0.0445,0.0185,-0.0546,0.0296,0.0685,0.039,0.0201,-0.0465,-0.0524,-0.0169,-0.0147,-0.038,-0.0781,-0.0906,-0.0314,-0.0307,0.0117,-0.0737,-0.0049,0.0334,-0.0223,0.0796,-0.0083,0.0379,-0.0409,-0.0823,0.0076,-0.0059,0.0602,0.028,-0.0388,0.0428,-0.047,0.0324,-0.0299,0.0016,-0.0647,0.0215,-0.0245,0.0271,0.0581,0.0001,-0.0072,0.0671,0.0352,0.0278,-0.0659,-0.0724,-0.0567,0.0641,-0.0587,0.0516,0.0219,-0.0156,0.0056,0.091,-0.006,-0.0014,0.0153,-0.002,0.0139,-0.0387,0.0038,0.0516,0.0144,-0.2946,0.0059,0.0188,0.0215,-0.0215,0.014,0.0182,0.0121,-0.0721,-0.003,0.0154,0.0719,0.0601,0.0282,0.0183,0.0241,0.0445,-0.0252,0.0678,-0.0655,0.0125,0.022,0.2076,-0.0393,0.0176,0.0307,-0.017,0.0286,0.0279,-0.0642,0.0421,0.0217,0.0406,-0.0612,0.0596,0.1005,-0.0464,0.0599,0.021,-0.0214,-0.009,-0.0014,0.0004,0.0063,0.1321,-0.0199,-0.0379,-0.0289,-0.0252,0.0404,-0.0282,0.0338,0.0295,-0.0051,0.0181,0.031,-0.0334,-0.0438,-0.0098,-0.0573,0.0393,-0.0698,-0.0177,-0.0234,-0.0231]}
{"key":"[DeepCrashTest: Turning Dashcam Videos into Virtual Crash Tests for Automated Driving Systems] The goal of this paper is to generate simulations with real-world collision scenarios for training and testing autonomous vehicles. We use numerous dashcam crash videos uploaded on the internet to extract valuable collision data and recreate the crash scenarios in a simulator. We tackle the problem of extracting 3D vehicle trajectories from videos recorded by an unknown and uncalibrated monocular camera source using a modular approach. A working architecture and demonstration videos along with the open-source implementation are provided with the paper.","layer":11,"vector":[-0.0568,-0.0192,0.0498,-0.0355,0.0532,0.1128,0.0237,0.0358,-0.0009,-0.012,0.0029,-0.0568,0.0488,0.0562,0.0025,0.0008,-0.0004,0.0387,-0.0051,0.0046,0.0052,-0.0482,-0.0103,-0.0629,-0.0171,0.039,0.0067,-0.0233,-0.0416,-0.2283,0.0062,-0.0274,0.0418,-0.0223,-0.0143,-0.0251,-0.0727,0.0798,0.005,-0.003,0.0316,-0.0003,-0.031,-0.0526,-0.0101,-0.0338,-0.0001,-0.0165,0.0405,-0.0611,-0.0035,-0.0538,0.0321,-0.0123,-0.0029,-0.0373,0.0847,0.0569,0.0719,0.0278,0.0415,0.0023,-0.178,0.0773,0.0451,0.0231,-0.0384,0.0225,0.0758,0.0409,-0.0607,0.029,0.0277,0.0455,-0.0202,-0.0224,0.0055,-0.0531,-0.0299,-0.0209,-0.0433,-0.0322,-0.045,0.0034,0.002,-0.0582,0.0129,-0.0388,0.0101,-0.0428,-0.0684,-0.013,-0.0071,0.0009,-0.0582,-0.0167,0.0422,-0.014,0.015,0.2138,-0.0426,0.047,0.0836,-0.03,0.0231,0.0242,-0.0338,-0.0452,-0.0532,0.0055,0.0211,-0.0196,0.0432,-0.0323,0.0084,0.0386,0.0376,0.0348,-0.0119,-0.0323,0.0018,0.04,0.0447,-0.063,0.0325,-0.1014,0.0378,0.152,0.0386,0.0179,0.0275,-0.015,-0.0477,-0.0193,0.0302,0.0016,0.0302,0.0063,-0.0055,0.0129,-0.0245,-0.0756,0.0089,-0.0554,-0.0168,0.0693,-0.0362,0.0434,-0.0355,-0.0289,0.0043,0.0346,-0.0335,0.0069,0.0435,0.0246,0.0616,0.0592,-0.0804,0.0478,-0.0341,-0.0338,0.0079,0.1109,-0.0075,-0.1084,-0.004,0.0474,0.0374,-0.0064,0.022,0.0278,-0.0526,0.0094,0.0828,0.0367,-0.0861,0.0622,-0.0159,-0.0123,0.0218,-0.0602,-0.0588,0.033,0.0397,0.0062,-0.0111,-0.019,0.0186,0.0568,-0.0262,0.0096,-0.0783,0.0015,-0.0203,0.0143,-0.0023,-0.0307,0.003,-0.0244,0.0112,-0.0043,0.0208,-0.0029,-0.0319,0.0015,-0.0274,-0.0444,0.0515,0.043,-0.057,-0.042,0.0224,-0.0252,-0.0195,-0.03,-0.012,0.0147,-0.0146,0.0451,-0.0324,-0.0553,-0.0013,-0.2208,0.0093,-0.0232,-0.0154,0.039,-0.0563,0.0356,0.0001,0.0683,0.0453,0.0846,-0.0509,0.0303,0.0039,0.0305,0.0425,-0.0408,0.0555,-0.0442,0.0234,-0.0452,0.0085,-0.0289,-0.0631,0.0617,-0.0007,0.2256,0.0475,0.033,-0.0162,0.0232,0.0329,0.018,-0.1075,0.0411,-0.0078,0.0687,0.0286,-0.0433,-0.038,-0.0659,0.0044,-0.0048,-0.1101,-0.0063,-0.0427,-0.0527,0.0526,-0.0483,-0.0097,0.0246,-0.0463,0.042,0.0155,-0.0052,-0.0096,-0.0817,0.0365,-0.0453,-0.0038,-0.0362,-0.015,0.0414,-0.0521,0.0783,-0.0049,-0.0103,-0.0894,0.0305,-0.0421,0.0027,0.0965,0.0053,0.0346,0.0545,0.02,0.0439,-0.023,-0.0272,-0.0553,0.0201,0.0152,0.0256,0.0291,0.059,0.0313,0.0444,-0.0276,0.0057,-0.0482,0.019,0.0224,-0.0807,-0.0259,0.0519,-0.0019,-0.2851,0.0238,-0.0199,0.0062,-0.007,-0.0355,0.0773,0.0272,-0.0368,-0.0071,-0.0179,0.0569,0.0531,-0.0037,0.0253,0.0589,0.0668,0.012,0.0397,-0.0457,-0.012,0.0502,0.1906,-0.0837,-0.0039,0.0147,-0.042,-0.042,0.069,-0.0459,-0.0155,-0.031,0.0617,-0.0613,0.0123,0.0883,-0.0115,0.0269,0.036,0.0304,0.0003,-0.0071,0.0446,-0.0068,0.0721,-0.0004,0.0034,-0.0488,0.0096,0.0506,0.014,-0.0672,-0.0439,-0.0069,0.0734,0.0246,-0.0344,-0.016,-0.0446,-0.0477,0.0069,-0.0406,0.0336,0.0292,-0.0047]}
{"key":"[Bootstrapping a DQN Replay Memory with Synthetic Experiences] An important component of many Deep Reinforcement Learning algorithms is the Experience Replay which serves as a storage mechanism or memory of made experiences. These experiences are used for training and help the agent to stably find the perfect trajectory through the problem space. The classic Experience Replay however makes only use of the experiences it actually made, but the stored samples bear great potential in form of knowledge about the problem that can be extracted. We present an algorithm that creates synthetic experiences in a nondeterministic discrete environment to assist the learner. The Interpolated Experience Replay is evaluated on the FrozenLake environment and we show that it can support the agent to learn faster and even better than the classic version.","layer":1,"vector":[-0.072,0.0236,0.0564,-0.0429,-0.0123,0.0647,-0.0001,0.0063,0.0026,-0.0615,-0.0038,-0.0227,0.0721,0.0644,-0.003,0.0508,-0.0219,0.0525,-0.0386,0.0096,0.0121,-0.0653,0.0014,-0.0442,-0.0332,0.0273,-0.0429,-0.0444,-0.0213,-0.2419,0.0447,-0.0309,0.0096,-0.0223,-0.0075,0.0005,-0.0484,0.0502,-0.0371,0.0572,0.0503,-0.0043,-0.0292,-0.0589,-0.007,-0.0531,-0.0203,-0.0515,0.0317,-0.0777,0.0063,0.0094,0.0156,-0.0099,0.0035,0.0494,0.0813,0.0427,0.0688,0.0234,0.0301,0.0252,-0.1533,0.0473,0.0193,0.08,-0.0519,-0.0217,0.0353,0.0318,-0.0821,0.0509,0.0316,0.0364,0.0543,0.0057,-0.0154,-0.0375,-0.0068,0.0157,-0.0027,-0.0245,0.0069,-0.0133,-0.0183,-0.047,0.0405,-0.0482,0.0463,0.0374,-0.0303,0.0104,-0.0306,0.0233,-0.0564,0.0139,0.0237,0.0541,-0.0343,0.207,-0.0269,0.0517,0.0346,0.0179,0.0057,-0.0459,-0.0147,-0.0394,-0.046,-0.0112,-0.0407,-0.0095,0.0224,-0.0211,-0.0028,0.0327,0.076,0.037,-0.0046,-0.0308,0.0058,0.028,0.0268,-0.0375,0.0124,-0.0657,0.0196,0.1545,-0.0214,0.0334,0.023,-0.0634,-0.0208,-0.0015,0.0298,-0.0035,0.036,-0.0342,0.0002,-0.0266,-0.0123,0.0185,0.0222,-0.0877,-0.0877,0.0875,0.032,0.042,-0.0363,0.0027,-0.0087,0.0003,-0.0437,-0.0441,0.0568,0.0216,0.0166,0.0489,-0.0704,0.0291,-0.0402,-0.0717,0.0051,0.1265,-0.0002,-0.0713,-0.0299,0.0159,0.0179,-0.0079,0.0139,-0.0014,-0.0341,0.0375,0.0879,0.0009,-0.0912,-0.0172,0.0312,-0.0151,0.0345,-0.0898,-0.0207,0.044,0.0509,-0.0312,-0.0101,-0.0375,-0.0058,0.0169,-0.0045,0.0113,-0.0345,-0.0019,-0.0058,-0.0062,0.0027,-0.0147,0.0352,-0.0286,0.0041,-0.005,-0.0344,-0.0061,-0.0103,0.0058,0.0018,-0.0112,0.0566,0.0079,-0.0633,0.0107,0.0326,0.0023,-0.0413,-0.0064,0.0005,0.0122,-0.0441,0.0462,-0.0071,-0.0328,-0.0309,-0.2667,-0.003,-0.0374,-0.0032,0.0527,-0.0695,0.0247,-0.0088,0.0379,0.0405,0.0198,-0.0472,-0.0108,0.0199,-0.0184,0.0492,0.0496,0.0218,0.0147,0.0033,-0.0321,-0.0261,-0.0078,-0.0847,0.0739,-0.0373,0.2584,0.0121,0.0455,0.0014,0.0377,0.0165,-0.0051,-0.1401,0.0619,0.0087,0.0807,0.0528,0.0045,-0.0645,-0.0058,0.0492,-0.0079,-0.1048,-0.0,-0.0542,-0.0607,0.0276,-0.0767,0.0119,0.0035,-0.0487,0.0459,-0.001,-0.0337,-0.0154,-0.1196,0.0259,-0.0222,0.0203,-0.0129,-0.0276,0.0335,-0.0469,0.0731,0.0098,0.0412,-0.0171,0.0722,-0.0004,-0.0052,0.0161,0.0054,0.0109,0.0579,0.005,-0.0044,-0.0439,-0.0548,-0.0332,0.0214,-0.0445,0.0217,0.0548,0.0401,0.0163,0.041,-0.0073,0.041,-0.0207,-0.0004,0.0224,-0.061,-0.0607,0.0434,-0.01,-0.2992,0.0275,-0.0013,0.0199,-0.0028,0.0043,0.0008,0.029,-0.0324,0.0385,0.0025,0.0331,0.0487,0.0511,0.0397,0.0142,0.0753,-0.0147,0.0482,-0.0777,-0.0045,0.0477,0.212,-0.0059,0.0201,-0.0116,-0.0316,0.0189,0.023,-0.0359,-0.0272,-0.0182,0.0776,-0.0249,0.0234,0.0602,-0.0244,0.0334,0.0312,0.0246,-0.0156,0.0089,-0.021,0.0314,0.0666,0.0064,0.0087,-0.0424,-0.0312,0.0448,-0.0012,0.0163,0.0143,-0.0032,0.0619,0.0209,-0.0219,-0.0606,-0.0246,-0.0427,0.0129,-0.0494,0.0445,-0.0442,-0.014]}
{"key":"[Predictive Auto-scaling with OpenStack Monasca] Cloud auto-scaling mechanisms are typically based on reactive automation rules that scale a cluster whenever some metric, e.g., the average CPU usage among instances, exceeds a predefined threshold. Tuning these rules becomes particularly cumbersome when scaling-up a cluster involves non-negligible times to bootstrap new instances, as it happens frequently in production cloud services. To deal with this problem, we propose an architecture for auto-scaling cloud services based on the status in which the system is expected to evolve in the near future. Our approach leverages on time-series forecasting techniques, like those based on machine learning and artificial neural networks, to predict the future dynamics of key metrics, e.g., resource consumption metrics, and apply a threshold-based scaling policy on them. The result is a predictive automation policy that is able, for instance, to automatically anticipate peaks in the load of a cloud application and trigger ahead of time appropriate scaling actions to accommodate the expected increase in traffic. We prototyped our approach as an open-source OpenStack component, which relies on, and extends, the monitoring capabilities offered by Monasca, resulting in the addition of predictive metrics that can be leveraged by orchestration components like Heat or Senlin. We show experimental results using a recurrent neural network and a multi-layer perceptron as predictor, which are compared with a simple linear regression and a traditional non-predictive auto-scaling policy. However, the proposed framework allows for the easy customization of the prediction policy as needed.","layer":0,"vector":[-0.0499,-0.0704,0.0163,-0.0226,0.0396,0.0589,0.0314,0.0082,0.0221,-0.0188,0.0158,-0.031,0.0161,0.0507,0.0049,-0.0021,-0.011,0.0364,-0.0453,0.0118,0.0444,-0.0225,-0.0489,-0.0556,0.0145,0.03,-0.0243,0.0007,-0.054,-0.2298,-0.0068,-0.0755,0.0559,-0.0368,0.0279,-0.0339,-0.0002,0.0471,-0.0184,0.0438,0.0097,0.028,-0.0564,-0.0421,-0.0081,-0.0296,-0.0037,0.0135,-0.0644,-0.0066,0.0245,-0.0274,0.0089,0.0421,0.0018,0.0173,0.0359,0.0301,0.0631,0.0324,0.0308,0.0263,-0.1633,0.0684,0.0468,0.0583,-0.0274,-0.045,0.0012,0.0483,-0.0321,0.0178,0.0171,0.0522,0.0202,0.0233,-0.0681,-0.0036,-0.0228,0.0337,0.0488,-0.0253,-0.058,-0.0122,-0.0428,-0.0131,0.0056,-0.0264,0.0621,-0.0099,-0.0543,0.0186,0.006,0.0053,-0.0656,0.0149,0.0119,0.0036,-0.0525,0.2335,-0.0432,0.0482,0.0285,-0.0054,0.0529,-0.0341,-0.0114,-0.0264,-0.0148,0.0033,-0.0154,-0.0413,0.0089,-0.0566,0.0506,-0.0075,0.0302,0.0426,0.0256,-0.0057,-0.0003,0.0015,0.0491,-0.0339,0.0025,-0.0499,0.0637,0.1672,-0.0441,-0.0271,-0.0175,-0.0042,-0.0601,-0.0085,0.0459,0.0397,0.0092,-0.0348,0.004,0.0197,-0.0572,-0.0221,0.0197,-0.0948,-0.027,0.1486,0.0095,0.0168,-0.0151,-0.0512,-0.0422,0.029,-0.0295,-0.0451,0.0139,0.0332,-0.0108,0.032,-0.0313,0.0127,-0.0583,-0.0339,-0.0477,0.0812,-0.0031,-0.0751,-0.0211,-0.0094,0.0412,0.0193,0.0269,-0.0012,-0.0354,-0.0074,0.0788,0.0282,-0.072,-0.0124,-0.007,0.0269,0.0422,-0.0073,-0.0002,0.0578,0.0684,-0.0359,0.0365,-0.0644,0.0218,0.023,-0.0416,0.0333,-0.0175,0.0017,0.0095,-0.0058,-0.0499,0.0338,0.0455,-0.0675,0.0191,0.0269,-0.0248,0.0088,0.0109,0.0097,-0.021,0.02,0.0401,0.0151,-0.0268,-0.0229,0.0355,-0.0046,-0.0761,0.0336,-0.0022,0.0387,-0.0193,0.037,0.0445,-0.0214,-0.0924,-0.1813,0.0312,-0.0182,-0.048,0.0837,-0.0417,0.0535,0.0077,0.0243,0.0091,0.0438,-0.0032,0.0031,0.0157,-0.0102,0.0562,0.0104,0.0302,-0.0198,0.0115,0.0274,-0.0175,-0.0412,-0.1066,0.0267,0.0336,0.1982,-0.0259,0.0559,-0.063,0.0514,0.0004,-0.0403,-0.1061,0.0676,0.0314,0.1215,0.0056,-0.0383,-0.013,-0.0554,0.0677,-0.0375,-0.0909,-0.0416,-0.0159,0.0035,0.04,-0.073,-0.0514,-0.0017,-0.0385,0.0306,-0.0027,-0.003,-0.055,-0.0738,0.016,-0.0206,-0.0053,0.0452,-0.0426,-0.03,-0.0646,0.056,-0.022,-0.0286,-0.0202,-0.005,-0.0261,-0.0242,0.0987,-0.045,0.0074,0.0713,-0.0302,-0.0123,-0.0143,-0.0628,0.0157,0.0691,-0.0587,0.0618,0.0706,0.0246,0.0272,0.0369,0.0208,0.0129,-0.0178,-0.0053,0.0061,-0.0462,-0.0316,0.0672,0.0095,-0.3189,0.0649,0.0037,0.0182,0.0224,0.0169,0.0219,0.0575,-0.0398,0.0077,-0.0124,0.0246,-0.0028,-0.0051,0.0102,0.0299,0.1114,-0.0345,0.0238,-0.0504,-0.0178,0.039,0.2467,-0.0264,0.0395,0.0359,-0.0308,-0.0038,0.067,-0.0116,0.0171,0.0059,0.062,-0.0349,0.0266,0.03,0.02,0.0401,0.0628,0.0048,0.0256,-0.0197,-0.0181,-0.0183,0.0601,-0.0249,-0.0642,-0.0852,0.011,0.0274,-0.0203,-0.0053,-0.0068,0.0187,0.0067,0.0674,-0.0565,-0.016,-0.0532,-0.0877,0.0334,-0.0768,-0.0007,-0.0147,0.0007]}
{"key":"[Artificial intelligence system based on multi-value classification of fully connected neural network for construction management] This study is devoted to solving the problem to determine the professional adaptive capabilities of construction management staff using artificial intelligence systems.It is proposed Fully Connected Feed-Forward Neural Network architecture and performed empirical modeling to create a Data Set. Model of artificial intelligence system allows evaluating the processes in an Fully Connected Feed-Forward Neural Network during the execution of multi-value classification of professional areas. A method has been developed for the training process of a machine learning model, which reflects the internal connections between the components of an artificial intelligence system that allow it to learn from training data. To train the neural network, a data set of 35 input parameters and 29 output parameters was used; the amount of data in the set is 936 data lines. Neural network training occurred in the proportion of 10% and 90%, respectively. Results of this study research can be used to further improve the knowledge and skills necessary for successful professional realization.","layer":12,"vector":[-0.0364,0.0277,0.0258,-0.0254,0.0196,0.0304,0.045,0.0416,0.0162,-0.0193,0.0018,-0.0429,0.0036,0.0312,0.0118,-0.0214,0.0069,0.052,-0.0005,0.0077,0.0387,0.0306,-0.011,-0.0545,0.0136,0.0269,-0.0223,-0.0382,-0.0442,-0.1595,-0.0126,-0.0423,0.0916,-0.0083,0.009,-0.0092,-0.0586,0.0779,0.0177,0.0584,0.0069,-0.0173,0.0235,-0.0969,-0.0165,-0.0019,0.0382,-0.0394,-0.013,-0.0394,0.0349,-0.0462,-0.0046,0.0052,-0.0078,0.0605,0.0292,0.0542,0.0185,0.0485,0.0427,0.053,-0.2358,0.0634,0.0248,0.0477,-0.0658,-0.0433,0.0281,0.0404,-0.0318,0.0726,0.0178,0.0055,0.063,0.034,0.0054,-0.0039,0.0229,0.0499,0.0148,-0.0108,-0.0571,-0.04,-0.0305,0.0069,-0.0078,-0.0563,0.0256,0.0247,0.0032,-0.023,-0.0375,0.0449,-0.0217,-0.0308,0.0119,0.0079,-0.0568,0.2167,-0.0573,-0.0002,0.0524,-0.0641,0.0213,-0.0096,-0.0268,-0.0462,-0.0714,0.0129,-0.0328,-0.0267,-0.0221,-0.006,-0.0186,0.0311,-0.0001,0.03,0.0162,-0.0103,0.0028,0.0052,0.0517,0.0099,0.0094,-0.0987,0.013,0.1301,0.0412,0.0449,0.0319,-0.0211,-0.0746,-0.0279,0.0262,0.0711,0.0528,-0.0033,-0.0052,-0.0089,-0.0486,-0.074,0.0285,-0.0713,-0.0505,0.1089,-0.0479,0.016,-0.0247,-0.0137,-0.0547,-0.0146,-0.0206,-0.0202,0.0423,0.0186,0.0304,0.0405,-0.058,-0.0127,-0.008,-0.0321,-0.0759,0.1012,0.0383,-0.0789,-0.029,-0.0161,-0.0207,-0.0602,0.0107,0.0411,-0.0578,0.0188,0.1015,0.0341,-0.06,-0.0067,-0.0066,0.0327,0.0497,-0.0257,-0.033,0.0479,0.0251,-0.0105,-0.0092,-0.0453,0.0279,0.0319,-0.0497,0.0538,-0.0441,-0.0374,-0.0302,-0.0076,0.0037,-0.0049,0.0186,-0.0229,-0.0073,-0.003,-0.03,0.0002,-0.0159,0.0462,-0.0224,0.004,0.0573,0.0492,-0.0101,0.0079,0.0771,-0.0508,-0.0234,0.0014,0.0335,0.069,0.0259,0.0727,0.0363,-0.0129,-0.0654,-0.1985,0.0192,0.0139,-0.0124,0.0435,-0.0434,0.0622,-0.0061,0.0273,0.042,0.0937,-0.0102,-0.0165,0.035,-0.0047,0.0214,0.0335,0.0477,-0.0517,-0.0058,0.0231,0.0208,0.0178,-0.0861,0.0392,-0.003,0.1387,-0.076,0.0372,-0.0279,0.0236,0.0364,-0.0459,-0.1316,0.0797,-0.0267,0.0505,0.0085,-0.0523,-0.0152,-0.0246,0.0217,-0.0004,-0.0909,-0.0241,-0.0179,-0.02,-0.0003,-0.0518,-0.0004,-0.0143,0.0123,0.0236,0.0367,-0.0169,-0.0063,-0.0824,0.0729,-0.0189,-0.0239,0.0288,-0.0863,0.0116,0.0125,0.0715,0.0368,-0.0134,0.0166,0.0252,-0.053,-0.0168,0.1005,0.0422,-0.0445,0.0489,-0.0165,-0.0351,-0.032,0.001,0.0221,0.0387,-0.0032,0.0109,0.0237,0.0317,0.0229,0.0375,-0.0122,0.0171,-0.0017,0.0044,-0.0276,-0.0341,0.007,0.0171,-0.0163,-0.3016,0.0864,-0.0089,0.0014,-0.0543,-0.0029,-0.035,0.0408,-0.0235,0.0077,0.0405,0.0028,-0.0111,-0.0379,0.0104,0.0009,0.0131,-0.0383,0.0551,-0.0625,-0.0037,0.0697,0.2678,-0.0742,0.0767,-0.0089,-0.043,-0.0272,0.0228,-0.0253,0.024,0.013,0.1215,-0.0591,0.0528,0.1015,-0.033,0.0389,0.0087,-0.0255,-0.016,0.0019,-0.0727,-0.013,0.1263,0.0225,-0.0345,-0.0586,-0.0199,-0.0188,-0.051,-0.0126,0.0115,-0.0052,0.0187,0.0063,-0.0058,-0.0815,-0.0995,-0.0449,0.0104,-0.0456,0.0396,-0.0094,-0.0179]}
{"key":"[D-GAN: Deep Generative Adversarial Nets for Spatio-Temporal Prediction] Spatio-temporal (ST) data for urban applications, such as taxi demand, traffic flow, regional rainfall is inherently stochastic and unpredictable. Recently, deep learning based ST prediction models are proposed to learn the ST characteristics of data. However, it is still very challenging (1) to adequately learn the complex and non-linear ST relationships; (2) to model the high variations in the ST data volumes as it is inherently dynamic, changing over time (i.e., irregular) and highly influenced by many external factors, such as adverse weather, accidents, traffic control, PoI, etc.; and (3) as there can be many complicated external factors that can affect the accuracy and it is impossible to list them explicitly. To handle the aforementioned issues, in this paper, we propose a novel deep generative adversarial network based model (named, D-GAN) for more accurate ST prediction by implicitly learning ST feature representations in an unsupervised manner. D-GAN adopts a GAN-based structure and jointly learns generation and variational inference of data. More specifically, D-GAN consists of two major parts: (1) a deep ST feature learning network to model the ST correlations and semantic variations, and underlying factors of variations and irregularity in the data through the implicit distribution modelling; (2) a fusion module to incorporate external factors for reaching a better inference. To the best our knowledge, no prior work studies ST prediction problem via deep implicit generative model and in an unsupervised manner. Extensive experiments performed on two real-world datasets show that D-GAN achieves more accurate results than traditional as well as deep learning based ST prediction methods.","layer":1,"vector":[0.0044,-0.0539,0.0806,-0.0279,0.0403,0.0193,0.0366,-0.0337,0.0195,-0.0117,-0.0588,-0.0515,0.0528,0.0887,-0.0057,-0.0094,0.0057,0.0274,-0.0275,0.0002,0.0473,0.0011,-0.0084,-0.0446,0.0082,0.0194,0.0208,-0.0468,-0.0405,-0.2266,0.0341,-0.0488,0.0299,-0.0299,-0.0576,-0.0584,-0.0525,0.092,0.0157,0.0306,-0.0033,0.0418,-0.0498,-0.0444,-0.0373,-0.0333,0.0157,-0.0044,-0.0181,-0.0461,0.0412,-0.0274,0.0055,0.0528,0.0226,0.0254,0.0601,0.0403,0.0394,0.0195,-0.0002,0.0195,-0.1697,0.0576,0.0522,0.0281,-0.0445,0.0107,0.0236,0.0151,-0.002,0.0392,-0.0047,0.0431,0.008,0.0038,-0.0421,-0.0017,-0.0418,0.0021,0.0531,0.0369,-0.0394,-0.041,-0.01,-0.0248,-0.0086,-0.0168,0.0506,0.0066,-0.0145,-0.04,-0.0462,0.0381,-0.0772,0.0304,0.0335,0.0228,-0.0436,0.1982,-0.1207,0.0737,0.0623,-0.0041,-0.0126,-0.022,-0.0222,-0.0563,-0.0564,0.0257,0.0009,-0.0023,0.0231,-0.0491,0.0377,-0.0391,0.0507,0.0396,-0.0304,-0.0158,-0.0593,0.048,0.0387,-0.0048,0.0244,-0.054,0.0232,0.1395,0.0282,0.0358,0.0291,0.012,-0.0754,0.0144,0.0205,-0.0146,0.0492,0.0013,-0.0398,-0.0154,-0.0248,-0.0341,-0.0116,-0.0361,-0.0086,0.0866,-0.031,0.0001,-0.0256,-0.0309,-0.0287,0.0134,-0.0253,-0.0304,0.0287,0.0674,0.0447,0.074,-0.0227,0.0225,-0.0408,-0.0222,-0.0636,0.0862,-0.0121,-0.1078,-0.0186,0.0414,-0.0014,-0.0184,0.0113,0.0022,-0.0323,0.0441,0.0986,0.0305,-0.0668,0.0269,-0.0351,0.0084,0.0029,-0.0449,-0.0073,0.0551,0.02,-0.0206,0.0012,-0.0492,-0.0328,0.0501,-0.0071,0.0129,0.0149,0.0003,-0.0184,-0.0166,-0.0339,-0.0059,0.0157,-0.0648,0.0012,-0.0121,-0.029,-0.035,-0.0216,0.0113,0.0006,-0.0224,0.0278,0.0228,-0.0142,0.0138,0.0825,-0.0077,-0.015,-0.0121,-0.029,0.0476,-0.04,0.0374,0.0216,-0.0491,-0.0174,-0.1943,0.0103,-0.006,-0.0275,0.0492,-0.0684,0.015,-0.0021,0.0523,0.0831,0.078,-0.0071,-0.0038,0.0058,0.0228,0.0462,0.0055,0.019,-0.0091,-0.0303,-0.0012,0.0259,-0.0153,-0.0945,0.0527,-0.0055,0.1902,0.0054,0.0786,-0.0565,0.0429,0.0341,-0.02,-0.0982,0.062,0.0152,0.0973,-0.0004,-0.0765,-0.0373,-0.0303,0.0559,0.0109,-0.0622,-0.0374,-0.015,-0.0084,0.0737,-0.0519,0.0074,0.0084,-0.0582,0.0892,-0.0035,0.0298,-0.0178,-0.1057,0.0641,-0.0287,-0.0101,0.0021,-0.0604,0.0104,-0.0692,0.0264,0.0171,-0.0681,-0.0947,-0.0359,0.0202,-0.0241,0.1269,-0.0323,0.0023,0.0892,-0.0463,0.0152,0.0104,-0.031,-0.0132,0.0976,-0.0441,0.0763,0.0526,0.0363,0.0001,0.0747,0.0175,0.0326,-0.0228,0.02,-0.0039,-0.0139,-0.0443,0.0432,-0.0231,-0.3044,0.0523,-0.007,0.0309,-0.009,-0.0387,0.0219,0.051,-0.0196,0.0079,0.0048,0.0296,0.0881,-0.0552,0.0229,0.0037,0.0888,-0.0225,0.0169,-0.0453,0.018,0.0491,0.2256,-0.0458,0.0253,0.0236,-0.0512,0.0064,0.0442,-0.0119,-0.0236,0.0181,0.0758,-0.0641,-0.0066,0.0686,-0.063,0.0593,0.0043,-0.0002,-0.0171,0.0381,-0.0211,-0.0216,0.0682,0.0113,-0.025,-0.0283,0.0007,0.0559,-0.037,-0.0144,-0.0384,0.0128,0.0173,0.024,-0.0532,-0.0624,-0.0423,-0.0112,0.0312,-0.0753,-0.0451,-0.0568,-0.0128]}
{"key":"[Support Vector Machines under Adversarial Label Contamination] Machine learning algorithms are increasingly being applied in security-related tasks such as spam and malware detection, although their security properties against deliberate attacks have not yet been widely understood. Intelligent and adaptive attackers may indeed exploit specific vulnerabilities exposed by machine learning techniques to violate system security. Being robust to adversarial data manipulation is thus an important, additional requirement for machine learning algorithms to successfully operate in adversarial settings. In this work, we evaluate the security of Support Vector Machines (SVMs) to well-crafted, adversarial label noise attacks. In particular, we consider an attacker that aims to maximize the SVM's classification error by flipping a number of labels in the training data. We formalize a corresponding optimal attack strategy, and solve it by means of heuristic approaches to keep the computational complexity tractable. We report an extensive experimental analysis on the effectiveness of the considered attacks against linear and non-linear SVMs, both on synthetic and real-world datasets. We finally argue that our approach can also provide useful insights for developing more secure SVM learning algorithms, and also novel techniques in a number of related research areas, such as semi-supervised and active learning.","layer":0,"vector":[0.0041,-0.0612,-0.0007,-0.0131,0.0229,0.0144,0.0564,0.0382,-0.0046,0.0024,0.0229,-0.0237,0.0145,0.0508,0.0234,0.0147,0.0368,0.0261,-0.04,0.0081,-0.0033,-0.0171,0.0159,-0.0381,0.0134,0.0743,-0.0457,-0.0442,-0.0785,-0.2233,0.0227,-0.0877,0.0479,0.0075,0.0006,-0.0328,-0.0306,0.0458,-0.0168,0.0363,0.0311,-0.0013,-0.0507,-0.1122,-0.0415,-0.0196,-0.0445,-0.0313,-0.0034,-0.013,0.0019,-0.0026,-0.0033,0.0107,0.028,0.0001,0.0669,0.0189,0.0253,0.1005,0.0145,0.0613,-0.1378,0.0653,0.0268,0.0301,-0.0307,-0.0279,0.0144,0.044,-0.0249,0.0283,-0.0159,0.0159,0.013,0.0165,-0.042,-0.0328,0.002,0.0147,0.0346,-0.0078,-0.0164,-0.0244,-0.0253,-0.0154,0.0263,-0.0257,0.0989,-0.0027,0.0102,0.007,-0.0097,0.0646,-0.0274,-0.0231,0.0371,0.0102,-0.1044,0.198,-0.0495,0.0147,0.0074,-0.0604,0.0471,-0.013,-0.0528,-0.0598,-0.0091,-0.0264,-0.0076,-0.0079,0.0361,-0.0376,0.0463,-0.0103,0.0717,-0.0013,-0.0389,-0.0214,-0.0121,0.0065,0.0491,0.0179,0.0384,-0.0442,0.0351,0.151,0.0248,0.0116,0.0039,-0.0359,-0.0454,0.0121,0.0256,0.0458,0.0075,0.0197,0.0407,-0.0019,-0.0418,-0.0319,0.0508,-0.0672,0.0,0.1138,-0.0423,0.0009,-0.0317,-0.0264,-0.0264,0.026,-0.0284,-0.0281,0.0023,0.0402,0.0241,0.0819,-0.0472,-0.0037,-0.0022,-0.0331,-0.0014,0.1336,-0.0003,-0.0907,-0.0314,-0.0101,-0.0118,-0.02,0.0111,0.0458,-0.0135,0.0037,0.007,0.0534,-0.0785,-0.0255,-0.0344,0.0235,-0.0266,-0.0729,-0.0444,0.0449,0.0714,-0.0427,0.0062,-0.0596,0.041,0.0266,-0.0552,0.0128,-0.0096,-0.0689,-0.0445,-0.0213,0.001,0.0025,-0.0037,-0.0321,0.0281,0.0065,-0.013,0.0103,-0.0151,0.0241,-0.0028,-0.0125,0.0089,0.0141,0.0104,0.0392,0.0096,-0.0583,-0.036,-0.0255,0.0027,0.108,-0.0065,0.038,0.0176,-0.0123,-0.0409,-0.2398,-0.0408,-0.0503,-0.0328,0.0231,-0.0823,0.0296,-0.0409,0.0503,0.0273,0.0463,0.0124,-0.068,-0.0136,0.012,0.0649,0.0155,0.0166,-0.0161,0.0287,-0.0439,0.0277,0.0229,-0.0523,0.0271,0.0121,0.2309,0.0632,0.0489,-0.0199,0.0527,0.0053,-0.0305,-0.1151,0.0681,0.0317,0.0356,0.0147,-0.0247,0.012,-0.01,0.0217,0.045,-0.1378,-0.0161,-0.0636,-0.0376,0.0042,-0.0457,0.0504,0.0402,0.0344,0.1202,0.0176,0.0302,-0.0339,-0.0929,0.0288,-0.0341,0.0604,-0.0428,-0.063,0.0408,-0.0924,0.0269,0.0287,-0.0191,-0.0524,0.0597,-0.0383,-0.0313,0.1258,0.0519,0.0022,0.0539,0.0099,-0.0261,-0.0387,-0.0642,-0.0031,0.0346,0.0186,0.026,0.0015,0.0303,-0.0401,0.087,0.0217,0.03,-0.0272,0.0156,0.0083,-0.0666,-0.0022,0.0442,-0.016,-0.2906,0.0237,0.0031,0.0784,-0.0249,-0.0068,0.048,-0.0066,-0.0573,0.0393,-0.0069,0.0424,0.0246,-0.0258,0.0219,0.0087,0.0387,-0.07,0.0286,-0.0559,0.0158,0.0464,0.1922,-0.0381,-0.0269,0.0158,0.0324,0.0722,0.0095,-0.0406,0.0289,-0.0251,0.0733,-0.0028,0.0489,0.0667,-0.0584,-0.0391,-0.0073,-0.0159,0.0049,-0.0123,-0.0903,0.0147,0.0734,-0.0273,0.0029,-0.0249,0.0476,0.0089,-0.0252,-0.0188,-0.0075,0.0049,0.0427,0.0383,-0.049,-0.0324,-0.0345,-0.0039,0.0595,-0.0676,-0.0146,0.0015,-0.0424]}
{"key":"[How Do We Fail? Stress Testing Perception in Autonomous Vehicles] Autonomous vehicles (AVs) rely on environment perception and behavior prediction to reason about agents in their surroundings. These perception systems must be robust to adverse weather such as rain, fog, and snow. However, validation of these systems is challenging due to their complexity and dependence on observation histories. This paper presents a method for characterizing failures of LiDAR-based perception systems for AVs in adverse weather conditions. We develop a methodology based in reinforcement learning to find likely failures in object tracking and trajectory prediction due to sequences of disturbances. We apply disturbances using a physics-based data augmentation technique for simulating LiDAR point clouds in adverse weather conditions. Experiments performed across a wide range of driving scenarios from a real-world driving dataset show that our proposed approach finds high likelihood failures with smaller input disturbances compared to baselines while remaining computationally tractable. Identified failures can inform future development of robust perception systems for AVs.","layer":0,"vector":[-0.0433,-0.0452,0.0516,-0.0031,0.0159,0.0816,0.0838,0.003,0.0029,0.0139,-0.0038,-0.0402,0.0369,0.0519,0.0076,0.0004,-0.0119,0.0383,-0.0106,-0.0184,0.0059,-0.0403,-0.0228,-0.0644,-0.033,0.0533,-0.0324,-0.0292,-0.0322,-0.2359,-0.019,-0.0906,0.0141,-0.0052,-0.0087,-0.0209,-0.0653,0.0595,-0.0167,0.0233,0.0438,-0.0063,-0.072,-0.0627,-0.014,-0.0622,0.009,-0.0213,-0.0111,-0.0677,0.004,-0.0524,0.0328,0.0014,0.0175,0.0445,0.0538,0.0533,0.049,0.0082,0.0087,0.0201,-0.2072,0.0449,0.0629,0.0667,-0.0533,-0.0436,0.0064,0.0233,-0.0467,0.0344,0.0075,0.0499,0.0069,-0.0117,-0.0076,-0.045,0.008,-0.0135,0.0215,-0.0177,-0.0579,0.0083,-0.0011,-0.0672,0.002,-0.046,0.0447,0.018,-0.0393,0.0099,0.0094,0.0241,-0.0394,0.0162,0.0139,-0.0335,-0.0488,0.2248,-0.0367,0.0358,0.0207,0.0096,0.019,-0.0196,-0.0198,-0.0673,-0.0424,0.0243,0.0003,-0.0024,0.0298,0.0054,-0.0017,0.0034,0.0675,0.0362,0.0202,-0.0043,0.0125,-0.0134,0.0864,-0.0003,0.0384,-0.0982,0.0449,0.1597,-0.0047,-0.0126,0.0312,-0.0793,-0.0277,-0.0302,0.0114,0.0008,0.0016,0.0022,0.002,0.0278,-0.0262,-0.0319,0.0339,-0.0817,-0.0353,0.109,-0.0441,0.0406,-0.0256,-0.0252,0.0067,0.0057,-0.0218,0.0003,0.0789,0.0339,0.0241,0.039,-0.0438,0.0266,-0.0088,-0.0331,-0.0258,0.0798,-0.0135,-0.0344,-0.0374,0.0122,0.0484,0.0187,-0.0109,0.016,-0.0107,0.0563,0.0598,-0.0043,-0.0984,0.0231,-0.0308,0.0121,0.0376,-0.0654,-0.0167,0.0332,0.0672,-0.0099,-0.0261,-0.0505,0.0046,-0.0018,-0.0085,-0.0192,-0.0111,-0.0024,-0.0102,-0.0092,-0.0128,-0.0133,0.0134,-0.045,0.0217,-0.0188,-0.0254,-0.0038,-0.0009,-0.0175,-0.016,-0.0076,0.0626,-0.0083,-0.0458,-0.0,0.0505,-0.025,-0.0275,-0.0204,-0.0052,0.0342,-0.0134,0.0138,0.0306,-0.0195,-0.0302,-0.2288,-0.0064,-0.0358,-0.0268,0.079,-0.0445,0.0421,-0.0261,0.0442,0.0444,0.0573,-0.0929,0.029,0.0052,0.0195,0.0495,-0.0099,-0.0062,-0.0159,0.0108,-0.0279,0.0359,-0.0632,-0.0996,0.037,-0.01,0.207,0.0269,0.0461,-0.0142,0.0064,0.0459,-0.0213,-0.0732,0.0864,0.018,0.0575,-0.0071,-0.0094,-0.0367,-0.0338,0.0463,-0.0237,-0.0298,-0.0143,-0.0206,-0.0331,0.0958,-0.0747,-0.005,0.0547,-0.0565,0.0428,-0.015,-0.019,-0.0362,-0.0871,0.0337,-0.039,0.0048,0.0286,-0.0174,0.0416,-0.052,0.0676,-0.0016,-0.0227,-0.1078,0.0373,-0.0003,-0.0065,0.1025,-0.0343,-0.0315,0.0644,-0.0213,0.0014,0.005,-0.0381,-0.0171,0.0672,-0.0368,0.0467,0.0501,0.067,-0.001,0.0321,-0.0266,0.0232,-0.0447,0.0275,0.0337,-0.0362,-0.0581,0.1257,0.02,-0.2874,0.0125,0.0428,0.0386,-0.0425,0.0012,0.0352,0.0301,-0.036,-0.0397,-0.0225,0.0522,0.0738,-0.0023,0.0481,0.023,0.044,-0.0103,0.0516,-0.0381,-0.0093,0.0712,0.2359,-0.0435,0.0307,0.0554,-0.0365,-0.039,0.0078,-0.0084,0.0209,0.0384,0.0664,-0.034,0.0137,0.071,-0.03,0.0323,0.0421,0.0172,0.0075,0.0567,0.0561,-0.0286,0.0677,-0.0261,-0.0112,-0.0345,-0.0135,0.0353,-0.0094,-0.0032,-0.0111,-0.0242,0.0218,0.0336,-0.0532,-0.0536,-0.0344,-0.065,-0.0036,-0.0587,0.0431,-0.047,0.0151]}
{"key":"[Clustering based on the In-tree Graph Structure and Affinity Propagation] A recently proposed clustering method, called the Nearest Descent (ND), can organize the whole dataset into a sparsely connected graph, called the In-tree. This ND-based Intree structure proves able to reveal the clustering structure underlying the dataset, except one imperfect place, that is, there are some undesired edges in this In-tree which require to be removed. Here, we propose an effective way to automatically remove the undesired edges in In-tree via an effective combination of the In-tree structure with affinity propagation (AP). The key for the combination is to add edges between the reachable nodes in In-tree before using AP to remove the undesired edges. The experiments on both synthetic and real datasets demonstrate the effectiveness of the proposed method.","layer":0,"vector":[-0.0044,0.0067,0.005,-0.0074,0.0627,0.0136,0.037,0.0113,0.0452,-0.0157,0.0524,-0.0876,0.0403,0.0361,0.018,0.0393,0.008,0.0762,-0.0322,-0.0189,0.0269,-0.0481,-0.0293,-0.0403,0.0674,0.0482,-0.0123,-0.0394,-0.0533,-0.2452,-0.0211,-0.0538,0.101,-0.0227,-0.0024,-0.0181,-0.0045,0.0345,-0.0273,0.0105,0.0394,0.0115,-0.0517,-0.0544,-0.0108,-0.0545,-0.0186,0.0019,-0.0288,-0.0521,-0.0187,-0.0585,0.0191,0.0596,0.0351,0.0353,0.0454,0.0269,0.0245,0.0348,0.0699,0.0662,-0.1296,0.0412,0.0733,-0.011,-0.0261,0.0069,0.0346,0.0755,-0.0097,0.0372,0.0128,0.0318,-0.006,0.0155,0.0158,-0.0558,-0.025,0.008,-0.0073,-0.0315,-0.0407,0.018,-0.002,-0.0379,0.0209,-0.0913,0.0368,0.0497,-0.0752,-0.0078,-0.0205,0.0638,-0.0804,-0.0595,0.0213,-0.0128,-0.0396,0.2043,-0.0639,0.0823,0.0174,-0.0532,-0.0107,-0.0756,-0.0015,-0.0482,0.0183,-0.0238,-0.0055,-0.0327,0.0189,-0.0591,0.0309,-0.0063,0.0801,0.0622,-0.0019,0.0151,-0.0254,-0.0063,0.0332,0.0257,0.0779,-0.0493,-0.0199,0.0747,0.0673,0.0339,0.0453,-0.0185,-0.051,0.0,0.0113,-0.0076,0.0051,-0.0286,-0.0188,-0.0127,-0.0065,-0.0598,0.0346,-0.0772,-0.0226,0.1031,-0.0314,-0.002,-0.0385,-0.0452,-0.0321,0.0107,-0.0506,-0.0069,0.0276,0.0121,0.0574,0.0395,-0.0265,-0.0054,-0.0176,-0.0342,-0.0438,0.0768,0.0458,-0.0994,-0.0664,0.0269,0.0346,0.0051,0.0453,0.0663,-0.0135,0.0707,0.0767,0.0121,-0.0804,-0.0215,-0.013,0.0052,0.0371,0.0021,-0.0279,0.0255,0.0366,-0.0232,-0.0424,0.0008,0.0165,0.047,-0.0825,0.0087,-0.0368,-0.0122,0.004,-0.0521,-0.0325,-0.0245,-0.0278,-0.036,0.0459,0.0262,-0.0627,0.0473,-0.0347,0.046,0.0252,-0.0302,0.0274,0.0213,-0.0481,0.0126,0.036,-0.0442,-0.0035,-0.0262,0.0274,0.0389,0.0396,0.0663,0.0414,-0.0217,-0.0639,-0.1945,-0.0089,0.0107,-0.0069,0.0201,-0.0619,-0.0116,0.0158,0.0774,0.0603,0.0257,0.0303,-0.0033,0.0278,-0.0382,0.0197,0.0561,0.0445,-0.0353,0.0058,0.0253,0.0313,-0.019,-0.0523,-0.0054,0.0222,0.2164,0.0162,0.012,0.0077,0.0235,0.0527,-0.0577,-0.0957,0.0155,0.0448,0.0552,-0.0229,-0.0232,0.0101,-0.0158,0.0236,0.0252,-0.1064,-0.0208,-0.0162,-0.0582,0.005,-0.044,-0.0097,0.0295,-0.006,0.0493,0.0197,0.0115,-0.0548,-0.0756,0.0393,-0.0078,0.0168,0.0324,-0.0615,-0.0405,-0.079,0.0534,0.0107,-0.0374,-0.0009,-0.0127,-0.0189,-0.0208,0.0649,-0.0116,-0.0001,0.0491,0.0464,-0.0054,0.0127,-0.055,-0.0506,0.0756,-0.0482,0.072,0.027,-0.004,0.0259,0.0796,-0.0533,0.0377,-0.0725,0.0236,-0.0013,-0.0529,-0.0114,0.0458,-0.0249,-0.2946,0.0168,0.003,0.0256,-0.0049,-0.0098,0.075,0.0379,-0.0057,-0.0132,0.0084,0.008,0.0241,-0.0345,0.0041,0.0661,0.0348,-0.0588,0.0524,-0.0693,0.0296,0.0185,0.2488,-0.0089,0.0411,0.0082,-0.0474,-0.0015,-0.0054,-0.0051,-0.0184,-0.0024,0.087,-0.0287,0.0454,0.0782,-0.0438,0.0496,0.0565,-0.0421,-0.0067,0.0105,-0.0649,-0.0251,0.1012,-0.0388,-0.008,-0.0915,0.0183,0.0513,-0.0442,-0.0042,-0.0183,-0.027,0.0415,0.0513,-0.0343,-0.0352,-0.0633,-0.0589,-0.0568,-0.0292,-0.0527,0.013,0.0167]}
{"key":"[Lifted Relational Neural Networks] We propose a method combining relational-logic representations with neural network learning. A general lifted architecture, possibly reflecting some background domain knowledge, is described through relational rules which may be handcrafted or learned. The relational rule-set serves as a template for unfolding possibly deep neural networks whose structures also reflect the structures of given training or testing relational examples. Different networks corresponding to different examples share their weights, which co-evolve during training by stochastic gradient descent algorithm. The framework allows for hierarchical relational modeling constructs and learning of latent relational concepts through shared hidden layers weights corresponding to the rules. Discovery of notable relational concepts and experiments on 78 relational learning benchmarks demonstrate favorable performance of the method.","layer":3,"vector":[-0.0589,0.0008,0.0509,0.004,0.0088,-0.0204,0.0386,0.0078,0.0138,-0.0191,-0.0102,-0.0422,0.0908,0.0381,0.0518,0.0118,0.0093,0.0714,-0.0762,-0.0415,0.0015,-0.0369,-0.0294,-0.0364,0.0117,0.0207,-0.0282,-0.0431,-0.0428,-0.2164,-0.0298,-0.0347,0.0185,-0.0078,0.0285,-0.0374,-0.0233,0.0095,-0.032,0.0016,0.0521,0.016,0.0388,-0.0474,-0.0068,-0.0386,0.0036,-0.0251,-0.0331,-0.0802,0.0063,-0.005,0.0076,0.0012,0.0044,0.0312,0.0545,0.0513,0.0238,0.0596,0.0134,0.0498,-0.1678,0.0698,0.0139,0.0105,-0.056,0.0024,0.0419,0.087,0.0037,0.0237,0.033,0.0627,0.0401,0.0317,-0.0021,-0.0567,0.0013,0.023,0.0253,-0.0032,-0.0613,-0.0343,-0.025,-0.0023,-0.0139,-0.0553,0.0008,0.0073,-0.0658,-0.0488,-0.0304,0.0309,-0.047,-0.013,0.0044,0.0149,-0.0631,0.2072,-0.0471,0.039,0.0509,-0.015,0.0001,-0.0163,-0.0276,-0.0338,-0.0014,-0.0068,-0.0233,-0.0411,-0.0115,-0.0513,0.1,0.0238,0.0633,0.053,-0.0388,-0.0332,-0.0179,0.0306,0.0325,-0.0031,-0.0225,-0.0694,-0.0239,0.1211,0.0603,0.0295,0.0458,-0.0386,-0.034,-0.0475,0.028,0.0022,-0.0002,-0.0129,-0.012,-0.0045,-0.0425,-0.0662,0.0412,-0.0871,-0.0844,0.094,-0.0167,0.0172,-0.0209,-0.0046,-0.0034,0.0325,0.0062,-0.0664,0.0145,0.0622,0.0147,0.0188,-0.0528,0.0223,0.0142,-0.077,-0.0288,0.0896,0.0343,-0.1089,-0.0389,-0.0038,0.0245,-0.054,0.0832,0.046,-0.0603,0.0405,0.0476,0.0197,-0.0424,0.0049,-0.0027,-0.039,0.0392,-0.0765,-0.0799,0.0626,0.0335,-0.039,0.0063,-0.0429,-0.0017,0.0237,-0.0219,0.0373,-0.0483,0.015,-0.0208,-0.0164,-0.0357,-0.002,-0.0053,-0.038,-0.0143,-0.009,-0.0367,0.0216,-0.0366,0.0282,0.0034,-0.0042,0.0577,0.0156,0.0029,0.002,0.0328,-0.0685,-0.0091,-0.0406,-0.0371,0.0285,-0.0521,0.0448,0.0537,-0.0626,-0.0172,-0.2429,0.0288,-0.0103,-0.0469,0.0604,-0.0294,0.0103,0.0199,0.001,0.0675,0.068,0.0106,-0.0402,0.0053,-0.035,0.0677,0.0548,0.0451,-0.0159,-0.0026,-0.0299,0.0217,-0.0044,-0.1008,0.0908,0.0276,0.2045,0.0048,0.0677,-0.0091,0.0288,0.013,-0.0259,-0.0964,0.0589,-0.0147,0.0657,0.0022,-0.0333,-0.0354,-0.0614,0.0259,0.0521,-0.1001,0.01,-0.0108,-0.0162,0.0302,-0.0352,0.0291,-0.0027,-0.0204,0.0105,0.042,-0.0422,-0.0335,-0.109,0.0234,-0.035,0.0129,0.0321,-0.0302,-0.0106,-0.0539,0.0947,-0.0208,-0.0266,-0.0044,0.0263,-0.0152,-0.0326,0.0814,-0.0053,0.0257,0.0511,-0.0153,0.014,-0.0009,-0.0318,0.0083,0.0437,-0.0211,0.0425,0.0323,0.0586,0.0263,0.0571,-0.0289,0.0685,0.0021,0.0043,0.0102,-0.0826,0.0038,0.0537,0.0048,-0.3108,0.0496,-0.0001,0.0582,-0.0215,0.0526,0.0385,0.0468,-0.0342,-0.0365,0.0138,0.0049,0.0544,0.0081,-0.0111,-0.001,0.0445,-0.0411,0.0773,-0.0203,-0.0204,0.0812,0.2179,-0.0052,0.0635,0.0005,-0.0069,0.0036,0.049,0.0205,0.0314,0.0408,0.0864,-0.0467,0.0049,0.1038,-0.0286,0.0312,0.0364,-0.0562,-0.0276,-0.0281,-0.0578,-0.0302,0.0766,-0.0135,-0.0208,-0.0311,0.0101,0.0465,0.0066,0.0071,-0.0275,-0.0044,0.0642,-0.0326,-0.0088,-0.0066,-0.0374,-0.0434,-0.0098,-0.0414,-0.0367,0.024,-0.0051]}
{"key":"[Achieving Fairness with a Simple Ridge Penalty] In this paper we present a general framework for estimating regression models subject to a user-defined level of fairness. We enforce fairness as a model selection step in which we choose the value of a ridge penalty to control the effect of sensitive attributes. We then estimate the parameters of the model conditional on the chosen penalty value. Our proposal is mathematically simple, with a solution that is partly in closed form, and produces estimates of the regression coefficients that are intuitive to interpret as a function of the level of fairness. Furthermore, it is easily extended to generalised linear models, kernelised regression models and other penalties; and it can accommodate multiple definitions of fairness. We compare our approach with the regression model from Komiyama et al. (2018), which implements a provably-optimal linear regression model; and with the fair models from Zafar et al. (2019). We evaluate these approaches empirically on six different data sets, and we find that our proposal provides better goodness of fit and better predictive accuracy for the same level of fairness. In addition, we highlight a source of bias in the original experimental evaluation in Komiyama et al. (2018).","layer":0,"vector":[-0.0142,-0.018,0.0219,-0.0044,0.028,0.0158,0.0366,0.0688,0.0632,-0.0075,0.0194,-0.0584,-0.0081,0.0192,0.0291,0.0181,-0.005,0.0303,-0.0851,0.0319,-0.0226,-0.0178,-0.0305,-0.0642,0.0328,0.0164,-0.0414,-0.0308,-0.0524,-0.2713,-0.0032,-0.0596,0.0476,-0.0072,0.0039,-0.0371,-0.0403,0.0065,-0.0139,0.0366,-0.0007,0.0139,-0.0099,-0.011,-0.0173,-0.0089,-0.0163,0.0181,-0.0791,-0.0385,0.0234,-0.0568,-0.0072,0.0655,0.0147,-0.0026,0.0686,0.0305,0.036,0.0307,0.0237,0.0103,-0.1598,0.0495,0.0495,0.0311,-0.022,-0.0285,-0.0349,0.0369,-0.0277,0.0474,0.0394,0.0181,-0.0132,-0.0186,0.0488,-0.0008,-0.0011,0.01,0.0326,0.0083,-0.0049,0.0161,0.001,-0.0708,0.0221,-0.0528,0.0412,0.0008,-0.0198,-0.0173,-0.0053,-0.0092,-0.0636,-0.0189,0.0423,0.0225,-0.0416,0.2345,-0.0461,0.0691,0.015,-0.0064,0.047,-0.0213,-0.0264,-0.0129,-0.0203,0.0057,0.0217,-0.0173,0.0206,-0.022,0.0102,0.0186,0.0471,0.021,0.0042,0.0067,-0.0109,-0.0046,0.0291,-0.0413,0.0234,-0.0269,0.0442,0.1844,0.0047,0.0223,0.0004,-0.0872,-0.057,-0.0208,-0.0001,0.022,0.0214,0.0355,0.0144,0.0209,-0.0634,-0.0756,0.0359,-0.0712,-0.0508,0.1305,-0.037,0.0117,-0.0534,0.0063,-0.0112,0.0373,-0.0088,-0.0227,0.0335,0.0363,0.0127,0.034,-0.0287,0.0405,0.0083,-0.0514,-0.0644,0.0801,-0.0426,-0.0864,-0.0056,0.0766,0.0138,-0.001,0.0527,-0.0142,-0.0408,0.0301,0.0742,0.0097,-0.0397,0.0593,-0.0295,-0.0056,0.0273,-0.0492,-0.0537,0.0446,0.0274,-0.0321,-0.0135,-0.0393,0.0642,0.0642,-0.0447,-0.0046,-0.0238,0.0129,0.0069,-0.0223,-0.0224,-0.0453,0.0179,-0.0323,0.0031,0.0066,-0.0442,0.0362,0.0036,0.0544,0.0219,-0.0317,0.0538,0.0126,-0.0519,-0.0135,0.0506,-0.0354,-0.0206,0.0595,0.055,0.0646,0.0229,0.0723,0.0299,-0.0337,-0.0424,-0.2234,-0.0077,0.0099,0.015,0.0611,-0.0277,0.0192,-0.0254,0.0296,0.0965,0.0741,-0.0257,-0.0428,0.072,0.007,0.0533,-0.0024,0.0487,-0.0363,-0.0051,-0.0482,-0.0136,-0.0179,-0.0587,0.0628,0.0116,0.2201,-0.004,-0.0382,-0.0365,0.0252,-0.0135,-0.0116,-0.117,0.0322,0.0175,0.0144,-0.0442,-0.0636,-0.0413,0.01,0.0409,0.0262,-0.1134,-0.0606,-0.011,-0.0353,0.0239,-0.0507,0.0232,0.0008,-0.0348,0.0787,-0.0087,0.0293,-0.041,-0.1096,0.0601,-0.0435,0.0462,0.0224,-0.1141,-0.0037,-0.0708,0.0272,0.0165,-0.0283,-0.076,-0.0167,-0.0047,0.0231,0.0817,-0.0144,-0.0104,-0.0043,0.0517,-0.028,-0.0125,-0.0246,-0.0035,0.1142,-0.0153,-0.005,0.0409,0.0259,-0.0194,0.0501,0.0176,0.0582,-0.0662,-0.0387,-0.0535,-0.0803,0.0092,0.0315,0.0145,-0.2507,0.0052,-0.0107,0.0198,-0.0389,0.0217,0.0454,-0.011,-0.0692,0.0136,0.0183,0.0701,0.0271,-0.0112,0.0167,0.0212,0.1042,-0.0718,0.0331,-0.0518,0.0366,0.0308,0.1849,-0.0515,0.0451,0.0129,0.0014,-0.0219,0.0572,0.0198,0.0093,0.0375,0.0801,-0.0148,0.0355,0.045,-0.0424,-0.0098,0.0009,-0.0225,-0.0258,0.0399,-0.0384,0.02,0.0921,0.0032,-0.0505,-0.0531,0.0187,0.0295,-0.0251,0.0472,-0.0233,-0.009,0.0295,0.0356,-0.0565,-0.0253,-0.0434,-0.0614,0.0141,-0.0278,-0.053,0.0194,-0.0529]}
{"key":"[Interpreting Layered Neural Networks via Hierarchical Modular Representation] Interpreting the prediction mechanism of complex models is currently one of the most important tasks in the machine learning field, especially with layered neural networks, which have achieved high predictive performance with various practical data sets. To reveal the global structure of a trained neural network in an interpretable way, a series of clustering methods have been proposed, which decompose the units into clusters according to the similarity of their inference roles. The main problems in these studies were that (1) we have no prior knowledge about the optimal resolution for the decomposition, or the appropriate number of clusters, and (2) there was no method with which to acquire knowledge about whether the outputs of each cluster have a positive or negative correlation with the input and output dimension values. In this paper, to solve these problems, we propose a method for obtaining a hierarchical modular representation of a layered neural network. The application of a hierarchical clustering method to a trained network reveals a tree-structured relationship among hidden layer units, based on their feature vectors defined by their correlation with the input and output dimension values.","layer":1,"vector":[-0.0277,-0.0159,0.0278,0.0234,0.0404,0.0275,0.0479,0.0101,0.0202,-0.0271,0.041,-0.0578,0.055,0.0379,0.0404,-0.0181,0.0121,0.0526,-0.0309,-0.0141,-0.0011,-0.0307,-0.067,-0.0598,0.0366,0.0173,-0.0378,-0.0256,-0.0359,-0.2595,0.0134,-0.0527,0.077,-0.0219,0.0103,-0.0077,-0.0222,0.044,-0.0348,0.0467,0.027,-0.0005,-0.0017,-0.0545,-0.0098,-0.0349,0.014,0.0068,-0.0255,-0.0441,0.0313,0.0055,-0.023,0.0396,0.0273,0.0208,0.0384,0.0526,0.0477,0.0371,-0.0062,0.0282,-0.1905,0.0527,0.0699,0.0434,-0.0296,0.0096,0.0248,0.0371,0.0134,0.0324,-0.0337,0.0087,0.0196,-0.0066,0.035,-0.0438,-0.0127,0.0113,0.0054,-0.0191,-0.0211,-0.0104,-0.0201,-0.0584,0.0256,-0.044,0.0338,-0.0246,-0.0603,-0.0419,-0.0635,0.0195,-0.0833,0.0027,0.0387,-0.0011,-0.0802,0.1927,-0.0501,-0.0025,0.0801,-0.0665,-0.0073,-0.0223,-0.0337,-0.01,-0.0527,-0.0126,-0.0207,-0.0452,0.0106,-0.0175,0.0192,0.0038,0.0632,0.053,-0.0124,-0.0348,0.0088,-0.0104,0.017,0.0055,0.0471,-0.044,0.0484,0.1222,0.0541,0.0139,0.054,-0.0106,-0.0264,-0.0277,0.0162,0.0269,0.0416,0.0002,-0.0683,-0.0047,-0.0175,-0.0162,0.0556,-0.0679,-0.0386,0.0962,-0.043,-0.0006,-0.0422,0.0388,-0.0497,0.0258,-0.055,-0.0639,0.0167,0.0388,-0.0013,-0.0058,-0.045,0.0082,-0.0175,-0.0663,-0.0542,0.1043,0.0158,-0.0637,-0.0372,-0.0051,0.0776,-0.0728,0.057,0.0337,0.0138,0.022,0.0672,-0.0016,-0.0769,-0.0196,0.0013,0.0149,0.0576,-0.0518,-0.0539,0.0703,0.0525,0.0084,0.0169,-0.0247,0.0355,0.0289,-0.0826,0.0181,-0.0039,0.0303,0.004,-0.0038,-0.0007,0.0125,0.0141,-0.0469,0.0117,0.0328,-0.0652,0.0285,-0.0771,0.0743,0.0141,0.0131,0.058,0.0206,-0.0057,0.003,0.0817,-0.0474,0.0199,0.0254,0.0212,0.0364,0.0155,0.0717,0.0645,-0.0938,-0.0916,-0.2134,0.0077,0.0587,-0.0206,0.0268,-0.0386,0.0334,-0.0244,0.0253,0.0298,0.0594,0.0254,-0.0421,0.0171,-0.0162,0.0451,0.0406,0.0117,-0.045,0.011,-0.0482,-0.0018,-0.006,-0.0623,0.0265,0.0076,0.1954,0.0163,0.0252,0.025,0.0266,0.0262,-0.0502,-0.0634,0.0719,0.0151,0.0794,-0.0084,-0.0185,-0.0373,-0.0618,0.0268,0.0152,-0.0909,-0.0221,-0.0106,-0.0303,0.0489,-0.0389,-0.0348,0.0143,-0.0122,0.0713,0.0071,0.0245,-0.0113,-0.0852,-0.0125,-0.072,0.0278,0.0288,-0.0431,-0.0223,-0.0968,0.0431,-0.0036,-0.0313,-0.027,0.0,-0.0473,-0.0327,0.0897,0.0076,-0.0301,0.0596,-0.0043,0.0139,-0.0153,-0.016,0.0342,0.0693,-0.0199,0.0769,0.0136,0.0388,-0.0085,0.0494,-0.044,0.0382,-0.0338,-0.0195,-0.0001,-0.0275,-0.0096,0.0426,0.0187,-0.3034,0.0258,0.0159,0.0351,0.0162,-0.0074,0.0131,-0.0023,-0.0109,-0.002,0.0177,0.005,0.0705,-0.0292,-0.0121,0.0453,0.0288,-0.0316,0.0762,-0.0081,0.0255,0.0634,0.2424,-0.0771,0.0346,-0.0011,-0.0445,0.0072,-0.0016,-0.0082,0.0235,-0.0028,0.0956,-0.0293,-0.0016,0.0453,-0.0186,0.023,0.0529,-0.0082,0.0163,0.025,-0.1035,-0.0636,0.0943,-0.0065,-0.0644,-0.0529,-0.0231,0.0468,-0.0004,0.0228,0.0029,-0.0358,0.0023,0.0363,-0.045,-0.052,-0.0626,-0.0345,0.0227,-0.0565,0.0118,0.0106,-0.0501]}
{"key":"[Prediction of peptide bonding affinity: kernel methods for nonlinear modeling] This paper presents regression models obtained from a process of blind prediction of peptide binding affinity from provided descriptors for several distinct datasets as part of the 2006 Comparative Evaluation of Prediction Algorithms (COEPRA) contest. This paper finds that kernel partial least squares, a nonlinear partial least squares (PLS) algorithm, outperforms PLS, and that the incorporation of transferable atom equivalent features improves predictive capability.","layer":4,"vector":[-0.085,-0.0334,0.0182,-0.0259,0.0286,-0.0246,0.059,0.0488,0.0058,-0.0261,0.0075,-0.0497,0.0211,0.0298,-0.0044,-0.0237,-0.0063,0.0638,-0.0634,0.0105,0.0192,-0.0152,-0.0317,-0.0782,0.0288,0.0568,-0.0295,-0.046,-0.036,-0.2221,-0.0171,-0.0535,0.0597,-0.0427,-0.0007,0.0028,-0.0461,0.0387,-0.0211,0.0471,0.0543,-0.0157,-0.0136,-0.0352,-0.0105,-0.0543,-0.0555,-0.0245,-0.0012,0.0057,0.0123,-0.024,0.0187,0.06,0.0503,0.07,0.0225,0.0417,0.0059,0.0484,-0.0122,0.0686,-0.1539,0.0895,0.0533,0.03,-0.0239,-0.0343,0.0491,0.0875,-0.0046,0.0656,0.0358,0.0312,-0.0214,0.0065,-0.0205,-0.0259,-0.0021,0.0247,0.015,0.0156,-0.0252,-0.0114,-0.0237,-0.0381,0.0026,-0.0262,0.0586,0.0697,-0.0541,-0.0493,-0.0055,-0.0046,-0.0967,0.007,0.02,-0.0064,-0.0596,0.196,-0.0702,0.0547,-0.0088,-0.0537,0.0269,-0.0638,-0.0278,-0.0427,-0.0014,0.0066,-0.0115,-0.0068,0.0211,-0.0607,0.0317,-0.0354,0.0466,0.0441,-0.0022,0.0079,-0.0146,0.0337,0.0463,0.0336,0.0334,-0.0065,-0.0268,0.1261,0.0234,0.0549,0.0452,-0.0072,-0.0549,-0.0458,0.0274,0.0144,0.0302,-0.0012,0.0049,0.0025,-0.0349,-0.0778,-0.0232,-0.0419,-0.0474,0.1407,-0.003,0.0027,-0.0529,-0.0021,-0.0007,0.0754,-0.0115,0.0068,0.0716,0.0181,0.0025,0.0423,-0.0343,0.01,-0.0549,-0.0428,-0.0605,0.0839,-0.0182,-0.0526,-0.0603,-0.0115,0.0505,-0.0021,0.0502,0.0338,-0.0279,-0.0001,0.0248,0.0603,-0.037,0.028,0.0017,0.0178,0.0129,-0.0444,-0.0723,0.0402,0.0566,-0.0386,-0.0193,-0.0113,-0.0011,0.0372,-0.0195,0.0326,-0.0431,0.0505,-0.0552,-0.0284,-0.0294,-0.0192,-0.0338,-0.0269,0.0484,0.0065,-0.0604,0.0385,-0.0134,-0.0073,0.0244,-0.0117,0.0195,-0.0001,-0.0531,0.0263,0.0096,-0.0636,-0.0611,-0.0246,-0.0085,0.0664,0.0284,0.0768,0.0819,-0.0448,-0.0611,-0.2271,0.021,-0.0184,-0.0318,0.062,-0.0556,0.0365,0.0006,0.0322,0.086,0.0371,0.0197,-0.0402,0.0249,-0.0127,-0.0061,0.0318,0.0196,-0.0008,0.0145,-0.0165,0.005,0.0042,-0.0125,0.0194,0.0214,0.159,0.0287,-0.0168,-0.0311,0.0175,0.056,-0.0642,-0.0903,0.0235,0.0294,0.0472,-0.0521,-0.0017,-0.0396,-0.0333,0.0101,0.0458,-0.0874,-0.0358,-0.0398,-0.0221,0.0092,-0.0456,0.0479,0.0726,-0.0149,0.0507,-0.018,0.0244,-0.0298,-0.092,0.0088,-0.0223,0.0401,0.0166,-0.0778,0.001,-0.0359,0.04,0.0137,-0.0315,-0.0205,0.0231,-0.0433,-0.0233,0.088,-0.0161,0.0606,0.0558,0.0207,-0.0235,-0.0417,-0.0382,0.0353,0.0538,-0.0483,0.0212,-0.0012,0.0286,0.0324,0.0887,0.0049,0.0102,-0.0773,-0.0293,-0.0109,-0.0527,-0.023,0.0793,0.0243,-0.3215,0.0359,0.0204,0.0468,-0.0202,-0.0174,0.0858,-0.0154,-0.0712,0.0021,-0.0079,0.0202,0.0621,-0.0148,0.0239,0.0024,0.0806,-0.0915,0.0164,-0.0472,0.0129,0.0357,0.2668,-0.0505,0.0367,0.0269,-0.0258,0.0135,0.0146,-0.0026,0.0209,0.0135,0.0813,-0.0439,0.0348,0.1096,-0.0418,-0.0193,0.0121,0.0241,0.0199,0.0155,-0.0583,-0.023,0.1038,-0.0118,-0.0232,-0.0288,0.0089,0.0155,-0.0407,0.0099,-0.0093,0.0009,0.0243,-0.009,-0.0505,-0.0448,-0.0172,-0.0137,0.0159,-0.0394,-0.0436,0.0099,-0.0039]}
{"key":"[Solving Traffic4Cast Competition with U-Net and Temporal Domain Adaptation] In this technical report, we present our solution to the Traffic4Cast 2021 Core Challenge, in which participants were asked to develop algorithms for predicting a traffic state 60 minutes ahead, based on the information from the previous hour, in 4 different cities. In contrast to the previously held competitions, this year's challenge focuses on the temporal domain shift in traffic due to the COVID-19 pandemic. Following the past success of U-Net, we utilize it for predicting future traffic maps. Additionally, we explore the usage of pre-trained encoders such as DenseNet and EfficientNet and employ multiple domain adaptation techniques to fight the domain shift. Our solution has ranked third in the final competition. The code is available at https://github.com/jbr-ai-labs/traffic4cast-2021.","layer":7,"vector":[-0.0671,-0.0447,0.0275,-0.0057,0.0537,0.0592,0.0099,0.0119,0.0201,-0.0131,-0.0403,-0.0323,-0.0011,0.0142,-0.0066,0.0271,-0.0079,0.0118,0.0128,0.0076,0.073,-0.0278,-0.0004,-0.0645,-0.0006,0.0411,0.0388,-0.0374,-0.0604,-0.1989,0.0071,-0.0799,0.0182,-0.0315,0.0202,-0.0489,-0.0405,0.0724,-0.0153,0.0403,0.0262,0.0292,-0.0199,-0.0819,-0.037,-0.0668,-0.0035,0.0347,-0.0339,-0.0324,0.0222,-0.039,0.0157,0.0315,0.0336,-0.0048,0.0504,0.0531,0.0374,0.042,0.0241,0.0432,-0.1983,0.0594,0.0472,0.0619,-0.0161,-0.0011,0.0011,0.0581,-0.0302,0.054,0.009,0.0355,0.0172,0.0463,-0.0138,-0.0106,0.0223,0.0188,0.0155,-0.0017,-0.0354,-0.0516,-0.0046,-0.0319,-0.0067,-0.0391,0.0514,-0.0004,-0.0807,-0.0333,-0.0202,0.0452,-0.0748,-0.0084,0.0152,-0.0082,-0.0004,0.1928,-0.0602,0.0221,0.0048,-0.004,0.0199,-0.0154,0.0138,-0.0073,-0.022,0.0366,-0.0126,-0.0388,0.0601,0.0025,0.0453,0.0127,0.0845,0.0633,-0.0351,0.0287,-0.0323,0.0128,0.0448,-0.0409,0.0355,-0.0426,0.0227,0.1357,0.0201,0.0342,0.0353,-0.0014,-0.0774,0.025,-0.0074,0.0044,0.0307,-0.0549,-0.0372,-0.0018,-0.0173,-0.0721,-0.0191,-0.0872,-0.0257,0.1116,-0.0124,0.0006,-0.0087,-0.01,-0.0305,0.0014,-0.0356,-0.0451,-0.0063,0.0336,0.0941,0.0481,-0.0364,0.0473,-0.0471,-0.0469,-0.0488,0.1016,0.0006,-0.0808,-0.0297,0.0247,0.0147,-0.0581,-0.0247,-0.0224,-0.0151,0.0291,0.0873,0.014,-0.0577,0.0269,-0.0093,0.0143,0.0296,-0.0632,-0.0518,0.0794,0.0458,-0.0578,-0.0164,-0.0324,0.013,0.0164,-0.0303,-0.0167,-0.0554,0.0565,-0.0387,-0.0106,-0.0173,0.0254,0.045,-0.0439,0.0191,0.0117,-0.0007,-0.0207,-0.0488,-0.0192,-0.0134,0.0073,0.0085,0.031,-0.0333,0.0209,0.0596,-0.0218,-0.0121,-0.0246,0.0243,0.042,0.0202,0.0698,0.0052,0.0081,-0.0393,-0.198,-0.0012,-0.0082,-0.0358,0.0719,-0.0159,0.028,0.0299,0.0386,0.0657,0.0683,-0.019,-0.0235,0.042,-0.0223,0.0403,0.0314,0.0376,-0.0252,-0.0196,0.019,0.0126,-0.0037,-0.1132,0.0597,0.0285,0.1977,0.0127,0.0376,-0.065,0.0321,0.0044,-0.0277,-0.0922,0.037,0.0112,0.1122,-0.009,-0.0443,-0.0387,-0.0417,0.028,0.013,-0.1077,-0.0535,-0.0316,-0.0302,0.0497,-0.0503,0.0112,0.0308,-0.0589,0.0718,0.0044,-0.0105,-0.0322,-0.0366,0.0307,0.0098,0.0478,0.0082,-0.0486,0.0178,-0.038,0.019,0.0014,-0.0539,-0.0496,-0.0111,-0.0197,-0.0568,0.0638,-0.0099,-0.0143,0.0467,-0.0057,0.0102,-0.0047,-0.0217,-0.0459,0.0777,-0.0577,0.0808,0.0186,0.0786,0.0443,0.0898,0.01,0.0009,0.0014,0.0242,0.0033,-0.0337,-0.092,0.0063,-0.0386,-0.3256,0.0443,0.0625,0.0349,-0.0072,-0.0326,0.0619,0.0321,-0.0485,-0.0062,0.0107,0.0325,0.0264,-0.0222,0.0114,0.0309,0.0703,-0.0222,-0.0238,-0.06,0.0113,0.0344,0.2395,-0.061,0.0948,0.0456,-0.0456,-0.0155,0.0462,-0.0451,-0.0046,-0.0235,0.0774,-0.0715,0.0157,0.0278,-0.0484,0.0218,0.0175,0.0202,-0.0249,0.0227,0.0076,0.0192,0.0621,0.0559,-0.0255,-0.072,-0.016,0.0249,-0.0124,-0.0366,-0.0152,-0.0006,0.0408,0.0331,-0.0564,-0.0486,-0.0564,-0.0257,0.0089,-0.0886,0.0063,-0.0167,0.0045]}
{"key":"[Pay Attention: Leveraging Sequence Models to Predict the Useful Life of Batteries] We use data on 124 batteries released by Stanford University to first try to solve the binary classification problem of determining if a battery is \"good\" or \"bad\" given only the first 5 cycles of data (i.e., will it last longer than a certain threshold of cycles), as well as the prediction problem of determining the exact number of cycles a battery will last given the first 100 cycles of data. We approach the problem from a purely data-driven standpoint, hoping to use deep learning to learn the patterns in the sequences of data that the Stanford team engineered by hand. For both problems, we used a similar deep network design, that included an optional 1-D convolution, LSTMs, an optional Attention layer, followed by fully connected layers to produce our output. For the classification task, we were able to achieve very competitive results, with validation accuracies above 90%, and a test accuracy of 95%, compared to the 97.5% test accuracy of the current leading model. For the prediction task, we were also able to achieve competitive results, with a test MAPE error of 12.5% as compared with a 9.1% MAPE error achieved by the current leading model (Severson et al. 2019).","layer":1,"vector":[-0.0557,0.0247,0.0158,-0.0329,0.0424,0.0382,0.0378,0.0238,0.0176,-0.0243,-0.0195,-0.031,0.0199,0.0954,0.0091,0.0244,0.0076,0.0347,-0.0515,-0.0056,0.0373,-0.0367,-0.0088,-0.0363,0.0411,0.0241,0.004,-0.0332,-0.0896,-0.27,-0.0165,-0.0414,0.0613,-0.0366,-0.0042,-0.0146,-0.0285,0.0259,-0.0357,0.0248,0.0522,0.0067,-0.0484,-0.023,-0.0418,-0.0595,0.0094,-0.0539,0.0098,-0.0632,0.0353,-0.0463,0.0256,0.0133,-0.0032,0.0403,0.0517,0.0192,0.0759,-0.0065,0.0228,0.0229,-0.1481,0.0399,0.0277,0.0356,-0.0516,-0.0176,-0.0381,0.0537,0.0113,0.0114,0.0207,0.0567,-0.0276,0.038,0.0039,-0.0249,-0.0345,-0.0174,0.0055,-0.027,-0.0138,-0.0389,0.0055,-0.045,0.0129,-0.0272,0.0442,-0.0042,-0.0398,0.0038,-0.0394,0.0369,-0.0754,-0.0128,0.0311,0.0006,-0.0604,0.1869,-0.0606,0.0472,0.0394,-0.0592,0.0044,0.003,-0.0252,-0.0234,-0.0723,-0.0281,-0.0037,-0.0024,0.0243,-0.0212,0.0298,0.0244,0.0618,0.0677,-0.0107,-0.0033,-0.0221,0.0235,0.066,-0.0423,0.0131,-0.0753,0.0427,0.1337,-0.0004,-0.0131,0.0353,-0.008,-0.0883,0.0148,0.0127,0.021,0.046,-0.0204,0.0529,-0.0239,-0.0338,-0.0175,0.0296,-0.0765,-0.0644,0.0731,-0.0175,-0.0128,-0.0724,-0.021,-0.0008,0.0249,0.0044,-0.0379,0.1006,0.0341,0.0459,0.0222,-0.0283,0.0329,-0.0409,0.0014,-0.0434,0.0865,-0.002,-0.0628,-0.0153,-0.0253,-0.0101,-0.0311,0.0637,0.057,-0.0222,0.0568,0.0627,0.0672,-0.0467,-0.0193,-0.0142,-0.0041,0.0093,-0.007,-0.0366,0.0351,0.063,-0.0386,0.0067,-0.0716,0.0124,0.076,-0.0379,0.0367,-0.0104,0.0527,0.0071,-0.0173,-0.0446,-0.0163,0.0242,-0.0236,0.0058,0.0123,-0.0206,0.057,0.0074,0.0214,-0.0242,0.0466,0.0563,-0.0027,-0.0187,-0.0089,0.049,-0.0508,-0.0404,-0.0473,0.0208,0.0236,0.0056,0.0644,0.0221,-0.0428,-0.0541,-0.2539,0.0091,0.003,-0.0019,0.0566,-0.0563,0.0455,-0.018,0.0395,0.0226,0.0417,-0.0465,-0.0242,-0.015,0.0072,0.0782,0.0417,0.0001,-0.0421,0.0176,0.0063,-0.0056,0.0399,-0.1122,0.0618,-0.012,0.2225,0.0169,0.0312,-0.0158,0.02,-0.0076,-0.0327,-0.0637,0.089,-0.01,0.0436,-0.002,-0.0661,-0.0193,-0.0767,0.0168,-0.0377,-0.1132,-0.0572,-0.0433,-0.0365,0.0105,-0.0702,0.0023,0.0553,-0.0364,0.0273,-0.0335,0.0105,-0.0643,-0.116,0.0656,-0.0161,-0.0135,-0.0177,-0.0234,-0.0074,-0.0382,0.0387,-0.0201,-0.0505,-0.0635,0.0196,-0.0218,-0.0028,0.1109,-0.0139,-0.0283,0.0675,-0.0055,0.0195,0.0023,-0.0233,0.0098,0.0792,0.0062,0.0226,-0.0054,0.0901,0.0621,0.0804,-0.0057,0.0514,-0.028,0.0245,0.0209,-0.0612,-0.0598,0.0561,0.019,-0.2587,0.0742,0.0031,0.0586,-0.0274,-0.0204,0.0368,0.0344,0.0186,0.0195,0.0025,0.0747,0.0149,-0.0183,0.0188,0.012,0.0626,-0.0884,0.0499,-0.0583,0.0267,0.0288,0.1868,-0.0617,0.044,0.0161,-0.0412,0.0077,0.0571,-0.0081,-0.0025,-0.0111,0.0915,-0.0234,0.0234,0.1115,-0.0235,0.0028,0.0481,0.0174,0.0045,0.0256,-0.0343,-0.0265,0.122,-0.0585,-0.0181,-0.0561,-0.0103,0.038,-0.0176,-0.0064,-0.0355,0.0226,-0.0022,0.0068,-0.0168,-0.0389,-0.0145,-0.0295,0.0282,-0.0559,-0.0133,0.0071,-0.0249]}
{"key":"[Relative concentration bounds for the spectrum of kernel matrices] In this paper we study the concentration properties for the eigenvalues of kernel matrices, which are central objects in a wide range of kernel methods and, more recently, in network analysis. We present a set of concentration inequalities tailored for each individual eigenvalue of the kernel matrix with respect to its known asymptotic limit. The inequalities presented here are of relative type, meaning that they scale with the eigenvalue in consideration, which results in convergence rates that vary across the spectrum. The rates we obtain here are faster than the typical $\\O(\\frac{1}{\\sqrt n})$ and are often exponential, depending on regularity assumptions of Sobolev type. One key feature of our results is that they apply to non positive kernels, which is fundamental in the context of network analysis. We show how our results are well suited for the study of dot product kernels, which are related to random geometric graphs on the sphere, via the graphon formalism. We illustrate our results by applying them to a variety of dot product kernels on the sphere and to the one dimensional Gaussian kernel.","layer":2,"vector":[-0.0345,-0.0404,0.03,-0.0357,-0.0038,0.0303,0.0452,0.0575,0.0442,-0.0058,0.0062,-0.0483,0.0505,0.0628,0.0357,0.0698,0.0349,0.02,-0.0654,0.0097,0.014,-0.0851,-0.0297,-0.0661,0.0582,0.0159,-0.0221,-0.0754,-0.0525,-0.254,0.0363,-0.0257,0.0755,-0.0163,-0.0019,-0.0465,0.018,-0.0256,-0.0229,0.0388,0.0276,0.0368,-0.0463,-0.0127,-0.0454,-0.0589,-0.0533,-0.0063,-0.033,-0.0552,0.0178,0.0089,0.0297,0.0286,0.0264,0.0185,0.0544,0.0197,0.0553,0.0927,-0.0024,0.0128,-0.1494,0.0595,0.0536,0.0013,-0.047,-0.0378,0.0103,0.0616,0.0348,0.0709,0.0029,0.0093,0.0205,0.0186,0.0043,-0.0219,-0.0376,0.0158,-0.015,-0.0334,-0.0376,-0.0024,-0.0079,-0.0217,-0.0016,-0.0425,0.0198,0.0719,-0.0456,-0.0099,-0.0039,0.0049,-0.028,-0.0194,0.0247,0.002,-0.0108,0.203,-0.0489,0.0041,0.0279,-0.0303,-0.0124,-0.0327,-0.0254,-0.0173,0.0352,0.0079,0.0192,-0.0578,0.0087,-0.0639,0.0486,-0.0103,0.051,0.0517,-0.0278,-0.0205,-0.0421,0.0404,0.0269,-0.0367,0.0517,-0.0193,-0.0178,0.1251,0.0535,0.0586,0.0119,-0.0072,-0.0492,-0.0047,-0.0062,-0.0019,-0.0206,0.0255,0.0037,0.0287,0.018,-0.0606,0.0225,-0.0759,0.0091,0.1321,-0.0477,0.0259,-0.0579,-0.0252,0.011,0.0522,-0.0297,-0.0093,0.0168,0.0072,0.017,0.0192,-0.0682,0.037,-0.0596,-0.0353,-0.0113,0.1583,0.0051,-0.0603,-0.0094,0.011,-0.0177,-0.0187,0.0417,0.0037,-0.0139,0.0124,0.0632,0.0041,-0.1065,0.0275,0.0142,-0.0159,0.0219,0.0197,-0.0448,0.0396,0.0296,-0.0433,-0.0051,-0.0013,0.032,0.0375,-0.0614,-0.0091,-0.0297,-0.012,-0.0616,-0.0356,0.0034,-0.03,0.0314,-0.0076,0.0357,-0.009,-0.0667,0.0426,-0.0051,0.0502,-0.0395,0.0289,-0.0072,-0.0103,-0.0634,-0.0381,0.0037,-0.0253,-0.0321,0.0275,0.0213,0.0252,-0.0085,0.0386,0.0327,-0.0314,-0.1311,-0.2117,-0.0363,-0.0385,0.0289,0.0524,-0.028,0.0568,-0.0065,0.101,0.0676,0.0372,0.0021,-0.0663,0.0082,0.0101,0.0457,0.0037,0.0339,0.0032,-0.0145,-0.0268,0.0163,-0.016,-0.026,0.0813,-0.0251,0.2064,0.0005,0.0138,-0.0538,0.0053,0.0505,-0.0462,-0.0153,0.0651,0.0553,0.0574,0.0069,-0.0338,0.0035,0.0025,-0.0113,-0.0087,-0.0866,-0.0326,-0.022,0.0107,0.0252,-0.0397,0.0105,0.0674,-0.0343,0.0805,-0.0292,0.0266,-0.0571,-0.0705,0.0122,-0.0525,0.0356,-0.0156,-0.1327,-0.0002,-0.0562,0.0682,0.0295,0.001,-0.0193,0.0414,0.011,-0.0034,0.0613,-0.0178,-0.0197,0.0661,0.009,0.0102,0.0228,-0.0144,0.0006,0.0836,-0.0866,0.0385,-0.0045,0.0576,-0.0014,0.0632,0.0114,-0.0015,-0.0492,-0.023,0.0421,-0.0733,0.0088,0.0494,-0.0106,-0.285,0.019,0.0109,0.0279,-0.0123,-0.0036,0.0365,0.0193,-0.0954,-0.0443,0.0739,0.0755,0.0202,-0.0263,0.0223,0.0735,0.0522,-0.0396,0.0246,-0.024,0.016,0.0032,0.2119,-0.0696,0.0216,0.0084,-0.0053,0.0312,-0.017,-0.0408,0.0124,0.01,0.0873,-0.05,0.0422,0.0589,-0.0661,0.0632,0.0079,-0.0223,0.0366,-0.0217,-0.0483,-0.0082,0.0668,-0.0327,-0.0352,-0.0821,0.0405,0.0076,-0.0106,0.0201,0.057,0.0198,-0.0119,0.0279,-0.0466,-0.0452,-0.0879,-0.0514,0.0009,-0.0541,-0.0225,-0.0177,0.0012]}
{"key":"[Imperceptible Adversarial Attacks on Tabular Data] Security of machine learning models is a concern as they may face adversarial attacks for unwarranted advantageous decisions. While research on the topic has mainly been focusing on the image domain, numerous industrial applications, in particular in finance, rely on standard tabular data. In this paper, we discuss the notion of adversarial examples in the tabular domain. We propose a formalization based on the imperceptibility of attacks in the tabular domain leading to an approach to generate imperceptible adversarial examples. Experiments show that we can generate imperceptible adversarial examples with a high fooling rate.","layer":1,"vector":[-0.0361,-0.0465,-0.009,-0.0152,0.0025,0.0305,0.0543,0.0072,-0.0013,0.0046,-0.0101,-0.0172,-0.0079,0.0642,-0.0164,0.0315,-0.0485,0.0107,-0.0503,0.0397,0.049,-0.022,-0.0159,-0.0492,0.0509,0.0053,-0.0271,-0.0398,-0.037,-0.2359,0.0375,-0.0448,-0.0003,-0.0318,0.0339,-0.0361,-0.0513,0.058,-0.0106,-0.0067,-0.0058,0.0203,-0.0152,-0.0596,0.0063,-0.0403,-0.0415,0.0103,0.0067,-0.0357,0.0169,-0.0336,0.0307,0.0296,0.0226,-0.0077,0.0459,0.0606,0.0248,0.0599,0.0636,0.0497,-0.1492,0.0687,0.0427,0.0472,-0.0418,-0.0212,0.012,0.0167,0.0056,0.0159,-0.0048,0.029,0.017,0.0169,-0.0453,-0.0565,-0.0361,0.0055,0.05,-0.0109,-0.0159,0.0183,-0.037,-0.0317,0.0221,-0.0175,0.098,-0.0103,-0.0096,0.0192,-0.0262,0.0621,-0.0504,-0.0078,0.0402,0.013,-0.0757,0.1964,-0.0475,-0.0013,0.0279,-0.0348,0.0166,-0.0175,-0.0379,-0.0619,-0.0019,-0.0208,-0.0241,-0.0091,0.0354,-0.0211,0.0071,-0.0384,0.0469,0.0132,0.005,-0.0423,-0.0083,0.0221,0.0531,0.0093,0.0267,-0.0804,0.0245,0.1563,0.0539,0.0213,0.0279,-0.0669,-0.0217,0.0453,0.0244,0.0076,-0.0023,0.0304,0.0218,0.0057,-0.0904,-0.0808,0.0159,-0.0509,-0.0124,0.1169,-0.0028,0.0595,-0.0431,-0.0392,0.0027,0.0108,-0.0251,-0.0226,0.0162,0.0449,0.004,0.0386,-0.0183,-0.001,-0.0316,-0.0607,-0.0393,0.1504,0.0367,-0.0922,-0.0131,0.0054,0.0476,-0.0088,-0.0019,0.0134,-0.0422,0.0526,0.0434,0.0449,-0.089,-0.0502,-0.0141,0.0322,0.0422,-0.0463,-0.0383,0.046,0.0228,-0.0353,0.0241,-0.0492,-0.0093,0.053,-0.0509,0.0269,-0.0729,-0.0254,-0.0471,-0.0386,-0.0032,-0.0273,0.0037,-0.0346,-0.005,0.0032,-0.0382,0.007,0.0069,0.0209,0.0049,-0.0251,0.0376,0.0018,-0.0323,-0.0008,0.0426,-0.0291,-0.0295,-0.0027,-0.0001,0.0765,-0.0123,0.0463,0.0457,-0.0395,-0.0371,-0.2629,-0.0256,-0.0492,-0.0581,0.0508,-0.07,0.0256,-0.0374,0.0051,0.0483,0.0285,-0.0646,-0.0218,0.0051,-0.0312,0.0456,-0.0086,0.0152,-0.0196,0.014,-0.0166,0.0521,-0.0317,-0.0607,0.0416,0.0535,0.2181,0.0286,0.0326,-0.0098,0.022,0.0209,-0.0012,-0.081,0.0547,-0.0064,0.0435,0.0054,-0.0245,-0.051,-0.0339,0.0472,0.0105,-0.074,-0.0103,-0.0105,-0.0506,0.0501,-0.0799,0.0685,0.0377,-0.0179,0.1052,0.031,0.0435,-0.0805,-0.0809,0.067,-0.043,0.0402,0.0112,-0.0594,0.0073,-0.0591,0.0468,-0.0011,-0.0188,-0.0884,0.0178,-0.0056,-0.018,0.0859,0.0721,-0.019,0.0582,0.0121,0.0206,-0.0533,-0.075,0.0022,0.0137,0.0046,0.0173,0.0413,0.0321,0.0192,0.0503,-0.0048,0.0678,-0.0386,-0.0117,0.0043,-0.076,0.0231,0.0743,-0.0147,-0.2926,0.0278,0.0064,0.0457,-0.0356,0.0441,0.0085,0.0041,-0.08,-0.0085,-0.0012,0.0385,0.05,-0.0161,0.0101,0.0067,0.077,-0.0842,0.0319,-0.0042,0.0534,0.0528,0.2404,-0.0459,0.0126,0.0154,-0.0225,0.0241,0.0033,0.0066,0.0447,-0.0226,0.0677,-0.0083,0.0121,0.0888,-0.0351,0.0354,0.0063,-0.0332,-0.0087,0.0618,-0.0655,0.0286,0.0716,0.0225,0.0353,-0.0126,-0.0057,0.0045,-0.0347,-0.0195,-0.0183,0.003,0.0426,0.017,-0.0381,-0.0343,0.0024,-0.0399,-0.0019,-0.0415,-0.0146,0.0065,-0.0154]}
{"key":"[Multi-layer Kernel Ridge Regression for One-class Classification] In this paper, a multi-layer architecture (in a hierarchical fashion) by stacking various Kernel Ridge Regression (KRR) based Auto-Encoder for one-class classification is proposed and is referred as MKOC. MKOC has many layers of Auto-Encoders to project the input features into new feature space and the last layer was regression based one class classifier. The Auto-Encoders use an unsupervised approach of learning and the final layer uses semi-supervised (trained by only positive samples) approach of learning. The proposed MKOC is experimentally evaluated on 15 publicly available benchmark datasets. Experimental results verify the effectiveness of the proposed approach over 11 existing state-of-the-art kernel-based one-class classifiers. Friedman test is also performed to verify the statistical significance of the claim of the superiority of the proposed one-class classifiers over the existing state-of-the-art methods.","layer":0,"vector":[-0.0233,-0.0399,0.04,-0.0213,0.0244,0.0098,0.0428,0.0593,-0.036,-0.0166,0.016,-0.0206,-0.0134,0.0384,0.072,-0.0118,0.0176,0.0393,-0.0251,-0.0197,0.0137,-0.0202,-0.014,-0.0155,0.0363,0.0211,0.0212,-0.1168,-0.0401,-0.2391,0.0233,-0.0475,0.0706,-0.0236,-0.0025,-0.0294,-0.0345,0.0461,-0.0658,0.0662,0.0149,-0.0043,0.0243,-0.0388,-0.0018,-0.0434,-0.0182,-0.0411,0.0076,-0.0017,0.0266,-0.037,-0.0011,0.0446,0.008,0.007,0.0462,0.0669,0.0297,0.0378,0.0308,0.0111,-0.1917,0.0579,0.0371,0.034,-0.0277,-0.0248,-0.0089,0.0518,-0.0005,0.0117,0.0201,0.0442,-0.0329,-0.0324,0.0472,0.0061,-0.0311,0.0168,0.0304,-0.0285,-0.0285,-0.0312,-0.0135,-0.0463,-0.0069,-0.0681,0.0711,0.0145,-0.0232,-0.0276,0.0177,0.0131,-0.0487,0.0126,0.0221,0.015,-0.0691,0.1912,-0.0733,0.02,0.0272,-0.0503,0.0247,-0.027,-0.0157,-0.0311,-0.0308,-0.0599,0.0083,-0.0397,-0.0045,-0.02,0.0399,-0.021,0.0708,0.0599,-0.0032,0.0179,0.0161,-0.0249,0.0252,-0.076,0.0585,-0.0661,0.0766,0.1546,0.0205,0.0371,-0.0041,-0.0392,-0.043,-0.0385,-0.0036,0.0199,0.0228,0.0263,-0.0162,-0.0085,-0.0412,-0.0376,0.0687,-0.0649,-0.0385,0.0971,-0.0631,0.0222,-0.0619,0.0195,-0.0111,0.0162,-0.0627,0.0017,0.0361,0.0308,0.0236,0.0306,-0.0355,0.0023,-0.0148,-0.0744,-0.0643,0.1051,0.0259,-0.1186,-0.0292,0.0062,0.0218,-0.0307,0.0591,0.0088,-0.045,0.02,0.0638,0.0412,-0.0709,0.0567,0.0053,0.0014,0.0134,-0.0249,-0.0653,0.0028,0.0571,-0.0221,-0.0195,-0.0397,0.0262,0.0576,-0.0474,-0.0084,-0.0021,-0.0311,-0.0047,-0.0066,-0.0325,-0.0058,0.0172,-0.0614,0.0164,0.0401,-0.0095,0.0417,-0.0147,0.039,0.0172,0.0265,0.057,-0.018,-0.0287,-0.0339,0.0546,-0.013,-0.0259,0.0371,0.03,0.103,0.0057,0.0604,0.0434,-0.0812,-0.0643,-0.2227,0.0257,0.0197,-0.016,0.0021,-0.0252,0.0645,-0.0119,0.0623,0.0348,0.0675,0.0301,-0.0318,0.0092,0.0001,0.0513,0.0582,0.0269,-0.0101,0.0157,-0.0006,0.0419,0.0126,-0.0571,0.0646,0.0108,0.1705,-0.0124,0.0386,-0.0143,0.0221,0.0021,-0.0178,-0.0881,0.0757,0.0021,0.0539,-0.0185,-0.0778,-0.02,-0.0303,0.0302,0.0184,-0.1221,-0.0372,-0.0731,-0.0418,0.0298,-0.0453,0.0273,0.029,-0.0393,0.0666,0.0077,-0.0048,-0.0188,-0.0999,0.0243,-0.0238,0.0146,0.0461,-0.0772,0.026,-0.062,0.048,0.017,-0.0765,-0.0291,0.0279,-0.0461,-0.01,0.0989,-0.004,-0.0068,0.0685,-0.0053,0.03,0.0039,-0.0135,-0.0194,0.0943,-0.033,0.0377,-0.0007,0.0524,0.0431,0.0715,0.008,-0.0016,-0.0532,-0.0352,0.0181,-0.0526,0.0027,0.0527,0.0277,-0.2892,-0.0103,0.0034,0.0335,-0.0236,-0.0062,0.0136,-0.0019,-0.0587,0.0057,-0.0011,0.0211,0.0468,-0.0186,0.0393,0.0153,0.055,-0.0711,0.0488,-0.0248,0.0321,0.0594,0.202,-0.0505,0.024,0.0117,-0.0264,-0.0157,0.0255,-0.0199,0.0369,0.0287,0.07,-0.0728,0.0084,0.0701,-0.0246,0.0062,0.017,0.0061,0.0019,0.0254,-0.0734,-0.0284,0.0929,0.0198,-0.0223,-0.0792,-0.0622,0.0023,-0.0239,0.0286,-0.0028,0.002,0.0354,0.0454,-0.0718,-0.0311,-0.0835,-0.0505,0.0811,-0.0598,-0.0345,0.0056,-0.0375]}
{"key":"[Robust and Scalable Routing with Multi-Agent Deep Reinforcement Learning for MANETs] Highly dynamic mobile ad-hoc networks (MANETs) are continuing to serve as one of the most challenging environments to develop and deploy robust, efficient, and scalable routing protocols. In this paper, we present DeepCQ+ routing which, in a novel manner, integrates emerging multi-agent deep reinforcement learning (MADRL) techniques into existing Q-learning-based routing protocols and their variants, and achieves persistently higher performance across a wide range of MANET configurations while training only on a limited range of network parameters and conditions. Quantitatively, DeepCQ+ shows consistently higher end-to-end throughput with lower overhead compared to its Q-learning-based counterparts with the overall gain of 10-15% in its efficiency. Qualitatively and more significantly, DeepCQ+ maintains remarkably similar performance gains under many scenarios that it was not trained for in terms of network sizes, mobility conditions, and traffic dynamics. To the best of our knowledge, this is the first successful demonstration of MADRL for the MANET routing problem that achieves and maintains a high degree of scalability and robustness even in the environments that are outside the trained range of scenarios. This implies that the proposed hybrid design approach of DeepCQ+ that combines MADRL and Q-learning significantly increases its practicality and explainability because the real-world MANET environment will likely vary outside the trained range of MANET scenarios.","layer":0,"vector":[-0.0712,0.0061,0.0265,0.0025,-0.0074,0.032,-0.0097,0.027,0.0402,0.0071,0.0315,-0.0414,0.0772,0.0714,0.0435,0.0291,-0.0202,0.0276,0.0072,-0.0039,0.0274,-0.0053,0.0002,-0.0353,0.0105,-0.0034,0.0015,-0.0324,-0.037,-0.2442,0.0407,-0.0218,-0.0044,-0.0199,0.0136,-0.0339,-0.0536,0.0319,0.0104,0.0358,0.0604,0.0272,-0.0014,-0.0441,-0.0264,-0.0759,-0.0297,-0.0421,-0.0048,-0.0336,0.0274,-0.0182,-0.0062,0.0327,0.0428,0.0224,0.0403,0.0637,0.0722,-0.0012,0.0469,0.0616,-0.1624,0.045,0.0536,0.0199,-0.0487,-0.03,0.0096,0.0557,-0.0224,0.0442,0.0352,0.0248,0.0443,0.0435,-0.0109,-0.0348,0.0081,-0.0091,0.02,-0.062,-0.0443,-0.0244,-0.0071,-0.0834,0.0026,-0.0552,0.037,-0.0249,-0.0132,0.0211,-0.0509,0.019,-0.0605,-0.0312,0.014,-0.0224,-0.0956,0.2147,-0.0007,0.0397,-0.015,-0.0205,0.0404,-0.0338,-0.0002,-0.0326,-0.0437,0.0396,-0.0215,-0.0472,0.0468,-0.0332,-0.0216,0.0203,0.0341,0.0467,-0.0061,0.0026,-0.048,-0.0346,0.085,-0.021,0.032,-0.0815,0.0076,0.1524,-0.0148,0.0716,0.0533,-0.0077,-0.0216,-0.0207,0.0247,0.0162,0.0346,0.011,-0.0183,0.0186,-0.0134,-0.0055,-0.0209,-0.1298,-0.0556,0.0469,-0.0047,0.0101,-0.0157,-0.0457,-0.0425,-0.0266,0.0048,-0.0188,-0.006,0.0211,0.0212,0.0792,-0.0346,0.0038,-0.0553,-0.0501,-0.0428,0.1099,-0.0106,-0.1266,-0.0414,-0.0311,0.0264,-0.0145,0.0436,-0.0213,-0.067,0.0156,0.0304,0.0207,-0.0827,0.0443,-0.0294,-0.0405,-0.0131,-0.0215,-0.0027,0.0157,0.0384,-0.0586,-0.0057,-0.0372,0.0264,0.0055,-0.0366,0.0301,-0.0651,-0.0175,-0.015,-0.0287,-0.002,0.0129,0.013,-0.0251,0.0061,-0.0069,-0.0553,-0.0043,-0.0618,-0.0367,-0.0364,0.0135,0.0092,0.0071,-0.0262,-0.0315,0.0459,-0.0128,-0.0376,-0.0046,0.0166,0.0368,-0.0243,0.0056,0.0395,-0.0483,-0.0281,-0.1861,-0.0155,0.0131,-0.0577,0.0627,-0.0383,0.0533,0.0096,0.0229,0.0631,0.0837,-0.0184,-0.0091,0.0221,-0.0084,0.1129,0.0457,0.0406,0.0045,0.0266,0.0324,-0.0186,-0.0415,-0.1214,0.0406,-0.0035,0.2206,0.0302,-0.002,0.018,0.0283,0.0228,-0.0217,-0.1486,0.0637,-0.0087,0.1329,-0.0367,-0.0063,-0.0187,-0.0064,0.0391,-0.0281,-0.1191,-0.0006,-0.0047,-0.0657,0.0805,-0.0521,-0.0277,0.0281,-0.0081,-0.0013,-0.0226,-0.0201,-0.0086,-0.0607,0.0302,-0.0248,0.0469,-0.0183,-0.0207,-0.0007,-0.0412,0.0977,0.0163,0.0083,-0.0122,0.0242,0.0079,-0.0391,0.0646,-0.0007,0.0405,0.0341,-0.0108,0.0442,0.0003,-0.0038,-0.0126,0.0555,-0.03,0.0342,0.0199,0.0083,-0.0006,0.0547,-0.0033,0.0669,0.015,-0.0094,-0.0113,-0.0464,-0.0561,0.0264,-0.0209,-0.2718,0.0364,0.039,0.026,-0.0313,0.0225,0.0688,0.0209,-0.0546,0.0071,0.0507,0.0897,0.0648,0.0175,0.0202,0.0033,0.0999,-0.0289,0.0182,-0.0432,0.0261,0.0218,0.2114,-0.0518,0.0499,0.0467,-0.0504,0.0155,0.037,-0.0636,0.0133,0.0041,0.0955,-0.0891,0.0717,0.078,-0.014,0.0414,0.0256,0.0304,-0.0417,0.0253,0.0085,-0.0007,0.0688,-0.0082,-0.0661,-0.0557,-0.0073,0.0484,-0.0295,-0.0357,-0.0408,0.0041,0.0393,0.0133,-0.0468,-0.0584,-0.0557,-0.0025,0.0294,-0.0498,0.0336,0.0123,-0.0147]}
{"key":"[Malware Classification Using Transfer Learning] With the rapid growth of the number of devices on the Internet, malware poses a threat not only to the affected devices but also their ability to use said devices to launch attacks on the Internet ecosystem. Rapid malware classification is an important tools to combat that threat. One of the successful approaches to classification is based on malware images and deep learning. While many deep learning architectures are very accurate they usually take a long time to train. In this work we perform experiments on multiple well known, pre-trained, deep network architectures in the context of transfer learning. We show that almost all them classify malware accurately with a very short training period.","layer":0,"vector":[-0.0416,-0.0293,0.0134,-0.0077,0.06,0.0045,0.0424,0.0119,0.0231,-0.011,0.0282,-0.0201,0.0348,0.0129,0.0157,0.0074,0.0163,-0.0029,-0.0352,0.0224,0.0291,0.01,0.0141,-0.0464,0.0217,-0.012,-0.006,-0.0247,-0.0933,-0.2122,0.0069,-0.0647,-0.0086,-0.0369,0.0636,-0.0358,-0.0443,0.0379,-0.0246,0.03,0.038,0.0533,-0.016,-0.0325,-0.0614,-0.0818,-0.0564,-0.0386,0.021,-0.0858,0.0413,-0.023,0.0349,0.086,0.0041,-0.0039,0.074,0.0236,0.0543,0.0517,0.0275,0.0708,-0.155,0.0694,0.0084,0.0518,-0.0436,-0.0098,-0.0123,0.054,-0.0778,0.0391,0.0017,0.0523,0.0043,0.0027,0.0013,0.0083,-0.0082,-0.0148,0.0112,-0.0186,-0.0234,-0.0352,-0.0041,-0.0239,0.0202,-0.0346,0.0565,0.0206,-0.0313,0.0144,0.0013,-0.001,-0.0278,0.0044,0.0238,0.006,-0.0672,0.1817,-0.0761,-0.006,0.0163,-0.0218,0.0576,0.0258,0.0059,-0.0322,-0.0259,0.0058,-0.0099,-0.003,0.0286,-0.0159,0.0424,-0.0082,0.0491,0.0229,0.0172,0.0019,-0.012,-0.0063,0.0698,-0.0383,0.051,-0.0343,-0.0019,0.1443,0.0181,0.0459,-0.0025,-0.036,-0.0225,0.0009,0.0276,0.047,0.0256,-0.0436,-0.0192,-0.0708,-0.0668,-0.0291,0.0424,-0.0846,-0.0556,0.0965,-0.0358,0.0344,-0.0089,0.0001,-0.0182,0.0304,-0.0125,-0.0734,0.0044,0.0088,0.0737,0.0863,-0.047,-0.029,0.0057,-0.0389,-0.0588,0.1114,0.0628,-0.0634,-0.0295,-0.0357,-0.0185,-0.0311,0.0347,0.0649,-0.006,-0.0019,0.0041,0.0202,-0.0793,-0.021,-0.0132,0.0003,0.018,-0.0743,-0.043,0.042,0.0869,0.0245,-0.0182,-0.0895,0.0197,0.0516,-0.0316,0.0454,-0.0241,-0.004,-0.0016,-0.017,-0.0337,0.0289,-0.0137,-0.0282,0.0282,0.0205,0.002,0.0108,-0.0477,0.0058,-0.0131,0.0048,0.0253,0.0439,-0.0327,0.0067,0.0056,-0.0344,-0.0233,-0.0313,-0.0271,0.0748,0.0188,0.0483,0.0413,-0.0318,-0.0341,-0.2357,-0.0101,-0.0035,-0.0562,0.0444,-0.0722,0.0503,-0.0218,0.0621,0.0344,0.0663,-0.0154,-0.0191,-0.012,-0.0508,0.0832,0.0477,0.0645,-0.0496,0.0333,-0.0201,0.0018,0.0168,-0.1011,0.0156,0.0343,0.224,0.0477,0.0626,-0.046,0.0144,0.0388,-0.0116,-0.1362,0.0672,0.0116,0.0888,-0.0057,-0.0199,-0.0194,-0.0498,0.0494,0.0227,-0.1118,0.0155,-0.0227,-0.0399,0.013,-0.0455,0.0093,0.0267,0.0096,0.0499,0.0308,0.009,-0.0576,-0.0926,0.0724,-0.0237,0.0442,-0.0001,-0.0844,0.015,-0.0896,0.0436,0.0087,-0.0404,-0.0379,0.054,-0.0189,-0.0434,0.0771,0.0005,0.0104,0.0687,-0.0171,0.025,-0.028,-0.0576,-0.0235,0.0371,-0.0146,-0.0036,0.0166,0.0295,0.0481,0.0791,-0.0044,0.0123,-0.034,0.0168,0.0005,-0.0598,-0.0314,0.0005,-0.0043,-0.2861,0.0231,0.0162,0.0791,0.0019,0.0033,0.0565,-0.0158,-0.0172,0.0135,0.0061,-0.0137,0.0791,-0.0352,0.0018,0.0094,0.058,-0.0569,0.0158,-0.0384,0.0089,0.0077,0.2434,-0.0307,0.0032,0.0043,0.0038,0.0002,0.0118,-0.0411,0.0653,0.006,0.1031,-0.0593,0.0052,0.0776,-0.0292,0.0017,0.0077,-0.0045,-0.0108,0.0098,-0.0279,0.0016,0.0832,-0.0412,0.0001,-0.0799,-0.0083,0.0645,-0.021,-0.0068,0.0003,0.003,0.0394,0.0117,-0.046,-0.0307,-0.064,-0.0563,0.0261,-0.0483,-0.0075,-0.0086,-0.0088]}
{"key":"[Detecting Adversarial Examples in Convolutional Neural Networks] The great success of convolutional neural networks has caused a massive spread of the use of such models in a large variety of Computer Vision applications. However, these models are vulnerable to certain inputs, the adversarial examples, which although are not easily perceived by humans, they can lead a neural network to produce faulty results. This paper focuses on the detection of adversarial examples, which are created for convolutional neural networks that perform image classification. We propose three methods for detecting possible adversarial examples and after we analyze and compare their performance, we combine their best aspects to develop an even more robust approach. The first proposed method is based on the regularization of the feature vector that the neural network produces as output. The second method detects adversarial examples by using histograms, which are created from the outputs of the hidden layers of the neural network. These histograms create a feature vector which is used as the input of an SVM classifier, which classifies the original input either as an adversarial or as a real input. Finally, for the third method we introduce the concept of the residual image, which contains information about the parts of the input pattern that are ignored by the neural network. This method aims at the detection of possible adversarial examples, by using the residual image and reinforcing the parts of the input pattern that are ignored by the neural network. Each one of these methods has some novelties and by combining them we can further improve the detection results. For the proposed methods and their combination, we present the results of detecting adversarial examples on the MNIST dataset. The combination of the proposed methods offers some improvements over similar state of the art approaches.","layer":1,"vector":[0.0196,-0.0056,-0.0087,-0.0246,0.0446,0.0528,0.0676,0.0005,-0.0053,-0.0572,-0.0026,-0.0276,0.0386,0.0745,0.0384,-0.0079,0.011,0.0434,-0.0578,0.0127,-0.0003,-0.0328,0.0148,-0.0167,0.0159,-0.0029,0.0234,-0.0336,-0.0564,-0.2191,-0.0085,-0.0702,0.0623,-0.0368,0.0204,0.0128,-0.0631,0.0327,-0.0461,0.0313,-0.0079,-0.0038,-0.0314,-0.0755,-0.0475,-0.0202,-0.0098,-0.0402,0.0385,-0.0662,0.0595,-0.0288,0.0522,-0.0036,0.0403,-0.0286,0.068,0.0394,0.0396,0.0507,0.0382,0.0954,-0.1755,0.0621,0.0405,0.0587,-0.0348,-0.0192,0.0076,0.0488,0.0177,0.0336,-0.0243,0.034,-0.0296,0.0282,0.0021,-0.0546,-0.0007,-0.0172,0.0478,0.0266,0.0181,-0.0193,0.0329,-0.0208,0.0227,-0.0337,0.0873,-0.0171,-0.0275,0.0191,-0.0419,0.0663,-0.0351,-0.0151,0.0583,0.0149,-0.0576,0.1854,-0.0708,0.0169,0.0437,-0.0497,0.0586,0.0051,-0.0503,-0.0222,-0.0154,0.0308,-0.0339,-0.0218,0.0083,-0.0402,0.0199,-0.044,0.0446,0.051,-0.0648,-0.0198,-0.0528,-0.0165,0.0502,-0.0513,0.0544,-0.0457,0.0177,0.1446,0.0543,0.0129,0.0454,-0.0371,-0.0371,-0.0041,0.0068,0.068,0.0098,0.0152,0.0013,-0.0111,-0.0331,-0.0898,0.0152,-0.0363,-0.0327,0.0574,-0.0254,0.0045,-0.0096,-0.0412,0.0011,-0.0017,-0.0551,0.0078,0.0206,0.0188,0.0268,0.0783,-0.0472,0.0217,-0.0426,-0.0937,-0.0475,0.1226,0.0103,-0.0902,-0.0496,-0.0452,0.0044,-0.0099,0.018,0.0146,-0.0018,0.0046,0.0518,0.0483,-0.1021,-0.0364,-0.0341,0.0237,-0.0077,-0.0529,-0.0344,0.0196,0.0591,-0.0101,0.0455,-0.0552,0.04,0.0605,-0.0653,0.0295,-0.0344,-0.0468,-0.0055,-0.029,0.0027,-0.0396,-0.0201,-0.064,0.0072,-0.0202,-0.0154,-0.0055,-0.0399,0.0326,-0.0442,-0.003,0.0176,0.0341,-0.0337,0.0123,0.0353,-0.0251,-0.0191,-0.0432,0.0049,0.0647,-0.0161,0.0514,0.0233,-0.05,-0.0437,-0.2236,-0.0376,0.0126,-0.0269,0.0289,-0.1102,0.0378,-0.0118,0.069,0.0275,0.0571,-0.0047,0.0109,-0.0281,0.0383,0.0577,0.0097,0.0396,-0.0343,0.021,-0.0362,0.0446,0.0251,-0.0907,0.0276,0.033,0.1962,0.0477,0.0441,-0.0482,0.0496,0.0113,-0.0359,-0.0747,0.0545,-0.013,0.0418,-0.0051,-0.0322,-0.0234,-0.0495,0.0305,0.0101,-0.0833,-0.0014,-0.0026,-0.0491,0.0407,-0.0589,0.0326,0.0174,-0.0119,0.0527,0.0074,0.0408,-0.0212,-0.1199,0.0311,-0.0499,0.0618,-0.0006,-0.0421,0.007,-0.083,0.082,0.0338,-0.0194,-0.0825,0.0181,-0.0104,0.0216,0.104,0.0691,-0.0372,0.0753,0.0133,0.0303,-0.0511,-0.0374,-0.0127,0.0456,0.0175,0.0124,0.018,0.0432,0.0079,0.0706,0.0169,0.0685,-0.0554,0.0412,0.0319,-0.0655,-0.0,0.0189,0.0284,-0.2851,0.0579,0.0378,0.074,-0.0253,0.0094,0.0056,0.0072,-0.0402,-0.0189,-0.0297,0.0076,0.0355,-0.0518,0.0004,0.0143,0.0192,-0.0209,0.0888,-0.0615,0.0061,0.044,0.2127,-0.0622,-0.0338,0.0284,0.0085,0.0129,0.0188,-0.0333,-0.0015,0.0265,0.0862,-0.0228,0.0147,0.1073,-0.043,0.0012,0.0004,-0.0152,-0.0189,0.0086,-0.0742,-0.0008,0.0699,-0.0072,0.01,-0.0011,0.0082,0.039,-0.0382,-0.0191,-0.013,0.0028,0.0372,0.0292,-0.0733,-0.0334,-0.0384,0.0182,0.0358,-0.0662,-0.0183,-0.0067,-0.0236]}
{"key":"[Deep learning with convolutional neural networks for EEG decoding and visualization] PLEASE READ AND CITE THE REVISED VERSION at Human Brain Mapping: http://onlinelibrary.wiley.com/doi/10.1002/hbm.23730/full Code available here: https://github.com/robintibor/braindecode","layer":3,"vector":[-0.0136,-0.0287,-0.0216,-0.0102,0.0434,0.0537,0.0511,0.0054,0.0511,-0.0047,0.0326,-0.063,0.0231,0.049,0.041,0.0166,0.0346,0.0229,-0.0182,0.0028,0.0216,-0.0195,0.0162,0.0255,-0.0166,0.0002,-0.0239,-0.0364,-0.057,-0.2082,0.0031,-0.0173,0.0797,-0.073,-0.0434,-0.0314,-0.0137,0.0756,-0.0356,0.0082,0.0524,-0.0051,-0.0426,-0.0597,-0.0129,-0.0623,-0.0502,-0.0147,0.0386,-0.0423,0.024,-0.0198,0.0072,0.0428,0.0151,0.048,0.0467,0.0515,0.0522,0.0505,0.0449,0.0532,-0.1869,0.028,0.0093,0.0348,-0.043,-0.0053,-0.0126,0.0147,-0.0342,0.0275,0.0158,-0.0108,0.0032,-0.0178,-0.0156,-0.0313,-0.0124,-0.0266,0.0098,-0.0036,-0.0592,-0.0357,0.0086,-0.0095,0.0093,-0.0379,0.017,0.0028,-0.0381,-0.0194,-0.1059,0.0188,-0.0157,0.0073,0.0373,0.0136,-0.0742,0.1906,-0.0541,0.0522,0.0513,0.0078,0.0656,-0.0476,-0.0191,-0.0428,-0.0299,0.0667,-0.0014,-0.0059,0.042,-0.0208,0.0287,0.0141,0.062,0.027,0.0272,-0.025,-0.0365,0.0212,0.0566,-0.033,0.0434,-0.072,0.0576,0.1451,0.0547,-0.0159,0.069,0.0177,-0.0396,0.0142,-0.0185,0.0207,0.018,-0.0129,-0.0455,-0.0148,-0.0233,-0.0361,0.0148,-0.0862,-0.0521,0.0845,-0.0423,-0.0199,-0.0868,0.0053,-0.0393,0.0115,-0.0317,-0.0253,0.0155,0.0384,0.0527,0.0772,-0.0562,0.0414,-0.0475,-0.088,-0.0117,0.1453,0.0203,-0.0958,-0.0312,-0.0356,-0.0016,-0.038,0.0561,0.01,0.0143,0.0455,0.0987,0.0874,-0.0565,-0.0222,-0.0244,0.0397,-0.0125,-0.0406,-0.0343,0.0334,0.0481,-0.0579,0.0731,-0.0545,0.009,0.0419,-0.0424,0.0436,-0.0266,0.0298,-0.0258,-0.0682,-0.034,-0.0462,-0.0082,-0.0361,-0.0059,-0.0117,-0.0388,0.0251,0.0287,0.0204,-0.0289,0.0309,0.0624,0.058,-0.01,0.0379,0.1081,-0.0528,0.0003,-0.0124,-0.0392,0.0038,-0.0131,0.0371,0.0021,-0.0628,-0.065,-0.2075,-0.0276,0.0067,-0.0598,0.0114,-0.0588,0.0259,0.0036,0.0664,0.0562,0.0738,0.0052,-0.0142,0.0041,0.0146,0.051,0.0111,0.0401,-0.0416,-0.0117,0.0003,0.0171,0.004,-0.069,0.0147,0.0185,0.2317,0.0177,0.1085,-0.0177,0.0035,0.0111,0.0278,-0.1104,0.0483,0.0333,0.0447,0.0286,-0.0084,-0.0577,-0.0569,0.0124,0.0056,-0.0662,-0.0302,-0.0099,-0.0435,-0.0096,-0.0575,0.0097,0.0337,-0.0611,0.05,-0.0186,0.0043,-0.0193,-0.0801,0.0197,-0.0553,0.007,-0.0101,-0.0398,-0.0094,-0.0424,0.0562,0.0242,-0.0114,-0.0375,0.0543,-0.0057,-0.006,0.1082,0.0036,0.0061,0.0874,0.0191,0.0458,-0.0319,0.0032,0.0012,0.0666,0.0033,-0.0031,0.0344,0.0176,0.0275,0.0532,-0.0427,0.0389,-0.0496,-0.0445,0.0305,-0.032,-0.0833,0.031,0.006,-0.2901,0.0749,0.0306,0.0519,0.0109,0.0175,0.0301,0.0216,-0.0384,-0.0041,-0.0617,0.0099,0.0262,-0.0318,0.0006,0.0331,0.045,-0.0669,0.0462,-0.0407,-0.0225,0.0389,0.1888,-0.0283,0.0272,0.0167,-0.0172,0.0148,0.0634,-0.0336,-0.0607,0.0199,0.0709,-0.0372,0.0482,0.0639,-0.0702,0.0061,0.0297,-0.0169,0.0609,0.0176,-0.0204,-0.0345,0.0698,-0.0034,-0.0495,-0.0207,0.0032,-0.0262,-0.0441,-0.0092,-0.0302,-0.0328,0.0384,0.0148,-0.0325,-0.0119,0.0022,-0.044,0.0663,-0.0859,-0.0055,-0.0268,-0.0287]}
{"key":"[On the Optimality of Perturbations in Stochastic and Adversarial Multi-armed Bandit Problems] We investigate the optimality of perturbation based algorithms in the stochastic and adversarial multi-armed bandit problems. For the stochastic case, we provide a unified regret analysis for both sub-Weibull and bounded perturbations when rewards are sub-Gaussian. Our bounds are instance optimal for sub-Weibull perturbations with parameter 2 that also have a matching lower tail bound, and all bounded support perturbations where there is sufficient probability mass at the extremes of the support. For the adversarial setting, we prove rigorous barriers against two natural solution approaches using tools from discrete choice theory and extreme value theory. Our results suggest that the optimal perturbation, if it exists, will be of Frechet-type.","layer":4,"vector":[-0.0512,-0.0409,0.0112,-0.0473,0.0268,0.0146,0.0657,0.0142,0.0513,0.0015,0.0478,-0.0023,0.0079,0.0942,0.0177,0.0479,-0.0271,0.0169,-0.0585,0.0195,0.0396,-0.0574,-0.0078,-0.0768,0.0295,-0.02,-0.0404,-0.0504,-0.0247,-0.2295,0.0304,-0.0138,0.0154,-0.067,-0.0322,-0.0307,-0.0154,0.0412,-0.0236,0.0868,0.015,0.0421,-0.0709,-0.0722,-0.0187,-0.0355,-0.0063,0.0078,-0.0294,-0.0314,0.0061,-0.033,0.0477,0.0091,0.0379,0.0187,0.0288,0.0702,0.0231,0.062,-0.0285,0.0546,-0.1655,0.0508,0.0327,0.0125,-0.0386,-0.0227,-0.0006,0.0662,0.0096,0.04,0.0047,0.0544,0.0422,-0.0045,-0.0037,-0.052,-0.0261,0.0421,0.015,-0.011,-0.0461,0.0107,-0.0051,-0.0961,0.0441,-0.0239,0.0749,0.0227,-0.0094,-0.012,-0.0128,0.0221,-0.0453,-0.0122,0.0431,0.0092,-0.0537,0.2,-0.0013,0.0458,0.0055,-0.0429,0.0505,-0.0241,-0.0791,-0.0355,-0.0244,-0.0034,-0.0066,0.0108,0.06,-0.035,0.0056,0.0254,-0.0107,0.042,-0.027,-0.017,-0.0479,0.0295,0.0825,0.0264,0.0021,-0.0774,-0.0052,0.1627,0.0037,0.0213,0.0371,-0.0595,-0.0371,-0.0325,0.0078,0.0126,-0.0294,0.014,0.0502,-0.0138,-0.0441,-0.0592,0.0251,-0.0985,-0.0183,0.102,-0.0118,0.0265,-0.0302,-0.0515,-0.0078,-0.0095,-0.0088,-0.0183,0.0239,0.0601,0.0128,0.0435,-0.0372,-0.0076,-0.0272,-0.0694,0.0128,0.0953,0.0011,-0.0527,-0.0201,-0.0371,-0.0563,0.003,-0.0076,0.0241,-0.0289,0.0256,0.0657,0.0421,-0.1297,-0.018,-0.0273,0.0238,0.0091,-0.0111,-0.0201,0.0407,0.0257,-0.0227,-0.0029,-0.0421,0.0512,0.0599,-0.0181,-0.0157,-0.0332,-0.031,-0.0606,-0.0583,-0.0227,0.0016,0.0109,0.001,0.0266,-0.0071,-0.0746,-0.032,0.0284,-0.013,0.0372,-0.0466,0.0185,0.0054,-0.0164,0.0272,0.0323,-0.0135,-0.0506,-0.0062,0.0461,0.0829,-0.0193,0.0439,0.0488,-0.0152,0.0025,-0.241,-0.0211,-0.0537,-0.0003,0.05,-0.0642,0.0471,-0.0554,0.0028,0.0747,0.0548,-0.0363,-0.041,0.0594,0.027,0.0669,-0.008,0.0101,0.0065,-0.0128,-0.0197,0.0354,-0.0321,-0.0759,0.0357,-0.0032,0.2234,0.0368,0.0251,-0.023,0.0534,0.0475,0.0077,-0.0654,0.0314,0.0626,0.0643,-0.0354,0.0103,-0.0309,-0.025,0.053,0.0283,-0.0986,-0.074,-0.0526,-0.0294,0.0122,-0.073,0.0299,0.0235,-0.0122,0.0745,-0.0262,0.0196,-0.0273,-0.1182,0.0307,-0.0047,0.0555,0.0193,-0.0906,0.0092,-0.0711,0.0639,-0.0171,-0.0087,-0.0321,0.0563,0.0007,-0.0174,0.0265,0.0027,0.0091,0.0131,0.0215,0.0466,-0.0327,-0.0505,-0.0106,0.0513,-0.0267,0.0156,0.0219,0.0011,0.0003,0.0783,-0.0331,0.0156,0.0116,0.007,0.0127,-0.0639,0.0087,0.046,0.0161,-0.2856,0.0225,0.051,0.0212,-0.0315,0.0339,0.0052,0.0025,-0.0362,-0.0149,0.0226,0.0861,0.0367,-0.0153,0.0149,-0.0127,0.0535,-0.0096,0.0372,-0.0482,0.0106,0.041,0.2251,-0.0346,0.0133,0.0267,-0.0201,0.0503,0.0094,-0.0732,0.0209,-0.0272,0.0785,-0.0621,0.0529,0.0863,-0.0671,0.0365,0.0364,-0.0032,-0.0315,0.0135,-0.0417,0.0165,0.1099,0.004,-0.0271,-0.0249,0.0115,0.0253,-0.0216,0.0181,-0.0134,-0.0414,0.0411,0.0036,-0.02,-0.0739,-0.0081,-0.003,0.0025,0.0184,-0.0203,-0.0188,-0.0057]}
{"key":"[Product-based Neural Networks for User Response Prediction over Multi-field Categorical Data] User response prediction is a crucial component for personalized information retrieval and filtering scenarios, such as recommender system and web search. The data in user response prediction is mostly in a multi-field categorical format and transformed into sparse representations via one-hot encoding. Due to the sparsity problems in representation and optimization, most research focuses on feature engineering and shallow modeling. Recently, deep neural networks have attracted research attention on such a problem for their high capacity and end-to-end training scheme. In this paper, we study user response prediction in the scenario of click prediction. We first analyze a coupled gradient issue in latent vector-based models and propose kernel product to learn field-aware feature interactions. Then we discuss an insensitive gradient issue in DNN-based models and propose Product-based Neural Network (PNN) which adopts a feature extractor to explore feature interactions. Generalizing the kernel product to a net-in-net architecture, we further propose Product-network In Network (PIN) which can generalize previous models. Extensive experiments on 4 industrial datasets and 1 contest dataset demonstrate that our models consistently outperform 8 baselines on both AUC and log loss. Besides, PIN makes great CTR improvement (relatively 34.67%) in online A/B test.","layer":0,"vector":[-0.065,0.0304,-0.005,-0.013,0.0169,0.0145,0.0502,0.0301,0.0089,-0.0404,-0.0188,-0.0387,0.0444,0.0521,0.015,-0.0181,0.0545,0.0276,-0.0297,0.0214,0.0227,-0.0207,-0.0391,-0.0756,-0.0312,-0.0318,-0.0444,-0.0138,-0.0533,-0.2327,0.0252,-0.0185,0.0412,-0.0136,0.0279,-0.0495,0.0175,0.0191,-0.0372,0.0602,0.0141,0.0069,-0.0198,-0.025,0.02,0.0017,0.0058,-0.0435,-0.0232,-0.0228,0.0318,-0.0242,0.0054,0.0233,0.0376,0.0293,0.0354,0.0625,0.0192,0.0277,0.0259,-0.0005,-0.1886,0.0739,0.0159,0.059,-0.059,-0.0079,0.0125,0.0648,0.0036,0.0545,0.0024,0.0711,0.0071,0.0112,-0.0028,0.0139,-0.0208,0.016,0.0106,0.0071,-0.0725,-0.042,0.0223,-0.0288,0.0377,-0.0033,0.02,-0.0173,-0.0599,-0.011,-0.0319,-0.0019,-0.0376,0.0183,0.0209,0.0066,-0.0794,0.2376,-0.0413,0.0485,0.0113,-0.0732,0.0133,-0.0119,-0.0365,-0.014,-0.0107,0.0113,0.0131,-0.0145,0.0072,-0.0465,0.0742,0.0029,0.0715,0.0357,0.0241,-0.0509,-0.0035,0.0252,0.0273,-0.0466,0.0184,-0.0571,-0.0014,0.134,0.0231,0.0389,0.041,-0.0318,-0.0972,-0.0021,0.0126,0.0151,-0.01,-0.0091,-0.0173,0.0174,-0.0192,-0.0442,0.0358,-0.0481,-0.0341,0.1125,-0.019,-0.035,-0.0555,-0.0156,-0.0299,0.0507,-0.0218,-0.0388,-0.0027,-0.0005,0.0155,0.0045,-0.0679,0.0045,-0.0019,-0.0322,-0.0534,0.0633,0.0151,-0.0865,-0.0278,0.0185,-0.0158,-0.0216,0.0546,-0.0189,-0.049,0.0017,0.0724,0.0674,-0.0566,0.0169,0.0173,0.01,0.0464,-0.0169,-0.0007,0.0279,0.0525,-0.0318,0.0076,-0.0052,0.0056,0.0156,-0.0078,0.0101,-0.0192,0.0287,-0.0217,-0.0165,-0.0305,-0.0237,0.0364,-0.0744,0.0068,-0.026,-0.0711,0.0633,0.0093,0.0547,-0.0176,-0.0239,0.0361,-0.0124,-0.0529,-0.0197,0.0692,-0.0954,-0.0104,-0.0372,0.0136,0.049,-0.0004,0.079,0.034,-0.0129,-0.0515,-0.21,-0.0237,0.0357,-0.0294,0.0255,-0.0986,0.0482,0.0028,0.0881,0.088,0.0643,0.0036,-0.0377,0.0482,-0.0029,0.0381,0.0215,0.032,-0.0182,0.0039,-0.0013,-0.0204,0.0256,-0.0719,0.0277,-0.0267,0.2077,0.0587,-0.0306,-0.0488,0.0563,0.1029,-0.0358,-0.0956,0.0847,0.0278,0.0582,-0.0151,-0.0594,-0.0181,-0.0286,0.0317,0.0286,-0.0941,-0.0342,-0.0285,-0.0271,0.0105,-0.0786,0.0513,0.0628,-0.0413,0.0768,0.0173,0.0128,-0.0601,-0.066,0.0366,-0.0128,0.0309,0.0265,-0.0838,0.0146,-0.0041,0.0033,0.0038,-0.0197,-0.0187,0.021,-0.0288,-0.0779,0.0765,-0.0336,0.0466,0.0482,-0.0058,0.0237,-0.0029,-0.0507,-0.0235,0.0795,0.0016,0.0569,0.0151,0.0331,-0.0132,0.0505,-0.0143,0.0285,-0.0313,-0.0059,-0.0017,-0.0198,-0.0449,0.0559,-0.0086,-0.3165,0.0519,0.0278,0.0732,-0.0289,0.0548,0.0538,0.0038,-0.0347,0.0063,0.0138,0.0175,0.0636,-0.0321,0.007,0.0121,0.0713,-0.0444,0.0497,-0.0361,0.0322,0.0693,0.2165,0.0025,0.0506,-0.0311,-0.0069,-0.0595,0.0372,-0.0098,0.04,0.0108,0.0788,-0.063,0.0059,0.0613,-0.0249,0.0025,0.0264,-0.0068,0.0031,-0.0162,-0.0813,-0.0412,0.0663,-0.03,-0.0437,-0.0621,-0.0237,0.01,-0.03,-0.0069,-0.0105,0.0096,0.0154,0.0477,-0.0469,-0.0345,-0.0816,-0.0327,0.0271,-0.022,-0.0368,-0.0188,0.0139]}
{"key":"[Learning Light Transport the Reinforced Way] We show that the equations of reinforcement learning and light transport simulation are related integral equations. Based on this correspondence, a scheme to learn importance while sampling path space is derived. The new approach is demonstrated in a consistent light transport simulation algorithm that uses reinforcement learning to progressively learn where light comes from. As using this information for importance sampling includes information about visibility, too, the number of light transport paths with zero contribution is dramatically reduced, resulting in much less noisy images within a fixed time budget.","layer":2,"vector":[-0.037,-0.0166,0.0204,-0.0055,0.035,0.0545,0.0439,0.0182,0.0205,0.0134,0.0305,-0.0392,0.0118,0.082,0.0055,0.0042,-0.0142,0.0352,-0.0539,-0.0284,0.0798,-0.0676,0.0019,-0.1035,0.0127,0.0076,-0.0338,-0.0521,-0.0299,-0.2389,-0.0146,-0.0471,0.0166,-0.026,-0.032,-0.0155,-0.0538,0.0297,-0.0211,0.0361,0.0616,-0.0214,-0.0134,-0.0634,-0.0412,-0.0752,0.0078,-0.0526,0.0292,-0.054,0.037,-0.061,0.0155,-0.0086,0.0308,0.0272,0.0768,0.0799,0.0563,0.0388,-0.0026,0.0669,-0.1801,0.0561,0.0386,-0.0008,-0.0286,-0.0391,0.0479,0.031,-0.0611,0.0319,-0.0224,0.0452,0.0293,-0.0504,0.0035,-0.0639,-0.0247,-0.0237,0.0174,-0.0211,-0.0402,0.0106,0.0107,-0.0132,0.0064,-0.0731,0.0254,0.0178,-0.0046,-0.029,-0.0786,0.0187,-0.0507,-0.0152,-0.0085,0.0038,-0.0144,0.1886,0.0082,0.0397,0.0462,0.0155,0.0634,-0.0473,-0.031,0.0102,-0.0635,0.0232,-0.0233,-0.0107,0.0458,-0.0253,0.0136,0.0377,0.0177,0.037,-0.0233,0.0003,-0.0074,0.0288,0.0802,0.0033,0.0413,-0.0984,0.019,0.1226,0.025,0.0355,0.0551,-0.0363,0.0103,-0.029,0.0085,0.0414,-0.0012,-0.0152,-0.0026,0.0045,-0.0285,0.0052,0.0288,-0.1153,-0.0446,0.1051,0.0288,0.0637,-0.0239,-0.0208,0.0109,-0.0107,0.0324,-0.0144,-0.004,0.0161,0.0031,0.0685,-0.0459,0.0511,-0.0245,-0.0542,-0.0102,0.0838,-0.0096,-0.053,-0.0161,-0.0123,-0.0031,-0.0129,0.0128,0.0542,-0.0219,0.0217,0.0955,-0.0275,-0.1242,0.0089,0.0055,-0.0315,0.0247,-0.0976,-0.0196,0.0321,0.0666,0.0001,0.0019,-0.0121,-0.0025,0.0558,0.0136,0.0189,-0.0062,0.0111,-0.0453,-0.0243,-0.0365,-0.0509,-0.0282,-0.0341,0.0096,-0.0399,-0.0335,-0.0289,-0.0209,-0.0246,0.0158,-0.0043,0.0395,0.0279,-0.062,-0.0145,0.0664,-0.0073,-0.0446,0.0211,0.0089,0.0018,0.0132,0.0401,0.019,-0.0544,-0.0701,-0.2471,0.0213,0.0016,0.0064,0.0537,-0.0533,0.0404,-0.0045,0.0202,0.0471,0.0797,-0.0586,0.017,0.0013,-0.008,0.0263,0.0046,0.0175,-0.0315,-0.0222,-0.0295,0.0316,-0.0128,-0.0942,0.0385,0.0076,0.2196,0.0412,0.0358,-0.0139,-0.0001,-0.0069,-0.0322,-0.0858,0.0326,0.0349,0.0801,-0.0058,0.008,-0.0503,0.0025,0.0331,-0.0943,-0.0559,-0.0318,-0.0235,-0.0306,0.0765,-0.053,-0.016,0.0602,-0.0224,0.0475,-0.0011,-0.0248,-0.0434,-0.0637,0.034,-0.0326,0.0475,-0.0142,-0.0288,0.0331,-0.0521,0.0413,0.0053,-0.0033,-0.0694,0.0671,0.0024,-0.0047,0.0603,-0.0088,0.0051,0.0378,0.0247,0.0377,0.0193,-0.0164,-0.046,0.0584,-0.0275,0.0187,0.0768,0.043,-0.0042,0.0414,-0.0533,-0.0092,0.0026,-0.016,0.0289,-0.0611,-0.0503,0.0304,-0.0134,-0.3237,0.0383,0.0341,0.0392,-0.0496,0.0301,0.0684,0.0218,-0.0208,-0.0423,-0.0119,0.0388,0.0482,0.0288,0.0181,0.0149,0.0855,-0.0359,0.0309,-0.0688,0.0075,0.024,0.2316,-0.038,0.0292,0.0121,-0.0334,0.0095,0.0107,0.011,0.014,0.0564,0.0531,-0.0664,0.0527,0.0651,-0.013,0.0127,-0.0057,0.0109,0.0051,0.0022,-0.0024,0.0057,0.0859,0.0105,0.0015,-0.0067,-0.0357,0.034,-0.0226,0.0236,0.0256,-0.0215,0.0394,0.0377,-0.0677,-0.0309,-0.0072,-0.0175,0.001,-0.0643,0.03,-0.0235,0.0384]}
{"key":"[Large-Scale Strategic Games and Adversarial Machine Learning] Decision making in modern large-scale and complex systems such as communication networks, smart electricity grids, and cyber-physical systems motivate novel game-theoretic approaches. This paper investigates big strategic (non-cooperative) games where a finite number of individual players each have a large number of continuous decision variables and input data points. Such high-dimensional decision spaces and big data sets lead to computational challenges, relating to efforts in non-linear optimization scaling up to large systems of variables. In addition to these computational challenges, real-world players often have limited information about their preference parameters due to the prohibitive cost of identifying them or due to operating in dynamic online settings. The challenge of limited information is exacerbated in high dimensions and big data sets. Motivated by both computational and information limitations that constrain the direct solution of big strategic games, our investigation centers around reductions using linear transformations such as random projection methods and their effect on Nash equilibrium solutions. Specific analytical results are presented for quadratic games and approximations. In addition, an adversarial learning game is presented where random projection and sampling schemes are investigated.","layer":4,"vector":[-0.0568,-0.0215,0.0086,-0.0261,0.0283,0.025,0.0286,0.0111,0.0525,-0.0028,-0.0023,-0.0145,0.0396,0.0675,0.0122,0.06,0.0053,-0.0118,-0.0353,0.0425,0.0403,-0.0368,-0.0478,-0.052,0.0051,0.014,-0.0359,-0.0587,-0.0422,-0.2267,0.0359,-0.0239,0.0325,-0.0359,-0.024,-0.0357,-0.0175,0.059,-0.0037,0.0427,0.0316,0.0234,-0.0016,-0.0771,-0.0309,-0.0503,-0.0358,0.0145,-0.0275,-0.0273,0.0162,-0.0359,0.0033,0.0155,0.0565,0.0176,0.0387,0.0617,0.0882,0.045,0.0165,0.0235,-0.1495,0.0658,0.0421,0.0542,-0.0143,0.001,-0.0101,0.0775,0.0222,0.0482,0.0116,0.024,0.0124,0.0182,-0.0241,-0.0289,-0.01,0.0392,0.0368,-0.0438,-0.0381,0.0268,-0.0068,-0.0427,0.0066,-0.0099,0.0811,-0.0345,-0.0179,0.0327,-0.0342,-0.0019,-0.0501,-0.0113,0.0425,0.019,-0.0806,0.1972,-0.0169,0.006,-0.0048,-0.0177,0.0378,-0.0174,-0.062,-0.0458,0.0096,0.0248,-0.0424,-0.0288,0.0266,0.0054,-0.003,0.0177,0.0412,0.0364,-0.031,-0.0082,-0.0329,0.0189,0.0381,0.0421,0.0544,-0.0417,-0.013,0.1672,0.0125,0.0367,-0.0182,-0.0114,-0.0607,-0.0176,0.0141,0.0185,0.0287,0.0259,0.0074,0.0337,-0.0221,-0.0359,0.0408,-0.1177,-0.0394,0.0959,0.0193,0.049,-0.034,-0.0104,0.0196,0.021,-0.0215,-0.0271,-0.0102,0.0175,0.012,0.0324,-0.0737,-0.0122,-0.0442,-0.0123,0.0209,0.1291,-0.0216,-0.0822,-0.0289,-0.0222,-0.0055,-0.0294,-0.0253,0.0345,-0.0491,0.0165,0.0743,0.0271,-0.1008,-0.0397,-0.0247,0.0101,-0.0075,-0.0057,-0.0131,0.0023,0.0141,-0.051,0.0137,-0.0186,-0.0007,0.0339,-0.0803,0.0216,-0.0437,-0.0099,-0.0529,-0.0477,0.0049,-0.0313,0.0449,0.0092,-0.015,-0.0098,-0.0962,0.0488,-0.0005,0.0256,-0.0125,-0.0461,0.0147,0.0154,0.0012,0.0089,0.0331,-0.0021,-0.0633,0.0291,0.0096,0.0446,-0.0216,0.0349,0.0422,0.0011,-0.0597,-0.2151,-0.0276,-0.049,-0.0489,0.0342,-0.0559,0.0295,-0.0652,0.0387,0.0823,0.0725,-0.0254,-0.0466,0.0277,-0.04,0.0217,-0.0125,0.0208,-0.0276,0.0205,-0.0111,0.0215,0.0062,-0.0904,0.0206,0.007,0.2627,0.0007,0.0077,0.0093,0.0445,0.0457,-0.022,-0.0734,0.0699,0.0419,0.0634,-0.0005,-0.0448,-0.0124,-0.0179,0.0209,-0.0036,-0.0995,-0.005,-0.028,-0.0277,0.0515,-0.0575,-0.0038,0.0603,-0.0027,0.0619,-0.0452,0.0199,-0.0245,-0.0854,0.0687,0.011,0.0171,-0.019,-0.0415,-0.0661,-0.02,0.0847,-0.0018,-0.0198,-0.0356,0.048,-0.0176,-0.0036,0.0139,0.0114,0.0232,0.0483,0.0411,0.051,-0.0145,-0.0161,-0.0006,0.0573,-0.0472,0.0601,0.0541,0.0646,-0.0687,0.0745,-0.0221,0.0227,0.0312,-0.0166,0.027,-0.0806,-0.0037,0.033,0.005,-0.3164,0.0482,0.0546,0.0447,-0.0681,-0.013,0.0289,0.0176,-0.0346,0.025,0.0182,0.047,0.026,0.0037,0.0082,-0.0115,0.0702,-0.0621,0.0648,-0.026,0.0523,-0.0204,0.2113,-0.071,0.0382,0.0393,-0.0296,0.0423,-0.0267,-0.0289,0.0147,0.0088,0.0955,-0.077,0.0419,0.0672,-0.008,0.0093,0.0215,-0.0054,-0.0526,0.0022,-0.0194,0.0066,0.0834,0.0484,-0.028,-0.0521,-0.0059,-0.002,-0.0628,0.0267,-0.0306,-0.0039,0.0374,0.0293,-0.0438,-0.0499,-0.0128,-0.0492,-0.0271,-0.0445,-0.0257,-0.075,-0.0087]}
{"key":"[FoodChem: A food-chemical relation extraction model] In this paper, we present FoodChem, a new Relation Extraction (RE) model for identifying chemicals present in the composition of food entities, based on textual information provided in biomedical peer-reviewed scientific literature. The RE task is treated as a binary classification problem, aimed at identifying whether the contains relation exists between a food-chemical entity pair. This is accomplished by fine-tuning BERT, BioBERT and RoBERTa transformer models. For evaluation purposes, a novel dataset with annotated contains relations in food-chemical entity pairs is generated, in a golden and silver version. The models are integrated into a voting scheme in order to produce the silver version of the dataset which we use for augmenting the individual models, while the manually annotated golden version is used for their evaluation. Out of the three evaluated models, the BioBERT model achieves the best results, with a macro averaged F1 score of 0.902 in the unbalanced augmentation setting.","layer":0,"vector":[-0.0592,-0.0211,-0.0048,-0.0265,0.0368,-0.0052,0.0332,0.0321,0.0007,-0.0204,-0.012,-0.0876,0.0577,0.0392,0.0774,0.004,0.031,0.0366,-0.0636,-0.0148,0.0547,-0.0324,-0.0199,-0.0324,0.0542,0.0707,-0.0168,-0.0111,-0.0464,-0.2164,-0.0263,-0.0364,0.0571,-0.0226,-0.0037,0.025,-0.012,0.0123,-0.0165,0.003,0.0565,-0.0209,0.0196,-0.0497,-0.0076,-0.0374,-0.0324,-0.0398,-0.0157,-0.0531,0.0209,-0.0359,0.0114,0.0205,0.027,0.0335,0.0303,-0.0194,0.0242,0.0588,0.0527,0.0639,-0.1355,0.0733,0.0623,0.0053,-0.1029,0.0038,-0.0052,0.0464,-0.0148,0.0343,0.0119,0.0343,0.0043,0.0137,0.0205,-0.0415,0.0076,0.0374,-0.0061,0.0224,-0.0323,-0.0366,-0.005,-0.0678,0.0046,-0.0421,0.0394,0.0472,-0.0679,-0.0672,0.0199,0.0161,-0.058,-0.0279,0.0127,-0.0187,-0.0475,0.1739,-0.0892,0.0063,0.029,-0.0684,0.0119,-0.0483,-0.0378,-0.019,-0.0216,-0.019,-0.0097,0.0252,0.0098,-0.0307,0.0517,0.0384,0.1262,0.0029,-0.0246,-0.0246,-0.0114,0.0364,0.0533,0.0392,0.0269,-0.0371,0.0343,0.1267,0.0462,0.0326,0.0647,0.0256,-0.0295,-0.0101,-0.0107,0.0374,0.0045,-0.006,0.035,-0.0038,-0.0162,-0.0794,-0.0014,-0.0956,-0.0673,0.1588,0.014,0.0102,-0.0878,-0.0146,0.0113,0.0281,-0.0059,-0.0137,0.047,0.0453,0.0171,0.0255,-0.0369,0.0105,0.0218,-0.061,-0.0121,0.0995,0.0379,-0.0819,-0.0352,0.0082,0.019,-0.0044,0.109,0.0468,-0.0826,0.0385,0.0414,0.0017,0.0175,-0.0032,-0.0075,-0.0184,0.0679,-0.0259,-0.0306,0.0887,0.0227,-0.08,-0.0116,-0.0488,0.0164,0.0405,-0.0292,0.0061,0.0043,0.0113,0.0058,-0.0251,-0.0085,-0.0121,-0.0054,-0.0755,0.0152,-0.0132,-0.0762,0.0144,-0.0089,-0.014,-0.0244,-0.0037,0.0321,0.0449,-0.0271,-0.0351,0.0051,-0.032,-0.0418,-0.0252,0.0341,0.0353,-0.0041,0.0187,0.0526,-0.0323,-0.0365,-0.2209,-0.0357,0.0238,0.0078,0.0881,-0.0314,0.0116,-0.0023,0.0006,0.1105,0.0364,0.0005,-0.056,0.0399,-0.0159,0.0443,0.0901,0.0,0.0358,0.005,-0.0151,-0.0182,0.0191,-0.0561,0.0655,-0.0229,0.2191,0.0604,-0.0105,-0.0154,0.0342,0.0603,-0.0433,-0.1292,0.0361,-0.0171,0.0348,-0.0194,-0.066,-0.0556,-0.073,-0.0015,-0.0061,-0.0831,-0.0261,-0.0415,-0.0002,-0.0086,-0.0488,0.0527,0.0407,0.0411,0.0368,0.0191,0.0189,-0.0027,-0.0661,0.0039,-0.0511,-0.005,-0.0046,-0.0338,0.009,-0.0764,0.0646,0.0095,-0.0286,0.0001,0.0317,0.0016,-0.026,0.1056,0.0086,0.0179,0.0305,0.05,0.0336,-0.0563,-0.0719,-0.0335,0.0278,-0.028,0.0433,0.0121,0.0208,-0.0153,0.0812,-0.0292,0.0599,0.0113,0.0069,0.0312,-0.073,-0.005,0.0387,-0.0115,-0.2822,0.0452,0.0223,0.0528,-0.0103,-0.0211,0.0342,0.004,-0.0535,-0.0293,0.0334,-0.0214,0.0252,-0.0372,-0.0344,0.0141,0.0732,-0.0817,0.0197,-0.035,0.0533,0.013,0.2279,-0.0075,0.0032,0.0285,-0.0334,-0.0059,-0.0169,0.0298,0.0205,0.01,0.1121,-0.0655,0.0713,0.086,-0.044,0.0142,0.0177,-0.0277,-0.0198,0.0248,-0.0728,-0.0744,0.0667,-0.0228,-0.0105,-0.0579,-0.0146,0.0283,-0.0352,-0.0024,-0.006,0.0066,0.0351,-0.0066,-0.0393,-0.0326,-0.0018,-0.0246,-0.0229,-0.0558,-0.0169,0.0447,-0.0271]}
{"key":"[On Image Segmentation With Noisy Labels: Characterization and Volume Properties of the Optimal Solutions to Accuracy and Dice] We study two of the most popular performance metrics in medical image segmentation, Accuracy and Dice, when the target labels are noisy. For both metrics, several statements related to characterization and volume properties of the set of optimal segmentations are proved, and associated experiments are provided. Our main insights are: (i) the volume of the solutions to both metrics may deviate significantly from the expected volume of the target, (ii) the volume of a solution to Accuracy is always less than or equal to the volume of a solution to Dice and (iii) the optimal solutions to both of these metrics coincide when the set of feasible segmentations is constrained to the set of segmentations with the volume equal to the expected volume of the target.","layer":0,"vector":[-0.039,-0.0627,0.0493,-0.003,0.0091,0.0001,0.0795,0.0313,0.0563,-0.0153,-0.0043,-0.0805,0.0115,0.0726,-0.0145,0.0454,0.0277,0.0312,-0.0157,0.061,0.0473,-0.0564,0.0258,-0.0681,0.0191,0.0336,-0.0305,-0.0473,-0.0512,-0.2589,0.0168,-0.0167,0.0446,-0.02,0.0147,-0.0342,-0.0241,0.0659,-0.0518,-0.0104,0.0457,0.052,-0.0086,-0.0497,-0.0263,-0.0718,-0.0263,-0.0166,-0.0131,0.0166,0.0356,-0.0249,0.0182,0.0242,0.0027,0.0264,0.0674,0.0376,0.0545,0.0856,-0.0006,0.0549,-0.1666,0.0621,0.0239,-0.0339,-0.0474,-0.0603,-0.0158,0.0723,0.0041,0.0315,0.0086,0.0506,0.0088,-0.0535,0.0236,-0.0501,-0.0036,-0.0132,-0.047,0.0046,-0.0452,-0.0281,-0.0422,-0.0645,0.0139,-0.0678,0.0465,-0.016,0.0016,0.0081,-0.0343,0.0204,-0.0346,-0.0328,0.0242,0.0056,-0.0286,0.1798,-0.0418,0.0269,0.0628,-0.0733,0.0486,-0.0209,-0.0496,-0.0359,-0.0169,-0.0062,0.0056,-0.0045,0.0776,0.0211,-0.0045,0.0313,0.0588,0.0182,0.016,-0.0113,-0.0521,-0.0279,0.0508,-0.0171,0.0192,-0.0497,0.0479,0.1239,0.031,0.0187,0.0543,-0.0019,-0.0101,0.0036,0.0116,-0.0008,0.0336,0.0046,0.0083,0.0267,-0.0332,-0.0637,0.0548,-0.1041,-0.0474,0.1574,-0.1064,0.0977,-0.055,-0.0725,0.0102,0.0071,-0.0319,-0.0051,0.0127,0.0112,0.0098,0.0099,-0.0493,0.0132,-0.0022,-0.053,0.0058,0.1267,-0.0026,-0.0352,-0.0491,-0.011,0.0147,-0.0193,0.0224,0.0365,-0.0079,0.0104,0.0148,-0.0013,-0.0809,-0.0076,0.0188,0.0118,0.0377,-0.048,-0.0293,0.0204,0.038,-0.0717,-0.0274,-0.0489,0.0684,0.0902,0.0352,0.0202,-0.0169,-0.0082,0.0012,-0.0914,-0.0077,0.0073,-0.0079,-0.0026,0.0102,-0.0353,0.0222,0.0809,0.0437,-0.0113,-0.0473,-0.0254,-0.003,0.0507,-0.0467,0.001,0.0241,-0.0203,-0.0518,0.023,0.1081,0.0205,0.0005,0.0592,0.0625,-0.0335,-0.0526,-0.2179,-0.009,0.0123,-0.0151,0.034,-0.0326,0.0194,0.0143,0.0335,0.0319,0.0426,-0.0593,0.0121,-0.03,-0.0297,0.0315,-0.0191,0.0281,-0.0255,-0.0151,0.0065,0.0536,-0.0399,-0.0331,0.055,-0.0159,0.2312,0.0196,0.0445,0.0099,0.0183,-0.0012,-0.0385,-0.0451,0.065,0.0554,0.0077,-0.0144,-0.0283,-0.0218,-0.0318,0.0053,0.0288,-0.0683,-0.0437,-0.043,-0.0446,0.0387,-0.0613,0.0029,0.016,-0.0462,0.0466,-0.007,0.0293,-0.0272,-0.0743,-0.024,-0.0539,0.0276,-0.0208,-0.0644,0.032,-0.0571,0.0582,0.0222,-0.0059,-0.057,-0.0116,-0.0208,-0.0242,0.0537,0.0121,-0.0212,0.0674,0.0217,0.0445,0.0008,-0.0457,-0.0198,0.0671,-0.0498,0.0246,-0.0016,0.0401,0.0129,0.074,-0.0156,-0.0235,-0.0313,0.0451,0.0303,-0.0875,0.0054,0.0251,-0.0049,-0.2641,0.0148,0.0154,0.0203,-0.0366,-0.0015,0.0342,0.0388,-0.0248,-0.0081,-0.0236,0.0364,0.0358,-0.0463,-0.0129,0.0274,0.0466,-0.0584,0.0943,-0.0528,0.0099,0.0136,0.2118,-0.1042,-0.0395,0.0325,0.0109,0.0007,0.0028,-0.0148,0.0243,0.0246,0.0934,-0.0472,0.0485,0.0935,-0.0368,-0.012,-0.0297,-0.0065,0.0211,-0.0154,-0.0122,0.0157,0.1193,0.0202,-0.0065,-0.019,0.0401,0.0374,-0.0652,0.0305,-0.0278,-0.0174,0.0186,0.0258,-0.0043,-0.0695,-0.0459,-0.02,-0.0048,-0.0729,0.0211,0.0393,-0.0191]}
{"key":"[Quaternion Capsule Networks] Capsules are grouping of neurons that allow to represent sophisticated information of a visual entity such as pose and features. In the view of this property, Capsule Networks outperform CNNs in challenging tasks like object recognition in unseen viewpoints, and this is achieved by learning the transformations between the object and its parts with the help of high dimensional representation of pose information. In this paper, we present Quaternion Capsules (QCN) where pose information of capsules and their transformations are represented by quaternions. Quaternions are immune to the gimbal lock, have straightforward regularization of the rotation representation for capsules, and require less number of parameters than matrices. The experimental results show that QCNs generalize better to novel viewpoints with fewer parameters, and also achieve on-par or better performances with the state-of-the-art Capsule architectures on well-known benchmarking datasets.","layer":1,"vector":[-0.0159,-0.0323,0.0072,-0.0319,-0.0004,0.0615,0.0587,0.021,-0.0334,-0.0376,0.0149,-0.0809,0.0276,0.0711,0.0287,-0.0118,0.0325,0.044,-0.0684,0.0115,-0.0169,-0.0455,-0.0335,-0.0379,0.0002,-0.0029,-0.0459,-0.0457,-0.0363,-0.2481,-0.0033,-0.0101,0.0662,-0.0136,0.0089,0.0029,-0.0333,0.0248,-0.0209,0.0161,0.0556,0.0545,-0.0178,-0.0617,-0.0344,-0.0523,-0.0072,-0.0378,0.0031,-0.0177,0.0514,-0.0378,0.0296,0.032,-0.0157,0.0495,0.0235,0.0294,0.0646,0.0506,0.029,0.026,-0.1449,0.0397,0.034,0.0301,-0.0448,-0.0387,0.0309,0.0177,-0.0072,0.0316,0.0302,0.0166,-0.0168,-0.0437,0.0005,-0.0514,-0.0013,-0.0265,0.0234,0.0018,-0.0074,0.0083,-0.0006,-0.0298,-0.0005,-0.0355,-0.0022,0.041,-0.0925,-0.0241,-0.0403,0.0532,-0.0778,-0.0314,-0.006,0.0373,-0.0379,0.2054,-0.0494,0.0204,0.0513,-0.0418,0.08,-0.0448,-0.0132,-0.0431,-0.016,0.0258,-0.0055,0.0039,0.0199,-0.0136,-0.0188,0.0128,0.0548,0.0692,-0.0029,-0.0407,-0.0138,-0.0006,0.0269,-0.0099,0.013,-0.0469,0.003,0.143,-0.0075,0.0464,0.0733,0.0166,-0.0114,-0.043,0.0106,0.0375,0.0752,-0.0335,-0.0019,-0.0267,0.0108,-0.0208,0.0102,-0.0711,-0.0674,0.1007,-0.0501,0.0133,-0.0195,0.0083,-0.0144,0.0293,-0.028,-0.0193,0.0284,-0.0134,-0.0064,0.0658,-0.0991,0.0252,-0.0641,-0.0342,-0.0559,0.1075,0.0166,-0.0824,-0.0379,0.0117,-0.0169,-0.0503,0.0395,-0.0008,0.0028,0.0341,0.1197,0.0189,-0.067,-0.007,0.0137,-0.0033,0.0315,-0.0886,-0.0241,0.0115,0.0554,-0.0276,0.0354,-0.021,-0.0219,0.0323,-0.0198,0.0404,-0.027,0.025,-0.0314,-0.0132,-0.0052,0.0181,0.0175,-0.0233,0.0203,0.0089,-0.054,0.0126,0.0001,-0.0035,-0.0111,0.0055,0.0205,0.0565,-0.0234,0.0091,0.0475,-0.0454,-0.0282,0.0055,-0.0011,0.0403,-0.0344,0.0175,0.0704,-0.0687,-0.0071,-0.2486,0.041,0.0021,-0.0311,0.0426,-0.0505,0.0136,-0.003,0.0466,-0.0023,0.0468,-0.0575,0.0118,0.0034,0.0212,0.0796,0.041,0.0555,0.0237,-0.0376,-0.0579,0.0147,-0.0139,-0.0677,0.0602,0.0059,0.2576,0.035,0.0696,-0.0131,0.0588,0.0306,-0.0155,-0.0618,0.0835,0.0393,0.0487,0.0061,-0.0569,-0.0074,-0.0527,-0.0076,0.0308,-0.1075,-0.0129,-0.0115,-0.0597,0.0553,-0.0644,0.0121,0.0433,-0.0662,0.012,0.016,-0.0114,-0.0261,-0.0746,0.0283,-0.0245,0.0965,-0.0077,-0.0391,0.0162,-0.0664,0.0703,0.0045,-0.0661,-0.0473,0.0365,-0.0599,0.0002,0.0746,0.0331,0.0325,0.0583,-0.0188,0.099,0.027,-0.0076,-0.0363,0.02,0.002,0.0178,-0.0276,0.0526,-0.007,0.0804,-0.0118,0.0384,-0.0322,0.0191,0.0443,-0.0567,0.0076,0.0731,0.0347,-0.2901,-0.0034,0.0206,0.0307,0.0003,0.0076,0.0388,-0.0412,-0.0123,-0.0074,-0.0055,0.0569,0.0572,-0.0014,0.0072,0.038,0.0431,-0.0234,0.0685,-0.04,0.0107,0.012,0.1977,-0.0235,-0.0065,-0.0312,-0.069,-0.0212,0.0026,0.0231,-0.0356,-0.0407,0.0587,-0.009,0.0051,0.0838,-0.0388,0.0224,0.0133,0.0427,-0.0367,-0.0515,-0.0256,-0.0,0.1046,-0.0121,0.0,0.0018,0.0117,0.0219,-0.0599,-0.0157,-0.0218,-0.0093,0.012,0.0136,-0.0214,-0.0779,-0.038,-0.0124,0.039,-0.0893,-0.0284,-0.0119,-0.0275]}
{"key":"[Sparse and Smooth: improved guarantees for Spectral Clustering in the Dynamic Stochastic Block Model] In this paper, we analyse classical variants of the Spectral Clustering (SC) algorithm in the Dynamic Stochastic Block Model (DSBM). Existing results show that, in the relatively sparse case where the expected degree grows logarithmically with the number of nodes, guarantees in the static case can be extended to the dynamic case and yield improved error bounds when the DSBM is sufficiently smooth in time, that is, the communities do not change too much between two time steps. We improve over these results by drawing a new link between the sparsity and the smoothness of the DSBM: the more regular the DSBM is, the more sparse it can be, while still guaranteeing consistent recovery. In particular, a mild condition on the smoothness allows to treat the sparse case with bounded degree. We also extend these guarantees to the normalized Laplacian, and as a by-product of our analysis, we obtain to our knowledge the best spectral concentration bound available for the normalized Laplacian of matrices with independent Bernoulli entries.","layer":0,"vector":[-0.0474,-0.0098,0.0127,0.0068,0.0251,0.0604,0.015,-0.008,0.0649,-0.0101,0.0387,-0.0464,0.0233,0.0616,0.0208,0.0495,-0.0119,0.0328,0.0109,-0.0284,-0.0107,-0.0266,-0.0096,-0.0599,0.0253,0.0025,-0.0492,-0.0546,-0.0592,-0.2674,0.0218,-0.0238,0.0825,-0.038,0.0383,-0.0256,0.0094,0.0771,-0.0291,0.0562,0.0128,0.0459,-0.0198,-0.0246,-0.0386,-0.0332,-0.0058,0.0232,-0.0446,-0.0531,0.0019,-0.0166,0.005,0.0459,0.0171,0.0218,0.0422,0.0366,0.0475,0.047,0.0086,0.0582,-0.1446,0.0496,0.0774,-0.02,-0.049,0.0178,0.0442,0.0203,0.0199,0.0673,0.0038,0.0237,0.0688,0.019,0.0108,-0.0494,-0.046,0.038,-0.0134,-0.0538,-0.0424,0.0008,-0.0193,-0.0333,0.0366,-0.0595,0.0042,0.0141,-0.0456,0.0234,-0.0169,0.0408,-0.0954,-0.0358,0.0291,0.0436,-0.004,0.1725,-0.0401,0.0508,0.0592,-0.0561,-0.0206,-0.0495,-0.0411,-0.022,0.0015,0.0019,0.0249,-0.0152,0.0497,-0.0828,0.0088,-0.0583,0.0719,0.0196,0.0034,-0.0371,-0.0266,-0.0108,0.0696,-0.0205,0.0712,-0.0308,0.023,0.1513,0.1049,0.0704,0.0206,0.0061,-0.0256,-0.0185,0.0098,0.0174,0.0034,-0.015,0.0232,-0.0182,0.0142,-0.0637,0.0142,-0.1057,-0.0091,0.103,-0.0574,0.0286,-0.0547,0.0024,-0.008,0.0239,-0.0133,-0.0182,0.0019,0.0237,0.0514,0.0352,-0.0478,-0.0067,-0.0327,-0.0846,-0.0176,0.1188,-0.0141,-0.0632,-0.0117,-0.0016,0.0007,-0.0247,0.0445,0.039,-0.0088,0.0369,0.0912,0.0157,-0.0799,-0.0092,-0.0152,0.023,0.0213,0.0057,-0.0271,0.0249,0.034,-0.0435,-0.0243,0.0043,0.0384,0.0068,-0.0626,-0.0298,-0.0242,-0.0041,-0.0287,-0.0344,-0.0013,-0.0518,-0.0112,-0.0305,0.0232,-0.0237,-0.0596,0.037,0.0119,0.0244,0.0014,0.0107,0.0024,0.0135,-0.0054,-0.0328,0.0383,-0.0411,-0.0242,0.0021,0.0554,0.0114,-0.0076,0.0584,0.0626,-0.0075,-0.1232,-0.2007,-0.0335,-0.0324,0.0484,0.0102,-0.0492,0.0411,-0.016,0.1002,0.0676,0.0497,-0.004,-0.0148,0.0152,0.0113,0.0203,0.0523,0.0479,-0.0095,0.012,-0.0104,-0.0214,-0.073,-0.0305,0.079,-0.016,0.1902,0.0092,0.0229,-0.0274,-0.0119,0.0485,-0.0641,-0.0694,0.027,0.0545,0.1167,0.0126,-0.0159,-0.0314,0.0224,-0.0157,-0.0278,-0.0993,-0.0583,-0.0217,0.038,0.0083,-0.0476,-0.0342,0.0251,-0.0099,0.0674,0.0089,0.0208,-0.0497,-0.0683,-0.0228,-0.0211,0.0227,-0.0159,-0.0705,-0.0009,-0.0676,0.091,0.01,-0.008,-0.0509,-0.0025,-0.0619,-0.0323,0.0852,-0.0178,-0.0015,0.0745,0.0177,-0.0054,-0.0205,-0.0286,-0.0245,0.0907,-0.0625,0.0405,0.0473,0.0291,-0.0126,0.0818,0.0142,-0.0052,-0.0022,-0.0046,0.002,-0.0757,0.02,0.0008,-0.0488,-0.2839,0.0315,0.0074,-0.0048,-0.0116,0.0204,0.0271,-0.0026,-0.0653,0.0005,0.0149,0.0651,0.0693,-0.019,0.0074,0.0511,0.0307,-0.0396,0.0482,-0.0842,-0.0139,0.0341,0.2275,-0.0538,0.0316,0.0156,0.0142,0.0601,0.0149,-0.0539,0.0045,0.0009,0.0911,-0.0552,0.0512,0.051,-0.036,0.0287,-0.0013,-0.0486,0.0075,-0.0143,-0.0313,0.0048,0.0875,-0.0387,-0.0256,-0.0579,0.0029,0.0761,-0.0311,-0.0017,-0.0071,-0.0091,0.0001,0.038,-0.047,-0.0547,-0.0104,-0.0635,-0.0285,-0.0335,-0.0534,-0.0281,0.0028]}
{"key":"[Geometry and Symmetry in Short-and-Sparse Deconvolution] We study the $\\textit{Short-and-Sparse (SaS) deconvolution}$ problem of recovering a short signal $\\mathbf a_0$ and a sparse signal $\\mathbf x_0$ from their convolution. We propose a method based on nonconvex optimization, which under certain conditions recovers the target short and sparse signals, up to a signed shift symmetry which is intrinsic to this model. This symmetry plays a central role in shaping the optimization landscape for deconvolution. We give a $\\textit{regional analysis}$, which characterizes this landscape geometrically, on a union of subspaces. Our geometric characterization holds when the length-$p_0$ short signal $\\mathbf a_0$ has shift coherence $\\mu$, and $\\mathbf x_0$ follows a random sparsity model with sparsity rate $\\theta \\in \\Bigl[\\frac{c_1}{p_0}, \\frac{c_2}{p_0\\sqrt\\mu + \\sqrt{p_0}}\\Bigr]\\cdot\\frac{1}{\\log^2p_0}$. Based on this geometry, we give a provable method that successfully solves SaS deconvolution with high probability.","layer":0,"vector":[-0.0133,-0.0016,0.0233,0.0048,0.0117,0.014,-0.0157,0.0016,0.0336,-0.0146,0.0189,-0.0625,0.0491,0.0223,0.0224,0.0374,0.0304,0.0634,-0.0326,0.0517,0.0259,-0.0643,0.0023,-0.0398,0.0584,0.0098,0.0056,-0.0482,-0.0589,-0.2729,0.027,0.0049,0.082,-0.0495,0.0277,-0.0459,-0.041,0.0087,-0.0574,0.0686,0.0273,0.011,-0.0329,-0.0174,-0.0479,-0.0474,-0.0133,-0.0268,-0.0113,-0.0433,0.003,0.0068,0.0156,0.0199,0.0425,0.0055,0.034,0.0617,0.0429,0.0473,0.0203,0.028,-0.1892,0.0612,0.0443,0.0512,-0.0172,-0.0609,0.0212,0.0586,-0.0062,0.0438,-0.0035,-0.0013,-0.0028,-0.0317,-0.0092,0.0058,-0.0323,-0.0044,0.046,-0.019,-0.0264,0.0516,-0.0316,-0.0832,0.0395,-0.055,0.0301,0.036,-0.0441,0.017,-0.0481,0.0439,-0.0659,-0.0258,0.0154,0.0726,-0.0005,0.1848,-0.0359,0.0521,0.0422,-0.0318,0.0402,-0.0397,-0.0475,-0.0269,-0.0132,-0.0104,0.0316,-0.0157,-0.0044,-0.0141,0.0281,-0.051,0.0478,0.0627,-0.0164,0.0137,-0.042,0.0407,0.0456,-0.0371,0.0477,-0.0521,0.0045,0.1272,0.0918,0.062,0.0512,0.0104,0.0187,-0.0289,0.0015,-0.0144,0.0302,0.0202,-0.0071,0.0007,-0.0487,-0.0753,0.0299,-0.0743,-0.0432,0.1235,-0.0465,-0.0206,-0.069,-0.0166,-0.0158,0.0136,-0.0466,-0.0161,0.0297,-0.0042,-0.0034,0.0214,-0.0333,0.0429,-0.035,-0.0476,-0.0245,0.1045,0.0113,-0.0641,-0.0248,-0.0262,-0.0037,-0.016,0.0146,0.0138,-0.0234,0.0377,0.0649,0.0349,-0.1028,0.0062,-0.0044,-0.0003,-0.012,-0.0531,-0.0013,0.0345,0.0602,-0.0377,0.0221,0.0021,0.0131,0.0192,-0.0136,-0.0034,-0.0667,-0.0068,-0.0765,-0.0197,0.0062,0.0044,-0.0261,-0.023,0.0018,-0.0273,-0.0736,0.057,0.0385,0.0336,0.0352,-0.0176,0.0271,0.0444,-0.0214,0.0207,0.0476,-0.0283,-0.0054,-0.0046,0.0189,0.0101,-0.0495,0.0432,-0.0221,-0.0739,-0.0831,-0.217,-0.0307,0.0039,-0.0001,0.0431,-0.0636,0.0143,-0.0022,0.1073,0.0672,0.049,0.0125,-0.0187,0.0151,0.0154,0.0676,0.0157,0.0543,-0.024,-0.0392,-0.0122,0.0248,-0.0225,-0.0503,0.069,-0.0125,0.1968,0.0152,0.066,0.0021,0.0089,0.0317,0.0121,-0.0551,0.0406,0.0537,0.0609,0.0342,-0.0599,-0.0366,-0.0478,-0.0452,0.0606,-0.0537,-0.0299,-0.0363,-0.0631,0.0305,-0.0558,0.02,0.0429,-0.0284,0.058,-0.0355,0.0465,-0.0469,-0.0777,0.0147,-0.0275,0.0329,0.0272,-0.0219,0.0054,-0.08,0.0397,0.0439,-0.01,-0.0418,0.0153,-0.0382,0.0044,0.0659,-0.0131,0.0318,0.0582,-0.0166,0.0703,-0.0476,-0.0351,-0.0621,0.105,-0.0185,0.0231,0.0136,0.0429,0.0204,0.0925,-0.0063,-0.005,-0.042,-0.0068,0.0411,-0.0247,-0.0151,-0.0117,0.0069,-0.2974,-0.009,0.0239,-0.0403,-0.0041,-0.0215,0.0553,-0.0072,-0.0892,-0.0282,-0.0209,0.0182,0.02,-0.0227,0.0423,0.0635,0.0662,-0.0839,0.0237,-0.0879,-0.023,-0.0031,0.2036,-0.0501,0.014,0.0356,-0.0133,0.0337,-0.0214,-0.0361,0.0159,0.0195,0.0722,-0.0774,0.0235,0.0537,-0.0464,0.066,0.0324,-0.0283,0.013,0.0224,-0.0173,-0.0254,0.109,-0.0616,-0.0393,-0.0014,0.0327,0.0179,-0.0289,0.0381,0.0061,-0.004,0.0164,0.0024,-0.0927,-0.0173,0.0137,-0.0168,0.0411,-0.0336,-0.0426,-0.0292,-0.0153]}
{"key":"[Anytime Online-to-Batch Conversions, Optimism, and Acceleration] A standard way to obtain convergence guarantees in stochastic convex optimization is to run an online learning algorithm and then output the average of its iterates: the actual iterates of the online learning algorithm do not come with individual guarantees. We close this gap by introducing a black-box modification to any online learning algorithm whose iterates converge to the optimum in stochastic scenarios. We then consider the case of smooth losses, and show that combining our approach with optimistic online learning algorithms immediately yields a fast convergence rate of $O(L/T^{3/2}+\\sigma/\\sqrt{T})$ on $L$-smooth problems with $\\sigma^2$ variance in the gradients. Finally, we provide a reduction that converts any adaptive online algorithm into one that obtains the optimal accelerated rate of $\\tilde O(L/T^2 + \\sigma/\\sqrt{T})$, while still maintaining $\\tilde O(1/\\sqrt{T})$ convergence in the non-smooth setting. Importantly, our algorithms adapt to $L$ and $\\sigma$ automatically: they do not need to know either to obtain these rates.","layer":0,"vector":[-0.0737,-0.023,0.0309,0.0512,-0.0201,0.0427,0.006,0.0482,0.0351,-0.0059,0.0487,-0.0281,0.0198,0.0606,-0.0132,0.0506,0.0225,0.0114,-0.029,0.0057,0.0483,-0.0437,-0.0052,-0.0551,0.0551,-0.0332,-0.0302,-0.0702,-0.0208,-0.2352,0.0346,-0.071,0.0288,-0.0217,0.0204,0.0113,-0.0265,0.0936,-0.0477,0.0439,0.0107,0.0452,-0.0717,-0.0419,-0.0116,-0.039,0.0095,0.0018,-0.0268,-0.0224,0.0273,-0.0443,0.0365,0.0117,-0.0022,0.0475,0.0251,0.073,-0.0008,0.0201,-0.0147,0.0541,-0.1807,0.0602,0.0092,0.0089,-0.0305,-0.016,-0.0205,0.1003,-0.0171,0.0472,0.0235,0.0524,0.0592,0.0034,0.012,-0.0314,-0.0268,0.0237,0.0261,-0.0332,-0.0575,-0.0093,-0.045,-0.0554,0.0305,-0.0295,0.0654,0.0438,-0.0292,-0.0323,-0.022,-0.0006,-0.0724,0.0016,0.0257,0.0468,0.0043,0.1718,-0.0387,0.0648,0.0077,-0.0107,0.0175,-0.041,-0.0141,-0.0121,0.0159,-0.0184,-0.0323,-0.0203,0.0605,-0.0281,-0.0048,-0.0055,0.0013,0.0135,0.024,-0.0008,-0.0451,0.0547,0.0795,-0.0077,0.03,-0.0632,0.0005,0.1361,0.0126,0.0629,0.0406,-0.0493,-0.0356,-0.0609,0.0144,-0.0048,0.0146,-0.0187,0.0338,-0.0095,-0.0342,-0.0619,-0.0167,-0.107,-0.011,0.1624,-0.001,0.0431,-0.0228,-0.0685,0.0368,-0.0026,-0.0185,-0.0045,0.0257,0.009,0.0462,0.0561,-0.0672,0.0052,-0.0427,-0.0352,-0.008,0.1022,-0.0322,-0.0323,-0.0286,-0.0084,0.0212,0.0044,0.0655,0.022,-0.0421,0.0261,0.0672,0.04,-0.0932,-0.0288,0.0276,0.01,0.0252,-0.0247,-0.03,0.0099,0.0393,-0.0244,0.0087,-0.0583,0.0418,0.0223,-0.0431,-0.0058,-0.0623,-0.0106,-0.0056,-0.0404,0.0101,-0.0346,0.0327,0.0012,0.0263,-0.0425,-0.0696,0.0475,0.0291,-0.0015,-0.0272,-0.0261,0.0555,0.0394,-0.0427,-0.0278,0.057,-0.0159,-0.0275,0.0481,0.0486,0.0521,-0.0124,0.0581,0.0209,0.0174,-0.0495,-0.1793,-0.0202,0.0028,-0.0104,0.0524,-0.0768,0.054,-0.0348,0.0659,0.0572,0.0465,-0.0479,0.0162,0.0011,0.0205,0.034,0.0729,0.01,0.0145,0.0115,-0.0125,0.0572,-0.0231,-0.0891,0.0605,-0.0055,0.2119,0.0003,0.0709,-0.0457,0.034,0.0341,0.0479,-0.0772,0.0384,0.0377,0.1009,-0.0448,-0.0497,-0.0089,0.0023,-0.0051,-0.0008,-0.1184,-0.0652,-0.0446,-0.025,0.0576,-0.0592,-0.0123,0.0393,-0.0204,0.0996,-0.0659,0.0165,-0.0399,-0.1049,0.0409,-0.014,0.0192,-0.0106,-0.0481,-0.0041,-0.0542,0.065,0.004,0.0087,-0.0397,0.0109,-0.0446,0.0216,0.0443,-0.0162,0.004,0.0401,-0.0016,0.0308,0.0167,-0.0809,-0.0065,0.0469,-0.0636,0.0369,0.027,0.0389,0.007,0.093,-0.0173,-0.0306,0.0257,-0.0255,-0.0352,-0.0816,-0.0151,0.0155,0.0116,-0.2846,0.0208,0.009,0.0086,-0.0398,-0.0057,0.0416,-0.0178,-0.0562,0.0108,-0.0231,0.0758,0.0012,0.0455,0.0432,0.0462,0.0545,-0.0499,0.0572,-0.086,-0.0109,0.0372,0.2031,-0.0647,-0.0272,-0.0011,-0.0248,0.0119,0.0482,-0.0561,0.0172,-0.0136,0.0399,-0.0655,0.0633,0.0696,-0.0385,0.0291,0.0153,0.0013,-0.0359,0.0048,-0.0223,-0.0003,0.0896,-0.0422,-0.0337,-0.0313,-0.0115,0.0023,-0.023,0.0416,0.0292,-0.0358,-0.002,0.014,-0.0599,-0.0777,-0.0415,-0.0487,0.0407,-0.0747,-0.0483,0.0115,0.0007]}
{"key":"[Auditing and Debugging Deep Learning Models via Decision Boundaries: Individual-level and Group-level Analysis] Deep learning models have been criticized for their lack of easy interpretation, which undermines confidence in their use for important applications. Nevertheless, they are consistently utilized in many applications, consequential to humans' lives, mostly because of their better performance. Therefore, there is a great need for computational methods that can explain, audit, and debug such models. Here, we use flip points to accomplish these goals for deep learning models with continuous output scores (e.g., computed by softmax), used in social applications. A flip point is any point that lies on the boundary between two output classes: e.g. for a model with a binary yes/no output, a flip point is any input that generates equal scores for \"yes\" and \"no\". The flip point closest to a given input is of particular importance because it reveals the least changes in the input that would change a model's classification, and we show that it is the solution to a well-posed optimization problem. Flip points also enable us to systematically study the decision boundaries of a deep learning classifier. The resulting insight into the decision boundaries of a deep model can clearly explain the model's output on the individual-level, via an explanation report that is understandable by non-experts. We also develop a procedure to understand and audit model behavior towards groups of people. Flip points can also be used to alter the decision boundaries in order to improve undesirable behaviors. We demonstrate our methods by investigating several models trained on standard datasets used in social applications of machine learning. We also identify the features that are most responsible for particular classifications and misclassifications.","layer":1,"vector":[-0.0396,0.0055,0.0528,-0.0131,0.0385,0.0517,0.0792,0.0387,0.0441,-0.0461,0.0223,-0.0357,-0.0179,0.0089,-0.0016,0.0206,-0.0371,0.015,-0.0131,0.0008,0.0253,-0.027,-0.028,-0.0386,0.0352,0.0181,-0.0326,-0.0729,-0.0528,-0.2466,0.0064,-0.0123,0.059,-0.0347,0.0232,-0.0158,-0.0222,0.0095,-0.0171,0.0445,0.0414,0.0103,-0.019,-0.0583,-0.0116,-0.0245,-0.044,-0.034,-0.0135,-0.0326,0.0261,-0.0358,0.0144,0.0102,0.0688,0.0389,0.098,0.0773,0.0174,0.0173,0.0368,0.0389,-0.1506,0.0357,0.0507,0.0622,-0.0327,-0.0145,-0.0017,0.0378,-0.0133,0.0168,0.0238,0.0224,0.0308,0.0173,0.0046,0.0167,-0.0128,0.0039,-0.0023,-0.0258,-0.0344,-0.0041,-0.0068,-0.0113,0.0586,-0.0518,0.0476,-0.0242,-0.0178,-0.0116,-0.0385,0.0201,-0.0491,0.0026,0.0204,0.0073,-0.0649,0.1926,-0.0642,0.0407,0.0294,-0.0302,0.0539,-0.0414,-0.0663,-0.0298,0.0015,-0.0185,-0.0307,-0.0203,0.0317,-0.0335,0.0244,0.0053,0.081,0.0628,0.0015,-0.0362,-0.026,0.0495,0.0749,-0.0004,0.0237,-0.0441,-0.0046,0.1391,0.0173,0.0029,0.0281,-0.0384,-0.0755,-0.0245,0.004,-0.0006,0.0141,0.0624,0.0151,-0.0264,-0.0381,-0.034,0.0328,-0.0678,-0.0411,0.1263,-0.0851,0.009,-0.0424,-0.024,-0.0161,0.026,-0.0617,-0.0443,0.0503,0.0322,0.0252,0.0261,-0.0804,0.018,0.0317,-0.0273,-0.0303,0.1067,0.0303,-0.0686,-0.0434,0.0047,-0.0247,0.0045,0.0138,0.0199,-0.0341,0.0035,0.0341,0.0174,-0.0617,-0.0053,-0.008,0.0174,0.0386,-0.048,-0.057,0.0218,0.0734,-0.0276,-0.0015,-0.078,0.0478,0.05,-0.0555,0.0008,-0.0625,0.0007,-0.0199,-0.0662,-0.0106,0.0007,-0.0056,-0.0057,-0.049,0.0399,-0.0458,0.0172,0.0159,0.027,-0.0241,-0.0211,0.0675,0.0122,-0.0226,0.0152,0.0248,-0.01,0.0034,-0.0006,0.0296,-0.0058,-0.043,0.0446,0.0428,-0.0289,-0.0441,-0.2249,-0.0222,0.0196,0.0051,0.0492,-0.0879,0.0442,-0.0171,0.027,0.0553,0.0756,-0.0259,-0.041,0.03,0.0079,0.0468,0.0044,0.0452,-0.0448,0.0517,-0.0294,0.0355,0.0277,-0.1135,0.0473,0.0361,0.2331,0.0358,0.0422,0.0367,0.0208,0.0075,-0.0222,-0.1419,0.0767,0.034,0.0779,-0.012,-0.0693,-0.0283,-0.0242,0.0211,0.0066,-0.0905,-0.0386,-0.0145,-0.0305,0.0045,-0.0623,0.0163,-0.0104,-0.0707,0.0751,-0.0012,-0.0558,-0.0508,-0.0998,-0.0003,-0.0354,0.0367,-0.0124,-0.027,0.0136,-0.0869,0.0664,-0.0112,-0.0626,-0.0629,0.0242,-0.0244,-0.0011,0.0867,0.0157,-0.014,0.0365,0.0065,0.0496,-0.0609,-0.0582,-0.0264,0.0512,-0.0225,0.0267,0.0135,0.0471,0.0081,0.0858,-0.0387,0.0309,-0.0112,-0.0044,0.004,-0.0701,-0.0077,0.0454,-0.022,-0.2849,0.0191,-0.0327,0.0432,-0.0054,0.0317,0.036,-0.0003,-0.0291,0.0057,0.0482,0.0461,0.0641,-0.0047,0.0324,0.0272,0.0577,-0.0711,0.0685,-0.0536,0.0314,0.036,0.1927,-0.0143,0.0496,0.0234,-0.0377,-0.0248,0.0542,-0.0216,-0.0187,0.0175,0.0959,-0.0344,0.0399,0.111,-0.0048,-0.0034,0.0647,-0.0042,-0.0041,-0.0124,-0.0287,-0.0438,0.0839,0.0011,-0.0416,-0.0341,0.064,-0.0051,-0.0176,-0.0133,-0.0257,0.0076,0.0489,0.0254,-0.0457,-0.0043,-0.0122,-0.0555,-0.0051,-0.0518,0.0151,-0.0433,-0.0012]}
{"key":"[Machine Learning and Meta-Analysis Approach to Identify Patient Comorbidities and Symptoms that Increased Risk of Mortality in COVID-19] Background: Providing appropriate care for people suffering from COVID-19, the disease caused by the pandemic SARS-CoV-2 virus is a significant global challenge. Many individuals who become infected have pre-existing conditions that may interact with COVID-19 to increase symptom severity and mortality risk. COVID-19 patient comorbidities are likely to be informative about individual risk of severe illness and mortality. Accurately determining how comorbidities are associated with severe symptoms and mortality would thus greatly assist in COVID-19 care planning and provision. Methods: To assess the interaction of patient comorbidities with COVID-19 severity and mortality we performed a meta-analysis of the published global literature, and machine learning predictive analysis using an aggregated COVID-19 global dataset. Results: Our meta-analysis identified chronic obstructive pulmonary disease (COPD), cerebrovascular disease (CEVD), cardiovascular disease (CVD), type 2 diabetes, malignancy, and hypertension as most significantly associated with COVID-19 severity in the current published literature. Machine learning classification using novel aggregated cohort data similarly found COPD, CVD, CKD, type 2 diabetes, malignancy and hypertension, as well as asthma, as the most significant features for classifying those deceased versus those who survived COVID-19. While age and gender were the most significant predictor of mortality, in terms of symptom-comorbidity combinations, it was observed that Pneumonia-Hypertension, Pneumonia-Diabetes and Acute Respiratory Distress Syndrome (ARDS)-Hypertension showed the most significant effects on COVID-19 mortality. Conclusions: These results highlight patient cohorts most at risk of COVID-19 related severe morbidity and mortality which have implications for prioritization of hospital resources.","layer":1,"vector":[-0.0284,-0.0129,0.0387,0.0275,0.0128,0.0243,0.073,0.0273,0.0554,-0.029,0.0184,-0.0367,0.013,0.0581,0.0054,0.0408,0.0047,-0.0081,-0.0787,0.0031,0.0014,0.0133,0.0312,-0.062,0.0403,0.0,-0.0445,-0.051,-0.0976,-0.165,-0.0244,-0.0455,0.0602,0.0042,-0.0194,-0.023,-0.0293,0.088,-0.0143,-0.0055,-0.0225,0.0102,0.0153,-0.0341,0.0043,-0.0741,-0.0383,-0.0028,0.0118,-0.0131,0.0141,-0.0026,0.0209,0.0505,0.0641,0.0346,0.003,0.0208,0.061,0.0258,0.007,-0.0026,-0.1952,0.0687,0.0461,0.0426,-0.0513,-0.0008,0.0435,0.0318,0.0001,0.0114,-0.0038,0.0462,-0.0052,0.055,-0.0022,-0.0251,0.0242,0.0629,0.0568,0.0359,-0.0139,-0.0404,-0.0303,-0.0841,-0.0208,-0.0492,-0.0202,-0.0597,-0.0365,-0.0239,0.0197,0.0238,-0.061,-0.0134,0.0106,0.0076,-0.0551,0.1593,-0.0329,-0.0185,0.0079,0.007,0.0713,-0.0774,-0.0784,-0.0321,0.0178,-0.0339,-0.0102,0.009,0.0609,-0.0025,-0.0262,-0.0005,0.0535,0.0302,0.0251,0.0465,0.015,-0.0101,0.0387,0.0146,0.0277,-0.04,0.0237,0.1447,0.0117,-0.0146,0.0851,0.0096,-0.0708,0.0038,0.0013,0.0316,0.0483,0.019,0.027,0.028,-0.0257,-0.0227,0.0128,-0.108,-0.061,0.1491,-0.0476,-0.0049,-0.0323,-0.0298,-0.0101,0.0622,-0.0777,-0.0197,-0.0127,0.0218,0.0809,0.0428,-0.0212,0.0169,-0.0039,-0.0731,-0.0724,0.1386,0.0138,-0.0563,-0.0201,-0.0108,0.0214,0.0025,0.0794,0.0215,0.0035,0.0148,0.1287,0.0239,-0.0265,-0.0283,-0.0293,0.0007,-0.0106,0.0603,0.0096,0.0542,0.0432,-0.0184,-0.0071,-0.0748,0.0045,-0.0107,-0.0176,0.0166,-0.0416,0.0204,-0.0053,0.0049,-0.0633,-0.0078,-0.0191,-0.0561,0.0008,-0.0053,-0.0059,0.0034,-0.0078,-0.0041,-0.0791,0.0345,0.0324,0.0031,-0.0048,-0.0301,0.0705,0.0235,-0.0582,-0.0235,0.0002,0.0227,-0.0037,0.0905,0.0504,0.0055,-0.0627,-0.2263,-0.0343,0.0173,-0.0555,-0.0205,-0.0652,0.0029,-0.0368,0.0141,0.0771,0.0219,0.0278,-0.0279,0.0009,0.0031,0.0865,0.023,-0.0292,-0.0723,-0.0029,-0.0132,0.0017,0.0305,-0.1012,0.0088,-0.0085,0.225,-0.0026,-0.0347,-0.0157,0.0392,-0.0036,-0.053,-0.1317,0.0961,-0.0134,0.0626,-0.0544,-0.0592,-0.0527,-0.0862,0.0312,0.0371,-0.0876,-0.0319,-0.0098,-0.0084,0.0637,-0.058,0.0498,0.0615,0.0017,0.0469,-0.0211,0.0213,-0.0494,-0.1279,0.0263,-0.0347,-0.0012,-0.0,-0.0383,0.0672,-0.0692,0.0575,-0.029,-0.0479,-0.0456,-0.0233,-0.0249,-0.0506,0.0898,-0.0489,-0.0398,0.0306,0.0489,0.0168,-0.0008,0.008,-0.0298,0.084,-0.0627,0.0637,0.051,0.0498,-0.0084,0.0712,0.0443,0.0032,-0.0443,-0.0114,-0.0321,-0.0417,-0.0258,0.0324,0.0167,-0.27,0.0477,0.0003,-0.0299,-0.0056,-0.0204,0.0233,0.0091,-0.0632,0.0149,0.0715,0.0526,0.0205,-0.0452,-0.0132,0.0407,0.0866,-0.0604,0.0787,-0.0566,-0.0022,0.0324,0.1531,-0.0561,0.0114,0.0144,0.0088,0.0268,-0.0053,-0.0258,-0.015,-0.0136,0.0809,-0.037,0.0501,0.0885,-0.0107,0.015,0.0455,0.0052,0.0217,0.0479,0.0087,-0.0237,0.0695,-0.0105,-0.0341,-0.0362,0.004,-0.0154,-0.0016,0.0152,-0.016,0.036,0.0144,-0.0302,0.0229,-0.0558,0.0122,-0.0113,0.0315,-0.0157,-0.0322,0.0214,0.0135]}
{"key":"[Difference of Convex Functions Programming Applied to Control with Expert Data] This paper reports applications of Difference of Convex functions (DC) programming to Learning from Demonstrations (LfD) and Reinforcement Learning (RL) with expert data. This is made possible because the norm of the Optimal Bellman Residual (OBR), which is at the heart of many RL and LfD algorithms, is DC. Improvement in performance is demonstrated on two specific algorithms, namely Reward-regularized Classification for Apprenticeship Learning (RCAL) and Reinforcement Learning with Expert Demonstrations (RLED), through experiments on generic Markov Decision Processes (MDP), called Garnets.","layer":0,"vector":[-0.0532,0.0086,0.0421,-0.0059,0.011,0.0412,0.044,0.0711,0.0253,-0.0104,-0.0034,-0.0151,0.0151,0.0687,0.0031,0.0599,0.0048,0.0632,-0.0543,0.0175,0.0272,-0.0783,-0.0427,-0.0892,0.03,0.0324,-0.0334,-0.0527,0.0123,-0.2097,0.0237,-0.0485,0.0132,-0.0143,-0.0221,-0.0006,-0.0233,0.0193,-0.053,0.0374,0.0284,-0.0364,-0.0041,-0.0395,-0.0258,-0.0598,-0.0421,-0.0284,-0.0327,-0.0154,0.0054,-0.0418,0.0198,0.0083,0.0333,0.027,0.0422,0.0466,0.0583,0.0591,0.0157,0.0237,-0.2018,0.084,0.0096,0.0721,-0.0614,-0.0606,0.0266,0.0645,-0.0239,0.0329,-0.0158,0.0352,-0.0079,-0.0284,-0.0286,-0.033,-0.0459,-0.0211,0.0214,-0.0776,-0.071,-0.0137,-0.0313,-0.001,0.0666,-0.0943,0.0684,0.0309,-0.0184,0.0191,-0.018,0.0486,-0.0674,0.0146,0.0489,0.0235,-0.0556,0.1945,-0.0433,0.0007,0.0057,0.0148,0.0656,-0.0205,-0.0472,-0.0185,-0.0087,-0.0019,-0.0393,-0.0389,0.0296,-0.0076,-0.0244,0.0461,0.071,0.0226,0.0037,0.0061,0.0003,-0.0106,0.0921,-0.0291,0.0015,-0.0655,0.0119,0.1302,-0.0006,0.0483,0.0596,-0.0694,-0.0659,-0.0004,0.0402,0.0055,-0.0028,-0.021,0.0251,0.0132,-0.0476,0.0078,0.0044,-0.1033,-0.0366,0.0973,0.0008,0.0192,-0.0355,-0.0239,0.0168,-0.0129,-0.0181,-0.0107,0.0032,0.0502,0.0403,0.0742,-0.0513,-0.0022,-0.0454,-0.0501,-0.0154,0.109,-0.0381,-0.0888,-0.0736,0.0002,0.0053,-0.0124,0.034,0.0155,-0.0488,-0.0043,0.0743,0.0291,-0.0991,-0.0294,0.0516,-0.0104,0.0548,-0.0783,-0.0069,-0.002,0.0402,-0.0283,-0.0148,-0.0322,0.0412,0.0309,-0.0374,-0.0037,-0.0246,-0.0455,-0.0165,-0.0458,-0.0128,-0.0303,-0.0004,-0.0052,0.017,0.0091,-0.0186,0.0158,-0.0363,0.0275,-0.0034,0.0201,0.0762,0.0293,-0.0512,-0.006,0.0629,-0.0034,-0.0472,0.0025,0.0849,0.0201,-0.0618,0.0346,0.0426,0.0412,-0.042,-0.2102,-0.0014,0.0093,-0.0002,0.0595,-0.0544,0.0371,-0.0283,-0.0132,0.0517,0.0654,-0.0246,-0.0284,-0.0131,-0.0121,0.0411,0.0828,0.0496,0.0044,0.0199,-0.0283,0.0079,0.0053,-0.0721,0.0167,-0.0212,0.1858,-0.0039,0.0751,-0.0013,0.0229,0.0562,-0.0114,-0.0668,0.0463,0.0093,0.057,-0.016,0.0019,-0.0451,-0.0059,0.0017,-0.0095,-0.048,-0.0234,-0.0152,-0.0537,0.0708,-0.0708,0.0297,0.0401,-0.0019,0.0368,-0.0275,-0.0306,0.0025,-0.101,0.0587,-0.0471,0.0138,0.0334,-0.0738,0.0019,-0.0326,0.0567,-0.0083,-0.006,-0.0176,0.0196,-0.0217,-0.0141,0.0697,0.0041,-0.0056,0.0095,0.0445,0.0056,0.0173,-0.0597,-0.0379,0.0362,-0.0448,0.0339,0.0233,0.0142,-0.0209,0.0407,-0.0134,0.0371,0.0041,0.0087,0.0185,-0.0543,0.0074,0.0425,0.0225,-0.3137,0.0573,0.0396,0.0171,-0.0553,0.027,0.0247,-0.0292,-0.0397,-0.0428,0.0434,0.0401,0.0379,0.0575,0.0145,0.035,0.0495,-0.0265,0.0491,-0.0567,-0.0035,0.0874,0.2316,-0.0566,0.0253,0.0119,-0.0197,-0.0321,0.0454,-0.0425,0.0484,-0.0019,0.0545,-0.0387,0.0642,0.0999,-0.0463,0.0348,0.0003,0.0041,-0.0672,-0.0259,-0.0393,0.0038,0.1152,0.0236,-0.004,-0.0684,-0.0158,0.0318,-0.0209,0.0258,-0.0016,-0.0352,0.0111,0.029,-0.0382,-0.0795,-0.0067,-0.0353,0.0351,-0.054,0.0236,-0.0373,0.0196]}
{"key":"[A Machine Learning-Based Migration Strategy for Virtual Network Function Instances] With the growing demand for data connectivity, network service providers are faced with the task of reducing their capital and operational expenses while simultaneously improving network performance and addressing the increased demand. Although Network Function Virtualization (NFV) has been identified as a promising solution, several challenges must be addressed to ensure its feasibility. In this paper, we address the Virtual Network Function (VNF) migration problem by developing the VNF Neural Network for Instance Migration (VNNIM), a migration strategy for VNF instances. The performance of VNNIM is further improved through the optimization of the learning rate hyperparameter through particle swarm optimization. Results show that the VNNIM is very effective in predicting the post-migration server exhibiting a binary accuracy of 99.07% and a delay difference distribution that is centered around a mean of zero when compared to the optimization model. The greatest advantage of VNNIM, however, is its run-time efficiency highlighted through a run-time analysis.","layer":0,"vector":[-0.0603,-0.0346,0.0398,-0.0509,0.0296,0.0301,0.0034,0.0395,0.0292,0.0052,0.0239,-0.0211,0.0716,0.013,-0.0064,-0.0025,-0.0303,0.0362,-0.0289,-0.0285,0.0156,-0.023,-0.0444,-0.0868,0.0291,0.0012,-0.0107,-0.0193,-0.0533,-0.2313,0.0131,-0.0441,0.0003,-0.0423,0.0311,-0.0281,0.0281,0.064,-0.0322,0.0964,0.0498,0.0197,0.0005,-0.0328,-0.0058,-0.0503,-0.018,-0.0228,0.0062,0.0003,0.0247,-0.0268,-0.0156,0.0434,0.0295,0.057,0.0572,0.0137,0.0409,0.0475,0.0661,0.0313,-0.1817,0.0528,0.0559,-0.0092,-0.0423,-0.061,-0.0145,0.0491,-0.0876,0.0381,-0.0151,0.0317,-0.0047,0.0312,-0.0293,-0.0104,0.0015,0.0172,0.0493,-0.0397,-0.0391,-0.0143,-0.0353,-0.0365,0.021,-0.0145,0.0311,-0.0048,-0.0456,0.0203,-0.0018,0.0073,-0.0856,0.0115,0.0404,-0.0103,-0.0742,0.205,-0.0734,-0.0133,0.0522,-0.0405,0.0193,0.0008,-0.0397,-0.0438,-0.0393,0.0008,-0.0327,-0.0519,-0.0085,-0.0351,0.0321,0.0584,0.0246,0.0332,0.0033,-0.0016,-0.0373,0.0201,0.0628,-0.026,0.0301,-0.0576,0.0444,0.1301,-0.023,0.0436,0.0343,-0.025,-0.0168,-0.0282,-0.0119,0.0424,0.0391,-0.0175,-0.0426,0.0128,-0.0163,-0.046,0.0277,-0.0776,-0.028,0.112,-0.001,0.0369,-0.0211,-0.0269,-0.015,0.0348,-0.0467,-0.0372,-0.0153,0.0144,0.0101,0.0897,-0.0602,-0.0214,-0.045,-0.0372,-0.0639,0.1293,0.0229,-0.1241,-0.0388,-0.0201,0.0105,-0.0281,-0.0035,0.0375,-0.0563,0.023,0.0989,0.0143,-0.0498,-0.0201,-0.0396,0.0127,0.0311,-0.0472,-0.0656,0.0099,0.0482,-0.0578,-0.0169,-0.0232,0.0142,0.0232,-0.0518,0.024,-0.0032,-0.0321,-0.0076,-0.0287,-0.0477,0.0141,0.0479,-0.0231,0.0367,0.0303,-0.0116,0.0146,-0.0443,-0.0026,-0.0086,-0.0024,0.009,0.0598,-0.0172,-0.0153,0.0812,0.0025,-0.0183,-0.0266,-0.0202,0.062,0.039,0.0375,0.045,-0.014,-0.0387,-0.2244,-0.0226,-0.0026,-0.0228,0.0526,-0.0257,0.0555,0.0334,0.0578,0.0408,0.0446,-0.0189,-0.011,0.0417,0.0064,0.0277,0.0675,0.0653,-0.0359,-0.0453,0.0339,0.0027,-0.0114,-0.0699,0.0363,0.0322,0.1753,-0.069,0.0614,-0.0647,0.0908,0.0063,-0.0273,-0.1154,0.0543,0.0251,0.1027,-0.0135,-0.0601,-0.0017,-0.0033,0.0419,-0.0227,-0.1028,-0.0152,-0.0136,-0.0281,0.0333,-0.1065,0.0026,0.0674,-0.0338,0.0717,0.0218,0.0312,-0.058,-0.0554,0.0254,-0.0592,0.0381,0.0279,-0.0454,-0.0015,-0.0434,0.077,0.0178,0.0374,0.032,0.0478,-0.0298,-0.0543,0.0782,-0.0123,0.0306,0.0422,-0.0055,-0.0134,0.0148,-0.0103,-0.0378,0.0606,-0.0609,0.0405,-0.001,-0.0031,0.0146,0.0548,0.0264,0.0124,-0.0213,-0.0558,-0.0259,-0.0344,0.0002,0.0462,-0.0016,-0.2788,0.0477,0.0091,0.0448,-0.0512,0.0326,0.0448,0.035,-0.0061,-0.0235,0.0309,-0.0037,0.033,-0.0175,0.0208,0.0515,0.0396,-0.0131,0.0433,-0.0778,0.0128,-0.0199,0.225,-0.064,0.0451,0.0302,-0.0332,0.0305,0.0607,-0.0533,-0.039,0.015,0.0749,-0.0544,0.0009,0.0335,-0.039,0.0356,0.0255,-0.0154,-0.0106,0.0091,-0.0277,0.0148,0.093,0.0036,-0.0073,-0.0658,-0.0007,0.0159,-0.0579,0.0159,0.0177,0.021,0.0392,0.0138,-0.0534,-0.0422,-0.0609,0.0085,0.0802,-0.0768,0.0369,0.0227,-0.024]}
{"key":"[Automated Concatenation of Embeddings for Structured Prediction] Pretrained contextualized embeddings are powerful word representations for structured prediction tasks. Recent work found that better word representations can be obtained by concatenating different types of embeddings. However, the selection of embeddings to form the best concatenated representation usually varies depending on the task and the collection of candidate embeddings, and the ever-increasing number of embedding types makes it a more difficult problem. In this paper, we propose Automated Concatenation of Embeddings (ACE) to automate the process of finding better concatenations of embeddings for structured prediction tasks, based on a formulation inspired by recent progress on neural architecture search. Specifically, a controller alternately samples a concatenation of embeddings, according to its current belief of the effectiveness of individual embedding types in consideration for a task, and updates the belief based on a reward. We follow strategies in reinforcement learning to optimize the parameters of the controller and compute the reward based on the accuracy of a task model, which is fed with the sampled concatenation as input and trained on a task dataset. Empirical results on 6 tasks and 21 datasets show that our approach outperforms strong baselines and achieves state-of-the-art performance with fine-tuned embeddings in all the evaluations.","layer":1,"vector":[-0.0376,-0.0393,0.0151,0.0181,0.0075,0.0513,0.022,0.0222,0.0222,-0.0351,-0.0142,-0.0249,0.0382,0.0457,0.0588,-0.0053,-0.0544,0.0802,-0.0345,0.0066,0.0378,-0.0451,0.0034,-0.0479,0.0059,0.02,-0.0476,-0.0463,-0.035,-0.2203,-0.0042,-0.0812,0.0475,-0.004,-0.0204,-0.0144,-0.0364,0.0906,-0.0406,0.0815,0.0275,0.0051,-0.014,-0.0728,-0.0005,-0.0364,-0.0359,-0.0248,-0.0163,-0.046,0.0621,-0.0685,0.0025,0.0665,0.0326,0.0289,0.0287,0.0762,0.0187,0.0652,0.0088,0.0327,-0.1632,0.0438,-0.0095,0.0374,-0.0563,0.0411,-0.0154,0.0977,-0.0007,0.0329,0.0105,0.059,0.0185,0.0324,0.0219,-0.004,-0.0016,-0.0101,0.0108,-0.0309,-0.0482,-0.0075,0.0038,-0.0627,-0.0029,-0.0175,0.0322,0.0314,-0.0412,-0.0393,-0.06,0.0102,-0.0761,0.0122,0.0565,0.0152,-0.0867,0.1702,-0.0516,0.0444,0.0229,-0.0531,0.0429,-0.0118,-0.0368,0.0065,-0.0579,-0.0586,-0.0419,-0.0224,0.048,-0.0291,0.0383,0.0171,0.0805,0.0555,-0.0071,-0.0142,-0.0402,-0.0087,0.0244,0.0072,0.0354,-0.0535,0.0791,0.1406,0.0402,0.0192,0.0331,0.0004,-0.0305,-0.008,0.0105,0.0407,0.021,-0.0036,-0.0011,-0.0168,-0.0417,0.012,0.0034,-0.0863,-0.0628,0.1273,-0.0413,-0.0173,-0.0288,-0.0044,-0.0201,-0.0011,0.0062,-0.0354,0.0371,0.0431,-0.0069,0.0201,-0.0687,-0.0309,-0.0041,-0.0395,-0.0657,0.0954,0.0258,-0.0989,-0.0328,-0.0169,0.0408,-0.0386,0.0548,-0.0111,-0.0507,0.0361,0.0693,0.0168,-0.0732,0.0229,0.0435,-0.005,0.0237,-0.0625,-0.0618,0.0193,0.0638,-0.05,0.007,-0.0579,0.0415,0.027,-0.0294,0.0474,-0.0137,0.0169,-0.0517,-0.0384,0.0207,-0.0068,0.0154,0.0124,0.001,0.0239,-0.0366,-0.0216,-0.0162,-0.0071,-0.0146,0.035,0.0664,0.0175,-0.0297,-0.011,0.0608,0.0098,-0.057,0.0042,0.0053,0.0431,0.0326,0.0683,-0.0014,-0.0245,-0.0263,-0.2147,-0.025,0.0041,-0.033,0.0149,-0.0541,0.0233,0.0188,0.061,0.0387,0.0022,-0.0661,0.008,0.0372,0.0034,0.041,0.0623,-0.0049,0.0019,0.0177,0.0277,-0.0014,-0.0258,-0.0691,0.0445,-0.0101,0.2189,0.0487,0.0362,-0.0054,0.0055,0.0184,-0.0533,-0.1234,0.065,0.0074,0.055,0.0174,-0.0319,-0.0341,-0.0528,0.0201,-0.0068,-0.0956,-0.0336,-0.0429,-0.0299,0.0524,-0.0755,0.013,0.0345,-0.0643,0.0503,0.0049,-0.0587,-0.0061,-0.0827,0.0275,-0.0164,0.014,0.051,-0.0247,0.0252,-0.0416,0.0196,0.0163,-0.01,-0.0262,0.0145,-0.0117,-0.0445,0.067,-0.0216,-0.0102,0.0581,0.005,0.0079,-0.0199,-0.0381,0.0151,0.0686,-0.0605,0.0592,0.0539,0.0478,-0.0242,0.0874,-0.0142,0.0652,-0.0127,0.0147,-0.0022,-0.0264,0.0131,0.0562,-0.0187,-0.3071,0.0723,0.0504,0.0075,-0.0094,0.0082,0.0345,-0.0178,-0.0432,0.0365,0.0119,0.0375,0.0473,-0.0097,-0.0218,0.0501,0.1039,-0.0564,0.01,-0.0555,-0.0396,0.0386,0.2588,-0.0478,0.0523,0.0044,-0.036,-0.04,0.0535,0.011,0.0203,0.0019,0.0723,-0.0617,0.0164,0.0561,-0.0229,0.0264,0.0611,0.0033,-0.0022,0.0066,-0.0306,-0.0256,0.0227,0.0126,-0.0253,-0.0255,-0.0235,0.0085,-0.0441,0.0138,-0.0024,-0.036,0.0342,0.0209,-0.0849,-0.0372,-0.0233,-0.0234,-0.0064,-0.0625,0.0001,-0.0144,-0.0209]}
{"key":"[AI-based Modeling and Data-driven Evaluation for Smart Manufacturing Processes] Smart Manufacturing refers to optimization techniques that are implemented in production operations by utilizing advanced analytics approaches. With the widespread increase in deploying Industrial Internet of Things (IIoT) sensors in manufacturing processes, there is a progressive need for optimal and effective approaches to data management. Embracing Machine Learning and Artificial Intelligence to take advantage of manufacturing data can lead to efficient and intelligent automation. In this paper, we conduct a comprehensive analysis based on Evolutionary Computing and Deep Learning algorithms toward making semiconductor manufacturing smart. We propose a dynamic algorithm for gaining useful insights about semiconductor manufacturing processes and to address various challenges. We elaborate on the utilization of a Genetic Algorithm and Neural Network to propose an intelligent feature selection algorithm. Our objective is to provide an advanced solution for controlling manufacturing processes and to gain perspective on various dimensions that enable manufacturers to access effective predictive technologies.","layer":3,"vector":[-0.0515,0.0456,0.0426,-0.0219,0.0252,0.0188,0.0666,0.0689,-0.003,-0.0535,-0.0168,-0.0322,0.0283,0.0223,0.0292,0.023,-0.0268,0.0274,-0.0406,-0.0043,0.0624,-0.0509,-0.0174,-0.0627,0.0064,-0.0013,-0.0123,-0.0092,-0.0784,-0.2437,0.0158,-0.0287,0.0615,-0.0295,0.023,-0.042,-0.0257,0.0507,-0.0112,0.0102,-0.0073,-0.0177,-0.0305,-0.0383,0.0047,-0.039,0.0067,-0.0016,-0.0176,-0.0285,0.0307,-0.021,0.02,0.02,0.0057,0.0491,0.0748,-0.0115,0.0558,-0.0028,0.0294,0.0406,-0.1706,0.0828,0.0442,0.0158,-0.0333,-0.0535,-0.0201,0.0245,-0.0092,0.0578,0.0256,0.0453,0.0066,0.0179,0.0243,-0.018,-0.0205,0.0416,0.0212,0.0188,-0.0667,0.016,-0.0407,-0.0015,-0.0005,-0.0185,0.0637,0.001,-0.0433,0.0256,-0.0534,-0.029,-0.0449,0.0021,0.0235,-0.008,-0.0673,0.1716,-0.0635,0.021,0.0211,-0.0299,-0.0145,-0.0221,-0.0509,-0.0157,-0.0533,0.0119,0.0073,-0.0042,0.005,-0.0135,0.0215,-0.0166,0.0628,0.0495,0.0402,0.0023,-0.0103,0.0089,0.0707,-0.0127,-0.0144,-0.0906,0.016,0.1376,-0.0169,-0.0076,0.0708,-0.0594,-0.0599,-0.0205,0.0443,0.0525,-0.0014,-0.015,-0.0178,-0.0328,-0.0953,-0.0397,0.0516,-0.1139,0.0129,0.1481,-0.0525,-0.0102,-0.0619,-0.0792,-0.0611,0.0282,-0.0225,-0.0151,0.0022,0.0567,-0.02,0.0516,-0.0559,0.0419,-0.0124,0.0124,-0.0255,0.105,-0.0212,-0.1093,-0.0257,0.0095,0.0137,0.0029,0.0195,0.0646,-0.0981,0.0371,0.0825,0.0493,-0.0288,-0.0329,-0.039,0.0578,0.0567,-0.0548,0.0012,-0.0228,0.0466,-0.0733,0.0368,-0.0182,-0.0208,0.0408,-0.0636,0.0426,-0.0039,0.0097,0.028,-0.0218,-0.0118,0.0042,0.032,-0.0476,0.0123,0.0358,-0.0187,-0.0162,-0.023,0.0068,-0.0069,0.0013,0.0399,0.0798,-0.0223,-0.0444,0.0562,-0.0421,-0.0777,-0.0248,-0.0036,0.0336,0.0245,0.0437,0.0316,0.026,-0.102,-0.2042,0.014,0.0064,0.0223,0.0321,-0.0567,-0.0031,-0.0228,0.0127,0.0419,0.0806,0.0194,0.0124,0.0058,-0.0285,0.0022,0.0485,0.0361,-0.0545,0.0321,-0.0393,0.0312,0.0137,-0.0965,0.0091,-0.0072,0.2075,-0.0381,0.0655,-0.0137,0.0129,0.0488,0.0119,-0.0862,0.072,0.0548,0.0491,0.0027,-0.0645,-0.019,-0.0439,0.0342,-0.0383,-0.0868,0.0043,-0.0294,-0.0216,0.0211,-0.0595,0.0171,0.0432,-0.0032,0.0632,0.0063,-0.0259,-0.0171,-0.0629,0.0799,-0.0425,-0.012,0.0158,-0.0392,-0.0142,-0.0166,0.0363,-0.0013,-0.0133,-0.0261,-0.019,-0.0244,-0.0207,0.1123,0.0387,-0.0178,0.0225,-0.0203,-0.0095,-0.0364,-0.0319,-0.0186,0.0917,-0.0343,0.0467,0.0412,0.0015,-0.0019,0.0941,-0.0064,0.0246,-0.0029,0.0029,-0.0043,-0.0616,0.0035,0.0489,-0.0061,-0.2671,0.0494,0.0055,0.0438,-0.0083,-0.0093,0.0146,0.0258,-0.0153,0.0259,0.0098,0.0163,0.0045,0.0091,0.0341,0.0368,0.0637,-0.0718,0.0572,-0.0753,0.0345,0.0649,0.2447,-0.0016,0.0476,0.0159,-0.0327,-0.0087,-0.0061,-0.0423,0.0066,0.0225,0.0838,-0.0469,0.0535,0.1077,-0.0397,-0.0073,-0.0211,0.006,0.0232,0.014,-0.0245,0.036,0.0875,-0.0174,-0.0766,-0.0663,-0.0139,0.0457,-0.0398,0.0091,-0.0486,0.0122,0.017,0.0164,-0.039,-0.0471,-0.0377,-0.0009,0.0349,-0.0864,-0.0061,-0.003,-0.0082]}
{"key":"[RNNPool: Efficient Non-linear Pooling for RAM Constrained Inference] Standard Convolutional Neural Networks (CNNs) designed for computer vision tasks tend to have large intermediate activation maps. These require large working memory and are thus unsuitable for deployment on resource-constrained devices typically used for inference on the edge. Aggressively downsampling the images via pooling or strided convolutions can address the problem but leads to a significant decrease in accuracy due to gross aggregation of the feature map by standard pooling operators. In this paper, we introduce RNNPool, a novel pooling operator based on Recurrent Neural Networks (RNNs), that efficiently aggregates features over large patches of an image and rapidly downsamples activation maps. Empirical evaluation indicates that an RNNPool layer can effectively replace multiple blocks in a variety of architectures such as MobileNets, DenseNet when applied to standard vision tasks like image classification and face detection. That is, RNNPool can significantly decrease computational complexity and peak memory usage for inference while retaining comparable accuracy. We use RNNPool with the standard S3FD architecture to construct a face detection method that achieves state-of-the-art MAP for tiny ARM Cortex-M4 class microcontrollers with under 256 KB of RAM. Code is released at https://github.com/Microsoft/EdgeML.","layer":0,"vector":[-0.0412,-0.0193,0.05,0.0066,0.0252,0.0477,0.0551,-0.0009,0.0246,-0.0526,0.0535,-0.089,0.0287,0.0199,0.0247,0.0027,0.035,0.0556,0.0071,0.0184,0.0185,-0.0517,0.0215,0.0173,-0.0236,-0.0289,0.0329,-0.0453,-0.1033,-0.2483,0.0207,-0.0389,0.0593,-0.0289,0.0376,-0.0308,-0.0231,0.0473,-0.0274,-0.0067,-0.0361,0.0445,-0.0511,-0.0561,-0.0405,-0.0377,-0.0334,-0.0102,0.0085,-0.0227,0.0453,-0.027,0.0399,0.0195,0.0297,0.0276,0.0588,0.0695,0.0584,0.0386,0.0293,0.0366,-0.1329,0.0366,0.0197,-0.009,-0.036,0.0141,0.0095,0.0074,-0.0075,0.0594,-0.01,0.0177,-0.0327,-0.0002,-0.0193,-0.0085,0.0387,-0.0032,0.005,0.0095,-0.0142,-0.0405,0.0271,-0.0534,-0.0389,-0.0412,0.0318,-0.0272,-0.0529,0.018,-0.0338,0.0168,-0.0365,-0.0097,0.0659,0.0327,-0.0619,0.219,-0.0111,0.0346,-0.0059,-0.0256,0.0332,0.0001,-0.039,0.0003,-0.0577,-0.0358,-0.0352,-0.0419,0.029,-0.0152,0.0486,-0.0134,0.0303,0.0172,-0.0202,0.0451,-0.0155,0.0033,0.0418,-0.0336,0.0132,-0.0529,0.0443,0.1602,0.0596,0.0261,0.0647,-0.016,-0.0362,-0.025,0.0551,0.0439,0.0336,-0.0029,0.0083,-0.0378,-0.0677,-0.0239,0.0529,-0.1044,-0.0435,0.0972,-0.0523,0.0426,-0.0261,-0.0001,0.016,0.0692,-0.0483,-0.0169,0.0269,0.0267,0.0245,0.0481,-0.065,0.0237,-0.0318,-0.0755,-0.001,0.0808,0.0186,-0.0901,0.0218,-0.0401,-0.0241,0.0029,0.0433,0.0353,-0.0075,-0.0057,0.0512,0.0411,-0.1027,-0.0073,-0.0165,0.0062,-0.0094,-0.046,-0.0471,0.0073,0.0317,-0.0114,0.0374,-0.042,0.0376,0.0315,-0.0915,0.0167,-0.0276,-0.0304,-0.0074,-0.0344,-0.0,-0.0131,0.0363,-0.0194,0.007,-0.0016,-0.0187,0.0206,0.0007,0.0178,-0.0403,0.0286,0.0427,0.046,-0.0375,-0.0306,0.0509,-0.016,-0.0442,-0.0231,0.0106,0.0242,0.0086,0.0177,0.0459,-0.065,-0.0693,-0.2458,0.0128,0.0267,-0.0312,0.0295,-0.0609,0.0123,0.0266,0.0796,0.0256,0.0359,-0.0172,-0.0056,0.0057,-0.0145,0.0663,0.0159,0.009,-0.0455,-0.0196,0.0225,0.0579,-0.0002,-0.0652,0.0913,0.0153,0.2216,0.0173,0.0269,-0.0123,0.0176,0.048,-0.0605,-0.1071,0.0627,0.0531,0.0652,0.0089,-0.0261,-0.0169,-0.0481,0.0188,0.006,-0.1057,-0.0565,0.0102,-0.0515,0.0238,-0.0107,-0.0239,0.0242,-0.0688,0.021,0.008,-0.0072,-0.0364,-0.0893,0.0252,-0.0703,0.0265,0.0198,-0.041,-0.0194,-0.0729,0.0841,0.0202,-0.0513,-0.0573,-0.0213,0.0073,-0.0164,0.0853,0.0469,-0.0171,0.0445,0.0158,0.0639,-0.0539,-0.0322,-0.0604,0.0248,-0.0268,0.0193,0.0549,0.0185,0.0082,0.0771,-0.0126,0.0116,-0.0249,0.0455,0.0181,-0.0534,-0.0393,0.0417,0.0121,-0.2979,0.029,-0.0183,0.0595,-0.0205,0.0316,0.0705,0.0288,-0.0185,-0.009,-0.0277,0.0417,0.0773,0.0215,0.0025,0.0305,0.0666,-0.0567,0.0434,-0.0373,-0.0025,-0.006,0.2136,-0.0322,0.0011,0.007,-0.011,-0.0168,0.0656,-0.0159,0.028,-0.0145,0.0672,-0.0494,-0.0079,0.0868,-0.0834,0.0285,0.0348,-0.0087,0.0299,0.0127,-0.033,-0.0049,0.0415,-0.0277,-0.0285,-0.0354,0.0236,0.0359,-0.012,-0.0184,0.0001,0.0073,0.0496,0.0389,-0.0548,-0.0484,-0.0886,0.0046,0.0326,-0.0654,-0.0181,-0.0131,-0.0163]}
{"key":"[A new smart-cropping pipeline for prostate segmentation using deep learning networks] Prostate segmentation from magnetic resonance imaging (MRI) is a challenging task. In recent years, several network architectures have been proposed to automate this process and alleviate the burden of manual annotation. Although the performance of these models has achieved promising results, there is still room for improvement before these models can be used safely and effectively in clinical practice. One of the major challenges in prostate MR image segmentation is the presence of class imbalance in the image labels where the background pixels dominate over the prostate. In the present work we propose a DL-based pipeline for cropping the region around the prostate from MRI images to produce a more balanced distribution of the foreground pixels (prostate) and the background pixels and improve segmentation accuracy. The effect of DL-cropping for improving the segmentation performance compared to standard center-cropping is assessed using five popular DL networks for prostate segmentation, namely U-net, U-net+, Res Unet++, Bridge U-net and Dense U-net. The proposed smart-cropping outperformed the standard center cropping in terms of segmentation accuracy for all the evaluated prostate segmentation networks. In terms of Dice score, the highest improvement was achieved for the U-net+ and ResU-net++ architectures corresponding to 8.9% and 8%, respectively.","layer":0,"vector":[-0.0636,-0.0347,0.028,-0.0211,0.064,0.0103,0.0168,0.0183,0.0072,0.0036,-0.0246,-0.0887,0.0267,0.0507,0.0002,0.0116,0.001,0.0444,-0.0102,0.03,0.0164,-0.0421,-0.0246,-0.0318,-0.0113,-0.0069,-0.0126,-0.0788,-0.0791,-0.2558,0.0657,-0.0075,0.0411,-0.0013,0.0029,-0.0768,-0.0204,0.0566,-0.0361,0.0049,0.0456,0.0377,-0.0355,-0.0316,-0.0139,-0.043,-0.0714,-0.0558,0.018,-0.0203,0.0587,-0.0442,0.0273,0.069,-0.0205,-0.0088,0.0414,0.0477,0.0389,0.0682,0.0446,0.0747,-0.193,0.0513,0.0695,-0.0253,-0.0509,-0.0491,0.0069,0.0265,0.0019,0.0377,0.0473,0.0286,0.0385,-0.0434,0.0284,-0.0441,-0.0159,0.0335,0.0111,0.0526,-0.0403,0.0139,-0.0445,-0.0665,0.0219,-0.0738,0.0122,0.0052,-0.0167,-0.0362,-0.0268,0.0299,-0.056,-0.0375,0.0136,-0.0218,-0.0656,0.2013,-0.0699,0.0176,0.063,-0.0296,0.0355,0.0375,-0.0215,0.0112,-0.0688,0.0298,0.0211,0.0055,0.028,-0.0209,0.0202,-0.0208,0.0563,0.0163,-0.0015,-0.0151,-0.0553,0.0033,0.0215,-0.0443,0.0442,-0.0281,0.0622,0.1166,0.045,0.0382,0.0435,-0.0178,-0.0302,-0.0373,0.0132,0.0136,0.021,-0.0185,0.0046,0.0057,-0.0096,-0.0097,0.0338,-0.0949,-0.0215,0.1061,-0.0838,0.0497,-0.0173,-0.0561,0.0175,0.0081,-0.0494,-0.0213,0.0306,0.0198,0.0137,0.0373,-0.0471,0.0272,-0.0022,-0.0596,-0.0566,0.1273,0.0387,-0.0876,-0.0584,-0.0092,0.0012,-0.0271,0.0339,-0.0053,-0.0103,0.0432,0.0721,-0.0048,-0.045,-0.0193,-0.0072,0.0053,0.0832,-0.0413,-0.0215,0.0361,0.0453,-0.0513,-0.0253,-0.013,0.0505,0.0178,-0.035,0.0281,-0.0425,0.0019,-0.0098,-0.0102,-0.0343,0.0044,-0.0036,0.0114,0.0038,-0.037,0.0184,-0.0058,0.0165,0.0278,-0.0141,0.017,-0.0079,0.0839,-0.0461,0.014,0.0442,-0.016,-0.0349,-0.0055,0.048,0.0187,-0.0173,0.0738,0.0855,0.0016,-0.0561,-0.1992,0.0108,0.0256,-0.0486,0.0653,-0.0707,0.0143,0.01,0.0586,0.0661,0.0744,-0.0063,0.0097,0.0221,0.003,0.0582,0.0405,0.0198,-0.0135,-0.0326,-0.0205,0.0548,-0.0258,-0.0985,0.0913,0.0121,0.2225,0.0014,0.0449,0.0293,-0.022,0.0205,-0.0337,-0.0835,0.0745,0.0151,0.0391,-0.0379,-0.0681,-0.0482,-0.0743,0.0027,0.0055,-0.1046,-0.039,-0.0464,-0.0433,0.0718,-0.0037,0.0321,-0.0088,-0.0762,0.0335,0.0341,-0.0067,-0.0218,-0.0868,0.0147,-0.0388,-0.0007,0.0333,-0.1014,0.0419,-0.0442,0.0484,-0.0092,-0.0298,-0.0176,-0.0159,-0.0212,0.0084,0.0513,0.0352,0.0209,0.0476,0.007,0.0564,-0.0011,-0.0347,-0.0567,0.0408,-0.0251,0.0298,0.0184,0.0244,-0.0095,0.0611,-0.0062,-0.0333,-0.0114,0.0039,0.0263,-0.061,0.0102,0.0063,-0.0369,-0.2703,0.0706,0.035,0.0367,-0.0012,0.0399,0.0458,0.0425,-0.0315,0.0032,-0.0104,0.0247,0.0362,-0.061,-0.0167,0.0434,0.0563,-0.0478,0.0474,-0.0351,0.0481,0.0241,0.1691,-0.0763,0.0227,0.0343,-0.0204,-0.0133,0.0294,0.0253,-0.0162,0.0535,0.0921,-0.0577,0.0336,0.1154,-0.0339,0.0019,-0.0171,-0.0276,-0.0145,-0.0128,-0.0396,0.0066,0.0343,-0.0555,-0.0138,-0.0198,0.0489,0.0001,0.0116,0.0006,-0.0369,0.0123,0.0387,0.047,-0.0254,-0.0921,-0.1028,0.0072,0.0369,-0.0685,-0.021,0.0137,-0.0035]}
{"key":"[Spacecraft Collision Risk Assessment with Probabilistic Programming] Over 34,000 objects bigger than 10 cm in length are known to orbit Earth. Among them, only a small percentage are active satellites, while the rest of the population is made of dead satellites, rocket bodies, and debris that pose a collision threat to operational spacecraft. Furthermore, the predicted growth of the space sector and the planned launch of megaconstellations will add even more complexity, therefore causing the collision risk and the burden on space operators to increase. Managing this complex framework with internationally agreed methods is pivotal and urgent. In this context, we build a novel physics-based probabilistic generative model for synthetically generating conjunction data messages, calibrated using real data. By conditioning on observations, we use the model to obtain posterior distributions via Bayesian inference. We show that the probabilistic programming approach to conjunction assessment can help in making predictions and in finding the parameters that explain the observed data in conjunction data messages, thus shedding more light on key variables and orbital characteristics that more likely lead to conjunction events. Moreover, our technique enables the generation of physically accurate synthetic datasets of collisions, answering a fundamental need of the space and machine learning communities working in this area.","layer":4,"vector":[-0.0456,-0.0016,0.0263,-0.0733,0.0058,0.0407,0.0326,0.0286,0.0395,-0.027,0.0096,-0.0504,0.0449,0.0642,0.0124,-0.0066,0.0274,0.0394,-0.023,-0.0063,0.0392,-0.0318,-0.0301,-0.0492,0.0194,0.0732,-0.0413,-0.0272,-0.0442,-0.2452,0.0197,-0.0477,0.0416,-0.0367,0.0099,-0.0014,-0.0369,0.0324,-0.0064,0.0566,0.0445,-0.0048,-0.0259,-0.0457,0.0125,-0.0319,0.011,-0.028,-0.0367,0.0201,0.0032,-0.089,0.0253,0.0376,0.0403,0.0145,0.045,0.0105,0.0264,0.0751,-0.0135,0.0249,-0.1663,0.064,0.0482,0.0134,-0.0253,0.0195,0.0423,0.0375,-0.032,0.0738,0.0091,0.083,0.0017,-0.0081,-0.008,-0.0531,0.0081,0.011,-0.0109,-0.0124,-0.1054,-0.009,-0.0404,-0.0626,0.0229,0.0098,0.0607,0.0131,-0.0617,-0.0209,-0.0207,0.0283,-0.0671,0.0152,0.0348,-0.0227,-0.0025,0.2041,-0.0756,0.0416,0.0113,0.0125,0.0293,-0.0465,-0.0274,-0.0433,-0.0118,-0.0628,0.0129,-0.0134,0.0029,-0.0351,-0.0268,0.043,0.0677,0.0186,-0.0537,-0.0555,-0.0497,-0.0103,0.0512,0.0295,0.0255,-0.0868,0.0344,0.1632,0.038,0.0348,0.0446,-0.0362,-0.0634,-0.012,0.0167,0.007,0.0274,-0.0043,0.0164,0.015,-0.0187,-0.0541,0.0142,-0.0721,-0.0753,0.1022,-0.0273,0.0439,-0.057,-0.0278,0.0072,0.0318,0.0302,-0.0363,0.043,0.0344,-0.0129,0.0318,-0.0515,0.0109,0.019,0.0051,-0.0138,0.1273,0.0064,-0.0618,-0.0368,0.0326,0.0496,-0.0026,0.001,0.0561,-0.0091,0.0034,0.0779,0.0364,-0.0288,0.0135,0.0495,0.0297,0.0265,-0.0591,-0.0053,0.0463,0.0161,-0.0855,-0.0335,-0.0153,0.0047,0.0186,-0.0007,-0.0151,0.0199,-0.0118,-0.0248,-0.0492,-0.005,-0.0453,0.0154,-0.0294,0.0391,0.021,-0.0859,0.0338,-0.0301,0.0025,0.0105,0.0102,0.0437,0.0342,-0.0188,-0.0491,0.0572,-0.0076,-0.0213,0.0036,-0.0059,0.0348,0.0071,0.0207,0.025,-0.0075,-0.0164,-0.2552,0.0206,0.0157,-0.0166,0.0322,-0.0439,0.0364,-0.0074,0.0102,0.0299,0.0862,-0.0571,-0.0164,0.0085,-0.0042,0.0325,-0.0566,0.025,-0.0228,-0.0177,-0.0531,0.0118,-0.0597,-0.0749,0.028,-0.0121,0.2092,0.0676,0.0131,-0.0578,0.0661,0.041,0.0058,-0.0622,0.09,0.0393,0.0592,0.0181,-0.0586,-0.0131,-0.0518,-0.0017,0.0031,-0.0766,0.0099,-0.0893,-0.0559,0.0402,-0.0296,0.0125,0.0391,-0.017,0.0326,-0.0106,-0.0021,-0.0332,-0.1005,0.0319,0.0013,0.0134,0.0054,-0.0083,0.0094,-0.0534,0.0648,-0.0115,-0.0127,-0.0655,0.0131,-0.05,-0.0178,0.1364,-0.0343,-0.0337,0.0493,-0.0109,0.0255,0.0043,-0.0371,-0.0169,0.0841,-0.0035,0.0291,0.0377,0.0171,-0.0147,0.0663,-0.0113,0.0298,-0.0394,0.0161,0.0141,-0.0445,0.0345,0.0321,0.0131,-0.2957,0.0465,-0.025,0.009,-0.0573,-0.0114,0.0707,0.0235,-0.0486,-0.0478,0.0027,0.0721,0.04,-0.005,-0.0149,0.0248,0.0426,-0.0461,0.0013,-0.0708,0.0231,-0.0113,0.2222,-0.0039,0.0587,0.0285,0.0025,0.0328,0.0331,-0.0322,-0.0265,0.0008,0.0308,-0.0581,0.0355,0.1034,0.0052,0.013,0.0042,-0.0428,-0.0064,0.0223,0.0236,-0.0129,0.1296,0.0034,-0.0405,-0.0631,0.0068,-0.0094,-0.0373,0.0027,-0.0311,-0.023,0.0099,0.0337,-0.0083,-0.0123,-0.025,-0.0236,0.0038,-0.0341,0.0186,-0.0453,-0.0369]}
{"key":"[The pitfalls of using open data to develop deep learning solutions for COVID-19 detection in chest X-rays] Since the emergence of COVID-19, deep learning models have been developed to identify COVID-19 from chest X-rays. With little to no direct access to hospital data, the AI community relies heavily on public data comprising numerous data sources. Model performance results have been exceptional when training and testing on open-source data, surpassing the reported capabilities of AI in pneumonia-detection prior to the COVID-19 outbreak. In this study impactful models are trained on a widely used open-source data and tested on an external test set and a hospital dataset, for the task of classifying chest X-rays into one of three classes: COVID-19, non-COVID pneumonia and no-pneumonia. Classification performance of the models investigated is evaluated through ROC curves, confusion matrices and standard classification metrics. Explainability modules are implemented to explore the image features most important to classification. Data analysis and model evaluations show that the popular open-source dataset COVIDx is not representative of the real clinical problem and that results from testing on this are inflated. Dependence on open-source data can leave models vulnerable to bias and confounding variables, requiring careful analysis to develop clinically useful/viable AI tools for COVID-19 detection in chest X-rays.","layer":1,"vector":[-0.0104,-0.003,0.039,0.0035,0.0632,-0.0015,0.0166,0.0042,0.0028,0.0043,0.0181,-0.0454,-0.0147,0.0727,-0.0027,-0.0161,-0.0249,-0.009,-0.0263,0.0256,-0.0175,-0.002,-0.0103,-0.0435,-0.0026,0.0497,-0.0129,-0.0596,-0.0759,-0.2185,-0.0027,-0.0342,0.064,0.0039,0.0297,-0.0324,-0.0174,0.0048,-0.0387,0.0017,-0.0007,0.0115,0.0096,-0.044,0.0042,-0.0637,-0.0265,0.005,-0.0052,-0.0074,0.0257,-0.0113,0.012,0.0573,0.0108,0.0163,0.0338,0.0393,0.0433,0.0287,0.0348,0.0507,-0.1623,0.0474,0.0601,0.0352,-0.0215,-0.0175,0.0485,0.0218,-0.0035,0.0135,0.0231,0.0279,0.023,0.0311,-0.0337,-0.0224,0.0195,-0.0022,-0.0065,0.0042,-0.0183,-0.0291,-0.0338,-0.0269,-0.008,-0.0738,0.0187,-0.0329,-0.015,0.0368,-0.0099,0.028,-0.0662,-0.0104,0.0776,0.0711,-0.0665,0.1698,-0.0612,-0.0241,0.0413,-0.0206,0.036,-0.034,-0.0502,-0.06,-0.0145,-0.0209,-0.0157,-0.0452,0.0464,-0.0425,0.0078,0.0067,0.0598,-0.0035,-0.0091,0.0127,-0.0062,0.0055,0.0511,0.0138,0.0028,-0.0127,0.0277,0.1177,-0.0103,0.0086,0.0556,-0.0768,-0.0639,0.0078,-0.0106,0.0176,0.0618,0.0279,-0.0178,0.0788,-0.0336,-0.0414,0.0386,-0.0834,-0.048,0.0968,-0.0687,0.0584,-0.0207,-0.0792,-0.0172,0.0206,-0.078,-0.0168,0.0432,-0.0203,0.0214,0.0284,-0.0566,0.0434,-0.0014,-0.0976,-0.0352,0.1299,-0.0184,-0.0436,-0.0332,0.0147,0.0379,-0.01,0.0903,0.0712,-0.041,0.0148,0.0737,0.0155,-0.0532,-0.0209,-0.0167,0.0006,0.0222,-0.0245,-0.0043,0.0523,0.0296,-0.0531,0.0022,-0.0536,0.0912,0.0423,-0.0275,0.0103,-0.0534,0.0135,0.0021,-0.0297,-0.024,0.0058,-0.0271,0.0124,0.0067,-0.0162,0.0387,0.0423,-0.0357,-0.0423,-0.0076,-0.009,0.0176,0.0066,-0.0552,0.0132,0.024,0.0056,-0.0663,-0.0118,0.0331,-0.0135,-0.0291,0.0822,0.0254,0.0013,-0.022,-0.2159,0.0029,0.0209,0.0245,0.0083,-0.1168,0.0574,0.0106,0.0213,0.0842,0.0523,0.0052,-0.0113,-0.0322,-0.0335,0.0287,0.0117,0.0356,-0.079,-0.0176,0.0033,0.0294,-0.0202,-0.1075,0.015,0.0071,0.2426,0.0056,0.0292,0.0096,-0.0049,-0.0103,-0.03,-0.1521,0.0585,0.0343,0.0854,0.0408,-0.0454,0.0275,-0.015,0.0072,0.0143,-0.0945,-0.0049,-0.0224,-0.0361,0.0669,-0.0869,0.0287,0.019,-0.0161,0.03,0.0066,-0.0043,-0.0381,-0.0902,0.0461,-0.0134,0.007,-0.0431,-0.0391,-0.0081,-0.1034,0.0356,-0.0384,-0.047,-0.054,0.0091,-0.0567,-0.0391,0.1254,-0.0283,-0.0153,0.0762,-0.015,0.0327,-0.0171,-0.0249,-0.0054,0.0622,0.0094,0.0213,0.0425,0.03,0.0257,0.0681,0.0169,0.0039,-0.0234,0.0199,0.0264,-0.0425,-0.0357,0.0237,0.0143,-0.3388,0.0501,0.0107,0.024,-0.0179,-0.0096,0.0355,-0.0022,-0.0104,0.0049,-0.0026,0.0245,0.0208,-0.0496,-0.0224,0.0433,0.0807,-0.0516,0.0749,-0.0012,-0.0236,0.0495,0.1997,-0.0372,0.0112,0.0107,-0.0224,-0.0244,0.033,0.0025,0.09,-0.0154,0.0755,-0.0559,0.0471,0.0985,0.0094,0.0715,0.006,-0.0278,0.0105,0.0295,-0.0338,-0.0142,0.0646,-0.008,-0.0243,-0.0345,-0.005,0.0229,0.0092,-0.0083,-0.0309,0.0126,0.0156,0.0218,-0.0568,-0.0635,-0.0142,-0.04,0.0191,-0.0503,0.0203,0.0626,0.0285]}
{"key":"[Discovering a set of policies for the worst case reward] We study the problem of how to construct a set of policies that can be composed together to solve a collection of reinforcement learning tasks. Each task is a different reward function defined as a linear combination of known features. We consider a specific class of policy compositions which we call set improving policies (SIPs): given a set of policies and a set of tasks, a SIP is any composition of the former whose performance is at least as good as that of its constituents across all the tasks. We focus on the most conservative instantiation of SIPs, set-max policies (SMPs), so our analysis extends to any SIP. This includes known policy-composition operators like generalized policy improvement. Our main contribution is a policy iteration algorithm that builds a set of policies in order to maximize the worst-case performance of the resulting SMP on the set of tasks. The algorithm works by successively adding new policies to the set. We show that the worst-case performance of the resulting SMP strictly improves at each iteration, and the algorithm only stops when there does not exist a policy that leads to improved performance. We empirically evaluate our algorithm on a grid world and also on a set of domains from the DeepMind control suite. We confirm our theoretical results regarding the monotonically improving performance of our algorithm. Interestingly, we also show empirically that the sets of policies computed by the algorithm are diverse, leading to different trajectories in the grid world and very distinct locomotion skills in the control suite.","layer":0,"vector":[-0.079,-0.0137,0.0591,-0.0324,-0.0067,0.0618,0.0327,0.0227,0.0531,0.0035,0.0358,-0.0163,0.0362,0.0689,-0.013,0.0252,-0.0104,0.0498,-0.0312,0.0004,0.0326,-0.071,-0.032,-0.0842,-0.0088,0.0173,-0.0737,-0.0551,-0.0363,-0.2372,0.034,-0.038,0.0026,-0.0381,-0.0009,0.0284,-0.0419,0.0621,-0.065,0.016,0.048,0.0369,-0.0247,-0.0851,-0.0151,-0.0518,0.0077,-0.0277,-0.0206,-0.0521,0.0076,-0.0399,0.019,0.0441,0.0691,0.0237,0.0476,0.0373,0.0026,0.0256,-0.0116,-0.0042,-0.1635,0.0383,0.0085,0.0613,-0.0566,0.0116,0.0152,0.0884,-0.0331,0.0299,0.0379,0.0482,0.0145,0.0209,-0.0029,-0.0179,-0.0206,0.0441,0.0082,-0.0429,-0.0057,-0.0078,0.0394,-0.0812,0.0028,-0.0513,0.0547,0.0437,-0.0322,0.0109,-0.0132,0.0206,-0.0367,0.021,0.0006,0.0176,-0.0661,0.1943,-0.0104,0.0426,0.0271,-0.0035,0.0419,-0.075,0.0097,-0.029,-0.0129,-0.043,-0.0676,-0.0118,0.0258,-0.0138,-0.0294,0.0094,0.0591,0.0362,-0.025,0.0001,0.0061,0.0003,0.0672,-0.0062,0.0109,-0.0643,0.0087,0.1595,-0.019,0.0314,0.0203,-0.0417,-0.0115,-0.0343,0.0221,0.0237,0.0178,0.0109,0.0317,0.0021,-0.0208,0.0253,0.0183,-0.1317,-0.0446,0.1369,-0.0048,0.0191,-0.0675,-0.0125,-0.0067,0.0068,-0.0085,-0.0494,0.0048,0.0177,0.0371,0.0266,-0.0446,0.0006,-0.0157,-0.0222,-0.0244,0.0906,-0.0175,-0.0677,-0.0363,-0.0426,-0.017,-0.0075,0.0242,0.0385,-0.0391,0.0014,0.0552,-0.0098,-0.0988,0.0342,-0.01,0.0108,0.063,-0.0576,-0.0749,0.0202,0.0659,0.0174,0.0264,-0.0468,0.0142,0.0333,-0.0399,0.0294,0.0193,-0.0294,-0.0219,-0.0606,0.0129,-0.0042,0.0047,-0.0083,-0.0237,0.0044,-0.0282,0.0143,-0.0068,0.0186,-0.0602,-0.0146,0.0639,0.0464,-0.017,0.0163,0.0652,0.0033,-0.026,-0.002,-0.0169,0.0031,-0.0347,0.0104,0.0703,-0.0152,-0.037,-0.2114,-0.0391,-0.0258,-0.0234,0.0561,-0.0505,0.0231,-0.019,-0.0189,0.0511,0.0412,-0.0627,-0.0053,0.0391,-0.0033,0.0499,0.0458,0.0296,-0.0386,0.0273,-0.0164,0.0241,-0.0007,-0.1041,0.0298,0.0049,0.2231,0.0549,0.0612,-0.0139,0.0179,0.05,-0.017,-0.1106,0.0599,0.0236,0.0856,-0.0035,0.0127,-0.0476,-0.0099,0.0308,-0.0386,-0.0931,-0.0287,-0.0257,-0.0482,0.0442,-0.0577,-0.0122,0.0344,-0.0168,-0.0224,-0.0347,-0.0244,-0.0247,-0.0976,0.0219,-0.0353,0.0487,-0.002,-0.0364,0.0107,-0.0346,0.0716,-0.0153,0.048,-0.0496,0.0139,-0.0,-0.0166,0.0293,0.0143,0.0044,0.0194,0.0161,0.0045,0.0078,-0.0707,-0.0367,0.0683,-0.0377,0.0129,0.0176,-0.0158,0.0029,0.0858,-0.0191,0.0116,-0.0155,-0.0189,0.0142,-0.0405,0.0057,0.0626,0.0369,-0.3112,0.0775,0.0286,0.0003,-0.0476,0.0021,0.0409,0.027,-0.0317,0.0046,0.0101,0.0974,0.0045,0.0253,0.0011,0.0482,0.0835,-0.0052,0.063,-0.0587,0.0203,0.0874,0.2325,-0.017,0.0242,0.004,-0.0188,-0.0229,0.0054,0.0021,-0.0045,0.028,0.0415,-0.0184,0.0676,0.0997,-0.0254,0.0218,0.0047,0.0479,-0.0618,0.0279,-0.0132,0.0203,0.1061,-0.021,-0.0612,-0.0469,-0.0188,0.0461,-0.0663,-0.0066,-0.0169,-0.0111,0.0246,0.0173,-0.036,-0.0544,-0.0475,-0.0276,0.0033,-0.0438,0.0263,-0.0008,-0.0026]}
{"key":"[Reliable Evaluation of Neural Network for Multiclass Classification of Real-world Data] This paper presents a systematic evaluation of Neural Network (NN) for classification of real-world data. In the field of machine learning, it is often seen that a single parameter that is 'predictive accuracy' is being used for evaluating the performance of a classifier model. However, this parameter might not be considered reliable given a dataset with very high level of skewness. To demonstrate such behavior, seven different types of datasets have been used to evaluate a Multilayer Perceptron (MLP) using twelve(12) different parameters which include micro- and macro-level estimation. In the present study, the most common problem of prediction called 'multiclass' classification has been considered. The results that are obtained for different parameters for each of the dataset could demonstrate interesting findings to support the usability of these set of performance evaluation parameters.","layer":11,"vector":[-0.0136,-0.0258,0.0242,-0.0324,0.0582,-0.0006,0.0221,0.052,0.0156,-0.0408,0.0031,-0.0484,-0.0021,0.055,0.0509,-0.0196,0.0326,0.0287,-0.0148,-0.0056,0.0238,0.0128,-0.0146,-0.0391,0.0408,0.0398,-0.0222,-0.0471,-0.0603,-0.2316,0.005,-0.0593,0.0847,0.0027,-0.0202,-0.0363,-0.0077,0.0598,-0.03,0.0587,0.0206,-0.0041,-0.0268,-0.0898,0.0087,-0.0182,-0.0362,-0.025,-0.0204,-0.0509,0.0267,-0.0493,0.0145,0.0167,0.0147,0.026,0.0169,0.0418,0.0564,0.0333,0.0125,0.0373,-0.1807,0.03,0.0163,0.0202,-0.0381,-0.0671,0.0024,0.0183,-0.0304,0.0103,0.0484,0.0745,-0.0036,-0.0221,0.0286,-0.0181,-0.02,0.0312,0.0227,-0.0308,-0.0266,-0.0362,-0.0045,0.0002,-0.0146,-0.0044,0.0205,-0.0135,-0.0335,0.0226,-0.0136,0.012,-0.0413,0.004,0.0249,0.0197,-0.0842,0.1944,-0.0601,-0.0088,0.0283,-0.0483,0.0424,-0.0357,-0.0253,-0.0424,-0.062,-0.0373,-0.0006,-0.0163,-0.0208,-0.0096,0.0049,0.022,0.0309,0.0084,-0.0001,-0.0189,0.0025,0.0029,0.0509,0.001,0.0401,-0.0666,0.0692,0.1417,0.0299,0.0216,0.0319,-0.0228,-0.0419,-0.0195,0.036,0.0106,0.0288,0.0135,-0.0023,-0.0117,-0.0392,-0.0769,0.0202,-0.0452,-0.0867,0.1187,-0.059,0.0547,-0.012,0.0018,-0.0588,0.0437,-0.0578,-0.0321,0.0393,0.0344,0.0154,0.0493,-0.0495,-0.0076,-0.0051,-0.0466,-0.0429,0.1073,0.0103,-0.0502,-0.0127,-0.0418,-0.0063,-0.0749,0.0403,0.0516,-0.0469,0.0204,0.0384,0.0501,-0.0838,-0.0248,-0.0117,-0.0137,0.0351,0.0013,-0.0493,0.0483,0.0542,-0.0055,-0.0158,-0.0368,0.0551,0.0589,-0.049,0.0065,-0.0456,-0.0163,-0.0198,-0.0363,0.0322,-0.0341,0.0479,-0.0397,0.0322,0.0208,0.004,-0.0174,-0.0012,0.0797,-0.0118,0.0208,0.0671,0.0445,-0.0416,-0.0042,0.059,-0.0485,-0.0505,-0.0189,0.0374,0.0622,0.0153,0.0741,0.0438,-0.0527,-0.0514,-0.2216,0.0162,0.0688,-0.012,0.0628,-0.0833,0.0569,0.0023,0.0073,0.0334,0.0933,0.0519,0.0064,0.0201,0.0095,0.0668,0.0299,0.0107,-0.0601,-0.0146,-0.0127,0.0493,0.0234,-0.1069,0.0607,-0.0039,0.1542,-0.0383,0.0064,-0.0347,0.0469,0.0244,-0.0027,-0.0956,0.0961,-0.0199,0.0558,-0.0347,-0.0574,-0.0471,-0.036,0.0284,0.0468,-0.108,-0.0222,-0.0607,-0.009,0.0306,-0.0747,-0.0043,0.006,-0.0273,0.0426,0.0134,-0.0058,-0.0194,-0.0903,0.0591,-0.0166,0.0354,0.0521,-0.0702,0.0264,-0.0578,0.0161,-0.0038,-0.0619,-0.0407,0.0173,-0.0582,-0.0427,0.1346,0.0466,-0.036,0.0403,-0.0445,0.0232,-0.0029,0.0153,-0.0009,0.0312,-0.0128,0.0414,0.0411,0.0366,-0.0041,0.0901,-0.0132,0.0456,-0.0064,0.0115,0.0119,-0.038,0.0329,0.0284,0.011,-0.2762,0.0439,0.0185,0.0665,-0.0014,-0.0097,0.0078,-0.0047,-0.0633,0.008,0.0128,0.0049,0.0671,-0.0175,0.0247,0.0106,0.0156,-0.0324,0.0665,-0.0163,0.0587,0.045,0.2266,-0.0466,0.0337,0.0148,-0.0206,-0.0144,-0.0263,-0.0242,0.0506,-0.0255,0.0679,-0.0613,0.0185,0.1004,-0.039,0.0188,0.0285,-0.0246,0.0619,0.0122,-0.0816,-0.0588,0.1055,0.0323,0.0012,-0.0759,-0.0379,0.0104,-0.0514,0.0158,-0.0293,-0.029,0.0188,0.0285,-0.0619,-0.0341,-0.074,-0.0235,0.0531,-0.0615,0.02,-0.0155,-0.0253]}
{"key":"[Differential Bayesian Neural Nets] Neural Ordinary Differential Equations (N-ODEs) are a powerful building block for learning systems, which extend residual networks to a continuous-time dynamical system. We propose a Bayesian version of N-ODEs that enables well-calibrated quantification of prediction uncertainty, while maintaining the expressive power of their deterministic counterpart. We assign Bayesian Neural Nets (BNNs) to both the drift and the diffusion terms of a Stochastic Differential Equation (SDE) that models the flow of the activation map in time. We infer the posterior on the BNN weights using a straightforward adaptation of Stochastic Gradient Langevin Dynamics (SGLD). We illustrate significantly improved stability on two synthetic time series prediction tasks and report better model fit on UCI regression benchmarks with our method when compared to its non-Bayesian counterpart.","layer":0,"vector":[-0.0847,-0.0216,-0.0026,-0.0352,0.0368,0.0565,0.0413,-0.0247,0.0567,-0.0033,-0.0025,-0.0327,0.0321,0.0421,0.0313,0.0019,-0.0385,0.0281,-0.043,-0.0093,0.0085,0.0044,0.0137,-0.0216,0.0386,0.031,-0.0067,0.0153,-0.0625,-0.2505,0.0021,-0.0971,-0.0139,-0.0243,0.0145,-0.0148,-0.0309,0.0542,0.0084,0.0581,-0.024,0.0446,0.0002,-0.0502,0.0206,-0.0477,-0.0138,0.002,-0.0111,-0.0489,-0.0104,-0.0191,0.041,0.0483,-0.0086,0.0348,0.0738,0.0213,0.0682,0.042,0.0266,0.0192,-0.1995,0.0686,0.0601,0.012,-0.0004,-0.0266,-0.0019,0.0537,-0.0222,0.0381,0.0189,0.0639,0.0468,0.0194,0.0161,-0.0556,-0.0031,0.0557,0.0413,-0.0095,-0.0057,-0.0561,-0.009,-0.0234,0.0433,-0.0444,0.0421,-0.0105,-0.0503,-0.0163,-0.0517,-0.0173,-0.0641,0.0366,0.049,0.0303,0.01,0.1762,-0.0267,0.0291,0.0037,0.0239,0.0226,-0.0169,-0.058,-0.0101,-0.0293,0.0141,-0.0235,-0.0255,0.0143,-0.0572,0.0049,0.003,0.0282,0.0054,-0.026,0.0054,-0.0287,0.0173,0.0618,-0.0494,-0.0029,-0.0578,0.0123,0.1511,0.046,0.0251,0.0574,-0.0064,-0.0485,-0.015,0.0355,0.016,0.0185,-0.0447,-0.0296,-0.0198,-0.0711,-0.0377,-0.0084,-0.0885,-0.091,0.1032,0.0013,0.0431,-0.0319,-0.0266,-0.0047,0.0578,-0.0234,-0.0217,0.022,0.0371,0.0074,0.0331,-0.1052,0.0088,-0.0253,-0.0278,-0.0407,0.1243,-0.003,-0.0392,-0.0204,0.0122,0.0176,-0.0096,0.0501,0.0109,-0.0177,0.0068,0.0791,0.0471,-0.0272,-0.0038,-0.0053,0.0209,-0.0091,-0.0231,-0.0141,0.0663,0.0357,-0.0385,0.0045,-0.0435,0.0036,0.0711,0.0039,-0.0181,0.0045,0.0199,-0.046,-0.02,-0.0584,-0.0152,0.0416,-0.0445,-0.0057,-0.0284,-0.063,0.0343,-0.0076,0.0109,-0.0083,0.0279,0.0147,0.0325,-0.015,0.0161,0.0648,-0.0663,-0.0185,0.0274,-0.0046,0.0212,-0.001,0.0167,0.0394,-0.0263,-0.0346,-0.2075,0.0179,0.0329,-0.0024,0.0976,-0.0826,0.0066,-0.0277,0.0614,0.0969,0.0321,0.0125,0.0024,-0.0195,0.0203,0.0561,0.018,0.0223,0.0064,-0.0015,-0.0424,-0.0199,-0.0183,-0.0999,0.0423,0.0027,0.1956,0.0074,0.1055,-0.044,0.029,0.0435,0.0042,-0.0517,0.056,0.0308,0.0871,-0.0161,-0.0632,-0.0547,-0.0466,0.028,-0.0234,-0.1207,-0.0645,-0.0227,-0.0092,0.0093,-0.0853,-0.0059,0.006,-0.0326,0.0883,-0.0239,-0.0126,-0.0439,-0.0632,0.0422,-0.0401,0.0125,-0.0033,-0.0536,0.0219,-0.0377,0.0182,-0.0073,-0.0113,-0.0634,0.0226,-0.0448,-0.0056,0.0885,0.0047,0.0031,0.0501,-0.0055,-0.0001,-0.0101,-0.0617,-0.012,0.0381,-0.0489,0.0822,0.0765,0.009,-0.0125,0.0599,-0.0292,0.0225,-0.0113,-0.0091,0.0069,-0.0552,0.0014,0.0399,0.0061,-0.2929,0.0452,0.0362,0.0037,-0.0127,0.0064,0.0776,0.0061,-0.0771,-0.0111,-0.0345,0.0651,0.0744,0.0195,-0.0083,0.0108,0.0623,-0.0737,0.0526,-0.0551,0.0307,0.0655,0.2262,-0.0369,0.0613,0.001,-0.0056,0.0194,0.052,-0.0565,0.0216,0.0262,0.0343,-0.0636,0.0503,0.0898,-0.0817,0.0274,0.0394,-0.0106,0.0272,-0.0252,-0.0174,-0.0591,0.0865,-0.0212,-0.005,-0.0209,-0.0107,0.0113,-0.029,0.0434,-0.0084,-0.0108,0.018,0.013,-0.0608,-0.0401,-0.0395,-0.0513,0.0036,-0.089,0.0012,-0.0171,-0.0368]}
{"key":"[Online Decision Transformer] Recent work has shown that offline reinforcement learning (RL) can be formulated as a sequence modeling problem (Chen et al., 2021; Janner et al., 2021) and solved via approaches similar to large-scale language modeling. However, any practical instantiation of RL also involves an online component, where policies pretrained on passive offline datasets are finetuned via taskspecific interactions with the environment. We propose Online Decision Transformers (ODT), an RL algorithm based on sequence modeling that blends offline pretraining with online finetuning in a unified framework. Our framework uses sequence-level entropy regularizers in conjunction with autoregressive modeling objectives for sample-efficient exploration and finetuning. Empirically, we show that ODT is competitive with the state-of-the-art in absolute performance on the D4RL benchmark but shows much more significant gains during the finetuning procedure.","layer":2,"vector":[-0.0518,-0.0163,0.0522,-0.0188,-0.0119,0.0484,0.0008,0.0462,0.0484,-0.0309,0.0079,0.0067,0.0238,0.0542,0.0054,0.0235,-0.0198,0.032,-0.0083,-0.0017,0.0426,-0.0852,-0.0172,-0.0569,0.0188,0.0305,-0.0489,-0.0812,-0.0322,-0.2323,0.0126,-0.0567,0.0241,0.0031,-0.0016,-0.0016,-0.0751,0.0478,-0.0428,0.0052,0.0263,0.0349,0.0033,-0.0461,-0.0306,-0.0356,-0.002,-0.0217,-0.0236,-0.0297,0.0444,-0.0261,-0.0116,0.0492,0.0303,0.0161,0.0551,0.0576,0.0733,0.0424,0.0057,0.0466,-0.1684,0.0589,-0.0134,0.0546,-0.0587,0.0048,0.0425,0.0426,-0.0512,0.0453,0.0062,0.0593,0.0397,-0.0076,-0.023,-0.0235,0.0134,-0.0131,0.0556,-0.0516,-0.0402,-0.0059,-0.0076,-0.0879,-0.0084,-0.0712,0.0608,0.0093,-0.0128,-0.0005,-0.0096,0.051,-0.0743,-0.0093,0.0446,0.029,-0.053,0.2001,-0.0189,0.0478,-0.0175,-0.0199,0.0418,-0.0124,-0.0484,-0.0421,-0.0046,0.005,-0.0223,0.0418,0.0231,-0.0235,0.0353,0.0229,0.0646,0.0254,0.0073,0.0105,-0.0066,0.0334,0.0386,-0.0064,0.0574,-0.0452,0.0259,0.1327,0.0214,0.0304,0.0674,-0.0419,-0.0322,-0.0325,0.0482,0.0231,0.0555,0.0015,0.0538,-0.006,-0.0376,0.0277,-0.0413,-0.1291,-0.0358,0.1168,0.0064,0.0305,-0.0315,-0.0027,-0.0231,0.0176,0.0062,-0.0198,0.0191,0.0643,0.0024,0.025,-0.0671,0.006,-0.0236,-0.0742,0.0082,0.0921,-0.0439,-0.0683,-0.0474,-0.0119,0.0284,0.0016,0.0601,0.0082,-0.0515,0.0168,0.0631,0.0056,-0.0591,-0.0112,0.0029,0.0155,0.0085,-0.0304,-0.0094,0.0156,-0.008,-0.0446,0.0232,-0.055,0.0331,-0.0117,-0.0206,-0.0042,-0.0108,-0.0131,-0.0329,-0.0277,-0.0047,-0.0151,0.0199,-0.0161,0.014,-0.0228,-0.0355,-0.0098,-0.0425,0.0361,-0.0352,-0.0291,0.04,0.0176,-0.0142,0.0296,0.022,0.0562,-0.0082,0.0098,0.0481,0.0019,0.0035,0.0454,0.0004,-0.0263,-0.014,-0.2329,-0.0193,-0.0333,-0.0036,0.0915,-0.0557,0.0671,-0.0619,0.0447,0.0883,0.0612,-0.055,-0.0132,0.0472,-0.0256,0.0713,0.0228,-0.0051,-0.0468,0.0448,0.013,0.0262,-0.0036,-0.1284,0.0436,0.0016,0.2267,0.0088,0.0515,-0.0414,0.0514,0.0471,-0.0275,-0.075,0.0652,0.0225,0.0951,-0.0342,-0.0016,-0.0546,-0.0121,0.0047,-0.0479,-0.1333,-0.0492,-0.0235,-0.0419,0.0169,-0.0661,-0.0057,0.0338,-0.0248,0.0585,-0.0259,-0.0292,-0.0478,-0.0941,0.0124,-0.027,-0.0159,0.0362,-0.0203,-0.0081,-0.0399,0.0075,0.0005,-0.0143,-0.0614,0.0521,-0.0282,-0.0224,0.0498,0.0101,0.0042,-0.0051,0.0145,0.0108,-0.0367,-0.0557,-0.0238,0.0889,-0.0574,0.0154,0.0271,0.0048,-0.0329,0.0908,-0.0038,0.0225,0.0331,0.0074,0.0141,-0.0769,-0.037,0.0612,0.0147,-0.2915,0.0724,0.0228,0.054,0.0056,-0.0163,0.055,-0.0071,-0.021,0.017,-0.0151,0.0369,0.0145,-0.0078,-0.0049,0.0328,0.1235,-0.0506,0.0418,-0.075,0.0098,0.0517,0.2172,-0.0368,0.0622,-0.0108,-0.0318,-0.0211,0.0405,-0.0157,0.0119,-0.0049,0.059,-0.0473,0.0481,0.0633,-0.0422,0.046,0.0409,-0.0198,-0.0262,-0.0027,0.001,-0.0329,0.0698,0.0142,-0.0189,-0.0341,-0.0296,0.0173,-0.0034,-0.0043,-0.017,-0.0298,0.0208,0.068,-0.0189,-0.0584,-0.0229,-0.0667,0.0081,-0.0655,0.004,-0.0227,-0.0043]}
{"key":"[Deep CNN based feature extractor for text-prompted speaker recognition] Deep learning is still not a very common tool in speaker verification field. We study deep convolutional neural network performance in the text-prompted speaker verification task. The prompted passphrase is segmented into word states - i.e. digits -to test each digit utterance separately. We train a single high-level feature extractor for all states and use cosine similarity metric for scoring. The key feature of our network is the Max-Feature-Map activation function, which acts as an embedded feature selector. By using multitask learning scheme to train the high-level feature extractor we were able to surpass the classic baseline systems in terms of quality and achieved impressive results for such a novice approach, getting 2.85% EER on the RSR2015 evaluation set. Fusion of the proposed and the baseline systems improves this result.","layer":0,"vector":[-0.074,-0.0236,0.0053,-0.069,-0.0188,0.0252,0.0742,0.044,-0.0052,-0.0573,-0.0478,-0.0485,0.0483,0.0603,0.0344,0.0223,0.0435,0.0012,-0.058,0.0399,0.0543,0.0127,-0.0029,-0.0463,0.033,-0.0321,-0.0141,-0.0592,-0.0264,-0.2428,0.0058,-0.0143,0.0651,-0.0345,-0.013,-0.0284,-0.0544,0.0278,-0.0317,-0.0147,0.01,0.0138,-0.0402,-0.0525,-0.0456,-0.0746,-0.051,-0.0091,-0.0161,-0.0149,0.0377,-0.0392,0.0764,0.0529,-0.0195,0.0395,0.0676,0.0656,0.0338,0.0326,0.0336,0.0139,-0.2101,0.0447,0.0137,0.0452,-0.0149,-0.05,0.0231,0.0388,0.0068,0.0364,0.0203,0.0417,0.0016,0.0274,0.0011,-0.0159,0.0179,0.0714,0.0357,-0.007,-0.0088,-0.0217,-0.0095,-0.025,-0.0223,-0.0104,-0.0198,-0.0039,-0.0735,-0.0196,-0.0088,0.0216,-0.054,-0.0196,0.0489,-0.0037,-0.049,0.2016,-0.0626,0.0309,-0.0214,-0.0449,0.0209,-0.0476,-0.0301,0.003,-0.0076,0.0005,-0.0326,-0.0431,0.0008,0.0055,0.0404,0.0122,0.0878,0.0327,-0.0197,-0.0242,-0.0018,0.0572,0.0068,-0.0288,0.0169,-0.0477,0.0628,0.1151,0.0658,0.0437,0.0665,-0.0415,-0.0357,-0.0271,0.0164,0.0328,-0.0116,0.0098,0.0232,-0.028,-0.0106,-0.0794,0.0127,-0.0578,-0.0465,0.1039,-0.0935,-0.0099,-0.0323,-0.0336,0.0246,0.0094,-0.0099,-0.0771,0.0414,0.0106,0.0653,0.0332,-0.0344,-0.0055,0.0312,-0.0284,-0.0259,0.0954,0.0346,-0.1505,-0.0244,-0.0174,-0.0236,0.0182,0.0547,0.0054,-0.0603,0.0304,0.0732,0.0546,-0.0608,0.0023,-0.0302,0.0182,0.0153,-0.0339,-0.0418,0.0218,0.0185,-0.0598,0.0015,-0.093,0.0342,0.0656,-0.0292,0.0485,-0.0501,-0.0207,-0.0578,-0.0252,-0.0432,0.0103,0.0041,-0.0342,-0.0131,0.0068,-0.0104,-0.0124,0.0332,0.0237,-0.029,0.0114,0.0624,0.0386,-0.0036,-0.006,0.0869,-0.0342,-0.0167,-0.0474,0.0138,0.0305,0.0007,0.0449,0.0119,-0.037,-0.0372,-0.2487,-0.0003,0.0314,-0.0047,0.0167,-0.0804,0.0726,-0.0135,0.0695,0.0616,0.029,-0.0014,0.0205,0.0256,0.0251,0.078,0.008,0.0228,-0.0073,0.0096,0.0006,-0.0044,-0.0234,-0.071,0.0162,-0.0145,0.1937,0.05,0.0153,0.0033,0.0427,0.0276,0.0193,-0.1352,0.0815,-0.0007,0.0642,-0.003,0.0036,-0.0331,-0.0506,0.0656,0.0122,-0.0874,-0.0476,-0.0162,-0.0248,0.0663,-0.0636,0.0417,0.0427,-0.0222,0.0272,0.0337,-0.005,0.0031,-0.0765,0.0201,-0.0574,-0.0149,-0.0185,-0.0618,0.0028,-0.0436,0.021,0.0189,-0.0512,-0.0252,0.0475,-0.0006,-0.0252,0.0667,-0.0198,0.0142,0.0794,0.0088,0.0496,-0.05,-0.0295,-0.0488,0.0702,0.0113,0.0196,0.0061,0.0206,0.011,0.0933,0.0146,0.0325,-0.0145,0.0029,-0.009,0.0205,-0.012,0.0659,0.0194,-0.275,0.0295,0.0111,0.0153,-0.0321,-0.0006,0.0266,-0.0014,-0.0459,0.0029,-0.0253,0.0671,0.008,-0.0307,0.0036,0.067,0.0953,-0.05,0.0062,-0.0388,0.0136,0.0272,0.1643,-0.0508,0.0206,-0.0253,-0.0254,-0.0158,0.0865,-0.0392,0.0373,0.0187,0.1262,-0.0271,-0.0311,0.0983,-0.032,0.0045,0.0324,-0.0099,-0.0332,-0.0175,-0.0467,-0.053,0.0539,-0.0201,0.0053,-0.0513,-0.0,0.0763,0.0027,0.0086,-0.0321,-0.0357,0.045,0.0514,-0.0243,-0.0294,-0.0406,-0.0164,0.0136,-0.0423,0.0276,-0.0246,-0.0194]}
{"key":"[Designing Neural Network Architectures using Reinforcement Learning] At present, designing convolutional neural network (CNN) architectures requires both human expertise and labor. New architectures are handcrafted by careful experimentation or modified from a handful of existing networks. We introduce MetaQNN, a meta-modeling algorithm based on reinforcement learning to automatically generate high-performing CNN architectures for a given learning task. The learning agent is trained to sequentially choose CNN layers using $Q$-learning with an $\\epsilon$-greedy exploration strategy and experience replay. The agent explores a large but finite space of possible architectures and iteratively discovers designs with improved performance on the learning task. On image classification benchmarks, the agent-designed networks (consisting of only standard convolution, pooling, and fully-connected layers) beat existing networks designed with the same layer types and are competitive against the state-of-the-art methods that use more complex layer types. We also outperform existing meta-modeling approaches for network design on image classification tasks.","layer":5,"vector":[-0.0324,0.0006,0.001,0.0101,-0.0096,0.041,0.014,0.0167,0.0095,0.0127,0.0226,-0.0572,0.0365,0.0773,0.0352,0.0044,0.0109,0.0321,-0.0152,-0.0292,0.0235,-0.0421,-0.0079,-0.0691,-0.0084,-0.0114,-0.0204,-0.0274,-0.0784,-0.2031,0.0295,-0.0216,0.0348,0.0069,-0.0348,-0.0202,-0.0471,0.0423,-0.0105,0.0333,0.0296,0.0206,-0.0134,-0.0649,-0.0128,-0.0263,-0.0225,-0.0709,0.0084,-0.0412,0.0029,-0.0197,0.0023,0.0316,0.0229,0.041,0.0439,0.0709,0.0694,0.0204,0.0018,0.0392,-0.1457,0.0716,-0.0173,0.0883,-0.0509,0.01,0.0377,0.027,-0.0203,0.0345,-0.0364,0.0103,-0.0038,0.0233,-0.0179,-0.0502,0.0314,-0.0031,0.0337,-0.0567,-0.0287,-0.0009,0.0105,-0.06,0.0032,-0.0391,0.0265,0.0217,-0.0207,0.003,-0.0747,0.0028,-0.0514,-0.0026,-0.0161,-0.0069,-0.1037,0.2152,-0.0153,0.007,0.017,-0.0143,0.0568,-0.0607,-0.0406,0.0053,-0.0795,0.0043,-0.025,-0.0048,0.0204,-0.0366,-0.0126,-0.0119,0.0089,0.0539,-0.0081,-0.0057,-0.016,-0.0148,0.0559,-0.0523,0.0353,-0.058,0.0157,0.1662,0.0116,0.0322,0.0521,-0.0365,0.0016,-0.0155,0.023,0.0306,0.0298,-0.0334,-0.0172,-0.008,-0.0403,0.0017,0.0077,-0.1122,-0.0787,0.0798,-0.0169,0.0038,-0.0121,-0.0348,-0.0034,0.0094,-0.0288,-0.0184,0.0295,0.0395,0.0076,0.0744,-0.0596,0.0031,-0.0202,-0.0873,-0.0696,0.1401,-0.0048,-0.0675,-0.0509,-0.0324,0.0169,0.0134,0.0385,0.0195,-0.0484,0.0262,0.0613,0.0107,-0.0887,-0.0071,0.018,-0.0255,0.0149,-0.0626,-0.0049,0.0302,0.0506,-0.0435,0.043,-0.0707,0.0016,0.036,-0.015,0.0929,-0.0207,0.0412,-0.0428,-0.0284,0.012,-0.0003,-0.0347,-0.0639,-0.0296,-0.0117,-0.0324,-0.0189,-0.0134,0.0135,-0.0502,0.0224,0.036,-0.0041,-0.0414,0.0001,0.0454,-0.0186,-0.0317,0.004,0.0187,0.0264,0.0087,0.0437,0.0657,-0.0528,-0.0342,-0.227,-0.0243,-0.0054,-0.0354,0.0461,-0.0711,0.0278,-0.005,0.021,0.0756,0.0552,-0.0272,-0.0074,-0.0057,0.0207,0.0651,0.0847,0.052,-0.0053,-0.0263,0.0167,0.0251,0.0089,-0.1276,0.058,0.0267,0.2306,0.0362,0.0352,0.0239,0.0732,0.0342,-0.055,-0.1121,0.0437,0.016,0.0873,0.018,-0.0595,-0.0632,-0.035,0.0422,-0.0107,-0.1372,-0.0223,-0.0059,-0.0189,0.0466,-0.0662,-0.0046,0.0175,-0.054,0.025,-0.0242,-0.0346,-0.0265,-0.0764,0.0246,-0.0457,0.0381,0.0451,-0.0394,0.051,-0.0329,0.0784,0.0189,-0.0246,-0.0432,0.0397,-0.0259,-0.0189,0.067,0.0148,0.0167,0.0463,-0.0144,0.0234,-0.0301,-0.0058,0.0123,0.0514,-0.0176,0.0258,-0.0001,0.0825,-0.0006,0.0858,-0.0044,0.0278,-0.0159,0.0333,0.031,-0.0525,0.0009,0.0448,-0.007,-0.2844,0.0855,0.0435,0.0507,-0.0013,0.0172,0.0645,0.0033,-0.0362,0.0042,0.0241,0.0352,0.0538,0.0127,0.0268,-0.0118,0.081,-0.005,0.0526,-0.0661,-0.0176,0.0153,0.2206,-0.0483,0.0357,-0.0069,-0.0379,-0.0077,0.004,-0.0237,0.0056,0.0301,0.0606,-0.0565,0.0367,0.1058,-0.0296,0.0261,-0.0012,0.0203,-0.0275,-0.0005,0.0041,0.001,0.0754,0.002,-0.0065,-0.0094,-0.0498,0.0353,-0.0236,0.0068,-0.0186,-0.0455,0.025,0.0054,-0.0463,-0.0369,-0.066,0.007,0.0486,-0.0435,0.074,-0.0115,-0.0221]}
{"key":"[The LAMBADA dataset: Word prediction requiring a broad discourse context] We introduce LAMBADA, a dataset to evaluate the capabilities of computational models for text understanding by means of a word prediction task. LAMBADA is a collection of narrative passages sharing the characteristic that human subjects are able to guess their last word if they are exposed to the whole passage, but not if they only see the last sentence preceding the target word. To succeed on LAMBADA, computational models cannot simply rely on local context, but must be able to keep track of information in the broader discourse. We show that LAMBADA exemplifies a wide range of linguistic phenomena, and that none of several state-of-the-art language models reaches accuracy above 1% on this novel benchmark. We thus propose LAMBADA as a challenging test set, meant to encourage the development of new models capable of genuine understanding of broad context in natural language text.","layer":0,"vector":[-0.0318,0.0146,0.0128,0.0024,0.0424,0.0105,0.0489,0.0415,0.0272,-0.03,-0.007,-0.0241,0.053,0.046,0.0489,0.003,0.0034,0.0375,-0.0501,0.0192,0.0779,-0.0004,0.0055,-0.0107,0.0045,0.0523,-0.0495,-0.0402,-0.0774,-0.1974,-0.0277,-0.0567,0.0489,-0.0132,0.0174,-0.015,-0.0185,0.0433,0.0153,0.0566,-0.006,0.0077,-0.0061,-0.0349,-0.0363,-0.0746,-0.0093,-0.0208,-0.0528,-0.0531,-0.0419,-0.0582,0.0172,0.009,0.0349,0.03,0.0416,0.0291,0.0432,0.0523,0.0235,0.0572,-0.1845,0.0799,0.0257,0.0144,-0.0723,0.0262,0.0142,0.0655,0.0097,0.0179,0.0498,0.0966,-0.012,0.0098,0.0081,-0.0315,0.0144,0.0147,0.0137,-0.006,-0.0227,-0.003,-0.0394,-0.0709,0.0031,-0.0147,-0.0019,0.0069,-0.0429,-0.0171,-0.0418,0.0316,-0.0681,-0.0317,0.0198,-0.0129,-0.0451,0.1949,-0.0672,0.0263,0.0099,-0.0685,0.0276,0.0233,-0.0047,-0.0278,-0.0246,-0.0107,-0.0333,-0.0411,0.026,-0.0037,0.0486,0.0094,0.1097,0.032,-0.0407,-0.0088,-0.0438,0.0292,-0.0069,-0.0257,0.0164,-0.0462,0.0642,0.1131,0.0226,0.0004,0.0584,-0.0459,-0.0589,-0.0208,0.0575,0.011,0.0672,-0.008,0.0237,-0.0379,-0.0428,-0.0665,-0.0177,-0.0629,-0.0903,0.14,-0.0176,0.0038,-0.0784,-0.0068,-0.0169,0.0519,0.0067,-0.0218,0.0249,0.0083,0.0452,0.033,-0.059,0.0016,0.0262,-0.0444,-0.054,0.076,0.0293,-0.0753,-0.052,0.0458,0.0574,-0.0346,0.0804,0.0117,-0.0489,0.0096,0.041,0.0311,-0.0757,0.0238,0.0204,0.0211,0.0355,-0.0378,-0.0739,0.0644,0.0246,-0.0972,0.0088,-0.0766,0.0793,0.0194,0.0144,0.0221,-0.0235,0.0122,-0.0618,-0.0208,-0.0015,-0.0204,-0.0203,-0.051,-0.042,0.0347,-0.0696,0.0109,0.0041,-0.0104,0.0084,0.0012,0.0382,-0.0114,-0.05,0.0088,0.0413,-0.0178,-0.0128,-0.0452,0.0051,0.0299,0.0004,0.0648,0.0295,-0.0564,-0.032,-0.2374,0.0277,0.0048,-0.0525,0.015,-0.0466,0.0304,0.0064,0.0872,0.0499,0.0567,-0.0607,-0.021,0.0116,0.0092,0.0639,0.0425,0.0474,-0.032,0.0406,0.0127,-0.0153,-0.0227,-0.079,0.0006,0.0154,0.2282,0.0687,0.0407,-0.0489,0.0233,0.0381,-0.0597,-0.1183,0.0765,0.0235,0.0704,-0.0332,-0.0432,0.0033,-0.0092,0.0373,-0.0355,-0.0635,-0.0419,-0.0581,-0.0346,-0.0104,-0.0327,0.0589,0.0464,-0.0548,0.0475,0.0057,0.002,-0.0273,-0.0974,0.0095,-0.0183,0.0201,-0.0171,0.001,0.0196,-0.0522,0.0009,0.0401,-0.003,-0.0232,0.0084,-0.0382,-0.0206,0.0728,-0.0642,-0.0355,0.0139,0.0318,0.0448,-0.058,-0.0423,-0.0229,0.0716,-0.0541,0.0654,0.0221,0.0475,0.0138,0.0925,-0.0162,0.0733,-0.0172,0.0033,0.0193,-0.0294,-0.0495,0.0148,-0.0258,-0.2964,0.0318,0.0088,0.0301,-0.0362,0.0227,0.0413,0.0201,-0.0487,0.0234,-0.0146,0.0911,0.0577,-0.0232,-0.0029,0.004,0.0797,-0.0309,-0.0027,-0.0412,0.0265,0.0318,0.1825,-0.0342,0.0644,0.0114,0.0149,-0.0557,0.0596,0.0251,0.0077,0.016,0.1006,-0.0292,0.0246,0.0371,-0.0042,0.0116,0.0642,0.0064,-0.0312,0.0283,-0.0466,-0.048,0.0363,0.0019,-0.0169,-0.0509,-0.0041,0.0322,-0.0344,0.0418,-0.0452,0.0027,0.0329,0.0407,-0.0394,-0.0249,-0.0051,-0.0266,-0.0381,-0.05,-0.0195,0.0019,0.0142]}
{"key":"[Gradient Descent for One-Hidden-Layer Neural Networks: Polynomial Convergence and SQ Lower Bounds] We study the complexity of training neural network models with one hidden nonlinear activation layer and an output weighted sum layer. We analyze Gradient Descent applied to learning a bounded target function on $n$ real-valued inputs. We give an agnostic learning guarantee for GD: starting from a randomly initialized network, it converges in mean squared loss to the minimum error (in $2$-norm) of the best approximation of the target function using a polynomial of degree at most $k$. Moreover, for any $k$, the size of the network and number of iterations needed are both bounded by $n^{O(k)}\\log(1/\\epsilon)$. In particular, this applies to training networks of unbiased sigmoids and ReLUs. We also rigorously explain the empirical finding that gradient descent discovers lower frequency Fourier components before higher frequency components. We complement this result with nearly matching lower bounds in the Statistical Query model. GD fits well in the SQ framework since each training step is determined by an expectation over the input distribution. We show that any SQ algorithm that achieves significant improvement over a constant function with queries of tolerance some inverse polynomial in the input dimensionality $n$ must use $n^{\\Omega(k)}$ queries even when the target functions are restricted to a set of $n^{O(k)}$ degree-$k$ polynomials, and the input distribution is uniform over the unit sphere; for this class the information-theoretic lower bound is only $\\Theta(k \\log n)$. Our approach for both parts is based on spherical harmonics. We view gradient descent as an operator on the space of functions, and study its dynamics. An essential tool is the Funk-Hecke theorem, which explains the eigenfunctions of this operator in the case of the mean squared loss.","layer":0,"vector":[-0.0453,-0.0343,0.0233,-0.0237,0.0009,0.047,0.0369,0.0223,0.0732,-0.0152,-0.0066,-0.0355,0.0719,0.0559,0.0162,0.0188,-0.0113,0.0462,-0.057,0.0144,0.0516,0.0058,-0.0174,-0.0491,0.0207,-0.0593,-0.0507,-0.0773,-0.0393,-0.2539,0.0159,-0.0301,0.0805,-0.0278,0.0269,-0.0176,-0.021,0.0426,0.0045,0.0319,0.0517,0.0488,-0.0236,-0.041,-0.0096,-0.0127,-0.0512,-0.0392,-0.0564,-0.0556,0.0123,0.0153,0.0456,0.018,0.0278,0.013,0.0116,0.0576,0.0345,0.0737,-0.0168,0.0347,-0.1544,0.0499,0.0365,0.0186,-0.0608,-0.0711,-0.0043,0.0518,0.016,0.0324,0.0042,0.0149,0.0213,0.0257,-0.0074,-0.0521,-0.0015,0.0311,0.0277,-0.0465,-0.0669,-0.0026,-0.0392,-0.0502,0.0299,-0.0315,0.0287,0.0323,-0.0346,-0.0062,-0.0301,0.0024,-0.0637,0.0062,0.0176,0.0405,-0.0569,0.1903,-0.0594,0.0236,0.0208,-0.0028,-0.0054,-0.0345,-0.0382,-0.0089,-0.0129,-0.0193,-0.0291,-0.0442,0.0119,-0.0668,-0.0311,0.0323,0.0231,0.0419,-0.0328,-0.0267,-0.0263,-0.0066,0.035,-0.0008,0.0655,-0.0649,-0.0105,0.1367,0.0523,0.0537,0.0309,-0.033,0.0008,-0.0127,0.0428,0.0511,0.0125,0.014,0.0044,0.0201,-0.0724,-0.0484,0.0304,-0.0675,-0.0375,0.1004,-0.0874,0.0172,-0.0341,-0.0394,-0.0021,0.0623,-0.0054,-0.0566,0.047,0.0413,0.0269,0.0074,-0.075,0.0177,-0.065,-0.045,-0.0172,0.1356,0.0196,-0.0618,-0.0034,-0.0026,0.0264,-0.0243,0.0809,0.0317,-0.0108,-0.0204,0.1008,-0.0019,-0.0498,-0.001,-0.0009,-0.0222,0.006,-0.0615,-0.0011,0.0205,-0.0046,-0.0153,-0.0124,-0.0379,0.0074,0.0297,-0.0729,0.0006,-0.0239,0.0045,-0.0658,-0.0406,-0.0247,-0.0162,0.022,-0.0205,-0.0036,-0.0337,-0.0152,0.0292,0.0208,0.0327,0.0095,-0.0059,0.0039,0.0282,-0.0345,-0.0256,0.0447,-0.0591,-0.0433,0.0127,0.0049,0.0277,-0.0377,0.0447,0.0267,-0.0444,-0.1224,-0.2165,-0.0057,0.0224,-0.0326,0.0389,-0.0883,0.0439,-0.0033,0.0412,0.0603,0.0417,0.0106,-0.0625,-0.0022,-0.0102,0.0263,0.0569,0.016,0.0197,0.0046,-0.0025,0.0452,-0.0032,-0.0435,0.0834,-0.0044,0.2105,-0.0105,0.0715,-0.0451,0.0116,0.0175,-0.0187,-0.0167,0.0513,0.0206,0.1092,-0.0184,-0.04,-0.006,-0.0255,0.0189,0.0004,-0.0689,-0.0308,-0.0064,-0.0385,0.0063,-0.075,0.0346,0.0431,-0.026,0.0823,0.0006,0.0058,-0.0349,-0.0797,0.0067,-0.0441,0.0713,0.0115,-0.0614,0.0302,-0.0614,0.0907,-0.0005,-0.0363,-0.0112,0.0462,-0.0256,0.0143,0.0623,0.0231,0.0144,0.0265,0.0053,0.0612,-0.0244,-0.0273,-0.0024,0.0628,-0.0509,0.0661,-0.0258,0.0261,0.0168,0.1454,-0.0498,0.0188,-0.0138,-0.0599,0.0152,-0.0267,0.001,0.0388,-0.0239,-0.2799,0.0288,0.025,0.0537,-0.011,0.0071,0.0618,0.0107,-0.0592,0.0167,0.0292,0.0336,0.0636,0.0,0.0094,0.055,0.0456,-0.0322,0.0459,-0.0522,0.0227,0.0814,0.2149,-0.065,0.0249,-0.0063,-0.0199,-0.0273,0.0106,-0.0257,0.0201,0.0102,0.0921,-0.0438,0.0382,0.0941,-0.0446,0.0522,0.0454,-0.0142,-0.0003,-0.0175,-0.0698,-0.0023,0.063,-0.0403,-0.0212,-0.0186,-0.0246,0.0103,0.0169,0.058,0.0221,-0.0075,0.0367,0.0221,-0.0928,-0.0328,-0.025,-0.0357,0.0093,-0.0898,-0.032,-0.0045,-0.0373]}
{"key":"[Federated Graph Neural Networks: Overview, Techniques and Challenges] With its powerful capability to deal with graph data widely found in practical applications, graph neural networks (GNNs) have received significant research attention. However, as societies become increasingly concerned with data privacy, GNNs face the need to adapt to this new normal. This has led to the rapid development of federated graph neural networks (FedGNNs) research in recent years. Although promising, this interdisciplinary field is highly challenging for interested researchers to enter into. The lack of an insightful survey on this topic only exacerbates this problem. In this paper, we bridge this gap by offering a comprehensive survey of this emerging field. We propose a unique 3-tiered taxonomy of the FedGNNs literature to provide a clear view into how GNNs work in the context of Federated Learning (FL). It puts existing works into perspective by analyzing how graph data manifest themselves in FL settings, how GNN training is performed under different FL system architectures and degrees of graph data overlap across data silo, and how GNN aggregation is performed under various FL settings. Through discussions of the advantages and limitations of existing works, we envision future research directions that can help build more robust, dynamic, efficient, and interpretable FedGNNs.","layer":0,"vector":[0.0235,-0.0456,-0.0194,-0.0172,0.0566,0.0369,0.0078,0.0263,0.0541,-0.0473,0.0293,-0.0435,0.0591,0.1039,0.0171,0.0395,-0.0098,0.0512,-0.0244,-0.0227,-0.0053,-0.0425,-0.0242,-0.0659,0.0227,0.047,-0.0394,-0.012,-0.0979,-0.216,0.0265,-0.0482,0.0621,-0.0355,-0.0121,-0.0146,0.0178,0.0509,-0.0099,0.0518,0.0082,0.0002,-0.0343,-0.0291,-0.009,-0.0093,-0.0156,0.0085,-0.0395,-0.0272,0.0532,-0.0375,0.0177,0.0168,0.0344,0.0441,0.0393,0.0258,0.0327,0.0365,0.0367,0.0619,-0.134,0.0475,0.0544,0.0682,-0.0288,0.0152,-0.0068,0.0309,0.0394,0.0233,-0.0068,-0.0183,0.0248,0.0218,-0.0225,-0.0297,-0.0264,-0.0162,-0.006,-0.005,-0.0387,-0.0293,0.0035,-0.0009,-0.0184,-0.0497,0.015,0.0163,-0.038,-0.0176,-0.0092,0.0057,-0.0646,-0.0231,0.0498,0.0344,-0.0758,0.1794,-0.0703,0.016,0.0251,-0.0234,0.0343,-0.041,-0.0073,-0.0624,-0.0254,-0.0105,-0.0314,-0.066,0.0175,-0.0656,-0.006,0.005,0.0763,0.072,-0.0102,-0.0318,-0.0108,0.0056,0.0634,-0.0275,0.0319,-0.0608,-0.0089,0.1034,0.0254,0.0117,0.0655,0.0005,-0.0241,-0.0218,0.0112,0.0543,0.0548,-0.0344,-0.0062,0.0261,-0.0244,-0.0272,0.0174,-0.0308,-0.0892,0.1326,-0.0068,0.0023,-0.0689,-0.013,-0.07,0.0115,-0.0298,-0.0122,0.0036,0.0365,0.067,0.0754,-0.086,0.0023,-0.0353,-0.0202,-0.0357,0.1322,0.068,-0.151,-0.033,-0.0251,0.0174,-0.0558,0.0271,0.019,0.0056,0.052,0.0596,0.0521,-0.0173,-0.0094,-0.0079,-0.0068,0.001,-0.0075,-0.0493,0.0342,-0.0086,-0.0437,0.032,-0.0614,-0.0183,0.05,-0.0722,0.047,-0.0184,0.0132,-0.023,-0.032,0.0068,-0.0384,-0.0027,-0.0573,-0.0052,-0.0282,-0.013,-0.0023,-0.0358,0.0323,-0.0293,0.0158,0.004,-0.0069,-0.0456,0.0197,0.0375,-0.0415,-0.0363,-0.0198,0.0165,0.0252,0.009,0.0508,0.0464,-0.0033,-0.0366,-0.1871,-0.0293,0.0202,-0.0378,0.0736,-0.0435,0.0298,0.0067,0.0444,0.0903,0.0782,0.0075,-0.0576,0.0176,-0.0039,0.0411,0.0325,0.0526,-0.0287,-0.0185,-0.017,0.0172,0.0206,-0.0892,0.0398,0.1045,0.2032,0.0171,0.0246,-0.0556,-0.0079,0.0441,-0.0608,-0.1163,0.0889,0.0305,0.0221,-0.0052,-0.0506,-0.0513,-0.0262,0.0043,0.013,-0.1232,-0.0134,-0.0016,-0.0156,-0.0158,-0.0875,0.0147,0.0346,-0.019,0.0547,0.0406,0.0048,-0.0634,-0.0476,0.0568,-0.0171,0.0479,0.0158,-0.0842,0.0044,-0.056,0.0848,-0.0046,-0.0185,-0.0046,0.0314,-0.0069,-0.003,0.1033,0.0464,-0.0125,0.0708,0.031,0.0153,-0.0083,-0.0356,0.0152,0.0753,-0.0351,0.0593,0.0028,-0.0107,0.0119,0.0644,0.0202,0.0803,-0.0428,0.0208,0.0061,-0.0441,-0.0429,0.0552,-0.0092,-0.3066,0.0329,0.0245,0.0815,-0.0164,0.0199,0.0553,0.0443,-0.0471,0.0149,0.0484,0.0562,0.0301,-0.0243,-0.023,0.0379,0.0089,-0.0374,0.0364,-0.0921,0.059,0.0257,0.21,-0.024,0.0438,0.0621,-0.0095,0.0158,0.0406,-0.0132,-0.0359,-0.0158,0.0617,-0.0519,0.0484,0.0754,-0.0459,0.0289,0.0385,-0.0317,0.0004,-0.0421,-0.0133,-0.0173,0.0448,0.0085,-0.0523,-0.044,-0.0004,0.0223,-0.0028,-0.0523,-0.049,-0.0076,0.0077,-0.015,-0.0339,-0.0124,-0.0734,-0.0579,-0.0082,-0.0843,-0.0314,-0.0245,-0.0391]}
{"key":"[Embedding Symbolic Temporal Knowledge into Deep Sequential Models] Sequences and time-series often arise in robot tasks, e.g., in activity recognition and imitation learning. In recent years, deep neural networks (DNNs) have emerged as an effective data-driven methodology for processing sequences given sufficient training data and compute resources. However, when data is limited, simpler models such as logic/rule-based methods work surprisingly well, especially when relevant prior knowledge is applied in their construction. However, unlike DNNs, these \"structured\" models can be difficult to extend, and do not work well with raw unstructured data. In this work, we seek to learn flexible DNNs, yet leverage prior temporal knowledge when available. Our approach is to embed symbolic knowledge expressed as linear temporal logic (LTL) and use these embeddings to guide the training of deep models. Specifically, we construct semantic-based embeddings of automata generated from LTL formula via a Graph Neural Network. Experiments show that these learnt embeddings can lead to improvements in downstream robot tasks such as sequential action recognition and imitation learning.","layer":2,"vector":[-0.0481,-0.0215,0.0409,-0.0557,-0.0062,0.0444,0.0249,0.0229,0.0545,-0.0272,0.0083,-0.0389,0.0454,0.0483,0.0313,0.0007,-0.0442,0.0995,-0.0454,-0.031,0.0586,-0.0743,-0.0136,-0.0405,0.0286,0.0306,-0.0267,0.0033,-0.0385,-0.2043,0.0236,-0.0558,0.0289,0.0014,-0.0157,-0.0697,-0.0718,0.0679,-0.0081,0.0593,0.0411,0.0041,0.0248,-0.0698,-0.0099,-0.0586,-0.0059,-0.0328,-0.0416,-0.0691,0.0041,-0.0207,0.0454,0.0175,0.0461,0.0031,0.0644,0.0624,0.0543,0.0204,0.0213,0.0356,-0.1413,0.0764,0.0155,0.0394,-0.0406,-0.0143,0.033,0.0691,-0.0024,0.0017,0.0417,0.0475,0.0016,0.0144,0.0068,-0.0375,-0.0299,-0.0068,-0.0031,-0.0107,-0.0174,-0.0443,-0.0314,-0.0154,-0.0034,-0.0538,-0.0022,0.0259,-0.057,-0.0224,-0.0041,0.0112,-0.0597,-0.016,0.0601,0.0353,-0.0416,0.1905,-0.0343,0.0352,0.0548,0.0016,0.0074,-0.0261,0.0118,-0.0388,-0.0262,-0.0271,-0.0453,-0.0055,0.0426,-0.04,0.0586,0.0236,0.0961,0.012,-0.0426,0.0017,0.0048,0.0096,0.0191,-0.0755,-0.0158,-0.0552,0.0249,0.1238,0.0225,0.0526,0.0456,-0.026,-0.0193,-0.0059,0.0099,0.0161,0.0309,-0.0367,0.0305,-0.0251,-0.0135,0.01,0.041,-0.0525,-0.0692,0.0671,-0.0203,-0.049,-0.0631,0.0116,-0.0474,0.0244,0.0155,-0.0125,0.032,0.0544,0.0557,0.0243,-0.0914,0.0026,-0.0455,-0.0236,-0.0355,0.1069,0.0243,-0.1184,-0.0351,-0.0248,0.0161,-0.0315,0.0638,0.0386,-0.0414,0.0146,0.0822,0.0469,-0.0198,0.009,0.0076,0.0219,0.0067,-0.0854,-0.0249,0.0477,0.0403,-0.0135,-0.023,-0.0343,0.018,0.0326,0.0082,0.0632,-0.0279,0.0303,-0.0343,-0.0173,0.0021,-0.0055,-0.0149,-0.0734,-0.0133,-0.0334,-0.0029,-0.0203,-0.005,-0.0286,-0.0451,0.0339,0.0437,0.0291,-0.0265,-0.001,0.0335,-0.0391,-0.0269,0.0135,-0.0356,-0.0007,0.014,0.0121,-0.0001,-0.0188,-0.046,-0.2033,-0.0239,0.0159,-0.0111,0.0447,-0.0726,-0.0134,-0.0369,0.0759,0.0493,0.0625,-0.0284,-0.0361,-0.029,-0.0389,0.0707,0.0717,0.0317,-0.0277,0.027,-0.0297,-0.0217,0.0024,-0.1135,0.0419,-0.0152,0.249,0.0497,0.0748,-0.0372,-0.0014,-0.0013,-0.0372,-0.088,0.0484,-0.0305,0.0501,0.0325,0.0205,-0.0436,-0.0619,0.0156,-0.0174,-0.0685,-0.0311,-0.0352,-0.0127,0.0143,-0.038,0.0193,0.0259,-0.0212,0.025,0.0108,-0.0862,-0.015,-0.0482,0.0382,-0.0354,0.0099,0.0029,-0.0523,-0.0046,-0.0445,0.0803,0.0017,-0.0367,-0.0654,-0.0037,0.0402,-0.0074,0.1211,0.0049,-0.0237,0.0786,0.0196,0.0055,-0.0121,-0.0404,0.0021,0.0701,-0.0634,0.022,0.0091,0.0564,-0.002,0.0613,-0.0016,0.0553,0.0082,0.0432,0.0198,-0.0748,0.0032,0.0381,-0.0362,-0.3043,0.0528,0.0372,0.0544,-0.0313,-0.0206,0.0633,-0.0108,-0.0552,0.0248,0.0027,0.0872,0.0755,0.0055,-0.0217,0.0237,0.0818,-0.0639,0.0519,-0.0482,-0.0038,0.0711,0.2256,-0.0184,0.05,-0.0018,0.0134,-0.0635,0.0698,0.0008,0.0344,0.0088,0.0931,-0.0375,0.0022,0.0652,-0.0469,0.0402,0.038,-0.0001,0.0085,-0.0061,-0.0364,-0.0396,0.058,-0.0008,0.0045,-0.0266,-0.0281,0.0502,-0.0133,0.0021,-0.0439,0.0229,0.0401,0.0144,0.0093,-0.0154,-0.0552,-0.0557,-0.0159,-0.0906,0.0082,-0.0189,-0.0433]}
{"key":"[A Comparative Approach to Explainable Artificial Intelligence Methods in Application to High-Dimensional Electronic Health Records: Examining the Usability of XAI] Explainable Artificial Intelligence (XAI) is a rising field in AI. It aims to produce a demonstrative factor of trust, which for human subjects is achieved through communicative means, which Machine Learning (ML) algorithms cannot solely produce, illustrating the necessity of an extra layer producing support to the model output. When approaching the medical field, we can see challenges arise when dealing with the involvement of human-subjects, the ideology behind trusting a machine to tend towards the livelihood of a human poses an ethical conundrum - leaving trust as the basis of the human-expert in acceptance to the machines decision. The aim of this paper is to apply XAI methods to demonstrate the usability of explainable architectures as a tertiary layer for the medical domain supporting ML predictions and human-expert opinion, XAI methods produce visualization of the feature contribution towards a given models output on both a local and global level. The work in this paper uses XAI to determine feature importance towards high-dimensional data-driven questions to inform domain-experts of identifiable trends with a comparison of model-agnostic methods in application to ML algorithms. The performance metrics for a glass-box method is also provided as a comparison against black-box capability for tabular data. Future work will aim to produce a user-study using metrics to evaluate human-expert usability and opinion of the given models.","layer":0,"vector":[-0.0352,-0.012,0.0143,0.0359,0.0268,-0.0015,0.0481,0.0626,0.0087,-0.0231,-0.0474,-0.0315,-0.0023,0.0496,-0.0121,0.0011,-0.0342,0.0117,-0.0382,0.0429,0.045,0.0237,-0.0244,-0.0453,0.0043,0.0367,-0.0383,-0.0542,-0.0459,-0.2058,0.0434,-0.0614,0.0748,-0.0464,-0.0124,-0.0432,0.001,0.0548,-0.019,0.0046,0.0212,-0.0107,-0.0137,-0.0111,-0.0024,-0.0196,-0.0514,-0.0145,-0.0421,-0.0038,0.0304,0.0023,0.012,0.0447,0.0275,0.0215,0.0628,0.0292,0.0011,0.0366,0.0469,0.0501,-0.1506,0.1115,0.0521,0.0572,-0.0228,-0.0369,-0.0057,0.0312,-0.0126,0.0049,0.0261,0.0985,-0.0056,-0.0173,0.0106,-0.0433,0.0295,0.0428,-0.005,0.0054,-0.0475,-0.0021,-0.0353,-0.0345,0.0088,-0.0464,0.0282,-0.0315,-0.0104,0.0237,-0.0636,-0.013,-0.0363,-0.0658,0.0328,0.0169,-0.0724,0.1835,-0.0508,-0.0206,0.0338,-0.0299,0.0192,-0.0402,-0.0439,-0.066,0.0046,-0.031,-0.0478,0.0066,0.0284,0.0116,0.0245,0.0494,0.0173,-0.0027,0.0187,-0.0314,-0.0087,0.0038,0.0394,0.0423,0.0158,-0.0221,0.0534,0.1376,0.0134,0.0145,0.0689,-0.0161,-0.056,0.0043,0.0294,0.0124,0.03,-0.0152,-0.0104,0.0164,-0.0606,-0.0666,-0.009,-0.0779,-0.0796,0.1122,-0.0527,0.0024,-0.0221,-0.0204,0.0257,-0.0194,-0.0419,-0.0573,0.0492,0.0363,0.0065,0.0147,-0.0589,0.0128,0.0412,-0.0859,-0.0561,0.1293,0.0317,-0.0886,-0.0286,-0.0108,0.095,0.0169,0.0513,0.0132,-0.0504,0.0338,0.0564,0.0328,-0.0414,-0.0292,0.0348,0.0113,0.0406,-0.0455,-0.056,0.0706,0.0414,-0.0309,0.026,-0.0422,0.0572,0.0572,-0.0108,0.0176,-0.0348,0.0495,-0.025,-0.0084,-0.0151,-0.012,-0.0266,-0.0386,-0.0048,0.0082,-0.0498,0.0357,-0.0388,-0.0074,-0.0269,0.0,0.0714,0.0333,-0.0448,0.0177,0.03,-0.0405,-0.0216,0.0216,0.0292,-0.0037,0.0142,0.0537,0.0123,-0.0237,-0.0598,-0.2615,-0.0105,-0.0278,-0.0116,0.0197,-0.0823,0.0358,-0.0157,0.0067,0.0763,0.0741,-0.0105,-0.0304,0.0018,-0.0327,0.023,0.0494,0.0138,-0.0706,-0.0178,0.0006,0.0298,0.0089,-0.1133,0.0003,-0.0008,0.2553,0.0114,0.0308,0.0256,-0.0019,0.032,-0.0601,-0.154,0.0691,0.0224,0.0394,-0.0097,-0.027,-0.0277,-0.0503,0.0485,-0.0069,-0.0797,-0.0472,-0.0181,-0.0209,0.0249,-0.0669,0.0297,0.0195,0.0166,0.0596,0.0146,0.0058,-0.0394,-0.0815,0.0233,-0.013,0.0259,-0.0249,-0.0192,0.0094,-0.0626,0.0173,-0.0492,-0.0318,-0.0097,0.0439,-0.0472,-0.037,0.1084,-0.0104,-0.0294,0.0853,0.0207,0.0304,-0.0519,-0.0369,0.0147,0.0399,0.0051,0.0136,0.0216,0.0465,0.009,0.0473,-0.0051,0.0228,-0.0157,0.0134,0.0047,-0.0454,-0.0491,0.0205,-0.0336,-0.2914,0.0473,-0.0155,0.066,-0.0725,-0.001,0.007,0.0222,-0.0081,0.0283,-0.0129,0.0332,0.0713,-0.0041,-0.0271,0.0198,0.0805,-0.0873,0.0684,0.0078,0.0331,0.0301,0.2122,-0.0461,0.014,0.0125,-0.0202,-0.036,0.0028,-0.0237,0.058,-0.0572,0.0596,-0.0085,0.0522,0.0653,-0.0019,0.039,0.0288,-0.0383,0.002,0.0107,-0.0576,-0.0173,0.107,0.033,-0.011,-0.0359,0.0173,0.0188,0.0249,-0.0121,-0.0441,0.0185,0.0138,0.0335,-0.0059,-0.0329,-0.0105,-0.0493,0.0474,-0.0327,0.0169,0.0393,-0.0115]}
{"key":"[A New Open-Access Platform for Measuring and Sharing mTBI Data] Despite numerous research efforts, the precise mechanisms of concussion have yet to be fully uncovered. Clinical studies on high-risk populations, such as contact sports athletes, have become more common and give insight on the link between impact severity and brain injury risk through the use of wearable sensors and neurological testing. However, as the number of institutions operating these studies grows, there is a growing need for a platform to share these data to facilitate our understanding of concussion mechanisms and aid in the development of suitable diagnostic tools. To that end, this paper puts forth two contributions: 1) a centralized, open-source platform for storing and sharing head impact data, in collaboration with the Federal Interagency Traumatic Brain Injury Research informatics system (FITBIR), and 2) a deep learning impact detection algorithm (MiGNet) to differentiate between true head impacts and false positives for the previously biomechanically validated instrumented mouthguard sensor (MiG2.0), all of which easily interfaces with FITBIR. We report 96% accuracy using MiGNet, based on a neural network model, improving on previous work based on Support Vector Machines achieving 91% accuracy, on an out of sample dataset of high school and collegiate football head impacts. The integrated MiG2.0 and FITBIR system serve as a collaborative research tool to be disseminated across multiple institutions towards creating a standardized dataset for furthering the knowledge of concussion biomechanics.","layer":1,"vector":[-0.0672,-0.0226,0.0461,-0.047,0.0329,0.0272,0.0799,0.053,0.0452,0.0009,-0.0122,-0.0695,0.0072,0.0469,0.0298,-0.0119,-0.0122,0.056,-0.0311,0.028,-0.0233,-0.0084,-0.0045,-0.0302,-0.001,0.0532,-0.0488,0.0076,-0.078,-0.2434,0.0104,-0.0436,0.0691,-0.0536,0.0045,-0.0088,-0.0245,0.0553,0.0338,0.0388,0.0311,0.074,-0.0311,-0.0628,0.0112,-0.0057,-0.0025,-0.0093,0.0088,-0.0055,0.0011,-0.0738,0.0455,0.0034,0.0157,0.0323,0.0141,-0.0095,0.0511,0.0692,0.0226,0.0189,-0.2043,0.0372,0.0069,0.0303,-0.0623,0.0183,0.083,0.0163,-0.0344,0.0112,0.0698,0.0032,-0.0124,0.0028,0.027,-0.0606,0.0364,-0.0174,0.0052,-0.0114,-0.0362,-0.0039,-0.0152,-0.0168,0.0251,-0.0297,-0.0111,-0.0164,-0.0453,-0.0231,0.0042,0.0077,-0.0457,-0.0008,0.0421,-0.0063,-0.0647,0.2196,-0.0256,0.036,-0.0002,-0.0047,0.0294,-0.0294,-0.0374,-0.0526,-0.0193,0.0412,0.0213,-0.0349,-0.0177,0.0098,0.0089,0.0202,0.0388,0.0421,0.0012,-0.0096,-0.0292,0.0175,0.0476,-0.019,-0.0311,-0.0594,-0.0004,0.1379,0.0457,0.0295,0.0558,-0.035,-0.0933,-0.0455,0.0368,0.0409,0.0129,-0.0043,-0.0111,0.0281,0.0001,-0.0707,0.0227,-0.0809,-0.0887,0.1381,-0.0666,-0.0124,-0.009,-0.0446,0.0036,0.0276,-0.0753,0.033,0.0337,0.0293,0.0161,0.0103,-0.0384,0.034,-0.0129,-0.0649,-0.0172,0.0751,0.0181,-0.1023,0.0144,0.0037,0.0214,-0.0475,0.061,0.0338,0.0016,0.0343,0.0546,0.0244,-0.0674,-0.0025,-0.0124,0.0002,0.0685,-0.0664,-0.053,0.0211,0.0216,-0.0292,-0.0103,-0.0313,0.0343,0.0498,-0.0517,-0.0114,-0.0119,-0.0248,0.014,0.0116,-0.0226,-0.0362,0.009,-0.011,-0.0066,0.0258,-0.018,0.0395,0.0299,0.0609,-0.0293,0.002,0.0627,0.0053,-0.0403,-0.0489,0.049,-0.041,-0.0607,-0.0091,-0.0099,0.0428,-0.0261,0.0649,0.0459,-0.024,-0.0747,-0.2167,-0.0266,-0.0032,0.0021,0.0316,-0.0151,0.0585,0.0143,0.0552,0.0541,0.1104,0.0071,-0.087,-0.0105,-0.0283,0.0358,0.0067,0.0256,-0.0423,-0.0125,0.021,0.0371,-0.0488,-0.04,0.0213,-0.0258,0.2096,0.028,0.0525,-0.0262,-0.0159,0.0382,0.0158,-0.1298,0.0893,0.0274,0.0239,0.0091,-0.0404,-0.0625,-0.0583,0.029,0.026,-0.1058,0.0296,-0.0075,-0.0098,-0.0267,-0.0605,0.0155,0.0031,-0.0015,0.0359,0.0054,0.024,-0.0699,-0.1083,0.0573,-0.0546,0.0239,-0.0247,-0.0187,0.0173,-0.0743,0.0885,0.0494,-0.0225,-0.054,0.0234,-0.0571,-0.013,0.0947,0.0028,0.0171,0.0152,-0.0151,0.0653,-0.0352,-0.0345,0.0506,0.0583,-0.0183,0.045,0.0246,0.0207,0.012,0.0355,-0.0067,0.0113,-0.0858,0.0258,-0.0089,-0.013,-0.0274,0.0057,-0.0213,-0.2883,0.0602,0.0153,0.0316,-0.0001,-0.0099,0.056,0.0235,-0.0046,-0.0014,-0.0103,0.0252,0.0234,-0.0197,-0.0415,0.0279,0.0518,-0.0358,0.0218,-0.0205,-0.0298,0.0714,0.1995,-0.0503,0.0311,0.0187,-0.0138,-0.0324,0.0614,-0.0283,0.0114,-0.0108,0.0928,-0.0496,-0.0244,0.087,-0.058,0.022,0.0344,-0.0338,0.0257,0.0212,-0.0096,-0.0089,0.0726,0.0108,-0.0333,-0.0647,0.021,0.038,-0.0357,-0.038,-0.0091,-0.0017,0.0334,0.0172,0.0285,-0.0362,-0.0377,-0.0448,0.0113,-0.033,-0.0506,0.0318,0.02]}
{"key":"[SliceNDice: Mining Suspicious Multi-attribute Entity Groups with Multi-view Graphs] Given the reach of web platforms, bad actors have considerable incentives to manipulate and defraud users at the expense of platform integrity. This has spurred research in numerous suspicious behavior detection tasks, including detection of sybil accounts, false information, and payment scams/fraud. In this paper, we draw the insight that many such initiatives can be tackled in a common framework by posing a detection task which seeks to find groups of entities which share too many properties with one another across multiple attributes (sybil accounts created at the same time and location, propaganda spreaders broadcasting articles with the same rhetoric and with similar reshares, etc.) Our work makes four core contributions: Firstly, we posit a novel formulation of this task as a multi-view graph mining problem, in which distinct views reflect distinct attribute similarities across entities, and contextual similarity and attribute importance are respected. Secondly, we propose a novel suspiciousness metric for scoring entity groups given the abnormality of their synchronicity across multiple views, which obeys intuitive desiderata that existing metrics do not. Finally, we propose the SliceNDice algorithm which enables efficient extraction of highly suspicious entity groups, and demonstrate its practicality in production, in terms of strong detection performance and discoveries on Snapchat's large advertiser ecosystem (89% precision and numerous discoveries of real fraud rings), marked outperformance of baselines (over 97% precision/recall in simulated settings) and linear scalability.","layer":15,"vector":[-0.0199,-0.0581,0.0109,-0.0258,0.0516,0.0103,0.0439,0.0129,0.0329,0.0119,0.0132,-0.0544,0.0182,0.0451,0.0242,0.0211,0.0121,0.0099,-0.028,-0.0204,0.0301,-0.0501,0.0515,-0.0166,0.0529,0.0696,-0.0288,-0.0578,-0.093,-0.208,0.0058,-0.0142,0.079,-0.0296,0.0112,-0.0275,-0.0122,0.0724,-0.0151,0.0515,0.0018,0.0091,-0.0223,-0.0564,-0.0454,-0.0312,-0.0081,0.0309,-0.0385,-0.0246,0.0011,-0.0415,0.001,0.0476,0.0247,-0.0069,0.0236,0.0465,0.0419,0.0458,0.0545,0.0438,-0.0965,0.043,0.0239,0.065,-0.036,-0.0108,0.0063,0.0199,0.0046,0.0404,-0.0121,0.0157,0.003,0.022,-0.0178,0.0284,-0.0224,0.0306,-0.0297,-0.0403,-0.0195,0.0065,0.0146,-0.002,-0.0029,-0.018,0.0724,0.0018,-0.0348,0.0221,0.0251,0.0082,-0.0864,-0.0154,-0.0078,0.0322,-0.036,0.2237,-0.0627,0.0485,0.062,-0.0324,0.0523,-0.0716,0.0065,-0.0431,0.0203,-0.0221,-0.0413,-0.0249,0.0131,-0.0737,0.0111,0.0054,0.0858,0.0349,-0.0235,-0.0297,-0.0064,0.0346,0.0868,-0.0326,-0.0067,-0.0795,0.0289,0.1258,0.0448,0.0128,-0.0031,0.0383,-0.051,-0.0077,0.009,0.0096,-0.0001,0.0061,0.0392,-0.023,-0.0667,-0.0487,0.01,-0.0824,-0.038,0.0996,-0.0205,0.03,-0.0574,-0.0271,-0.0457,0.0484,-0.0406,-0.0429,0.0377,-0.0002,0.0318,0.0481,-0.0314,0.0183,0.0233,-0.0147,-0.0489,0.1209,0.0384,-0.1264,-0.0069,0.017,-0.0155,-0.0372,0.0298,0.029,-0.0234,0.0317,0.006,-0.0154,-0.0634,0.0259,0.0039,-0.0215,0.0602,-0.0714,-0.0732,0.0627,0.0526,-0.0325,0.0302,-0.0244,0.0471,0.0327,-0.056,0.0254,-0.0557,-0.0084,-0.0435,-0.0084,-0.0005,-0.0098,-0.0102,-0.018,0.0301,0.0106,-0.0774,-0.0028,0.0006,0.0232,-0.0399,-0.0305,0.0248,0.015,-0.0225,-0.0093,-0.0084,0.0081,0.0002,-0.0062,0.0278,0.0468,-0.0073,0.041,0.0248,-0.0526,-0.031,-0.2504,-0.0618,0.0097,0.0022,0.0164,-0.0609,0.0435,-0.0438,0.0743,0.0868,0.0452,-0.0499,-0.0314,0.0277,0.001,0.0955,0.0077,0.0511,-0.033,0.0174,-0.027,-0.0128,-0.0208,-0.0657,0.0399,0.0321,0.2481,0.0977,-0.0337,-0.0011,0.0324,0.045,-0.0266,-0.0933,0.0503,0.0423,0.0627,-0.0176,-0.0222,-0.0127,-0.0693,-0.0065,-0.0076,-0.0684,-0.0163,-0.041,-0.0423,0.0283,-0.0214,0.0581,0.0413,-0.0449,0.0655,0.0683,-0.0081,-0.0736,-0.0579,0.0292,-0.0382,0.0372,0.0059,-0.0607,0.0223,-0.0614,0.0736,0.0008,-0.0343,-0.0192,0.0216,-0.0147,-0.0298,0.1078,-0.0156,-0.0433,0.023,-0.0156,0.0374,-0.0918,-0.0231,0.0054,0.0811,0.0159,0.0216,0.0099,0.0111,-0.0122,0.0293,0.0243,0.0847,-0.0411,-0.0093,0.0002,-0.0706,-0.0138,0.0469,-0.0265,-0.3094,0.0027,-0.0245,0.0309,0.0161,0.0152,0.0439,0.0027,-0.0472,-0.0369,0.0411,0.0796,0.0224,-0.0164,-0.0115,0.0526,0.0239,-0.061,0.0142,-0.0172,0.0258,0.0225,0.2222,-0.0007,-0.0101,0.0506,-0.0025,0.0018,0.0121,0.0187,0.0082,-0.0299,0.0473,-0.0417,0.0219,0.0456,-0.0404,0.0126,0.0313,-0.0307,-0.0407,-0.0126,-0.0576,-0.0249,0.1154,-0.0243,-0.0229,-0.0434,0.0371,0.0581,-0.0219,-0.0482,-0.0437,-0.0008,0.0253,0.018,-0.0535,-0.0068,-0.011,-0.059,-0.0435,-0.0084,-0.0198,0.0147,-0.0061]}
{"key":"[Beyond Fine-Tuning: Transferring Behavior in Reinforcement Learning] Designing agents that acquire knowledge autonomously and use it to solve new tasks efficiently is an important challenge in reinforcement learning. Knowledge acquired during an unsupervised pre-training phase is often transferred by fine-tuning neural network weights once rewards are exposed, as is common practice in supervised domains. Given the nature of the reinforcement learning problem, we argue that standard fine-tuning strategies alone are not enough for efficient transfer in challenging domains. We introduce Behavior Transfer (BT), a technique that leverages pre-trained policies for exploration and that is complementary to transferring neural network weights. Our experiments show that, when combined with large-scale pre-training in the absence of rewards, existing intrinsic motivation objectives can lead to the emergence of complex behaviors. These pre-trained policies can then be leveraged by BT to discover better solutions than without pre-training, and combining BT with standard fine-tuning strategies results in additional benefits. The largest gains are generally observed in domains requiring structured exploration, including settings where the behavior of the pre-trained policies is misaligned with the downstream task.","layer":1,"vector":[-0.0549,0.0006,0.0272,-0.0201,-0.0138,0.0115,0.0331,0.0452,0.0491,0.0023,0.0273,-0.0351,0.0443,0.0376,-0.004,-0.0053,-0.0288,0.0588,-0.0228,0.0113,0.011,-0.0155,-0.0208,-0.035,0.0085,0.0244,-0.0475,-0.0664,-0.0437,-0.2425,0.0183,-0.0534,-0.0143,-0.01,-0.0129,-0.0164,-0.0308,0.0756,-0.0282,0.0554,0.0853,0.0212,-0.0295,-0.046,-0.0369,-0.0639,-0.0119,-0.0273,0.004,-0.0871,0.0224,-0.0543,0.0118,0.021,0.0176,0.0286,0.0656,0.0367,0.0544,0.0388,0.0145,0.043,-0.1761,0.0585,0.0477,0.0728,-0.042,-0.0056,0.0038,0.074,-0.0538,0.0493,0.0389,0.0157,0.0707,-0.0103,-0.0233,-0.0406,-0.0063,0.0352,0.0204,-0.0563,-0.0411,-0.0065,-0.0268,-0.0405,0.0227,-0.0807,0.0494,0.0511,0.0041,-0.0142,-0.0375,0.0448,-0.0516,0.0097,0.0338,0.0013,-0.0807,0.2179,-0.0278,0.0491,0.0106,0.02,0.0056,-0.0152,-0.0096,-0.0098,-0.0296,0.0052,-0.0618,-0.0114,0.0624,-0.0233,0.0113,0.0259,0.0487,0.0498,0.0254,-0.0292,0.0124,0.0071,0.0636,-0.0279,0.0323,-0.0811,-0.01,0.1229,-0.0232,-0.009,0.0306,-0.0518,-0.0317,-0.011,0.0244,0.0399,0.0321,-0.0527,-0.0001,0.0136,-0.0122,0.0231,0.0128,-0.1592,-0.0632,0.1018,0.0295,-0.0247,0.0187,-0.01,-0.0395,0.0124,0.0092,-0.0096,-0.0012,0.0121,0.0367,0.0742,-0.0578,-0.0264,-0.0221,-0.0243,-0.0478,0.0672,0.0147,-0.067,-0.0555,-0.0319,0.0125,-0.0393,0.0238,0.0358,-0.0333,0.0138,0.0518,0.0149,-0.0909,0.0121,0.0011,-0.0078,0.062,-0.0733,-0.0196,0.0046,0.0767,-0.0017,0.0229,-0.0385,0.0075,0.0089,-0.0126,0.0406,-0.0216,-0.0108,-0.0328,0.0052,-0.0081,0.0025,0.009,-0.0032,-0.0019,-0.0079,-0.0603,-0.0019,-0.0498,0.0165,-0.0226,-0.011,0.0532,0.0084,-0.0277,0.0484,0.0433,-0.0072,-0.0515,-0.0024,0.0088,0.0478,0.0041,0.021,0.015,0.0002,-0.0149,-0.2568,-0.0258,0.0127,-0.0218,0.0617,-0.0222,0.0814,-0.0164,0.0039,0.0333,0.0305,-0.0538,-0.0554,0.0332,-0.0346,0.0506,0.0303,0.0308,-0.0057,0.0558,-0.0023,0.0046,-0.0209,-0.1256,0.0472,-0.0099,0.2413,0.0362,0.0387,-0.0124,0.0102,0.0125,-0.0383,-0.1219,0.0762,-0.0069,0.0797,-0.0082,-0.0001,-0.0595,-0.0019,0.014,-0.0309,-0.0842,-0.0044,0.0154,-0.0215,0.0212,-0.0471,0.0033,0.0372,-0.0123,0.0154,-0.0004,-0.0469,-0.0581,-0.067,0.0397,-0.0525,0.0123,0.0087,-0.0506,0.0025,-0.0743,0.0373,0.024,0.0039,-0.0256,0.0669,0.0022,-0.0242,0.0609,0.0191,0.0061,-0.0071,0.0053,-0.0305,0.0008,-0.0642,-0.011,0.0475,-0.0179,0.0526,0.0201,0.0178,-0.0302,0.0616,-0.0516,0.0244,-0.0164,0.0096,0.0153,-0.0751,-0.0348,0.0637,0.0044,-0.2997,0.0343,0.0217,0.0103,0.0016,0.0363,0.0458,-0.0142,-0.0385,0.0159,0.01,0.0319,0.0413,0.053,0.0313,0.0358,0.0797,-0.0031,0.049,-0.0848,-0.0036,0.0229,0.218,-0.022,0.0232,0.0065,-0.0175,-0.0313,0.0382,-0.0086,0.0199,0.0106,0.0842,-0.0493,0.0464,0.0842,-0.0286,0.0148,-0.0064,-0.0128,-0.0233,0.0257,0.0337,0.021,0.0688,0.0331,-0.0339,-0.0238,-0.0472,0.0507,-0.015,0.0182,-0.009,-0.0366,0.0523,0.0531,-0.0652,-0.02,-0.0379,-0.046,0.0072,-0.064,0.0201,-0.0104,-0.0178]}
{"key":"[LSCP: Locally Selective Combination in Parallel Outlier Ensembles] In unsupervised outlier ensembles, the absence of ground truth makes the combination of base outlier detectors a challenging task. Specifically, existing parallel outlier ensembles lack a reliable way of selecting competent base detectors, affecting accuracy and stability, during model combination. In this paper, we propose a framework---called Locally Selective Combination in Parallel Outlier Ensembles (LSCP)---which addresses the issue by defining a local region around a test instance using the consensus of its nearest neighbors in randomly selected feature subspaces. The top-performing base detectors in this local region are selected and combined as the model's final output. Four variants of the LSCP framework are compared with seven widely used parallel frameworks. Experimental results demonstrate that one of these variants, LSCP_AOM, consistently outperforms baselines on the majority of twenty real-world datasets.","layer":3,"vector":[-0.041,-0.054,0.0124,0.0087,0.0613,-0.015,0.0432,0.0434,0.0198,-0.0385,0.0355,-0.055,-0.0239,0.0732,0.0234,-0.0135,0.0055,0.054,-0.0169,0.0304,-0.0424,-0.041,-0.0473,-0.0316,0.0474,0.0237,-0.0068,-0.0675,-0.0582,-0.268,-0.0131,-0.0552,0.0454,-0.0316,0.0418,0.0046,-0.0253,0.0042,-0.0335,0.0041,-0.0287,0.0084,0.0022,-0.0809,-0.0493,-0.0439,0.0218,0.0282,-0.0304,0.0116,0.069,-0.0449,0.0131,0.0289,0.0153,0.0292,0.0098,0.017,0.027,0.0617,0.0518,0.0314,-0.1486,0.0165,0.0703,0.0067,-0.0211,0.0025,0.0265,0.027,-0.0069,0.0429,0.0478,0.0371,-0.0056,0.0094,0.0103,-0.0083,-0.0212,0.0256,0.0512,-0.0248,-0.0413,-0.0149,-0.0262,-0.0568,0.0162,-0.0125,0.0587,-0.0236,-0.0216,0.029,-0.0095,0.0307,-0.0535,-0.0502,0.0466,0.0159,-0.005,0.2337,-0.0643,0.0118,0.0178,-0.0027,0.0405,-0.055,-0.0417,-0.0458,-0.0317,0.0132,0.0301,-0.0024,-0.0339,-0.0361,0.0114,0.0215,0.0764,0.0303,-0.028,0.0237,-0.0346,-0.0058,0.1066,-0.0708,0.0664,-0.0598,0.044,0.1447,0.0383,0.0019,0.0152,0.035,-0.0556,-0.0574,-0.0107,0.0151,0.009,0.0484,0.046,-0.0322,-0.0376,-0.0577,0.0092,-0.0567,-0.0252,0.1191,-0.037,0.0305,-0.0416,-0.0511,-0.0192,0.0169,-0.0102,0.0032,0.0486,-0.0104,0.029,0.0419,0.0006,0.0154,-0.017,-0.0347,0.0244,0.0862,-0.0183,-0.0826,-0.0456,-0.009,0.011,0.0014,0.0681,0.0125,-0.0253,0.0363,0.0423,0.0074,-0.0623,0.025,0.013,-0.0186,0.0181,-0.0464,-0.0549,0.0459,0.058,-0.0393,-0.0048,-0.0508,0.0373,0.0162,-0.0701,0.0068,-0.029,-0.0041,-0.0301,-0.0325,-0.0001,-0.0038,0.0442,-0.0382,0.0209,0.0408,-0.0229,0.0151,-0.0471,0.0272,0.0266,-0.0237,0.0281,0.0467,-0.011,0.014,0.0455,0.0099,-0.0327,-0.0224,0.0081,0.0305,-0.0047,0.0561,0.033,-0.0379,-0.0635,-0.2415,0.0004,0.0269,-0.0192,0.039,-0.0581,0.0035,-0.0095,0.0565,0.0842,0.0644,0.0077,-0.0153,0.0318,0.0006,0.0745,0.0011,0.0332,-0.0288,0.017,-0.0226,0.0462,-0.0514,-0.0592,0.0593,-0.0267,0.1766,0.0491,0.0304,-0.0397,0.0473,0.0464,-0.0146,-0.0683,0.0694,0.0432,0.0748,-0.0235,-0.0681,-0.012,-0.0374,0.0451,0.0153,-0.1133,-0.039,-0.0353,-0.0133,0.0114,-0.076,0.0205,0.06,-0.0516,0.0445,0.0005,0.0413,-0.0432,-0.1307,0.0227,-0.018,0.0094,0.0165,-0.0439,0.0192,-0.0814,0.0786,-0.0126,-0.0605,-0.0846,0.0663,-0.0502,-0.0392,0.063,-0.0233,0.0008,0.0178,0.029,0.0492,-0.0345,-0.0351,-0.0352,0.0947,-0.0095,-0.0218,0.0431,0.0367,-0.0062,0.0704,0.0205,0.0389,-0.0377,0.0087,0.0241,-0.0293,-0.0072,0.0423,0.0187,-0.2724,0.0003,-0.0169,0.0499,-0.0312,-0.0153,0.018,0.0086,-0.0079,0.0067,-0.0111,0.0346,0.0095,-0.0463,-0.0222,0.0524,0.0601,-0.0557,0.0537,-0.0792,0.0193,0.0654,0.2093,-0.018,0.0058,0.0564,-0.0004,-0.0363,0.0508,-0.0334,0.0327,0.0083,0.0396,-0.0263,0.025,0.097,-0.0538,-0.0005,0.023,-0.0417,-0.0142,0.0218,-0.0209,-0.049,0.1095,-0.0504,-0.0344,-0.0443,0.0034,0.0446,-0.0133,-0.0441,-0.0243,-0.0263,0.0052,0.0662,-0.0364,-0.0784,-0.05,-0.0404,0.0303,-0.0306,-0.027,0.0031,-0.0348]}
{"key":"[Residual Dynamic Mode Decomposition: Robust and verified Koopmanism] Dynamic Mode Decomposition (DMD) describes complex dynamic processes through a hierarchy of simpler coherent features. DMD is regularly used to understand the fundamental characteristics of turbulence and is closely related to Koopman operators. However, verifying the decomposition, equivalently the computed spectral features of Koopman operators, remains a major challenge due to the infinite-dimensional nature of Koopman operators. Challenges include spurious (unphysical) modes, and dealing with continuous spectra, both of which occur regularly in turbulent flows. Residual Dynamic Mode Decomposition (ResDMD), introduced by (Colbrook & Townsend 2021), overcomes some of these challenges through the data-driven computation of residuals associated with the full infinite-dimensional Koopman operator. ResDMD computes spectra and pseudospectra of general Koopman operators with error control, and computes smoothed approximations of spectral measures (including continuous spectra) with explicit high-order convergence theorems. ResDMD thus provides robust and verified Koopmanism. We implement ResDMD and demonstrate its application in a variety of fluid dynamic situations, at varying Reynolds numbers, arising from both numerical and experimental data. Examples include: vortex shedding behind a cylinder; hot-wire data acquired in a turbulent boundary layer; particle image velocimetry data focusing on a wall-jet flow; and acoustic pressure signals of laser-induced plasma. We present some advantages of ResDMD, namely, the ability to verifiably resolve non-linear, transient modes, and spectral calculation with reduced broadening effects. We also discuss how a new modal ordering based on residuals enables greater accuracy with a smaller dictionary than the traditional modulus ordering. This paves the way for greater dynamic compression of large datasets without sacrificing accuracy.","layer":2,"vector":[-0.0518,0.0319,0.0837,0.0007,0.0273,0.0399,0.0075,-0.0026,0.0306,-0.0158,-0.0179,-0.0467,0.0323,0.0337,-0.0012,0.0097,0.0268,0.0642,-0.0253,0.0106,0.0175,-0.027,-0.0031,-0.0234,0.0453,0.0157,-0.0534,-0.0429,-0.0302,-0.2555,0.0278,-0.0368,-0.006,-0.0626,0.0471,-0.0425,-0.0613,0.0592,-0.0221,0.0113,0.0409,-0.0065,-0.0032,-0.0306,-0.04,-0.0212,-0.0219,0.024,0.013,-0.032,-0.0007,-0.0228,-0.0024,0.0018,0.0268,0.0372,0.0544,0.0451,0.0594,0.0281,0.0024,0.0144,-0.214,0.09,0.0495,0.0198,-0.0006,-0.0135,0.0395,0.0254,-0.0591,0.0188,0.013,-0.0072,0.0217,-0.0275,-0.0357,-0.0498,-0.0182,-0.0012,0.0263,-0.0422,-0.0307,-0.045,-0.0172,0.0082,0.0772,-0.0589,0.0339,0.0071,-0.058,-0.0627,0.0117,0.0213,-0.0559,0.0134,0.0331,0.0301,0.0142,0.1894,-0.0538,-0.0313,-0.0128,0.011,0.0184,-0.0258,-0.0614,-0.0192,-0.0029,0.0057,-0.0104,-0.0065,0.081,-0.0466,0.0316,-0.0616,0.0221,-0.0145,-0.0338,0.0174,0.0024,0.0015,0.0276,-0.0083,0.044,-0.0671,0.0422,0.1264,0.055,0.0518,0.0322,0.0314,-0.0341,-0.0384,0.0079,-0.0143,0.0238,-0.0013,0.0169,0.0323,-0.0466,-0.0532,-0.0433,-0.1117,-0.0036,0.1244,-0.0483,0.0448,-0.0422,0.0294,-0.013,0.0457,-0.0421,-0.0141,0.0212,0.0475,-0.0359,0.0851,-0.0543,0.0384,-0.0349,-0.0895,-0.0702,0.0727,-0.0403,-0.0656,-0.0279,0.0117,0.0216,0.0075,-0.0206,0.0297,-0.0471,0.0063,0.076,0.025,-0.0417,0.0098,0.014,0.0287,0.0646,-0.0624,-0.01,0.0177,0.0204,-0.0608,-0.0228,-0.0117,0.0277,0.0339,-0.0541,-0.0352,0.0044,-0.0274,-0.0416,0.0024,0.0054,-0.0354,0.0025,-0.0734,0.036,-0.0078,-0.0308,0.0617,-0.0449,0.0469,-0.0114,0.0259,-0.0054,0.0231,-0.0362,0.0169,0.0648,-0.0306,-0.0135,-0.0143,0.0204,0.0275,-0.02,0.0476,0.0362,-0.062,-0.0869,-0.2391,0.0096,-0.0018,0.0157,0.0747,-0.0481,0.0317,-0.0126,0.0719,0.012,0.0225,0.0333,0.0009,-0.0321,0.0224,0.0427,0.0398,-0.023,-0.0768,-0.0218,-0.0194,0.0428,-0.0761,-0.0521,0.0463,-0.0127,0.2018,0.0057,0.0535,-0.0098,0.0153,0.0321,-0.0192,-0.0539,0.0521,0.0313,0.0854,-0.005,-0.0051,-0.0235,-0.0039,0.0025,-0.0121,-0.0632,-0.003,-0.0378,-0.0067,0.0348,-0.0371,-0.0078,0.0505,-0.0193,0.0713,-0.0382,0.0331,-0.0247,-0.0574,0.0167,-0.0199,-0.0064,-0.0013,-0.0164,0.0011,-0.0358,0.0537,0.0006,0.0278,-0.0114,0.0508,-0.0721,-0.0264,0.0864,-0.0274,0.0276,0.0951,0.0208,0.0025,0.004,-0.0118,-0.0149,0.1192,-0.0378,0.077,0.0368,0.0615,-0.0265,0.0478,0.0036,-0.035,-0.0547,-0.0185,0.021,-0.0522,0.0045,-0.0086,0.0375,-0.3117,0.0118,0.0137,-0.0127,-0.0361,0.0113,0.0363,-0.0254,-0.0386,-0.0106,-0.0805,0.0805,0.0439,-0.0042,0.0508,0.0452,0.0399,-0.0639,0.0377,-0.0157,0.0089,0.0549,0.2288,-0.058,0.0128,0.0248,0.0055,0.0238,-0.0116,-0.0283,0.0454,-0.0039,0.1134,-0.0751,0.0181,0.0183,-0.021,0.0534,0.0369,-0.0454,0.0222,0.0226,0.0197,-0.046,0.0646,-0.0114,-0.0138,-0.0396,0.0182,0.0124,-0.0201,0.0086,0.0056,0.0268,0.0345,0.0617,-0.066,-0.0291,0.0105,-0.0243,-0.0201,-0.0369,-0.0581,-0.0464,0.0054]}
{"key":"[Deep Reinforcement Learning using Cyclical Learning Rates] Deep Reinforcement Learning (DRL) methods often rely on the meticulous tuning of hyperparameters to successfully resolve problems. One of the most influential parameters in optimization procedures based on stochastic gradient descent (SGD) is the learning rate. We investigate cyclical learning and propose a method for defining a general cyclical learning rate for various DRL problems. In this paper we present a method for cyclical learning applied to complex DRL problems. Our experiments show that, utilizing cyclical learning achieves similar or even better results than highly tuned fixed learning rates. This paper presents the first application of cyclical learning rates in DRL settings and is a step towards overcoming manual hyperparameter tuning.","layer":2,"vector":[-0.0626,-0.0117,0.0133,-0.0272,0.004,0.0031,-0.0055,0.011,0.0622,-0.0243,0.0434,-0.0171,0.0478,0.0902,0.0031,0.0133,-0.018,0.0166,-0.0075,0.0146,0.0582,-0.0655,-0.0126,-0.0862,0.0022,-0.0181,-0.0737,-0.0763,-0.0527,-0.2478,0.0236,0.0086,-0.0092,-0.0262,-0.0121,-0.0375,-0.0301,0.0728,-0.0372,0.064,0.0242,0.047,-0.0158,-0.048,0.0053,-0.0435,-0.0183,-0.0349,0.0107,-0.0077,0.0472,-0.0232,-0.0157,0.0116,-0.0022,0.0257,0.0592,0.1106,0.0695,0.0146,-0.0063,0.0261,-0.1627,0.0438,0.0275,0.0401,-0.0382,-0.0259,0.0158,0.052,-0.0391,0.059,0.0082,0.0227,0.0023,0.0101,-0.0355,-0.0528,0.013,0.021,0.036,-0.0526,-0.0101,-0.0291,-0.0045,-0.0729,0.0146,-0.0425,0.0667,0.0187,-0.0294,0.0082,-0.0057,-0.0176,-0.0596,0.035,-0.0082,0.0244,-0.1085,0.1685,-0.0409,0.0287,0.0179,-0.0125,0.0307,-0.0294,-0.0351,-0.0097,-0.0561,-0.0044,-0.0334,-0.0143,0.0757,-0.0197,-0.0192,0.0302,0.0304,0.0453,-0.0275,0.0013,0.0064,-0.026,0.0574,0.0126,0.0287,-0.0653,0.0176,0.1468,0.0209,0.003,0.0431,-0.0494,-0.0298,-0.0347,0.0244,0.0338,0.0355,-0.0387,0.0166,-0.0043,-0.0652,0.0029,0.0294,-0.09,-0.0379,0.091,0.0028,0.0095,-0.0458,-0.0098,-0.0205,0.0115,0.0037,-0.0565,0.0472,0.0531,0.0294,0.0432,-0.0291,0.0082,-0.0357,-0.051,-0.0089,0.123,-0.0013,-0.0695,-0.0433,-0.0219,-0.0164,-0.0042,0.0514,0.0662,-0.0608,0.0236,0.1001,0.0579,-0.1064,0.0079,0.0035,0.0176,0.0475,-0.0314,-0.0353,0.0142,0.0829,-0.0325,0.0062,-0.0979,0.0212,-0.0008,-0.0086,0.0194,-0.0217,0.0205,-0.0413,-0.0277,-0.0104,-0.0193,0.0148,0.0017,-0.0087,-0.033,-0.0306,0.0483,-0.0079,0.0152,-0.0499,0.0059,0.0939,0.0205,-0.0205,0.0337,0.0576,-0.036,-0.0165,0.0085,-0.0089,0.0276,-0.0447,0.0308,0.0429,-0.0052,-0.0083,-0.2202,-0.0143,0.0232,-0.0306,0.0765,-0.0479,0.0463,-0.0182,0.0529,0.0391,0.0436,-0.0397,-0.0422,-0.0107,0.0004,0.0563,0.0714,0.0093,-0.0024,-0.0094,0.0358,0.0124,-0.0096,-0.1123,0.0337,0.0002,0.2096,0.0043,0.0472,-0.0201,0.0382,-0.0006,0.0072,-0.0982,0.0357,-0.0005,0.0923,0.0067,-0.0131,-0.0523,-0.0091,0.0402,-0.0257,-0.072,-0.0536,-0.0388,-0.0413,0.0711,-0.065,-0.0193,0.0122,-0.0411,0.0115,-0.0325,-0.0248,-0.0478,-0.0902,0.0289,-0.0379,0.0155,0.0001,-0.0411,0.0227,-0.0392,0.0469,0.0024,-0.0011,-0.0595,0.0095,0.0143,-0.0113,0.0579,0.0258,0.0028,0.057,0.0129,-0.0061,-0.002,-0.0494,-0.0015,0.0452,-0.0161,0.0412,0.0463,-0.0081,-0.0048,0.0516,0.0024,0.0842,0.011,0.0104,0.0382,-0.0526,0.0082,0.013,-0.0214,-0.2881,0.0766,0.0029,0.0323,-0.033,0.0295,0.0521,-0.0104,-0.0418,-0.0126,0.0175,0.0678,0.0573,0.0251,0.0528,0.0529,0.0693,-0.0168,0.0452,-0.0599,0.0323,0.0508,0.2544,-0.0658,0.0255,-0.0169,-0.0456,0.0302,0.0304,-0.0374,-0.0386,-0.0121,0.0721,-0.0214,0.0408,0.0854,-0.019,0.0318,0.009,0.0139,0.0045,0.0201,-0.0137,-0.0071,0.1005,-0.0627,-0.0423,-0.0489,-0.0183,0.0459,-0.0461,-0.0046,-0.0189,-0.019,0.0404,0.0104,-0.0632,-0.0496,-0.0392,-0.0445,-0.0196,-0.0709,0.0009,-0.0072,-0.0095]}
{"key":"[Romanian Diacritics Restoration Using Recurrent Neural Networks] Diacritics restoration is a mandatory step for adequately processing Romanian texts, and not a trivial one, as you generally need context in order to properly restore a character. Most previous methods which were experimented for Romanian restoration of diacritics do not use neural networks. Among those that do, there are no solutions specifically optimized for this particular language (i.e., they were generally designed to work on many different languages). Therefore we propose a novel neural architecture based on recurrent neural networks that can attend information at different levels of abstractions in order to restore diacritics.","layer":5,"vector":[-0.0998,-0.0103,0.0281,-0.0102,-0.0119,0.0141,-0.004,0.0323,0.007,-0.0348,-0.0138,-0.0715,0.0212,0.0422,0.001,-0.0345,0.0084,0.0676,-0.0202,-0.0134,0.0107,-0.0323,0.0093,-0.026,-0.0076,-0.0024,-0.034,-0.0446,-0.0441,-0.24,-0.0083,-0.0102,0.0334,-0.0184,-0.0202,0.0183,-0.0477,0.053,0.0042,0.0499,0.0024,0.0261,-0.0192,-0.0803,0.0101,-0.041,-0.0323,-0.0165,-0.0115,-0.0153,0.0311,-0.0091,0.0027,0.0516,0.0273,0.0482,0.1103,0.0531,0.0367,0.0539,0.0037,0.0183,-0.1866,0.0667,0.0065,0.0197,0.0021,-0.0228,0.0075,0.059,-0.0343,0.0606,0.0345,0.0533,0.0022,-0.013,0.0022,-0.0339,-0.0048,0.0207,0.0091,0.0133,-0.0317,-0.0383,-0.0225,-0.0359,-0.0056,-0.0113,0.012,-0.0323,-0.033,-0.0164,0.0006,0.047,-0.0924,-0.0379,0.0271,0.0261,-0.0272,0.2273,-0.0499,-0.0331,0.0499,-0.0495,0.0241,0.0095,-0.0022,-0.0272,-0.0499,0.0163,-0.0233,-0.0329,0.0363,-0.0311,0.0838,0.0228,0.0542,0.022,-0.0225,0.0053,-0.0549,-0.0003,0.003,-0.0071,0.0457,-0.0498,0.0439,0.1143,0.0299,0.0788,0.0352,-0.0135,-0.0511,-0.0194,0.0206,-0.001,-0.0041,-0.0239,-0.0132,-0.0372,-0.0422,-0.0808,-0.0119,-0.0257,-0.0734,0.0886,-0.0602,0.0118,-0.0181,-0.004,-0.0518,0.0273,-0.0142,-0.0513,0.0107,0.0174,0.0367,0.0331,-0.0297,0.0183,0.01,-0.0623,-0.0563,0.1254,0.0315,-0.0682,-0.046,0.0281,0.0089,-0.0056,0.0422,0.0323,-0.0181,0.0342,0.0672,0.0867,-0.0444,-0.0345,-0.0194,0.0506,0.0325,-0.0533,-0.0348,0.0763,-0.0068,-0.044,-0.004,-0.0533,0.0589,0.0327,-0.0304,0.0441,-0.0268,-0.0358,-0.0256,0.0017,-0.0089,-0.02,-0.024,-0.0367,-0.0069,0.0109,-0.0056,-0.0387,0.042,0.0146,-0.0159,-0.0059,0.0423,0.0248,-0.0214,0.004,0.0657,-0.0567,-0.0366,-0.0312,0.0151,0.0045,-0.0011,0.0662,0.0315,-0.0565,-0.0485,-0.2063,0.0393,0.0143,-0.0576,0.0741,-0.0852,0.0367,-0.0063,0.0943,0.0234,0.0171,-0.0099,-0.004,0.0378,-0.0076,0.0645,0.0505,0.0295,-0.0391,-0.0151,0.0043,-0.008,0.0641,-0.0956,0.0346,-0.0094,0.2159,0.0261,0.0646,-0.027,0.019,0.0266,-0.0572,-0.1099,0.0807,0.018,0.0657,0.0452,-0.0338,-0.0378,0.031,0.0204,0.0004,-0.0566,-0.0469,-0.0033,-0.0429,-0.0159,-0.0427,0.0364,0.0345,-0.0021,0.0067,0.0512,-0.0287,-0.0084,-0.1304,0.0001,-0.055,-0.013,0.016,-0.0483,0.0373,-0.0314,0.0617,0.0548,0.0309,-0.0401,0.0436,-0.0116,-0.0484,0.1043,-0.0211,-0.0214,0.0501,0.0087,0.0033,-0.0427,-0.04,-0.0173,0.0547,-0.0412,0.069,0.0402,0.0349,-0.005,0.0848,-0.046,0.0022,0.0112,0.021,0.0114,-0.0525,0.0175,0.027,-0.0138,-0.3191,0.0614,0.0142,0.0419,0.005,0.0135,-0.0144,0.0543,-0.0145,-0.0204,-0.0005,0.0051,0.0463,-0.0435,-0.0113,0.0448,0.0675,-0.0634,0.0348,-0.0548,0.0092,0.0491,0.2171,-0.0606,0.0235,-0.0241,-0.0102,-0.0205,0.02,0.0184,0.0154,0.0089,0.0797,-0.0244,0.0024,0.0725,-0.0406,0.0193,0.0352,0.0079,-0.0476,0.047,-0.0561,-0.0534,0.0738,-0.0344,-0.0293,-0.0337,0.0003,0.0047,-0.0251,-0.0003,-0.0055,-0.0022,0.0416,0.0351,-0.0896,-0.0321,-0.0148,0.0115,0.0415,-0.0547,-0.0116,0.0205,0.0087]}
{"key":"[Multi-task manifold learning for small sample size datasets] In this study, we develop a method for multi-task manifold learning. The method aims to improve the performance of manifold learning for multiple tasks, particularly when each task has a small number of samples. Furthermore, the method also aims to generate new samples for new tasks, in addition to new samples for existing tasks. In the proposed method, we use two different types of information transfer: instance transfer and model transfer. For instance transfer, datasets are merged among similar tasks, whereas for model transfer, the manifold models are averaged among similar tasks. For this purpose, the proposed method consists of a set of generative manifold models corresponding to the tasks, which are integrated into a general model of a fiber bundle. We applied the proposed method to artificial datasets and face image sets, and the results showed that the method was able to estimate the manifolds, even for a tiny number of samples.","layer":2,"vector":[0.0033,-0.0479,0.0295,0.0135,0.0073,0.0463,0.0111,0.042,-0.0128,-0.0033,0.0148,-0.0966,-0.0215,0.0559,0.0285,0.0331,0.0329,0.0389,-0.0673,0.0042,0.038,-0.028,-0.0038,-0.0203,-0.0237,-0.0179,0.0071,-0.089,-0.0299,-0.2434,0.0617,-0.0451,0.0453,-0.0049,0.0205,-0.0504,-0.0393,0.0412,-0.042,0.0603,0.0061,0.0489,-0.051,-0.0326,-0.0604,-0.0703,-0.043,-0.004,-0.0008,-0.016,0.0159,-0.0619,0.0198,0.0346,0.04,0.0228,0.0536,0.0281,0.0649,0.0708,0.0295,0.0068,-0.1597,0.0632,0.0286,0.028,-0.0569,-0.009,0.0095,0.0525,-0.044,0.0283,0.0044,0.0442,0.0254,-0.0109,0.0158,-0.0109,-0.0195,0.0304,0.0039,0.0098,-0.0181,-0.023,-0.0481,-0.0031,0.0125,-0.0571,0.027,0.0137,-0.0281,-0.0325,-0.0675,0.0449,-0.0628,-0.0373,0.0238,0.0502,-0.0006,0.1903,-0.0827,0.0262,0.0561,-0.0354,0.0303,-0.0416,-0.0471,-0.0325,-0.0226,-0.0116,-0.0005,-0.041,-0.0219,-0.0373,0.0368,-0.0029,0.0815,0.0803,-0.022,-0.0407,-0.0313,0.0136,0.065,-0.0388,0.0368,-0.0575,0.01,0.1284,0.0788,0.0218,0.0287,-0.0222,-0.0153,-0.0451,-0.0094,-0.0265,0.0194,0.0455,0.0211,-0.0008,-0.0187,-0.067,0.0253,-0.0814,-0.0575,0.1537,-0.0249,0.0402,-0.0347,-0.0015,-0.009,0.0647,-0.0337,-0.0189,-0.0044,0.0008,0.0647,0.0172,-0.0342,0.0241,-0.0112,-0.059,-0.0494,0.0841,0.003,-0.0911,-0.0475,-0.02,0.0211,-0.0009,0.0426,0.0424,-0.0371,0.0369,0.1025,0.0645,-0.0652,-0.0124,0.0072,0.0365,0.0414,-0.0606,-0.0365,-0.0175,-0.0331,-0.004,0.029,-0.0582,0.049,0.0183,-0.005,0.0118,0.0134,0.0038,-0.0359,-0.0345,-0.0084,-0.0086,0.0119,-0.013,0.0182,0.0037,-0.0536,0.0364,-0.0446,0.0173,-0.0057,0.0012,0.0406,0.0328,-0.0073,-0.0243,0.0448,-0.0231,-0.0142,-0.0186,-0.0018,0.0308,-0.0018,0.021,0.0256,-0.0348,-0.0202,-0.2309,0.0209,-0.002,0.0221,0.0297,-0.0586,0.0715,0.0003,0.07,0.0707,0.0684,0.0046,-0.0175,0.0407,-0.0135,0.0458,-0.0046,0.0355,-0.0472,-0.0498,0.0121,0.0229,0.0005,-0.1166,0.051,-0.0566,0.2252,0.0143,-0.0215,-0.0391,0.0169,0.0536,-0.0143,-0.1165,0.0653,-0.0014,0.035,-0.0238,-0.0489,0.0095,-0.0248,-0.0125,0.0395,-0.0962,-0.0041,-0.0193,-0.0683,-0.0057,-0.0468,0.0202,0.0514,-0.014,0.0151,-0.0316,-0.0668,-0.0342,-0.0868,0.0102,-0.0481,0.0291,-0.0136,-0.0484,0.0529,-0.0583,0.0786,0.0287,-0.0307,-0.0779,0.024,-0.0092,-0.0278,0.0852,0.0209,0.0159,0.0452,-0.0037,0.0226,0.0182,0.0024,-0.061,0.0498,-0.0487,-0.0042,0.0427,0.0617,0.0378,0.0995,-0.0082,0.0145,-0.0658,0.0299,-0.0007,-0.0172,0.0187,0.0579,0.0207,-0.2773,0.02,-0.0243,0.0579,-0.024,-0.0008,0.0126,0.0018,0.0035,-0.0306,0.0177,0.0431,0.0543,0.0239,0.02,0.0648,0.0523,-0.024,0.0272,-0.0323,-0.0058,0.0385,0.2265,-0.014,0.0059,0.0302,-0.0353,-0.0229,0.0329,0.0054,0.01,0.0421,0.0832,-0.0335,0.0225,0.0746,-0.0433,0.0094,0.0074,-0.0212,0.027,-0.0039,-0.033,-0.0269,0.1094,-0.0141,0.0045,-0.0417,-0.0598,0.0468,0.0282,-0.012,0.0295,-0.0298,0.0169,0.0146,-0.039,-0.0532,-0.0224,-0.0139,0.0069,-0.059,-0.0715,-0.0581,-0.0181]}
{"key":"[HAKE: Human Activity Knowledge Engine] Human activity understanding is crucial for building automatic intelligent system. With the help of deep learning, activity understanding has made huge progress recently. But some challenges such as imbalanced data distribution, action ambiguity, complex visual patterns still remain. To address these and promote the activity understanding, we build a large-scale Human Activity Knowledge Engine (HAKE) based on the human body part states. Upon existing activity datasets, we annotate the part states of all the active persons in all images, thus establish the relationship between instance activity and body part states. Furthermore, we propose a HAKE based part state recognition model with a knowledge extractor named Activity2Vec and a corresponding part state based reasoning network. With HAKE, our method can alleviate the learning difficulty brought by the long-tail data distribution, and bring in interpretability. Now our HAKE has more than 7 M+ part state annotations and is still under construction. We first validate our approach on a part of HAKE in this preliminary paper, where we show 7.2 mAP performance improvement on Human-Object Interaction recognition, and 12.38 mAP improvement on the one-shot subsets.","layer":0,"vector":[-0.0405,-0.0116,0.0365,-0.0343,0.0305,0.0294,0.0718,-0.0037,0.0234,-0.0067,0.0052,-0.0883,0.0258,0.0681,0.0227,-0.027,0.0263,0.0646,-0.044,0.0083,0.0185,-0.0187,-0.0159,-0.0018,-0.027,0.0181,-0.0343,-0.0166,-0.0451,-0.2258,0.0362,-0.0353,0.0722,0.0077,-0.0015,-0.0698,-0.038,0.072,-0.041,0.031,0.0168,0.0271,0.0003,-0.0339,0.0148,-0.0512,-0.0366,-0.0186,-0.0359,-0.0493,0.0151,-0.018,0.0248,0.0551,0.0614,0.0187,0.0734,0.0758,-0.0039,0.0099,0.0574,0.0151,-0.168,0.0338,0.0219,0.0232,-0.0293,-0.0469,0.044,0.0121,-0.0128,0.0601,0.048,0.0718,0.0044,-0.0378,0.0036,-0.0077,-0.0117,-0.0359,-0.0213,-0.0157,-0.0136,0.0094,0.0353,-0.0563,0.0009,-0.0473,0.0095,0.0387,-0.0625,0.0009,-0.0236,0.0158,-0.0038,-0.0078,0.0056,0.0298,-0.056,0.1993,-0.0838,0.0429,0.0316,-0.014,-0.0152,0.0123,-0.0094,-0.0364,-0.0217,-0.0202,0.0198,0.0067,0.0198,0.0019,0.0581,-0.015,0.0817,0.0505,-0.0236,-0.0083,0.0218,0.0142,0.0665,-0.0216,-0.0037,-0.0605,0.0568,0.1477,0.0272,0.0198,0.061,0.0069,-0.0279,-0.0418,0.0464,0.0383,0.0251,-0.032,0.0372,-0.0124,0.0066,-0.0561,0.0603,-0.1081,-0.0436,0.0876,-0.0011,0.0156,-0.0417,-0.0122,-0.0361,0.054,-0.0495,-0.0073,0.013,0.0487,0.0802,0.0106,-0.0414,-0.0279,-0.0195,-0.0853,-0.0331,0.0428,0.0211,-0.102,-0.0378,-0.0078,0.0029,-0.0152,0.0395,0.0338,-0.0176,0.055,0.1421,0.06,-0.0729,0.0314,-0.0054,0.0086,0.0488,-0.0656,-0.0422,0.002,0.0364,-0.0619,-0.0018,-0.0301,0.0377,0.0706,-0.0232,0.0389,-0.0156,0.0099,-0.0153,-0.0198,-0.0575,0.0003,0.0059,-0.0362,0.0391,0.011,-0.05,0.0174,-0.0186,0.042,-0.0819,0.0212,0.0403,0.0383,-0.0149,-0.0204,0.036,-0.0085,0.0019,-0.0297,0.0188,0.0026,-0.0225,0.0133,0.0131,-0.0078,-0.0353,-0.2305,-0.0024,0.0205,-0.0394,-0.0112,0.0038,0.0177,-0.0173,0.0756,0.0515,0.0677,-0.0358,-0.049,0.0131,-0.0064,0.0623,0.0251,0.0421,-0.0543,0.0125,-0.0019,-0.0016,-0.0413,-0.0967,0.039,-0.0079,0.2312,0.0488,0.0251,-0.0194,0.004,0.0638,-0.0325,-0.128,0.0845,-0.0012,0.0658,-0.0234,-0.02,-0.0445,-0.07,0.0085,-0.0273,-0.0999,-0.0195,-0.0351,0.0053,0.0154,-0.0357,-0.0073,0.053,-0.0432,0.0231,-0.0126,-0.044,-0.0094,-0.0609,0.0142,-0.0765,0.0405,-0.0038,-0.0289,-0.0265,-0.0482,0.0796,0.0163,-0.0732,-0.0516,0.0252,-0.0604,-0.0688,0.0914,-0.0528,0.0041,0.0652,0.0056,0.0396,-0.0059,-0.0149,0.0057,0.0474,-0.026,-0.0009,-0.0044,0.0482,-0.0301,0.0925,-0.0461,0.014,-0.0665,0.0641,0.0198,-0.0348,-0.0323,0.0169,-0.042,-0.2776,0.0091,-0.005,0.0316,-0.0624,0.0165,0.0946,0.0003,-0.0135,-0.0082,-0.0211,0.0631,0.0074,-0.0027,-0.0099,0.044,0.0726,-0.0503,0.0414,-0.0811,0.0434,0.0487,0.2105,-0.0262,0.0219,0.0037,-0.013,-0.0473,0.0529,-0.0234,-0.0453,-0.0329,0.0812,-0.0204,0.0282,0.0527,-0.0223,-0.0176,0.0431,0.023,0.0199,-0.0075,-0.0256,-0.0455,0.0749,0.025,-0.0224,-0.0707,0.0269,0.0025,0.0008,-0.0275,-0.0297,0.0249,0.0379,0.0309,-0.0029,-0.0329,-0.041,-0.0303,0.0101,-0.0254,0.0338,-0.0062,-0.0207]}
{"key":"[Induced Domain Adaptation] We formulate the problem of induced domain adaptation (IDA) when the underlying distribution/domain shift is introduced by the model being deployed. Our formulation is motivated by applications where the deployed machine learning models interact with human agents, and will ultimately face responsive and interactive data distributions. We formalize the discussions of the transferability of learning in our IDA setting by studying how the model trained on the available source distribution (data) would translate to the performance on the induced domain. We provide both upper bounds for the performance gap due to the induced domain shift, as well as lower bound for the trade-offs a classifier has to suffer on either the source training distribution or the induced target distribution. We provide further instantiated analysis for two popular domain adaptation settings with covariate shift and label shift. We highlight some key properties of IDA, as well as computational and learning challenges.","layer":3,"vector":[-0.022,-0.0113,0.0172,-0.0194,0.0478,0.0024,0.0198,0.0216,0.0011,-0.0198,0.0221,-0.0274,-0.0082,0.0446,0.03,0.0504,-0.0055,0.0415,-0.0679,-0.0114,0.0373,-0.0017,0.0164,-0.0024,-0.0036,0.0081,-0.0134,-0.0462,-0.0403,-0.238,0.0646,-0.0322,0.0246,-0.0056,0.029,-0.0468,-0.0793,0.0382,0.0058,0.0424,0.0137,0.0303,0.0034,-0.0571,-0.0281,-0.0574,-0.0238,0.0105,-0.0571,-0.0192,0.0068,-0.0517,0.03,0.0261,0.0278,0.0477,0.0514,0.0621,0.0571,0.0509,-0.0104,0.1159,-0.1391,0.074,0.0576,0.0463,-0.0496,-0.047,-0.0036,0.06,-0.0252,0.0694,0.0177,0.0283,0.0161,0.0048,-0.0007,-0.0009,0.0209,0.0011,0.0444,-0.0393,-0.0496,0.0038,-0.0187,-0.0751,0.0761,-0.0547,0.0578,0.0541,-0.0644,0.0014,-0.0076,0.0486,-0.0483,-0.0113,0.0407,-0.0117,-0.0441,0.1941,-0.011,0.0222,-0.0176,-0.0446,0.0041,-0.0042,-0.0065,-0.0106,-0.0031,-0.0125,-0.0428,-0.009,-0.0059,-0.0129,0.0199,-0.032,0.0872,0.0152,0.0032,-0.0275,-0.0335,0.0102,0.0505,-0.0088,0.0143,-0.0039,0.0413,0.1306,0.0059,-0.0153,0.0051,-0.0366,-0.0584,-0.0078,0.0126,0.0414,0.0165,-0.0117,0.0186,0.0392,-0.0253,-0.0536,0.0087,-0.0721,-0.0292,0.139,-0.0306,0.0386,-0.0395,-0.0046,-0.0165,0.0184,-0.0446,-0.0319,-0.0314,0.0506,0.0342,0.0063,-0.0481,-0.0107,-0.005,-0.0779,0.0049,0.0784,-0.01,-0.0646,-0.0617,-0.0026,0.0527,-0.0069,0.0482,0.0472,-0.0219,0.0525,0.0495,0.0104,-0.0825,-0.0188,0.0321,0.0015,0.0499,-0.057,-0.0002,0.0744,0.0529,-0.0474,-0.0148,-0.0375,0.0145,0.0532,-0.005,0.0127,-0.0414,0.0219,-0.0521,-0.0281,-0.0198,0.0137,0.0105,-0.0261,-0.0109,0.037,-0.0357,0.0121,-0.0689,-0.0149,-0.0163,-0.0089,0.0228,0.0162,-0.0246,0.0164,0.0578,-0.0496,-0.0353,0.0091,0.0437,0.0484,0.0343,0.0358,0.0171,0.0019,0.0229,-0.2643,-0.0089,0.0038,-0.0217,0.0273,-0.0473,0.0632,0.0124,0.0607,0.0776,0.0574,-0.0504,-0.0569,0.0236,-0.0314,0.0048,0.0179,-0.0281,-0.0074,-0.0493,-0.0196,0.0075,0.0027,-0.1235,0.0422,-0.0021,0.2025,0.0268,0.0288,-0.0516,-0.0282,0.0263,0.0013,-0.1307,0.0309,0.0272,0.0656,-0.0219,-0.0349,-0.0081,0.0059,0.0253,0.0434,-0.1371,-0.024,-0.0175,-0.0291,0.0047,-0.0774,0.034,0.0253,-0.0222,0.0256,-0.0199,-0.0517,-0.0275,-0.064,0.046,-0.0456,0.0297,0.0013,-0.0341,0.0555,-0.0886,0.0366,-0.0275,-0.0263,-0.0503,0.0308,-0.0387,-0.0602,0.0779,0.0107,0.0209,0.0398,-0.0064,-0.0165,-0.0348,-0.0734,-0.0078,0.056,-0.009,0.0213,0.0291,0.0427,0.0243,0.0908,-0.0104,0.0305,0.0002,-0.0027,0.0241,-0.0675,-0.0158,0.053,-0.0529,-0.2925,0.0068,0.0177,0.0526,-0.0016,0.0237,0.0327,0.0193,-0.0694,-0.0157,0.0018,0.0314,0.0266,0.0095,0.0141,0.0482,0.0718,-0.0602,0.0049,-0.1068,0.0085,0.0428,0.2203,-0.0399,0.0732,-0.0007,-0.0475,0.0116,0.0145,-0.0288,-0.0032,0.0174,0.0769,-0.0404,0.0349,0.087,-0.0702,0.0318,0.0178,-0.018,-0.0016,0.0114,-0.0351,0.0132,0.0917,0.0321,0.0214,-0.0515,-0.012,0.0425,0.0138,-0.0177,0.0393,0.025,0.0402,-0.039,-0.0269,-0.0364,-0.0646,-0.0191,0.056,-0.0865,-0.0099,-0.0033,-0.0334]}
{"key":"[Compiler-Level Matrix Multiplication Optimization for Deep Learning] An important linear algebra routine, GEneral Matrix Multiplication (GEMM), is a fundamental operator in deep learning. Compilers need to translate these routines into low-level code optimized for specific hardware. Compiler-level optimization of GEMM has significant performance impact on training and executing deep learning models. However, most deep learning frameworks rely on hardware-specific operator libraries in which GEMM optimization has been mostly achieved by manual tuning, which restricts the performance on different target hardware. In this paper, we propose two novel algorithms for GEMM optimization based on the TVM framework, a lightweight Greedy Best First Search (G-BFS) method based on heuristic search, and a Neighborhood Actor Advantage Critic (N-A2C) method based on reinforcement learning. Experimental results show significant performance improvement of the proposed methods, in both the optimality of the solution and the cost of search in terms of time and fraction of the search space explored. Specifically, the proposed methods achieve 24% and 40% savings in GEMM computation time over state-of-the-art XGBoost and RNN methods, respectively, while exploring only 0.1% of the search space. The proposed approaches have potential to be applied to other operator-level optimizations.","layer":4,"vector":[-0.0514,-0.0035,0.0329,-0.0266,-0.0138,0.0259,-0.0175,0.0312,0.0447,0.0194,0.0221,-0.0089,0.0185,0.0462,0.0362,0.0016,0.0061,0.0373,-0.0437,-0.0352,0.0101,-0.0586,-0.0049,-0.067,0.0491,0.0089,-0.0509,-0.0364,-0.0325,-0.2628,0.0142,-0.0161,0.0923,-0.0297,-0.0158,-0.0571,-0.0077,0.0708,-0.0433,0.0106,-0.0084,0.0339,-0.0057,0.0258,-0.0229,-0.0443,-0.0194,-0.0471,-0.0227,-0.0203,0.0054,-0.0093,-0.0011,0.0452,0.0276,-0.0052,0.0844,0.0732,0.0065,0.0088,0.0033,0.026,-0.1615,0.0687,0.0534,0.0109,-0.0227,-0.05,0.0336,0.0847,-0.0307,0.059,0.0357,0.0025,0.0002,0.0167,-0.0088,-0.0165,0.0375,-0.0248,0.0157,-0.0247,-0.0269,-0.0252,-0.0174,-0.0656,0.014,-0.0062,0.0461,0.0118,-0.0223,-0.0052,-0.0317,0.0206,-0.0516,0.0005,0.0113,0.0172,-0.1177,0.238,-0.0295,0.0356,0.013,-0.0007,0.0246,-0.031,-0.0299,0.0192,-0.0504,-0.0624,0.0254,-0.0253,0.0462,-0.0234,0.0433,0.034,0.0403,0.0579,-0.0279,0.0292,-0.0343,-0.0471,0.0364,-0.0104,0.0148,-0.0769,-0.0291,0.1414,0.0356,0.0438,0.0467,-0.0158,-0.0191,-0.0477,0.0383,0.0445,0.0253,-0.0135,-0.0057,-0.0138,-0.0494,0.028,-0.0056,-0.0532,-0.0294,0.0837,-0.0241,-0.0035,-0.028,-0.0306,-0.0194,0.009,-0.0238,-0.0179,0.0216,0.0102,0.0209,0.0491,-0.07,0.023,-0.0743,-0.0433,-0.0101,0.0882,0.01,-0.0689,-0.0201,-0.0035,0.0118,-0.0232,0.0598,0.0355,-0.0978,-0.0033,0.0681,0.0296,-0.0648,-0.0036,-0.0454,0.0087,0.0516,-0.0375,-0.0403,0.0051,0.0589,-0.0688,0.0475,-0.0414,-0.0377,-0.0251,-0.0699,0.0673,-0.0618,0.008,-0.0389,-0.0278,0.0031,-0.0063,0.0384,0.0298,0.0233,0.0174,-0.0376,0.0171,0.0054,-0.0005,0.008,-0.0386,0.0242,0.0349,-0.016,-0.0103,0.0846,-0.0204,-0.0556,-0.0004,0.0068,0.0243,-0.0258,0.0745,0.026,-0.0366,-0.0468,-0.2278,-0.0066,-0.0125,-0.0564,0.0793,-0.0806,0.0546,-0.0254,0.0123,0.0916,0.0452,-0.0172,0.0194,0.0369,-0.0155,0.0467,0.053,0.0478,-0.0204,-0.0049,0.0001,0.0245,0.0081,-0.0802,0.0372,-0.001,0.1802,0.0251,0.0376,0.0118,0.0337,0.0259,-0.0145,-0.072,0.046,0.0254,0.0619,0.0073,-0.0049,-0.0266,0.0217,0.015,-0.0165,-0.1116,-0.0295,-0.034,-0.0374,0.0288,-0.0248,-0.0038,0.0279,-0.0698,0.0464,0.012,0.0086,-0.0576,-0.1102,0.028,-0.0448,0.0432,0.0015,-0.0991,-0.0013,-0.0426,0.0711,-0.0162,0.0397,-0.0541,0.0204,-0.007,-0.0018,0.0522,0.0376,0.006,0.0404,-0.0322,0.0411,0.0105,-0.0354,-0.0208,0.0478,-0.0265,0.0322,-0.0222,0.0145,0.0168,0.0444,0.0046,0.0071,-0.0012,-0.0059,0.0132,-0.0404,-0.0084,0.0472,0.0201,-0.3062,0.0676,0.0437,-0.0156,-0.0549,-0.021,0.0307,-0.0438,-0.0601,0.0312,-0.0119,0.0641,0.0306,0.0032,0.0224,0.0135,0.0942,-0.044,0.0329,-0.0507,0.0125,0.0367,0.2276,-0.0374,0.0405,0.017,-0.0051,-0.0061,0.0447,0.0075,-0.0212,0.019,0.081,-0.0744,0.0367,0.1313,-0.0449,0.0419,0.0362,0.0604,0.0014,0.0211,-0.0408,-0.0217,0.1025,-0.0118,-0.0133,-0.0361,-0.0207,0.0098,-0.051,-0.0093,0.0262,-0.0297,0.028,-0.0154,-0.0746,-0.0578,-0.0171,-0.0022,0.0356,-0.0446,-0.0308,-0.0079,0.0004]}
{"key":"[Off-the-shelf deep learning is not enough: parsimony, Bayes and causality] Deep neural networks (\"deep learning\") have emerged as a technology of choice to tackle problems in natural language processing, computer vision, speech recognition and gameplay, and in just a few years has led to superhuman level performance and ushered in a new wave of \"AI.\" Buoyed by these successes, researchers in the physical sciences have made steady progress in incorporating deep learning into their respective domains. However, such adoption brings substantial challenges that need to be recognized and confronted. Here, we discuss both opportunities and roadblocks to implementation of deep learning within materials science, focusing on the relationship between correlative nature of machine learning and causal hypothesis driven nature of physical sciences. We argue that deep learning and AI are now well positioned to revolutionize fields where causal links are known, as is the case for applications in theory. When confounding factors are frozen or change only weakly, this leaves open the pathway for effective deep learning solutions in experimental domains. Similarly, these methods offer a pathway towards understanding the physics of real-world systems, either via deriving reduced representations, deducing algorithmic complexity, or recovering generative physical models. However, extending deep learning and \"AI\" for models with unclear causal relationship can produce misleading and potentially incorrect results. Here, we argue the broad adoption of Bayesian methods incorporating prior knowledge, development of DL solutions with incorporated physical constraints, and ultimately adoption of causal models, offers a path forward for fundamental and applied research. Most notably, while these advances can change the way science is carried out in ways we cannot imagine, machine learning is not going to substitute science any time soon.","layer":2,"vector":[-0.0487,0.0305,0.0239,-0.0258,0.0349,0.0128,0.0449,0.0179,0.0756,-0.048,0.0409,-0.0656,0.0307,0.0756,0.0332,0.0228,-0.0095,0.0052,-0.0333,0.003,0.0369,-0.0306,-0.0079,-0.0614,0.0102,0.0559,-0.0373,-0.0222,-0.0476,-0.2757,0.0023,-0.0606,0.0065,-0.0165,0.0199,-0.034,-0.0277,0.0565,-0.0191,0.0499,0.0194,0.0156,-0.011,-0.0405,-0.0083,-0.0448,0.009,-0.0271,-0.0315,-0.0366,0.0143,-0.0069,0.0543,0.0554,0.0329,0.0695,0.0561,0.0508,0.058,0.0579,0.0505,0.0416,-0.1943,0.0653,0.1092,0.0076,-0.0063,-0.002,0.0268,0.065,0.0112,0.0342,0.0137,0.0623,0.0296,0.0083,0.0254,-0.0324,-0.021,-0.0259,0.0272,0.0049,-0.021,-0.0096,-0.0683,-0.0327,0.037,-0.0566,0.0165,0.029,-0.0893,0.0014,-0.0294,0.0147,-0.073,0.0314,0.0364,0.0099,-0.0027,0.18,-0.0496,0.0547,-0.0353,-0.0303,0.0225,-0.0275,-0.0394,-0.0198,-0.0167,0.0177,0.007,-0.0212,0.004,-0.0114,0.0129,-0.0159,0.0548,0.0015,-0.0114,-0.0234,-0.0429,0.0126,0.0224,-0.0265,-0.0272,-0.0836,-0.0392,0.1427,-0.0014,0.0165,0.0528,-0.0493,-0.0625,-0.0037,0.0345,0.0171,0.0309,-0.0289,0.0106,0.0269,-0.0422,0.0043,0.0118,-0.0824,-0.0707,0.0813,-0.0469,0.0013,-0.0392,-0.0143,-0.0129,0.0271,-0.0517,-0.0338,0.0453,0.0009,-0.0155,0.0427,-0.0595,0.0442,-0.0365,-0.0304,-0.0417,0.1123,0.0063,-0.0378,-0.0275,0.0302,0.0127,-0.0263,0.0393,0.0595,-0.0367,-0.0001,0.0925,0.0412,-0.0688,-0.0119,-0.0276,0.0506,0.0068,-0.0365,-0.0179,0.0381,0.0698,-0.0218,-0.0144,-0.0215,-0.0214,0.0311,0.0529,0.0259,-0.0376,0.0088,0.0042,0.0236,-0.0227,0.0111,-0.0186,0.0103,-0.052,-0.0402,-0.0472,0.0254,-0.0445,0.0226,0.0251,0.0474,0.0449,-0.0086,0.0065,-0.0146,0.0494,-0.0614,-0.0522,0.0214,0.0237,-0.0056,-0.0349,0.0419,0.041,-0.0684,-0.0543,-0.2079,0.0064,0.0024,-0.0358,0.0571,-0.0624,0.0212,-0.0177,0.0318,0.0528,0.0322,0.0085,-0.0407,-0.0513,-0.0236,0.0029,0.0329,0.0463,-0.0392,0.0248,-0.047,-0.0104,-0.0415,-0.091,0.0231,0.0429,0.222,0.0674,0.0189,0.0148,0.0273,0.0294,-0.0067,-0.133,0.0403,0.0003,0.0827,0.029,-0.0627,-0.0535,-0.0651,0.0103,-0.0253,-0.0666,-0.0158,0.019,-0.0184,0.0219,-0.0255,0.0394,0.0155,-0.0245,0.0479,0.0203,-0.0426,-0.037,-0.0772,0.0478,-0.026,-0.0133,0.0036,-0.0285,-0.0144,-0.0352,0.011,-0.0107,-0.004,-0.0706,0.0342,-0.0174,0.0144,0.1275,-0.0051,-0.0304,0.0499,0.0081,0.0306,-0.0145,-0.0522,-0.0167,0.0856,0.0299,0.0222,0.0553,0.0105,0.0081,0.0706,-0.0433,0.0247,-0.0106,-0.0223,0.0091,-0.0564,-0.0057,0.0418,-0.0022,-0.2708,0.0581,0.0338,0.0359,-0.0475,0.0015,0.0476,0.0401,-0.0263,-0.0081,-0.0358,0.0256,0.0255,0.033,-0.0566,0.0137,0.0763,-0.0605,0.056,-0.0731,0.0141,0.0578,0.2449,-0.0344,0.0214,0.0389,0.021,0.0114,0.028,-0.0317,0.0239,0.0388,0.0411,-0.0788,0.0107,0.0787,-0.0578,0.0203,0.0555,-0.0509,0.0118,0.0037,-0.0363,-0.0418,0.1084,0.0106,-0.0148,-0.0568,-0.0084,-0.002,-0.0334,0.0176,-0.0273,-0.0164,0.0165,-0.0047,-0.0447,-0.0288,0.0044,-0.0692,0.0244,-0.0262,-0.0494,0.0273,-0.0218]}
{"key":"[Aligning Subjective Ratings in Clinical Decision Making] In addition to objective indicators (e.g. laboratory values), clinical data often contain subjective evaluations by experts (e.g. disease severity assessments). While objective indicators are more transparent and robust, the subjective evaluation contains a wealth of expert knowledge and intuition. In this work, we demonstrate the potential of pairwise ranking methods to align the subjective evaluation with objective indicators, creating a new score that combines their advantages and facilitates diagnosis. In a case study on patients at risk for developing Psoriatic Arthritis, we illustrate that the resulting score (1) increases classification accuracy when detecting disease presence/absence, (2) is sparse and (3) provides a nuanced assessment of severity for subsequent analysis.","layer":1,"vector":[-0.0719,0.0127,0.0264,0.0137,0.0086,0.007,0.0736,0.06,0.0408,-0.033,-0.0014,-0.0294,0.0216,0.0785,-0.031,0.0142,0.0047,0.0719,-0.0648,0.0352,0.0226,-0.0132,0.0024,-0.0903,0.0336,0.0575,-0.011,-0.0273,-0.0529,-0.2015,0.0018,-0.0338,0.0703,-0.0297,0.0071,-0.0327,-0.0342,0.0582,-0.0221,0.0196,0.0198,-0.016,-0.0148,-0.0006,-0.004,-0.0606,-0.0137,-0.0045,-0.0328,0.0145,0.0244,-0.0272,0.0085,0.0323,0.0206,0.0355,0.0301,0.0081,0.0075,0.0626,0.0731,0.05,-0.1348,0.0514,0.0666,-0.0003,-0.0451,-0.0691,-0.0389,0.0371,-0.007,0.0243,0.0358,0.0604,-0.0055,0.0019,-0.0271,-0.0246,-0.0042,0.0495,-0.0005,0.0003,-0.0546,-0.0181,0.0211,-0.0606,0.0072,-0.0451,0.0349,0.0195,-0.0008,-0.023,-0.029,0.0315,-0.0387,0.0006,0.0317,0.0179,-0.048,0.1987,-0.0833,-0.0036,0.0205,-0.0357,0.0492,-0.0377,-0.0254,-0.0611,-0.0218,-0.0103,0.0116,0.0123,0.0155,-0.0541,0.0224,0.062,0.0547,0.0516,0.0219,-0.0216,0.0113,-0.0026,0.0751,-0.027,0.0615,-0.0374,0.0613,0.1302,0.0156,0.0305,0.0616,-0.0569,-0.0684,0.0078,-0.008,0.0046,0.0003,0.0372,0.061,0.0284,0.0069,-0.0649,0.0165,-0.0852,-0.0514,0.149,-0.0832,0.0313,-0.0344,-0.0031,0.0143,-0.0225,-0.0333,-0.0229,0.0478,-0.0151,0.0182,-0.0212,-0.0292,0.0225,-0.0064,-0.0543,-0.0634,0.1151,0.0116,-0.0591,-0.0365,-0.0333,-0.0113,0.0205,0.0304,0.021,-0.0177,0.0511,0.0396,-0.0109,-0.0169,-0.0092,0.0086,-0.0115,0.0468,-0.0007,-0.0376,0.0607,0.0281,0.0083,-0.0233,-0.0343,0.0465,0.0402,-0.0607,-0.0529,-0.0126,0.037,-0.0165,-0.0037,-0.0072,-0.0174,-0.008,-0.012,0.021,0.0002,0.0024,0.0234,0.0112,0.06,-0.0604,0.0189,0.0698,0.0183,-0.0326,-0.0011,0.0296,0.0192,-0.0925,-0.0176,0.0243,0.0186,-0.0014,0.024,0.047,0.012,-0.0551,-0.2327,-0.0258,0.026,0.0178,0.0255,-0.0609,0.0341,-0.0324,0.0104,0.0993,0.0798,-0.0034,-0.0126,0.0051,-0.031,0.0568,0.0462,-0.007,-0.0302,-0.0149,-0.0212,0.0396,0.0294,-0.0725,0.0613,-0.0183,0.2318,0.0552,-0.0262,-0.0134,-0.0161,-0.0175,-0.0586,-0.1139,0.047,0.002,0.0454,-0.0653,-0.0652,-0.0231,-0.0288,0.0056,0.0108,-0.0826,-0.0598,-0.0373,0.0096,0.0451,-0.0521,0.0694,0.0297,-0.0823,0.0586,-0.0313,-0.019,-0.0224,-0.1254,0.038,-0.0388,0.0154,0.0016,-0.063,0.037,-0.0514,0.004,-0.0419,-0.0416,0.0076,0.0358,-0.0069,-0.0228,0.0446,-0.0422,-0.0119,0.0565,0.0369,0.0612,-0.026,-0.0124,-0.0359,0.0298,-0.0616,0.0411,-0.0031,0.0151,-0.0364,0.0948,0.0199,-0.0039,-0.0248,-0.0102,0.0035,-0.0674,-0.0226,0.0324,-0.0156,-0.3056,0.0408,-0.005,-0.014,-0.0374,-0.0139,0.0571,0.0073,-0.0645,-0.0289,0.0339,0.0062,0.0313,-0.0669,-0.013,0.0033,0.0436,-0.0916,0.0753,-0.0547,0.0325,-0.0097,0.2247,-0.0203,0.0484,0.0361,-0.0376,-0.002,-0.0317,-0.0134,0.0361,0.0014,0.0713,-0.0273,0.0608,0.089,-0.0139,0.0505,0.0324,-0.0147,0.0054,0.0005,-0.0207,0.004,0.1318,0.0168,-0.0244,-0.0239,0.034,0.0034,-0.0197,0.0225,-0.0112,0.0256,0.0175,0.0232,-0.0364,-0.0142,-0.0245,-0.0839,-0.0135,-0.0495,-0.0304,0.0373,0.0137]}
{"key":"[Hyperparameter Learning for Conditional Kernel Mean Embeddings with Rademacher Complexity Bounds] Conditional kernel mean embeddings are nonparametric models that encode conditional expectations in a reproducing kernel Hilbert space. While they provide a flexible and powerful framework for probabilistic inference, their performance is highly dependent on the choice of kernel and regularization hyperparameters. Nevertheless, current hyperparameter tuning methods predominantly rely on expensive cross validation or heuristics that is not optimized for the inference task. For conditional kernel mean embeddings with categorical targets and arbitrary inputs, we propose a hyperparameter learning framework based on Rademacher complexity bounds to prevent overfitting by balancing data fit against model complexity. Our approach only requires batch updates, allowing scalable kernel hyperparameter tuning without invoking kernel approximations. Experiments demonstrate that our learning framework outperforms competing methods, and can be further extended to incorporate and learn deep neural network weights to improve generalization.","layer":5,"vector":[-0.0166,-0.0337,0.0262,0.024,-0.0195,0.0286,0.0096,0.0183,0.0115,-0.0188,0.0192,-0.0859,0.048,0.0612,0.0153,0.0144,0.0386,0.0807,-0.0792,-0.003,0.044,-0.0335,-0.0089,-0.0533,0.0144,0.0049,-0.0077,-0.0448,-0.0405,-0.2669,0.0321,-0.049,0.0571,0.004,0.0169,-0.0538,-0.0266,0.0179,-0.0207,0.0723,-0.012,0.0406,-0.0165,-0.0559,-0.0242,-0.0782,-0.0411,-0.0202,-0.0335,-0.0171,0.0496,-0.0355,0.0226,-0.0015,0.0589,0.0457,0.0687,0.0395,0.091,0.0666,-0.0138,0.0523,-0.1394,0.0191,0.0178,0.0113,-0.0501,-0.0104,0.0305,0.0506,0.0035,0.0203,0.0052,0.0429,-0.0232,0.0184,0.0403,-0.0236,-0.0355,0.0144,0.0208,-0.0354,-0.0103,-0.0173,-0.0206,-0.0602,-0.0097,-0.0355,0.0197,0.0026,-0.0435,-0.043,-0.0415,-0.0335,-0.0673,-0.0035,0.0679,0.0005,-0.0495,0.2146,-0.0369,0.0071,0.0285,-0.0071,0.0056,-0.0455,-0.016,0.0063,-0.0264,-0.0171,-0.0158,-0.0294,0.0295,-0.0397,0.0576,0.0362,0.1003,0.0483,-0.0029,-0.0288,-0.0322,0.0096,0.0322,-0.0216,0.0268,-0.0361,0.0081,0.1237,0.0531,0.0381,0.015,-0.0412,-0.0817,-0.0118,0.0146,0.053,0.0166,0.0082,-0.0052,-0.0158,-0.0077,-0.0147,-0.0054,-0.0498,-0.0575,0.1285,-0.0651,0.008,-0.0375,0.0056,0.0011,0.0127,-0.0256,-0.0647,0.0136,0.0177,0.0058,0.001,-0.0297,0.0036,-0.0145,-0.0879,-0.0103,0.096,0.0061,-0.0516,-0.044,-0.0021,0.0214,0.006,0.0621,0.0325,-0.0137,0.02,0.1038,0.0359,-0.1025,0.0211,0.0356,0.0051,0.0359,-0.051,-0.0573,0.0348,0.0794,-0.0481,0.0201,-0.061,0.0032,0.0074,-0.0092,-0.0037,-0.0225,-0.0093,-0.012,-0.053,-0.0338,-0.0168,0.0494,-0.0392,-0.0032,0.0233,-0.0439,0.0433,-0.0403,0.0102,-0.0002,0.0342,0.0518,0.0217,-0.035,0.0093,0.0747,-0.0349,-0.0353,-0.004,0.0164,0.0633,0.0213,0.0446,0.0221,-0.0423,-0.0154,-0.2157,-0.0131,0.0485,-0.0222,0.0605,-0.065,0.064,0.0201,0.0746,0.0803,0.0126,-0.0508,-0.0221,0.0418,0.0083,0.0612,0.0325,0.0144,-0.0507,-0.0101,-0.0214,0.0244,-0.0268,-0.0582,0.0563,-0.0538,0.2137,0.0297,0.0396,-0.1044,0.0164,0.0152,-0.0115,-0.1132,0.0813,0.0177,0.0335,0.0123,-0.0498,0.0165,-0.0163,0.0283,-0.002,-0.1296,-0.0469,-0.0528,0.0084,0.0278,-0.0458,0.0337,0.0285,-0.0566,0.0633,-0.0309,-0.0221,-0.0264,-0.1029,0.0293,-0.0556,0.0463,0.0494,-0.0731,-0.0045,-0.0333,0.0414,-0.0427,-0.0284,-0.0204,0.0356,-0.0374,-0.0428,0.0748,-0.0212,0.0627,0.0715,0.0209,0.0234,0.0072,-0.0304,-0.0057,0.1101,0.0058,0.036,0.0178,0.038,0.0176,0.0888,-0.0199,0.0541,-0.0248,-0.0009,-0.0139,-0.072,-0.0099,0.0558,-0.0018,-0.2964,0.0218,0.0046,0.023,-0.0452,0.018,0.0108,-0.0202,-0.0532,-0.0295,0.0217,0.0738,0.0614,-0.023,-0.0496,0.0203,0.0435,-0.0523,0.0346,-0.0401,0.016,0.0457,0.1737,-0.0303,0.032,-0.0096,-0.0344,-0.0234,0.0613,-0.027,0.0102,-0.0024,0.074,-0.0208,0.0286,0.0779,-0.0206,0.0411,0.0254,-0.0142,0.038,-0.0325,-0.0499,-0.0042,0.0719,-0.0348,0.0022,0.005,0.0114,0.026,-0.0345,0.0223,-0.0056,0.0369,0.0263,-0.0222,-0.0138,-0.0663,-0.0603,-0.0343,0.0047,-0.0336,-0.0153,-0.0086,-0.0194]}
{"key":"[SUTD-PRCM Dataset and Neural Architecture Search Approach for Complex Metasurface Design] Metasurfaces have received a lot of attentions recently due to their versatile capability in manipulating electromagnetic wave. Advanced designs to satisfy multiple objectives with non-linear constraints have motivated researchers in using machine learning (ML) techniques like deep learning (DL) for accelerated design of metasurfaces. For metasurfaces, it is difficult to make quantitative comparisons between different ML models without having a common and yet complex dataset used in many disciplines like image classification. Many studies were directed to a relatively constrained datasets that are limited to specified patterns or shapes in metasurfaces. In this paper, we present our SUTD polarized reflection of complex metasurfaces (SUTD-PRCM) dataset, which contains approximately 260,000 samples of complex metasurfaces created from electromagnetic simulation, and it has been used to benchmark our DL models. The metasurface patterns are divided into different classes to facilitate different degree of complexity, which involves identifying and exploiting the relationship between the patterns and the electromagnetic responses that can be compared in using different DL models. With the release of this SUTD-PRCM dataset, we hope that it will be useful for benchmarking existing or future DL models developed in the ML community. We also propose a classification problem that is less encountered and apply neural architecture search to have a preliminary understanding of potential modification to the neural architecture that will improve the prediction by DL models. Our finding shows that convolution stacking is not the dominant element of the neural architecture anymore, which implies that low-level features are preferred over the traditional deep hierarchical high-level features thus explains why deep convolutional neural network based models are not performing well in our dataset.","layer":4,"vector":[-0.0399,-0.0208,0.0313,0.0009,0.0115,0.0314,-0.0002,0.0445,-0.0178,-0.0273,-0.0049,-0.0637,0.047,0.0162,0.0555,0.0191,-0.0122,0.06,-0.0026,0.0376,0.0507,-0.0257,-0.0573,-0.0793,0.0098,0.0182,-0.0151,0.0062,-0.0754,-0.2323,0.0523,-0.025,0.0455,-0.0006,0.0263,-0.0378,-0.0466,0.0291,-0.04,0.044,0.0095,0.0192,-0.0163,-0.0789,-0.0059,-0.0539,-0.0247,-0.0378,0.0154,-0.0518,0.017,-0.0415,-0.0084,0.0323,0.0234,0.0349,0.0172,0.0112,0.0519,0.0527,0.0221,0.0534,-0.1576,0.1001,0.0022,0.0498,-0.0432,-0.0396,0.0322,0.0329,-0.0301,0.0104,0.0025,0.0045,-0.0096,0.0058,-0.0474,-0.0759,-0.013,0.0121,0.0188,-0.0148,-0.017,0.0074,-0.0251,-0.0031,0.0021,-0.0394,0.022,0.0311,-0.0147,-0.0187,-0.0864,-0.0083,-0.0778,-0.0306,0.0197,0.0343,-0.0352,0.2112,-0.036,-0.008,0.006,-0.0308,0.0109,-0.0187,-0.0091,0.0198,-0.0516,0.001,-0.0176,-0.0229,-0.003,-0.0459,-0.0152,-0.0025,0.0433,0.0339,-0.0073,-0.0272,-0.0383,0.0046,0.0541,-0.0211,0.0198,-0.0329,-0.014,0.1228,0.0371,0.0286,0.0454,-0.0279,-0.0279,-0.0489,0.0178,0.0388,0.0259,-0.0365,-0.019,0.0043,-0.0556,-0.0472,-0.0106,-0.0787,-0.0263,0.1108,-0.0656,0.0299,-0.075,-0.0039,-0.032,0.0411,-0.0532,-0.0025,0.0157,0.0146,-0.0305,0.0789,-0.0182,0.014,-0.0054,-0.0468,-0.0739,0.1868,-0.0075,-0.0969,-0.0447,-0.007,0.0528,-0.0287,0.0104,0.0516,0.0056,0.0055,0.0689,0.0422,-0.0367,-0.0313,0.0008,0.0107,0.0079,-0.0381,-0.0265,0.0508,0.0338,-0.0443,0.0331,-0.0328,-0.0169,0.0221,-0.0433,0.0664,-0.0532,0.0113,-0.046,-0.0086,-0.0146,-0.0253,0.0365,-0.041,0.0008,0.0028,-0.0305,0.0503,0.0204,0.0267,0.0194,0.0383,0.0141,0.0456,-0.0265,-0.0345,0.0618,-0.0165,0.0367,0.0198,0.0162,0.0186,0.021,0.1018,0.0194,-0.0904,-0.1121,-0.2214,-0.0207,-0.002,-0.0242,0.0629,-0.0451,0.0435,0.0201,0.0318,0.0488,0.0537,0.0476,-0.0539,-0.0165,-0.0045,0.0227,0.0089,0.0309,-0.0475,-0.0168,-0.0008,0.018,0.0514,-0.0883,0.0657,0.0206,0.2179,0.0496,-0.0169,-0.0104,0.0685,0.015,-0.0123,-0.0942,0.0314,0.0266,0.1022,-0.0424,-0.0423,-0.0402,-0.0382,0.0367,0.0233,-0.0691,-0.0363,-0.0185,-0.0215,0.0159,-0.0402,0.0434,0.0343,-0.0833,0.0106,-0.0037,-0.0252,-0.0375,-0.1091,-0.0023,-0.0117,0.0186,0.0463,-0.0117,0.0294,-0.0739,-0.0001,0.0589,-0.0429,-0.0353,0.0512,-0.0831,-0.0151,0.1016,0.026,0.0454,0.0462,-0.0331,0.0689,-0.0024,0.0347,-0.0101,0.082,0.0328,0.0239,0.0062,0.0403,0.0466,0.0683,-0.0677,0.0062,-0.0649,0.0148,0.0224,-0.0651,0.0088,0.0037,0.0071,-0.2832,0.0633,0.0551,0.0527,-0.0265,-0.0148,0.0747,0.0275,-0.0216,-0.012,-0.0477,0.0586,0.0483,-0.0078,0.0267,0.0214,0.0608,-0.0452,0.032,-0.0481,-0.0163,0.0487,0.2166,-0.0423,0.0121,-0.0028,-0.0048,-0.0021,0.0071,-0.0319,0.0215,0.0094,0.0471,-0.0402,0.0361,0.0998,-0.0068,0.0096,-0.0029,-0.0091,0.0237,0.0174,-0.0441,-0.009,0.0804,-0.0423,-0.0128,0.0177,0.0038,0.0072,-0.0499,0.019,-0.0464,-0.0294,0.0406,0.019,-0.0467,-0.0293,-0.0211,-0.0223,0.0541,-0.0181,-0.0346,-0.0237,0.0453]}
{"key":"[Hierarchical Program-Triggered Reinforcement Learning Agents For Automated Driving] Recent advances in Reinforcement Learning (RL) combined with Deep Learning (DL) have demonstrated impressive performance in complex tasks, including autonomous driving. The use of RL agents in autonomous driving leads to a smooth human-like driving experience, but the limited interpretability of Deep Reinforcement Learning (DRL) creates a verification and certification bottleneck. Instead of relying on RL agents to learn complex tasks, we propose HPRL - Hierarchical Program-triggered Reinforcement Learning, which uses a hierarchy consisting of a structured program along with multiple RL agents, each trained to perform a relatively simple task. The focus of verification shifts to the master program under simple guarantees from the RL agents, leading to a significantly more interpretable and verifiable implementation as compared to a complex RL agent. The evaluation of the framework is demonstrated on different driving tasks, and NHTSA precrash scenarios using CARLA, an open-source dynamic urban simulation environment.","layer":0,"vector":[-0.0611,-0.0041,0.0325,-0.0446,-0.0278,0.0654,0.0266,0.0181,0.0074,-0.0464,-0.0075,-0.0425,0.0527,0.0509,-0.0061,-0.0329,-0.0404,0.0375,-0.0058,-0.0167,0.0142,-0.0537,-0.0484,-0.0642,-0.0018,0.0501,-0.0461,-0.0339,-0.0307,-0.2256,-0.0138,-0.0533,0.0353,-0.0221,-0.0084,-0.0166,-0.095,0.059,-0.0104,0.0083,0.0443,0.0225,-0.016,-0.0495,-0.012,-0.0631,0.0156,0.0033,0.0121,-0.0401,0.0349,-0.0213,0.0036,0.0524,0.0207,-0.0011,0.0564,0.0652,0.0571,0.0167,0.0105,0.0486,-0.1746,0.0632,0.0331,0.0543,-0.04,-0.0307,0.0347,0.0585,-0.0363,0.0173,0.0019,0.0403,0.0357,-0.0521,0.0207,-0.0402,-0.0122,-0.0314,0.0259,-0.0408,-0.0661,-0.0085,-0.0401,-0.0683,0.0048,-0.0269,0.0489,-0.0169,-0.0413,-0.0233,-0.0089,-0.001,-0.0379,0.0173,0.0272,-0.0315,-0.078,0.2025,-0.0528,0.0263,0.0105,0.0097,0.0308,-0.016,0.0042,0.0182,-0.0277,-0.0113,-0.0199,-0.0243,0.0506,0.0146,-0.0107,0.0411,0.1057,0.0374,0.016,-0.0139,0.0062,-0.0051,0.0522,-0.0454,0.036,-0.081,0.0364,0.1435,0.0202,0.0403,0.0502,-0.0329,-0.0499,-0.0218,0.0391,-0.0078,0.0133,0.0222,-0.0342,0.0237,-0.0547,0.0139,-0.0094,-0.1132,-0.0388,0.073,-0.0059,0.0173,-0.0227,0.0209,-0.007,-0.0135,-0.0559,-0.0464,0.0074,0.0294,0.0544,0.0713,-0.0449,0.0119,-0.0114,-0.0498,-0.0458,0.0998,-0.0194,-0.0819,-0.0131,-0.0061,0.024,-0.0109,0.0337,0.0325,-0.0384,0.0333,0.0582,0.0331,-0.0876,0.0626,-0.0086,0.009,0.0318,-0.0531,-0.044,0.0038,0.0677,-0.0032,-0.0299,-0.0456,0.017,0.014,-0.0183,0.0317,0.0164,-0.0041,0.0026,-0.0443,0.0203,-0.0228,0.0086,-0.0306,-0.0214,-0.0185,-0.0329,0.0241,-0.0486,0.0046,-0.0215,-0.0087,0.0726,0.0188,-0.026,0.0054,0.0608,-0.0215,-0.0472,0.0039,-0.0292,0.0261,-0.0114,0.034,0.0009,-0.0149,-0.0177,-0.2273,-0.0102,-0.0074,-0.0302,0.0368,-0.0343,0.0429,-0.0331,0.0186,0.0536,0.07,-0.0343,0.0032,0.0342,0.0229,0.0412,-0.0037,0.0283,-0.0103,0.015,0.0126,-0.0347,-0.0813,-0.0715,0.055,0.0152,0.2113,0.0199,0.066,-0.0025,0.0176,0.0439,-0.0187,-0.1022,0.0665,0.0014,0.0901,0.0094,-0.0305,-0.0653,-0.0115,0.035,-0.0471,-0.0965,-0.0052,-0.0174,-0.0362,0.0744,0.0027,-0.0269,0.0413,-0.0627,0.0027,0.0096,-0.0372,-0.0532,-0.0886,0.0203,-0.0162,-0.0057,0.0017,0.0026,0.0116,-0.069,0.081,-0.0,0.0082,-0.0829,0.0311,0.0014,-0.0372,0.0736,-0.0167,0.0088,0.0422,0.0153,-0.0005,-0.0137,-0.0197,-0.0168,0.0359,-0.005,0.0417,0.0722,0.0463,-0.0162,0.0488,-0.0131,0.0035,-0.0044,0.0473,0.0592,-0.0743,-0.0314,0.0648,-0.009,-0.3097,0.0423,0.0178,0.0284,-0.0435,-0.002,0.0269,-0.0055,-0.054,-0.0168,0.0009,0.0694,0.0535,0.0412,0.0347,0.0375,0.0885,0.0106,0.0333,-0.0522,-0.0028,0.0791,0.2223,-0.0704,0.0423,0.0585,-0.0481,-0.0432,0.0775,-0.0202,-0.0247,0.0251,0.0607,-0.0519,0.0246,0.0724,-0.0615,0.049,0.0368,0.0533,-0.0025,0.0604,0.0186,-0.0521,0.0475,-0.03,-0.0441,-0.0307,-0.0079,0.0869,0.021,-0.0142,-0.0301,-0.024,0.055,0.006,-0.0258,-0.088,-0.0271,-0.0382,0.0097,-0.0345,0.0377,0.0329,0.0001]}
{"key":"[A Compositional Model of Multi-faceted Trust for Personalized Item Recommendation] Trust-based recommender systems improve rating prediction with respect to Collaborative Filtering by leveraging the additional information provided by a trust network among users to deal with the cold start problem. However, they are challenged by recent studies according to which people generally perceive the usage of data about social relations as a violation of their own privacy. In order to address this issue, we extend trust-based recommender systems with additional evidence about trust, based on public anonymous information, and we make them configurable with respect to the data that can be used in the given application domain: 1 - We propose the Multi-faceted Trust Model (MTM) to define trust among users in a compositional way, possibly including or excluding the types of information it contains. MTM flexibly integrates social links with public anonymous feedback received by user profiles and user contributions in social networks. 2 - We propose LOCABAL+, based on MTM, which extends the LOCABAL trust-based recommender system with multi-faceted trust and trust-based social regularization. Experiments carried out on two public datasets of item reviews show that, with a minor loss of user coverage, LOCABAL+ outperforms state-of-the art trust-based recommender systems and Collaborative Filtering in accuracy, ranking of items and error minimization both when it uses complete information about trust and when it ignores social relations. The combination of MTM with LOCABAL+ thus represents a promising alternative to state-of-the-art trust-based recommender systems.","layer":6,"vector":[-0.0204,-0.0334,0.0134,-0.0389,-0.0215,0.0289,0.0548,0.0248,0.0246,-0.0396,0.0253,-0.0391,0.0182,0.0708,0.0622,0.0079,0.0209,0.0088,-0.0236,-0.0193,0.0465,-0.017,-0.0176,-0.0925,0.0133,0.0192,-0.0434,-0.0543,-0.1016,-0.2026,-0.0093,-0.0798,0.0525,0.006,-0.0029,-0.0246,0.0087,0.0146,-0.0498,0.03,0.0127,0.0339,-0.0176,-0.0329,-0.0214,-0.0287,-0.006,0.0172,-0.0336,-0.0256,0.0269,-0.0041,0.0331,0.0561,0.0381,0.0573,0.0162,0.0395,0.0161,0.0577,0.0589,0.038,-0.1323,0.0437,0.0458,0.0642,-0.0381,0.0141,-0.0455,0.0166,0.0144,0.0461,0.0162,0.029,0.0087,0.0199,0.0034,0.0098,-0.0567,0.0653,-0.0496,-0.0004,-0.045,-0.0678,0.0367,-0.0634,0.0486,-0.0585,0.0047,-0.0151,-0.0222,-0.0207,0.0062,0.0331,-0.0415,-0.0475,0.0027,0.0559,-0.0949,0.2015,-0.0905,0.0561,0.0064,-0.049,0.0618,-0.0062,0.0282,-0.0311,-0.0112,0.0001,-0.0361,0.0105,0.0356,-0.0451,0.0663,0.04,0.0857,0.0736,0.0217,-0.0613,-0.0298,0.0268,0.0845,0.0116,0.0183,-0.0534,0.0304,0.1061,0.0062,0.0477,0.041,-0.0017,-0.0724,-0.0547,-0.0006,0.0488,-0.0393,-0.0203,0.0768,-0.016,-0.0657,-0.0625,-0.023,-0.0649,-0.0484,0.0963,-0.0131,-0.0089,-0.0244,-0.0168,-0.0196,0.0205,-0.0263,-0.0464,0.0404,-0.0476,0.0282,0.0321,-0.0565,0.0351,0.0017,-0.0588,-0.0318,0.0594,-0.0284,-0.0852,-0.0005,0.0102,0.0229,-0.0237,0.0211,0.0229,-0.0173,0.0125,0.0675,0.0551,-0.0599,0.0008,0.0289,0.0191,0.0284,-0.0157,-0.0436,0.0799,0.009,0.0232,-0.0199,-0.0412,0.0074,0.027,-0.0376,0.0017,-0.0353,-0.0104,0.0326,-0.027,-0.0437,-0.047,-0.0186,-0.0494,0.0176,0.0035,-0.0366,0.0396,-0.0476,0.0759,-0.0068,-0.0326,0.0482,-0.0208,-0.082,0.0,0.0305,-0.0346,-0.0767,0.0278,0.0399,0.023,0.055,0.025,0.0297,-0.0277,-0.0531,-0.2336,-0.0144,-0.0071,0.0359,0.0631,-0.0601,0.0203,-0.0363,-0.0043,0.1141,0.0858,-0.0165,-0.046,0.094,0.0194,0.0784,0.0208,-0.0012,-0.0056,0.0236,-0.0242,0.0234,0.0018,-0.0998,0.0347,0.0228,0.2093,0.0456,-0.0435,-0.0337,0.0462,0.0642,-0.0407,-0.1079,0.057,0.0227,0.0546,-0.037,-0.0512,-0.0653,-0.0007,0.0244,0.0117,-0.0886,-0.0241,-0.0334,-0.0049,0.0393,-0.0578,0.0077,0.0306,0.0025,0.0496,-0.0226,-0.0201,-0.0688,-0.0464,0.0229,-0.0173,0.0611,-0.028,0.0001,0.017,-0.0896,0.0617,-0.0113,-0.0247,0.0169,0.0501,-0.0226,-0.0254,0.0557,0.0137,-0.0268,0.0485,0.026,0.0361,-0.041,-0.0641,-0.0622,0.0588,-0.0075,0.0287,0.0349,0.008,-0.0224,0.0654,0.0213,0.0418,-0.0587,0.0263,-0.027,-0.0791,-0.0441,0.0906,-0.0212,-0.314,-0.0011,-0.0407,0.0427,-0.0233,-0.0466,0.0487,0.0378,-0.0012,0.0156,0.0345,0.0395,0.0511,-0.0235,0.0018,0.0266,0.0438,-0.0484,0.0357,0.0049,0.0379,0.0089,0.2387,0.0027,0.0194,0.0075,0.0184,-0.0177,0.0216,0.0089,0.0204,-0.0183,0.0741,-0.0597,0.0072,0.059,0.0023,-0.0075,0.0079,-0.0246,-0.02,-0.022,-0.0343,0.0265,0.0784,-0.0112,-0.0114,-0.0517,0.0103,0.0036,-0.0402,-0.0263,-0.0332,0.0151,0.0077,0.0354,-0.0448,0.0251,-0.0246,-0.0332,0.001,0.0138,-0.014,0.0022,0.0052]}
{"key":"[Training and Evaluating a Jupyter Notebook Data Science Assistant] We study the feasibility of a Data Science assistant powered by a sequence-to-sequence transformer by training a new model JuPyT5 on all publicly available Jupyter Notebook GitHub repositories and developing a new metric: Data Science Problems (DSP). DSP is a collection of 1119 problems curated from 306 pedagogical notebooks with 92 dataset dependencies, natural language and Markdown problem descriptions, and assert-based unit tests. These notebooks were designed to test university students' mastery of various Python implementations of Math and Data Science, and we now leverage them to study the ability of JuPyT5 to understand and pass the tests. We analyze the content of DSP, validate its quality, and we find that given 100 sampling attempts JuPyT5 is able to solve 77.5\\% of the DSP problems. We further present various ablation and statistical analyses and compare DSP to other recent natural language to code benchmarks.","layer":2,"vector":[-0.0801,-0.0261,0.0137,-0.0495,-0.0179,0.009,-0.0123,0.0552,0.028,-0.0109,0.0033,-0.0609,0.0406,0.0345,0.0092,-0.0081,-0.0286,-0.0029,-0.0249,0.0229,0.0247,-0.0562,-0.0655,-0.0799,0.0145,0.0781,-0.0173,-0.0276,-0.0659,-0.2812,-0.0128,-0.0469,0.0355,-0.0375,0.0295,0.0146,-0.011,0.0364,-0.0346,0.0204,0.0244,0.0015,-0.033,-0.025,-0.0366,-0.0661,-0.0239,-0.042,-0.0151,-0.0452,-0.0047,-0.0747,-0.0228,0.0181,0.0156,0.0273,0.0878,0.0503,0.0433,0.031,0.0029,0.0334,-0.1648,0.1044,0.0622,0.0272,-0.0597,-0.033,0.0237,0.0455,-0.0682,-0.0002,-0.0043,0.0983,0.0217,0.025,-0.0131,-0.0599,0.021,-0.0114,-0.0287,-0.0449,-0.026,-0.007,0.0081,-0.0308,0.0056,-0.001,0.0308,-0.0063,0.0103,-0.001,0.0032,0.064,-0.0627,-0.0452,0.0403,0.0277,-0.034,0.2003,-0.052,0.0242,0.0162,-0.0188,0.0108,-0.0488,0.002,-0.0728,-0.0221,-0.0515,-0.0071,0.012,0.0309,-0.0286,0.0255,-0.0246,0.0513,-0.03,-0.0505,0.0381,-0.0286,0.0667,0.0163,-0.0025,0.0595,-0.0362,0.0139,0.1169,0.02,0.0409,0.0657,0.01,-0.0755,0.0056,-0.0057,-0.0001,0.0282,0.0063,0.0208,-0.0476,-0.0353,-0.0459,-0.0215,-0.0313,-0.0776,0.1238,-0.053,0.0458,-0.0523,0.0012,-0.0133,0.0344,-0.0288,-0.0513,0.05,0.0798,0.0473,0.0455,-0.0537,-0.0076,-0.0196,-0.029,-0.0406,0.0581,0.0107,-0.1024,0.0144,0.0073,0.0237,-0.0272,0.0416,0.0344,-0.0176,0.0522,0.0419,-0.0007,-0.0232,0.0125,-0.0053,0.0306,0.0286,-0.0166,0.0324,0.0368,0.0295,-0.0565,-0.0037,-0.0341,0.0342,0.0358,0.0004,0.0442,-0.0314,-0.0223,-0.0239,-0.0396,-0.002,0.0068,-0.0096,-0.0229,0.0043,-0.0143,-0.016,0.0461,-0.0117,-0.0205,-0.0023,0.0033,0.0685,0.0315,-0.0383,-0.0406,0.0008,0.0021,-0.0551,-0.0026,0.0262,0.0416,0.05,0.0588,-0.033,-0.0231,-0.0318,-0.2591,-0.0312,0.0559,-0.0046,0.0765,-0.0148,0.0474,-0.0138,0.0038,0.0782,0.027,-0.0442,-0.0208,-0.0311,-0.039,0.0306,0.0053,-0.0042,-0.0269,-0.0182,-0.0107,-0.0045,0.0095,-0.0719,0.0188,-0.0327,0.225,0.021,0.0635,0.0001,0.0358,0.0102,-0.0322,-0.1176,0.0875,0.0066,0.0413,0.0065,0.0091,-0.0601,0.0138,0.0385,-0.0038,-0.0867,-0.0201,-0.0,-0.0185,-0.0358,-0.0562,0.0601,0.049,-0.0009,0.035,0.0031,-0.0142,-0.0185,-0.1076,0.0584,-0.0014,0.0272,0.0001,-0.042,0.0292,-0.042,0.0328,-0.0259,0.0153,-0.0222,0.0484,-0.0333,-0.0335,0.1027,-0.0066,-0.0096,0.009,0.0474,0.0213,-0.017,-0.0204,-0.0179,0.0683,0.0021,0.018,-0.0009,0.0431,0.0157,0.0742,0.0213,0.0731,-0.0282,-0.0062,0.0035,-0.0467,0.0125,0.0196,0.0384,-0.2808,0.032,-0.019,0.0339,-0.0338,0.0197,0.028,0.0133,-0.0566,-0.0157,0.0109,0.0317,0.0504,-0.0099,0.0234,0.0277,0.076,-0.0465,0.017,-0.0334,0.0152,0.0722,0.2144,-0.0382,0.0012,0.0364,0.0325,-0.0233,0.0644,-0.0276,-0.0128,-0.03,0.0781,-0.041,0.0236,0.0858,-0.0277,0.0416,0.0534,-0.0174,0.0246,-0.0037,-0.0291,-0.0301,0.1081,0.0173,0.0039,-0.1026,0.0123,0.0187,-0.004,-0.0154,-0.0142,-0.022,0.0181,0.0391,-0.0245,-0.0118,-0.043,-0.0796,-0.0064,-0.0995,0.0327,0.0504,0.0103]}
{"key":"[Disentangled Representations from Non-Disentangled Models] Constructing disentangled representations is known to be a difficult task, especially in the unsupervised scenario. The dominating paradigm of unsupervised disentanglement is currently to train a generative model that separates different factors of variation in its latent space. This separation is typically enforced by training with specific regularization terms in the model's objective function. These terms, however, introduce additional hyperparameters responsible for the trade-off between disentanglement and generation quality. While tuning these hyperparameters is crucial for proper disentanglement, it is often unclear how to tune them without external supervision. This paper investigates an alternative route to disentangled representations. Namely, we propose to extract such representations from the state-of-the-art generative models trained without disentangling terms in their objectives. This paradigm of post hoc disentanglement employs little or no hyperparameters when learning representations while achieving results on par with existing state-of-the-art, as shown by comparison in terms of established disentanglement metrics, fairness, and the abstract reasoning task. All our code and models are publicly available.","layer":0,"vector":[-0.0403,-0.0004,-0.0164,-0.0189,-0.0135,-0.0012,0.0493,-0.0038,0.014,0.0043,0.0395,-0.1004,0.0727,0.0527,0.0353,-0.0063,0.0076,0.0542,-0.0533,0.0047,0.0022,-0.0521,-0.0027,-0.0529,0.0205,-0.0036,-0.0389,-0.0221,-0.0371,-0.2626,0.0075,-0.0171,0.0194,0.0046,0.0488,-0.0272,-0.0391,0.0422,-0.026,0.0432,-0.0054,0.0445,0.0044,-0.0315,-0.0507,-0.0376,-0.0467,-0.0127,-0.027,-0.0617,0.0265,-0.042,-0.0083,0.0233,0.0645,0.0421,0.0818,0.0328,0.0495,0.0646,0.0129,0.0366,-0.134,0.043,0.0662,0.0442,-0.0734,-0.0339,0.0216,0.0452,-0.0177,0.0079,0.0099,0.0195,0.0352,0.0035,-0.0071,-0.0141,-0.0259,0.0311,0.0095,0.0069,-0.0301,-0.0069,-0.0168,-0.04,0.0012,-0.0296,0.0043,0.0078,-0.0877,-0.0393,-0.0208,-0.0067,-0.0247,0.0003,0.04,0.029,-0.0428,0.2125,-0.0698,-0.0176,0.0167,-0.0522,0.0052,-0.0244,-0.0448,-0.0119,-0.0409,-0.001,-0.0043,0.0076,0.0191,-0.0691,0.0313,0.0121,0.0956,0.0695,-0.0309,0.0001,-0.0407,0.0323,0.0249,-0.0452,0.0159,-0.0549,0.0292,0.15,0.0264,0.0748,0.001,0.0046,-0.0258,0.0328,0.0111,-0.0034,0.0357,0.0195,-0.0225,-0.0141,-0.0324,-0.0214,0.0069,-0.0688,-0.0452,0.1102,-0.0626,0.0305,-0.0461,0.0176,-0.0232,-0.0162,-0.0331,-0.0497,0.0112,0.0584,0.028,0.0295,-0.0212,0.0309,-0.0137,-0.0438,-0.0198,0.0885,0.0159,-0.0798,-0.0422,0.0023,0.0369,0.0122,0.0712,0.0156,-0.0192,0.0575,0.1046,0.0152,-0.0781,-0.0052,0.0335,0.0365,0.0119,-0.0918,-0.046,0.03,0.0288,-0.0553,0.0236,-0.0525,0.0166,0.0259,-0.031,-0.0135,-0.0585,-0.0044,0.0105,0.0035,-0.0278,-0.0183,0.0217,-0.0282,-0.0097,0.0312,-0.0371,-0.0052,-0.0342,0.0199,0.0287,-0.0508,0.0661,0.0365,-0.0367,0.0083,0.0208,-0.008,-0.0211,-0.0031,0.008,0.0559,0.0247,0.0156,0.0202,-0.0652,-0.0187,-0.2166,0.0173,0.0093,-0.0222,0.0407,-0.0684,0.0241,0.0366,0.027,0.0829,0.036,-0.0309,-0.0101,0.0196,0.0031,0.0636,0.0362,0.0888,-0.0345,0.032,-0.0048,0.0036,-0.0161,-0.106,0.0261,-0.0148,0.2341,0.0497,0.033,-0.0159,-0.0052,0.0077,-0.0482,-0.1249,0.0737,0.0222,0.0268,-0.0005,-0.0211,-0.0476,-0.0145,0.0423,-0.0199,-0.126,0.0101,-0.0408,-0.0545,0.0172,-0.0154,0.024,0.0224,-0.0927,0.0495,-0.0253,-0.0507,-0.023,-0.0994,0.0312,-0.0837,0.0327,0.0253,-0.0531,0.0066,-0.0981,0.0518,-0.0125,-0.0791,-0.0287,0.0208,-0.0216,-0.0197,0.085,-0.0087,0.022,0.0521,0.0166,0.0134,-0.0207,-0.0427,0.0359,0.0957,0.0378,0.0429,0.0032,0.0278,-0.0173,0.0711,-0.0114,0.0788,-0.0364,-0.0042,0.0076,-0.0462,0.0207,0.0667,-0.0489,-0.2805,0.0414,0.03,0.0328,-0.0536,0.0253,0.0656,0.0317,-0.0635,-0.0506,-0.0187,0.0069,0.0461,-0.0376,0.0007,0.0308,0.0701,-0.0715,0.059,0.013,0.0016,0.0311,0.2098,0.003,0.0419,-0.03,-0.0278,0.0058,0.0613,-0.0285,-0.005,0.004,0.0943,-0.0451,0.0543,0.0356,-0.0126,0.0274,0.0059,-0.0246,-0.0177,-0.0283,-0.0066,0.0053,0.0909,0.0026,0.0137,0.0189,-0.0036,0.0037,-0.0341,0.0481,-0.0075,0.0262,0.0307,-0.0252,-0.0388,-0.0485,-0.0014,-0.0263,0.01,-0.0518,-0.0141,0.0093,-0.0149]}
{"key":"[On Counterfactual Explanations under Predictive Multiplicity] Counterfactual explanations are usually obtained by identifying the smallest change made to an input to change a prediction made by a fixed model (hereafter called sparse methods). Recent work, however, has revitalized an old insight: there often does not exist one superior solution to a prediction problem with respect to commonly used measures of interest (e.g. error rate). In fact, often multiple different classifiers give almost equal solutions. This phenomenon is known as predictive multiplicity (Breiman, 2001; Marx et al., 2019). In this work, we derive a general upper bound for the costs of counterfactual explanations under predictive multiplicity. Most notably, it depends on a discrepancy notion between two classifiers, which describes how differently they treat negatively predicted individuals. We then compare sparse and data support approaches empirically on real-world data. The results show that data support methods are more robust to multiplicity of different models. At the same time, we show that those methods have provably higher cost of generating counterfactual explanations under one fixed model. In summary, our theoretical and empiricaln results challenge the commonly held view that counterfactual recommendations should be sparse in general.","layer":1,"vector":[-0.0047,-0.009,0.0072,0.0025,0.0252,0.0054,0.0074,0.0389,0.0327,-0.0401,-0.0029,-0.0593,-0.0038,0.0592,0.0221,0.0298,-0.0185,0.0132,-0.0313,0.0066,0.0637,-0.0383,-0.0304,-0.0558,0.0267,0.022,-0.0175,-0.0927,-0.0486,-0.2476,0.03,-0.0144,0.0727,0.0059,0.0552,-0.0329,-0.0041,0.0213,-0.0384,0.079,0.0248,0.0327,-0.0473,-0.0496,-0.0495,-0.0495,0.0157,-0.0052,-0.0594,-0.0062,0.0163,-0.0359,0.0313,0.018,0.0549,0.0115,0.0107,0.0721,0.0241,0.0464,0.0437,0.0387,-0.1445,0.0617,0.065,0.0424,-0.02,-0.0548,0.0172,0.088,-0.0157,0.055,0.0207,0.0388,0.0141,-0.0307,-0.03,0.018,-0.015,0.0237,0.0615,0.0014,-0.051,-0.0254,-0.0011,-0.0681,0.052,0.0011,0.0368,-0.0338,-0.0434,-0.0179,0.0162,0.0153,-0.0649,0.0159,0.0182,0.0567,-0.0925,0.1804,-0.0254,-0.0001,0.0253,-0.0182,0.0247,-0.0663,-0.0375,-0.0418,-0.0148,-0.0337,-0.0006,-0.0156,0.0504,-0.0495,0.0247,0.032,0.0657,0.0088,-0.0058,-0.008,-0.0134,-0.0176,0.048,-0.0283,0.0065,-0.0607,0.0034,0.1721,-0.004,-0.0058,0.0311,-0.0442,-0.0723,-0.0347,0.0494,-0.0155,0.0439,0.066,0.0216,0.0189,-0.0143,-0.0537,-0.0034,-0.0932,-0.0822,0.1293,-0.0458,0.0219,-0.034,-0.0409,-0.0346,0.0399,-0.0434,-0.0579,0.0094,0.0047,0.0145,-0.0179,-0.0488,0.0449,0.0248,-0.0497,-0.0345,0.0863,0.0126,-0.0481,-0.007,-0.009,-0.0072,-0.0233,0.0346,0.0435,-0.0186,0.0022,0.0652,0.0475,-0.0867,-0.0084,0.0132,0.0,0.029,-0.0257,-0.0755,0.1084,0.006,-0.0419,-0.0079,-0.0176,0.0281,0.0278,-0.0276,0.0079,-0.0181,0.0314,-0.0305,0.0067,-0.003,-0.0141,0.0306,-0.072,-0.0268,0.0332,-0.0467,-0.0068,-0.0083,0.0399,0.0119,-0.0061,0.0906,0.0178,-0.0482,0.0245,0.0145,-0.0279,-0.0343,0.0226,0.0643,0.0072,0.0079,0.0249,0.0302,-0.0354,-0.032,-0.2268,-0.0259,-0.0118,0.0284,0.051,-0.083,0.0508,-0.0246,-0.0033,0.0863,0.0341,-0.0491,-0.0472,0.0153,-0.0071,0.0535,0.0091,0.049,-0.0389,0.0178,-0.0144,0.0244,0.0169,-0.0869,0.054,0.0311,0.2003,0.0248,0.0026,-0.0115,0.0373,0.0334,-0.0088,-0.0996,0.0818,0.0229,0.0584,-0.082,-0.0567,-0.0364,-0.0058,0.0351,-0.014,-0.0762,-0.0648,-0.0139,-0.0278,0.0089,-0.0565,0.0487,0.0143,-0.02,0.0595,0.0219,0.0177,-0.0451,-0.117,0.0219,-0.0271,0.018,0.0225,-0.0345,0.029,-0.0639,0.0374,0.017,-0.0273,-0.0493,0.0124,0.018,-0.0069,0.1004,-0.0529,-0.053,0.036,0.0218,0.0213,-0.0387,-0.0764,-0.0071,0.0929,-0.0163,0.0028,-0.0046,0.007,-0.0161,0.0911,0.0157,0.0549,-0.0086,0.0108,-0.0137,-0.0232,0.0053,0.05,-0.0033,-0.3067,0.04,0.0225,0.0102,-0.0207,0.0265,0.0456,0.043,-0.0181,-0.0044,0.013,0.0277,0.0527,0.0202,0.0099,0.0137,0.0588,-0.0396,0.0106,-0.0376,0.0455,0.0378,0.2392,-0.0668,-0.0043,0.0268,-0.0338,-0.0114,0.0083,-0.0275,0.0111,0.0024,0.0634,-0.0394,0.0292,0.0569,-0.0643,0.0485,0.0413,-0.035,-0.0119,-0.0161,-0.0298,-0.0582,0.1139,0.0058,-0.0179,-0.0386,-0.0058,0.0204,-0.0071,0.0163,-0.0314,-0.011,-0.0008,0.0051,-0.0559,-0.0261,-0.0091,-0.0615,0.0128,-0.0089,-0.0279,0.0053,0.0022]}
{"key":"[Bayesian Perceptron: Towards fully Bayesian Neural Networks] Artificial neural networks (NNs) have become the de facto standard in machine learning. They allow learning highly nonlinear transformations in a plethora of applications. However, NNs usually only provide point estimates without systematically quantifying corresponding uncertainties. In this paper a novel approach towards fully Bayesian NNs is proposed, where training and predictions of a perceptron are performed within the Bayesian inference framework in closed-form. The weights and the predictions of the perceptron are considered Gaussian random variables. Analytical expressions for predicting the perceptron's output and for learning the weights are provided for commonly used activation functions like sigmoid or ReLU. This approach requires no computationally expensive gradient calculations and further allows sequential learning.","layer":0,"vector":[-0.0493,-0.0275,0.0165,-0.0145,0.035,0.0426,0.0383,0.0393,0.0458,-0.0481,0.0061,-0.0762,-0.0064,0.0888,0.0311,0.0207,0.0082,0.0556,-0.0147,-0.0049,0.0447,-0.005,-0.0003,-0.0213,0.0087,0.0088,-0.0094,0.0027,-0.0676,-0.2047,-0.0018,-0.0759,0.0571,-0.0343,-0.0139,-0.0244,-0.0409,0.051,-0.0061,0.0623,0.0457,-0.0015,-0.0259,-0.0769,0.003,-0.0132,-0.0148,-0.0224,-0.0306,-0.0207,0.0407,-0.0178,0.0294,-0.0024,-0.0076,0.0275,0.0122,0.0505,0.0559,0.0784,-0.0005,0.0668,-0.196,0.0697,0.0292,0.0178,-0.0394,-0.07,0.0039,0.0594,-0.0643,0.0698,0.0263,0.0408,0.0177,-0.0011,0.0077,-0.0175,0.0021,0.0477,0.0194,-0.0034,-0.0444,-0.0112,-0.007,-0.0221,0.0131,-0.0332,0.0378,0.0255,-0.0286,0.059,-0.056,-0.0002,-0.045,0.0155,0.0267,0.0231,-0.0792,0.1798,-0.0332,0.0256,0.02,-0.0175,0.0598,-0.0232,-0.0434,-0.0322,-0.029,-0.0054,-0.0356,-0.0601,0.0001,-0.0272,-0.0131,0.0519,0.0388,-0.0095,-0.036,-0.032,-0.0108,0.0047,0.0467,0.0313,0.0173,-0.0425,-0.0086,0.1259,0.0006,0.0346,0.1003,-0.0623,-0.0496,0.006,0.05,0.0326,0.0596,-0.035,0.0065,0.0066,-0.0602,-0.0633,0.0036,-0.0635,-0.0762,0.1053,-0.057,0.0315,-0.0469,-0.0585,-0.0059,0.0628,0.0205,-0.0189,0.0509,0.0283,0.0121,0.0309,-0.0669,0.0088,-0.0204,-0.0515,-0.0404,0.1074,0.0242,-0.0545,-0.0676,0.003,0.0224,-0.0065,0.027,0.0443,-0.0226,0.0027,0.0996,0.0639,-0.033,-0.0122,0.0108,0.0438,0.0013,-0.0421,-0.0036,0.0565,0.0099,-0.0388,0.0111,-0.087,0.0055,0.048,-0.0104,0.024,-0.0143,-0.0141,-0.043,-0.0363,-0.0259,-0.0117,0.016,-0.0359,0.0052,-0.02,-0.0672,-0.0144,0.0008,0.0578,-0.0294,0.0219,0.0771,0.0331,0.0135,-0.0034,0.0819,-0.0724,-0.0188,-0.0295,-0.027,0.0363,0.0211,0.0448,0.0397,-0.0528,-0.0653,-0.1969,0.018,0.0204,-0.0216,0.0698,-0.0805,0.0085,-0.0044,0.0403,0.0099,0.0839,0.019,-0.0119,0.0208,-0.0047,0.0452,0.0078,0.0204,-0.0221,0.0151,-0.0071,0.0251,-0.0475,-0.1068,0.0721,0.0209,0.195,0.0055,0.0666,-0.06,0.0156,0.0082,-0.0246,-0.051,0.0693,0.0216,0.039,-0.005,-0.0382,-0.0177,-0.0062,0.005,-0.0115,-0.1006,-0.0409,-0.0482,-0.0359,0.0255,-0.0617,-0.0053,0.0221,-0.0387,0.0654,-0.0093,-0.0011,-0.0485,-0.0746,0.0296,-0.0313,0.0386,0.0462,-0.0661,-0.0017,-0.0754,0.0255,-0.0463,-0.0251,-0.0461,0.0603,-0.0133,-0.0395,0.1318,0.0168,0.0168,0.0755,-0.0054,0.034,-0.0163,-0.0496,-0.0262,0.0472,-0.0343,0.0557,0.049,0.0303,-0.0324,0.0788,-0.0497,0.0365,0.0132,-0.0185,-0.0139,-0.0259,0.0412,0.0372,0.0267,-0.2695,0.0146,0.011,0.0442,-0.0096,0.0076,0.0564,0.0023,-0.0637,-0.0166,-0.0058,0.0152,0.0624,0.0161,-0.049,-0.0189,0.0363,-0.0328,0.0666,-0.0612,-0.0231,0.0387,0.2301,-0.0545,0.0459,0.0312,-0.0004,-0.0137,0.0477,-0.0282,0.0295,0.0058,0.0587,-0.0571,0.0067,0.1105,-0.049,0.0297,0.0356,-0.0505,0.0305,0.0054,-0.0703,-0.0424,0.1322,-0.0019,-0.0232,-0.0484,-0.0215,0.0455,-0.0201,0.0583,-0.0186,-0.0626,0.0249,0.0104,-0.0458,-0.0407,-0.0692,-0.0476,0.0232,-0.0638,0.0302,0.0027,-0.0625]}
{"key":"[Spectral Simplicial Theory for Feature Selection and Applications to Genomics] The scale and complexity of modern data sets and the limitations associated with testing large numbers of hypotheses underline the need for feature selection methods. Spectral techniques rank features according to their degree of consistency with an underlying metric structure, but their current graph-based formulation restricts their applicability to point features. We extend spectral methods for feature selection to abstract simplicial complexes and present a general framework which can be applied to 2-point and higher-order features. Combinatorial Laplacian scores take into account the topology spanned by the data and reduce to the ordinary Laplacian score in the case of point features. We demonstrate the utility of spectral simplicial methods for feature selection with several examples of application to the analysis of gene expression and multi-modal genomic data. Our results provide a unifying perspective on topological data analysis and manifold learning approaches.","layer":2,"vector":[-0.0591,-0.0064,0.0204,0.0147,0.0384,0.0046,0.032,0.068,0.0188,-0.0251,0.0089,-0.0593,0.0094,0.0371,0.0223,0.0604,0.0077,0.0595,-0.0668,0.0303,0.029,-0.049,-0.0186,-0.0612,0.0297,-0.009,-0.0222,-0.0307,-0.0366,-0.2551,0.0325,-0.0035,0.0563,-0.0379,0.0191,-0.0621,0.0254,0.054,-0.0498,0.0084,0.0298,0.0366,-0.0211,-0.0042,-0.0543,-0.0693,0.0439,-0.0122,-0.0118,-0.0228,0.0079,-0.063,0.0206,0.0448,0.0355,0.0236,0.0831,0.0201,0.0231,0.0388,0.039,0.0466,-0.1386,0.0545,0.0395,0.0223,-0.0583,-0.0213,0.0231,0.0747,-0.0184,0.0408,-0.0041,0.0286,0.0346,0.0104,0.0023,-0.0393,0.0021,0.0372,0.0112,-0.0149,-0.0514,-0.0125,-0.0118,0.0055,-0.018,-0.0546,-0.0028,-0.0193,-0.0129,-0.0084,-0.0267,0.0162,-0.0502,-0.0569,-0.0079,-0.0006,0.0219,0.1718,-0.0669,0.021,0.0071,-0.0535,0.0121,-0.0721,-0.0615,-0.0621,-0.0099,-0.0122,0.018,0.0027,-0.0088,-0.0421,-0.0271,-0.0115,0.087,0.0547,-0.0164,-0.0194,-0.0112,0.0218,0.0779,-0.0333,0.0377,-0.0467,0.0091,0.108,0.0564,0.012,0.0635,0.0436,-0.0257,0.0057,-0.0332,0.02,0.0446,0.0166,0.0128,0.0227,-0.0373,-0.0657,0.0374,-0.0496,-0.0504,0.158,-0.091,0.0168,-0.0581,0.0192,-0.0316,0.0292,-0.0394,-0.03,-0.0041,0.0315,-0.0242,0.0003,-0.0207,0.0259,-0.0025,-0.0414,-0.0056,0.1514,-0.0147,-0.1038,-0.0408,0.0175,0.0154,0.0179,0.0409,0.0665,-0.0541,0.0266,0.1079,0.0107,-0.0877,-0.0141,0.0405,0.0297,0.0616,-0.032,-0.0574,0.0317,0.0502,-0.0136,-0.0201,-0.001,0.0183,0.0439,-0.0323,0.014,0.0059,-0.0313,-0.0385,-0.015,0.0068,-0.0029,0.0099,-0.0658,0.0023,0.0161,-0.0106,0.0246,-0.0076,0.0057,-0.0256,0.0118,0.002,0.0046,-0.0028,-0.0176,-0.0002,-0.0153,-0.0317,0.0028,0.0194,0.0163,0.0097,0.052,0.0187,-0.0385,-0.0866,-0.2222,-0.054,-0.005,0.0179,0.0174,-0.0885,0.0495,-0.0004,0.0679,0.0537,0.0593,0.0314,-0.0253,0.0273,-0.0097,0.0363,0.0246,-0.0013,-0.0204,0.0209,0.0067,-0.0196,-0.0429,-0.082,0.0514,-0.0297,0.2273,0.0261,0.0214,-0.0295,0.0311,0.019,-0.0181,-0.0364,0.0661,0.0494,0.0434,-0.0062,-0.0021,-0.0064,0.0008,0.0509,-0.0193,-0.0879,-0.0467,-0.0366,-0.029,0.0119,-0.0534,0.0247,0.056,0.0064,0.0527,-0.0145,0.0061,-0.0486,-0.0967,0.0012,-0.0472,0.0008,-0.0062,-0.0828,0.0498,-0.0105,0.0811,-0.0085,-0.0355,-0.0454,0.0248,-0.0372,-0.0239,0.0747,0.0291,-0.0049,0.0608,0.0097,-0.0007,-0.0262,0.0113,-0.0541,0.0327,-0.0876,-0.0378,-0.0047,-0.0012,0.0222,0.1005,-0.0113,0.0261,-0.0341,-0.0149,0.0143,-0.016,-0.0403,0.0377,0.0128,-0.2966,0.0734,-0.0109,0.007,0.0064,-0.0039,0.0384,-0.0179,-0.0487,-0.0044,0.0875,0.019,0.0552,-0.0072,-0.0231,0.0619,0.0695,-0.0401,0.0672,-0.0586,0.0234,0.0329,0.2384,-0.0269,0.0396,0.0441,-0.0219,0.0021,-0.0124,-0.0339,0.0173,0.048,0.0947,-0.0614,0.0188,0.096,-0.0221,0.0175,0.0217,-0.0497,0.0392,-0.0096,-0.057,-0.0333,0.091,-0.04,-0.0355,-0.0023,0.0125,0.0794,-0.0421,-0.0151,-0.0547,-0.0158,-0.0435,0.013,-0.0401,-0.0363,0.0113,-0.0344,-0.014,-0.0453,-0.0431,0.0056,-0.0141]}
{"key":"[Time Series Alignment with Global Invariances] In this work we address the problem of comparing time series while taking into account both feature space transformation and temporal variability. The proposed framework combines a latent global transformation of the feature space with the widely used Dynamic Time Warping (DTW). The latent global transformation captures the feature invariance while the DTW (or its smooth counterpart soft-DTW) deals with the temporal shifts. We cast the problem as a joint optimization over the global transformation and the temporal alignments. The versatility of our framework allows for several variants depending on the invariance class at stake. Among our contributions we define a differentiable loss for time series and present two algorithms for the computation of time series barycenters under our new geometry. We illustrate the interest of our approach on both simulated and real world data.","layer":0,"vector":[-0.0135,-0.0475,0.0418,-0.0287,0.055,0.0004,-0.0044,0.0226,0.05,-0.0391,0.0065,-0.0395,-0.0007,0.0537,-0.0129,-0.0176,-0.0377,0.0494,-0.0485,0.0205,0.0352,-0.0382,0.0056,0.0042,0.0336,-0.0088,-0.0139,-0.007,-0.0174,-0.2519,0.0035,-0.0429,0.0356,-0.0273,-0.0253,-0.0343,-0.0347,0.0552,-0.0124,0.0687,0.0338,0.0115,-0.0197,-0.0451,-0.0382,-0.086,-0.0209,0.0222,-0.0392,0.0277,-0.016,-0.0556,0.036,0.067,0.0112,0.0604,0.085,0.0469,0.0512,0.0199,0.0316,0.0247,-0.1864,0.0732,0.0275,0.0077,-0.0008,-0.0062,0.0112,0.0202,-0.0336,0.0256,0.0404,0.004,0.0139,0.0219,-0.0215,-0.0281,-0.0413,-0.0236,0.0621,-0.0062,-0.042,-0.0305,-0.0274,-0.032,0.0088,-0.0767,0.0459,0.0033,-0.0792,-0.0126,0.0191,0.0487,-0.0951,-0.0564,0.0507,0.0555,-0.0206,0.1921,-0.0784,0.0454,0.0495,-0.0235,0.0817,-0.0507,-0.0388,-0.0245,-0.0091,0.0136,-0.018,-0.0077,0.0073,-0.0621,0.0257,-0.0179,0.0315,0.0215,0.0263,-0.0014,0.016,0.0335,0.0383,-0.0334,0.0177,-0.0578,0.0449,0.1084,0.0144,0.02,0.0745,0.0109,-0.0343,0.0083,0.0172,0.0351,-0.0054,0.0155,0.0338,-0.0398,-0.0395,-0.05,0.0354,-0.0546,-0.0347,0.1357,-0.0508,0.0231,-0.0461,0.0344,-0.0727,-0.0102,-0.0044,-0.0417,0.0182,0.0384,0.0013,0.0148,-0.0525,-0.0175,-0.037,-0.0067,-0.0121,0.1071,0.0126,-0.0981,-0.0132,0.0214,0.0198,0.0226,0.055,0.0045,-0.0182,0.0512,0.1064,0.0641,-0.0172,0.0367,0.0506,0.0088,0.069,-0.0553,-0.0222,0.0265,0.032,-0.0374,0.0024,-0.0006,0.019,0.0282,0.0077,-0.0331,-0.0232,0.0453,-0.0204,0.0052,-0.0103,0.0014,0.0155,-0.0501,0.01,-0.0026,-0.0209,-0.002,-0.0171,0.0033,-0.0665,0.0195,0.0222,0.0477,0.0147,-0.0251,0.0292,-0.0373,-0.0424,-0.0045,0.0226,0.0598,-0.0207,0.0525,0.0133,-0.0379,-0.0247,-0.2556,-0.0084,0.0095,0.0422,0.0518,-0.0627,-0.0213,-0.0326,0.0709,0.029,0.0244,0.0123,0.0001,-0.0029,0.0033,0.057,0.0338,0.0627,-0.0277,-0.0297,-0.0228,0.0101,-0.034,-0.1009,0.0706,-0.0012,0.2071,0.0156,0.0434,-0.0506,0.0108,0.0071,-0.0016,-0.0592,0.0445,0.0617,0.0707,-0.0016,-0.0641,-0.0389,-0.0451,0.0022,0.0647,-0.0768,-0.0548,-0.0152,-0.0565,0.0138,-0.0526,-0.0047,0.044,-0.0498,0.0745,0.0075,-0.0039,-0.0651,-0.066,-0.0156,-0.0318,0.0159,-0.0276,-0.0355,0.0368,-0.053,0.0328,-0.0023,-0.0184,-0.0244,-0.023,-0.0046,-0.0451,0.0435,-0.066,0.0064,0.0677,-0.0075,0.032,0.0336,-0.0575,-0.0223,0.0595,-0.0594,0.0528,0.0236,0.0827,-0.0064,0.072,-0.0204,0.0293,0.0027,0.0001,-0.009,-0.0378,-0.0103,0.0095,-0.0018,-0.278,0.0322,-0.0259,-0.0015,-0.0193,-0.0523,-0.0208,0.0376,0.0034,-0.0278,-0.0268,0.0274,0.0729,-0.0032,0.0072,0.0508,0.0668,-0.0883,0.0487,-0.0704,0.0258,0.0357,0.2303,-0.0255,0.0224,0.0202,-0.003,-0.0154,0.0489,-0.021,0.025,0.0091,0.0837,-0.0231,0.0316,0.0633,-0.0408,0.0578,0.0348,-0.0144,0.0189,0.0405,-0.0039,-0.0295,0.114,-0.0057,-0.0271,-0.0484,-0.014,0.0498,-0.0569,0.0065,-0.0484,0.0264,-0.0052,0.0437,-0.0629,-0.0386,-0.0183,-0.0796,-0.0066,-0.0802,-0.0492,-0.0372,-0.028]}
{"key":"[Seeing All From a Few: Nodes Selection Using Graph Pooling for Graph Clustering] Recently, there has been considerable research interest in graph clustering aimed at data partition using the graph information. However, one limitation of the most of graph-based methods is that they assume the graph structure to operate is fixed and reliable. And there are inevitably some edges in the graph that are not conducive to graph clustering, which we call spurious edges. This paper is the first attempt to employ graph pooling technique for node clustering and we propose a novel dual graph embedding network (DGEN), which is designed as a two-step graph encoder connected by a graph pooling layer to learn the graph embedding. In our model, it is assumed that if a node and its nearest neighboring node are close to the same clustering center, this node is an informative node and this edge can be considered as a cluster-friendly edge. Based on this assumption, the neighbor cluster pooling (NCPool) is devised to select the most informative subset of nodes and the corresponding edges based on the distance of nodes and their nearest neighbors to the cluster centers. This can effectively alleviate the impact of the spurious edges on the clustering. Finally, to obtain the clustering assignment of all nodes, a classifier is trained using the clustering results of the selected nodes. Experiments on five benchmark graph datasets demonstrate the superiority of the proposed method over state-of-the-art algorithms.","layer":0,"vector":[0.021,-0.0125,0.0069,0.0002,0.063,0.0337,0.0103,0.0248,0.0058,-0.0241,-0.0114,-0.0789,0.0192,0.0669,0.0229,0.0717,0.0366,0.0563,-0.0251,-0.0063,0.0254,-0.03,-0.0019,-0.0765,0.0581,0.0535,-0.0071,-0.0082,-0.0636,-0.2352,0.0174,-0.0539,0.0617,-0.0285,-0.0045,-0.0516,0.0372,0.0395,-0.0597,0.0523,0.013,-0.0045,-0.054,-0.0331,-0.0339,0.0012,-0.0263,-0.0003,-0.0003,-0.0635,0.0582,-0.0281,0.0275,0.0064,0.0432,0.0727,0.0308,0.0446,0.0068,0.0498,0.0424,0.0474,-0.129,0.017,0.0627,0.0055,-0.0567,0.0331,0.0137,0.042,0.0372,0.0297,-0.0053,0.0018,0.0357,0.0542,0.0127,-0.0254,0.0141,-0.0354,-0.0441,-0.0265,-0.012,-0.0161,0.0065,0.0011,0.0082,-0.0558,0.0124,0.0035,-0.0188,0.0018,-0.0589,0.0123,-0.0852,-0.0577,0.0681,0.0183,-0.0428,0.1964,-0.0622,0.0527,0.0356,-0.0355,0.0101,-0.1107,0.0154,-0.0407,-0.0361,-0.0216,0.0035,-0.0021,-0.014,-0.0818,0.0205,0.0135,0.0591,0.0491,-0.0006,0.0174,-0.0106,0.0133,0.0453,-0.0415,0.0647,-0.0571,0.0046,0.0963,0.0429,0.0199,0.0804,0.0194,-0.0392,0.0094,0.0132,0.018,0.0018,-0.0231,-0.0038,-0.0194,-0.0243,-0.0344,0.0161,-0.0923,-0.0674,0.1281,-0.0738,-0.0258,-0.0025,-0.0362,-0.0234,-0.0261,-0.0545,-0.0147,-0.0288,0.0315,0.0618,0.0506,-0.042,0.0011,-0.0299,-0.0462,-0.0565,0.1265,0.0498,-0.1113,-0.0312,-0.0058,0.0202,-0.0531,0.0167,0.055,-0.0312,0.0443,0.0936,0.0202,-0.0726,-0.0405,0.0178,-0.0207,0.0114,0.0193,-0.0346,0.0112,0.0547,-0.0216,-0.0307,-0.0502,0.0211,0.0524,-0.0732,0.0196,-0.0177,-0.0121,-0.0204,-0.0406,-0.0291,-0.0413,-0.0163,-0.0322,0.0225,0.0575,-0.0046,0.0436,-0.0312,0.0283,-0.0217,-0.0167,0.0045,0.0254,-0.058,-0.0132,0.065,-0.0405,-0.018,-0.0042,0.0364,0.0589,0.0557,0.0517,0.055,-0.036,-0.0658,-0.1988,-0.047,0.0048,-0.027,0.0326,-0.0289,0.0104,0.0326,0.0684,0.069,0.0603,-0.0157,-0.0215,0.0102,-0.0273,0.0336,0.0434,0.0607,-0.0027,0.013,0.0132,0.0204,-0.0027,-0.0265,0.0679,0.0081,0.2288,0.0182,0.0277,-0.0094,0.0052,0.0193,-0.0938,-0.1059,0.0533,0.0552,0.0181,-0.0085,-0.037,0.0026,-0.053,-0.0085,-0.007,-0.1032,-0.0096,-0.0323,-0.0157,0.0125,-0.0432,-0.018,0.025,-0.0018,0.06,0.0191,-0.0529,-0.0307,-0.0527,0.0346,-0.0492,0.0073,0.0317,-0.1062,-0.0107,-0.0753,0.0631,0.0083,-0.0733,0.0015,-0.0106,-0.0242,-0.0075,0.0999,0.0275,-0.0173,0.0481,0.0107,0.0367,-0.0158,-0.022,-0.0123,0.0521,-0.0683,0.0605,0.0387,0.0205,0.0128,0.0575,0.0131,0.0226,-0.0098,0.0003,-0.0197,-0.0474,-0.0232,0.0247,-0.0286,-0.3059,0.0176,0.0153,0.0211,-0.0335,-0.005,0.0246,0.0585,-0.009,0.0182,0.0604,0.0269,0.0388,-0.0176,-0.0123,0.0599,0.0407,-0.0184,0.0337,-0.0295,0.0363,0.0485,0.2192,-0.0113,0.0389,0.0262,-0.0271,-0.0188,0.015,-0.0163,-0.0382,0.0145,0.1098,-0.0674,0.0103,0.085,-0.0199,0.0243,0.0362,-0.0323,0.0092,-0.0227,-0.0493,-0.0285,0.0735,0.0025,-0.007,-0.049,0.0202,0.0274,-0.0458,0.0082,-0.0334,0.0044,0.0452,0.0279,-0.0397,-0.0406,-0.1085,-0.0622,0.0017,-0.051,0.0118,-0.0276,0.0028]}
{"key":"[Gradient Boosting Machine: A Survey] In this survey, we discuss several different types of gradient boosting algorithms and illustrate their mathematical frameworks in detail: 1. introduction of gradient boosting leads to 2. objective function optimization, 3. loss function estimations, and 4. model constructions. 5. application of boosting in ranking.","layer":0,"vector":[-0.0724,0.0138,0.0063,-0.0151,0.0351,-0.0029,0.0151,0.0266,0.0496,0.018,0.0192,-0.0712,0.0307,0.0558,0.0242,0.0135,0.0351,0.0629,-0.0492,-0.0259,0.0515,-0.0064,-0.0226,-0.0874,0.0149,0.003,-0.0387,0.0034,-0.0679,-0.2242,-0.0246,-0.0678,0.0879,-0.0469,-0.0019,-0.0013,-0.0285,0.0209,-0.0584,0.0187,0.0057,0.0104,-0.0461,-0.0109,-0.0198,-0.0567,-0.0424,-0.0365,-0.0441,-0.0071,0.0336,0.0063,0.008,0.0131,0.0239,0.0142,0.0728,0.0655,0.0375,0.0126,0.0496,0.027,-0.1737,0.0252,0.0155,0.0012,-0.0156,-0.0368,-0.0294,0.079,-0.0058,-0.0033,0.0243,0.0196,-0.0093,0.0076,0.0071,-0.0287,-0.0026,0.0016,0.0244,-0.018,-0.0626,-0.0546,0.0467,-0.0413,0.0215,-0.0428,0.0521,0.0171,0.0054,0.0274,-0.0207,0.0325,-0.0392,-0.0163,0.0456,0.0356,-0.0534,0.2137,-0.0455,0.0665,0.0376,-0.0615,0.0245,-0.0317,-0.0064,-0.0028,-0.0082,-0.0311,-0.0647,0.0221,0.034,-0.0155,0.0127,0.0092,0.0316,0.0904,-0.0159,-0.009,-0.0237,0.0168,0.0743,-0.0462,0.0339,-0.0417,0.0276,0.1158,0.0251,0.0568,0.0604,-0.0404,-0.0464,-0.013,0.0012,0.0152,-0.0081,-0.0161,0.0389,0.0228,-0.0382,-0.0522,0.0113,-0.1007,-0.03,0.1272,-0.0511,0.0203,-0.0485,-0.0813,-0.0386,0.0024,-0.0309,-0.0457,0.0433,0.0417,0.0334,0.064,-0.0499,0.0126,-0.0503,-0.001,-0.0291,0.1164,0.0243,-0.0937,-0.0428,-0.02,-0.0203,-0.0159,0.0489,0.0483,-0.0491,0.0482,0.0773,0.0091,-0.0469,-0.0655,0.0118,-0.012,0.0585,-0.0364,-0.0608,0.0497,0.04,0.0059,-0.0095,-0.077,0.0395,0.0497,-0.055,-0.0028,-0.0238,-0.0174,-0.0121,-0.0611,-0.0505,-0.0129,0.0406,-0.0435,0.0026,0.0383,-0.0204,0.0078,-0.0061,0.0306,-0.0122,-0.0194,0.0286,0.0374,-0.0459,-0.0348,0.0821,-0.017,-0.0332,-0.0235,0.0559,0.034,-0.006,0.046,0.0229,-0.0081,-0.0541,-0.2126,-0.0167,0.0044,-0.0032,0.0145,-0.0615,0.061,0.0066,0.0537,0.0829,0.0723,0.007,-0.0518,0.0291,0.0066,0.0431,-0.005,0.0031,0.0108,-0.003,0.0032,0.0489,0.0243,-0.078,0.0621,0.0108,0.1699,0.0139,0.0245,-0.0394,0.0398,-0.0316,-0.0335,-0.0583,0.0735,-0.0045,0.047,-0.037,-0.0436,0.0341,-0.0194,0.0398,0.0567,-0.0801,-0.0325,-0.0301,-0.0529,0.0108,-0.0442,0.0462,0.0183,-0.0368,0.0613,-0.0178,0.0058,-0.0685,-0.098,0.0147,-0.0435,0.0202,0.0238,-0.0832,0.0272,-0.0788,0.0837,-0.0293,-0.0114,0.0218,-0.0073,-0.0171,-0.0242,0.0784,0.0179,0.0311,0.0586,-0.0091,0.0426,-0.0391,-0.0471,-0.0349,0.0263,-0.0109,0.0433,-0.0107,0.0161,-0.0235,0.09,-0.0106,0.0417,-0.0007,-0.0308,0.0019,-0.0579,0.0098,0.0425,-0.0063,-0.3061,0.0179,0.0429,0.0242,-0.0119,-0.0035,0.0587,0.0067,-0.035,0.0407,-0.0283,0.058,0.012,-0.0638,0.0142,0.0367,0.0186,-0.011,0.0474,-0.0629,0.0243,0.0558,0.2581,0.0036,0.0424,-0.0006,-0.0339,-0.0036,0.0102,-0.0346,-0.0164,0.0342,0.1104,-0.0395,0.0252,0.1033,-0.0009,0.0008,-0.0209,-0.0434,0.004,0.0547,-0.0586,-0.0158,0.0841,-0.0088,-0.022,-0.0209,-0.0208,-0.0068,-0.044,0.0205,0.0106,0.02,0.0337,0.0049,-0.0614,-0.0138,-0.033,-0.0167,0.0266,-0.0687,-0.0306,0.004,0.0005]}
{"key":"[Deep Learning in Detection and Diagnosis of Covid-19 using Radiology Modalities: A Systematic Review] Purpose: Early detection and diagnosis of Covid-19 and accurate separation of patients with non-Covid-19 cases at the lowest cost and in the early stages of the disease are one of the main challenges in the epidemic of Covid-19. Concerning the novelty of the disease, the diagnostic methods based on radiological images suffer shortcomings despite their many uses in diagnostic centers. Accordingly, medical and computer researchers tended to use machine-learning models to analyze radiology images. Methods: Present systematic review was conducted by searching three databases of PubMed, Scopus, and Web of Science from November 1, 2019, to July 20, 2020 Based on a search strategy, the keywords were Covid-19, Deep learning, Diagnosis and Detection leading to the extraction of 168 articles that ultimately, 37 articles were selected as the research population by applying inclusion and exclusion criteria. Result: This review study provides an overview of the current state of all models for the detection and diagnosis of Covid-19 through radiology modalities and their processing based on deep learning. According to the finding, Deep learning Based models have an extraordinary capacity to achieve an accurate and efficient system for the detection and diagnosis of Covid-19, which using of them in the processing of CT-Scan and X-Ray images, would lead to a significant increase in sensitivity and specificity values. Conclusion: The Application of Deep Learning (DL) in the field of Covid-19 radiologic image processing leads to the reduction of false-positive and negative errors in the detection and diagnosis of this disease and provides an optimal opportunity to provide fast, cheap, and safe diagnostic services to patients.","layer":2,"vector":[-0.0012,0.0203,0.0556,0.0049,0.0725,0.0229,0.0398,0.0079,0.0391,0.0137,0.0331,-0.0527,0.0023,0.0878,-0.0102,-0.009,0.0083,-0.003,-0.0062,-0.0306,0.0007,-0.0061,-0.0228,-0.071,0.0271,0.0067,-0.023,-0.0516,-0.0625,-0.1678,0.0419,-0.0321,0.0379,-0.0363,0.028,-0.0424,-0.0435,0.0443,-0.0384,-0.0197,0.0128,-0.0093,0.0331,-0.058,-0.0142,-0.0714,-0.0097,-0.0136,0.0183,-0.0118,0.0472,0.0144,0.0227,0.0526,0.0089,0.0436,0.0663,0.0518,0.0828,-0.0109,0.0223,0.0523,-0.212,0.064,0.0388,0.0236,-0.0112,-0.0543,0.029,0.0394,0.0073,0.0537,-0.0102,-0.0216,0.0543,0.0067,-0.0116,-0.0022,0.0347,0.0079,-0.0084,0.0735,-0.0143,-0.017,-0.0314,-0.0558,-0.0403,-0.0502,0.0278,-0.0234,0.0027,0.0175,-0.0483,0.0438,-0.0624,-0.0107,0.0101,0.0585,-0.0562,0.1564,-0.0424,-0.0105,0.0492,-0.0414,0.0134,-0.0332,-0.0583,-0.0311,-0.0294,-0.0113,-0.0334,-0.0447,0.0865,-0.0189,-0.0105,0.0124,0.0653,0.0387,-0.0148,-0.0032,-0.0302,-0.0077,0.063,0.008,-0.0021,-0.0345,0.0226,0.1236,0.0069,0.0305,0.0681,-0.0435,-0.0163,0.0145,-0.0056,0.003,0.0128,0.0255,-0.0283,0.0516,-0.0647,-0.0842,0.036,-0.0977,-0.0426,0.0782,-0.0898,0.0128,-0.0338,-0.101,-0.0549,0.0033,-0.079,-0.0236,0.0203,0.0119,0.0336,0.0571,-0.0655,0.0239,0.0213,-0.0521,-0.0771,0.1567,-0.01,-0.0454,-0.0209,-0.0014,0.0369,-0.0344,0.0565,0.0831,-0.0341,-0.0242,0.0748,0.0304,-0.0402,-0.0117,-0.0261,0.0139,0.0072,-0.0036,-0.0273,0.0592,0.0431,-0.0575,0.0353,-0.0855,0.0666,-0.0051,-0.0145,0.0426,-0.0301,-0.0211,0.0431,-0.0138,-0.039,-0.017,-0.0217,0.006,0.0077,-0.0323,0.0047,0.0264,-0.0254,-0.0284,-0.0045,0.0015,0.0167,0.0147,-0.0146,-0.0078,0.0544,-0.0085,-0.0517,0.0076,0.045,0.0356,-0.0413,0.0547,0.0587,-0.0216,-0.0693,-0.205,0.0064,0.0113,-0.0035,0.009,-0.0874,0.0183,0.0199,0.0636,0.0426,0.0682,0.0567,-0.0003,-0.044,-0.0162,0.0502,0.0338,0.0592,-0.0562,-0.0415,-0.0065,0.0389,0.0112,-0.0984,0.0128,0.0068,0.1898,0.0263,0.0165,-0.0028,0.0063,0.0017,-0.0029,-0.1163,0.0601,0.0334,0.0817,0.0402,-0.0657,-0.0282,-0.0211,0.0108,0.0009,-0.0745,-0.0151,-0.0152,-0.0334,0.0763,-0.0253,0.0336,0.0554,-0.0369,-0.0204,0.0079,-0.0368,-0.0405,-0.1116,0.008,-0.0356,-0.0066,0.0006,-0.0632,-0.0173,-0.0789,0.0222,0.0033,-0.0527,-0.0218,0.0058,0.007,-0.0233,0.1063,-0.0289,-0.002,0.0699,-0.0051,0.075,0.0013,-0.0166,-0.0637,0.0483,-0.0103,0.0636,0.0746,0.0365,0.0133,0.094,-0.0052,-0.0011,-0.0345,0.0049,0.0202,-0.0731,-0.0448,0.0205,0.0063,-0.3148,0.0582,0.0132,0.0162,-0.0048,0.0048,0.0143,0.0069,-0.0131,0.0007,-0.0196,0.0045,0.0356,-0.0583,-0.0368,0.0305,0.0689,-0.0479,0.082,-0.0548,-0.0299,0.03,0.1941,-0.0755,0.0055,0.0505,-0.0182,-0.0103,0.0147,0.0429,-0.0014,-0.0087,0.0662,-0.039,0.0072,0.1552,-0.0044,0.0783,-0.0215,-0.0264,0.0215,0.066,-0.006,-0.0141,0.0507,-0.0081,-0.0246,-0.01,-0.0177,0.0209,-0.0603,0.0017,-0.0237,-0.0283,0.0464,-0.0098,-0.051,-0.0514,-0.0038,-0.0368,0.0313,-0.042,-0.0067,0.0542,-0.0013]}
{"key":"[Gated Information Bottleneck for Generalization in Sequential Environments] Deep neural networks suffer from poor generalization to unseen environments when the underlying data distribution is different from that in the training set. By learning minimum sufficient representations from training data, the information bottleneck (IB) approach has demonstrated its effectiveness to improve generalization in different AI applications. In this work, we propose a new neural network-based IB approach, termed gated information bottleneck (GIB), that dynamically drops spurious correlations and progressively selects the most task-relevant features across different environments by a trainable soft mask (on raw features). GIB enjoys a simple and tractable objective, without any variational approximation or distributional assumption. We empirically demonstrate the superiority of GIB over other popular neural network-based IB approaches in adversarial robustness and out-of-distribution (OOD) detection. Meanwhile, we also establish the connection between IB theory and invariant causal representation learning, and observed that GIB demonstrates appealing performance when different environments arrive sequentially, a more practical scenario where invariant risk minimization (IRM) fails. Code of GIB is available at https://github.com/falesiani/GIB","layer":0,"vector":[0.0082,-0.0183,0.0213,-0.0362,0.0242,0.0394,0.046,-0.0171,0.0303,-0.0475,-0.0343,-0.0395,0.0571,0.0902,0.0272,0.0141,0.0234,0.0136,-0.0521,-0.0326,0.0337,-0.0459,0.0165,-0.0474,-0.0016,-0.0049,-0.0307,0.0212,-0.0283,-0.2523,0.0179,-0.0362,0.0342,-0.0115,0.025,-0.0389,-0.0606,0.0917,0.0027,0.0345,0.0063,0.0463,-0.0476,-0.0958,0.0025,-0.0024,-0.0363,0.0087,-0.0146,-0.0708,-0.005,-0.0052,0.0288,0.0088,0.0349,0.0413,0.0339,0.0551,0.0496,0.0508,-0.0176,0.0381,-0.1444,0.0382,0.0704,0.0165,-0.0523,-0.0055,0.0248,0.0512,0.0114,0.033,0.0072,0.0666,-0.0098,0.028,-0.0299,-0.0388,0.0191,0.0252,0.0654,0.0084,-0.0423,-0.0414,-0.0188,-0.0586,0.0237,-0.0488,0.0524,-0.0204,-0.0555,-0.0102,0.015,0.0735,-0.0357,0.0111,0.0502,0.0266,-0.0543,0.1762,-0.0374,0.0218,0.0319,-0.0027,0.0754,-0.022,-0.0271,-0.0124,-0.0493,0.0053,-0.0037,-0.0166,0.0188,-0.0414,0.0243,0.0333,0.0809,0.0209,-0.0283,-0.0013,-0.0197,-0.0182,0.0382,-0.0259,0.0164,-0.0522,0.008,0.1129,0.0457,0.02,0.0431,-0.0363,-0.0518,-0.0009,0.0234,0.0199,0.0134,0.0624,0.0044,-0.0214,-0.0598,0.0225,0.0344,-0.0843,-0.0797,0.137,-0.0735,0.0176,-0.0215,-0.009,-0.0008,-0.0134,-0.0066,-0.0518,0.0121,0.0362,0.0244,0.05,-0.0785,0.0134,-0.022,-0.0547,-0.0129,0.0755,0.0191,-0.091,-0.0222,0.0259,0.0558,-0.0436,-0.0044,0.041,-0.0193,0.0379,0.0635,-0.0151,-0.084,0.0045,-0.0104,0.0315,0.0046,-0.0417,0.0056,0.0256,0.0424,-0.0176,0.0172,-0.0227,0.0404,0.0014,-0.0648,-0.0091,-0.0335,-0.0251,-0.0155,0.0182,-0.0291,-0.0023,-0.0128,-0.0061,-0.0219,-0.0142,-0.0679,0.0127,0.0066,0.0217,-0.0266,-0.0306,0.0371,0.0316,-0.0055,-0.0363,0.0282,-0.0367,0.0009,-0.0124,0.0291,0.0155,-0.006,0.0252,0.0249,-0.051,-0.0519,-0.2594,-0.0112,-0.0323,-0.0189,0.0261,-0.0858,0.0621,0.0179,0.0245,0.0872,0.0306,-0.0293,-0.0316,0.0269,-0.0188,0.0882,0.021,0.0447,-0.0287,0.0305,-0.0257,0.0474,0.0094,-0.0733,0.0376,-0.0055,0.2263,0.0232,0.044,-0.0277,-0.017,0.0598,-0.0316,-0.0785,0.0322,0.0135,0.0512,-0.0003,-0.052,-0.0241,-0.0314,0.0026,0.0344,-0.1199,-0.0037,-0.0246,-0.0305,0.0157,-0.0599,-0.0041,0.0531,-0.0395,0.0459,0.0443,0.0224,-0.0422,-0.0775,0.0356,-0.0976,0.0145,0.0057,-0.0314,-0.0342,-0.0127,0.0211,-0.0166,-0.0045,-0.0871,0.057,0.009,-0.0116,0.0431,-0.0003,-0.0147,0.0905,0.005,0.0491,-0.0358,-0.0761,-0.0076,0.075,-0.0081,0.0173,0.004,0.0129,-0.0012,0.0681,-0.0325,0.0216,-0.0082,0.0011,0.0164,-0.0556,0.0016,0.0549,-0.0123,-0.3114,0.0598,0.0292,0.0233,-0.0125,0.0107,0.0375,0.0061,-0.04,-0.0223,-0.0207,0.0351,0.0208,0.0206,0.0105,0.0351,0.0516,-0.0865,0.0419,-0.0389,0.0043,0.0688,0.241,-0.0139,-0.003,0.0054,-0.0074,0.0251,0.011,-0.0278,0.0489,0.0337,0.0791,-0.0705,0.0167,0.0888,-0.0194,0.0377,0.0237,-0.0141,-0.0116,0.0251,-0.02,-0.0263,0.0869,-0.0267,-0.0256,-0.0201,0.0001,0.0574,-0.0273,-0.0127,-0.0323,-0.0205,0.0602,0.0434,-0.0587,-0.0188,-0.0322,-0.0701,0.018,-0.0682,-0.0581,0.0136,-0.0499]}
{"key":"[Neural Copula: A unified framework for estimating generic high-dimensional Copula functions] The Copula is widely used to describe the relationship between the marginal distribution and joint distribution of random variables. The estimation of high-dimensional Copula is difficult, and most existing solutions rely either on simplified assumptions or on complicating recursive decompositions. Therefore, people still hope to obtain a generic Copula estimation method with both universality and simplicity. To reach this goal, a novel neural network-based method (named Neural Copula) is proposed in this paper. In this method, a hierarchical unsupervised neural network is constructed to estimate the marginal distribution function and the Copula function by solving differential equations. In the training program, various constraints are imposed on both the neural network and its derivatives. The Copula estimated by the proposed method is smooth and has an analytic expression. The effectiveness of the proposed method is evaluated on both real-world datasets and complex numerical simulations. Experimental results show that Neural Copula's fitting quality for complex distributions is much better than classical methods. The relevant code for the experiments is available on GitHub. (We encourage the reader to run the program for a better understanding of the proposed method).","layer":2,"vector":[-0.0529,-0.0166,0.0426,-0.0385,0.0545,0.0586,0.0168,0.0104,0.0714,-0.0119,0.019,-0.0662,0.0349,0.0463,0.0008,0.0027,-0.0226,0.0851,-0.0596,0.0081,0.0323,-0.0305,-0.0434,-0.0702,0.0272,0.0166,-0.0551,-0.004,-0.0247,-0.257,0.0286,-0.0453,0.0526,-0.0654,0.0329,-0.0367,-0.0262,0.0378,0.0213,0.0588,-0.0185,0.0084,-0.0361,-0.0465,0.0026,-0.0987,-0.0248,0.0026,-0.0171,-0.0334,0.0302,0.0007,0.0133,0.0194,0.0147,-0.0214,0.0286,-0.0168,0.0373,0.0654,-0.005,0.0785,-0.1905,0.026,0.0589,0.0105,-0.0133,-0.0148,0.0234,0.0345,-0.0308,0.04,0.0351,0.0499,0.0297,0.0027,-0.0026,-0.0665,-0.0469,0.0058,0.0367,0.0015,-0.0582,-0.0122,0.0112,-0.0563,0.0441,-0.0471,0.0313,-0.0032,-0.0076,-0.0088,-0.0445,0.0351,-0.0886,-0.0227,0.0637,0.0008,-0.021,0.1935,-0.0941,-0.0167,0.1003,-0.0056,0.0096,-0.0027,-0.0531,-0.017,-0.0121,-0.0378,-0.0072,-0.0271,-0.0099,-0.0071,0.0228,-0.0212,0.0404,-0.0123,-0.0004,-0.0104,-0.0118,0.0102,0.0735,0.0155,0.0224,-0.0466,-0.0131,0.0984,0.0157,0.0068,0.089,-0.0169,-0.0587,-0.0315,-0.0047,0.064,0.0045,0.0127,-0.0349,0.0289,-0.0437,-0.0402,0.0377,-0.0915,-0.0436,0.1249,-0.0358,-0.0139,0.0195,-0.0352,-0.0417,0.0562,-0.0195,-0.0398,0.0466,0.0444,0.0128,0.0695,-0.0333,0.0452,-0.0348,-0.0538,-0.0589,0.1091,-0.013,-0.05,-0.0295,0.0206,0.0431,-0.0345,0.0378,0.0372,-0.0488,0.0346,0.1091,0.0014,-0.0446,-0.0008,0.0185,0.0192,0.0481,-0.0236,-0.0421,0.0124,0.0262,-0.0476,0.0128,-0.0234,-0.0149,0.0344,-0.0323,-0.0088,-0.0042,0.0216,-0.0185,-0.0354,-0.0009,-0.0006,0.0402,-0.025,-0.0019,-0.0115,-0.0625,0.018,-0.0154,0.0724,0.0141,-0.0225,0.0204,0.0655,0.0078,-0.0204,0.1266,-0.0013,-0.0149,0.0589,0.0073,0.0292,-0.0009,0.0732,0.0153,-0.076,-0.0363,-0.2384,-0.0028,0.0153,-0.0169,0.049,-0.0558,0.041,0.0033,0.0537,0.0907,0.0987,0.025,-0.0129,0.0151,-0.0066,0.0528,0.0159,0.0378,-0.0667,-0.0194,-0.0129,-0.0273,-0.0,-0.0436,0.0597,0.0394,0.1303,-0.0217,0.0551,-0.0384,0.0363,0.0215,0.018,-0.0841,0.0696,0.0362,0.064,-0.0306,-0.0737,-0.0406,-0.0363,0.0708,0.0069,-0.061,-0.0228,-0.0207,-0.0008,0.0473,-0.0795,-0.006,0.0145,-0.0189,0.0714,-0.0097,0.0145,-0.0817,-0.0579,0.0089,-0.0435,0.004,0.0136,-0.0734,0.0186,-0.107,0.0171,-0.0214,-0.0224,-0.0338,0.0147,-0.0093,-0.03,0.119,-0.0104,0.0113,0.0449,0.014,0.059,-0.0221,-0.0425,-0.0468,0.0434,-0.0359,0.057,0.0022,0.0449,-0.04,0.074,-0.0303,0.0296,-0.0421,0.0101,-0.006,-0.045,-0.0028,0.0305,0.0376,-0.2827,0.0292,-0.0129,0.0024,0.0005,0.0018,0.0179,0.0156,-0.0339,0.0314,0.0016,0.008,0.0605,-0.0398,0.0225,-0.0064,0.003,-0.0871,0.0685,-0.0341,0.0505,0.0476,0.2008,-0.0443,0.0325,-0.0231,-0.0597,0.0029,0.048,-0.0598,0.0191,0.0271,0.0731,-0.0327,0.0471,0.0701,-0.049,0.0415,0.0587,-0.0205,0.0352,0.0246,-0.0358,-0.036,0.1013,0.0047,-0.0363,-0.0521,-0.0065,0.0051,-0.05,0.0243,0.0051,0.0237,0.0186,0.0124,-0.0312,-0.059,-0.051,-0.0364,-0.0182,-0.0608,-0.0138,-0.0126,-0.0142]}
{"key":"[Efficiently escaping saddle points on manifolds] Smooth, non-convex optimization problems on Riemannian manifolds occur in machine learning as a result of orthonormality, rank or positivity constraints. First- and second-order necessary optimality conditions state that the Riemannian gradient must be zero, and the Riemannian Hessian must be positive semidefinite. Generalizing Jin et al.'s recent work on perturbed gradient descent (PGD) for optimization on linear spaces [How to Escape Saddle Points Efficiently (2017), Stochastic Gradient Descent Escapes Saddle Points Efficiently (2019)], we propose a version of perturbed Riemannian gradient descent (PRGD) to show that necessary optimality conditions can be met approximately with high probability, without evaluating the Hessian. Specifically, for an arbitrary Riemannian manifold $\\mathcal{M}$ of dimension $d$, a sufficiently smooth (possibly non-convex) objective function $f$, and under weak conditions on the retraction chosen to move on the manifold, with high probability, our version of PRGD produces a point with gradient smaller than $\\epsilon$ and Hessian within $\\sqrt{\\epsilon}$ of being positive semidefinite in $O((\\log{d})^4 / \\epsilon^{2})$ gradient queries. This matches the complexity of PGD in the Euclidean case. Crucially, the dependence on dimension is low. This matters for large-scale applications including PCA and low-rank matrix completion, which both admit natural formulations on manifolds. The key technical idea is to generalize PRGD with a distinction between two types of gradient steps: \"steps on the manifold\" and \"perturbed steps in a tangent space of the manifold.\" Ultimately, this distinction makes it possible to extend Jin et al.'s analysis seamlessly.","layer":3,"vector":[-0.0445,-0.0411,0.0092,0.0239,-0.0491,0.0135,-0.0212,0.0527,0.0349,0.0223,0.0118,-0.0582,0.0129,0.0591,0.0055,0.041,0.0189,0.0923,-0.0677,0.0619,0.0349,-0.0201,-0.0427,-0.1015,0.0336,-0.0165,0.0013,-0.0159,-0.0175,-0.2646,0.0219,-0.0496,0.0403,-0.0371,0.0077,0.0183,-0.0185,0.0313,-0.059,0.0383,-0.0004,0.0307,-0.066,-0.0059,0.0014,-0.0303,-0.0096,-0.0357,-0.0439,-0.0327,-0.0062,-0.0375,0.0195,0.0146,0.0326,0.0364,0.0618,0.0289,0.0206,0.0598,0.0275,0.0219,-0.1484,0.0499,0.0343,0.019,-0.0261,-0.0557,0.0027,0.1084,-0.0168,-0.0066,-0.0011,0.0325,0.0091,-0.0126,0.0225,-0.0012,-0.0241,0.0194,0.0375,0.0055,-0.0704,0.02,-0.0269,-0.0391,0.0408,-0.0363,0.0607,-0.0004,-0.0214,-0.0153,-0.0382,0.0257,-0.042,-0.016,0.0578,0.0541,-0.0257,0.1941,-0.0649,0.0458,0.0123,0.0101,0.0267,-0.035,-0.017,-0.008,0.0106,-0.0175,-0.0192,-0.0229,0.0227,-0.0195,0.0021,0.0125,0.0501,0.0809,-0.0391,0.0439,-0.0649,-0.006,0.0917,-0.0175,0.0286,-0.0699,-0.0127,0.1205,0.0275,0.0478,0.0535,-0.0159,0.0042,-0.0063,-0.0063,-0.0222,-0.0207,0.004,0.0348,0.008,-0.0668,-0.0575,0.0183,-0.0988,-0.0363,0.1492,-0.0631,0.0295,-0.051,-0.0527,-0.0092,0.021,-0.0422,-0.0158,-0.0038,-0.0087,-0.0122,0.0064,-0.0586,0.0169,-0.0227,-0.0716,-0.0064,0.1291,-0.027,-0.0719,-0.0137,0.0038,-0.0037,0.0006,0.0335,0.0503,-0.0388,0.0179,0.1102,0.0432,-0.1153,-0.0138,-0.0158,-0.0055,0.058,-0.0085,-0.027,0.0007,0.0313,-0.0472,-0.0088,-0.0248,0.0423,0.0334,-0.0341,-0.0043,-0.0333,-0.0337,-0.0132,-0.0512,-0.0162,-0.0082,0.0419,0.0168,0.0151,-0.0034,-0.0334,0.0528,0.0091,0.0447,0.0247,-0.0251,-0.0173,0.0536,-0.0136,-0.0554,0.0268,-0.0381,-0.0365,-0.0018,0.001,-0.0036,-0.0348,0.0522,0.041,0.0081,-0.0502,-0.2091,-0.0181,-0.014,-0.0111,0.0266,-0.107,0.0342,0.0036,0.0279,0.0627,0.0389,-0.002,-0.0395,0.052,0.0397,0.0661,-0.0093,0.0144,-0.0253,-0.0451,0.0234,-0.0002,-0.042,-0.0946,0.0648,-0.0436,0.2295,0.0365,0.0457,-0.0561,0.0372,-0.0099,-0.0046,-0.0828,0.0634,-0.0043,0.0752,-0.0358,-0.0211,-0.0241,-0.0418,0.063,0.0308,-0.0733,-0.0333,-0.0616,-0.0419,0.0169,-0.0485,0.0406,0.0693,-0.0071,0.0496,-0.0468,-0.0175,-0.0439,-0.0641,-0.015,-0.0444,0.0721,-0.0253,-0.0831,0.0152,-0.0597,0.0568,-0.0001,-0.0147,-0.019,0.0394,-0.0277,-0.015,0.0606,0.0195,-0.0154,0.0476,-0.0117,0.0376,0.0493,-0.0136,-0.0472,0.0427,-0.0291,0.0267,0.0416,0.0193,-0.0148,0.1078,-0.0633,0.0014,-0.0326,-0.001,0.012,-0.0727,0.0016,0.0651,0.0276,-0.2648,0.0035,0.0223,-0.0289,-0.0084,0.0366,0.0564,-0.0019,-0.0514,-0.0164,0.0256,0.0687,0.0086,-0.0089,0.0607,0.0393,0.0666,-0.0338,0.0475,-0.0729,0.0009,0.0545,0.2285,-0.0366,0.0231,0.008,-0.0245,0.021,0.0367,-0.0109,0.0136,0.009,0.08,-0.0585,0.0502,0.085,-0.0519,0.0329,0.0153,-0.0096,0.0521,0.0035,-0.0145,0.0163,0.0661,0.001,-0.0276,-0.0221,0.0078,0.025,0.0166,0.0221,0.0015,0.0218,0.0198,0.017,-0.0457,-0.0523,-0.0044,-0.0353,-0.014,-0.0803,-0.0562,-0.015,-0.0187]}
{"key":"[RandomBoost: Simplified Multi-class Boosting through Randomization] We propose a novel boosting approach to multi-class classification problems, in which multiple classes are distinguished by a set of random projection matrices in essence. The approach uses random projections to alleviate the proliferation of binary classifiers typically required to perform multi-class classification. The result is a multi-class classifier with a single vector-valued parameter, irrespective of the number of classes involved. Two variants of this approach are proposed. The first method randomly projects the original data into new spaces, while the second method randomly projects the outputs of learned weak classifiers. These methods are not only conceptually simple but also effective and easy to implement. A series of experiments on synthetic, machine learning and visual recognition data sets demonstrate that our proposed methods compare favorably to existing multi-class boosting algorithms in terms of both the convergence rate and classification accuracy.","layer":2,"vector":[-0.0319,-0.0006,0.021,-0.0233,0.0291,0.0064,0.0248,0.0092,0.0051,-0.0054,0.035,-0.0685,0.0083,0.0581,0.0359,0.0222,0.0594,0.0411,-0.0489,-0.0126,0.0017,0.0123,0.0,-0.0513,0.0382,0.0172,0.0024,-0.0648,-0.0565,-0.2324,-0.0137,-0.0594,0.0706,-0.0424,0.0192,-0.054,-0.0166,0.0672,-0.0242,0.0289,0.0204,0.0007,-0.0562,-0.0457,-0.0331,-0.0498,-0.0325,-0.032,-0.0172,-0.0362,0.0206,-0.0139,0.0035,0.0195,-0.0159,0.0168,-0.0091,0.0689,0.0379,0.0304,0.0384,0.0404,-0.1273,0.0499,0.045,0.0271,-0.0503,-0.0427,0.0038,0.0477,0.013,0.0166,0.0356,0.0098,0.0056,-0.031,0.0151,-0.0123,0.0136,-0.0061,0.0287,-0.0358,-0.0362,0.0009,0.0401,-0.0093,-0.0044,-0.0512,0.0666,0.0279,-0.0582,-0.013,-0.0415,0.0436,-0.0224,-0.0111,0.0485,0.0252,-0.0395,0.1981,-0.0288,0.0326,0.0739,-0.0254,0.0504,-0.0414,-0.0036,-0.01,-0.0541,-0.0591,-0.0032,-0.013,0.0084,-0.0094,-0.0022,-0.0298,0.0502,0.057,-0.0432,-0.0227,-0.0241,-0.0333,0.04,-0.0488,0.0022,-0.0312,0.0723,0.1556,0.028,0.0314,0.0291,-0.0072,-0.0558,-0.0497,0.0073,0.0277,0.0177,0.0498,0.0464,-0.0253,0.0091,-0.0408,0.0335,-0.0694,-0.0662,0.1074,-0.0894,0.0514,-0.0238,-0.0594,-0.0354,-0.0006,-0.0635,-0.0614,0.0451,-0.0043,0.0564,0.0366,-0.0475,0.01,-0.0218,-0.0565,0.0403,0.0784,0.0349,-0.077,-0.0505,-0.0074,-0.0161,-0.016,0.0094,0.0916,-0.0467,0.0547,0.0697,0.048,-0.0771,-0.0122,0.0114,-0.0091,0.0055,-0.0363,-0.0866,0.0414,0.0258,0.0107,-0.0256,-0.0484,0.0183,0.027,-0.032,-0.0049,-0.058,-0.0321,-0.027,-0.0858,-0.0219,-0.0277,0.0536,-0.0459,0.0104,0.0335,-0.0142,0.0255,0.0394,0.0219,0.0006,0.0021,0.0314,0.0246,-0.0199,0.0058,0.0702,0.0084,-0.0618,-0.003,0.0256,0.0827,-0.0113,0.0113,0.0419,-0.032,-0.073,-0.2306,0.0127,0.0232,-0.0107,0.0407,-0.0942,0.0257,-0.0008,0.0565,0.077,0.0557,0.0004,-0.0237,0.029,-0.0134,0.048,0.0284,-0.0009,-0.0447,-0.0005,0.0132,0.0589,0.0292,-0.087,0.0859,-0.0025,0.2076,0.031,0.0199,-0.0105,0.0947,0.0408,-0.0263,-0.0909,0.096,0.0051,0.0465,-0.0346,-0.0462,0.0099,0.0093,0.0189,0.0337,-0.1042,-0.0403,-0.0711,-0.0422,0.0133,-0.0627,0.0236,0.0214,-0.0074,-0.0006,-0.0096,0.0249,-0.0254,-0.0919,0.0639,-0.0013,0.01,0.0235,-0.0673,0.0401,-0.0723,0.0826,0.0092,-0.0545,-0.0383,0.026,-0.0436,-0.0484,0.0865,0.008,-0.0124,0.0377,-0.014,0.0122,0.0007,-0.0446,-0.0169,0.021,-0.0115,0.0135,-0.0234,0.0323,-0.0216,0.1133,0.0036,0.0212,0.0291,0.0291,0.0367,-0.0413,0.0463,0.031,0.0064,-0.2997,0.0063,0.0331,0.0171,-0.0152,0.0261,0.0294,-0.0216,-0.0405,0.0159,-0.0046,0.007,0.0516,-0.023,0.0239,0.06,0.0487,-0.0538,0.0422,-0.066,0.0296,-0.0017,0.2106,0.0051,0.0324,-0.0036,-0.0149,0.0097,-0.0022,-0.0381,0.0089,0.0372,0.1094,-0.069,0.0454,0.1035,-0.0142,-0.0062,-0.0097,-0.0301,0.018,-0.0049,-0.1103,-0.0526,0.1117,0.0136,0.0025,-0.0361,-0.0107,0.0059,-0.0284,-0.0015,-0.0066,-0.0024,0.0228,0.0103,-0.0182,-0.0344,-0.0614,0.0047,-0.0133,-0.0413,-0.0291,-0.0187,-0.02]}
{"key":"[Subspace Clustering using Ensembles of $K$-Subspaces] Subspace clustering is the unsupervised grouping of points lying near a union of low-dimensional linear subspaces. Algorithms based directly on geometric properties of such data tend to either provide poor empirical performance, lack theoretical guarantees, or depend heavily on their initialization. We present a novel geometric approach to the subspace clustering problem that leverages ensembles of the K-subspaces (KSS) algorithm via the evidence accumulation clustering framework. Our algorithm, referred to as ensemble K-subspaces (EKSS), forms a co-association matrix whose (i,j)th entry is the number of times points i and j are clustered together by several runs of KSS with random initializations. We prove general recovery guarantees for any algorithm that forms an affinity matrix with entries close to a monotonic transformation of pairwise absolute inner products. We then show that a specific instance of EKSS results in an affinity matrix with entries of this form, and hence our proposed algorithm can provably recover subspaces under similar conditions to state-of-the-art algorithms. The finding is, to the best of our knowledge, the first recovery guarantee for evidence accumulation clustering and for KSS variants. We show on synthetic data that our method performs well in the traditionally challenging settings of subspaces with large intersection, subspaces with small principal angles, and noisy data. Finally, we evaluate our algorithm on six common benchmark datasets and show that unlike existing methods, EKSS achieves excellent empirical performance when there are both a small and large number of points per subspace.","layer":6,"vector":[-0.0503,-0.0113,0.0329,0.012,0.0338,0.0274,0.0639,0.0165,0.0643,0.0118,0.0442,-0.0649,0.0031,0.035,-0.0001,0.0161,0.036,0.0693,-0.0469,-0.0085,-0.0266,-0.0479,-0.0162,-0.0162,0.0251,0.03,-0.0187,-0.0173,-0.0554,-0.2687,-0.0217,-0.0434,0.0805,-0.0093,-0.0062,-0.0317,-0.034,0.0625,-0.0472,0.0349,0.0071,0.0408,-0.0245,-0.0549,-0.0594,-0.0379,-0.005,0.0399,-0.0048,-0.0147,0.0042,-0.0153,-0.0007,0.0579,0.0325,0.0236,0.0212,0.0199,0.0576,0.0386,0.0621,0.0081,-0.1621,0.0434,0.0468,-0.0054,0.0004,-0.0265,0.0226,0.041,-0.0024,0.0768,0.0286,0.0295,0.03,-0.0108,0.0016,-0.01,-0.0131,0.0371,-0.0101,-0.0357,-0.0321,0.0236,-0.0675,-0.0424,0.0157,-0.0488,0.0587,0.0367,-0.0339,0.0397,-0.0062,0.0307,-0.0731,-0.063,0.0232,0.0184,0.0387,0.1949,-0.0698,0.0543,0.038,-0.0281,-0.0079,-0.0987,-0.0378,-0.0389,-0.0063,-0.0638,0.0275,0.0329,-0.0164,-0.0477,0.0183,0.0036,0.1127,0.0655,-0.0434,-0.0292,-0.0074,-0.0181,0.0636,-0.0017,0.0631,-0.0741,0.015,0.1313,0.0668,0.0021,0.0311,0.0357,-0.0307,-0.035,0.0019,0.0199,0.0212,0.0046,-0.0209,-0.0098,-0.0116,-0.0668,0.0527,-0.0433,-0.0359,0.1636,-0.0541,0.001,-0.0304,-0.0121,0.0026,0.037,-0.0754,0.0085,-0.0135,-0.0128,0.0314,0.0057,-0.0214,0.0307,-0.0746,-0.0447,0.0079,0.1399,0.0138,-0.0927,-0.049,0.0084,0.0501,-0.0186,0.0505,0.0547,0.0104,0.075,0.0607,0.0036,-0.0852,0.01,0.0379,-0.0142,0.0396,-0.0314,-0.0705,0.045,0.071,-0.0366,-0.0119,0.0004,-0.0038,0.0298,-0.0644,-0.0015,-0.0369,-0.0313,-0.0472,-0.0311,-0.0119,0.0013,0.0239,-0.036,0.0175,0.0159,-0.0749,0.0429,0.0039,0.0239,0.0015,-0.0155,0.0551,0.0328,0.0046,0.0191,0.0411,-0.0565,0.0075,-0.0189,0.004,0.0313,-0.0235,0.044,0.0459,-0.033,-0.0781,-0.2269,-0.0421,-0.0011,0.0157,0.0097,-0.0408,0.0325,0.0074,0.0669,0.0724,0.0158,0.0186,-0.0301,0.0403,-0.0077,0.0347,0.0164,0.0291,-0.0563,0.0171,-0.0348,0.0175,-0.0561,-0.0279,0.0367,0.0073,0.2123,0.0744,0.0074,-0.0007,0.0301,0.0345,-0.0325,-0.0925,0.0494,0.0417,0.0312,-0.0038,-0.046,-0.0346,-0.0415,0.0058,0.0167,-0.0835,-0.0194,-0.0372,-0.0272,0.0127,-0.0227,-0.0014,0.0454,-0.0436,0.0491,-0.0237,0.0073,-0.0287,-0.0677,0.0312,-0.022,0.01,0.0245,-0.0826,0.0164,-0.0902,0.0806,-0.0061,-0.055,-0.0277,-0.0048,-0.0521,-0.0556,0.0791,-0.0039,0.0319,0.0542,-0.0054,0.0485,-0.0487,-0.0582,-0.0205,0.0435,-0.0572,-0.0034,0.0318,0.0483,0.0533,0.1043,-0.0194,0.0261,-0.0222,-0.0019,0.0273,-0.0841,0.0103,0.0238,0.0205,-0.2688,0.0242,-0.0105,-0.0025,-0.0362,-0.0231,0.009,-0.0151,-0.0409,-0.0352,0.0017,0.0598,0.0344,-0.0173,-0.0117,0.049,0.0595,-0.0353,0.0231,-0.0814,0.0212,0.0091,0.1984,-0.0275,0.0137,0.0068,-0.0114,-0.0103,-0.0218,-0.0194,0.0031,0.001,0.0937,-0.0355,0.0652,0.0724,-0.0077,0.0106,0.0304,-0.0001,-0.007,0.0061,-0.0364,-0.0272,0.096,-0.0022,-0.0087,-0.0834,0.0334,0.0255,-0.0234,-0.0165,-0.0291,-0.0299,0.0378,0.0369,-0.0481,-0.0505,-0.005,-0.03,-0.0401,-0.0464,-0.0424,-0.0088,0.0224]}
{"key":"[Extrapolating Beyond Suboptimal Demonstrations via Inverse Reinforcement Learning from Observations] A critical flaw of existing inverse reinforcement learning (IRL) methods is their inability to significantly outperform the demonstrator. This is because IRL typically seeks a reward function that makes the demonstrator appear near-optimal, rather than inferring the underlying intentions of the demonstrator that may have been poorly executed in practice. In this paper, we introduce a novel reward-learning-from-observation algorithm, Trajectory-ranked Reward EXtrapolation (T-REX), that extrapolates beyond a set of (approximately) ranked demonstrations in order to infer high-quality reward functions from a set of potentially poor demonstrations. When combined with deep reinforcement learning, T-REX outperforms state-of-the-art imitation learning and IRL methods on multiple Atari and MuJoCo benchmark tasks and achieves performance that is often more than twice the performance of the best demonstration. We also demonstrate that T-REX is robust to ranking noise and can accurately extrapolate intention by simply watching a learner noisily improve at a task over time.","layer":3,"vector":[-0.0761,-0.0175,0.0509,-0.047,0.0093,0.0187,0.0345,0.0717,0.0249,0.0326,0.0285,-0.0236,0.0203,0.0448,-0.0071,0.0115,-0.0091,0.0586,-0.0455,-0.0106,0.0287,-0.0752,-0.0055,-0.0538,0.0207,0.0379,-0.0773,-0.0672,-0.024,-0.2378,0.0116,-0.0581,0.0505,-0.0401,-0.0156,0.0016,-0.069,0.0662,-0.0267,0.0187,0.0187,-0.0175,-0.0321,-0.0625,-0.0519,-0.0879,-0.0147,-0.0337,0.0153,-0.0243,0.0242,-0.0051,-0.0178,0.0244,0.0914,0.0118,0.0562,0.056,0.053,0.0354,-0.0047,0.0086,-0.1579,0.0117,0.0242,0.0496,-0.0486,0.0127,0.032,0.0641,-0.022,0.0229,0.0344,0.0723,0.0367,-0.0065,-0.0348,-0.052,-0.0316,-0.0195,0.0422,-0.0699,-0.0268,0.0082,0.0058,-0.0386,0.0312,-0.029,0.0739,0.0365,-0.035,-0.0177,-0.0527,0.0496,-0.0644,0.0289,0.0314,0.0298,-0.0276,0.2115,0.0058,0.0441,0.0134,-0.006,0.0567,-0.0417,-0.0297,-0.0175,-0.0233,0.0112,-0.0662,0.0076,0.0495,-0.0399,0.012,0.0276,0.0518,0.0434,-0.0101,-0.0308,-0.008,0.0139,0.0518,-0.0727,0.0197,-0.0717,0.0118,0.157,0.0123,0.02,0.0303,-0.1076,-0.0222,0.0104,0.0098,-0.0235,0.0174,-0.0139,0.0438,0.0379,0.023,-0.0096,0.0135,-0.0799,-0.0675,0.0766,0.0064,0.0171,-0.0256,0.0042,-0.0293,0.0382,-0.0155,-0.0092,0.0329,0.0163,0.0582,0.0411,-0.0418,0.0409,-0.0405,-0.0769,-0.0418,0.0752,-0.0104,-0.0632,-0.0574,0.0073,0.0075,0.0019,-0.0179,0.0181,-0.0961,0.0245,0.1003,-0.0114,-0.0895,-0.0082,-0.0004,0.0124,0.0576,-0.125,-0.0412,0.026,0.0694,0.006,0.0046,-0.0362,0.041,0.0494,-0.0435,0.0085,-0.0576,0.0093,-0.0344,-0.048,-0.0072,0.0054,-0.0033,-0.0463,-0.006,-0.0455,-0.0427,0.0126,-0.0275,0.0334,0.0003,-0.0208,0.0939,-0.0081,-0.0364,0.0431,0.0488,-0.027,-0.042,-0.0072,0.003,-0.0093,-0.0318,-0.0017,-0.0059,0.0048,-0.0213,-0.2266,0.0028,-0.0149,0.0139,0.0647,-0.0588,0.0546,-0.0221,0.0766,0.0287,0.0579,-0.0505,-0.01,0.0377,-0.0069,0.0673,0.0142,0.0169,0.0189,-0.0165,-0.0306,0.012,0.0075,-0.099,0.0394,-0.0058,0.2258,0.0651,0.0574,-0.0017,-0.01,0.019,-0.0144,-0.0722,0.0545,0.03,0.097,-0.025,-0.0188,-0.0373,-0.0443,0.015,-0.0069,-0.0821,-0.0426,0.0073,-0.0442,0.0823,-0.0423,0.0344,0.0294,-0.0414,0.0145,-0.0353,-0.0183,-0.0129,-0.0715,0.0328,-0.0277,0.011,0.0205,-0.0124,0.0047,-0.0566,0.0184,0.0211,0.0194,-0.0492,0.0711,0.0032,-0.0106,0.0342,0.009,-0.0041,-0.005,0.0141,0.0347,-0.0147,-0.0776,-0.0206,0.052,-0.0165,0.0178,0.0243,-0.0021,0.0071,0.0432,-0.0359,0.0175,0.008,0.002,0.0059,-0.0472,-0.0297,0.0459,0.0288,-0.3071,0.0096,0.0435,0.0264,-0.0312,0.0004,0.0389,0.0109,-0.0074,0.0008,0.0098,0.0447,0.0244,0.0203,0.0287,0.0304,0.0631,-0.0626,0.0619,-0.0771,0.0133,0.0575,0.2157,-0.0306,0.0152,0.0087,-0.0439,-0.0143,0.0408,-0.054,0.0166,-0.026,0.0336,-0.0626,0.0745,0.1145,-0.0144,0.0313,-0.013,0.0194,-0.0469,0.0067,-0.0217,-0.0401,0.101,0.0209,0.0164,-0.0182,-0.0101,0.0225,-0.0154,0.0034,-0.0026,-0.0177,0.049,0.0293,-0.02,-0.0074,-0.0343,-0.0154,0.0322,-0.0419,0.035,-0.0229,0.0031]}
{"key":"[A Gegenbauer Neural Network with Regularized Weights Direct Determination for Classification] Single-hidden layer feed forward neural networks (SLFNs) are widely used in pattern classification problems, but a huge bottleneck encountered is the slow speed and poor performance of the traditional iterative gradient-based learning algorithms. Although the famous extreme learning machine (ELM) has successfully addressed the problems of slow convergence, it still has computational robustness problems brought by input weights and biases randomly assigned. Thus, in order to overcome the aforementioned problems, in this paper, a novel type neural network based on Gegenbauer orthogonal polynomials, termed as GNN, is constructed and investigated. This model could overcome the computational robustness problems of ELM, while still has comparable structural simplicity and approximation capability. Based on this, we propose a regularized weights direct determination (R-WDD) based on equality-constrained optimization to determine the optimal output weights. The R-WDD tends to minimize the empirical risks and structural risks of the network, thus to lower the risk of over fitting and improve the generalization ability. This leads us to a the final GNN with R-WDD, which is a unified learning mechanism for binary and multi-class classification problems. Finally, as is verified in the various comparison experiments, GNN with R-WDD tends to have comparable (or even better) generalization performances, computational scalability and efficiency, and classification robustness, compared to least square support vector machine (LS-SVM), ELM with Gaussian kernel.","layer":2,"vector":[-0.0036,-0.0077,0.0178,-0.0423,0.0876,0.008,0.013,0.0044,-0.0076,-0.0058,-0.0112,0.0008,0.0427,0.0485,0.0131,0.0074,0.0516,0.0432,-0.0152,0.0399,0.0333,-0.0275,0.0006,-0.0713,0.0276,0.0383,-0.043,-0.021,-0.0314,-0.2548,0.0034,-0.0586,0.0781,-0.0329,-0.0138,-0.0372,-0.0278,0.0158,-0.0284,0.0274,-0.0138,0.0243,-0.0225,-0.0325,-0.0359,-0.0034,-0.0367,-0.0148,-0.0166,-0.0389,0.0532,-0.0239,0.0278,0.0169,0.0281,0.0234,0.0399,0.0024,0.0052,0.0569,-0.0178,0.0326,-0.161,0.0072,0.0434,0.0348,-0.0737,-0.0337,0.0414,0.0627,0.0076,0.0263,0.0263,0.0377,0.0116,0.016,0.0436,-0.0627,-0.0221,0.0453,0.003,-0.0011,-0.0283,-0.0433,-0.011,-0.0462,-0.0153,-0.0697,0.0272,0.0044,-0.0541,-0.0242,0.0075,0.0272,-0.0495,-0.0259,0.0464,0.0113,-0.0867,0.2012,-0.077,0.0147,0.0387,-0.0205,0.0565,-0.0099,-0.0261,-0.0442,-0.0511,-0.0343,0.0054,-0.0455,-0.0093,-0.0243,-0.0276,0.0149,0.0143,0.0337,-0.0089,0.0057,-0.0365,0.0037,0.0424,-0.0305,0.0611,-0.0538,-0.0116,0.1326,0.0263,0.0601,0.0085,-0.0338,-0.0801,-0.0357,0.0322,0.032,0.0222,0.0066,-0.0157,-0.0219,-0.034,-0.0374,0.0148,-0.0665,-0.0454,0.0633,-0.0588,0.0245,-0.0162,-0.0364,-0.0048,-0.0104,0.0044,-0.034,0.023,0.0701,0.0588,0.0526,-0.0569,-0.0235,-0.0056,-0.0644,-0.0072,0.0967,0.0394,-0.0792,-0.061,-0.0412,0.0312,-0.0332,0.0412,0.0044,0.0024,0.0138,0.0999,0.0104,-0.0109,-0.0078,-0.0264,0.0336,0.0433,-0.0282,-0.0525,0.0217,0.0288,-0.0606,0.0232,-0.0702,0.0248,0.0289,-0.039,0.019,-0.0285,-0.0254,0.0224,-0.0114,-0.0043,0.0278,0.0432,-0.0509,0.0597,0.0259,-0.009,0.0009,-0.017,0.0442,0.0245,0.0003,0.0128,0.0481,-0.0389,-0.0183,0.0915,-0.0516,-0.0242,0.0187,0.0098,0.0785,-0.0026,0.0103,0.1047,-0.0698,-0.1013,-0.2461,-0.0298,-0.007,0.0064,0.039,-0.0787,0.069,-0.0015,0.0521,0.0654,0.0794,0.0509,-0.0644,0.0149,0.0043,0.0845,0.0461,0.035,-0.005,0.0159,-0.0212,0.0521,0.0125,-0.04,0.053,-0.0082,0.1829,-0.0124,0.0443,-0.0595,0.023,-0.0059,-0.0092,-0.0652,0.065,0.0061,0.063,-0.0201,-0.0366,-0.0356,-0.0103,0.0004,0.0308,-0.0869,-0.0437,0.0105,-0.0173,-0.0008,-0.0604,0.0186,0.076,0.0109,0.0377,0.0067,0.0268,-0.0336,-0.0759,0.0205,-0.0865,0.0117,0.0218,-0.0926,-0.0133,-0.0485,0.0441,0.028,-0.0424,-0.0246,0.0459,-0.0337,-0.0284,0.0903,0.0466,-0.0232,0.075,-0.0281,0.0396,-0.0235,-0.0435,0.01,0.0471,-0.0042,0.0467,-0.027,0.0145,0.013,0.0651,-0.0205,-0.013,0.0193,0.0162,0.0071,-0.0317,-0.0111,0.0338,0.0207,-0.3,0.0432,-0.0024,0.0283,-0.0512,0.0057,0.0082,0.0065,-0.0362,-0.0136,-0.0243,0.0252,0.0455,-0.0394,0.0226,0.0051,0.0312,-0.0559,0.0619,-0.0329,0.0262,0.0704,0.2442,-0.0654,0.0349,0.0221,-0.021,-0.0291,0.0031,-0.0293,0.0624,-0.0063,0.0808,-0.0942,0.0802,0.113,-0.011,0.0479,0.0639,-0.0096,0.0092,0.0039,-0.0464,-0.0108,0.078,-0.0087,-0.0348,-0.0367,-0.0571,0.0321,-0.0269,0.0195,0.0019,-0.0105,-0.0146,0.0,-0.0476,-0.0353,-0.0437,-0.0469,0.0122,-0.0228,-0.0191,-0.0106,0.0005]}
{"key":"[A Gentle Lecture Note on Filtrations in Reinforcement Learning] This note aims to provide a basic intuition on the concept of filtrations as used in the context of reinforcement learning (RL). Filtrations are often used to formally define RL problems, yet their implications might not be eminent for those without a background in measure theory. Essentially, a filtration is a construct that captures partial knowledge up to time $t$, without revealing any future information that has already been simulated, yet not revealed to the decision-maker. We illustrate this with simple examples from the finance domain on both discrete and continuous outcome spaces. Furthermore, we show that the notion of filtration is not needed, as basing decisions solely on the current problem state (which is possible due to the Markovian property) suffices to eliminate future knowledge from the decision-making process.","layer":0,"vector":[-0.0557,-0.0127,0.0624,-0.0283,-0.027,0.0195,0.0823,0.0495,0.0659,-0.004,0.0258,-0.0438,0.0397,0.0647,-0.0095,0.0165,-0.0437,0.0459,-0.044,0.0039,0.0745,-0.0625,-0.0296,-0.0694,0.0192,0.0454,0.0029,-0.0789,-0.0115,-0.2268,0.0308,-0.0353,0.0129,-0.0575,-0.0058,0.0042,-0.0272,0.0615,-0.0254,0.0612,0.0496,0.0056,-0.0113,-0.0597,-0.0573,-0.0697,0.0021,-0.0368,-0.0447,-0.0331,0.008,-0.0299,0.0308,0.0349,0.0517,0.0351,0.0429,0.0701,0.0294,0.012,0.0033,0.0002,-0.1816,0.0633,0.0366,0.0429,-0.026,-0.0189,0.0563,0.0919,-0.0154,0.0632,0.0151,0.039,0.0358,-0.0293,0.0161,-0.0537,-0.0126,0.037,0.031,-0.0424,-0.0397,-0.006,-0.0578,-0.0407,0.0107,-0.0651,0.0486,0.045,-0.0154,0.0113,0.0153,0.0342,-0.0392,0.0385,0.0395,0.0363,-0.0178,0.1789,0.0114,0.0498,0.0209,-0.0184,0.0587,-0.0529,-0.0306,-0.0261,-0.019,0.0053,-0.0418,-0.0153,0.0639,-0.0248,-0.0209,0.0361,0.0405,0.0361,0.0094,0.0048,-0.0427,0.0335,0.0794,-0.0168,0.0086,-0.0559,0.0115,0.1418,-0.0055,0.027,0.0263,-0.0577,-0.0216,0.0007,0.0108,0.0503,0.0214,-0.0121,-0.0042,-0.033,-0.0456,-0.0005,-0.0124,-0.103,-0.0733,0.0978,0.0178,0.0317,-0.0256,-0.019,-0.0164,0.0174,-0.0041,-0.0802,0.0552,0.0286,-0.0016,0.0278,-0.0473,0.0194,-0.0313,-0.0633,-0.0456,0.1483,-0.0161,-0.0323,-0.0346,-0.0349,0.0208,-0.0017,0.012,0.0317,-0.0498,0.0008,0.0734,0.0152,-0.0965,-0.008,0.0078,-0.0238,0.0327,-0.0434,-0.0108,0.0058,0.0303,-0.0402,0.0248,-0.0312,0.0044,0.0564,-0.0219,-0.0015,0.0115,-0.0131,-0.0533,-0.0633,0.0382,0.0016,-0.0476,-0.0441,-0.0594,0.0019,-0.05,0.0463,-0.0296,0.0006,-0.0022,-0.0081,0.0701,-0.0095,-0.0383,0.0614,0.0086,-0.023,-0.0111,0.022,0.0073,0.0017,-0.0205,0.0009,0.0083,0.0007,-0.0115,-0.2395,0.0127,-0.0444,-0.0055,0.0681,-0.0463,0.0212,-0.037,0.0049,0.0687,0.0501,-0.0499,-0.0152,0.0134,0.0076,0.0188,0.0359,0.0232,-0.0675,0.0021,-0.0406,0.008,-0.0209,-0.0832,0.0494,-0.0025,0.2388,0.028,0.0524,-0.0148,-0.0091,0.0301,-0.0478,-0.0899,0.0514,0.0037,0.017,0.0152,0.0268,-0.0684,0.009,0.0173,-0.051,-0.0548,-0.0285,-0.0292,-0.038,0.049,-0.0581,-0.0179,0.0223,-0.0338,0.0153,-0.0117,0.0163,-0.045,-0.0865,0.0151,-0.0422,0.0404,0.0246,-0.0293,0.0293,-0.0291,0.0666,0.0037,0.0193,-0.0385,-0.0075,-0.0115,-0.0337,0.0369,-0.0081,-0.0394,0.0034,0.0264,0.029,-0.0235,-0.0361,-0.0181,0.0727,-0.0507,0.023,0.0693,0.0094,-0.0082,0.0516,-0.0103,0.0266,-0.0532,0.0113,0.0277,-0.0631,-0.0234,0.0486,-0.0205,-0.3051,0.0422,0.0571,0.0184,-0.0447,0.0121,0.0474,0.0393,-0.0596,0.0187,0.0382,0.0656,0.0386,0.002,0.0099,0.0057,0.0798,-0.0572,0.0658,-0.0426,0.0392,0.0514,0.2522,-0.0525,0.0534,-0.0029,-0.0371,-0.0428,0.0199,-0.0078,0.0469,0.0187,0.066,-0.0458,0.0456,0.0187,-0.0308,0.0395,0.0274,-0.0082,-0.0156,0.0135,-0.0139,-0.0014,0.0925,0.0393,-0.0067,-0.0614,-0.0208,0.0412,-0.0125,0.0103,-0.0126,-0.0453,0.0433,-0.0024,-0.0406,-0.0711,-0.0005,-0.0315,-0.0127,-0.0369,0.021,-0.0048,0.0233]}
{"key":"[Constraints on parameter choices for successful reservoir computing] Echo-state networks are simple models of discrete dynamical systems driven by a time series. By selecting network parameters such that the dynamics of the network is contractive, characterized by a negative maximal Lyapunov exponent, the network may synchronize with the driving signal. Exploiting this synchronization, the echo-state network may be trained to autonomously reproduce the input dynamics, enabling time-series prediction. However, while synchronization is a necessary condition for prediction, it is not sufficient. Here, we study what other conditions are necessary for successful time-series prediction. We identify two key parameters for prediction performance, and conduct a parameter sweep to find regions where prediction is successful. These regions differ significantly depending on whether full or partial phase space information about the input is provided to the network during training. We explain how these regions emerge.","layer":4,"vector":[-0.061,-0.0239,0.028,0.0168,0.0068,0.0248,0.0266,0.0423,0.046,-0.0019,-0.0122,-0.0215,0.0255,0.035,0.0023,-0.006,-0.0401,0.0382,-0.024,0.0223,0.0539,0.0125,-0.0591,-0.049,0.0058,0.0054,-0.0114,-0.0332,-0.0587,-0.2386,0.0096,-0.0545,0.0383,-0.0171,0.0192,-0.0312,0.0084,0.0785,-0.0128,0.045,0.0235,0.0403,0.0218,-0.0711,-0.0346,-0.064,0.0105,-0.0396,-0.0303,-0.0169,0.0003,0.0085,0.0092,0.0145,0.0131,0.0169,0.0629,0.0599,0.0324,0.0089,0.0338,0.0037,-0.1764,0.067,0.0358,0.0398,-0.021,-0.0254,0.0414,0.054,0.027,0.0245,0.0245,0.027,0.0189,0.0047,-0.0185,-0.0055,-0.0134,0.0022,0.0195,-0.0517,-0.0203,-0.0606,-0.0244,0.0095,0.0422,-0.0347,-0.0163,0.0046,-0.0577,-0.0068,-0.032,0.0076,-0.0686,0.0236,0.0274,0.0508,-0.035,0.1953,-0.0565,0.0292,0.039,-0.0426,0.0268,-0.0594,-0.0122,-0.0177,-0.0597,0.0023,-0.0164,-0.0361,0.0544,-0.0465,0.0302,0.016,0.0288,0.043,-0.0069,0.0041,-0.0302,0.0152,0.0005,-0.0373,0.019,-0.0467,0.0362,0.121,-0.0022,-0.0026,-0.0104,-0.0216,-0.0477,-0.0336,0.03,0.0301,0.0067,-0.0153,0.0293,0.0148,-0.0231,-0.0357,0.028,-0.0893,-0.0832,0.1236,-0.0452,0.0192,-0.0594,0.0053,-0.0457,-0.0069,0.0071,-0.0695,0.025,0.0329,0.0075,-0.0174,-0.076,0.0156,-0.0824,-0.0578,-0.0341,0.0849,0.0398,-0.0635,-0.0345,0.0046,0.0068,-0.0113,0.0293,-0.0288,-0.0198,-0.0111,0.0774,0.0125,-0.0517,-0.0016,-0.0051,0.0105,0.0232,-0.0371,-0.0239,0.0596,0.0425,-0.0409,-0.0143,-0.0414,-0.0074,0.0362,-0.0031,-0.0188,-0.0049,0.0606,-0.0692,-0.0444,-0.0011,-0.015,0.0291,-0.0791,-0.0184,0.0043,-0.0232,0.0296,0.0256,0.0096,-0.0198,0.0508,-0.0101,0.0268,-0.0167,0.0028,0.0502,-0.0525,-0.0666,0.0141,0.0023,0.0867,-0.0217,0.0657,0.0854,-0.0162,-0.0739,-0.2352,0.0164,0.0201,-0.0307,0.1138,-0.0801,0.0465,-0.0233,0.0665,0.0367,0.0082,-0.0072,0.007,0.0303,0.0324,0.0609,0.0351,0.0213,-0.0053,0.0125,-0.0152,0.0111,-0.0558,-0.0974,0.0896,0.0088,0.1698,0.0077,0.0619,-0.037,0.022,0.0136,-0.0632,-0.0704,0.0625,0.0409,0.0869,-0.0345,-0.0318,-0.0581,-0.0634,0.0497,-0.0061,-0.0572,-0.0406,-0.0315,-0.0508,0.03,-0.0816,-0.0153,0.081,-0.0577,0.0603,-0.0125,0.023,-0.046,-0.0606,0.0094,-0.0382,0.0285,0.0018,-0.0377,-0.0146,-0.0439,0.0439,0.0049,-0.0523,-0.0337,0.0406,-0.0046,-0.0051,0.0866,0.0106,0.0356,0.0811,-0.0278,-0.0091,-0.0405,-0.0134,-0.0236,0.0725,-0.079,0.0783,0.0873,0.0411,-0.0011,0.111,-0.0029,-0.0183,-0.0072,0.0033,0.0309,-0.0453,-0.0399,0.0529,-0.0051,-0.3055,0.0104,0.0066,0.0471,-0.0154,-0.0079,0.0259,0.0321,-0.0387,0.0264,0.0061,0.0481,0.0424,0.0394,-0.0034,0.0383,0.084,-0.0632,0.0144,-0.0792,0.025,0.0647,0.2115,-0.0121,0.0623,0.0028,-0.0372,0.0022,0.0369,-0.0071,-0.0011,0.0257,0.0768,-0.0429,0.0363,0.0568,-0.0267,0.0249,0.0387,-0.0193,0.0103,-0.0024,-0.0085,-0.0313,0.0721,-0.0144,-0.0265,-0.0523,-0.0013,0.0665,-0.0482,0.0318,0.0126,-0.013,0.0057,0.0844,-0.0223,-0.0561,-0.0108,-0.0266,0.0595,-0.0883,0.042,-0.0325,-0.0548]}
{"key":"[On the model-based stochastic value gradient for continuous reinforcement learning] For over a decade, model-based reinforcement learning has been seen as a way to leverage control-based domain knowledge to improve the sample-efficiency of reinforcement learning agents. While model-based agents are conceptually appealing, their policies tend to lag behind those of model-free agents in terms of final reward, especially in non-trivial environments. In response, researchers have proposed model-based agents with increasingly complex components, from ensembles of probabilistic dynamics models, to heuristics for mitigating model error. In a reversal of this trend, we show that simple model-based agents can be derived from existing ideas that not only match, but outperform state-of-the-art model-free agents in terms of both sample-efficiency and final reward. We find that a model-free soft value estimate for policy evaluation and a model-based stochastic value gradient for policy improvement is an effective combination, achieving state-of-the-art results on a high-dimensional humanoid control task, which most model-based agents are unable to solve. Our findings suggest that model-based policy evaluation deserves closer attention.","layer":0,"vector":[-0.0779,0.0042,0.0518,-0.0391,-0.0258,0.0457,0.0096,0.047,0.059,0.0022,0.0603,-0.0402,0.0378,0.0917,0.0265,-0.0186,-0.0333,0.0392,-0.0021,0.0093,0.038,-0.0336,-0.0205,-0.0517,-0.0139,0.0199,-0.0544,-0.0676,-0.033,-0.2617,0.0035,-0.0362,0.0154,-0.0553,-0.0232,-0.0043,-0.0482,0.0092,0.0005,0.0477,0.0116,0.0496,-0.0232,-0.0774,0.0148,-0.0278,-0.0114,-0.0223,-0.0437,-0.0111,0.0455,-0.0218,0.0013,0.0019,0.044,0.0226,0.0755,0.0395,0.0362,-0.0028,0.021,0.0432,-0.1629,0.0744,0.0431,0.0774,-0.0353,-0.034,0.0402,0.03,-0.0171,0.0626,0.0339,0.0413,0.0276,-0.0476,0.0061,-0.069,-0.0011,0.0146,0.0058,-0.065,-0.0583,0.0017,-0.0192,-0.076,0.0402,-0.0332,0.0624,0.0143,-0.0191,0.0148,-0.0407,0.0358,-0.034,0.0132,0.0104,0.0057,-0.0637,0.2028,-0.0081,0.0353,0.0281,0.0032,0.0367,-0.0672,-0.0531,-0.0247,-0.0395,0.0091,-0.0153,0.0186,0.0239,-0.0425,-0.0054,0.0185,0.0751,0.0255,0.012,-0.0555,-0.0291,0.0455,0.062,0.005,-0.0053,-0.0851,0.0025,0.1738,0.0367,0.0156,0.0411,-0.0628,-0.0179,-0.0637,-0.0176,-0.0016,0.0268,0.0347,0.0264,0.0321,-0.0313,0.0053,-0.0125,-0.1374,-0.0776,0.0864,-0.0006,0.0207,-0.0134,0.0271,0.0252,0.041,0.0099,-0.0386,0.0331,0.0196,0.0116,0.0507,-0.0587,-0.002,-0.0273,-0.0383,-0.0527,0.0785,-0.0301,-0.0429,-0.042,-0.0288,-0.0125,0.0022,0.0348,0.03,-0.0594,0.033,0.1053,0.0199,-0.0841,0.0068,-0.0169,-0.0081,0.0394,-0.0512,-0.0401,-0.0043,0.0451,-0.0328,-0.0092,-0.0488,0.0112,0.048,-0.0161,-0.0249,-0.0015,-0.0163,-0.0322,-0.04,-0.0157,-0.0373,0.0103,-0.0173,-0.0161,0.0142,-0.0471,-0.0389,-0.0124,0.018,-0.0364,-0.002,0.0479,0.0195,-0.0372,0.0359,0.0461,-0.0483,-0.012,0.0031,0.0227,0.0039,-0.0043,0.0055,0.0362,0.0296,-0.0312,-0.1983,0.0139,-0.005,-0.0112,0.0387,-0.0697,0.0039,-0.0262,0.0281,0.042,0.0606,-0.0358,-0.0146,0.0247,-0.0153,0.0397,0.056,0.0162,-0.0404,-0.005,0.032,-0.0242,-0.0375,-0.0846,0.0661,-0.0234,0.2164,0.0226,0.0387,-0.0177,-0.0199,0.0585,-0.0424,-0.0989,0.0603,0.0042,0.0834,-0.0329,0.0134,-0.0362,-0.002,0.0046,-0.0014,-0.1352,-0.0353,-0.0287,0.0005,0.047,-0.0228,-0.0134,0.0213,-0.0186,0.0154,0.0013,-0.0286,-0.051,-0.0645,0.0287,-0.0313,0.0355,0.0031,-0.0501,0.0288,-0.0334,0.0543,-0.0306,0.0348,-0.0741,0.0286,-0.0222,-0.0112,0.0961,0.0138,0.0013,0.0401,0.0046,0.0229,-0.0079,-0.0502,-0.0115,0.0491,-0.0234,0.0163,0.0747,0.0092,0.0011,0.0867,-0.0326,0.0239,0.0084,0.0159,-0.0013,-0.0604,0.0285,0.0397,-0.0303,-0.2845,0.066,0.0286,0.037,-0.0289,-0.0036,0.03,-0.0306,-0.0699,-0.0042,0.0345,0.1055,0.0399,0.05,0.0321,0.0343,0.0582,-0.034,0.0888,-0.0846,0.0302,0.0387,0.2074,-0.0311,0.0561,0.0148,-0.0275,-0.0026,0.0331,-0.0202,0.0099,0.0289,0.0789,-0.0236,0.0655,0.0902,-0.0323,0.0251,-0.0016,-0.0168,-0.004,0.0422,0.0296,-0.0131,0.1023,0.0045,-0.0282,-0.0209,-0.0409,0.0457,-0.0294,-0.0085,-0.0551,-0.0216,0.0374,0.0394,-0.0309,-0.0591,-0.0369,-0.0536,-0.0236,-0.0679,0.0316,0.0127,-0.0447]}
{"key":"[TrustNet: Learning from Trusted Data Against (A)symmetric Label Noise] Robustness to label noise is a critical property for weakly-supervised classifiers trained on massive datasets. Robustness to label noise is a critical property for weakly-supervised classifiers trained on massive datasets. In this paper, we first derive analytical bound for any given noise patterns. Based on the insights, we design TrustNet that first adversely learns the pattern of noise corruption, being it both symmetric or asymmetric, from a small set of trusted data. Then, TrustNet is trained via a robust loss function, which weights the given labels against the inferred labels from the learned noise pattern. The weight is adjusted based on model uncertainty across training epochs. We evaluate TrustNet on synthetic label noise for CIFAR-10 and CIFAR-100, and real-world data with label noise, i.e., Clothing1M. We compare against state-of-the-art methods demonstrating the strong robustness of TrustNet under a diverse set of noise patterns.","layer":5,"vector":[-0.0145,-0.0465,-0.0296,-0.0159,0.0147,0.0478,0.0413,-0.0045,0.0002,-0.0459,-0.0147,-0.0365,0.0417,0.0745,0.026,0.0234,0.0414,0.0483,-0.052,0.0225,0.0317,-0.0503,0.0223,-0.0403,0.0019,0.0481,-0.0289,-0.0726,-0.0818,-0.271,0.0085,-0.0766,0.0282,-0.0114,0.0777,-0.0305,-0.0156,0.0222,-0.0313,0.0367,0.0451,0.0412,0.0005,-0.0876,0.0123,-0.0457,0.0069,-0.0142,-0.0428,-0.0269,0.0156,-0.0263,0.0003,0.0644,0.0374,0.0283,0.0715,0.0234,0.0531,0.0522,0.0116,0.0496,-0.1465,0.0448,0.0393,0.0312,-0.0787,-0.0212,-0.0122,0.0374,0.0382,0.0337,0.0428,0.067,0.0248,0.0176,0.0283,-0.0125,-0.0232,0.0121,0.0072,-0.0203,0.0016,-0.0304,-0.0325,-0.048,0.0414,-0.0403,-0.0009,-0.0041,-0.0048,-0.0155,-0.0196,0.0401,-0.0479,-0.0288,0.0187,0.0632,-0.044,0.19,-0.0696,0.0178,-0.0305,-0.0047,0.0533,-0.0256,-0.0286,-0.0329,-0.0136,-0.028,-0.0498,0.0098,0.0249,-0.0409,0.0084,0.0234,0.0901,0.013,-0.0326,-0.0034,-0.0535,0.0131,0.0933,0.0133,0.0367,-0.0544,0.0361,0.1438,0.0536,0.0437,-0.0009,-0.0282,-0.0376,0.0007,0.0419,0.0122,-0.0389,0.0267,0.0747,-0.0294,-0.068,-0.0313,0.0241,-0.0836,-0.0892,0.0924,-0.0153,0.0467,-0.0084,-0.0231,-0.0029,0.0208,0.0095,-0.0457,0.0487,0.0222,0.0309,0.0296,-0.0415,0.0084,0.0223,-0.0645,0.0025,0.0852,-0.01,-0.0697,-0.0264,-0.0131,0.0299,-0.0227,0.0031,0.0256,0.0038,0.0251,0.0101,0.0345,-0.067,-0.0084,0.0071,-0.0129,-0.0331,-0.0676,-0.0445,0.0632,0.0396,-0.0189,-0.0248,-0.0562,0.0215,0.0206,-0.0191,-0.0113,-0.044,-0.0081,-0.0442,-0.0496,-0.024,0.0107,-0.0199,-0.0319,-0.0198,0.0055,-0.0046,0.0405,-0.0075,0.006,0.0023,-0.0143,0.0083,0.0202,-0.0063,0.0273,0.0301,-0.0447,-0.0394,-0.0026,0.0511,0.0731,0.0017,0.0255,0.0247,-0.0394,-0.0483,-0.2797,-0.0053,-0.0072,0.0117,0.0844,-0.0513,0.0302,-0.0406,0.0163,0.0715,0.0635,0.0284,-0.0766,0.02,-0.009,0.0903,0.0215,0.0101,-0.005,0.007,-0.04,0.0516,-0.0264,-0.0818,0.0754,0.0055,0.2338,-0.0145,0.0211,-0.0251,0.0297,0.0087,-0.0492,-0.0753,0.0693,0.0382,0.0558,0.0165,-0.0435,-0.027,-0.0213,-0.0118,0.0307,-0.1268,-0.0197,-0.0495,-0.0134,0.004,-0.0712,0.0443,0.0355,0.0364,0.0716,0.0014,-0.0218,-0.0219,-0.0765,0.038,-0.0437,0.0327,0.0025,-0.0463,0.0071,-0.0863,0.033,0.0043,-0.0427,-0.0285,0.0446,-0.0479,-0.0276,0.083,-0.0184,-0.0502,0.0851,-0.0244,0.0051,-0.0291,-0.0733,-0.0209,0.0838,-0.0095,0.0462,0.0053,0.0587,0.0292,0.0672,0.024,0.0683,-0.0092,0.0528,0.0235,-0.0752,-0.0377,0.0687,-0.0187,-0.2726,-0.0066,-0.0283,0.055,-0.0762,0.0015,0.0643,0.008,-0.0474,-0.0127,0.002,0.0581,0.0237,-0.0267,-0.0013,0.0462,0.0472,-0.0505,0.0603,-0.0075,0.019,0.0263,0.1784,-0.0403,0.0294,0.0207,-0.0293,0.0379,0.0101,-0.0118,0.0207,-0.0193,0.0386,-0.038,0.0304,0.0625,-0.0492,-0.0086,0.0184,-0.0033,-0.0175,-0.0196,-0.0343,0.002,0.0828,-0.0335,-0.012,-0.0163,0.0079,0.0238,-0.041,0.0292,0.0023,-0.0038,-0.0078,0.0691,-0.0262,-0.0215,-0.0432,-0.0443,0.0221,-0.0314,-0.0063,0.0114,-0.0575]}
{"key":"[On-the-Fly Adaptation of Source Code Models using Meta-Learning] The ability to adapt to unseen, local contexts is an important challenge that successful models of source code must overcome. One of the most popular approaches for the adaptation of such models is dynamic evaluation. With dynamic evaluation, when running a model on an unseen file, the model is updated immediately after having observed each token in that file. In this work, we propose instead to frame the problem of context adaptation as a meta-learning problem. We aim to train a base source code model that is best able to learn from information in a file to deliver improved predictions of missing tokens. Unlike dynamic evaluation, this formulation allows us to select more targeted information (support tokens) for adaptation, that is both before and after a target hole in a file. We consider an evaluation setting that we call line-level maintenance, designed to reflect the downstream task of code auto-completion in an IDE. Leveraging recent developments in meta-learning such as first-order MAML and Reptile, we demonstrate improved performance in experiments on a large scale Java GitHub corpus, compared to other adaptation baselines including dynamic evaluation. Moreover, our analysis shows that, compared to a non-adaptive baseline, our approach improves performance on identifiers and literals by 44\\% and 15\\%, respectively.","layer":3,"vector":[-0.096,-0.0335,0.0277,-0.0304,0.0518,0.0021,-0.0,0.0118,-0.023,-0.0156,-0.023,-0.0224,0.0273,0.0037,0.0307,-0.0254,0.007,0.0537,-0.0695,-0.0326,0.02,-0.0177,0.0359,-0.0152,0.0447,0.0411,-0.0311,-0.0143,-0.0513,-0.236,-0.0069,-0.0196,0.0431,0.0324,0.0083,0.0148,-0.0359,0.0359,0.0077,0.0568,0.0028,0.0391,-0.0124,-0.0724,-0.0794,-0.0355,-0.017,-0.003,-0.0256,-0.0163,-0.0266,-0.0326,0.0396,0.0167,-0.0097,0.0177,0.0584,0.0653,0.0414,0.0392,-0.0135,0.0515,-0.1433,0.0541,0.0318,0.0603,-0.0337,-0.0072,0.0155,0.0408,-0.0498,0.0375,-0.0091,0.058,0.0151,0.0126,0.021,0.0176,0.0212,0.0142,-0.0057,-0.0614,-0.0239,0.0182,-0.0046,-0.0372,0.0291,0.0128,0.0699,-0.0033,-0.0388,-0.0347,0.0081,0.0113,-0.0559,0.0127,0.0183,0.0299,-0.0376,0.212,-0.0411,0.0052,0.0138,-0.0278,0.074,-0.0239,0.0101,0.0012,0.036,-0.0304,-0.0118,0.0044,0.0169,-0.0391,0.0457,0.0051,0.0811,0.0134,-0.0299,0.0369,-0.0091,0.0083,0.0292,-0.0645,0.006,-0.0479,0.0023,0.1305,0.0294,0.0004,0.0479,0.0016,-0.0495,-0.0183,0.0169,0.0064,0.0175,-0.0056,0.0171,-0.0056,-0.0334,-0.0633,-0.0135,-0.0483,-0.0636,0.1177,-0.0366,0.0398,-0.0492,-0.0573,-0.0025,0.0479,-0.0618,0.0039,0.0538,0.037,0.0292,0.0494,-0.0332,0.0081,0.0114,-0.0106,-0.0798,0.0584,-0.0413,-0.0511,-0.0704,-0.0167,0.0385,0.0012,0.0209,0.0336,-0.0077,0.0065,0.0507,0.0191,-0.0434,-0.0146,0.0474,0.0451,0.0551,-0.0814,-0.0313,0.0747,0.0193,-0.0503,-0.0178,-0.035,0.0114,0.0536,-0.0274,0.0469,0.0145,0.0042,-0.0015,-0.0383,-0.0284,-0.0178,0.0061,-0.0823,-0.0195,0.0639,-0.0414,-0.0121,-0.0339,0.0141,-0.0286,-0.0054,0.044,-0.0021,-0.0522,-0.031,0.0674,-0.0284,-0.022,-0.0469,0.0256,-0.0126,0.0438,0.0781,-0.0176,-0.0585,0.0074,-0.2363,-0.011,0.0455,-0.0373,0.0276,-0.0793,0.0082,-0.0479,0.0307,0.037,0.0527,-0.0671,0.0059,0.0326,-0.0293,0.0548,0.0409,0.0076,-0.0191,-0.0207,0.0248,0.0235,0.0213,-0.1138,0.0154,0.001,0.2085,0.0269,0.0477,-0.0428,0.0349,0.0073,-0.0178,-0.1778,0.067,0.0353,0.0585,-0.0422,-0.0172,-0.0011,0.0263,-0.0187,-0.0198,-0.1253,-0.0681,-0.0689,-0.0065,-0.0447,-0.0217,0.0203,0.0136,-0.0171,0.0271,0.0029,-0.0721,-0.0157,-0.0456,0.0311,0.0079,0.0333,0.0263,-0.0143,0.0279,-0.0719,0.0467,0.0139,-0.0002,-0.0308,0.0388,-0.0476,-0.013,0.0703,0.0007,-0.0417,0.0097,-0.0041,0.0052,-0.0386,-0.0587,-0.04,0.065,0.002,0.0662,0.036,0.0556,0.0507,0.0638,-0.0068,0.0344,-0.0143,0.0024,0.0059,-0.0468,-0.0071,0.0626,-0.01,-0.3188,0.0537,0.0193,0.0019,-0.0402,0.0645,0.0606,-0.0136,-0.0575,0.0025,0.008,0.0377,0.0121,-0.0541,0.0514,0.053,0.0929,-0.0373,0.039,-0.0888,0.0103,0.0604,0.2284,-0.0187,0.0528,0.0115,-0.02,0.0325,0.0714,0.0134,0.0228,0.0048,0.099,-0.0199,0.0375,0.0366,-0.0185,0.0261,0.0029,-0.0179,-0.0082,0.009,-0.0443,-0.0251,0.0526,-0.0237,0.0315,-0.0703,-0.0228,0.0116,-0.029,-0.0083,-0.0155,-0.0071,0.0257,0.0006,-0.0259,-0.036,-0.0649,-0.0372,0.0425,-0.0681,0.0564,0.0279,-0.0268]}
{"key":"[FastSTMF: Efficient tropical matrix factorization algorithm for sparse data] Matrix factorization, one of the most popular methods in machine learning, has recently benefited from introducing non-linearity in prediction tasks using tropical semiring. The non-linearity enables a better fit to extreme values and distributions, thus discovering high-variance patterns that differ from those found by standard linear algebra. However, the optimization process of various tropical matrix factorization methods is slow. In our work, we propose a new method FastSTMF based on Sparse Tropical Matrix Factorization (STMF), which introduces a novel strategy for updating factor matrices that results in efficient computational performance. We evaluated the efficiency of FastSTMF on synthetic and real gene expression data from the TCGA database, and the results show that FastSTMF outperforms STMF in both accuracy and running time. Compared to NMF, we show that FastSTMF performs better on some datasets and is not prone to overfitting as NMF. This work sets the basis for developing other matrix factorization techniques based on many other semirings using a new proposed optimization process.","layer":8,"vector":[-0.0576,0.0139,0.0297,0.0206,0.0749,0.0506,0.0124,0.0176,0.0012,-0.0026,0.001,-0.0648,0.0451,0.0547,0.0081,0.012,-0.0245,0.0595,-0.0403,-0.0157,0.0325,-0.0262,-0.0255,-0.0644,0.0485,0.0347,-0.0333,-0.0587,-0.0361,-0.2454,0.0017,-0.0073,0.0467,-0.0328,0.0254,-0.0277,-0.0215,0.0195,-0.061,0.0232,-0.0328,0.0367,0.0052,-0.0283,-0.0251,-0.0692,-0.0571,0.0062,-0.0185,-0.0373,0.0228,-0.0494,0.009,0.0503,0.0156,0.0019,0.0288,0.026,0.003,0.0179,0.0462,0.0728,-0.177,0.0733,0.0515,0.0289,-0.0212,-0.022,0.0405,0.0579,-0.0511,0.0472,0.047,0.0559,0.0248,0.0243,0.0144,-0.0205,-0.007,0.0602,0.0502,-0.0183,-0.0426,0.003,-0.0102,-0.0285,0.0345,-0.0497,-0.0015,0.0042,-0.0223,-0.0133,-0.0002,0.0561,-0.0919,-0.026,0.0112,0.0129,-0.0594,0.2213,-0.0717,0.0382,0.0068,0.0072,-0.0033,-0.027,-0.0373,-0.0442,0.0052,-0.0049,0.0294,-0.0086,-0.014,-0.0254,0.01,-0.0351,0.0653,0.0532,-0.0414,0.0286,-0.0205,0.0227,0.0276,-0.0235,0.0092,-0.0632,0.015,0.1456,0.0666,0.0349,0.072,-0.0078,-0.0726,0.0008,0.042,-0.019,0.0257,0.0136,-0.0142,-0.0126,-0.036,-0.0578,-0.0115,-0.0722,-0.045,0.1048,-0.0302,-0.0125,-0.0834,-0.0338,-0.0197,0.0187,0.0006,-0.0113,0.0258,-0.0039,0.0296,0.0409,-0.0123,0.0068,-0.0401,-0.0242,-0.0262,0.0713,0.018,-0.0951,-0.0136,0.0216,0.0273,0.0045,0.0265,0.0443,-0.0586,0.0163,0.0388,0.0431,-0.0265,-0.0062,0.0058,0.0221,0.0587,-0.0332,-0.0329,0.0601,0.0286,-0.0617,0.0333,-0.0293,0.0051,-0.0394,-0.019,0.0089,-0.0196,0.0156,-0.0103,-0.0721,0.0163,-0.0206,0.0382,-0.0474,0.0527,0.0156,-0.0362,0.0392,-0.0088,0.0187,0.0241,-0.0351,0.005,0.0849,0.0018,-0.0133,0.0634,-0.0379,-0.0453,0.0246,0.0358,0.0445,0.0238,0.0626,0.0179,-0.0669,-0.076,-0.2272,-0.0238,0.0082,-0.0136,0.0394,-0.0522,0.0021,-0.028,0.0966,0.0771,0.0308,0.0008,-0.0008,0.0583,-0.0273,0.0644,0.0349,0.0344,0.0036,-0.0302,-0.0154,0.0182,0.0109,-0.0359,0.0555,0.0218,0.198,0.0364,-0.0081,-0.0317,0.0456,0.0182,0.0033,-0.1076,0.0748,-0.032,0.048,-0.0282,-0.026,-0.0124,0.0219,-0.0165,0.0136,-0.0938,-0.0377,-0.0422,-0.0366,0.0039,-0.0384,-0.0004,0.0374,-0.0123,0.0479,-0.034,0.0215,-0.0612,-0.098,0.0204,-0.0664,0.0152,0.0207,-0.0875,0.0299,-0.0692,-0.013,-0.027,-0.0101,-0.005,0.0397,-0.0309,-0.0581,0.0958,0.0055,0.005,0.0878,-0.0366,0.0117,-0.0472,-0.0599,-0.0129,0.1016,-0.0415,0.0462,0.0001,0.043,0.0373,0.1043,0.0007,0.037,-0.0316,0.0112,-0.018,-0.024,-0.0124,0.0357,0.0072,-0.3023,0.0685,0.0075,-0.0285,-0.0148,0.0015,0.099,-0.0114,-0.0344,0.0074,0.0009,0.0309,0.0382,-0.014,0.0256,0.0139,0.0674,-0.0164,0.0318,-0.0768,-0.0197,-0.0098,0.239,0.0028,0.0129,0.0501,-0.0321,-0.0024,0.0519,-0.0496,0.0259,0.0317,0.0855,-0.045,0.0204,0.0176,-0.0231,0.0279,0.0232,-0.0316,0.0041,0.0376,-0.0817,-0.04,0.0661,-0.0436,-0.0045,-0.052,-0.0041,0.0324,-0.0578,-0.0072,0.002,-0.0195,-0.0127,-0.0129,-0.0376,-0.0246,-0.0053,-0.0466,0.004,-0.0501,-0.0452,0.0071,-0.0111]}
{"key":"[Contextual Search in the Presence of Adversarial Corruptions] We study contextual search, a generalization of binary search in higher dimensions, which captures settings such as feature-based dynamic pricing. Standard formulations of this problem assume that agents act in accordance with a specific homogeneous response model. In practice, however, some responses may be adversarially corrupted. Existing algorithms heavily depend on the assumed response model being (approximately) accurate for all agents and have poor performance in the presence of even a few such arbitrary misspecifications. We initiate the study of contextual search when some of the agents can behave in ways inconsistent with the underlying response model. In particular, we provide two algorithms, one based on multidimensional binary search methods and one based on gradient descent. We show that these algorithms attain near-optimal regret in the absence of adversarial corruptions and their performance degrades gracefully with the number of such agents, providing the first results for contextual search in any adversarial noise model. Our techniques draw inspiration from learning theory, game theory, high-dimensional geometry, and convex analysis.","layer":0,"vector":[-0.0564,-0.0168,-0.0005,-0.0121,0.0058,-0.011,0.0584,0.0112,0.0212,0.0001,-0.0171,-0.0122,0.0013,0.0583,0.0354,-0.0072,-0.0058,0.042,-0.0203,0.0347,0.0425,-0.0578,0.0263,-0.05,0.0328,-0.0012,-0.0463,-0.0489,-0.052,-0.2324,0.0144,0.001,0.0288,-0.0431,0.0111,-0.0053,-0.0039,0.0357,0.0207,0.0654,0.0215,0.0448,-0.0412,-0.0774,-0.0306,-0.0261,0.0095,0.0007,-0.0296,-0.0566,0.0205,-0.0363,0.0369,0.0517,0.0341,0.0053,0.0597,0.0702,0.0406,0.04,0.0041,0.0401,-0.1398,0.0513,0.0436,0.0444,-0.0189,-0.0134,-0.0148,0.0638,0.019,0.0354,0.0074,0.0258,0.027,-0.0081,-0.0287,-0.0631,0.0063,0.0026,0.033,-0.0128,-0.0394,0.0144,-0.0237,-0.0813,0.0175,-0.008,0.075,-0.0188,0.0005,0.0283,-0.0413,0.0073,-0.093,-0.0062,0.0166,0.0474,-0.0563,0.2245,-0.0156,0.0305,0.0267,-0.043,0.024,-0.0056,-0.0273,-0.0331,-0.0035,-0.0022,-0.0197,-0.0089,0.0667,0.0015,-0.0094,-0.0192,0.032,0.0201,-0.0177,-0.0401,-0.0604,0.0185,0.0466,-0.0282,0.0175,-0.068,0.0027,0.1519,0.004,0.0005,-0.0033,-0.06,-0.0455,-0.0054,0.0275,0.044,-0.0369,0.0263,0.0103,-0.0215,-0.0822,-0.0263,0.0086,-0.064,-0.0484,0.1214,-0.0175,0.044,-0.0393,-0.0183,-0.0301,0.0085,-0.0299,-0.0076,-0.0017,0.0597,0.021,0.0435,-0.0589,0.0067,-0.0468,0.0083,-0.0065,0.1326,-0.0272,-0.0668,-0.0593,0.0031,-0.0309,0.0164,0.0044,0.0214,-0.0656,0.0252,0.0686,0.0172,-0.0949,-0.0007,-0.0176,0.0196,-0.0082,-0.0462,-0.0019,-0.0182,0.0142,-0.0442,0.014,-0.0494,0.0316,0.0367,-0.0245,0.0001,-0.0366,-0.0244,-0.028,-0.0378,0.0005,-0.0201,0.0127,0.0162,0.0149,-0.0119,-0.0475,0.0224,0.032,0.0394,-0.012,-0.0433,0.0188,0.014,0.0077,0.0365,0.0404,-0.0426,-0.0254,-0.0421,0.0366,0.054,-0.0282,0.0777,0.0083,-0.0368,-0.013,-0.2445,-0.0223,-0.0809,-0.0075,0.0253,-0.0995,0.0192,-0.0318,0.0401,0.0759,0.057,-0.0637,-0.0005,0.0197,0.0252,0.0442,0.0247,0.0251,0.0165,0.0271,-0.0036,-0.0022,0.0094,-0.0585,0.0213,0.0002,0.2641,0.0456,0.0092,-0.0065,0.0303,0.0103,-0.0357,-0.0824,0.0663,0.0068,0.0658,0.0018,-0.0501,-0.0493,-0.0401,-0.0165,-0.0252,-0.061,-0.0633,0.0199,-0.0276,0.0522,-0.0664,0.0507,0.0218,-0.0191,0.0714,-0.0173,-0.0025,-0.048,-0.0949,-0.0112,-0.0253,0.027,0.0128,-0.032,0.0088,-0.0384,0.0454,-0.0047,0.0599,-0.0499,0.0179,-0.0143,-0.0092,0.046,-0.0056,-0.0007,0.054,0.0053,0.055,-0.0712,-0.0471,-0.0099,0.0706,-0.0036,0.0617,0.0218,0.057,-0.02,0.0728,-0.0039,0.0594,-0.0211,-0.0251,0.0302,-0.0653,0.0055,0.0491,0.0018,-0.3059,0.0547,0.0457,0.0457,-0.0447,0.0488,0.0268,-0.0058,-0.0267,-0.0071,0.008,0.0497,0.039,0.0012,0.0189,0.023,0.0812,-0.0517,0.0333,-0.0186,0.0369,0.0433,0.2459,-0.0714,0.0017,0.0141,-0.0223,0.0076,0.0197,-0.0145,0.02,0.0094,0.0906,-0.0619,0.0459,0.0523,-0.0224,0.0009,0.0143,-0.0249,-0.0714,0.0404,-0.0264,0.0068,0.0803,0.0242,-0.002,-0.0136,-0.0334,0.0377,-0.0565,0.0189,0.0038,-0.0162,0.0513,0.0381,-0.0474,-0.058,0.0064,-0.0598,0.0164,-0.0565,-0.0261,-0.0304,-0.0283]}
{"key":"[Person-Job Fit: Adapting the Right Talent for the Right Job with Joint Representation Learning] Person-Job Fit is the process of matching the right talent for the right job by identifying talent competencies that are required for the job. While many qualitative efforts have been made in related fields, it still lacks of quantitative ways of measuring talent competencies as well as the job's talent requirements. To this end, in this paper, we propose a novel end-to-end data-driven model based on Convolutional Neural Network (CNN), namely Person-Job Fit Neural Network (PJFNN), for matching a talent qualification to the requirements of a job. To be specific, PJFNN is a bipartite neural network which can effectively learn the joint representation of Person-Job fitness from historical job applications. In particular, due to the design of a hierarchical representation structure, PJFNN can not only estimate whether a candidate fits a job, but also identify which specific requirement items in the job posting are satisfied by the candidate by measuring the distances between corresponding latent representations. Finally, the extensive experiments on a large-scale real-world dataset clearly validate the performance of PJFNN in terms of Person-Job Fit prediction. Also, we provide effective data visualization to show some job and talent benchmark insights obtained by PJFNN.","layer":0,"vector":[-0.0747,-0.0068,0.0037,-0.0597,0.0318,0.0453,0.0687,-0.0558,-0.0187,-0.023,-0.0002,-0.0643,0.0436,0.0475,0.0084,0.0003,0.0065,0.0611,-0.042,0.0005,0.0064,-0.0033,-0.0091,-0.0713,0.0471,0.0278,-0.0345,-0.0306,-0.0508,-0.2306,-0.0115,-0.0135,0.0366,-0.0179,0.0146,0.0111,-0.0367,0.0558,-0.0318,0.0195,0.0217,0.0359,-0.0504,-0.0328,-0.0276,-0.0283,-0.032,-0.0486,-0.0234,-0.0372,0.0394,-0.0451,0.0274,0.0484,0.042,0.0471,0.0936,0.0302,-0.0135,0.0416,0.0276,0.024,-0.1891,0.0384,0.0314,0.0104,-0.041,-0.0275,-0.0487,0.0212,-0.0089,0.0267,-0.0019,0.0399,-0.0311,0.0138,-0.0096,-0.0404,0.0119,0.0391,0.0242,-0.008,-0.02,0.0338,0.0395,-0.0269,0.0456,-0.024,0.0518,0.0082,-0.0337,-0.0079,-0.0973,0.0176,-0.0738,-0.0109,0.0667,0.0137,-0.0592,0.2137,-0.0169,0.0513,0.054,-0.0012,0.0396,0.0204,-0.038,-0.0128,-0.0135,-0.0164,-0.0447,-0.0189,-0.0183,-0.0251,0.0447,0.0216,0.0401,0.059,0.0132,-0.0059,0.0034,0.0513,0.0567,0.0014,0.0422,-0.0566,0.0246,0.1347,0.0217,0.0246,0.0464,-0.0245,-0.0759,-0.0296,0.0045,0.0317,-0.0101,0.0248,-0.0254,0.0112,0.0132,-0.0523,0.0234,-0.0576,-0.0279,0.126,-0.0458,0.0263,-0.0018,-0.027,-0.0737,0.0057,-0.0428,0.0001,0.0358,0.045,0.037,0.0621,-0.0745,0.0215,-0.0007,-0.0604,-0.0508,0.0368,0.0343,-0.1168,0.0218,-0.0086,0.005,-0.0125,0.0135,0.0044,-0.0805,0.0556,0.0743,0.0154,-0.06,0.0444,-0.009,0.0308,0.0399,-0.0846,-0.0528,0.0071,0.0334,-0.0263,0.0084,-0.0445,-0.007,0.0446,-0.0073,0.0624,-0.0446,-0.0153,0.0286,-0.0305,-0.0198,0.0109,0.0114,-0.0477,-0.0667,0.0395,-0.0523,0.019,-0.0183,0.0119,-0.0174,0.003,0.0726,0.0471,-0.035,-0.0203,0.0632,0.0056,-0.0005,-0.0125,0.0342,0.0279,0.0521,0.0462,0.0115,-0.0104,-0.0246,-0.2513,0.0236,-0.0252,-0.0207,0.0484,-0.0153,0.0219,0.0131,0.0333,0.0424,0.0862,0.0059,-0.0278,0.0276,0.0044,0.0218,0.0513,0.0356,-0.0461,-0.0326,0.0135,0.0261,0.0122,-0.0958,0.0773,0.008,0.2031,0.0231,0.0299,-0.0463,0.0363,0.0388,-0.0533,-0.1206,0.064,0.0043,0.0598,-0.0137,-0.0579,-0.0192,-0.022,0.0091,-0.015,-0.0872,-0.0131,-0.032,-0.0419,0.0327,-0.094,0.0291,0.0202,-0.0425,0.0632,-0.0225,-0.0227,-0.0366,-0.0965,0.0689,-0.067,0.0295,-0.0348,-0.0583,-0.0153,-0.0264,0.0375,0.0019,-0.0306,-0.0051,0.0044,-0.0152,-0.0229,0.0621,0.0322,-0.0082,0.0632,-0.0078,0.0155,-0.0211,0.0069,-0.0179,0.0686,-0.032,-0.0025,0.0014,0.0828,0.0281,0.0739,-0.0449,0.0487,-0.0485,-0.0452,-0.0131,-0.0458,0.0024,0.0029,0.025,-0.2913,0.0976,0.011,0.0131,-0.0185,-0.014,0.0233,0.0155,-0.0399,-0.0111,0.0104,0.0112,0.0181,-0.0362,0.0221,0.0445,0.0584,-0.0325,0.0398,-0.0497,-0.007,0.0304,0.2233,-0.0869,0.0389,0.0143,-0.0025,-0.0434,0.0529,-0.0362,0.0089,0.0087,0.1093,-0.0564,-0.0088,0.0762,-0.0144,0.0573,0.0062,0.0054,-0.0269,-0.0087,-0.0296,-0.0161,0.0968,0.0094,0.0151,-0.0423,0.0115,0.003,-0.067,0.0008,-0.0202,-0.0161,0.0153,0.0123,-0.0382,-0.0487,-0.0295,-0.0367,0.0247,-0.0049,-0.0133,0.0185,-0.0034]}
{"key":"[Weakly Supervised Learning of Rigid 3D Scene Flow] We propose a data-driven scene flow estimation algorithm exploiting the observation that many 3D scenes can be explained by a collection of agents moving as rigid bodies. At the core of our method lies a deep architecture able to reason at the \\textbf{object-level} by considering 3D scene flow in conjunction with other 3D tasks. This object level abstraction, enables us to relax the requirement for dense scene flow supervision with simpler binary background segmentation mask and ego-motion annotations. Our mild supervision requirements make our method well suited for recently released massive data collections for autonomous driving, which do not contain dense scene flow annotations. As output, our model provides low-level cues like pointwise flow and higher-level cues such as holistic scene understanding at the level of rigid objects. We further propose a test-time optimization refining the predicted rigid scene flow. We showcase the effectiveness and generalization capacity of our method on four different autonomous driving datasets. We release our source code and pre-trained models under \\url{github.com/zgojcic/Rigid3DSceneFlow}.","layer":11,"vector":[0.0084,-0.0142,0.0025,-0.0117,0.0195,0.0846,0.0431,0.0042,-0.0004,-0.0123,0.0354,-0.0573,0.0149,0.0497,0.0323,-0.0037,0.0282,0.084,-0.0407,-0.0339,0.0409,-0.0687,0.0166,-0.0266,0.0013,0.0514,-0.011,-0.0303,-0.0245,-0.2097,-0.0175,-0.038,0.0021,0.0015,-0.001,-0.0529,-0.0381,0.0677,-0.0307,0.0106,0.0147,0.0244,-0.0454,-0.0505,-0.0183,-0.0139,-0.0165,-0.0287,-0.0042,-0.044,0.013,-0.0488,0.05,0.0201,0.0422,0.0342,0.0408,0.046,0.0659,0.0124,0.0407,0.0342,-0.1459,0.0453,0.0833,0.05,-0.0656,-0.0065,0.0042,0.0067,-0.028,0.0352,0.0024,0.0058,0.0133,-0.0619,0.018,-0.0584,-0.0035,-0.0288,0.0167,-0.0175,-0.0705,-0.028,-0.0141,-0.0351,0.0323,-0.0378,0.0251,-0.0015,-0.0786,-0.0223,-0.0309,0.021,-0.0263,-0.0373,0.0136,0.0174,-0.0322,0.2101,-0.0208,0.055,0.0362,0.0035,0.0064,-0.0064,-0.0273,-0.0116,-0.0206,0.0131,-0.015,0.0179,0.0541,-0.0351,0.0435,-0.0035,0.0643,0.0624,-0.0764,0.0094,-0.0059,-0.0472,0.0758,-0.0394,0.0339,-0.0859,0.0429,0.1547,0.0445,0.0251,0.054,0.0204,-0.1268,-0.0284,0.0071,0.0138,0.019,0.017,-0.0029,-0.033,-0.085,-0.0087,-0.0121,-0.0738,-0.0388,0.0952,-0.036,0.0741,-0.0439,-0.0614,0.0042,0.0116,-0.0312,-0.0365,-0.0195,0.0261,0.0483,0.0642,-0.0944,0.0474,-0.0216,-0.0522,-0.0157,0.0773,-0.0061,-0.1121,0.004,-0.0147,0.0107,0.0153,-0.0158,0.0328,-0.0225,0.0126,0.0695,0.0114,-0.0877,0.0349,-0.0105,0.021,0.0082,-0.0775,-0.0163,0.0014,0.0295,-0.0348,-0.0052,-0.0147,-0.0052,0.0828,-0.0088,0.0218,-0.0097,0.0178,-0.0026,-0.0009,0.0332,0.0114,-0.0278,-0.0055,0.0097,0.025,-0.0236,0.0282,-0.0413,-0.003,-0.041,0.0135,0.0111,0.0409,-0.0445,-0.0418,0.0288,-0.01,-0.0036,-0.018,-0.0178,0.0176,-0.0023,0.0156,0.0121,-0.0258,-0.0328,-0.2271,-0.0035,0.0062,-0.0503,0.0621,-0.025,0.0333,-0.0034,0.0538,0.0787,0.0697,-0.052,-0.0029,0.0422,0.0133,0.0729,0.0104,0.0511,-0.0412,0.0266,0.0026,0.0203,-0.0695,-0.0799,0.0322,0.0144,0.2663,0.0328,0.0358,-0.0078,0.0146,0.0206,-0.0472,-0.1046,0.0308,0.0022,0.0642,-0.0291,-0.0101,-0.0417,-0.076,-0.0183,0.0181,-0.1126,-0.0634,-0.0261,-0.0122,0.0433,-0.04,0.0051,0.0511,-0.0786,0.0336,-0.0179,-0.0322,-0.0079,-0.0443,0.04,-0.0388,0.0681,-0.0012,-0.0225,0.0311,-0.0661,0.0705,0.0008,-0.0349,-0.0849,0.002,0.0054,-0.0241,0.0739,0.0053,-0.0206,0.1005,0.0181,0.0415,0.0064,-0.0144,-0.0647,0.0664,-0.0024,0.0467,0.0487,0.0547,0.0196,0.0473,-0.0621,0.0084,-0.0441,0.0694,0.0228,-0.093,-0.0253,0.0576,-0.0067,-0.2874,0.0017,0.0025,0.0519,-0.0265,0.0289,0.0478,-0.0142,-0.0384,-0.0197,-0.0152,0.0426,0.0112,-0.0014,0.0075,0.0498,0.1019,-0.0107,0.0473,-0.0588,-0.0219,0.039,0.2049,-0.0342,0.0108,0.0319,-0.0654,-0.0058,0.0517,-0.0358,0.0334,0.0328,0.0613,-0.0493,0.0288,0.0592,0.0048,0.0067,0.0219,0.0239,0.0045,0.0339,0.0043,-0.0426,0.0222,0.0259,-0.043,-0.0378,-0.017,0.0096,0.0076,0.0138,0.0051,-0.0061,0.0665,0.0072,-0.0296,-0.0429,-0.0581,-0.0037,0.0028,-0.0872,0.0366,0.0225,-0.0278]}
{"key":"[A Provably-Efficient Model-Free Algorithm for Constrained Markov Decision Processes] This paper presents the first model-free, simulator-free reinforcement learning algorithm for Constrained Markov Decision Processes (CMDPs) with sublinear regret and zero constraint violation. The algorithm is named Triple-Q because it includes three key components: a Q-function (also called action-value function) for the cumulative reward, a Q-function for the cumulative utility for the constraint, and a virtual-Queue that (over)-estimates the cumulative constraint violation. Under Triple-Q, at each step, an action is chosen based on the pseudo-Q-value that is a combination of the three \"Q\" values. The algorithm updates the reward and utility Q-values with learning rates that depend on the visit counts to the corresponding (state, action) pairs and are periodically reset. In the episodic CMDP setting, Triple-Q achieves $\\tilde{\\cal O}\\left(\\frac{1 }{\\delta}H^4 S^{\\frac{1}{2}}A^{\\frac{1}{2}}K^{\\frac{4}{5}} \\right)$ regret, where $K$ is the total number of episodes, $H$ is the number of steps in each episode, $S$ is the number of states, $A$ is the number of actions, and $\\delta$ is Slater's constant. Furthermore, Triple-Q guarantees zero constraint violation, both on expectation and with a high probability, when $K$ is sufficiently large. Finally, the computational complexity of Triple-Q is similar to SARSA for unconstrained MDPs and is computationally efficient.","layer":12,"vector":[-0.1163,0.0107,0.048,-0.0108,-0.0158,0.0281,0.0319,0.0068,0.0543,0.0329,0.0492,-0.0278,0.0075,0.0713,0.0388,0.0067,-0.0232,0.0569,-0.0234,0.0237,0.0441,-0.0505,-0.0241,-0.0507,0.0504,0.029,-0.0198,-0.0491,-0.0146,-0.2204,0.0426,-0.0108,0.0243,-0.0429,-0.0303,-0.0168,-0.0487,0.0653,-0.0095,0.0527,0.0516,0.0959,-0.0042,-0.063,-0.057,-0.0602,-0.014,-0.0247,-0.0027,0.006,0.0076,-0.0056,0.005,0.0042,0.0534,0.009,0.0647,0.0324,0.0321,0.0104,0.0079,0.0456,-0.1604,0.0444,0.0425,0.0173,-0.0548,0.0119,0.0167,0.0359,-0.0809,0.0542,0.0095,0.0318,0.0194,-0.0416,-0.0037,-0.0399,-0.0226,-0.0073,0.0175,-0.0435,-0.0446,0.0204,-0.0404,-0.0716,-0.0236,-0.0355,0.016,-0.0018,-0.04,-0.0153,-0.0083,-0.021,-0.0522,-0.0217,0.0414,0.0091,-0.0866,0.195,0.0051,0.0438,0.0044,-0.029,0.0437,-0.0519,-0.0285,-0.0364,-0.0188,-0.0349,-0.0262,-0.0196,0.0939,-0.0427,-0.046,0.0478,0.0694,0.0309,-0.0008,-0.0267,-0.0355,0.0429,0.0583,0.0113,-0.0212,-0.0776,0.0266,0.1549,-0.0053,-0.0222,0.0437,-0.0735,-0.0348,-0.0027,0.0281,-0.0014,0.0071,-0.0126,0.0176,-0.0277,-0.0498,-0.0343,-0.0158,-0.1397,-0.0499,0.1151,0.0179,0.0253,-0.0201,0.009,-0.0133,0.0001,0.0249,-0.051,0.0361,0.0065,0.0162,0.0422,-0.0356,0.0262,-0.0648,-0.0658,0.0044,0.1239,0.006,-0.0654,0.0106,-0.0033,-0.0089,-0.0054,0.0531,0.0675,-0.1104,-0.0055,0.0636,0.0116,-0.0724,-0.0444,0.0075,0.0033,-0.01,-0.0448,-0.0164,0.004,0.0608,-0.0228,0.0003,0.0131,0.0097,-0.0063,-0.0084,-0.0034,0.0056,-0.0064,-0.0557,-0.0301,-0.0169,-0.0052,0.0348,-0.0153,0.0323,-0.0296,-0.0583,0.041,0.0118,0.0019,-0.0224,0.0373,0.0313,0.0089,-0.0649,0.0352,0.0454,-0.0192,-0.0366,0.0194,0.0564,0.0078,-0.0194,0.0196,0.0551,0.034,-0.0392,-0.2036,0.0075,-0.0371,0.027,0.0552,-0.0242,0.0309,-0.0669,0.0363,0.0653,0.0648,-0.0567,-0.0307,0.0106,-0.004,0.0663,0.0395,0.0358,-0.0461,0.0347,0.0261,0.0201,-0.0355,-0.1048,0.0537,0.0108,0.2317,0.0277,0.0377,-0.0087,0.0314,0.0391,0.0049,-0.0771,0.0293,0.0515,0.039,-0.0039,-0.0006,-0.0451,-0.0028,0.006,-0.0093,-0.1036,-0.0279,-0.0249,-0.0502,0.0567,-0.0472,0.0176,0.0167,-0.0012,0.0159,-0.0321,-0.023,-0.0253,-0.0674,0.0301,-0.0307,0.0069,0.0498,-0.036,-0.0072,-0.0281,0.0582,-0.0057,0.0146,-0.0332,0.0534,-0.0076,-0.0198,0.064,0.0205,-0.0091,0.0059,0.0079,0.0283,-0.0668,-0.0468,-0.0812,0.0747,-0.0565,-0.0088,0.0352,-0.0092,0.0022,0.0584,-0.0087,0.001,-0.0262,-0.0004,0.0121,-0.0292,0.0194,0.0826,-0.0211,-0.3177,0.0225,-0.0122,0.0137,-0.0081,0.0278,0.0432,-0.0048,-0.0727,0.0014,0.0222,0.0528,0.0068,0.0091,0.0551,0.0306,0.0632,-0.0141,0.0639,-0.0839,0.0221,0.0238,0.192,-0.0385,0.0542,0.0309,-0.0181,-0.0197,0.0552,-0.0109,0.0018,-0.0241,0.0422,-0.1002,0.0775,0.0717,-0.0361,0.0678,0.0422,-0.0029,-0.0213,0.0241,0.0028,0.0094,0.0975,0.0141,-0.037,-0.0236,-0.0303,0.0513,-0.0287,0.0376,0.0065,0.0078,0.0144,0.0015,-0.0516,-0.086,-0.031,-0.0625,0.0501,-0.0401,0.0448,0.011,-0.0242]}
{"key":"[Robust Nonparametric Regression under Huber's $\\epsilon$-contamination Model] We consider the non-parametric regression problem under Huber's $\\epsilon$-contamination model, in which an $\\epsilon$ fraction of observations are subject to arbitrary adversarial noise. We first show that a simple local binning median step can effectively remove the adversary noise and this median estimator is minimax optimal up to absolute constants over the H\\\"{o}lder function class with smoothness parameters smaller than or equal to 1. Furthermore, when the underlying function has higher smoothness, we show that using local binning median as pre-preprocessing step to remove the adversarial noise, then we can apply any non-parametric estimator on top of the medians. In particular we show local median binning followed by kernel smoothing and local polynomial regression achieve minimaxity over H\\\"{o}lder and Sobolev classes with arbitrary smoothness parameters. Our main proof technique is a decoupled analysis of adversary noise and stochastic noise, which can be potentially applied to other robust estimation problems. We also provide numerical results to verify the effectiveness of our proposed methods.","layer":1,"vector":[0.0014,-0.0568,0.0175,-0.0203,0.0403,0.0046,0.0499,0.0218,-0.003,-0.0027,0.025,-0.051,0.0157,0.0265,-0.0098,0.0237,0.0553,0.0409,-0.012,0.0416,0.0128,-0.0404,0.0253,-0.0406,0.0231,0.0046,-0.0217,-0.0894,-0.0671,-0.266,0.0117,-0.0586,0.0373,-0.0243,0.041,-0.005,0.0151,0.059,0.0019,0.0538,-0.0057,0.0366,-0.0172,-0.0686,-0.0535,-0.0477,0.0284,-0.0319,-0.0185,-0.0489,0.0459,-0.0266,0.0212,-0.0033,0.062,-0.0117,0.0491,0.035,0.0289,0.0817,-0.0011,0.0452,-0.1517,0.0327,0.057,0.0423,-0.0467,-0.0391,0.0076,-0.0003,0.0242,0.0184,-0.0002,0.0247,0.0094,-0.0189,0.0132,-0.0648,-0.0402,0.0083,0.0486,-0.0099,-0.0302,0.0088,-0.0507,-0.0514,0.0253,-0.0237,0.0415,0.0041,-0.0214,0.0041,-0.007,0.029,-0.0404,-0.0053,0.0724,0.0319,-0.0369,0.2025,-0.0378,0.036,0.0058,-0.0632,0.0647,-0.0236,-0.017,-0.0511,0.0205,0.0091,0.0043,-0.0013,0.0224,-0.0261,0.0155,-0.0107,0.0443,0.0359,-0.0141,0.0082,-0.0271,0.0244,0.0716,-0.0208,0.0457,-0.0533,-0.0055,0.1662,0.0408,0.028,-0.0071,-0.0497,-0.0191,-0.0025,0.0156,-0.0116,-0.0277,0.0249,0.0442,0.0146,-0.0569,-0.1038,0.0103,-0.1015,-0.055,0.123,-0.0587,0.0426,-0.0382,-0.0603,0.0085,0.0662,-0.0192,0.0086,0.0284,0.0078,0.0221,0.0453,-0.0576,0.0086,-0.0375,-0.0282,0.009,0.131,-0.037,-0.0468,-0.0339,0.0047,0.0112,0.0407,0.0343,0.0104,-0.0214,0.045,0.071,0.029,-0.0617,0.0442,-0.0096,0.0409,-0.0259,-0.056,-0.0655,0.0567,0.0222,-0.0323,0.0203,-0.0484,0.0713,0.0431,-0.0638,0.0118,-0.0627,-0.0195,-0.0343,-0.0446,-0.0246,-0.0234,-0.0056,-0.0518,-0.0112,0.0022,-0.0402,0.0073,0.0033,0.0331,0.0035,-0.0484,-0.0187,0.0333,-0.0217,-0.0245,0.0345,-0.0232,0.0186,0.0327,0.034,0.0513,0.0051,0.0569,0.0281,-0.0447,-0.0614,-0.2184,-0.014,0.0086,-0.0007,0.062,-0.0635,0.0158,-0.0084,0.1188,0.0923,0.0348,-0.0094,-0.0227,0.0603,-0.0388,0.0908,0.022,0.0086,-0.0334,-0.027,-0.0224,0.0237,-0.0574,-0.0492,0.0531,-0.02,0.2013,0.0006,0.0691,-0.0207,0.0313,-0.0022,-0.0055,-0.0589,0.0248,0.0117,0.0524,-0.014,-0.0519,-0.0076,-0.0304,-0.0078,0.0032,-0.0864,-0.0742,-0.0482,-0.0543,0.038,-0.0706,0.0514,0.0309,-0.0205,0.1055,-0.0145,0.0209,-0.0486,-0.0804,0.054,-0.0363,0.0797,-0.0028,-0.0665,0.0163,-0.0852,0.0583,0.0372,-0.0125,-0.0778,0.0384,-0.015,-0.0103,0.0752,0.0025,0.0216,0.0684,0.0401,-0.0015,-0.0425,-0.0618,-0.0202,0.0769,-0.0,0.026,0.0256,0.0167,-0.0313,0.0618,0.0107,0.0187,-0.0304,-0.0181,-0.0335,-0.0775,-0.0009,0.0427,0.0291,-0.2859,-0.0262,0.0001,0.0071,-0.0188,-0.0081,0.0238,0.0096,-0.0497,0.0332,-0.0261,0.0521,0.036,-0.0221,0.0456,-0.0044,0.0444,-0.0942,0.043,-0.0413,0.014,-0.028,0.1788,-0.0593,-0.0076,-0.0061,-0.0074,0.061,0.0212,-0.0702,0.0373,0.0213,0.0644,-0.0286,0.0275,0.0775,-0.0426,0.0324,0.013,-0.0291,-0.0412,0.0128,-0.0221,-0.0002,0.1011,-0.0484,-0.0116,-0.0333,0.0146,0.0354,-0.013,0.034,0.01,0.0307,0.0202,0.0464,-0.0666,-0.0132,-0.0529,0.0103,0.0239,-0.0343,-0.0458,0.0045,-0.0325]}
{"key":"[ActionSpotter: Deep Reinforcement Learning Framework for Temporal Action Spotting in Videos] Summarizing video content is an important task in many applications. This task can be defined as the computation of the ordered list of actions present in a video. Such a list could be extracted using action detection algorithms. However, it is not necessary to determine the temporal boundaries of actions to know their existence. Moreover, localizing precise boundaries usually requires dense video analysis to be effective. In this work, we propose to directly compute this ordered list by sparsely browsing the video and selecting one frame per action instance, task known as action spotting in literature. To do this, we propose ActionSpotter, a spotting algorithm that takes advantage of Deep Reinforcement Learning to efficiently spot actions while adapting its video browsing speed, without additional supervision. Experiments performed on datasets THUMOS14 and ActivityNet show that our framework outperforms state of the art detection methods. In particular, the spotting mean Average Precision on THUMOS14 is significantly improved from 59.7% to 65.6% while skipping 23% of video.","layer":0,"vector":[-0.0743,-0.0038,0.0307,-0.0276,0.033,0.0537,0.059,0.0227,0.0645,-0.0128,-0.0149,-0.0415,-0.0115,0.0747,0.0171,-0.0078,0.0027,0.0317,-0.0334,-0.0383,0.0193,-0.0543,0.0303,-0.0413,0.0074,-0.0152,-0.0364,-0.0728,-0.0342,-0.2301,0.0125,-0.0538,0.0505,0.0091,-0.0049,-0.0252,-0.0582,0.0615,-0.0635,0.0395,0.048,0.0219,-0.0191,-0.0542,-0.0353,-0.0529,0.0299,-0.0361,-0.0099,-0.0538,0.0274,-0.0608,0.0172,0.0198,0.0269,-0.0229,0.064,0.0446,0.0612,0.0396,0.0348,0.0209,-0.1471,0.0358,0.0307,0.0599,0.0024,-0.0039,0.0433,0.0073,-0.0454,0.0778,0.0051,0.0558,-0.0172,-0.0071,-0.0053,-0.0363,-0.0163,-0.0372,0.0187,-0.0208,-0.056,-0.0095,-0.0125,-0.0421,0.023,-0.0623,0.0395,-0.0064,-0.0345,0.0339,-0.0279,0.0448,-0.0463,-0.0001,0.0211,-0.0301,-0.0387,0.2275,-0.029,0.0262,0.0478,0.0045,0.0556,-0.0139,-0.0349,0.0077,-0.0178,0.0138,0.0021,-0.001,0.0391,-0.0269,0.0708,-0.0178,0.0482,0.0407,-0.0035,-0.0198,0.0175,-0.0015,0.0586,-0.0607,0.0004,-0.0785,0.0489,0.161,0.0458,0.0065,0.0367,-0.0378,-0.0105,-0.0244,0.0273,0.0343,-0.0024,-0.005,0.0394,-0.043,-0.0217,0.0132,0.0233,-0.0813,-0.0099,0.1201,-0.0159,-0.0045,-0.0255,-0.0221,-0.0265,0.0379,-0.0314,-0.0065,0.0259,0.0178,0.0724,0.0531,-0.0433,0.0062,-0.0299,-0.0925,-0.0146,0.0856,0.0188,-0.1222,-0.0084,0.0003,-0.0167,0.0,-0.0213,0.0267,-0.0555,0.0432,0.0808,0.0171,-0.0577,0.0114,0.0045,-0.0004,0.0517,-0.0552,-0.0118,0.0606,0.0325,-0.045,-0.0067,-0.0114,0.0514,0.0464,-0.0538,0.0274,-0.0139,-0.0019,-0.0101,-0.0156,0.0062,-0.0095,-0.0049,0.0026,0.036,-0.054,-0.0328,0.0058,0.0111,-0.013,-0.073,-0.0178,0.0422,0.0196,-0.0185,0.0193,0.0526,-0.0538,-0.0262,-0.0032,0.0097,0.0019,-0.0232,0.012,0.0321,-0.0234,-0.0047,-0.2433,-0.0385,-0.0024,0.0132,0.0207,-0.0531,0.019,-0.0245,0.0693,0.0473,0.0209,-0.0623,-0.0244,-0.0104,0.0007,0.0695,0.0323,0.0323,-0.0246,-0.0013,0.0217,-0.0176,-0.0423,-0.1067,0.0247,-0.0053,0.2284,0.084,0.0219,-0.0243,0.0519,0.0458,-0.0401,-0.1084,0.0189,0.0021,0.0841,-0.0278,-0.0202,-0.0641,-0.0446,0.0288,-0.0262,-0.1187,-0.0146,-0.0463,-0.0336,0.0165,-0.0515,-0.0116,0.0444,-0.0597,0.0674,0.0217,-0.0334,-0.0035,-0.0235,0.0311,-0.0401,0.0295,-0.0369,-0.0431,0.0386,-0.0591,0.0808,0.0369,-0.0094,-0.0634,0.0074,0.0136,0.0126,0.0497,-0.0104,-0.01,0.0445,0.0135,0.0477,-0.0279,-0.0317,-0.0459,0.0684,-0.0115,-0.0096,-0.0263,-0.0058,-0.0102,0.0328,-0.0136,0.0439,-0.028,0.0227,0.0507,-0.0904,0.0274,0.0119,0.0307,-0.2999,0.0478,-0.0014,0.01,-0.0062,-0.0271,0.0753,-0.0076,-0.0303,0.0497,0.0109,0.0514,0.0007,-0.0028,-0.0394,0.0511,0.1152,-0.0297,0.0488,-0.069,0.0579,0.0011,0.1981,-0.0387,0.0058,-0.0142,-0.0088,-0.0204,0.0713,-0.0549,-0.0211,-0.0111,0.0738,-0.0387,0.0019,0.0806,0.0027,0.0314,0.0126,0.0707,-0.0156,-0.009,-0.0126,-0.0259,0.0802,-0.0463,-0.0187,-0.014,-0.0341,0.0742,-0.031,0.0005,-0.0335,0.0288,0.0426,0.0388,-0.0208,-0.0246,-0.0439,-0.0162,0.0066,-0.077,0.0581,-0.0225,0.0186]}
{"key":"[Deeper Learning By Doing: Integrating Hands-On Research Projects Into a Machine Learning Course] Machine learning has seen a vast increase of interest in recent years, along with an abundance of learning resources. While conventional lectures provide students with important information and knowledge, we also believe that additional project-based learning components can motivate students to engage in topics more deeply. In addition to incorporating project-based learning in our courses, we aim to develop project-based learning components aligned with real-world tasks, including experimental design and execution, report writing, oral presentation, and peer-reviewing. This paper describes the organization of our project-based machine learning courses with a particular emphasis on the class project components and shares our resources with instructors who would like to include similar elements in their courses.","layer":3,"vector":[-0.022,-0.0157,-0.0038,-0.0154,0.0561,0.01,0.0112,0.0655,-0.0069,0.0005,0.0433,-0.0292,0.0387,0.0258,0.0536,-0.0064,-0.011,0.031,-0.0518,-0.0199,0.0233,-0.0449,0.0067,-0.0536,0.0319,0.0334,-0.0164,-0.0572,-0.0458,-0.2384,-0.0139,-0.0397,0.0667,0.002,0.0274,-0.0163,-0.0113,0.0405,-0.0346,0.027,0.0725,-0.0097,-0.014,-0.1014,0.0139,-0.0425,-0.0477,-0.072,-0.0129,-0.0318,0.0254,-0.0754,0.0246,0.0292,0.0145,0.0561,0.0324,0.062,0.0358,0.0539,0.058,0.0409,-0.1991,0.0647,0.014,0.0571,-0.0287,0.0034,0.019,0.0659,-0.0259,0.0361,0.0074,0.0927,0.0071,0.0289,-0.0072,-0.0702,0.0202,-0.0239,0.0081,-0.0081,-0.0145,0.0047,0.0203,-0.0546,0.017,-0.0199,0.0341,0.0108,-0.0383,-0.0417,-0.0471,0.0214,-0.0449,0.0268,0.0469,-0.0052,-0.0505,0.1843,-0.0841,0.0427,0.0195,-0.0494,0.0069,-0.0445,-0.0131,-0.0516,0.0115,-0.013,-0.0058,0.0228,-0.0036,-0.038,-0.0047,0.0275,0.0676,0.0647,-0.0111,0.0013,0.0036,-0.002,0.0099,-0.0392,0.0014,-0.0609,0.0273,0.1124,0.0488,0.024,0.0449,-0.0324,-0.1028,-0.0274,0.0225,0.0465,-0.0044,0.0138,0.0357,-0.0108,-0.038,0.0033,0.0115,-0.0818,-0.0921,0.1098,-0.0218,0.0453,-0.0245,-0.0223,-0.0128,0.0083,-0.0555,-0.0357,0.0207,0.0106,0.0229,0.059,-0.0412,0.0411,-0.0292,-0.0341,-0.0028,0.1008,0.0158,-0.0848,-0.0084,-0.0118,-0.0058,-0.0272,0.0391,0.054,-0.0155,0.0148,0.0672,0.0168,-0.0374,0.0105,0.0084,0.0044,0.0206,-0.0401,-0.0247,0.028,0.0541,-0.0332,0.0192,-0.0726,-0.0118,0.0534,-0.0213,0.06,-0.0043,-0.0279,-0.018,-0.0307,0.0262,-0.0223,-0.0172,-0.0112,-0.0415,0.0353,-0.0209,0.0513,-0.0076,0.0327,0.0258,0.0189,0.0472,0.0241,-0.0283,-0.0236,0.0516,-0.0244,-0.0951,0.0063,0.0274,0.0409,-0.0188,0.0961,0.0091,-0.0491,-0.0666,-0.2169,-0.0113,0.0378,-0.0668,0.0172,-0.0678,0.0435,-0.0643,0.0286,0.0456,0.0977,-0.0318,-0.0365,-0.0089,-0.0118,0.0325,0.032,-0.0129,-0.0486,-0.0154,-0.0178,-0.0108,0.0223,-0.0966,0.0169,0.0229,0.2055,0.0319,0.0149,0.008,0.0413,0.0216,-0.0072,-0.1455,0.0245,-0.0435,0.0857,0.0016,-0.049,-0.0136,-0.0255,0.0328,-0.0206,-0.1087,-0.0599,-0.008,-0.0376,0.0018,-0.0536,0.0181,0.0042,-0.0407,0.0567,-0.0099,-0.0716,0.0097,-0.0588,0.0514,-0.0311,0.0308,-0.0069,-0.033,0.0297,-0.0508,0.0483,-0.0061,-0.0228,-0.0313,0.0369,-0.0422,-0.0074,0.0982,0.0036,0.0099,0.0536,0.0009,0.0215,-0.0444,-0.0326,-0.001,0.0888,-0.0434,0.0313,0.0221,0.0339,0.0219,0.0785,-0.0244,0.0489,-0.0017,0.0048,0.0074,-0.0559,0.0046,0.0545,0.0131,-0.2914,0.0282,0.0213,0.0414,-0.0594,0.0044,0.0623,0.0104,-0.0083,0.019,0.0693,-0.0048,0.0347,0.0018,0.0014,0.0336,0.1182,-0.0436,0.0555,-0.0374,0.0109,0.0251,0.1892,-0.0305,0.0447,-0.0021,-0.0231,-0.0072,0.0696,-0.0456,0.0068,-0.0331,0.1149,-0.0449,0.0231,0.1106,-0.0494,0.0136,0.0174,0.0042,0.0104,-0.0093,-0.0592,-0.0258,0.0782,0.0215,0.0002,-0.0931,-0.0163,0.0153,0.0001,-0.0192,-0.043,0.0152,0.045,0.0134,-0.0346,-0.018,-0.0635,-0.0484,0.0156,-0.0511,-0.01,-0.0311,0.0068]}
{"key":"[Self-supervised Pre-training with Hard Examples Improves Visual Representations] Self-supervised pre-training (SSP) employs random image transformations to generate training data for visual representation learning. In this paper, we first present a modeling framework that unifies existing SSP methods as learning to predict pseudo-labels. Then, we propose new data augmentation methods of generating training examples whose pseudo-labels are harder to predict than those generated via random image transformations. Specifically, we use adversarial training and CutMix to create hard examples (HEXA) to be used as augmented views for MoCo-v2 and DeepCluster-v2, leading to two variants HEXA_{MoCo} and HEXA_{DCluster}, respectively. In our experiments, we pre-train models on ImageNet and evaluate them on multiple public benchmarks. Our evaluation shows that the two new algorithm variants outperform their original counterparts, and achieve new state-of-the-art on a wide range of tasks where limited task supervision is available for fine-tuning. These results verify that hard examples are instrumental in improving the generalization of the pre-trained models.","layer":5,"vector":[-0.0242,-0.0583,0.0285,-0.017,0.0401,0.0487,0.0195,-0.0082,-0.0336,-0.0541,0.0145,-0.0653,0.0354,0.1094,0.0362,0.0321,0.0228,0.0566,-0.0677,-0.0162,0.0098,-0.0211,-0.0262,-0.0127,0.0442,-0.016,-0.0379,-0.0379,-0.0148,-0.2421,0.0209,-0.0594,0.0223,-0.0089,0.0271,-0.0632,-0.0445,0.1058,-0.058,0.017,0.0092,-0.0054,-0.0457,-0.0499,-0.0299,-0.0394,-0.0381,-0.0587,-0.0063,-0.0384,0.0109,-0.0442,-0.0001,0.0223,0.0081,0.0181,0.0447,0.0118,0.0238,0.067,0.0316,0.0581,-0.1841,0.0714,0.0098,0.0423,-0.0789,-0.0265,-0.0093,0.0434,-0.0129,0.019,-0.0016,-0.002,-0.0118,0.0198,0.0047,-0.0231,0.0083,-0.0165,0.0071,0.0036,-0.0367,0.0031,0.0238,-0.0405,0.0273,-0.0502,0.0989,0.0432,-0.0515,-0.0138,-0.0347,0.0501,-0.0229,0.0043,0.0387,0.0325,-0.0598,0.1946,-0.0606,0.0509,0.0635,-0.0162,0.0671,-0.0094,-0.0311,-0.0323,-0.0334,0.0056,-0.0068,-0.002,-0.016,-0.0396,0.0474,-0.0256,0.0991,0.014,-0.0092,0.0051,-0.0117,-0.0055,0.0375,0.0124,0.0348,-0.0729,0.0178,0.1626,0.0172,0.0181,0.0314,-0.0256,-0.0378,0.0176,-0.0138,0.0203,0.0125,-0.0111,-0.0168,-0.0221,-0.0162,0.0124,0.0012,-0.0467,-0.0374,0.0957,-0.0626,0.0483,-0.044,-0.048,-0.0089,0.0261,-0.0106,0.0009,0.023,0.0382,0.0241,0.0289,-0.0404,0.0213,-0.014,-0.0777,-0.0341,0.0828,0.0404,-0.1082,-0.002,-0.0145,0.0144,-0.0107,-0.0256,0.0059,-0.0222,0.0275,0.0618,0.0616,-0.1167,-0.0164,-0.0213,0.0047,0.0134,-0.0578,-0.009,0.0449,0.003,-0.0418,0.0284,-0.0607,0.0188,0.0433,-0.0164,0.0731,-0.073,-0.0115,-0.0026,-0.0178,-0.0215,-0.0058,0.0067,-0.0061,-0.028,0.0075,-0.0014,0.0265,-0.0149,-0.0263,-0.0301,-0.0211,0.0594,0.0385,-0.0213,0.0014,0.0512,-0.0008,-0.0327,0.0009,-0.0092,0.0964,0.0094,-0.0113,0.0497,-0.0647,-0.019,-0.2555,0.0121,-0.0125,-0.046,0.0265,-0.0864,0.0278,0.0248,0.0604,0.0631,0.059,-0.023,-0.0212,-0.0118,-0.0306,0.0397,0.0369,0.055,-0.0018,0.0196,-0.0208,0.0305,-0.0086,-0.0862,0.0404,0.0217,0.232,0.0301,0.0369,-0.0209,0.0117,0.0015,-0.0781,-0.1136,0.0465,-0.0107,0.0847,-0.0106,-0.0075,-0.0214,0.0177,-0.0002,0.0409,-0.1302,-0.0255,-0.0035,-0.017,0.0337,-0.0605,0.0349,0.0404,-0.0407,0.0547,-0.0331,-0.0252,-0.0196,-0.0713,0.0224,-0.043,0.0238,-0.0472,-0.0833,0.0173,-0.0881,0.0599,0.0462,-0.0382,-0.06,0.0233,-0.0138,-0.0456,0.072,0.0019,-0.0136,0.0606,0.0084,-0.0117,-0.0195,-0.0458,-0.0002,0.0221,0.041,0.0192,0.011,0.051,0.041,0.0538,-0.0031,0.0342,-0.0079,-0.0339,0.0252,-0.0592,-0.0041,0.0754,-0.0152,-0.2852,0.0514,0.0342,0.0965,-0.0032,0.0469,0.0591,0.0109,-0.0289,-0.0027,-0.0288,-0.0041,0.0774,-0.0116,-0.0289,0.0417,0.0644,-0.0133,0.0569,-0.0422,0.001,-0.0055,0.1966,-0.048,-0.0139,0.0033,-0.0547,-0.0077,0.0332,-0.0115,0.0243,0.0239,0.1123,-0.0175,0.0123,0.0657,-0.0577,-0.0132,0.0123,0.0046,-0.0004,-0.0008,-0.0304,-0.036,0.0564,0.0306,0.0436,-0.0149,-0.02,0.0185,-0.0258,-0.0014,0.0261,-0.0266,0.0102,0.0261,-0.0256,-0.0208,-0.0279,-0.0183,0.0379,0.0041,0.0094,-0.0244,0.0275]}
{"key":"[The universal approximation theorem for complex-valued neural networks] We generalize the classical universal approximation theorem for neural networks to the case of complex-valued neural networks. Precisely, we consider feedforward networks with a complex activation function $\\sigma : \\mathbb{C} \\to \\mathbb{C}$ in which each neuron performs the operation $\\mathbb{C}^N \\to \\mathbb{C}, z \\mapsto \\sigma(b + w^T z)$ with weights $w \\in \\mathbb{C}^N$ and a bias $b \\in \\mathbb{C}$, and with $\\sigma$ applied componentwise. We completely characterize those activation functions $\\sigma$ for which the associated complex networks have the universal approximation property, meaning that they can uniformly approximate any continuous function on any compact subset of $\\mathbb{C}^d$ arbitrarily well. Unlike the classical case of real networks, the set of \"good activation functions\" which give rise to networks with the universal approximation property differs significantly depending on whether one considers deep networks or shallow networks: For deep networks with at least two hidden layers, the universal approximation property holds as long as $\\sigma$ is neither a polynomial, a holomorphic function, or an antiholomorphic function. Shallow networks, on the other hand, are universal if and only if the real part or the imaginary part of $\\sigma$ is not a polyharmonic function.","layer":0,"vector":[-0.0405,-0.012,0.0269,-0.0061,-0.0253,0.0621,0.0487,0.0503,0.0226,-0.0307,-0.0065,-0.0706,0.0813,0.0608,0.0213,0.0415,-0.0171,0.027,-0.0812,0.0061,0.0465,-0.0176,-0.0172,-0.0248,-0.0015,-0.0133,-0.0067,-0.0175,-0.0441,-0.2449,-0.0117,-0.0312,0.0951,-0.0227,0.0045,-0.0412,-0.011,0.016,-0.0335,0.0452,0.0514,0.0492,-0.0314,-0.0272,-0.0277,-0.0856,-0.0441,-0.0169,-0.0381,-0.0331,0.0551,0.0007,0.0456,0.0346,-0.0085,-0.0068,0.0495,0.0514,0.0419,0.0674,0.0237,0.0464,-0.1665,0.0377,0.0501,0.0282,-0.0178,-0.0954,0.0286,0.0432,0.0142,0.0338,-0.0117,0.0354,0.0365,-0.0262,0.0213,-0.0263,-0.047,0.0619,-0.0042,-0.0328,-0.0555,-0.008,0.0167,-0.0141,0.0325,-0.0678,0.001,-0.0147,-0.0366,-0.0386,-0.0143,-0.0183,-0.0266,-0.0235,0.0225,-0.0012,-0.0505,0.1948,-0.0011,0.0148,0.0432,-0.0342,0.0612,0.0074,-0.0457,-0.0731,-0.0294,0.0076,-0.0501,-0.0594,0.008,-0.0443,0.0069,0.0109,0.0419,0.0163,0.013,-0.0618,-0.0002,-0.0031,0.0687,0.008,0.0268,-0.0761,0.0033,0.1324,0.0888,0.0133,0.0189,-0.0078,-0.0342,-0.0158,0.058,0.0455,0.0278,0.0025,-0.0138,0.0226,-0.0557,-0.0771,0.0094,-0.0811,-0.0279,0.0751,-0.0603,0.021,-0.0095,-0.0043,-0.0122,0.0108,-0.0036,-0.0211,0.0489,0.0377,0.0455,0.0041,-0.0572,0.0017,-0.0419,-0.0471,-0.011,0.1376,0.0312,-0.0549,0.0169,-0.0103,0.0116,-0.0299,0.0239,0.026,-0.0201,0.02,0.1407,0.0099,-0.0889,-0.0442,-0.0046,-0.0136,0.0152,-0.0347,-0.0282,0.056,0.0183,-0.0137,-0.0072,-0.0749,0.0465,0.0655,-0.0679,0.021,-0.0559,-0.0048,-0.0437,-0.0377,0.0027,0.0103,0.0225,-0.0194,0.0061,-0.0217,-0.0259,-0.0326,0.0109,0.0125,-0.0404,-0.0003,0.0216,0.0131,-0.0332,0.0038,0.0451,-0.0491,-0.0175,-0.003,0.0179,0.0025,-0.0232,0.0249,0.0386,-0.0553,-0.1033,-0.2062,-0.0243,0.0234,-0.0198,0.0856,-0.089,0.0453,-0.0146,0.0025,0.0463,0.0571,0.0152,0.0074,0.0265,0.0072,0.061,0.0308,0.0061,-0.0343,-0.0205,-0.0133,0.0325,0.02,-0.0632,0.1026,-0.0131,0.2235,0.0007,0.118,-0.0212,-0.0156,0.0426,-0.0168,-0.0538,0.0527,0.0435,0.1052,-0.0066,-0.0053,-0.0451,-0.0179,0.0373,-0.01,-0.0859,0.0053,0.0457,-0.0085,0.0038,-0.1066,-0.0169,0.0266,-0.0175,0.0113,0.026,0.0233,-0.0299,-0.0731,0.0031,-0.0358,0.0472,0.0059,-0.0717,-0.0285,-0.0322,0.0677,0.0451,-0.0065,-0.0487,0.0459,0.0133,-0.0456,0.0759,0.0037,0.0182,0.0794,-0.0199,0.0077,-0.0222,-0.0125,-0.0007,0.0757,-0.0293,0.0194,0.0092,0.0185,0.0074,0.098,-0.0487,0.0016,-0.0275,-0.0178,-0.0104,-0.0537,-0.021,0.0216,-0.0235,-0.2676,0.0262,-0.0205,0.0247,-0.0259,0.0242,0.0227,0.0007,-0.0351,-0.0104,0.0471,0.0815,0.0233,-0.0163,-0.0165,0.0568,0.0363,-0.0362,0.0653,-0.0531,0.0207,0.0484,0.2298,-0.0503,0.0132,0.0187,-0.0343,-0.0327,-0.0156,-0.0175,0.0144,0.0594,0.0518,-0.0497,0.0607,0.1006,-0.0053,0.0381,0.0184,-0.0225,-0.0037,-0.0241,-0.0742,-0.0235,0.088,-0.009,-0.0309,-0.018,-0.0158,0.0206,-0.0545,0.0141,0.0104,-0.0213,0.0233,0.0357,-0.0551,-0.063,-0.0538,-0.0056,0.0166,-0.0634,0.0074,0.0319,-0.0415]}
{"key":"[Faster Distributed Deep Net Training: Computation and Communication Decoupled Stochastic Gradient Descent] With the increase in the amount of data and the expansion of model scale, distributed parallel training becomes an important and successful technique to address the optimization challenges. Nevertheless, although distributed stochastic gradient descent (SGD) algorithms can achieve a linear iteration speedup, they are limited significantly in practice by the communication cost, making it difficult to achieve a linear time speedup. In this paper, we propose a computation and communication decoupled stochastic gradient descent (CoCoD-SGD) algorithm to run computation and communication in parallel to reduce the communication cost. We prove that CoCoD-SGD has a linear iteration speedup with respect to the total computation capability of the hardware resources. In addition, it has a lower communication complexity and better time speedup comparing with traditional distributed SGD algorithms. Experiments on deep neural network training demonstrate the significant improvements of CoCoD-SGD: when training ResNet18 and VGG16 with 16 Geforce GTX 1080Ti GPUs, CoCoD-SGD is up to 2-3$\\times$ faster than traditional synchronous SGD.","layer":7,"vector":[-0.0506,0.0072,0.0497,0.0252,0.0025,0.028,0.0085,-0.0165,0.0657,-0.0432,0.0144,-0.0435,0.0633,0.0872,0.0292,0.0373,0.0122,0.0058,-0.0007,-0.0176,0.0194,-0.051,-0.0122,-0.0829,0.0159,-0.0295,-0.0509,-0.0558,-0.0512,-0.219,0.0259,-0.0189,0.0519,-0.0251,0.0361,-0.0443,0.006,0.0853,-0.0669,0.0427,-0.0128,0.0571,-0.0601,-0.026,0.0356,-0.0309,-0.0281,-0.0495,0.0059,-0.0156,0.0659,-0.0151,0.0014,-0.0049,-0.0433,0.0536,0.0476,0.0322,0.0176,0.0053,-0.0033,0.0664,-0.1883,0.0776,0.0413,-0.0144,-0.0041,-0.0198,0.0285,0.0499,-0.0232,0.0242,0.0691,0.0292,0.0464,-0.0297,0.0054,-0.0522,0.0127,-0.0096,0.0253,0.0044,-0.0448,0.0059,-0.0067,-0.0167,0.061,0.0091,0.0175,-0.008,-0.0351,-0.0261,-0.0202,-0.0042,-0.0916,0.021,0.0078,0.0228,-0.0709,0.2188,-0.0658,0.0001,0.025,-0.0164,0.023,-0.0105,-0.0208,-0.0055,0.0115,0.0199,-0.0157,-0.0204,0.0359,-0.0117,0.0195,0.0206,0.0432,0.022,-0.05,0.018,-0.0611,0.0339,0.071,-0.0038,0.0557,-0.0947,0.0018,0.1218,0.0305,0.0291,0.0586,-0.0172,-0.0092,-0.0364,0.0576,0.0091,0.0198,-0.0186,0.0142,-0.0062,-0.0377,0.003,0.0059,-0.0729,-0.0172,0.106,-0.0234,0.0554,-0.048,-0.0462,0.0116,0.0244,-0.0016,-0.0002,-0.0068,0.0224,0.0458,0.0607,-0.0713,0.0076,-0.014,-0.034,-0.0066,0.1035,0.0479,-0.0931,-0.0497,-0.0022,0.0126,-0.0472,0.0282,0.019,-0.0354,0.0061,0.0631,0.0421,-0.0844,-0.0132,-0.0256,0.0239,0.0292,-0.0295,-0.0127,0.0562,0.0081,-0.0652,0.0222,-0.0493,0.0219,-0.0035,-0.0482,0.0166,-0.0848,-0.0047,-0.0306,-0.0464,-0.038,-0.0285,-0.0048,-0.012,0.0038,-0.0181,-0.0419,0.0361,-0.0355,-0.0045,-0.0015,-0.0131,0.0025,0.0459,-0.0321,-0.0208,0.0642,-0.0275,-0.0076,0.0199,0.0041,0.0468,-0.0182,0.0271,0.0467,-0.0396,-0.0477,-0.1805,-0.0153,0.0407,-0.0522,0.0684,-0.0561,0.0681,-0.0084,0.0726,0.0769,0.066,0.0009,-0.0259,0.0089,0.0173,0.0741,0.0406,0.0214,-0.0096,0.0081,0.0134,-0.0002,0.0303,-0.0872,0.0684,0.0208,0.2132,-0.0245,0.0827,-0.0458,0.0199,0.0337,-0.0263,-0.0962,0.0674,0.0072,0.0926,-0.0024,-0.0718,-0.0028,-0.0328,0.047,0.0658,-0.1284,-0.062,-0.03,-0.034,0.0145,-0.0434,0.0049,0.0188,-0.0261,0.0611,0.0223,0.0098,-0.0321,-0.1164,0.0391,-0.0298,0.0167,-0.026,-0.0487,-0.0152,-0.0521,0.0722,-0.0094,-0.0397,-0.0697,0.0244,-0.0071,0.0002,0.0391,-0.0067,0.0193,0.0521,-0.0086,0.0103,-0.0011,-0.0239,-0.0033,0.0875,-0.0379,0.0513,0.0549,0.0339,-0.0017,0.0413,0.0318,-0.0053,-0.0405,-0.0373,0.0244,-0.066,0.0202,-0.0028,-0.0098,-0.309,0.0389,0.0075,0.0071,-0.0248,-0.0052,0.0837,0.0284,-0.0705,0.0047,-0.0063,0.0267,0.0345,0.0195,0.0301,0.0584,0.064,-0.0424,0.0357,-0.0545,-0.0091,0.0491,0.1905,-0.0391,0.0454,-0.0257,-0.0342,0.0221,0.059,-0.0265,0.0029,-0.0165,0.0735,-0.0777,0.0082,0.0923,-0.0104,0.018,0.0423,-0.0227,-0.0037,0.0004,0.0199,-0.0417,0.0787,-0.0044,-0.0505,-0.0638,-0.0278,0.0234,-0.0448,0.0021,0.0218,-0.0327,-0.0105,0.0102,-0.0196,-0.0863,-0.0635,-0.0409,0.0301,-0.0694,-0.0652,-0.0218,-0.0108]}
{"key":"[A Stochastic Tensor Method for Non-convex Optimization] We present a stochastic optimization method that uses a fourth-order regularized model to find local minima of smooth and potentially non-convex objective functions with a finite-sum structure. This algorithm uses sub-sampled derivatives instead of exact quantities. The proposed approach is shown to find an $(\\epsilon_1,\\epsilon_2,\\epsilon_3)$-third-order critical point in at most $\\bigO\\left(\\max\\left(\\epsilon_1^{-4/3}, \\epsilon_2^{-2}, \\epsilon_3^{-4}\\right)\\right)$ iterations, thereby matching the rate of deterministic approaches. In order to prove this result, we derive a novel tensor concentration inequality for sums of tensors of any order that makes explicit use of the finite-sum structure of the objective function.","layer":3,"vector":[-0.0275,-0.0199,0.0482,-0.0072,-0.0236,0.0552,0.0143,0.0027,0.0447,-0.0101,0.013,-0.0393,0.0199,0.0789,0.0149,0.0562,-0.0131,0.0373,-0.0271,0.0362,0.0847,-0.0742,-0.018,-0.0522,0.0783,-0.0378,-0.0179,-0.0466,-0.0449,-0.2742,0.0399,-0.0353,0.0216,-0.091,0.0009,-0.0148,-0.0455,0.0743,-0.0447,0.0576,0.0132,0.0301,-0.0717,-0.0442,-0.0246,-0.0552,-0.0046,0.0066,-0.0357,0.0058,0.0199,-0.0459,-0.0034,-0.0069,0.0547,0.0196,0.0213,-0.0109,0.0547,0.0621,-0.0128,0.0301,-0.1689,0.0319,0.0558,0.0274,-0.0117,-0.0227,0.012,0.1133,-0.0226,0.0586,0.0105,0.0158,0.0241,-0.013,0.0535,-0.0378,-0.0258,0.0259,0.016,-0.0702,-0.027,0.0117,-0.0195,-0.023,0.0388,-0.0433,0.0607,0.055,-0.0098,-0.0299,-0.0263,0.0361,-0.0282,-0.0122,0.0364,0.0567,-0.0106,0.1873,-0.0545,0.0291,0.0442,-0.0184,0.0105,-0.006,-0.0319,-0.0089,-0.0012,0.0307,-0.0058,-0.0283,0.0593,-0.0675,0.0142,-0.0008,0.0262,-0.0215,-0.0145,0.0205,-0.0466,0.0333,0.0553,-0.001,-0.0233,-0.0569,-0.0254,0.1477,0.0266,0.0751,0.0216,-0.0384,-0.0506,-0.0395,0.01,-0.0125,-0.0316,-0.0176,0.0307,0.0016,-0.0288,-0.0506,0.0053,-0.1366,-0.0298,0.159,-0.039,0.0147,-0.0498,-0.047,0.0458,0.0199,-0.0178,0.0112,-0.0085,0.0516,0.0091,0.0144,-0.0898,0.0244,-0.0556,-0.0942,0.0206,0.1201,-0.0287,-0.0293,-0.0322,0.0089,0.0082,-0.0197,0.0561,0.0349,-0.0203,0.0375,0.0814,0.0437,-0.0868,0.0084,0.0108,0.0095,0.0056,0.0115,-0.0484,0.0255,-0.0033,-0.0232,0.0232,-0.0199,0.0212,0.0067,-0.0627,-0.0061,-0.0471,-0.0432,-0.0766,-0.0424,0.0262,-0.0136,0.0212,0.0024,0.0235,-0.0079,-0.0757,0.0316,0.0243,0.0258,-0.0296,-0.0236,0.0223,0.0797,-0.0345,-0.0013,0.065,-0.0047,-0.0162,0.0165,0.0426,0.0327,-0.0249,0.0531,0.0644,-0.0102,-0.0738,-0.2192,-0.0203,-0.0371,0.0157,0.0427,-0.0776,0.0346,-0.0134,0.0424,0.0674,0.0824,0.0093,-0.0275,0.0344,-0.0165,0.0384,0.0192,-0.0227,0.0273,-0.0157,0.0202,0.0548,-0.0064,-0.0519,0.0412,-0.046,0.1842,0.0295,0.0483,-0.0352,0.0595,0.0185,-0.0033,-0.0429,0.051,0.0626,0.1045,-0.0454,-0.0621,-0.0246,0.0078,0.0404,0.0005,-0.0523,-0.044,-0.0486,-0.0558,0.0314,-0.0316,0.0016,0.0159,-0.0607,0.04,-0.061,0.0116,-0.0503,-0.0844,0.0161,-0.0236,-0.0015,0.0019,-0.064,-0.0245,-0.0099,0.0356,0.0079,0.0105,-0.0277,0.021,-0.0275,0.0254,0.0584,-0.005,0.032,0.0557,0.0108,0.0729,0.0271,-0.021,-0.0153,0.0695,-0.0578,0.0523,0.0237,0.0471,0.0164,0.0576,-0.0139,-0.0103,-0.0171,-0.0168,0.0098,-0.0684,0.0287,0.0386,-0.0019,-0.2749,0.0066,0.0254,0.0045,-0.0237,-0.0349,0.0495,-0.0325,-0.0661,0.0142,-0.0225,0.0677,0.0513,-0.0001,0.0026,0.0139,0.0314,-0.0408,0.066,-0.079,0.0095,0.0164,0.2166,-0.0332,0.0128,0.0108,-0.0142,0.0377,0.0253,-0.0339,0.0007,-0.0095,0.0775,-0.0756,0.0343,0.0531,-0.0144,0.0233,-0.0004,-0.018,0.0035,-0.0031,-0.0231,0.0069,0.0822,-0.0457,-0.0346,-0.0227,0.0378,-0.0142,-0.0064,0.0498,0.0017,-0.0094,0.0271,0.0262,-0.0403,-0.0818,-0.0473,-0.0081,-0.0156,-0.081,-0.0446,-0.0275,-0.0167]}
{"key":"[Understanding Alternating Minimization for Matrix Completion] Alternating Minimization is a widely used and empirically successful heuristic for matrix completion and related low-rank optimization problems. Theoretical guarantees for Alternating Minimization have been hard to come by and are still poorly understood. This is in part because the heuristic is iterative and non-convex in nature. We give a new algorithm based on Alternating Minimization that provably recovers an unknown low-rank matrix from a random subsample of its entries under a standard incoherence assumption. Our results reduce the sample size requirements of the Alternating Minimization approach by at least a quartic factor in the rank and the condition number of the unknown matrix. These improvements apply even if the matrix is only close to low-rank in the Frobenius norm. Our algorithm runs in nearly linear time in the dimension of the matrix and, in a broad range of parameters, gives the strongest sample bounds among all subquadratic time algorithms that we are aware of. Underlying our work is a new robust convergence analysis of the well-known Power Method for computing the dominant singular vectors of a matrix. This viewpoint leads to a conceptually simple understanding of Alternating Minimization. In addition, we contribute a new technique for controlling the coherence of intermediate solutions arising in iterative algorithms based on a smoothed analysis of the QR factorization. These techniques may be of interest beyond their application here.","layer":6,"vector":[-0.0615,0.0121,-0.0041,-0.0036,-0.0083,0.0387,0.0117,0.0317,0.0451,-0.0203,0.0587,-0.053,0.016,0.0315,0.016,0.0178,0.0314,0.0805,-0.0083,-0.0056,-0.0024,-0.0367,-0.039,-0.0724,0.069,-0.0177,-0.0267,-0.0657,-0.0358,-0.2699,0.0236,-0.0158,0.0994,-0.0394,0.0239,-0.0568,-0.045,0.0504,-0.0546,-0.0072,0.0205,0.059,-0.0282,-0.0261,-0.039,-0.0585,-0.037,-0.0212,-0.0245,-0.041,0.0236,-0.0221,0.0039,0.0352,0.0632,0.0317,0.0598,0.0179,0.0163,0.0293,0.0216,0.0037,-0.1765,0.0732,0.0773,-0.0219,-0.0274,-0.0477,-0.0261,0.1007,-0.0424,0.0419,0.0519,0.0208,0.0413,-0.0137,0.0134,-0.0169,-0.0396,0.0544,0.0235,-0.0401,-0.0009,0.0309,-0.0399,-0.0156,0.0208,-0.0601,0.0022,0.0269,-0.0138,-0.0173,0.0037,0.0482,-0.0614,-0.0193,0.0507,0.0509,-0.0467,0.2135,-0.065,0.0328,0.0334,-0.0229,-0.0262,-0.0348,-0.0372,-0.0225,-0.0239,-0.0107,-0.0175,-0.0157,0.0601,-0.0643,0.0387,0.0061,0.0554,0.0538,-0.0284,0.0083,-0.0519,-0.0112,0.0638,-0.0379,0.0386,-0.0277,-0.0073,0.123,0.007,0.0431,0.0626,-0.0504,-0.0047,-0.0652,0.021,-0.0169,0.0179,0.0051,0.0374,0.0063,-0.0442,-0.0687,0.0169,-0.0791,-0.0333,0.1571,-0.0193,0.0149,-0.0257,-0.0542,0.0166,0.0203,-0.0354,0.0055,0.0314,0.0213,0.0516,0.0183,-0.0469,0.032,-0.0622,-0.0445,-0.0014,0.1311,-0.007,-0.0584,-0.0067,0.0106,0.0223,0.0013,0.0296,0.0214,-0.0108,0.0325,0.0362,0.0271,-0.069,0.0156,-0.0017,0.0276,0.0345,-0.0168,-0.0324,0.0212,0.0155,-0.0313,0.0469,-0.0197,0.0037,-0.0391,-0.085,-0.001,-0.0358,-0.0092,-0.0573,-0.0125,-0.0038,-0.0397,0.027,-0.0131,0.036,-0.0136,-0.02,0.0834,0.0313,0.011,0.0172,-0.0897,0.0332,0.0733,-0.0292,-0.0474,0.0666,-0.0348,-0.0283,0.0045,0.0436,0.0084,-0.0132,0.0538,0.0218,-0.0138,-0.073,-0.2134,-0.0055,-0.0398,0.0218,0.0461,-0.09,0.0321,-0.0167,0.045,0.0371,0.0296,0.0002,-0.0081,0.0612,-0.0205,0.0721,0.0228,0.0389,-0.0029,-0.0009,-0.001,0.0322,-0.0135,-0.0112,0.0246,-0.0147,0.2211,0.0392,0.0285,0.0268,0.0207,0.0329,-0.0402,-0.0273,0.0089,0.035,0.0783,-0.0126,-0.0241,-0.0425,-0.0001,0.0042,-0.0168,-0.0737,-0.0128,-0.0683,-0.0517,0.0016,-0.0513,0.0282,0.0719,-0.055,0.0364,-0.0367,0.0081,-0.0137,-0.0838,0.0351,-0.0304,0.0601,-0.0145,-0.066,-0.0018,-0.0548,0.0574,0.013,-0.0097,-0.0025,0.0017,-0.072,-0.0046,0.0518,0.0352,-0.0026,0.0468,-0.0143,0.0256,-0.0127,-0.033,0.0089,0.0659,-0.0636,0.0391,0.0075,0.0292,0.0137,0.067,0.0176,-0.0095,0.0269,0.0057,0.0066,-0.0268,0.024,0.038,-0.0277,-0.2718,-0.0225,0.023,-0.0383,-0.0311,-0.0269,0.0503,0.0013,-0.0465,0.038,-0.0511,0.0881,0.0246,-0.0494,0.0108,0.045,0.0756,-0.0559,0.0213,-0.0799,-0.0383,0.0063,0.2061,-0.1006,0.0025,0.0186,-0.0483,-0.0227,0.0062,-0.0566,0.0067,0.0222,0.0978,-0.0526,0.0637,0.0789,-0.0108,0.0876,-0.0126,0.03,-0.0459,-0.0125,-0.0198,-0.0242,0.0928,-0.0349,-0.007,-0.0187,0.0105,0.012,-0.006,0.0167,0.0321,0.0031,0.0185,0.0289,-0.0754,-0.0367,-0.0137,-0.0029,-0.0103,-0.0392,-0.0454,-0.0318,0.0115]}
{"key":"[The Global Anchor Method for Quantifying Linguistic Shifts and Domain Adaptation] Language is dynamic, constantly evolving and adapting with respect to time, domain or topic. The adaptability of language is an active research area, where researchers discover social, cultural and domain-specific changes in language using distributional tools such as word embeddings. In this paper, we introduce the global anchor method for detecting corpus-level language shifts. We show both theoretically and empirically that the global anchor method is equivalent to the alignment method, a widely-used method for comparing word embeddings, in terms of detecting corpus-level language shifts. Despite their equivalence in terms of detection abilities, we demonstrate that the global anchor method is superior in terms of applicability as it can compare embeddings of different dimensionalities. Furthermore, the global anchor method has implementation and parallelization advantages. We show that the global anchor method reveals fine structures in the evolution of language and domain adaptation. When combined with the graph Laplacian technique, the global anchor method recovers the evolution trajectory and domain clustering of disparate text corpora.","layer":1,"vector":[-0.0255,-0.0123,0.0045,0.0092,0.0419,0.0504,0.0019,0.0193,0.0099,-0.0337,0.0504,-0.0173,0.0233,0.007,0.0035,0.0167,-0.0204,0.0322,-0.0546,-0.0331,0.0335,-0.0439,0.0311,-0.0275,0.024,0.0533,-0.0221,-0.0069,-0.0356,-0.2444,-0.0133,-0.0389,0.0286,-0.0264,0.0061,-0.011,-0.0565,0.0271,0.0319,0.0552,-0.0073,0.0187,-0.0089,-0.0784,-0.0419,-0.047,-0.0276,0.0338,-0.0491,-0.0176,-0.018,-0.0754,0.0261,0.0172,0.0357,0.0698,0.0832,0.0551,0.0454,0.0568,0.0272,0.0351,-0.1765,0.1105,0.0312,0.0206,-0.0375,-0.0133,-0.0078,0.0593,-0.0104,0.0237,0.0297,0.0276,0.0471,0.0389,0.0214,-0.0129,-0.0071,-0.0136,0.0634,-0.0254,-0.0458,-0.0127,-0.0142,-0.0699,0.0287,-0.0019,0.0025,0.0201,-0.054,-0.0305,0.0187,0.0232,-0.0698,-0.0482,0.0074,0.0272,-0.0264,0.1959,-0.0294,0.0055,0.0079,-0.0452,0.0425,-0.0394,-0.0096,-0.0191,0.006,-0.0041,-0.0198,0.0133,0.0156,-0.0448,0.0204,-0.0342,0.0859,0.0252,-0.0234,0.0063,-0.0469,0.0303,0.0049,-0.05,0.0425,-0.0144,0.0689,0.1161,0.081,0.0119,0.0393,0.009,-0.0207,-0.0206,-0.0304,0.0513,-0.0076,-0.0287,0.0421,-0.0089,-0.0175,-0.0744,0.0074,-0.0603,-0.0654,0.1624,-0.0555,-0.0178,-0.0329,0.0378,-0.0304,0.0017,-0.0124,-0.0356,0.0174,0.007,0.0459,0.0178,-0.027,0.0063,-0.0083,-0.0351,-0.055,0.1213,0.0675,-0.1102,-0.0614,-0.0157,0.0148,-0.0485,0.089,0.0541,-0.0303,0.0319,0.0705,0.0033,-0.0402,0.0021,0.0489,-0.0093,0.0771,-0.0089,-0.0443,0.0924,0.0232,-0.0467,-0.0544,-0.0208,0.0706,0.0196,-0.0347,0.0254,-0.0196,-0.0095,-0.0234,-0.0025,-0.0088,-0.0176,0.0062,-0.0325,0.029,0.0456,-0.0521,-0.0466,-0.0527,-0.0238,0.0063,-0.0104,0.0439,0.0296,-0.0323,0.0093,0.0435,-0.0266,-0.0018,0.0017,0.0064,0.0066,0.0022,0.0495,0.0183,-0.0448,-0.0102,-0.2424,-0.0554,0.0165,-0.0253,0.0529,-0.0298,0.0149,-0.0565,0.116,0.0511,0.0131,-0.0451,-0.041,0.0469,0.0273,0.0513,0.0573,0.0545,0.0044,-0.0136,0.0219,-0.0114,-0.0221,-0.1061,0.0381,-0.0267,0.2193,0.0349,0.0295,-0.0515,0.0161,0.0044,-0.0147,-0.1146,0.0811,0.0499,0.0555,-0.026,-0.0424,-0.0048,0.0233,0.0251,0.0446,-0.0977,-0.0423,-0.0379,-0.0285,-0.0545,-0.0754,0.0384,0.0314,-0.0262,0.031,0.0302,-0.0654,-0.0374,-0.0499,0.0087,-0.0171,-0.0178,-0.0071,-0.0235,0.0283,-0.0484,0.0319,0.0038,-0.0263,-0.0178,0.0294,-0.033,-0.0614,0.058,-0.0056,-0.0121,0.0309,0.0328,0.0055,-0.0207,-0.0555,-0.0262,0.0984,-0.0134,0.0261,-0.0047,0.0158,0.0117,0.0893,-0.0163,0.0218,0.0185,-0.0367,0.0083,-0.0747,-0.0478,0.0211,-0.0384,-0.2803,0.0488,0.0461,0.0102,-0.0133,0.0365,0.0211,0.0484,-0.0386,0.019,0.0047,0.0394,0.019,-0.0253,-0.0197,0.0467,0.0649,-0.031,0.039,-0.017,0.0193,0.033,0.2189,-0.0128,0.0299,-0.0373,-0.0232,-0.004,0.0065,-0.0073,-0.0283,0.0173,0.0463,-0.023,0.0402,0.0715,-0.0099,0.0012,0.0584,-0.0101,-0.0088,0.0276,-0.0625,-0.031,0.1009,0.0002,0.0289,-0.0653,0.0076,0.0326,-0.053,-0.0211,0.0034,0.0297,0.016,0.0103,-0.0578,-0.0615,-0.0425,-0.0535,0.0027,-0.0682,-0.0477,0.0146,-0.0047]}
{"key":"[Learning outside the Black-Box: The pursuit of interpretable models] Machine Learning has proved its ability to produce accurate models but the deployment of these models outside the machine learning community has been hindered by the difficulties of interpreting these models. This paper proposes an algorithm that produces a continuous global interpretation of any given continuous black-box function. Our algorithm employs a variation of projection pursuit in which the ridge functions are chosen to be Meijer G-functions, rather than the usual polynomial splines. Because Meijer G-functions are differentiable in their parameters, we can tune the parameters of the representation by gradient descent; as a consequence, our algorithm is efficient. Using five familiar data sets from the UCI repository and two familiar machine learning algorithms, we demonstrate that our algorithm produces global interpretations that are both highly accurate and parsimonious (involve a small number of terms). Our interpretations permit easy understanding of the relative importance of features and feature interactions. Our interpretation algorithm represents a leap forward from the previous state of the art.","layer":1,"vector":[-0.0438,0.0405,0.0256,-0.0087,0.0752,-0.0165,0.0399,0.0439,0.0254,-0.0235,0.0356,-0.0493,0.0408,0.0586,0.0005,0.0103,-0.0099,0.0838,-0.0442,-0.0208,0.0337,-0.0104,0.002,-0.0872,0.0198,0.0149,-0.0365,0.0139,-0.034,-0.2531,0.0085,-0.0191,0.0413,-0.0284,0.0172,-0.0427,-0.0138,0.0563,-0.0366,0.0344,0.0205,-0.0089,-0.0236,-0.0181,-0.0023,-0.049,-0.0322,-0.0089,-0.0549,-0.0471,-0.0123,-0.0438,0.0108,0.0346,-0.0084,0.0251,0.079,0.0587,0.058,0.0212,0.0276,0.0216,-0.1693,0.0394,0.043,0.0606,-0.0471,-0.0477,0.0184,0.0577,0.0121,-0.0153,-0.0196,0.0364,-0.0425,-0.0268,0.0141,-0.014,-0.0104,0.017,0.0555,-0.01,-0.0726,-0.0087,-0.0376,-0.0286,0.0085,-0.0599,0.0366,0.0119,-0.0466,-0.0073,-0.0564,0.0414,-0.0798,-0.034,0.0462,-0.0134,-0.0394,0.2022,-0.0476,0.0236,0.0489,-0.0066,0.0147,-0.0319,-0.0651,-0.0195,-0.0406,-0.0096,-0.0259,-0.0285,0.0408,-0.0276,-0.0289,0.0204,0.0305,0.0411,0.0271,0.0008,-0.0064,0.0051,0.0521,-0.0309,0.0346,-0.008,0.0244,0.1447,0.0518,0.0345,0.0413,-0.0205,-0.0398,-0.0393,0.0467,0.0209,0.0473,0.0403,0.007,0.0004,-0.0297,-0.0504,0.0234,-0.0844,-0.0557,0.1071,-0.0705,0.0306,-0.0341,0.0047,-0.0148,0.011,-0.0247,-0.0111,0.0307,0.02,0.0277,0.0442,-0.0242,0.0381,-0.0389,-0.0372,-0.0596,0.0694,0.0377,-0.0585,-0.0218,-0.0232,0.0246,-0.0332,0.0603,0.0567,-0.0232,0.0224,0.0822,0.0137,-0.081,-0.0257,-0.02,-0.0144,0.0178,-0.0713,-0.0566,0.0582,0.021,-0.0391,0.0165,-0.0059,0.0073,0.0244,-0.033,0.0241,-0.0419,0.0088,-0.0154,-0.0236,0.0002,-0.0083,0.0037,-0.0521,0.0178,0.0127,-0.0397,0.0203,-0.0223,0.0219,0.0211,-0.0002,0.0265,0.0267,-0.0298,-0.0042,0.0614,-0.0205,0.0182,0.0521,0.0475,0.0027,-0.0018,0.0445,0.042,-0.0791,-0.0616,-0.2366,-0.011,0.0416,-0.0277,0.0184,-0.0786,-0.0051,-0.0461,0.0378,0.0872,0.0833,-0.0415,-0.0474,0.0331,-0.0093,0.0505,0.0404,0.0149,-0.0271,0.0431,-0.0366,0.0082,-0.0313,-0.0722,0.0121,0.0262,0.2284,0.0321,0.0217,-0.0342,0.0161,0.0227,0.0184,-0.088,0.0448,0.0141,0.0864,-0.0186,-0.0233,-0.0233,-0.0299,-0.0008,0.0033,-0.0551,-0.0292,-0.0305,-0.0239,0.025,-0.0549,0.0261,0.026,-0.0205,0.046,-0.0269,0.019,0.0249,-0.0649,0.0135,-0.0393,0.0626,-0.0394,-0.0612,-0.0218,-0.0714,0.0769,0.0015,-0.0097,-0.0459,0.0433,-0.0279,-0.0033,0.0486,0.0018,-0.0241,0.0575,0.0162,0.06,-0.0096,-0.0803,0.0115,0.0524,0.0075,0.0142,-0.0447,0.0358,0.0097,0.0849,-0.0496,0.0109,-0.0011,-0.0185,-0.0068,-0.0379,-0.052,0.052,0.0025,-0.2944,0.0243,0.0013,0.0291,-0.0309,0.0115,0.0544,-0.0173,-0.0795,0.0071,-0.0143,0.0366,0.049,0.0082,0.0605,0.042,0.0875,-0.0527,0.0846,-0.0381,0.05,0.0847,0.2244,-0.068,0.0397,0.0069,-0.0061,-0.0468,-0.0046,-0.0145,0.0447,0.0132,0.0758,-0.0325,0.0199,0.058,-0.0223,0.0685,0.0493,-0.0646,0.0116,-0.0144,-0.0628,-0.0585,0.1322,-0.0163,-0.0102,-0.0035,-0.0253,0.0025,-0.0054,0.0044,0.0117,0.0023,0.0043,0.0406,-0.0209,-0.0343,-0.0351,0.0092,-0.0145,-0.0837,-0.0452,-0.0176,-0.049]}
{"key":"[Deep Shading: Convolutional Neural Networks for Screen-Space Shading] In computer vision, convolutional neural networks (CNNs) have recently achieved new levels of performance for several inverse problems where RGB pixel appearance is mapped to attributes such as positions, normals or reflectance. In computer graphics, screen-space shading has recently increased the visual quality in interactive image synthesis, where per-pixel attributes such as positions, normals or reflectance of a virtual 3D scene are converted into RGB pixel appearance, enabling effects like ambient occlusion, indirect light, scattering, depth-of-field, motion blur, or anti-aliasing. In this paper we consider the diagonal problem: synthesizing appearance from given per-pixel attributes using a CNN. The resulting Deep Shading simulates various screen-space effects at competitive quality and speed while not being programmed by human experts but learned from example images.","layer":0,"vector":[-0.0482,-0.0186,0.0094,-0.0218,0.0388,0.0425,0.0071,0.0038,0.0009,0.0134,0.0153,-0.0836,0.0595,0.0494,0.0216,0.0296,0.0228,0.0603,-0.0429,-0.0085,0.028,-0.0335,-0.0069,-0.0329,0.0152,-0.0157,-0.0039,-0.06,-0.0508,-0.2317,-0.0001,0.0104,0.0737,-0.0107,-0.0083,-0.06,-0.038,0.071,-0.0488,0.0317,-0.001,-0.0163,-0.0446,-0.0304,0.001,-0.0347,-0.0223,-0.0402,-0.008,-0.0431,0.0383,-0.0395,0.0224,0.0003,0.025,0.0234,0.0998,0.0314,0.0666,0.0249,-0.0032,0.0454,-0.1695,0.0484,0.0824,0.0016,-0.0031,-0.0606,0.0353,0.0341,-0.0777,0.0512,0.0144,0.0397,0.0178,-0.0181,-0.0089,-0.0382,-0.0119,-0.0251,0.0091,-0.0187,-0.0543,0.0496,0.0162,-0.0353,0.0035,-0.0049,0.0122,0.0509,-0.0534,-0.0101,-0.0867,0.0132,-0.0912,-0.0027,0.0408,-0.0062,-0.0305,0.2252,-0.0203,0.0015,0.0579,-0.0304,0.0493,-0.0326,-0.0078,0.0041,-0.0496,0.0502,-0.031,-0.039,0.0187,0.009,0.013,0.0095,0.0098,0.0667,-0.002,-0.0694,-0.0313,0.0422,0.0359,-0.0363,0.0343,-0.0617,0.0192,0.1467,0.0217,0.0428,0.0469,-0.0451,-0.0207,-0.005,0.0177,0.0704,0.011,0.0171,0.0023,0.002,-0.0323,-0.038,0.0136,-0.0477,-0.0037,0.0758,-0.0504,0.0595,-0.0401,-0.0382,-0.0132,0.0487,-0.0432,0.0069,-0.007,-0.0186,-0.011,0.0701,-0.06,0.0454,0.0011,-0.0574,-0.0031,0.0984,-0.0003,-0.0757,-0.035,0.0131,0.0031,0.0085,0.0303,0.0137,-0.039,0.0304,0.109,0.0262,-0.1436,-0.0019,-0.008,0.0101,0.0514,-0.0635,-0.0416,0.0248,0.0785,-0.0623,0.019,-0.0572,-0.0048,0.0679,-0.0607,0.0153,-0.0969,0.0189,0.012,0.0193,-0.026,-0.0349,-0.0258,-0.016,0.0361,0.0004,-0.0602,-0.0234,-0.0014,-0.0087,0.0118,-0.0106,0.0197,0.0454,-0.0278,-0.0308,0.0579,-0.011,-0.0139,-0.0107,-0.0171,-0.0077,0.0064,0.0582,0.0289,-0.091,-0.0713,-0.2171,0.0346,0.0292,-0.0316,0.0324,-0.0595,0.0396,0.0281,0.0499,0.0383,0.0679,-0.0316,0.0277,-0.0163,0.005,0.0047,0.0173,0.0495,-0.0254,-0.0823,-0.0154,0.0444,-0.0054,-0.0947,0.0619,0.019,0.2347,0.0372,0.037,-0.0085,0.0708,0.0054,-0.0128,-0.1058,0.0471,0.0414,0.0974,0.0208,-0.0463,-0.03,0.0086,-0.0142,-0.0302,-0.0697,-0.0138,-0.0048,-0.0336,0.0659,-0.0386,-0.0024,-0.0133,-0.0541,0.0416,-0.0007,-0.0118,-0.0222,-0.1127,0.0391,-0.0532,-0.0079,0.0055,-0.0785,-0.0092,-0.0301,0.0525,0.0135,-0.0298,-0.0539,0.0258,0.0131,-0.0042,0.0637,0.0159,0.0077,0.0754,-0.0117,0.0545,0.0389,-0.0185,-0.0376,0.0671,-0.0179,0.0418,0.0399,0.0626,0.0081,0.0354,-0.0352,0.0499,-0.0347,-0.0016,0.0124,-0.0709,-0.0365,0.0215,-0.027,-0.2947,0.0401,-0.0322,0.0419,-0.0373,0.0106,0.01,0.051,-0.0347,-0.038,-0.0159,-0.0185,0.0408,-0.0215,0.0309,0.0118,0.0885,-0.046,0.0315,-0.0444,-0.006,-0.0029,0.1973,-0.0517,0.0166,0.0021,-0.0071,-0.0356,0.0207,0.0224,0.0349,0.0416,0.0997,-0.0335,0.0227,0.077,-0.0036,0.0309,-0.0202,0.0298,-0.0225,0.0045,-0.0072,-0.0245,0.0733,-0.0079,0.0024,0.0045,-0.0027,-0.0188,-0.0261,0.0252,-0.0143,-0.016,0.016,-0.0022,-0.0652,-0.0395,-0.0162,0.0448,0.0422,-0.0317,-0.0058,0.0067,0.0203]}
{"key":"[Mining Closed Episodes with Simultaneous Events] Sequential pattern discovery is a well-studied field in data mining. Episodes are sequential patterns describing events that often occur in the vicinity of each other. Episodes can impose restrictions to the order of the events, which makes them a versatile technique for describing complex patterns in the sequence. Most of the research on episodes deals with special cases such as serial, parallel, and injective episodes, while discovering general episodes is understudied. In this paper we extend the definition of an episode in order to be able to represent cases where events often occur simultaneously. We present an efficient and novel miner for discovering frequent and closed general episodes. Such a task presents unique challenges. Firstly, we cannot define closure based on frequency. We solve this by computing a more conservative closure that we use to reduce the search space and discover the closed episodes as a postprocessing step. Secondly, episodes are traditionally presented as directed acyclic graphs. We argue that this representation has drawbacks leading to redundancy in the output. We solve these drawbacks by defining a subset relationship in such a way that allows us to remove the redundant episodes. We demonstrate the efficiency of our algorithm and the need for using closed episodes empirically on synthetic and real-world datasets.","layer":3,"vector":[-0.0357,0.0024,0.02,-0.0238,0.0311,0.0052,0.0178,0.0205,0.06,-0.0423,-0.0033,-0.0212,0.0286,0.0483,-0.0008,0.0049,0.0045,0.0743,-0.0472,-0.0342,0.0563,-0.0199,-0.019,-0.0758,0.0144,0.0645,-0.0059,-0.0659,-0.0722,-0.2238,-0.0061,-0.0199,0.0544,-0.0339,0.0253,-0.0291,-0.0767,0.0822,-0.0632,0.0494,0.0165,0.0391,0.0145,-0.0668,-0.0419,-0.0319,-0.0014,0.0087,0.0046,-0.0436,-0.0205,-0.042,-0.0061,0.0169,0.0157,0.0231,0.0338,0.0125,0.0502,0.0182,0.0338,0.0235,-0.1389,0.0505,0.0434,0.0153,-0.0353,-0.0074,-0.003,0.0801,-0.0101,0.0162,0.0119,0.0444,0.027,0.0071,0.0067,-0.0277,-0.0297,0.0179,-0.0374,-0.0363,-0.02,-0.0158,-0.0026,-0.0503,0.0209,-0.0708,0.013,0.0353,-0.0409,0.0249,0.018,0.0533,-0.0728,-0.0063,0.0192,0.0213,-0.005,0.1985,-0.0945,0.0463,0.014,-0.011,0.0027,-0.0242,0.0185,-0.0527,-0.0103,-0.0511,0.0013,-0.0041,0.0459,-0.0885,0.0367,0.0324,0.0792,0.0435,-0.0315,0.0345,0.0153,0.0231,0.0772,-0.0383,-0.0006,-0.0379,0.051,0.115,0.0335,-0.0489,0.0401,-0.0013,-0.0337,0.0069,0.0139,0.0006,0.0357,-0.0033,0.0103,-0.0231,-0.0232,-0.0634,0.005,-0.0805,-0.0388,0.1263,-0.042,0.0137,-0.0548,-0.0337,-0.0524,0.0134,0.003,-0.0565,0.0196,0.0661,0.0708,0.0265,-0.0584,0.0396,-0.0437,-0.0128,-0.0263,0.119,0.0121,-0.1109,0.0152,-0.0018,0.0052,0.0132,0.0378,0.0806,0.0017,0.0094,0.0437,0.0225,-0.056,-0.0073,0.0062,-0.0142,0.0584,-0.0374,-0.0469,0.0969,0.0029,-0.0728,-0.0022,-0.0078,0.0453,0.0144,-0.0068,-0.0168,0.0006,-0.0174,-0.0276,-0.0556,0.0233,-0.0307,0.008,-0.0183,0.0135,-0.02,-0.0564,0.0663,-0.0384,0.0381,-0.0653,-0.0352,0.0248,0.0299,-0.0059,0.0238,-0.0082,-0.016,-0.0293,-0.0043,0.0097,0.0684,0.0151,0.0649,0.0319,-0.0467,-0.0414,-0.2722,-0.0584,0.044,-0.0067,0.0184,-0.0637,0.0256,-0.0385,0.0232,0.0446,0.0246,-0.0235,-0.0653,-0.0013,-0.0239,0.0734,0.0399,0.022,-0.0267,0.0397,-0.0025,-0.0031,0.0195,-0.1025,0.034,0.0077,0.2284,0.0449,-0.0107,-0.0498,-0.0032,-0.0082,-0.0178,-0.0384,0.0488,0.0091,0.0444,-0.0009,-0.024,-0.0391,-0.0873,0.0326,-0.0306,-0.0673,-0.0012,-0.0332,-0.0447,-0.0417,-0.0091,0.0348,0.0479,-0.0306,0.0446,0.0047,-0.0151,-0.0749,-0.0677,0.0052,-0.0131,0.0114,0.0012,-0.0565,0.0086,-0.0211,0.0579,-0.0042,-0.0237,-0.0084,-0.0228,-0.0205,-0.0518,0.0925,-0.0222,-0.0607,0.0505,0.0093,0.0119,-0.0191,-0.0332,-0.0039,0.0912,-0.0832,0.0378,0.0184,0.0709,0.0142,0.0981,-0.0029,0.0685,0.0013,0.0153,0.0307,-0.0216,-0.0131,0.0263,-0.002,-0.2756,0.0484,-0.0265,0.0444,0.0009,0.0504,0.0391,0.0512,-0.0157,-0.0057,0.0325,0.0279,0.0255,-0.0345,-0.0068,0.071,0.0637,-0.016,0.0214,-0.0499,0.024,0.0504,0.2155,-0.0148,0.0261,0.0327,-0.0067,-0.034,-0.0125,0.0259,0.0081,-0.0316,0.0701,-0.0529,0.0396,0.051,-0.0094,0.1058,0.0296,-0.0241,-0.0425,-0.0107,-0.0803,-0.0377,0.1176,-0.039,-0.0549,-0.0461,0.01,0.0626,-0.0416,-0.0102,-0.015,0.0518,-0.0103,0.0621,-0.0401,-0.0028,0.0054,-0.055,-0.0025,-0.0545,-0.003,-0.0144,-0.0071]}
{"key":"[Generalizing Hamiltonian Monte Carlo with Neural Networks] We present a general-purpose method to train Markov chain Monte Carlo kernels, parameterized by deep neural networks, that converge and mix quickly to their target distribution. Our method generalizes Hamiltonian Monte Carlo and is trained to maximize expected squared jumped distance, a proxy for mixing speed. We demonstrate large empirical gains on a collection of simple but challenging distributions, for instance achieving a 106x improvement in effective sample size in one case, and mixing when standard HMC makes no measurable progress in a second. Finally, we show quantitative and qualitative gains on a real-world task: latent-variable generative modeling. We release an open source TensorFlow implementation of the algorithm.","layer":2,"vector":[-0.0634,0.0191,0.0303,-0.0168,-0.0174,0.0426,0.0201,-0.0163,0.047,0.0084,0.0163,-0.0617,0.0152,0.0809,0.0128,0.027,-0.0232,0.0193,-0.0514,-0.0343,0.015,-0.0319,0.0072,-0.023,0.0291,0.0206,0.0128,-0.0288,-0.0409,-0.2537,0.0286,-0.0416,0.0458,-0.0514,-0.0002,-0.0215,-0.0505,0.0422,-0.0067,0.0305,-0.0042,0.012,-0.0343,-0.0353,0.0078,-0.0711,-0.0464,-0.0025,-0.0025,-0.0435,0.0005,-0.0483,0.0232,-0.0098,0.0626,0.0185,0.0636,0.0051,0.0676,0.081,-0.003,0.1006,-0.178,0.0615,0.0505,0.0497,-0.0513,0.0295,0.0027,0.0685,-0.0499,0.0484,0.0182,0.0261,0.007,-0.0073,0.0378,-0.0574,-0.0313,0.0247,0.0157,-0.0469,-0.0425,-0.0643,-0.0274,-0.0257,0.0104,-0.0212,0.0447,0.0121,-0.0487,-0.0056,-0.0094,0.0168,-0.033,-0.0012,0.0512,0.0046,0.007,0.2055,-0.0017,0.0465,0.0366,0.031,0.0192,-0.0246,-0.0812,-0.0259,0.0023,0.0038,0.0197,-0.0143,0.0288,-0.0896,0.011,0.0178,0.0695,0.0467,-0.0336,0.0275,-0.0174,-0.0108,0.0317,0.0155,-0.007,-0.059,-0.0502,0.1455,0.0398,0.0158,0.0629,0.0205,-0.0567,-0.0469,0.0308,0.0036,-0.0132,0.0143,0.0035,0.0239,-0.0337,-0.03,-0.0141,-0.0993,-0.042,0.1327,-0.0278,0.0405,-0.0576,-0.0173,-0.0073,0.014,-0.0269,-0.0685,0.0364,0.0624,0.0196,0.0252,-0.0565,0.0498,-0.0504,-0.0645,-0.0061,0.0966,0.0441,-0.066,0.002,0.0152,0.0173,-0.0109,0.0407,0.0456,-0.0258,0.0432,0.0589,-0.0275,-0.0998,0.0341,0.0437,0.0069,-0.0045,-0.0052,-0.0312,0.0702,0.0644,-0.064,0.0053,-0.035,0.0091,0.0321,-0.0174,0.0118,-0.0198,-0.033,-0.037,-0.0308,-0.0479,0.0171,0.023,0.0127,-0.0225,-0.0262,-0.0815,-0.0078,0.0031,0.0309,-0.0243,0.0154,0.0776,0.0307,-0.0641,-0.0308,0.0458,-0.0252,-0.0321,0.0235,-0.0239,0.0327,0.0111,0.0044,0.0264,-0.0579,-0.0557,-0.2219,0.0444,0.0256,0.0036,0.0815,-0.0858,0.0262,0.0002,0.0522,0.0512,0.041,-0.0156,0.0177,0.0174,-0.0223,0.0242,0.0151,0.0144,-0.0071,0.0261,0.0271,0.0173,-0.0638,-0.0882,0.0413,0.0042,0.2014,0.0204,0.0165,-0.0301,0.0031,0.0003,-0.0154,-0.0665,0.025,-0.018,0.1099,0.0015,-0.0639,-0.0303,-0.0258,0.042,-0.0014,-0.1129,0.0012,-0.0302,-0.0486,0.0272,-0.0404,0.0089,0.0494,-0.0108,0.052,-0.0392,-0.024,-0.0586,-0.0929,0.0116,-0.0226,0.0106,0.0296,-0.0556,0.0296,-0.0277,0.0469,-0.0302,0.0137,-0.0671,0.0202,-0.0266,0.0212,0.0592,-0.0034,0.0456,0.0694,-0.0202,0.0047,-0.0196,-0.0492,0.0068,0.0742,0.0028,0.0176,0.0429,0.0292,-0.0108,0.0658,0.0064,0.0233,-0.0174,-0.0168,0.0036,-0.0413,-0.0154,0.0349,-0.0087,-0.295,0.0612,0.001,0.0411,-0.0235,-0.0057,0.0638,0.0042,-0.0234,-0.0185,-0.0021,0.0575,0.0769,0.0044,-0.0072,0.0185,0.069,-0.0664,0.0285,-0.0551,0.0282,0.0182,0.2423,-0.0251,0.0105,0.013,0.0002,0.037,0.0789,-0.0519,-0.0238,0.0286,0.0526,-0.0616,0.0646,0.0782,-0.0272,0.0412,0.0192,-0.0306,-0.0039,0.003,-0.0088,-0.0601,0.076,-0.0533,-0.02,-0.0552,-0.0096,0.0172,-0.0307,0.0171,-0.0438,-0.0089,0.0076,0.0167,-0.0321,-0.0459,-0.0357,-0.0211,0.0236,-0.0718,-0.0186,-0.011,-0.0276]}
{"key":"[On Faster Convergence of Cyclic Block Coordinate Descent-type Methods for Strongly Convex Minimization] The cyclic block coordinate descent-type (CBCD-type) methods, which performs iterative updates for a few coordinates (a block) simultaneously throughout the procedure, have shown remarkable computational performance for solving strongly convex minimization problems. Typical applications include many popular statistical machine learning methods such as elastic-net regression, ridge penalized logistic regression, and sparse additive regression. Existing optimization literature has shown that for strongly convex minimization, the CBCD-type methods attain iteration complexity of $\\mathcal{O}(p\\log(1/\\epsilon))$, where $\\epsilon$ is a pre-specified accuracy of the objective value, and $p$ is the number of blocks. However, such iteration complexity explicitly depends on $p$, and therefore is at least $p$ times worse than the complexity $\\mathcal{O}(\\log(1/\\epsilon))$ of gradient descent (GD) methods. To bridge this theoretical gap, we propose an improved convergence analysis for the CBCD-type methods. In particular, we first show that for a family of quadratic minimization problems, the iteration complexity $\\mathcal{O}(\\log^2(p)\\cdot\\log(1/\\epsilon))$ of the CBCD-type methods matches that of the GD methods in term of dependency on $p$, up to a $\\log^2 p$ factor. Thus our complexity bounds are sharper than the existing bounds by at least a factor of $p/\\log^2(p)$. We also provide a lower bound to confirm that our improved complexity bounds are tight (up to a $\\log^2 (p)$ factor), under the assumption that the largest and smallest eigenvalues of the Hessian matrix do not scale with $p$. Finally, we generalize our analysis to other strongly convex minimization problems beyond quadratic ones.","layer":2,"vector":[-0.0408,0.0179,0.0298,-0.0097,0.0061,0.0573,-0.0043,0.0424,0.0479,-0.0339,0.0269,-0.036,0.0224,0.0588,0.0305,0.0451,0.0288,0.0506,-0.0232,0.0454,0.0503,-0.0489,-0.0207,-0.0753,0.0652,-0.0274,-0.0228,-0.0568,0.0114,-0.2869,0.0416,-0.0125,0.047,-0.0368,0.0297,-0.0259,-0.0099,0.0546,-0.0296,0.0345,0.024,0.0657,-0.0597,-0.0341,0.0277,-0.0704,-0.0325,0.0076,-0.033,-0.0236,-0.0017,-0.0447,0.0168,0.0289,0.007,0.0315,0.0094,0.0327,0.0278,0.0549,0.0283,0.0162,-0.1731,0.0508,0.0748,0.0001,-0.0194,-0.0623,-0.0179,0.0926,0.0132,0.0393,0.0274,0.0096,0.0006,0.015,-0.0279,-0.0203,-0.043,0.0042,0.0124,-0.0263,-0.0242,-0.0084,-0.0087,-0.0743,0.0277,-0.0497,0.0559,0.0098,-0.0268,-0.0527,-0.0242,-0.0064,-0.0788,-0.047,0.0321,0.0505,-0.0683,0.2088,-0.0409,0.089,-0.0078,-0.0147,0.0519,-0.0125,-0.0433,-0.009,-0.0237,-0.0296,-0.0233,0.0035,0.052,-0.0216,-0.025,0.0059,0.0532,0.0676,-0.0084,0.0306,-0.0168,-0.0058,0.0512,0.0157,0.0377,-0.0671,-0.0026,0.079,0.0828,0.0615,0.0255,-0.0221,-0.056,-0.0293,-0.0108,0.0205,-0.0044,0.0091,0.0233,-0.0225,-0.0378,-0.0169,0.0616,-0.0946,-0.0383,0.1324,-0.0701,0.0248,-0.0564,-0.0628,0.0084,0.0266,-0.0359,-0.0097,0.0023,0.0266,0.0444,0.0274,-0.0683,0.016,-0.0216,-0.0422,0.0015,0.1204,-0.0229,-0.0802,0.0,-0.0048,0.0098,-0.018,0.064,0.0458,-0.0147,-0.0021,0.0763,0.039,-0.1213,-0.0125,-0.0049,0.0263,0.0299,-0.0288,-0.0325,0.0368,0.0745,-0.0368,0.0123,-0.0441,0.0067,-0.011,-0.063,-0.0401,-0.0496,0.0097,-0.0172,-0.0038,-0.0108,0.0042,0.0268,-0.0002,0.029,-0.0133,-0.0545,0.0719,0.0229,0.0038,0.0118,0.0042,0.0163,0.0613,-0.0153,-0.0328,0.0837,-0.0306,-0.0158,0.0349,0.0318,0.038,-0.0483,0.0607,0.0384,-0.0035,-0.0828,-0.1969,-0.0356,0.0251,0.0004,0.0233,-0.0785,0.038,-0.0229,0.0307,0.0673,0.0241,-0.0172,-0.0511,0.0438,0.0104,0.0462,0.0435,0.0025,-0.0135,0.0039,0.017,-0.003,-0.0369,-0.0631,0.048,-0.0394,0.1954,0.0114,0.0798,0.0278,0.003,0.0247,-0.0128,-0.0663,0.0335,0.0111,0.0863,-0.002,-0.0117,-0.0245,-0.0228,0.03,0.027,-0.0692,-0.0645,0.0008,-0.0494,0.0319,-0.063,-0.0051,0.0751,-0.0189,0.0422,-0.0495,0.0107,-0.0233,-0.0934,0.0559,-0.034,0.0159,-0.0431,-0.0932,-0.0128,-0.0246,0.0297,0.0127,-0.0232,-0.0242,-0.0078,-0.0478,0.012,0.0765,-0.0261,0.0039,0.048,0.0037,0.0276,0.0344,-0.0192,-0.0535,0.0485,-0.0337,0.0389,0.011,0.0047,0.0082,0.0815,-0.0021,0.0056,-0.005,-0.0291,0.0096,-0.0664,-0.0138,0.0282,-0.0032,-0.2924,-0.0137,-0.0119,-0.0158,-0.0463,0.0281,0.0492,0.0183,-0.0618,0.0266,0.0118,0.0837,0.0302,-0.0146,0.0406,0.0288,0.0363,-0.0311,0.0338,-0.1024,-0.0025,0.0335,0.2043,-0.0698,0.0205,0.0015,-0.0219,0.0073,-0.0011,-0.0115,0.0077,-0.0232,0.0736,-0.0274,0.0555,0.0787,-0.0045,0.0224,0.0012,0.0024,-0.001,0.0072,-0.0323,-0.0258,0.1114,-0.0343,-0.059,-0.0369,0.0271,-0.0057,-0.0601,0.0137,0.04,0.013,0.0219,0.0442,-0.0592,-0.0248,-0.0595,-0.044,0.0215,-0.0418,-0.0877,0.0003,-0.0013]}
{"key":"[Use Dimensionality Reduction and SVM Methods to Increase the Penetration Rate of Computer Networks] In the world today computer networks have a very important position and most of the urban and national infrastructure as well as organizations are managed by computer networks, therefore, the security of these systems against the planned attacks is of great importance. Therefore, researchers have been trying to find these vulnerabilities so that after identifying ways to penetrate the system, they will provide system protection through preventive or countermeasures. SVM is one of the major algorithms for intrusion detection. In this research, we studied a variety of malware and methods of intrusion detection, provide an efficient method for detecting attacks and utilizing dimension reduction.Thus, we will be able to detect attacks by carefully combining these two algorithms and pre-processes that are performed before the two on the input data. The main question raised is how we can identify attacks on computer networks with the above-mentioned method. In anomalies diagnostic method, by identifying behavior as a normal behavior for the user, the host, or the whole system, any deviation from this behavior is considered as an abnormal behavior, which can be a potential occurrence of an attack. The network intrusion detection system is used by anomaly detection method that uses the SVM algorithm for classification and SVD to reduce the size. Steps of the proposed method include pre-processing of the data set, feature selection, support vector machine, and evaluation.The NSL-KDD data set has been used to teach and test the proposed model. In this study, we inferred the intrusion detection using the SVM algorithm for classification and SVD for diminishing dimensions with no classification algorithm.Also the KNN algorithm has been compared in situations with and without diminishing dimensions,the results have shown that the proposed method has a better performance than comparable methods.","layer":0,"vector":[-0.0442,-0.0094,0.0521,-0.0172,0.0382,0.0066,0.049,0.0493,0.0245,-0.0424,0.0398,-0.0077,0.0456,0.0208,0.0258,-0.0207,0.0239,-0.0097,-0.0231,0.0212,0.0149,-0.0258,-0.0106,-0.0555,0.0303,0.0451,0.0089,-0.0524,-0.0666,-0.1662,0.0228,-0.0678,0.0896,-0.0267,-0.0306,-0.0236,-0.0287,0.0555,-0.0295,0.0124,0.0091,-0.0028,0.0323,-0.0413,-0.0249,-0.0549,-0.0322,-0.0349,0.0348,-0.0417,0.0183,-0.0512,-0.0039,0.0444,0.0302,-0.0168,0.0331,0.003,0.0703,0.0277,0.024,0.0512,-0.176,0.0462,0.0705,0.0183,-0.0409,-0.0214,0.0301,0.0444,-0.0393,0.0186,0.0051,0.0477,0.0243,-0.0205,-0.0281,-0.001,0.0488,0.0179,-0.0122,-0.0097,0.019,-0.0332,-0.0319,-0.0212,-0.0205,-0.0733,0.0706,-0.0238,-0.0104,0.0098,-0.0219,-0.0203,-0.0527,-0.0226,0.0466,-0.0228,-0.0741,0.1822,-0.0611,0.0002,-0.0021,-0.0311,0.026,-0.018,-0.0356,-0.0801,0.0266,-0.0249,-0.0004,-0.0177,0.0155,-0.0333,0.0194,-0.0107,0.04,0.0231,-0.035,0.0195,-0.0066,0.0065,0.0687,-0.037,0.0403,-0.1107,0.0015,0.1239,0.0152,0.0162,0.0064,-0.0065,-0.0701,-0.0351,0.0388,0.0514,0.0055,0.0149,-0.0373,-0.047,-0.0418,-0.0567,0.0517,-0.0916,-0.0202,0.0816,-0.0315,0.008,-0.0187,-0.036,-0.0081,0.0076,-0.0277,-0.0451,-0.0383,0.0024,0.0302,0.0619,-0.0662,0.0471,-0.0276,-0.0459,-0.0537,0.1492,0.0154,-0.1097,-0.0605,-0.0112,-0.0136,-0.0138,-0.0228,0.0627,-0.058,0.0265,0.0316,-0.0001,-0.0504,-0.0005,-0.0118,0.0033,0.0804,-0.0107,-0.0444,0.0292,0.0568,-0.0581,-0.0023,0.0134,0.0371,0.0267,-0.0586,0.0254,-0.0249,-0.0699,-0.0111,-0.0096,0.0095,-0.0239,0.0278,-0.0348,0.0499,0.029,-0.0518,0.0324,-0.036,0.0803,0.008,-0.0135,0.0129,0.0114,-0.0082,-0.0036,0.0242,-0.0384,-0.0238,0.015,0.0177,0.0789,0.0082,0.0382,0.0577,-0.0205,-0.0715,-0.2463,-0.003,-0.0013,-0.0202,0.0078,-0.0679,0.0469,-0.0332,0.0491,0.0371,0.0646,0.0399,-0.0245,-0.0164,0.0193,0.097,0.0426,0.0469,-0.0725,0.0103,-0.0308,0.0126,0.0237,-0.0561,0.0276,0.0252,0.1488,0.0243,0.0301,-0.0695,0.0107,0.03,-0.0341,-0.1259,0.0783,0.0302,0.026,0.0172,-0.0269,-0.0419,-0.0358,0.0093,-0.0096,-0.0607,-0.0049,-0.0418,0.0094,0.0149,-0.0388,0.0152,0.0507,0.05,0.0585,0.0192,0.042,-0.0628,-0.0716,0.0309,0.0023,0.0225,0.0206,-0.1027,0.0065,-0.1229,0.0678,0.0371,-0.0553,0.0043,0.0206,-0.0193,-0.0722,0.1163,0.0538,-0.0299,0.0592,0.012,-0.0064,-0.0047,-0.0313,-0.0141,0.0465,-0.0505,0.0518,0.0282,0.0161,-0.0139,0.0948,0.061,0.0002,-0.0299,0.0408,-0.0092,-0.009,-0.0161,0.0131,0.0004,-0.2763,0.0501,-0.0236,0.0308,-0.0392,-0.0036,0.0565,0.019,-0.031,-0.0151,0.0475,0.0006,0.0172,-0.0126,0.0232,0.0309,0.0601,-0.0635,0.025,-0.0445,0.0334,0.0518,0.2546,-0.0547,0.0156,0.0172,0.0775,0.0313,-0.0072,-0.0419,0.0326,-0.0279,0.1083,-0.0319,0.0454,0.0544,-0.0066,0.0494,-0.0036,-0.0195,-0.02,-0.0108,-0.0687,0.0046,0.1179,-0.0218,-0.0095,-0.0661,0.0661,0.0367,-0.0285,-0.0002,0.0327,0.0497,0.0247,0.0591,-0.0735,-0.0124,-0.0571,-0.0358,0.0298,-0.0881,-0.0223,0.0062,-0.0112]}
{"key":"[Black-Box Reductions for Parameter-free Online Learning in Banach Spaces] We introduce several new black-box reductions that significantly improve the design of adaptive and parameter-free online learning algorithms by simplifying analysis, improving regret guarantees, and sometimes even improving runtime. We reduce parameter-free online learning to online exp-concave optimization, we reduce optimization in a Banach space to one-dimensional optimization, and we reduce optimization over a constrained domain to unconstrained optimization. All of our reductions run as fast as online gradient descent. We use our new techniques to improve upon the previously best regret bounds for parameter-free learning, and do so for arbitrary norms.","layer":1,"vector":[-0.0452,-0.0108,0.0437,0.0408,0.0192,0.0375,-0.0145,0.0675,0.0259,-0.0338,0.0275,-0.0116,0.0274,0.055,0.027,0.0146,0.0162,0.0524,-0.0638,0.0107,0.0437,-0.053,-0.0065,-0.047,0.0535,-0.0188,-0.0376,-0.0709,-0.0373,-0.2161,0.021,-0.0408,0.0513,0.0019,0.0152,-0.0241,-0.0047,0.0774,-0.0536,0.0289,0.0151,0.0394,0.0047,-0.0125,-0.0226,-0.0339,0.0067,-0.0148,-0.0009,-0.0407,0.0143,-0.0443,0.0177,0.0515,0.0177,0.0118,0.0475,0.0681,-0.0095,0.0316,0.0082,0.0481,-0.1558,0.0679,0.0166,0.022,-0.0461,0.0043,0.0117,0.071,-0.0115,0.0527,0.0118,0.0421,0.0177,-0.0168,-0.0195,-0.0038,0.0307,0.0066,0.013,-0.036,-0.0299,0.0127,-0.0396,-0.0755,-0.0097,-0.0422,0.0516,0.032,-0.0502,-0.0382,0.0032,0.0165,-0.0685,-0.0227,0.0016,0.056,-0.0486,0.1929,-0.036,0.0493,-0.0008,-0.0627,0.001,-0.062,-0.0022,-0.0224,0.0071,-0.0216,-0.0368,-0.0278,0.0431,-0.0118,-0.0181,0.0522,0.0309,0.0249,0.0247,-0.0122,-0.041,0.0102,0.0925,-0.0247,0.0463,-0.0819,-0.0098,0.1501,0.0249,0.0533,0.0276,-0.0578,-0.0364,-0.0668,0.0057,0.0034,0.0387,0.0179,0.0105,-0.0106,-0.0604,-0.0584,-0.0205,-0.0828,-0.0221,0.1214,-0.0072,0.0394,-0.0418,-0.0374,0.0045,-0.0072,-0.0388,0.0102,0.0224,0.0296,0.0307,0.0361,-0.0657,0.0101,-0.0607,-0.0588,0.0259,0.089,-0.0157,-0.0815,-0.0544,-0.0204,0.0155,0.0315,0.0299,0.0046,-0.0493,0.024,0.0942,0.0374,-0.1078,-0.033,0.0094,0.0283,-0.0057,-0.0479,-0.0144,0.0327,0.05,-0.0146,0.034,-0.03,0.0425,0.0225,-0.0334,-0.0083,-0.0612,-0.0214,-0.0628,-0.0249,0.0036,-0.0345,0.0289,-0.0237,0.0278,-0.0132,-0.06,0.0249,0.0151,0.0254,0.0274,0.0159,0.0747,0.0204,-0.0686,-0.0014,0.0082,-0.012,0.0153,0.016,0.0541,0.0471,-0.0009,0.0412,0.0229,-0.0075,-0.0258,-0.2102,-0.0239,-0.0093,-0.0413,0.0668,-0.0671,0.0616,-0.014,0.0475,0.0944,0.0328,-0.0568,-0.0115,0.0291,-0.0273,0.057,0.0818,-0.0025,-0.026,0.0143,-0.0445,0.0401,-0.0038,-0.0999,0.0585,-0.0188,0.2018,-0.0133,0.0617,-0.0258,0.0339,0.019,0.0592,-0.1112,0.029,0.051,0.0423,-0.007,-0.0181,-0.018,-0.0045,-0.0044,0.0103,-0.132,-0.0475,-0.0344,-0.0072,0.0299,-0.0786,0.0089,0.0539,-0.0122,0.0526,-0.0528,0.0131,-0.0314,-0.0821,0.0355,-0.0256,0.0316,0.0036,-0.0234,-0.0204,-0.0747,0.0665,0.0176,-0.0352,-0.0467,0.0202,-0.0326,-0.0226,0.0508,0.0311,-0.0029,0.0223,-0.0092,0.0228,-0.0141,-0.0583,0.0116,0.0641,-0.0217,0.0209,0.001,-0.0037,0.0344,0.1113,-0.0177,0.0002,0.0109,-0.0239,0.0008,-0.1032,-0.0019,0.0428,0.0205,-0.3208,0.0405,0.0149,0.0149,-0.0208,0.0205,0.0388,-0.0132,-0.0318,-0.0222,-0.0198,0.0318,0.0131,0.0228,0.0429,0.0503,0.0691,-0.0602,0.0563,-0.0565,0.0026,0.0341,0.2105,-0.0684,0.0648,0.0164,-0.0145,-0.0304,0.0526,-0.0327,-0.0005,-0.0135,0.0854,-0.0659,0.0639,0.0633,-0.0584,0.0519,0.0237,-0.0356,-0.038,-0.0026,-0.0466,-0.009,0.0719,0.0068,-0.0452,-0.0647,-0.0117,0.021,0.0056,0.0355,0.0432,0.0198,-0.0089,0.0248,-0.018,-0.0705,-0.032,-0.0639,0.0355,-0.0444,-0.0301,-0.0235,0.011]}
{"key":"[Context R-CNN: Long Term Temporal Context for Per-Camera Object Detection] In static monitoring cameras, useful contextual information can stretch far beyond the few seconds typical video understanding models might see: subjects may exhibit similar behavior over multiple days, and background objects remain static. Due to power and storage constraints, sampling frequencies are low, often no faster than one frame per second, and sometimes are irregular due to the use of a motion trigger. In order to perform well in this setting, models must be robust to irregular sampling rates. In this paper we propose a method that leverages temporal context from the unlabeled frames of a novel camera to improve performance at that camera. Specifically, we propose an attention-based approach that allows our model, Context R-CNN, to index into a long term memory bank constructed on a per-camera basis and aggregate contextual features from other frames to boost object detection performance on the current frame. We apply Context R-CNN to two settings: (1) species detection using camera traps, and (2) vehicle detection in traffic cameras, showing in both settings that Context R-CNN leads to performance gains over strong baselines. Moreover, we show that increasing the contextual time horizon leads to improved results. When applied to camera trap data from the Snapshot Serengeti dataset, Context R-CNN with context from up to a month of images outperforms a single-frame baseline by 17.9% mAP, and outperforms S3D (a 3d convolution based baseline) by 11.2% mAP.","layer":0,"vector":[-0.0163,0.0259,0.0338,-0.0126,0.0782,0.0254,0.0609,-0.0064,0.0259,0.0024,-0.0016,-0.0591,0.0312,0.0799,0.0398,0.0033,0.0205,0.0268,-0.0257,-0.0228,-0.0124,-0.0409,0.0031,-0.037,0.0171,0.0188,-0.0278,-0.0176,-0.0573,-0.2011,-0.0266,-0.0415,0.0248,-0.0078,0.0091,-0.0028,-0.0402,0.0225,-0.0136,0.026,0.0178,0.0412,-0.0538,-0.0941,-0.0748,-0.0459,0.0132,-0.0082,-0.027,-0.0946,0.0207,-0.0506,0.099,0.0154,-0.0327,0.0263,0.0436,0.0439,0.075,-0.0066,0.0407,0.0613,-0.1766,0.0374,0.0354,0.02,-0.0441,-0.0001,0.037,0.0359,-0.0528,0.0319,-0.0048,0.0312,0.0122,-0.0107,-0.0158,-0.0615,-0.0149,-0.0146,0.0361,-0.0187,-0.0183,-0.0347,0.0331,-0.0664,-0.0235,-0.0635,0.0405,0.0059,-0.051,-0.0043,0.0006,0.0216,-0.0657,-0.0224,0.0219,0.0109,-0.0448,0.1852,-0.0346,0.0324,0.0367,-0.0149,0.0455,-0.0424,-0.0164,-0.0132,-0.045,0.0336,0.0065,-0.054,0.012,-0.0111,0.0471,-0.0086,0.094,0.0768,-0.0209,0.0073,-0.0225,-0.0015,0.0534,-0.0674,0.0349,-0.068,0.0356,0.1407,0.0433,0.0272,0.029,-0.0092,-0.1029,-0.0274,-0.0018,0.0216,0.0542,0.0453,0.0222,-0.0761,-0.073,-0.0324,0.0353,-0.0683,-0.0157,0.0942,-0.0254,0.0585,-0.0589,-0.061,0.0133,0.0614,-0.0146,-0.0489,0.0347,0.048,0.0369,0.0467,-0.0598,-0.0022,-0.049,-0.0393,-0.0332,0.0616,0.0258,-0.0969,-0.0141,-0.0306,-0.0148,0.0156,0.0339,0.0348,0.0124,0.0304,0.0837,0.0523,-0.1067,-0.0001,-0.0231,-0.0041,0.0153,-0.0904,-0.054,-0.0089,0.0322,-0.0533,0.0309,-0.0648,0.024,0.0791,-0.0366,-0.0012,-0.0093,0.0022,-0.0235,0.013,0.0136,0.002,-0.0076,-0.0098,0.0215,-0.0215,0.0171,-0.005,-0.0023,-0.0059,-0.0478,0.0097,0.0022,0.0224,-0.0178,0.0208,0.0347,-0.0172,-0.0006,-0.0389,0.0342,0.01,0.0185,0.04,0.0322,-0.0529,-0.0296,-0.2197,0.0213,-0.0165,-0.0187,0.0395,-0.064,0.0306,0.0171,0.0613,0.0422,0.0385,-0.0439,-0.0011,0.0146,0.0408,0.103,0.0057,0.0199,-0.0519,0.0062,-0.0037,0.0144,0.0002,-0.0686,0.0689,-0.0148,0.2317,0.0249,0.0092,-0.0401,0.0288,0.0031,-0.0643,-0.0952,0.0179,0.0032,0.0439,0.0313,-0.0496,-0.0465,-0.0449,0.0077,-0.0157,-0.0707,-0.0819,-0.0198,-0.0232,0.0614,-0.0009,-0.0049,0.0427,-0.0887,0.0176,-0.012,-0.0102,-0.0389,-0.0547,0.0004,-0.0311,0.0292,0.0192,-0.0383,0.0297,-0.0412,0.068,-0.0271,-0.0439,-0.0613,-0.0137,0.0049,-0.0132,0.1144,0.0031,-0.0367,0.0826,-0.0287,0.0691,-0.0446,-0.0492,-0.0137,0.0858,-0.0158,0.0095,0.0366,0.0923,0.0347,0.0709,0.0006,0.0026,0.0017,0.0675,0.0062,-0.04,-0.0389,0.0324,-0.0001,-0.3175,0.0459,0.0198,0.0245,-0.0209,0.0528,0.0653,0.0139,-0.0121,-0.01,-0.0241,0.0486,0.0522,-0.0014,0.0041,0.031,0.0604,-0.0182,0.0361,-0.0676,-0.0084,0.0158,0.2009,-0.0205,-0.0035,-0.0003,-0.036,-0.0228,0.07,-0.003,0.0046,0.0144,0.0729,-0.07,-0.0252,0.0721,-0.0191,0.055,0.0209,0.0271,-0.0002,0.0213,0.0092,-0.0518,0.0605,0.0187,-0.0061,-0.0302,-0.0253,0.0463,-0.0149,-0.0366,-0.0364,0.007,0.0742,0.0553,-0.0288,-0.0504,0.0021,-0.0121,0.0181,-0.0761,-0.0144,0.0042,-0.0114]}
{"key":"[Transfer Learning for Video Recognition with Scarce Training Data for Deep Convolutional Neural Network] Unconstrained video recognition and Deep Convolution Network (DCN) are two active topics in computer vision recently. In this work, we apply DCNs as frame-based recognizers for video recognition. Our preliminary studies, however, show that video corpora with complete ground truth are usually not large and diverse enough to learn a robust model. The networks trained directly on the video data set suffer from significant overfitting and have poor recognition rate on the test set. The same lack-of-training-sample problem limits the usage of deep models on a wide range of computer vision problems where obtaining training data are difficult. To overcome the problem, we perform transfer learning from images to videos to utilize the knowledge in the weakly labeled image corpus for video recognition. The image corpus help to learn important visual patterns for natural images, while these patterns are ignored by models trained only on the video corpus. Therefore, the resultant networks have better generalizability and better recognition rate. We show that by means of transfer learning from image to video, we can learn a frame-based recognizer with only 4k videos. Because the image corpus is weakly labeled, the entire learning process requires only 4k annotated instances, which is far less than the million scale image data sets required by previous works. The same approach may be applied to other visual recognition tasks where only scarce training data is available, and it improves the applicability of DCNs in various computer vision problems. Our experiments also reveal the correlation between meta-parameters and the performance of DCNs, given the properties of the target problem and data. These results lead to a heuristic for meta-parameter selection for future researches, which does not rely on the time consuming meta-parameter search.","layer":0,"vector":[0.0073,-0.0189,-0.0449,-0.0226,0.0411,0.0672,0.0406,-0.0195,-0.0004,-0.0201,0.0378,-0.0865,0.0358,0.082,0.0361,0.0057,0.0589,0.0554,-0.078,0.0204,0.0211,-0.0304,0.0071,-0.0276,0.0209,-0.0134,-0.0042,-0.0183,-0.066,-0.2508,-0.0216,-0.0204,0.065,0.0313,0.029,-0.0314,-0.0381,0.0536,-0.0669,0.0276,0.0131,0.0153,0.0045,-0.0679,-0.0462,-0.0566,0.0163,-0.0687,0.002,-0.0599,0.0069,-0.007,0.036,0.0226,0.0072,0.0519,0.0397,0.0256,0.0375,0.0211,0.0356,0.0482,-0.1882,0.0474,-0.015,0.0415,-0.0563,0.0087,0.0262,0.0292,-0.0383,0.0454,-0.0043,0.0202,-0.0095,-0.0278,0.0163,-0.0373,-0.0009,-0.0351,0.0193,-0.0096,-0.0177,-0.0533,-0.0101,0.0056,-0.0038,-0.0423,-0.0051,0.0355,-0.0567,0.0162,-0.0644,0.0071,-0.059,-0.0163,0.0605,0.0499,-0.037,0.1932,-0.0521,0.0542,0.0332,-0.0499,0.0648,-0.0076,-0.0722,0.0077,-0.0441,0.0042,-0.0151,0.0281,0.0132,-0.0243,0.0311,-0.0392,0.0203,0.0529,-0.0402,-0.0085,0.0137,-0.0221,0.0418,-0.0873,0.0071,-0.0458,-0.0083,0.1341,0.0652,0.0153,0.0584,-0.0177,-0.0331,-0.033,0.0165,0.0652,0.0389,0.0013,-0.0271,-0.0167,-0.0349,-0.0625,0.0246,-0.0358,-0.0316,0.0908,-0.0322,0.0113,-0.0097,-0.0291,0.0385,0.0179,-0.0534,0.0014,0.0232,-0.0314,0.0443,0.0476,-0.052,0.0341,-0.0196,-0.0427,0.0042,0.0824,-0.0044,-0.1076,-0.0804,-0.0135,-0.0111,-0.0026,0.0332,0.0428,-0.0044,0.0279,0.1194,0.0622,-0.1037,-0.0293,-0.012,0.0166,-0.0102,-0.0934,-0.0072,0.0681,0.0919,-0.0068,-0.0048,-0.0751,-0.0017,0.0446,-0.0628,0.0502,-0.0181,0.0227,-0.0318,0.0294,-0.0167,-0.0178,-0.013,-0.0185,0.003,0.017,0.006,0.0329,-0.0163,-0.0259,-0.0263,0.0107,0.0228,0.0021,-0.0065,0.0195,0.0331,-0.0197,-0.044,-0.02,0.0468,0.0437,0.0305,0.023,0.027,-0.0501,-0.0278,-0.2469,-0.0147,0.0284,0.0053,0.0822,-0.0495,0.0393,0.0176,0.0683,0.0486,0.0344,-0.0061,-0.0385,-0.0147,0.0071,0.0683,0.0548,0.0319,-0.025,-0.0117,-0.0425,0.0158,0.0122,-0.0964,0.0465,0.0236,0.2105,0.0049,0.055,-0.0288,0.0602,0.05,-0.0566,-0.0798,0.0423,-0.0154,0.0581,0.0144,0.0064,-0.0229,-0.0239,-0.0245,0.042,-0.1246,-0.032,-0.0027,-0.0313,0.0169,-0.0555,0.0283,0.0291,-0.0237,0.0187,-0.0309,-0.0017,0.0201,-0.0674,0.052,-0.0177,0.0247,-0.0156,-0.0384,0.0361,-0.0711,0.0617,0.0167,-0.0384,-0.0617,0.0022,-0.0044,0.0059,0.1163,0.022,0.0282,0.0704,0.0203,0.0174,-0.0217,-0.061,-0.0475,0.0614,0.0123,0.0125,0.0091,0.0815,0.0557,0.0661,-0.032,0.0176,-0.0392,0.0496,-0.0234,-0.0796,-0.0051,0.0682,0.0326,-0.3106,0.0572,0.026,0.0438,0.0162,0.0115,0.0383,-0.0192,-0.0281,-0.0043,-0.0086,-0.0039,0.0619,-0.0646,-0.0042,0.0343,0.0378,-0.0423,0.0371,-0.0607,-0.006,0.0072,0.1684,-0.0749,0.0217,-0.0314,-0.0305,-0.0064,0.019,-0.0492,-0.0013,0.0312,0.0625,-0.046,0.0153,0.0813,-0.0327,0.0383,0.034,0.0345,0.0177,-0.0058,-0.0221,-0.0311,0.0501,-0.0034,0.0101,-0.0241,-0.0496,0.0262,-0.0298,-0.0303,0.0155,-0.0165,0.0217,0.0015,-0.0197,-0.0378,-0.0511,-0.0163,0.0237,-0.0539,0.0088,-0.0137,-0.015]}
{"key":"[Large-scale probabilistic predictors with and without guarantees of validity] This paper studies theoretically and empirically a method of turning machine-learning algorithms into probabilistic predictors that automatically enjoys a property of validity (perfect calibration) and is computationally efficient. The price to pay for perfect calibration is that these probabilistic predictors produce imprecise (in practice, almost precise for large data sets) probabilities. When these imprecise probabilities are merged into precise probabilities, the resulting predictors, while losing the theoretical property of perfect calibration, are consistently more accurate than the existing methods in empirical studies.","layer":2,"vector":[-0.0429,0.0099,0.0304,-0.0303,0.0217,0.038,0.0484,0.0515,0.0108,0.0087,0.0162,-0.0916,-0.0187,0.0498,0.0061,0.0409,-0.0052,0.0464,-0.0122,0.0319,0.0547,0.0086,-0.0021,-0.0589,0.046,0.0322,-0.0166,-0.0419,-0.0346,-0.2482,0.005,-0.0707,0.0376,-0.0377,-0.0076,-0.0491,-0.0174,0.0339,-0.0084,0.0307,0.0314,0.0072,-0.0356,-0.0331,-0.0024,-0.0509,0.0041,-0.0098,-0.033,-0.0183,0.0173,-0.0551,0.0154,0.0444,-0.0075,0.0311,0.0516,0.0522,0.0741,0.035,0.0107,0.038,-0.1684,0.0519,0.0209,0.0206,-0.0693,-0.0627,-0.0004,0.0706,0.0176,0.0575,0.0275,0.0601,0.0175,-0.0202,0.0211,-0.0356,-0.0413,0.06,0.0191,-0.0336,-0.0461,-0.0045,-0.0423,-0.0451,0.0185,-0.0312,0.079,-0.0256,-0.0335,-0.0175,-0.0341,0.0361,-0.072,0.0065,0.045,0.0205,-0.0292,0.1594,-0.0545,0.0316,-0.011,-0.0247,0.0708,-0.0292,-0.0436,-0.0223,-0.0115,-0.0602,-0.0232,-0.0453,0.0461,-0.0698,-0.0036,0.0179,0.0597,0.0449,0.0333,-0.0282,-0.0214,-0.0014,0.0635,0.049,0.0049,-0.0632,0.0088,0.1337,0.0287,0.0174,0.018,-0.0496,-0.064,-0.0163,0.021,0.0285,0.0464,0.0054,0.0218,0.016,-0.0291,-0.0504,-0.0274,-0.0629,-0.0716,0.1363,-0.0315,0.0434,-0.0123,-0.0578,-0.0149,0.0436,0.0035,-0.0627,0.0524,0.0526,0.0408,-0.0003,-0.0358,-0.0198,-0.0201,-0.0361,-0.0197,0.0787,-0.0061,-0.0511,0.0011,0.0183,0.0185,-0.0298,0.0279,0.0008,-0.0239,0.0516,0.054,0.0396,-0.0788,0.0203,0.0053,0.0013,0.0453,-0.01,-0.0217,0.0633,0.0249,-0.0525,-0.0099,-0.0452,0.0191,0.0457,0.0077,0.0002,-0.0268,-0.0413,-0.0178,-0.0426,-0.0325,-0.0153,0.0344,-0.0651,-0.0142,-0.0046,-0.0586,0.0036,-0.0001,0.0564,-0.0083,0.011,0.0768,0.0512,-0.0094,-0.0347,0.0501,-0.0599,-0.0562,0.0028,0.0386,0.0407,0.008,0.0352,0.0474,-0.0413,-0.0658,-0.2075,0.0021,-0.0011,0.0258,0.0595,-0.0667,0.043,0.0124,0.0159,0.0619,0.0447,-0.0205,-0.0437,0.0459,-0.0304,0.0548,0.0023,-0.0027,-0.0594,0.0321,-0.0143,0.0244,-0.0534,-0.0653,0.0219,0.0158,0.2007,-0.0089,0.025,-0.0275,0.0281,-0.0192,-0.0058,-0.0914,0.0751,0.0428,0.0324,0.0002,-0.0177,-0.0074,-0.0099,0.0706,-0.0188,-0.1332,-0.0546,-0.0547,-0.0228,0.0644,-0.0566,0.0343,0.0343,-0.0182,0.0676,-0.0447,-0.0001,-0.0339,-0.0733,0.0795,-0.0076,0.0171,0.0211,-0.0433,0.0345,-0.0537,0.0189,-0.026,-0.0195,-0.0352,0.0135,-0.026,-0.0017,0.1215,-0.0406,-0.0331,0.0842,-0.0083,0.0237,-0.0713,-0.0044,-0.0012,0.0661,-0.0256,0.0602,0.07,0.056,-0.0043,0.0726,-0.0044,0.0225,0.0096,0.0142,0.0152,-0.0743,0.0021,0.0691,-0.0065,-0.3059,0.0071,-0.0354,-0.0029,-0.0311,-0.002,0.0213,0.0179,-0.0639,0.0015,0.0131,0.0145,0.0786,-0.0178,-0.0161,0.0127,0.0749,-0.0431,0.045,-0.0463,0.0018,0.0055,0.234,-0.0534,0.0383,0.0528,0.0057,0.0179,0.0449,-0.0551,0.03,-0.0525,0.0348,-0.0407,0.0253,0.097,-0.0416,-0.0153,0.0077,-0.0454,-0.0024,0.0039,-0.0337,-0.0485,0.1471,-0.0188,-0.0305,-0.0063,0.0146,0.0386,-0.0452,-0.0003,-0.039,-0.0282,-0.0026,0.0393,-0.0178,-0.0133,-0.0375,-0.0477,-0.0082,-0.024,-0.0009,-0.0183,-0.0178]}
{"key":"[Causality-Aided Falsification] Falsification is drawing attention in quality assurance of heterogeneous systems whose complexities are beyond most verification techniques' scalability. In this paper we introduce the idea of causality aid in falsification: by providing a falsification solver -- that relies on stochastic optimization of a certain cost function -- with suitable causal information expressed by a Bayesian network, search for a falsifying input value can be efficient. Our experiment results show the idea's viability.","layer":0,"vector":[-0.0704,-0.0011,-0.0185,-0.0445,0.0225,0.0303,0.0114,0.0489,0.024,-0.0296,0.0349,-0.066,0.0111,0.0655,0.0204,-0.0056,0.0027,0.0502,-0.0131,-0.0436,0.0494,-0.0263,-0.0559,-0.0413,-0.0003,0.0518,-0.0197,-0.0486,-0.0525,-0.2389,-0.0066,-0.0597,-0.0014,-0.0229,0.0231,-0.0545,-0.0259,0.0528,-0.0118,0.0651,0.024,0.0352,-0.0551,-0.0667,-0.0308,-0.0922,-0.0021,-0.0006,-0.0486,-0.039,-0.0104,-0.0192,0.0564,0.0184,0.0361,0.0515,0.0626,0.0623,0.0597,0.037,0.0041,0.0121,-0.1489,0.0621,0.0689,0.0208,-0.0215,-0.0479,0.0154,0.0807,-0.0159,0.0346,0.0115,0.0729,0.0212,-0.0089,0.0112,-0.0185,-0.0209,0.0325,-0.0092,-0.0539,-0.0189,0.0017,-0.0646,-0.0366,0.0585,-0.0187,0.0279,-0.0006,-0.0343,0.0008,0.039,0.0355,-0.0484,0.0022,0.0123,-0.0206,-0.0004,0.1672,-0.0362,0.0176,0.0261,-0.0084,0.0582,-0.0225,-0.0562,-0.0307,-0.035,0.0086,-0.0166,-0.0272,0.0544,-0.0061,0.0026,0.037,0.0461,0.0034,-0.0062,-0.0275,0.0024,0.0225,0.0489,-0.0464,0.0071,-0.0932,0.0367,0.176,0.0001,0.0355,0.0286,-0.0394,-0.0232,-0.0154,0.0356,0.0206,0.01,0.011,0.0073,0.0002,-0.0689,-0.0754,0.0017,-0.092,-0.1056,0.0946,-0.0313,0.0427,-0.0538,-0.0671,-0.0041,0.019,-0.0381,-0.0489,0.013,0.0361,0.0223,0.0221,-0.0838,0.0315,0.0057,-0.0403,-0.0554,0.1297,-0.0256,-0.0606,-0.0095,0.0437,0.021,-0.0247,0.0295,0.0231,0.003,0.0193,0.0322,0.0382,-0.0547,0.0105,-0.0061,-0.0068,0.0409,-0.0167,-0.0439,0.0536,0.0656,-0.0512,0.0301,-0.0145,0.0132,0.0379,-0.0385,-0.0036,-0.014,-0.0069,-0.0328,-0.0342,-0.0096,-0.0169,-0.0023,-0.0362,0.0248,0.0019,-0.0619,0.027,-0.0379,-0.0035,-0.0353,-0.0042,0.0227,0.0411,-0.0149,0.0232,0.0721,-0.0376,-0.0325,0.0503,0.0328,0.0111,-0.009,0.0328,0.0367,-0.0218,-0.0444,-0.2243,-0.0314,-0.0093,0.0136,0.0483,-0.0349,0.0376,-0.0041,-0.0023,0.069,0.0618,-0.0068,-0.0166,0.017,-0.0289,0.0365,0.0148,-0.0048,-0.0215,0.0075,-0.0612,0.0329,0.0028,-0.0713,0.0397,0.0331,0.2333,-0.011,0.0396,-0.0513,0.0154,-0.0102,0.0145,-0.0926,0.0423,0.0667,0.0306,0.0598,0.0113,-0.0629,-0.0259,0.0606,-0.0494,-0.1112,-0.04,-0.0783,0.0067,0.053,-0.0739,0.0295,0.0161,-0.0196,0.0584,0.0262,-0.0102,-0.0543,-0.0686,0.0138,-0.0468,0.0182,0.0235,-0.0129,0.0058,-0.0585,0.0509,-0.0053,0.0048,-0.0014,0.0242,-0.0101,0.0149,0.0986,-0.0172,-0.0308,0.0756,0.0553,0.0186,-0.0434,-0.0437,-0.0533,0.0639,-0.036,0.0012,0.0111,-0.0237,-0.0221,0.0402,0.0232,0.019,0.0,0.0088,0.0077,-0.0024,0.0249,0.0413,-0.0336,-0.3069,0.0192,0.0402,-0.0165,-0.0707,-0.0278,0.0327,0.0308,-0.0361,-0.013,-0.0317,0.0107,0.0352,0.0169,0.0115,0.0454,0.028,-0.0806,0.0708,-0.0557,0.0089,0.065,0.2672,-0.0462,-0.002,0.0582,-0.0124,0.0206,0.0699,0.0007,0.0843,0.0343,0.0358,-0.0293,0.0341,0.0489,-0.0256,0.0679,0.0282,-0.026,-0.0395,-0.0099,-0.016,-0.0217,0.1046,0.0031,-0.0303,-0.0359,0.0092,0.0287,-0.0195,0.0232,-0.0287,-0.0169,0.0058,0.0052,-0.0289,-0.0139,-0.0154,-0.041,0.0026,-0.0165,0.0227,0.0244,-0.0348]}
{"key":"[Spectrally Adapted Physics-Informed Neural Networks for Solving Unbounded Domain Problems] Solving analytically intractable partial differential equations (PDEs) that involve at least one variable defined in an unbounded domain requires efficient numerical methods that accurately resolve the dependence of the PDE on that variable over several orders of magnitude. Unbounded domain problems arise in various application areas and solving such problems is important for understanding multi-scale biological dynamics, resolving physical processes at long time scales and distances, and performing parameter inference in engineering problems. In this work, we combine two classes of numerical methods: (i) physics-informed neural networks (PINNs) and (ii) adaptive spectral methods. The numerical methods that we develop take advantage of the ability of physics-informed neural networks to easily implement high-order numerical schemes to efficiently solve PDEs. We then show how recently introduced adaptive techniques for spectral methods can be integrated into PINN-based PDE solvers to obtain numerical solutions of unbounded domain problems that cannot be efficiently approximated by standard PINNs. Through a number of examples, we demonstrate the advantages of the proposed spectrally adapted PINNs (s-PINNs) over standard PINNs in approximating functions, solving PDEs, and estimating model parameters from noisy observations in unbounded domains.","layer":6,"vector":[-0.0903,-0.037,0.0352,-0.0038,0.0243,0.0183,0.0016,0.0366,0.0527,-0.0096,0.0105,-0.0597,0.0457,0.0315,0.0248,0.0156,0.0034,0.0538,-0.0443,0.0251,-0.0055,0.0093,-0.032,-0.0267,0.022,-0.0213,-0.0428,0.0115,-0.0514,-0.262,0.0496,-0.0233,-0.015,-0.0225,0.0501,0.0171,-0.0142,0.0508,0.0377,0.0423,0.0408,-0.0049,0.0019,-0.0531,0.0155,-0.0688,0.0213,0.0084,0.0094,-0.06,0.0091,-0.0329,0.0167,0.01,0.0306,0.006,0.0371,0.0257,0.041,0.0325,0.0398,0.0384,-0.1618,0.0781,0.0782,0.0286,0.002,-0.0116,0.0422,0.0472,-0.0198,0.0345,0.0243,0.0289,0.0577,0.0215,0.014,-0.0408,-0.0134,0.0157,0.0317,-0.0386,-0.0482,-0.0314,-0.0075,0.0177,0.0171,-0.0409,0.0079,0.0167,-0.0863,-0.0599,-0.0326,0.0236,-0.059,-0.0126,0.064,0.0018,-0.0412,0.2015,-0.0252,-0.0199,0.0131,-0.0425,-0.0044,-0.022,-0.0854,-0.0135,-0.0104,0.0059,-0.0061,-0.0175,0.017,-0.0156,0.0341,0.0177,0.0222,-0.0285,-0.018,-0.0142,-0.0322,-0.0462,0.0349,0.0126,0.0266,-0.0724,-0.012,0.1223,0.05,0.0352,0.0786,-0.0038,-0.066,-0.0573,-0.0261,0.0082,0.0145,-0.0185,0.0004,0.0301,-0.0499,-0.0844,0.0112,-0.1055,-0.0552,0.0857,-0.062,0.0364,-0.0332,-0.0115,-0.0618,0.0286,-0.0641,-0.043,0.0178,0.0388,-0.0135,0.0065,-0.0499,0.0401,-0.0047,-0.0358,-0.0234,0.1316,0.0102,-0.0403,-0.0144,0.0221,0.0376,-0.04,-0.0044,0.0616,-0.0212,-0.0163,0.1022,0.0229,-0.0358,0.0142,0.0037,0.0591,0.0286,-0.0214,-0.024,0.0609,0.0595,-0.0522,0.0453,-0.0237,0.0351,0.0693,-0.048,-0.0173,-0.0397,0.0288,-0.0266,-0.0288,-0.0069,-0.0406,-0.0258,-0.0501,0.0048,-0.0147,-0.0554,0.0463,-0.012,-0.0168,0.0041,0.0278,-0.0257,0.0066,-0.0391,0.0251,0.0584,-0.0527,-0.0498,0.0315,0.0117,0.0155,0.0334,0.0649,0.0528,-0.0476,-0.0647,-0.2253,-0.0174,-0.0001,-0.0314,0.057,-0.0854,0.0699,-0.0272,0.0859,0.0436,0.0077,0.038,-0.0137,0.0048,0.0103,0.007,0.035,-0.016,-0.0062,-0.0026,-0.0184,0.0171,-0.0225,-0.0926,0.0595,0.0092,0.1954,0.0198,0.0448,-0.0758,-0.0152,0.0213,-0.0053,-0.0446,0.0492,0.029,0.1108,-0.0051,-0.0149,-0.0327,0.0145,0.0404,-0.0289,-0.0669,-0.0428,-0.01,0.0236,0.019,-0.0451,-0.0133,0.0642,-0.0335,0.0553,-0.008,-0.0248,-0.0425,-0.0619,0.046,-0.0279,-0.0148,-0.0481,-0.0833,0.0299,-0.034,0.0415,0.012,-0.0065,-0.0311,0.0807,-0.0643,-0.0311,0.1062,0.0187,0.021,0.0357,0.0294,0.0168,-0.002,-0.0363,0.0241,0.0864,-0.0367,0.048,0.012,0.0284,0.0097,0.079,-0.0166,-0.0138,-0.0459,-0.0557,0.028,-0.0695,0.0296,0.0136,-0.0114,-0.2733,0.0589,-0.0088,0.0254,-0.0198,0.0144,0.0221,0.0237,-0.0768,0.0199,-0.0286,0.0346,0.0264,0.0422,0.0166,0.0147,0.0379,-0.0558,0.0598,-0.0906,0.0544,0.0368,0.237,-0.0951,0.0515,0.063,-0.0126,0.0122,0.0251,-0.0313,0.0102,0.0216,0.0413,-0.0755,0.0508,0.1012,-0.0078,0.0363,0.0582,-0.0322,0.0409,-0.0032,-0.0142,-0.0124,0.0542,-0.0003,-0.0185,-0.0464,0.012,0.014,-0.0425,0.0115,-0.0107,-0.0084,0.0099,0.0352,-0.05,-0.0872,-0.0579,-0.0627,0.0143,-0.0568,-0.0188,0.0026,-0.0146]}
{"key":"[PFGE: Parsimonious Fast Geometric Ensembling of DNNs] Ensemble methods have been widely used to improve the performance of machine learning methods in terms of generalization, while they are hard to use in deep learning systems, as training an ensemble of deep neural networks (DNNs) incurs an extremely higher computational overhead of model training. Recently, advanced techniques such as fast geometric ensembling (FGE) and snapshot ensemble have been proposed. These methods can train the model ensembles in the same time as a single model, thus getting around the hurdle of training time. However, their memory overhead for test-time inference remains much higher than single model based methods. Here we propose a parsimonious FGE (PFGE) that employs a lightweight ensemble of higher-performing DNNs, generated by successively-performed stochastic weight averaging procedures. Experimental results across different advanced DNN architectures on benchmark datasets CIFAR-$\\{10,100\\}$ and Imagenet, demonstrate that PFGE matches the state-of-the-art FGE method in terms of the generalization error, yet requires only 20% memory overhead for test-time inference. Our code is available at https://github.com/ZJLAB-AMMI/PFGE.","layer":3,"vector":[-0.0052,0.005,0.0256,-0.0067,0.0249,0.0292,-0.0004,0.0059,0.0159,0.0181,-0.0112,-0.0553,0.0808,0.0692,-0.0242,0.0214,0.0287,0.0182,-0.0419,-0.014,0.0043,-0.0393,-0.0457,-0.0245,0.0054,0.0276,-0.0373,-0.0054,-0.0782,-0.2835,0.0684,-0.0292,0.0199,-0.0492,-0.0121,-0.0187,-0.0408,0.0162,-0.033,0.0527,0.018,0.0176,-0.0532,-0.0743,-0.0001,-0.021,-0.0256,-0.0141,-0.033,-0.0246,0.0303,-0.0438,0.0194,0.0131,0.0229,0.0556,0.0009,0.0312,0.0537,0.0129,0.0339,0.0096,-0.1594,0.0655,0.0537,0.0383,-0.0089,-0.0637,-0.0099,0.0446,-0.0185,0.0329,0.0071,0.0494,0.0137,0.0065,0.0117,-0.0439,-0.0352,0.0344,0.0486,0.0044,-0.056,0.0065,0.0164,0.0029,0.0422,-0.0129,0.0495,-0.0214,-0.0072,0.0153,-0.0523,0.0359,-0.0383,0.0043,0.0553,0.0127,-0.0414,0.2239,-0.0571,0.0457,0.0486,-0.0087,0.0359,-0.0675,-0.0599,-0.0316,-0.0381,-0.0345,0.0227,0.0083,0.0293,-0.0684,0.0054,0.0266,0.0488,0.0677,-0.0558,-0.029,0.0082,-0.0137,0.0316,-0.026,0.0231,-0.0765,-0.0335,0.1712,0.0569,0.0155,0.0575,-0.0005,-0.0406,-0.029,0.025,0.0096,0.0713,-0.0057,0.0123,-0.0245,-0.0588,-0.0391,0.0243,-0.0616,-0.0166,0.1427,-0.044,0.0491,-0.058,-0.0385,-0.0067,-0.0235,-0.04,-0.021,0.0132,-0.004,0.0252,0.0257,-0.036,-0.0095,-0.0408,0.0066,-0.0254,0.0909,0.0134,-0.0749,-0.0183,-0.0363,0.005,-0.0609,0.0348,-0.0202,-0.0216,0.0363,0.0567,0.0182,-0.0613,-0.0015,-0.0086,0.0337,0.0209,-0.0072,-0.053,-0.0113,0.0357,-0.0298,0.0125,-0.0613,0.0331,0.0405,-0.037,0.036,-0.0317,-0.005,-0.0326,0.0062,-0.0154,0.0236,0.0389,-0.0287,0.0128,0.0247,-0.0088,-0.0027,0.001,0.0387,-0.0159,0.0009,0.051,0.0473,-0.0264,-0.0108,0.0522,-0.0455,-0.0088,0.0185,0.0026,-0.0041,-0.0301,0.0549,0.047,-0.0682,-0.0628,-0.2063,-0.0124,0.0179,-0.0289,0.0766,-0.0516,0.0315,-0.0023,0.1008,0.0571,0.0218,0.0048,-0.0065,0.0397,0.0071,0.0435,0.0145,0.0473,-0.0274,0.013,-0.0079,0.0477,-0.0348,-0.0559,0.0579,0.0047,0.2125,0.0214,0.0492,-0.0365,0.0264,0.0591,-0.0199,-0.1011,0.0794,0.0295,0.0689,0.0326,-0.0424,-0.0485,-0.0558,0.0225,0.0222,-0.1641,-0.0326,-0.0092,-0.0354,0.0308,-0.043,0.0106,0.0363,-0.0777,0.0417,0.0095,-0.0169,-0.0365,-0.1286,0.0431,-0.0642,0.0334,0.018,-0.0586,0.0243,-0.0473,0.0619,0.0019,-0.0811,-0.085,0.0046,-0.0028,-0.0514,0.0609,-0.0008,0.0125,0.0801,0.0256,0.0726,-0.0177,0.0008,-0.0522,0.0563,0.0003,-0.0116,0.0275,0.0255,0.0262,0.0651,0.0154,0.0594,0.0153,0.0008,0.02,-0.043,0.0236,0.0283,-0.0053,-0.2856,0.0532,0.0039,0.0275,-0.0176,0.0041,0.0127,-0.0257,-0.0158,-0.04,-0.0033,0.0474,0.071,-0.0216,0.0073,0.027,0.0624,-0.0354,0.0172,-0.0868,-0.0234,0.0269,0.2049,-0.0377,-0.0046,0.0714,-0.0137,-0.0152,0.0496,-0.0113,0.0393,-0.0135,0.0585,-0.0873,0.0101,0.059,-0.0085,0.0028,0.0098,-0.0243,0.0157,-0.0022,0.0012,0.0131,0.0661,-0.0084,-0.0224,-0.0763,-0.0144,0.0296,-0.0043,-0.0018,-0.0205,-0.0292,0.0112,0.0164,0.0028,-0.0292,-0.0598,-0.0454,0.0361,-0.0512,-0.0361,0.0117,-0.0299]}
{"key":"[Learning States Representations in POMDP] We propose to deal with sequential processes where only partial observations are available by learning a latent representation space on which policies may be accurately learned.","layer":3,"vector":[-0.0513,-0.0333,0.0399,-0.072,-0.0066,0.03,0.033,0.0313,0.0512,-0.0138,0.0407,-0.0435,0.0173,0.0632,-0.011,0.0287,-0.0263,0.0747,-0.0129,0.008,0.0164,-0.0369,-0.0198,-0.0346,0.009,0.0286,-0.0192,-0.0272,-0.0366,-0.2389,0.0194,-0.0487,0.022,-0.0404,0.0332,-0.0267,-0.0312,0.0618,0.0091,0.0463,0.0229,-0.0215,-0.0357,-0.0557,-0.0508,-0.0619,-0.002,-0.0387,-0.0168,-0.0559,0.0214,0.0091,0.0335,0.0318,0.0868,0.0042,0.0254,0.0689,0.0102,0.0187,0.005,0.0266,-0.1526,0.032,0.0132,0.0806,-0.0456,0.0112,0.0243,0.0565,-0.0204,0.0335,0.0126,0.0534,-0.0103,0.0036,0.0025,-0.0214,-0.0362,0.0099,0.001,-0.0144,-0.0118,-0.0118,-0.0237,-0.0745,-0.0074,-0.0616,0.0534,0.0141,-0.0516,-0.0181,-0.0289,0.0152,-0.0396,0.0099,0.0576,0.0482,-0.023,0.1846,-0.0126,0.05,0.0569,0.0051,0.0478,-0.0342,-0.0223,0.0017,-0.0273,0.0257,-0.0473,-0.0009,0.0149,-0.044,0.0206,-0.0331,0.0639,0.0626,-0.0303,-0.0057,0.0058,0.0201,0.0474,-0.0389,0.0216,-0.0867,0.0149,0.1334,0.0361,0.0464,0.0386,-0.0679,-0.0038,-0.0026,0.0147,0.0535,0.0173,-0.0059,0.0234,-0.022,-0.0489,0.0017,-0.0092,-0.1146,-0.0802,0.158,-0.0173,0.0088,-0.0466,0.0004,-0.0276,0.0414,0.0069,-0.0633,0.0452,0.0287,0.0339,0.0158,-0.052,0.0572,-0.045,-0.052,-0.0141,0.0989,-0.0002,-0.0596,-0.0325,-0.0152,0.0108,-0.0223,0.0698,0.0663,-0.039,-0.0018,0.0748,0.0137,-0.0738,-0.0002,0.0133,0.0,0.0034,-0.1103,-0.0096,0.0251,0.0414,-0.0361,-0.0055,-0.0408,-0.0135,0.0092,-0.01,0.0326,-0.0271,-0.0049,-0.0273,-0.0739,-0.0186,0.0063,-0.0033,-0.054,-0.0238,0.0091,-0.0921,0.0228,-0.0082,0.021,-0.0371,0.0156,0.0563,0.0023,-0.0102,0.0421,0.0528,0.0204,-0.0235,0.0143,-0.0145,0.0419,0.0181,0.0293,0.0061,-0.0187,-0.031,-0.2198,0.0069,0.0005,-0.0216,0.0426,-0.0758,0.0049,-0.0296,0.0141,0.0636,0.0358,-0.0269,-0.0152,0.0076,0.0161,0.0279,0.075,0.0645,-0.0548,0.0181,-0.0371,-0.0011,-0.0412,-0.1003,0.0596,-0.0184,0.2364,0.0447,0.0277,-0.0296,-0.0063,0.0122,-0.0545,-0.1026,0.071,0.0108,0.0326,0.0104,0.0249,-0.0378,0.007,0.0019,-0.0452,-0.0662,-0.0006,-0.0505,-0.0097,0.0472,-0.0562,-0.008,0.0525,-0.0686,0.0325,-0.0466,-0.0344,-0.0248,-0.1121,0.0364,-0.0731,0.0158,0.0122,-0.0618,0.024,-0.0414,0.0573,-0.0197,-0.0126,-0.0466,0.0309,0.0028,-0.0519,0.0403,-0.0541,-0.0337,0.0592,0.0183,0.0103,-0.0148,-0.083,0.0085,0.0786,-0.0539,0.0165,0.0416,0.0286,0.0024,0.0799,0.0092,0.0319,-0.0185,-0.001,0.0235,-0.0535,0.0385,0.0351,-0.0237,-0.3002,0.0663,0.0279,0.0468,0.0036,-0.0045,0.0278,0.0338,-0.0681,-0.0142,0.0218,0.0673,0.0411,0.0099,0.0057,0.0344,0.0778,-0.0155,0.0373,-0.0695,0.0358,0.0557,0.2265,-0.0172,0.057,-0.018,-0.0173,-0.001,0.0255,0.0088,0.0305,0.0029,0.0587,-0.0132,0.0193,0.0665,-0.0336,0.0506,-0.0082,-0.0021,-0.0198,0.0022,-0.008,-0.0286,0.1094,0.0172,-0.0226,-0.0256,-0.0325,0.0681,-0.0094,0.0086,-0.0176,-0.0165,0.0412,0.0262,-0.0262,-0.065,-0.0158,-0.0469,0.0113,-0.0879,0.0324,-0.0134,0.007]}
{"key":"[iRNN: Integer-only Recurrent Neural Network] Recurrent neural networks (RNN) are used in many real-world text and speech applications. They include complex modules such as recurrence, exponential-based activation, gate interaction, unfoldable normalization, bi-directional dependence, and attention. The interaction between these elements prevents running them on integer-only operations without a significant performance drop. Deploying RNNs that include layer normalization and attention on integer-only arithmetic is still an open problem. We present a quantization-aware training method for obtaining a highly accurate integer-only recurrent neural network (iRNN). Our approach supports layer normalization, attention, and an adaptive piecewise linear approximation of activations (PWL), to serve a wide range of RNNs on various applications. The proposed method is proven to work on RNN-based language models and challenging automatic speech recognition, enabling AI applications on the edge. Our iRNN maintains similar performance as its full-precision counterpart, their deployment on smartphones improves the runtime performance by $2\\times$, and reduces the model size by $4\\times$.","layer":5,"vector":[-0.0856,-0.0263,0.0248,-0.0297,-0.0164,0.0431,0.0067,0.034,-0.0008,-0.0538,0.0249,-0.0237,0.0396,0.0754,0.0477,0.0148,0.0133,0.0121,-0.0611,-0.0021,0.0502,-0.0238,0.0081,-0.0127,0.0201,-0.0039,-0.0361,-0.0206,-0.0403,-0.2385,0.0302,-0.0127,0.0634,-0.0055,-0.0226,-0.0263,-0.0766,0.0448,-0.0074,0.0362,0.0151,0.0458,-0.0109,-0.0721,-0.008,-0.0648,-0.0435,-0.011,0.0208,-0.0242,0.0314,-0.0129,0.0606,0.0134,0.0102,0.0273,0.0544,0.0631,0.0271,0.0353,0.0207,0.0183,-0.1388,0.0357,0.0184,0.0068,-0.0511,-0.0283,-0.0089,0.0061,-0.0196,0.0354,0.0371,0.0134,0.0231,0.0088,-0.0002,0.0129,0.0134,0.0165,-0.0009,-0.0034,-0.054,-0.0187,0.0046,-0.0672,0.0219,-0.0267,-0.0128,-0.0075,-0.0435,0.0251,-0.0318,0.0414,-0.0032,-0.031,0.0468,0.033,-0.1016,0.204,-0.0188,0.0323,-0.0433,-0.0456,0.0513,-0.0017,-0.0652,-0.0337,-0.0527,0.0246,-0.0505,-0.0646,0.0667,-0.0198,0.0376,0.0176,0.0767,0.0196,0.0276,0.0079,-0.0033,-0.0144,-0.0295,-0.0276,0.028,-0.0422,0.0477,0.103,0.0285,0.0428,0.0518,-0.0036,-0.0298,-0.0246,0.0545,0.0138,0.0039,-0.0505,0.0154,0.0104,-0.0338,-0.0669,0.0156,-0.0557,-0.0438,0.1269,-0.0492,0.016,-0.0225,-0.0375,-0.0002,0.0374,-0.0051,-0.0559,0.0327,0.0231,0.0289,0.0001,-0.0349,-0.0061,-0.0262,-0.0492,-0.0722,0.0894,0.0093,-0.0954,0.0219,-0.0063,-0.007,-0.0411,0.07,0.0319,-0.0592,0.0482,0.0894,0.0573,-0.0563,-0.0088,-0.0219,0.0493,-0.0164,-0.0833,0.0043,0.0227,0.0163,-0.0416,0.0599,-0.0659,0.0047,0.0461,-0.0292,0.0543,-0.0332,0.0013,-0.0029,-0.02,-0.0043,0.0046,-0.0183,-0.0629,0.0056,-0.0212,-0.0224,-0.0039,0.0366,0.0346,-0.0333,0.038,0.0364,0.0573,-0.0082,-0.0054,0.0716,-0.0508,-0.0344,0.0057,0.0141,0.0285,0.0127,0.0483,0.0383,-0.0411,-0.0611,-0.2375,-0.0148,0.0302,-0.0583,0.0926,-0.0621,0.0139,-0.0215,0.033,0.0581,-0.0011,-0.0429,-0.027,0.033,0.035,0.0397,0.0268,0.0071,0.0044,-0.0312,0.0095,0.0002,-0.0408,-0.083,0.0527,0.0188,0.2267,0.0386,0.1015,-0.0295,0.0375,0.007,-0.0451,-0.0706,0.0819,0.0147,0.0718,0.0174,-0.055,-0.0366,-0.0584,0.0327,-0.0117,-0.1039,-0.0639,0.038,-0.0439,-0.0201,-0.0453,0.0097,0.007,-0.0373,0.0477,0.0343,-0.0069,-0.0226,-0.056,0.028,-0.0408,0.0206,0.0044,-0.0368,0.0046,-0.015,0.0406,0.0311,-0.0184,-0.03,0.0443,0.0082,-0.0657,0.0732,0.048,0.0112,0.0288,-0.0075,0.036,-0.0689,-0.0219,-0.0497,0.054,-0.047,0.0709,0.008,0.0052,0.0157,0.1106,-0.0115,0.0311,0.009,0.0096,0.0348,-0.0511,-0.033,0.0111,-0.0264,-0.3224,0.0404,-0.02,0.0407,-0.0515,0.0296,0.0306,0.0378,-0.0749,0.0002,-0.0379,0.0709,0.0674,-0.018,-0.0328,0.0271,0.0705,-0.0492,0.0497,-0.0269,0.0506,0.0375,0.1979,-0.0195,0.0312,-0.0067,-0.03,-0.005,0.0456,-0.0127,0.0114,0.0049,0.112,-0.0173,0.0138,0.0289,0.0149,0.0346,0.0016,0.0224,-0.0165,-0.0057,-0.0667,-0.0211,0.0823,-0.0372,-0.0092,-0.0296,0.0094,0.048,-0.0221,0.0123,-0.0383,-0.0042,0.0475,0.0596,-0.0462,-0.0502,-0.0306,-0.0132,0.0411,-0.0711,-0.0447,0.0204,-0.0784]}
{"key":"[Recurrent Neural Networks with Top-k Gains for Session-based Recommendations] RNNs have been shown to be excellent models for sequential data and in particular for data that is generated by users in an session-based manner. The use of RNNs provides impressive performance benefits over classical methods in session-based recommendations. In this work we introduce novel ranking loss functions tailored to RNNs in the recommendation setting. The improved performance of these losses over alternatives, along with further tricks and refinements described in this work, allow for an overall improvement of up to 35% in terms of MRR and Recall@20 over previous session-based RNN solutions and up to 53% over classical collaborative filtering approaches. Unlike data augmentation-based improvements, our method does not increase training times significantly. We further demonstrate the performance gain of the RNN over baselines in an online A/B test.","layer":1,"vector":[-0.0682,-0.0217,0.0144,-0.0421,0.0064,0.0394,0.028,0.03,0.0373,-0.0562,0.0084,-0.01,0.0432,0.0916,0.0314,-0.0341,0.0391,0.0023,-0.0308,-0.0015,0.0181,-0.0239,-0.0118,-0.0587,-0.033,-0.0358,-0.0231,-0.031,-0.0856,-0.2279,-0.02,-0.0625,0.0524,0.005,-0.0232,0.009,-0.0138,0.0336,-0.0178,0.0199,0.0422,0.042,-0.0097,-0.079,-0.0105,0.0238,0.0087,-0.0239,-0.0169,0.0051,0.0523,-0.0102,0.035,0.025,0.0355,0.0533,0.022,0.0704,-0.0161,0.0517,0.039,0.03,-0.1618,0.0247,-0.0105,-0.0021,-0.0715,0.0267,-0.0405,0.0519,-0.0212,0.0454,-0.0023,0.0527,-0.0031,0.0116,-0.0019,-0.0006,-0.0039,0.0446,-0.0249,-0.0044,-0.0709,-0.0538,0.0476,-0.0421,0.0264,-0.0661,0.0079,0.016,-0.0281,0.0072,-0.0336,0.0129,-0.0508,0.0155,0.0383,0.057,-0.0535,0.2,-0.0359,0.0769,0.0136,-0.024,0.0066,-0.0731,0.0485,-0.0331,-0.0103,0.0221,-0.0519,0.014,0.0505,-0.0386,0.0535,0.0234,0.1016,0.0014,0.0369,0.0093,-0.0323,0.0032,0.0791,-0.0224,0.07,-0.0439,0.0027,0.0945,0.0321,0.0304,0.0482,-0.0109,-0.0685,-0.0265,0.0045,0.0362,-0.0039,-0.0622,0.0404,-0.0101,-0.0215,-0.0546,-0.0011,-0.0555,-0.0541,0.1187,-0.0325,-0.0076,-0.0587,-0.0144,-0.0301,0.0268,-0.0372,-0.0545,0.0427,0.0227,0.0751,0.03,-0.0887,0.0208,-0.0178,-0.071,-0.026,0.0646,0.0026,-0.0616,-0.0294,0.0099,-0.0019,0.0113,0.0053,0.007,-0.0284,-0.0003,0.0863,0.0469,-0.0322,-0.0369,0.0117,-0.0042,0.0263,-0.0374,-0.0383,0.0329,0.0256,0.0008,-0.0108,-0.0616,0.0311,0.0411,-0.0545,-0.0369,-0.0169,0.0126,-0.0113,-0.0179,-0.0234,-0.0179,0.0214,-0.0346,0.0163,0.0068,-0.0377,0.0319,0.0059,0.0656,-0.0197,0.0009,0.0619,-0.0189,-0.0423,0.004,0.0584,-0.0258,-0.0351,-0.0222,0.0268,0.0285,0.0305,0.0573,0.0551,-0.0065,-0.0357,-0.2628,0.0166,0.0255,-0.015,0.0774,-0.0843,0.0366,-0.0265,0.0621,0.0731,0.0473,0.0203,-0.0284,0.0621,0.0267,0.0623,0.0368,-0.0067,0.0007,-0.0456,-0.027,0.0156,0.003,-0.0916,0.0555,0.0127,0.2157,0.0488,-0.0062,-0.0558,0.0593,0.0315,-0.0055,-0.0997,0.0477,0.0093,0.1002,0.0241,-0.0231,-0.0629,0.0001,0.0211,0.0161,-0.1041,-0.0512,-0.04,-0.0515,0.0029,-0.0449,0.052,0.0143,-0.0367,0.0659,-0.0271,-0.0495,-0.0205,-0.0836,0.0007,-0.0556,0.0475,0.0066,-0.0329,0.0232,-0.0754,0.0649,-0.0195,-0.0097,0.0161,0.0266,0.0001,-0.0477,0.0423,-0.0235,0.0089,0.0608,0.0079,0.0422,-0.0476,-0.0675,-0.0105,0.0612,-0.0398,0.0628,-0.0106,0.0269,-0.0165,0.0797,0.0063,0.0133,0.0027,-0.0091,-0.0045,-0.0699,-0.0518,0.0886,-0.0209,-0.2766,0.0482,-0.0159,0.04,-0.0216,-0.0195,0.0259,0.0252,-0.0149,0.0026,0.012,0.0433,0.0555,-0.044,-0.0251,0.0581,0.0358,-0.0376,0.0382,-0.029,0.0271,0.0335,0.2211,-0.0117,0.0456,-0.0199,-0.0075,-0.0292,0.0523,0.0017,0.0335,0.0024,0.1019,-0.0598,0.0031,0.0599,-0.0504,-0.0166,0.0371,-0.038,0.0071,0.0238,-0.0664,-0.0305,0.0878,-0.0036,-0.0114,-0.0491,-0.0118,-0.0122,-0.0708,-0.0166,-0.0135,0.019,0.0382,0.078,-0.0437,-0.05,-0.0228,-0.0438,0.0116,-0.0705,0.0216,-0.0241,-0.0044]}
{"key":"[A Critical Examination of RESCAL for Completion of Knowledge Bases with Transitive Relations] Link prediction in large knowledge graphs has received a lot of attention recently because of its importance for inferring missing relations and for completing and improving noisily extracted knowledge graphs. Over the years a number of machine learning researchers have presented various models for predicting the presence of missing relations in a knowledge base. Although all the previous methods are presented with empirical results that show high performance on select datasets, there is almost no previous work on understanding the connection between properties of a knowledge base and the performance of a model. In this paper we analyze the RESCAL method and prove that it can not encode asymmetric transitive relations in knowledge bases.","layer":8,"vector":[-0.039,-0.013,0.0284,0.0067,0.0576,-0.0048,0.0152,0.0717,0.0458,0.002,-0.0029,-0.0615,0.0521,0.0684,0.028,0.0108,0.0056,0.0727,-0.0557,0.0232,0.0592,-0.0601,-0.0245,-0.0646,0.0482,0.0409,-0.0309,-0.0478,-0.0188,-0.2145,-0.021,-0.0326,0.0041,0.0041,-0.0007,-0.0193,-0.0059,0.056,-0.0181,-0.0154,0.0594,-0.0116,0.0365,-0.0315,-0.033,-0.0356,0.0076,-0.0159,-0.032,-0.0154,0.013,-0.0382,-0.0061,0.0294,0.0491,0.0686,0.072,0.0208,0.0493,0.0613,0.09,0.0362,-0.1713,0.0065,0.033,0.0309,-0.0527,-0.0134,-0.0058,0.0763,0.0195,0.0249,0.0602,0.0761,0.0179,0.0219,-0.0119,0.0153,-0.0292,0.0085,-0.0046,-0.0304,-0.0502,-0.0357,-0.0236,-0.0317,0.0185,-0.0714,0.0019,0.0232,-0.0513,-0.041,-0.0117,-0.0108,-0.1072,-0.0163,0.0631,0.0075,-0.046,0.158,-0.1027,0.0353,0.022,-0.0291,0.0044,-0.0284,0.0073,-0.0282,-0.0388,-0.0256,-0.0003,-0.0146,0.0295,-0.0465,0.0828,0.0033,0.1055,0.0484,-0.0408,-0.0284,-0.0055,0.0355,0.0329,-0.0299,-0.0073,-0.0125,-0.0227,0.1462,0.0171,0.0327,0.0562,-0.019,-0.0144,-0.0205,-0.0032,0.0121,0.0463,-0.0309,-0.0103,-0.0121,-0.0096,-0.0673,-0.0036,-0.0539,-0.0894,0.1393,-0.0128,0.0344,-0.0627,-0.017,-0.0192,0.0415,0.0155,-0.0465,0.0275,0.0522,0.0297,0.0262,-0.0724,0.0304,0.0107,-0.0491,-0.0518,0.0683,0.0347,-0.0911,-0.0301,0.0009,0.0019,-0.0116,0.0501,0.0658,-0.0292,0.0396,0.0738,0.0122,-0.094,-0.0181,0.0194,0.0195,0.0598,-0.0489,-0.0364,0.0684,-0.0042,-0.0089,0.0062,-0.0138,0.0267,0.0299,-0.0156,0.0093,-0.0153,-0.0172,-0.0367,0.0055,-0.0254,-0.0315,0.0137,-0.0615,0.0026,0.0256,-0.042,0.0263,-0.0552,0.0045,-0.0109,-0.0003,0.034,0.0123,-0.0287,-0.0539,0.0266,-0.0389,0.0111,-0.0371,0.0227,-0.0012,-0.021,0.0265,0.0641,-0.032,-0.0343,-0.2317,-0.0162,-0.0102,0.0377,0.0775,-0.0848,0.024,0.0065,0.0249,0.0785,0.0584,0.007,-0.0103,0.0005,-0.0173,0.0669,0.0819,0.0526,-0.035,-0.0045,-0.0212,0.0163,-0.0253,-0.0906,0.0361,0.0287,0.1893,0.028,0.0159,-0.0534,0.0068,0.0444,-0.0557,-0.0992,0.074,0.013,0.0384,-0.013,0.0077,-0.0606,-0.0289,0.0004,-0.0711,-0.044,-0.0578,-0.045,-0.0325,-0.0177,-0.0267,0.0852,0.011,0.0127,0.0276,0.0507,-0.0184,-0.0018,-0.0899,0.0014,-0.036,0.0286,0.0329,-0.0204,-0.017,-0.0787,0.0667,-0.0099,-0.0065,-0.0152,0.0242,-0.0339,-0.0132,0.1038,-0.0025,-0.0159,0.0297,-0.021,0.003,-0.0683,-0.0389,-0.0253,0.0472,-0.0872,0.0793,0.0467,0.0378,0.0051,0.0866,-0.0326,0.0543,-0.0468,0.025,-0.01,-0.0114,-0.0004,0.0596,-0.0288,-0.3033,0.0524,0.039,0.0006,-0.0405,-0.0012,0.0806,0.0308,-0.0175,-0.0122,0.0362,0.0295,0.0198,-0.0442,-0.0257,-0.013,0.0773,-0.048,0.0245,-0.0503,0.0163,0.0013,0.2239,0.0061,0.0335,0.0162,-0.0512,-0.0107,0.0026,0.0311,0.0185,0.0116,0.0522,-0.0619,0.0405,0.0389,-0.0346,0.0349,0.0733,-0.0114,0.0018,-0.0165,-0.0761,-0.0493,0.1039,0.0045,-0.0145,-0.0589,0.0321,0.0161,-0.0651,-0.0182,-0.0131,0.0372,0.0205,0.0252,-0.015,-0.023,-0.0517,-0.0489,-0.0192,-0.0419,0.0031,-0.0096,0.0071]}
{"key":"[JamBot: Music Theory Aware Chord Based Generation of Polyphonic Music with LSTMs] We propose a novel approach for the generation of polyphonic music based on LSTMs. We generate music in two steps. First, a chord LSTM predicts a chord progression based on a chord embedding. A second LSTM then generates polyphonic music from the predicted chord progression. The generated music sounds pleasing and harmonic, with only few dissonant notes. It has clear long-term structure that is similar to what a musician would play during a jam session. We show that our approach is sensible from a music theory perspective by evaluating the learned chord embeddings. Surprisingly, our simple model managed to extract the circle of fifths, an important tool in music theory, from the dataset.","layer":2,"vector":[-0.0545,-0.0571,0.0328,-0.0445,0.0013,0.0498,-0.0188,0.0266,0.0214,-0.0281,0.0624,-0.0085,0.0409,0.0118,0.028,0.0225,0.01,0.0599,-0.0434,-0.0099,0.0626,-0.0012,-0.0416,-0.0625,-0.0025,0.0062,-0.0319,-0.0438,-0.0884,-0.1948,0.0045,-0.0013,0.0947,-0.0379,-0.0058,-0.0314,-0.0197,0.064,-0.0549,0.0727,0.0137,0.021,0.0038,-0.0805,-0.0224,-0.0566,-0.0506,-0.0035,0.0109,-0.0227,-0.0015,-0.0489,-0.0115,0.0193,0.0276,0.0609,0.0397,0.0693,0.0197,0.06,0.0224,0.0392,-0.1299,0.0312,0.0249,0.0763,-0.0383,-0.0277,-0.0118,0.0426,-0.0392,0.0137,0.0226,0.0217,0.0333,0.0104,-0.0114,-0.0447,-0.055,0.0262,-0.0174,-0.0211,-0.0435,0.0032,-0.0298,-0.023,-0.0159,-0.0635,-0.0022,0.0018,-0.0769,0.0011,-0.0355,0.0022,-0.0677,-0.0097,0.023,0.0164,-0.0183,0.2118,-0.0535,0.0245,0.0447,-0.0471,0.0026,-0.0499,-0.0132,0.0083,-0.026,-0.0219,0.0052,-0.0121,0.0052,-0.0319,0.0162,0.0118,0.0514,0.0659,0.0348,-0.0076,-0.0539,-0.0069,0.0505,-0.0054,0.0602,-0.0409,0.0028,0.0892,0.0329,0.0349,0.0171,-0.005,-0.0361,-0.0378,0.0183,0.0058,0.0401,-0.0102,0.0456,0.0059,-0.0284,-0.0676,0.002,-0.0865,-0.0334,0.1046,-0.0066,0.0374,-0.0485,0.0316,-0.021,0.0062,-0.0199,-0.0146,0.0696,0.0594,0.0623,0.0055,-0.0276,0.0386,-0.0477,-0.0646,-0.0211,0.1193,-0.0083,-0.053,-0.0207,-0.0094,0.018,-0.0641,-0.0061,0.029,-0.0529,0.0386,0.1208,0.0008,-0.0572,-0.0264,0.051,0.0218,0.0465,-0.0724,-0.0441,0.0475,0.0309,-0.0282,-0.0128,-0.0512,0.0256,0.0293,0.0068,0.0322,-0.0034,0.0081,-0.0207,-0.0631,-0.0132,0.0169,0.0089,-0.0183,-0.0378,0.0399,-0.0422,0.0335,-0.0039,-0.0129,0.0039,0.0096,0.0308,0.0806,-0.0396,-0.0,0.0753,-0.0153,-0.0018,-0.0018,-0.02,0.0678,-0.0032,0.0576,-0.0456,-0.0563,-0.0672,-0.2125,0.0079,0.0573,-0.0219,0.0831,-0.0402,0.0081,-0.005,0.0455,0.012,0.0709,0.0085,0.0195,0.0308,-0.0553,0.0282,-0.0147,0.0138,-0.0037,0.0274,-0.0195,0.0065,-0.0537,-0.086,0.0262,-0.0468,0.2292,0.0559,0.0148,-0.0205,0.02,0.0272,-0.0644,-0.1413,0.0102,0.0331,0.0564,-0.0025,-0.0486,-0.038,-0.038,0.0363,0.0006,-0.074,-0.0599,-0.0414,-0.0447,0.0181,-0.0107,0.0036,0.0455,-0.0327,0.1195,-0.033,-0.0248,-0.0942,-0.1111,0.0468,-0.0301,0.0622,0.0445,-0.011,0.0163,-0.0698,0.0228,-0.0173,-0.0321,-0.0244,0.0573,-0.0571,0.0222,0.0704,0.0485,0.0225,0.0808,0.0115,-0.0039,-0.0466,-0.0411,0.007,0.0293,-0.0006,0.0226,-0.0268,0.0078,0.0171,0.066,0.0436,0.038,-0.056,0.0185,0.0173,-0.024,-0.0178,0.0199,0.0526,-0.2982,0.0373,-0.013,0.0225,-0.056,-0.009,0.0244,0.0224,-0.0889,0.0024,0.0001,0.0598,0.0111,-0.0163,0.0095,0.0139,0.0571,-0.0371,0.0275,-0.0621,0.0132,0.065,0.2584,-0.0197,-0.0134,0.0163,0.0127,0.0086,0.0235,0.0029,-0.0188,0.0076,0.0944,-0.0453,0.0103,0.0226,-0.0689,0.0361,0.0055,-0.0415,0.0091,0.0094,-0.06,0.0125,0.1001,-0.0221,0.0136,-0.0211,0.0175,0.0643,0.0004,-0.0021,-0.0157,0.0328,0.012,0.0353,-0.0474,-0.0064,0.0374,0.0004,0.011,-0.1048,0.0255,0.0027,-0.0142]}
{"key":"[Benchmarking and Optimization of Gradient Boosting Decision Tree Algorithms] Gradient boosting decision trees (GBDTs) have seen widespread adoption in academia, industry and competitive data science due to their state-of-the-art performance in many machine learning tasks. One relative downside to these models is the large number of hyper-parameters that they expose to the end-user. To maximize the predictive power of GBDT models, one must either manually tune the hyper-parameters, or utilize automated techniques such as those based on Bayesian optimization. Both of these approaches are time-consuming since they involve repeatably training the model for different sets of hyper-parameters. A number of software GBDT packages have started to offer GPU acceleration which can help to alleviate this problem. In this paper, we consider three such packages: XGBoost, LightGBM and Catboost. Firstly, we evaluate the performance of the GPU acceleration provided by these packages using large-scale datasets with varying shapes, sparsities and learning tasks. Then, we compare the packages in the context of hyper-parameter optimization, both in terms of how quickly each package converges to a good validation score, and in terms of generalization performance.","layer":3,"vector":[-0.0231,0.0403,0.0311,0.0112,0.0476,0.0258,-0.0131,0.0205,0.074,0.0195,0.0002,-0.0512,0.0068,0.0547,0.0013,0.037,0.0111,0.0051,-0.0499,0.0068,0.0356,-0.0288,-0.0458,-0.0979,0.0631,0.0153,-0.0276,-0.0041,-0.0521,-0.2622,0.0137,-0.0519,0.0635,0.0013,-0.007,0.0067,-0.014,0.0347,-0.0326,0.0257,-0.0085,0.0414,-0.0282,-0.0159,-0.0022,-0.0632,-0.0076,-0.0307,-0.053,0.0023,0.0297,-0.0453,0.0205,0.0135,0.0003,0.0208,0.0535,0.0215,0.0468,0.0274,0.0064,0.0231,-0.1547,0.0544,0.0297,0.0399,-0.0282,-0.0376,0.0161,0.0675,-0.0254,0.036,0.0491,0.0313,0.0143,0.0136,0.0219,-0.0091,0.0183,-0.0047,-0.0095,-0.0379,-0.0533,-0.039,0.0264,-0.0504,-0.0229,-0.0192,0.0434,0.0285,0.0074,-0.005,-0.0501,0.014,-0.0777,-0.0193,0.0635,0.043,-0.0746,0.1924,-0.0249,0.0209,-0.0096,-0.0141,0.0202,-0.0174,-0.0281,-0.0272,-0.0372,0.0048,0.0049,-0.0614,0.0375,0.024,-0.0269,0.0511,0.0291,0.0358,-0.042,0.0307,-0.0451,0.0143,0.0594,0.0004,0.0367,-0.0545,0.0094,0.1068,0.0252,0.0422,0.0465,-0.032,-0.0424,-0.0082,0.0462,0.0164,0.0656,0.0087,-0.0002,0.0262,-0.0701,-0.0088,0.0248,-0.062,-0.0275,0.1096,-0.063,0.0659,-0.0691,-0.0295,-0.0048,0.0197,-0.001,-0.056,-0.0125,0.0434,0.0153,0.0499,-0.033,-0.0023,-0.0062,-0.0039,-0.0326,0.102,0.0007,-0.0808,-0.0697,-0.0316,0.0067,0.0266,0.0736,0.0757,-0.0197,0.0202,0.0803,-0.0052,-0.0491,-0.0413,0.0128,-0.006,0.0868,-0.0331,-0.0542,0.0449,0.0641,-0.0494,-0.0087,-0.0494,0.0235,0.0299,-0.0441,-0.0432,-0.0267,-0.0326,-0.0041,-0.0506,-0.0665,-0.0156,0.0569,-0.0253,0.0163,0.0052,-0.0289,0.0292,0.0001,0.0059,-0.041,-0.0085,0.0259,-0.0186,-0.0295,-0.0003,0.0774,-0.0129,-0.046,0.006,0.022,0.06,-0.0287,0.0804,0.036,-0.0198,-0.0421,-0.2173,-0.0525,0.0066,-0.0134,0.0503,-0.0631,0.0317,-0.0149,0.0295,0.0732,0.0684,0.0032,-0.0184,0.0289,-0.016,0.0116,0.0081,0.029,-0.0447,-0.0205,-0.044,0.0266,-0.0061,-0.0933,0.0078,0.0097,0.2348,0.0264,0.0245,-0.0452,0.0275,-0.0098,-0.0097,-0.1023,0.0817,0.0399,0.063,-0.0016,-0.0361,0.0213,-0.0445,0.0467,-0.0036,-0.1239,-0.0729,-0.0169,-0.0029,0.0557,-0.0616,0.0199,0.0234,-0.0348,0.0585,-0.013,0.0172,-0.0716,-0.1114,0.0529,-0.0019,0.0067,0.0233,-0.0725,-0.0029,-0.0497,0.0496,-0.0612,-0.0446,-0.01,-0.0056,-0.0368,-0.0051,0.0734,0.0073,-0.0145,0.0655,0.0191,0.0313,-0.0103,-0.0436,-0.0225,0.0204,-0.0134,0.0549,0.0318,0.0135,-0.0209,0.0722,0.0117,0.0383,-0.0288,0.0025,-0.0032,-0.0352,-0.0016,0.0577,0.0164,-0.2916,0.0322,0.0267,0.0043,-0.0098,0.0117,0.0689,-0.0337,-0.0198,0.008,-0.0091,0.047,0.0499,-0.0308,-0.0443,0.0162,0.0902,-0.0294,0.0175,-0.0257,0.0157,0.0343,0.2239,0.015,0.0469,0.0144,-0.0371,-0.0244,0.0275,-0.0387,-0.0135,0.0047,0.0849,-0.0786,0.0519,0.1381,-0.0254,0.0182,0.0467,-0.0269,0.0418,0.0047,-0.064,-0.0544,0.0908,-0.0398,-0.0167,-0.0269,0.0125,0.0246,-0.0392,0.0348,-0.0373,-0.0436,0.0164,-0.009,-0.0356,-0.0294,-0.0545,-0.0437,-0.0182,-0.0411,-0.0216,-0.0168,0.041]}
{"key":"[Boosting for Online Convex Optimization] We consider the decision-making framework of online convex optimization with a very large number of experts. This setting is ubiquitous in contextual and reinforcement learning problems, where the size of the policy class renders enumeration and search within the policy class infeasible. Instead, we consider generalizing the methodology of online boosting. We define a weak learning algorithm as a mechanism that guarantees multiplicatively approximate regret against a base class of experts. In this access model, we give an efficient boosting algorithm that guarantees near-optimal regret against the convex hull of the base class. We consider both full and partial (a.k.a. bandit) information feedback models. We also give an analogous efficient boosting algorithm for the i.i.d. statistical setting. Our results simultaneously generalize online boosting and gradient boosting guarantees to contextual learning model, online convex optimization and bandit linear optimization settings.","layer":1,"vector":[-0.0544,0.0121,0.0278,-0.0082,0.0035,0.0333,0.0271,0.033,0.0211,-0.0056,0.0183,-0.0265,0.0325,0.0773,0.0205,0.0591,0.0104,0.0308,-0.0487,0.0189,0.0139,-0.0864,-0.0064,-0.0819,0.0398,-0.0008,-0.0395,-0.0646,-0.0189,-0.1845,-0.0128,-0.0435,0.0353,-0.0055,0.0053,0.0055,-0.0047,0.0406,-0.029,0.0599,0.0098,0.0434,-0.0424,-0.0461,-0.0017,-0.0551,0.0073,-0.0086,-0.0437,-0.0421,0.0558,-0.0197,0.0408,0.0128,0.021,0.0326,0.0338,0.0909,0.0339,0.0414,0.0196,0.0336,-0.1562,0.0384,0.0174,0.0376,-0.0469,-0.0092,-0.0276,0.074,0.0131,0.0398,0.0393,0.0523,0.0255,-0.0118,0.0085,-0.0036,0.0045,0.0041,0.0528,-0.0688,-0.063,-0.0064,0.0003,-0.0628,-0.0016,-0.0274,0.0818,0.0361,-0.0308,-0.0086,-0.0062,0.0134,-0.0635,-0.0032,0.0076,0.0172,-0.0719,0.2255,-0.0166,0.0709,0.0303,-0.0109,0.0528,-0.0549,0.0022,-0.0118,0.0004,-0.0318,-0.0333,0.0023,0.0384,0.0057,0.022,0.0159,0.0472,0.059,-0.0165,-0.0102,-0.0507,0.0166,0.0826,-0.0257,0.026,-0.0692,0.0096,0.1254,0.0329,0.0559,0.0464,-0.0589,-0.0548,-0.0631,0.0403,-0.0303,0.0009,0.0095,0.0314,-0.0223,-0.0418,0.0015,0.0083,-0.0974,-0.0341,0.1431,-0.0032,0.0365,-0.0527,-0.071,-0.0024,-0.0359,-0.0529,-0.0403,0.01,0.0317,0.0122,0.0474,-0.045,0.025,-0.0397,-0.0235,0.0164,0.0949,-0.003,-0.0567,-0.0559,0.023,0.0043,0.0088,0.0491,0.0031,-0.0632,0.0474,0.0863,0.048,-0.1111,-0.0358,0.0263,-0.0209,0.0122,-0.031,-0.0446,0.0117,0.0089,-0.0054,0.018,-0.0663,0.0129,0.0318,-0.0412,-0.0102,-0.0368,-0.0309,-0.0142,-0.0413,-0.0072,-0.0114,0.0107,0.0163,0.0143,-0.014,-0.0819,0.0225,0.0359,0.0094,0.0106,-0.0204,0.0515,-0.0091,-0.0328,-0.0146,0.0547,0.0048,-0.0586,0.0009,0.0693,0.0479,-0.0565,0.0303,0.0243,0.0199,-0.0368,-0.2279,-0.0041,-0.0368,-0.0108,0.0377,-0.0655,0.078,-0.0285,0.0284,0.053,0.0603,-0.0503,-0.0236,0.0447,0.0123,0.0545,0.0298,-0.0023,-0.0031,0.0356,-0.0044,0.0363,-0.0172,-0.0717,0.0572,-0.0214,0.2361,0.0417,0.055,-0.0182,0.0496,0.0115,-0.0246,-0.0802,0.0543,0.0044,0.0672,-0.0169,-0.0356,-0.0174,-0.009,-0.0013,-0.0058,-0.0919,-0.0622,-0.0192,-0.0386,0.0521,-0.0745,-0.0031,0.0325,-0.0029,0.0162,-0.0132,0.0129,-0.0568,-0.122,0.0524,-0.0292,0.0382,0.0207,-0.0618,-0.0016,-0.0607,0.0612,-0.015,0.0196,-0.0311,0.0201,-0.0179,-0.0102,0.0601,-0.0212,0.0131,-0.0003,0.029,0.0347,-0.0509,-0.0723,-0.0059,0.0301,-0.0454,0.0234,0.0156,0.0112,-0.0276,0.0689,-0.0048,-0.0043,0.0253,-0.0256,0.0054,-0.0769,0.0255,0.0421,0.0169,-0.3276,0.0581,0.0312,0.0243,-0.0549,0.0497,0.0263,-0.0263,-0.0316,-0.0098,0.0317,0.0689,-0.0043,-0.0049,0.0078,0.0497,0.0655,-0.0333,0.0165,-0.0845,0.0099,0.0482,0.2038,-0.0336,0.0717,0.0217,-0.0309,-0.0319,0.0342,-0.0286,-0.0178,0.0273,0.0758,-0.0693,0.0663,0.0886,-0.021,0.0108,0.0148,-0.0038,-0.0484,0.0274,-0.0235,0.0037,0.0661,0.0089,-0.0272,-0.0451,-0.0416,0.0069,-0.0323,0.0349,0.0157,-0.0153,-0.0032,0.014,-0.0317,-0.0882,-0.0212,-0.0306,-0.0093,-0.0471,-0.017,-0.0182,0.0194]}
{"key":"[Robust Sparse Mean Estimation via Sum of Squares] We study the problem of high-dimensional sparse mean estimation in the presence of an $\\epsilon$-fraction of adversarial outliers. Prior work obtained sample and computationally efficient algorithms for this task for identity-covariance subgaussian distributions. In this work, we develop the first efficient algorithms for robust sparse mean estimation without a priori knowledge of the covariance. For distributions on $\\mathbb R^d$ with \"certifiably bounded\" $t$-th moments and sufficiently light tails, our algorithm achieves error of $O(\\epsilon^{1-1/t})$ with sample complexity $m = (k\\log(d))^{O(t)}/\\epsilon^{2-2/t}$. For the special case of the Gaussian distribution, our algorithm achieves near-optimal error of $\\tilde O(\\epsilon)$ with sample complexity $m = O(k^4 \\mathrm{polylog}(d))/\\epsilon^2$. Our algorithms follow the Sum-of-Squares based, proofs to algorithms approach. We complement our upper bounds with Statistical Query and low-degree polynomial testing lower bounds, providing evidence that the sample-time-error tradeoffs achieved by our algorithms are qualitatively the best possible.","layer":3,"vector":[-0.0237,-0.0554,0.0209,-0.0083,0.0088,0.0166,0.0605,-0.0165,0.0321,-0.0147,0.0179,-0.0411,0.0583,0.0648,-0.0251,0.0369,0.0298,0.0262,-0.0429,0.0397,0.051,-0.0691,-0.0072,-0.0387,0.0469,-0.0168,-0.0224,-0.097,-0.0543,-0.2613,0.0212,-0.0518,0.0626,-0.0309,0.0402,-0.0041,-0.0471,0.051,0.0176,0.0502,0.0134,0.0337,-0.0447,-0.0659,-0.0141,-0.028,-0.0162,0.0157,-0.0451,-0.0758,0.0215,-0.0159,0.0434,0.0125,0.0595,0.0013,-0.0009,0.006,0.0478,0.0681,0.0073,0.0395,-0.1526,0.0248,0.0495,0.0381,-0.0484,-0.0481,0.0149,0.0199,0.0237,0.0267,0.0127,0.0496,-0.0182,-0.0341,-0.0018,-0.0566,-0.033,0.0094,0.0619,-0.0147,-0.0235,0.0391,-0.0595,-0.0745,0.0286,-0.051,0.0556,-0.0287,-0.0243,0.0131,-0.032,0.0498,-0.0618,-0.023,0.0559,-0.0056,0.0039,0.2237,-0.0102,0.0499,0.0395,0.0079,0.0423,-0.0232,-0.0054,-0.0347,-0.0046,0.0007,0.0025,0.0165,0.0346,-0.0511,-0.0081,-0.0508,0.0434,0.0022,-0.0344,-0.027,-0.001,0.0386,0.078,-0.0328,0.0703,-0.0611,0.0254,0.1439,0.0608,0.0677,-0.0049,-0.024,-0.0411,-0.0319,0.0175,0.0232,-0.0147,0.0599,0.0363,-0.0023,-0.0221,-0.054,0.0302,-0.0401,-0.0204,0.1227,-0.0446,0.0138,-0.0142,-0.0834,-0.009,0.0277,-0.0129,-0.0205,0.0334,0.0319,0.0212,0.0728,-0.0497,0.0069,-0.0325,-0.0783,0.0253,0.0916,0.0012,-0.0541,-0.0245,0.0405,0.0173,0.0281,0.0288,0.0135,-0.0077,0.0573,0.0747,0.0158,-0.0408,-0.006,-0.0017,0.0105,-0.0333,-0.0259,-0.0247,0.0571,0.0252,-0.0201,0.0236,-0.0203,0.0023,0.0336,-0.0707,-0.0146,-0.058,-0.0122,-0.0383,-0.0377,0.0157,-0.0055,-0.0171,-0.0517,-0.0036,-0.0109,-0.0502,0.012,0.037,0.0211,0.0063,-0.0409,0.0264,0.041,-0.0153,-0.0299,0.0937,-0.0094,-0.0486,0.0209,0.0373,0.0505,-0.0337,0.0517,0.0085,-0.0794,-0.0846,-0.2011,-0.0352,-0.0267,0.0093,0.0049,-0.0965,0.0297,-0.0254,0.0971,0.0811,0.0316,0.0175,-0.0139,0.0385,0.0098,0.0849,-0.0044,0.0198,-0.0151,-0.0259,-0.0492,0.0396,-0.0044,-0.0471,0.0492,0.0143,0.2133,0.0384,-0.0158,-0.0663,0.009,0.0468,-0.0146,-0.0635,0.0421,0.024,0.0639,0.0359,-0.0242,-0.0179,-0.0488,-0.0134,0.0192,-0.0691,-0.0565,0.002,-0.0265,-0.006,-0.0832,0.0262,0.0314,0.0003,0.0913,-0.0623,0.0221,-0.051,-0.1322,0.0137,-0.0129,0.0398,0.0163,-0.0575,0.0105,-0.0517,0.0711,-0.0011,-0.0005,-0.0693,0.0372,-0.0426,-0.0042,0.0449,-0.0145,0.0199,0.0485,-0.0056,0.0468,-0.0497,-0.0762,-0.0386,0.0744,-0.0201,0.0025,-0.0263,0.0523,0.0401,0.0698,0.0202,0.0129,-0.0249,-0.0288,0.0147,-0.0118,-0.0603,0.0168,0.0347,-0.3068,0.0209,0.0127,0.0083,-0.0194,-0.0142,0.0298,-0.0,-0.048,0.0226,-0.0303,0.0548,0.0273,-0.0505,-0.0171,0.0361,0.0269,-0.0601,0.0495,-0.0919,0.0268,0.003,0.1797,-0.0336,-0.0283,0.0293,-0.0095,0.0247,0.0142,-0.0576,0.0291,0.0087,0.0894,-0.0199,0.0288,0.0706,-0.0367,0.0432,0.0103,-0.0213,0.0049,-0.0104,-0.0128,0.0348,0.1062,-0.0526,-0.022,-0.0042,0.0352,0.004,-0.0258,-0.0103,0.0473,0.0338,-0.0019,0.0333,-0.0587,-0.0205,-0.0257,-0.0376,-0.0021,-0.0335,-0.0598,0.0128,0.0144]}
{"key":"[Adversarial Sampling and Training for Semi-Supervised Information Retrieval] Ad-hoc retrieval models with implicit feedback often have problems, e.g., the imbalanced classes in the data set. Too few clicked documents may hurt generalization ability of the models, whereas too many non-clicked documents may harm effectiveness of the models and efficiency of training. In addition, recent neural network-based models are vulnerable to adversarial examples due to the linear nature in them. To solve the problems at the same time, we propose an adversarial sampling and training framework to learn ad-hoc retrieval models with implicit feedback. Our key idea is (i) to augment clicked examples by adversarial training for better generalization and (ii) to obtain very informational non-clicked examples by adversarial sampling and training. Experiments are performed on benchmark data sets for common ad-hoc retrieval tasks such as Web search, item recommendation, and question answering. Experimental results indicate that the proposed approaches significantly outperform strong baselines especially for high-ranked documents, and they outperform IRGAN in NDCG@5 using only 5% of labeled data for the Web search task.","layer":2,"vector":[-0.0316,0.0036,0.0128,-0.001,0.0231,0.0294,0.0045,0.0024,-0.0275,-0.0001,-0.0225,-0.0182,0.0629,0.1,-0.002,0.03,0.0333,0.0152,-0.0437,0.0769,-0.0015,-0.0105,0.02,-0.0159,0.0183,0.0079,-0.0393,-0.0473,-0.0578,-0.2378,0.0224,-0.0343,0.0743,0.0075,0.0236,-0.007,-0.0327,0.057,0.015,0.0735,0.0024,0.0223,-0.0209,-0.035,0.0003,0.0025,-0.0365,-0.0102,-0.0114,-0.0583,0.0322,-0.0499,-0.0066,-0.0253,0.0381,0.0342,0.0471,0.0236,0.0323,0.0496,0.0298,0.0332,-0.1601,0.0774,0.0416,0.0365,-0.0505,-0.0211,0.0141,0.0572,0.0015,0.0548,0.0289,0.0408,-0.0004,0.0148,-0.001,-0.0409,-0.0162,0.0083,0.0261,-0.005,-0.0593,-0.0117,0.016,-0.0697,-0.0003,-0.0111,0.0752,-0.016,0.0161,0.0018,-0.0301,0.0651,-0.0596,-0.0336,0.0494,0.0634,-0.0776,0.2177,-0.078,0.0632,0.0617,-0.0641,0.0106,-0.0231,-0.0306,-0.0511,-0.0572,0.0058,-0.042,0.0198,0.0735,-0.0584,0.0876,0.0002,0.0695,0.0394,-0.0217,-0.0398,-0.0386,0.0061,0.0478,-0.0303,0.0319,-0.0404,0.0222,0.1292,0.0381,0.0201,-0.0066,-0.0596,-0.0621,-0.0156,0.0021,0.0073,-0.0411,-0.0184,0.0009,0.0178,-0.0205,-0.0653,0.004,-0.0536,-0.0607,0.143,-0.0303,0.0119,-0.0597,-0.0366,-0.0019,0.0625,-0.0099,-0.0142,-0.0348,0.0386,0.0187,0.0592,-0.0273,0.0068,-0.0355,-0.0361,-0.0165,0.1204,-0.0044,-0.0679,-0.0461,0.0327,-0.0383,-0.0379,0.022,0.0006,-0.0375,0.0237,0.0777,0.0215,-0.0884,-0.0231,-0.0204,0.0076,0.0176,-0.0465,0.0209,0.0611,0.0216,-0.0132,0.0248,-0.0581,-0.0062,0.0517,-0.0595,-0.0223,-0.0589,-0.0367,-0.0223,-0.0297,0.0116,0.0053,0.0032,-0.0009,-0.0009,-0.0114,-0.0387,-0.0128,0.0065,0.0552,-0.0148,-0.045,0.0282,0.0138,-0.0236,-0.0124,-0.0039,-0.0256,-0.0301,-0.0434,-0.0218,0.0354,-0.0217,0.0552,0.02,-0.0474,-0.0089,-0.2197,-0.0073,-0.0236,0.0015,0.0605,-0.0969,0.0506,0.0089,0.055,0.0927,0.009,0.0002,-0.0258,0.0435,-0.0109,0.0823,0.0383,0.0232,0.0165,0.03,-0.0014,0.0076,0.0017,-0.0949,0.0448,-0.0277,0.2002,0.0564,0.0272,-0.0291,0.0352,0.0277,-0.0248,-0.1121,0.0718,0.0217,0.0456,0.0031,-0.0617,-0.0038,-0.0225,0.0402,0.0259,-0.0975,-0.0515,-0.0507,-0.028,0.0195,-0.0415,0.062,0.0148,-0.0126,0.0461,0.0003,-0.0414,-0.0396,-0.0783,-0.0119,-0.0597,0.0248,-0.0161,-0.0611,0.041,-0.0936,0.0282,0.0195,-0.036,0.0101,0.0473,-0.0363,-0.0264,0.0533,-0.0007,0.0749,0.0058,0.0146,0.0315,-0.0196,-0.0373,-0.0522,0.0569,-0.0178,0.0543,-0.023,0.0393,0.0074,0.0783,-0.0088,0.0532,-0.0203,-0.012,0.0246,-0.0808,-0.0235,0.0373,0.0181,-0.2869,0.0311,0.0549,0.0987,-0.0628,0.015,0.0197,-0.0007,-0.0357,0.0222,-0.0034,0.0306,0.0196,-0.0538,-0.0078,0.0321,0.0422,-0.064,0.0269,-0.0177,-0.0182,0.0692,0.2145,0.0061,0.034,0.0043,-0.0266,0.0195,0.0164,-0.0409,0.0337,-0.0003,0.0959,-0.0198,0.0168,0.0848,-0.0152,0.0342,0.0193,-0.0119,-0.048,-0.0033,-0.06,-0.0125,0.075,0.0021,-0.0309,-0.0358,-0.0239,0.021,-0.0413,-0.027,0.0009,-0.0014,0.0654,0.04,-0.0776,-0.0522,-0.0055,-0.0395,0.0461,-0.0289,-0.0254,-0.0239,-0.0015]}
{"key":"[Towards Scalable and Privacy-Preserving Deep Neural Network via Algorithmic-Cryptographic Co-design] Deep Neural Networks (DNNs) have achieved remarkable progress in various real-world applications, especially when abundant training data are provided. However, data isolation has become a serious problem currently. Existing works build privacy preserving DNN models from either algorithmic perspective or cryptographic perspective. The former mainly splits the DNN computation graph between data holders or between data holders and server, which demonstrates good scalability but suffers from accuracy loss and potential privacy risks. In contrast, the latter leverages time-consuming cryptographic techniques, which has strong privacy guarantee but poor scalability. In this paper, we propose SPNN - a Scalable and Privacy-preserving deep Neural Network learning framework, from algorithmic-cryptographic co-perspective. From algorithmic perspective, we split the computation graph of DNN models into two parts, i.e., the private data related computations that are performed by data holders and the rest heavy computations that are delegated to a server with high computation ability. From cryptographic perspective, we propose using two types of cryptographic techniques, i.e., secret sharing and homomorphic encryption, for the isolated data holders to conduct private data related computations privately and cooperatively. Furthermore, we implement SPNN in a decentralized setting and introduce user-friendly APIs. Experimental results conducted on real-world datasets demonstrate the superiority of SPNN.","layer":2,"vector":[-0.0291,-0.0201,-0.0083,-0.0537,0.0155,0.029,0.0538,-0.0113,0.0841,-0.0116,0.0052,-0.0417,0.0686,0.0713,-0.0039,-0.0008,0.0002,0.0202,-0.0417,0.0319,0.016,-0.0163,-0.0056,-0.0764,-0.0256,0.0088,-0.0379,-0.0218,-0.0598,-0.2148,0.0429,-0.0774,0.0701,-0.029,0.0069,-0.0255,-0.0064,0.0442,-0.0544,0.0394,-0.0064,0.0221,0.0062,-0.0493,-0.0014,-0.0,-0.0242,-0.0328,-0.0059,-0.0334,0.0649,0.0053,-0.0035,0.0781,0.0466,0.0316,0.0696,0.0367,0.0209,0.0324,0.0098,0.0671,-0.1464,0.0267,0.0166,0.1042,-0.0212,-0.0467,0.0415,0.006,0.009,0.0708,0.0339,0.0082,0.047,0.0515,-0.0189,0.0063,0.0035,0.0027,-0.0213,-0.0124,-0.0398,-0.0101,-0.0677,-0.0199,0.0115,-0.0458,-0.0063,-0.0547,-0.0193,0.0008,0.0011,-0.0021,-0.0513,-0.0278,-0.0013,0.0421,-0.0892,0.201,-0.0494,0.0505,0.0408,-0.0474,0.0252,-0.0182,-0.0008,-0.0061,-0.025,0.0081,0.0004,-0.017,0.0203,-0.0037,-0.0062,0.0035,0.0527,0.0366,-0.0096,-0.0317,-0.0016,0.0162,0.0413,0.0307,0.043,-0.0416,-0.0018,0.1496,0.012,0.0524,0.054,-0.0106,-0.0434,-0.0294,0.0405,-0.0069,-0.0301,-0.0137,0.0181,-0.0357,-0.0743,0.0027,0.0377,-0.0636,-0.0192,0.112,0.0106,0.0273,-0.0177,-0.054,-0.0638,0.0123,-0.025,-0.0346,0.0293,0.0186,0.0173,0.0659,-0.0717,0.0504,-0.0164,-0.0172,-0.0378,0.1458,0.0383,-0.1373,0.0266,0.0125,-0.0004,-0.0336,0.0305,0.0253,-0.0474,0.0165,0.0498,0.0352,-0.0483,-0.023,-0.0193,0.0416,-0.0264,-0.0529,-0.0117,0.0486,0.0327,-0.0488,0.016,-0.0699,-0.01,0.0549,-0.0879,0.0726,-0.0766,-0.028,-0.0514,-0.0517,0.0014,-0.032,-0.0046,0.0017,-0.0103,-0.0294,-0.0297,-0.0094,-0.0309,0.0386,0.0009,-0.0007,0.0464,0.0424,-0.0445,-0.0064,0.0617,-0.0486,-0.0496,0.0179,0.02,0.0299,-0.0184,0.0339,0.0454,-0.078,-0.0606,-0.1953,-0.0191,-0.0068,-0.0489,0.094,-0.0819,0.0541,-0.0131,0.0413,0.0648,0.0772,0.028,-0.0373,0.0286,-0.0405,0.0849,0.0564,0.0372,-0.0276,-0.0197,-0.0058,0.0339,0.0109,-0.0823,0.0387,0.0386,0.2232,-0.0008,0.0795,-0.0335,-0.0084,0.0164,-0.0057,-0.1444,0.0045,-0.0316,0.0364,-0.0107,-0.0359,-0.0601,-0.0273,0.022,0.0218,-0.0929,-0.0055,-0.0444,-0.0278,0.0266,-0.0486,0.0307,0.069,-0.0023,0.0551,-0.0259,0.0046,-0.0617,-0.0642,0.0295,-0.037,0.0524,-0.0291,-0.0361,-0.0184,-0.0344,0.0637,-0.0169,-0.0299,-0.0486,0.0232,0.0224,-0.0368,0.0758,0.0321,0.0121,0.0581,-0.009,0.0436,-0.0034,-0.0275,-0.0133,0.0781,0.005,0.0317,0.0663,0.0303,0.0354,0.0911,0.0167,0.0116,-0.0297,0.0061,-0.009,-0.069,-0.0656,0.0365,0.0098,-0.3067,0.035,-0.0189,0.0433,-0.0471,0.0151,0.0355,0.0179,-0.0698,-0.0016,0.0216,0.0636,0.0489,-0.0174,-0.0002,0.0231,0.0967,-0.0094,0.0544,-0.0107,0.0108,0.0597,0.1865,-0.0468,0.0012,-0.0005,0.0199,0.0332,0.0398,-0.008,-0.0353,0.0037,0.0504,-0.0449,-0.001,0.0703,-0.0118,-0.0194,0.0466,0.0046,0.007,-0.0135,-0.0268,0.0097,0.0732,-0.0253,-0.0628,-0.0502,0.0039,0.0138,-0.0349,-0.0278,-0.0271,-0.0092,0.015,0.0213,-0.0512,-0.0386,-0.0545,-0.0058,-0.0006,-0.0764,-0.0207,-0.0049,-0.0315]}
{"key":"[Enel: Context-Aware Dynamic Scaling of Distributed Dataflow Jobs using Graph Propagation] Distributed dataflow systems like Spark and Flink enable the use of clusters for scalable data analytics. While runtime prediction models can be used to initially select appropriate cluster resources given target runtimes, the actual runtime performance of dataflow jobs depends on several factors and varies over time. Yet, in many situations, dynamic scaling can be used to meet formulated runtime targets despite significant performance variance. This paper presents Enel, a novel dynamic scaling approach that uses message propagation on an attributed graph to model dataflow jobs and, thus, allows for deriving effective rescaling decisions. For this, Enel incorporates descriptive properties that capture the respective execution context, considers statistics from individual dataflow tasks, and propagates predictions through the job graph to eventually find an optimized new scale-out. Our evaluation of Enel with four iterative Spark jobs shows that our approach is able to identify effective rescaling actions, reacting for instance to node failures, and can be reused across different execution contexts.","layer":1,"vector":[-0.0457,-0.0374,0.0052,-0.0449,0.0339,0.0171,0.0222,0.0292,-0.0073,-0.0365,0.0137,-0.0534,0.0425,0.0177,0.0158,0.0243,-0.0094,0.042,-0.0513,-0.0341,0.0359,-0.0639,-0.0164,-0.0803,0.0733,0.0729,-0.0467,-0.0436,-0.073,-0.2387,0.0054,-0.0524,0.0285,-0.0161,0.0123,-0.0381,0.0015,0.0618,-0.0346,0.044,-0.0062,0.0387,-0.0586,-0.0132,-0.0581,-0.0456,-0.028,-0.0046,-0.0584,-0.0251,0.0041,-0.0533,0.0255,0.0335,0.0237,0.069,0.0611,0.0162,0.0514,-0.0098,0.0455,0.0118,-0.1376,0.0613,0.0588,0.0186,-0.0269,-0.0292,0.0039,0.0236,-0.0224,0.0168,-0.0045,0.0679,0.0166,0.0101,-0.0299,-0.0274,0.0152,0.051,0.0103,-0.063,-0.0215,0.0096,-0.0317,-0.0044,-0.0012,0.0096,0.0904,0.001,-0.0374,-0.0032,-0.0212,-0.0197,-0.0693,-0.0387,0.0204,-0.0194,-0.0389,0.2244,-0.0496,0.0698,0.0272,0.0005,0.0263,-0.0716,0.0214,-0.0189,-0.0086,-0.0019,-0.0015,-0.0409,0.0269,-0.0624,0.0684,0.0081,0.0318,-0.0122,-0.0311,0.0154,-0.019,0.0454,0.0327,-0.0492,0.0631,-0.038,0.0857,0.151,0.0024,-0.019,0.0059,0.0072,-0.0628,-0.0162,0.0069,0.0127,0.0472,-0.0203,0.0163,-0.0144,-0.0472,-0.0249,0.0038,-0.0937,-0.0174,0.1593,0.0243,0.0425,-0.0424,-0.044,-0.061,0.0316,-0.0356,-0.001,-0.0261,0.0581,-0.0085,0.0591,-0.0641,0.0002,0.0007,-0.0061,-0.034,0.0746,0.006,-0.0857,-0.0039,0.0389,0.0218,0.0018,-0.0189,-0.0246,-0.0249,-0.0084,0.0297,0.0196,-0.0404,-0.0299,0.0009,0.0333,0.0627,0.0083,0.0259,0.0223,0.0029,-0.0592,0.0037,0.0138,0.0227,0.0231,-0.0364,0.0341,0.0071,0.0168,-0.0211,-0.0341,0.025,0.0165,-0.0029,-0.005,0.0353,0.0497,-0.0357,0.0473,-0.0623,0.0109,-0.0724,-0.0179,0.0021,-0.0075,-0.0348,-0.0308,0.0491,-0.0003,-0.0165,0.0113,0.0601,0.0327,0.0127,0.0637,0.0431,-0.0244,-0.0674,-0.228,-0.016,-0.0049,-0.0401,0.0906,-0.0001,0.0324,-0.0111,0.0339,0.0438,0.0551,-0.0204,-0.0234,-0.0055,-0.0004,0.0331,0.0106,0.0405,-0.0362,-0.003,0.0561,-0.0028,-0.029,-0.0847,0.0074,0.0509,0.2273,-0.015,0.0269,-0.0556,0.0277,0.0045,-0.0607,-0.0783,0.0522,0.0604,0.0225,-0.0157,-0.0295,-0.009,-0.0321,0.0357,-0.0281,-0.1194,-0.028,-0.0252,-0.0002,0.0301,-0.0687,0.0202,0.0396,-0.0376,0.0718,0.0191,0.0271,-0.0619,-0.0536,0.0583,-0.0174,0.0053,0.0044,-0.0304,-0.0181,-0.0243,0.0793,-0.0062,0.0134,-0.0333,-0.032,-0.0629,0.0035,0.0619,-0.0497,-0.0195,0.0872,0.0354,-0.0207,-0.0522,-0.043,-0.0344,0.0712,-0.048,0.0862,0.0513,0.0436,0.0271,0.0707,0.0037,0.0429,-0.0461,0.0069,-0.0163,-0.0665,-0.0228,0.0117,-0.006,-0.3007,-0.0068,-0.0172,-0.0096,-0.0536,0.0268,0.0503,0.0525,-0.0184,-0.008,0.0274,0.0422,0.0023,0.0135,-0.0233,0.0597,0.0822,-0.0315,-0.0207,-0.065,0.0466,0.0224,0.211,0.0098,0.0029,0.05,-0.0226,0.0007,0.0221,0.0192,0.0218,-0.0129,0.057,-0.0481,0.0614,0.0128,-0.0009,0.0521,0.1061,-0.0176,0.0173,-0.0016,-0.012,-0.0003,0.0491,-0.0219,-0.0081,-0.1103,0.0207,0.0618,-0.0259,0.0043,0.0034,0.018,0.0043,0.0649,-0.0393,-0.0179,-0.0718,-0.0691,0.013,-0.0611,-0.0078,-0.0199,0.0033]}
{"key":"[Learning Multiple Tasks with Multilinear Relationship Networks] Deep networks trained on large-scale data can learn transferable features to promote learning multiple tasks. Since deep features eventually transition from general to specific along deep networks, a fundamental problem of multi-task learning is how to exploit the task relatedness underlying parameter tensors and improve feature transferability in the multiple task-specific layers. This paper presents Multilinear Relationship Networks (MRN) that discover the task relationships based on novel tensor normal priors over parameter tensors of multiple task-specific layers in deep convolutional networks. By jointly learning transferable features and multilinear relationships of tasks and features, MRN is able to alleviate the dilemma of negative-transfer in the feature layers and under-transfer in the classifier layer. Experiments show that MRN yields state-of-the-art results on three multi-task learning datasets.","layer":1,"vector":[-0.0194,0.0012,0.017,-0.0203,-0.0021,0.0223,0.0435,0.0106,0.0165,-0.017,-0.0035,-0.047,0.0477,0.079,0.0517,-0.0044,0.0274,0.0777,-0.1019,0.0051,0.0524,-0.0362,-0.0171,-0.0388,0.0337,-0.0078,-0.037,-0.0499,-0.0653,-0.2548,0.0105,-0.0254,0.032,-0.0024,0.0195,-0.016,-0.0415,0.0393,-0.0363,0.0236,0.0157,0.035,-0.0277,-0.0761,-0.025,-0.0482,0.0072,-0.0305,0.0087,-0.0678,0.0352,-0.08,0.0138,0.0485,0.0463,0.0078,0.0348,-0.0005,0.0626,0.0675,0.0321,0.0216,-0.1857,0.0448,0.0438,0.0116,-0.0664,-0.0149,-0.0114,0.0641,-0.0439,0.0149,-0.0027,0.0342,0.0327,0.0147,0.0311,-0.0281,-0.0226,0.0063,0.0368,-0.0152,0.0062,-0.0274,-0.0225,0.0101,0.0305,-0.0682,0.0249,-0.0088,-0.0716,-0.0656,-0.0313,0.0468,-0.0365,-0.015,0.0639,0.0331,-0.0465,0.176,-0.078,0.0149,0.0428,-0.0067,0.0021,0.0111,-0.0211,-0.0307,-0.0176,0.0362,-0.0342,-0.0337,0.0124,-0.0607,0.0735,0.03,0.0628,0.0439,-0.0238,-0.0371,-0.0268,-0.0064,0.0419,-0.0162,-0.0004,-0.0811,-0.0163,0.1223,0.0507,0.0002,0.0542,-0.0352,-0.063,-0.0111,0.0103,0.0143,-0.0035,-0.0281,-0.0056,0.0009,-0.0338,-0.0489,0.0398,-0.0768,-0.0568,0.1451,-0.0189,-0.0113,-0.0395,0.0157,-0.0253,0.0345,0.0162,-0.023,0.0187,0.0167,0.0482,-0.0109,-0.0857,0.0322,0.0188,-0.057,-0.0584,0.0774,0.0521,-0.1083,-0.0403,-0.0252,-0.0185,0.0216,0.0411,0.0423,-0.026,0.0151,0.0901,0.0227,-0.0843,-0.0019,-0.0014,-0.0085,0.0535,-0.0378,-0.0059,0.0179,0.0526,-0.0046,0.0115,-0.0127,0.0079,0.0195,-0.0305,0.0397,-0.0422,0.0122,-0.0235,-0.0075,-0.0261,-0.0035,-0.0254,-0.0035,-0.0082,0.0038,-0.0283,0.0215,-0.0213,0.0149,-0.0463,0.0042,0.0182,0.0508,0.0008,-0.005,0.0374,-0.0556,-0.023,-0.0225,-0.0036,0.0654,0.0036,0.0489,0.054,-0.0233,-0.0231,-0.2642,-0.0114,0.0048,-0.0232,0.0556,-0.0281,0.0779,0.0306,0.0572,0.1014,0.0974,0.0014,-0.0276,0.0213,-0.0184,0.0809,0.0207,0.0062,0.003,-0.0018,-0.0204,0.0233,0.0148,-0.091,0.0442,0.0149,0.2083,0.0016,-0.0141,-0.0133,0.0406,0.0656,-0.0262,-0.0955,0.0725,0.0156,0.1044,-0.0397,-0.0382,-0.0268,-0.0497,0.0097,0.0217,-0.11,-0.0281,-0.0281,-0.0302,0.0154,-0.024,0.0134,0.0112,-0.0045,0.0153,-0.0332,-0.0428,-0.0303,-0.1154,0.0366,-0.0613,-0.0094,0.0262,-0.0173,-0.0104,-0.05,0.0679,0.0373,-0.0331,-0.0023,0.0389,0.0086,-0.0247,0.0495,0.0387,0.0049,0.0794,-0.0237,0.0444,0.0063,-0.0158,-0.0087,0.0775,-0.0489,0.0001,0.0406,0.0403,0.0188,0.0713,-0.0157,0.0398,-0.0267,-0.0095,-0.0264,-0.0416,-0.0411,0.0662,-0.0122,-0.2967,-0.002,0.0241,0.0271,-0.0332,0.0081,0.0378,0.0261,-0.029,-0.0042,0.0092,0.0334,0.0477,-0.021,0.0008,0.0284,0.0579,-0.0137,0.0134,-0.0114,-0.0166,0.0436,0.1953,-0.0247,0.0262,-0.0055,-0.0144,-0.0177,0.0209,0.0147,0.0262,0.0188,0.0712,-0.0329,0.0201,0.1051,0.0093,0.0233,0.02,-0.0012,-0.0221,-0.0013,-0.0528,-0.103,0.1055,-0.0152,-0.0018,-0.042,-0.0432,0.0179,0.0408,-0.0111,-0.0035,-0.0066,0.0708,-0.012,0.0121,-0.034,-0.0461,-0.0165,0.0055,-0.0326,-0.0246,-0.0722,-0.0384]}
{"key":"[Stimuli-Sensitive Hawkes Processes for Personalized Student Procrastination Modeling] Student procrastination and cramming for deadlines are major challenges in online learning environments, with negative educational and well-being side effects. Modeling student activities in continuous time and predicting their next study time are important problems that can help in creating personalized timely interventions to mitigate these challenges. However, previous attempts on dynamic modeling of student procrastination suffer from major issues: they are unable to predict the next activity times, cannot deal with missing activity history, are not personalized, and disregard important course properties, such as assignment deadlines, that are essential in explaining the cramming behavior. To resolve these problems, we introduce a new personalized stimuli-sensitive Hawkes process model (SSHP), by jointly modeling all student-assignment pairs and utilizing their similarities, to predict students' next activity times even when there are no historical observations. Unlike regular point processes that assume a constant external triggering effect from the environment, we model three dynamic types of external stimuli, according to assignment availabilities, assignment deadlines, and each student's time management habits. Our experiments on two synthetic datasets and two real-world datasets show a superior performance of future activity prediction, comparing with state-of-the-art models. Moreover, we show that our model achieves a flexible and accurate parameterization of activity intensities in students.","layer":2,"vector":[-0.0371,0.0095,0.0566,-0.0202,0.0103,0.0176,0.0432,-0.0104,0.0478,-0.0492,0.0325,-0.0261,0.0336,0.0518,0.0061,0.0083,-0.0348,0.0363,-0.0141,-0.0057,0.0151,-0.0165,-0.0157,-0.0316,0.0056,0.0244,-0.0276,-0.0388,-0.0529,-0.2396,0.0045,-0.0542,0.0512,0.0279,0.0235,-0.0567,-0.0579,0.043,-0.0337,0.0484,0.0259,0.0104,-0.0471,-0.0124,-0.0314,-0.055,-0.0323,-0.0561,-0.0147,-0.0474,0.0058,-0.0507,0.0169,0.022,0.0225,0.0543,0.0649,0.0441,0.0334,0.0195,0.0039,0.0267,-0.1654,0.065,0.0466,0.032,-0.0464,-0.023,-0.0046,0.0471,-0.0782,0.0649,0.0193,0.093,0.0321,-0.0084,0.0076,-0.0111,0.0107,-0.0182,-0.0093,-0.006,-0.025,-0.0027,0.0202,-0.0446,0.0196,-0.0436,0.0861,-0.0169,-0.0733,-0.0448,-0.0334,0.0213,-0.0809,-0.0362,0.0273,0.0709,-0.0551,0.2034,-0.1033,0.0778,0.0664,-0.0295,0.0409,-0.0275,0.0169,-0.0007,-0.0307,0.0008,0.0268,-0.0358,0.0627,-0.0694,0.0555,0.0326,0.0905,0.015,0.029,-0.017,-0.0193,0.0352,0.035,-0.0449,0.0239,-0.0826,0.061,0.1222,0.0115,-0.0034,0.0159,-0.0169,-0.0363,-0.028,0.005,0.0018,0.0167,-0.0349,0.0508,-0.0469,-0.0268,0.0399,0.0053,-0.0868,-0.0719,0.1369,0.0416,0.0534,-0.0204,-0.0134,-0.0587,0.0095,-0.0298,-0.0344,0.002,0.0298,0.0569,0.0153,-0.0247,0.0198,-0.058,-0.0694,0.0199,0.0506,0.0359,-0.0233,0.0143,0.0276,0.0219,-0.0289,0.0356,0.0327,-0.0257,0.026,0.1027,0.037,-0.0082,0.0064,0.0072,0.0147,0.0661,-0.0742,-0.0249,0.0236,0.0253,-0.0351,0.026,-0.0483,0.0539,-0.008,-0.0312,-0.0253,-0.0306,0.0106,-0.033,-0.0302,0.0145,-0.0278,-0.0133,-0.0335,0.0235,0.0104,-0.0482,-0.0002,0.0463,0.0264,-0.0138,0.0251,0.0867,-0.0092,-0.0036,-0.0314,0.0288,-0.0398,-0.0419,0.0039,0.0231,0.0351,0.0317,0.0636,0.0273,-0.0265,-0.0037,-0.2091,0.0071,0.0322,-0.0341,0.0473,-0.0414,0.0279,-0.0291,0.0483,0.0475,0.0439,-0.0337,0.0013,0.0278,0.0148,0.0165,0.0177,0.0208,-0.0202,-0.0367,-0.0079,0.0086,-0.0227,-0.08,0.0004,-0.0068,0.1978,0.0542,0.0225,-0.0361,0.058,0.0157,-0.0196,-0.1227,0.0124,0.0186,0.0902,-0.0305,-0.0229,-0.0577,-0.0198,0.0427,0.0089,-0.0841,-0.0674,-0.0301,-0.0026,0.0215,-0.0644,0.0065,0.0445,-0.0359,0.0727,-0.0494,-0.0212,-0.0299,-0.0841,0.0532,-0.0578,0.0215,-0.0346,-0.0578,0.0099,-0.0385,0.0243,0.0141,-0.0373,-0.0784,-0.0253,-0.0431,-0.0105,0.0941,-0.0046,-0.0026,0.0235,-0.0294,0.0144,-0.027,-0.0212,-0.0144,0.0683,-0.065,0.0314,0.0274,0.0206,-0.0349,0.0567,0.0259,0.0225,-0.0273,0.0226,0.0088,-0.0555,0.0375,0.0369,-0.0105,-0.3128,0.0052,-0.0045,0.0386,-0.0004,-0.005,-0.0105,0.0496,-0.0624,-0.0056,-0.0054,0.0843,0.0329,-0.0055,0.0021,0.0308,0.1052,-0.0579,0.0231,-0.0753,0.0279,0.0231,0.2223,-0.01,0.0782,0.0226,0.0072,0.0031,0.0906,-0.0306,0.021,0.0063,0.08,-0.0577,0.0165,0.0241,-0.0474,0.0641,-0.0014,-0.0237,-0.0059,-0.0312,-0.0468,-0.024,0.1028,0.019,-0.0799,-0.0608,-0.0323,0.0158,-0.0178,-0.023,-0.0401,0.0151,0.0329,0.0405,-0.0141,-0.0291,-0.017,-0.0521,0.0018,0.0016,0.0141,-0.0057,0.0015]}
{"key":"[Label-Only Membership Inference Attacks] Membership inference attacks are one of the simplest forms of privacy leakage for machine learning models: given a data point and model, determine whether the point was used to train the model. Existing membership inference attacks exploit models' abnormal confidence when queried on their training data. These attacks do not apply if the adversary only gets access to models' predicted labels, without a confidence measure. In this paper, we introduce label-only membership inference attacks. Instead of relying on confidence scores, our attacks evaluate the robustness of a model's predicted labels under perturbations to obtain a fine-grained membership signal. These perturbations include common data augmentations or adversarial examples. We empirically show that our label-only membership inference attacks perform on par with prior attacks that required access to model confidences. We further demonstrate that label-only attacks break multiple defenses against membership inference attacks that (implicitly or explicitly) rely on a phenomenon we call confidence masking. These defenses modify a model's confidence scores in order to thwart attacks, but leave the model's predicted labels unchanged. Our label-only attacks demonstrate that confidence-masking is not a viable defense strategy against membership inference. Finally, we investigate worst-case label-only attacks, that infer membership for a small number of outlier data points. We show that label-only attacks also match confidence-based attacks in this setting. We find that training models with differential privacy and (strong) L2 regularization are the only known defense strategies that successfully prevents all attacks. This remains true even when the differential privacy budget is too high to offer meaningful provable guarantees.","layer":1,"vector":[-0.0046,-0.0661,-0.0576,0.0214,0.0223,0.0092,0.0722,-0.001,0.0179,-0.0328,0.0398,-0.0433,0.035,0.0248,0.0183,0.0044,-0.0153,0.0512,-0.0511,0.0217,0.0052,-0.0612,0.0046,-0.025,0.0269,0.032,-0.0412,-0.0208,-0.0508,-0.241,0.0336,-0.0769,0.0342,-0.0005,0.0433,-0.038,-0.0301,0.0214,-0.0096,0.0485,0.0362,0.0205,-0.0326,-0.0835,-0.0015,-0.0079,-0.029,0.0101,-0.037,-0.0324,0.032,-0.0032,0.0028,0.0449,0.0333,0.0291,0.0499,0.0076,0.0416,0.0852,-0.003,0.0326,-0.1812,0.0424,0.0286,0.0403,-0.0414,-0.0231,0.015,0.043,0.0188,0.0555,0.0026,0.0465,0.0197,0.0056,0.0112,-0.0039,-0.0233,0.0266,-0.0019,-0.0202,-0.0179,-0.0111,-0.0149,-0.08,0.0297,-0.0546,0.0429,0.0146,-0.0141,-0.0206,0.0034,0.0558,-0.0228,-0.0276,0.034,0.0318,-0.0903,0.1986,-0.0837,0.0424,-0.0005,-0.0367,0.0467,-0.0234,-0.0107,-0.0678,-0.0141,-0.018,0.0098,-0.0021,0.0144,-0.0287,0.036,-0.0163,0.1129,0.0196,-0.0314,-0.0092,-0.0192,0.0023,0.0481,0.0321,0.0094,-0.0539,0.0404,0.1496,0.0216,-0.007,0.0412,-0.0696,-0.0182,0.0019,0.0287,0.0243,0.0254,0.0469,0.0804,0.0119,-0.0417,-0.0089,0.0481,-0.0431,-0.0405,0.157,0.0181,0.0322,-0.0466,-0.0079,0.021,0.0323,-0.0298,-0.0107,0.0431,0.0043,0.0096,0.0391,-0.0563,-0.011,-0.0005,-0.0417,-0.0147,0.0968,0.0053,-0.0733,-0.0052,-0.004,-0.005,-0.0412,0.0453,0.0368,-0.0188,0.0336,0.0094,0.0152,-0.0889,-0.0073,-0.0299,0.0175,-0.0322,-0.0489,-0.0274,-0.0113,0.0369,-0.0147,-0.0168,-0.059,0.0078,0.0572,-0.0634,0.0253,-0.0752,-0.0173,-0.0634,-0.066,0.0037,0.0318,-0.0142,-0.0552,-0.0198,-0.0271,-0.0426,0.0205,-0.0124,0.0246,0.0198,-0.0123,0.035,-0.0071,-0.0437,0.0006,0.0139,-0.0598,0.0024,0.0027,0.0335,0.0786,0.0037,0.0313,0.0437,-0.0219,-0.0359,-0.2342,0.0113,-0.003,-0.0342,0.0512,-0.1006,0.073,-0.0253,0.0594,0.0682,0.0482,-0.0202,-0.0305,0.0316,-0.0117,0.0619,0.0204,0.0285,-0.0134,0.0253,-0.0353,0.0408,-0.0264,-0.0957,0.0199,0.032,0.2318,0.0285,0.0643,-0.0226,0.0243,0.0328,-0.0504,-0.083,0.0717,0.0083,0.0109,-0.0066,-0.0134,-0.0363,-0.0317,0.0168,0.0256,-0.1339,-0.028,-0.0406,-0.0252,0.0448,-0.0528,0.0229,0.0651,0.016,0.0944,0.0125,0.0387,-0.0443,-0.0566,-0.0016,-0.0817,0.0719,-0.0002,-0.0481,0.0266,-0.1113,0.0512,-0.01,-0.0564,-0.0873,0.0853,-0.0142,-0.0221,0.0573,0.0253,-0.025,0.0255,0.0061,0.0036,-0.0751,-0.0912,-0.0503,0.0832,0.0135,0.0233,-0.0045,0.0265,0.014,0.0886,0.0285,0.0631,-0.035,0.0109,-0.007,-0.0592,-0.0172,0.066,-0.002,-0.276,-0.0103,-0.0196,0.0469,-0.0648,0.0178,0.0614,0.028,-0.071,0.002,-0.0006,0.073,0.0016,-0.007,-0.0034,0.0366,0.0543,-0.0692,0.0472,-0.0177,0.0291,0.0247,0.1768,-0.0174,0.0057,0.0096,-0.0045,0.0332,0.0179,0.0013,0.0092,-0.0077,0.0717,-0.0062,0.0267,0.0465,-0.0379,-0.036,0.0311,-0.0079,0.0175,-0.0468,-0.0632,0.0077,0.0817,-0.0082,-0.018,-0.0166,0.0018,-0.0172,-0.0252,-0.0152,-0.0364,0.0112,0.0545,0.0323,-0.053,-0.0321,-0.0141,-0.0166,-0.0027,-0.0386,-0.028,0.0223,-0.0117]}
{"key":"[MERLOT: Multimodal Neural Script Knowledge Models] As humans, we understand events in the visual world contextually, performing multimodal reasoning across time to make inferences about the past, present, and future. We introduce MERLOT, a model that learns multimodal script knowledge by watching millions of YouTube videos with transcribed speech -- in an entirely label-free, self-supervised manner. By pretraining with a mix of both frame-level (spatial) and video-level (temporal) objectives, our model not only learns to match images to temporally corresponding words, but also to contextualize what is happening globally over time. As a result, MERLOT exhibits strong out-of-the-box representations of temporal commonsense, and achieves state-of-the-art performance on 12 different video QA datasets when finetuned. It also transfers well to the world of static images, allowing models to reason about the dynamic context behind visual scenes. On Visual Commonsense Reasoning, MERLOT answers questions correctly with 80.6% accuracy, outperforming state-of-the-art models of similar size by over 3%, even those that make heavy use of auxiliary supervised data (like object bounding boxes). Ablation analyses demonstrate the complementary importance of: 1) training on videos versus static images; 2) scaling the magnitude and diversity of the pretraining video corpus; and 3) using diverse objectives that encourage full-stack multimodal reasoning, from the recognition to cognition level.","layer":0,"vector":[-0.0335,0.0192,0.0079,-0.0452,0.0217,0.0474,0.0326,0.0124,-0.0038,-0.0037,-0.0166,-0.0773,0.0461,0.0734,0.0234,-0.0066,-0.0036,0.0351,-0.0437,0.002,0.0508,-0.0208,-0.0131,-0.0592,0.0052,0.0162,-0.0326,-0.0304,-0.0239,-0.2242,0.0163,-0.0265,0.0077,-0.0223,0.0008,-0.0029,-0.02,0.0862,-0.0095,0.043,0.0119,0.0071,0.018,-0.0912,-0.0462,-0.0946,-0.0016,-0.0171,-0.0167,-0.0466,-0.0081,-0.0624,0.0222,0.0073,-0.0097,0.0198,0.0026,0.1009,0.0374,0.0484,0.0157,0.0655,-0.197,0.1059,0.029,0.0327,-0.0168,0.0325,-0.0254,0.0233,-0.0215,0.0111,0.0178,0.0318,0.0087,-0.0394,-0.0171,-0.0473,-0.0105,-0.0227,0.0062,-0.0256,-0.0361,-0.0276,-0.0098,-0.028,0.016,-0.0109,0.0091,0.0142,-0.0583,0.0121,-0.0401,0.0218,-0.0476,-0.0403,0.0445,0.0128,-0.0296,0.1947,-0.05,0.0103,0.0679,-0.0625,0.0417,-0.0056,-0.0463,-0.0377,-0.0357,0.0109,-0.0008,0.0006,0.0657,0.002,0.0461,0.0082,0.0527,0.0169,-0.0319,-0.0379,0.02,0.0025,0.0409,-0.0503,0.0036,-0.0816,0.0532,0.1342,0.0231,-0.0264,0.0526,0.0193,-0.0194,0.0222,0.0381,0.0273,0.0386,-0.0231,0.0287,-0.0675,-0.0302,-0.0364,0.0088,-0.0475,-0.0735,0.0923,-0.0081,0.032,-0.0514,0.0024,-0.0455,0.0185,-0.0456,-0.0258,-0.0016,0.0036,0.073,0.0414,-0.055,0.0202,-0.0179,-0.0414,-0.02,0.0815,0.0104,-0.0919,-0.0337,-0.0373,0.0403,-0.02,0.0366,-0.0032,0.003,-0.0045,0.1034,0.0181,-0.076,0.0409,0.0008,0.007,0.0341,-0.0915,-0.0205,0.0877,0.0259,-0.0775,0.037,-0.0693,0.0345,0.072,0.0422,0.0382,-0.0132,-0.0061,-0.0299,0.0387,-0.0034,0.0154,0.0375,-0.0336,-0.0291,-0.0159,-0.0245,0.0023,-0.022,0.0177,-0.0412,0.0366,0.0693,0.0245,-0.0359,0.03,0.0426,-0.0154,0.0087,-0.0301,0.0213,0.0178,-0.0203,-0.0054,0.0313,-0.0653,-0.0199,-0.2371,0.0187,0.0069,0.0104,0.0325,-0.0404,0.0122,0.0163,0.0629,0.0519,0.0524,-0.0437,-0.0098,-0.0185,-0.0099,0.0471,0.035,0.061,-0.0318,0.0159,-0.0461,0.0059,-0.0231,-0.1098,0.0649,-0.0097,0.2195,0.0559,0.0196,-0.0299,0.0463,0.0266,-0.0262,-0.1245,0.0515,0.0106,0.0707,0.0008,-0.0122,-0.0251,-0.0382,0.0094,-0.0074,-0.1164,-0.079,-0.0248,-0.0139,0.0295,-0.0608,-0.0116,0.0445,-0.0354,0.0282,0.0184,-0.0443,0.0207,-0.0326,0.0053,-0.0382,0.0584,0.0276,-0.0196,0.0274,-0.068,0.0908,-0.0295,-0.0406,-0.0656,0.0189,-0.0234,-0.0541,0.0954,-0.0181,0.0118,0.0353,0.0035,0.0936,-0.0453,-0.0719,-0.037,0.065,0.0055,0.0151,-0.0186,0.0236,0.0127,0.0558,0.0066,0.0644,-0.0122,0.0189,0.0151,-0.0533,-0.0225,0.0154,0.0124,-0.3115,0.0101,0.0168,0.0404,-0.0264,0.0478,0.0498,0.0362,-0.0308,0.0257,-0.0435,0.0615,0.0472,-0.0141,-0.0313,0.0445,0.0998,-0.0118,0.0343,-0.0477,0.0639,0.0125,0.1752,-0.0272,0.0051,0.0193,-0.0306,-0.0409,0.063,-0.0207,0.0046,-0.0124,0.0824,-0.0342,0.0238,0.0128,-0.073,0.0382,0.0386,0.0207,0.0123,-0.0248,-0.0321,-0.0472,0.0746,0.0357,0.0308,-0.0353,-0.0447,0.0031,-0.0142,0.0177,-0.0144,0.0389,0.0098,0.0301,0.0248,-0.0133,-0.0204,-0.058,-0.0184,-0.0874,0.0119,0.0409,-0.0347]}
{"key":"[Fast spectral algorithms from sum-of-squares proofs: tensor decomposition and planted sparse vectors] We consider two problems that arise in machine learning applications: the problem of recovering a planted sparse vector in a random linear subspace and the problem of decomposing a random low-rank overcomplete 3-tensor. For both problems, the best known guarantees are based on the sum-of-squares method. We develop new algorithms inspired by analyses of the sum-of-squares method. Our algorithms achieve the same or similar guarantees as sum-of-squares for these problems but the running time is significantly faster. For the planted sparse vector problem, we give an algorithm with running time nearly linear in the input size that approximately recovers a planted sparse vector with up to constant relative sparsity in a random subspace of $\\mathbb R^n$ of dimension up to $\\tilde \\Omega(\\sqrt n)$. These recovery guarantees match the best known ones of Barak, Kelner, and Steurer (STOC 2014) up to logarithmic factors. For tensor decomposition, we give an algorithm with running time close to linear in the input size (with exponent $\\approx 1.086$) that approximately recovers a component of a random 3-tensor over $\\mathbb R^n$ of rank up to $\\tilde \\Omega(n^{4/3})$. The best previous algorithm for this problem due to Ge and Ma (RANDOM 2015) works up to rank $\\tilde \\Omega(n^{3/2})$ but requires quasipolynomial time.","layer":3,"vector":[-0.0598,-0.0014,0.0263,0.0257,0.0008,0.0379,0.0085,-0.0002,0.0644,-0.0358,0.039,-0.0675,0.043,0.036,0.031,0.0591,0.0111,0.0479,-0.0306,0.0055,0.0336,-0.0585,-0.0474,-0.0602,0.05,-0.0391,-0.0304,-0.0722,-0.0684,-0.241,0.0435,-0.0473,0.0666,-0.0551,0.0414,0.0003,-0.0284,0.0464,-0.0795,0.0408,0.0317,0.0337,-0.0557,-0.0193,-0.0291,-0.0473,-0.0042,-0.0085,-0.017,-0.0345,-0.0281,-0.0145,0.0177,0.0125,0.0454,-0.0088,0.0275,0.0158,0.0146,0.0677,0.007,0.0317,-0.1813,0.0253,0.0597,0.0188,-0.0035,-0.0506,0.0215,0.0746,-0.0116,0.0632,0.0323,0.0055,0.009,0.0016,0.006,-0.0292,0.0099,0.0217,0.0226,-0.0305,-0.0478,0.0322,-0.0467,-0.0178,0.0612,-0.0333,0.0411,0.0246,-0.0531,0.005,-0.0177,0.0566,-0.0449,-0.051,0.0198,0.0601,0.0115,0.1949,-0.0322,0.0345,0.016,-0.0089,-0.0084,-0.0686,-0.0358,-0.0207,-0.0317,-0.0251,-0.0023,-0.0297,0.0501,-0.0472,-0.0044,-0.0367,0.0561,0.0045,-0.0329,0.0262,-0.0174,0.0187,0.0847,-0.0226,0.0331,-0.0643,0.0104,0.1336,0.0669,0.0974,-0.0033,-0.0168,-0.0359,-0.0663,0.0188,0.0101,0.0222,0.036,0.005,0.0153,-0.0176,-0.0635,0.038,-0.0667,-0.0134,0.1352,-0.0405,-0.012,-0.0144,-0.0348,-0.004,0.049,-0.0182,-0.0238,0.0254,0.0287,0.0314,0.0272,-0.0523,0.0405,-0.0361,-0.0677,0.0167,0.1037,0.0139,-0.0657,0.0015,0.0094,0.0306,0.0061,0.034,0.0315,0.0081,0.0203,0.08,0.0235,-0.0704,0.0183,0.0349,0.0283,0.02,-0.0327,-0.0217,0.071,0.0257,-0.024,0.0155,0.0165,-0.0048,-0.0174,-0.0856,-0.0284,-0.0826,-0.0374,-0.0529,-0.0196,-0.0016,-0.0209,0.0107,-0.0049,-0.0201,-0.003,-0.0631,0.0449,0.0376,-0.0079,-0.0013,-0.0296,0.0051,0.0514,-0.0114,0.0314,0.0433,-0.0302,-0.0321,0.0194,0.0343,0.0433,-0.02,0.0454,0.008,-0.0879,-0.1173,-0.2035,-0.0428,-0.0125,0.023,0.0204,-0.0873,0.0544,-0.043,0.0735,0.0547,0.0498,0.024,-0.0013,-0.0051,-0.0285,0.0567,0.0142,-0.0002,-0.0121,-0.0059,-0.0362,0.0441,0.0055,-0.0472,0.0649,-0.0253,0.2218,0.037,0.0331,-0.0446,0.025,0.0557,-0.0046,-0.0768,0.0713,0.0488,0.1046,0.0366,-0.0195,-0.0269,-0.0029,0.006,-0.0015,-0.0854,-0.0359,-0.0444,-0.0346,0.0017,-0.0064,0.0179,0.0184,-0.0285,0.0659,-0.0579,0.039,-0.0334,-0.0928,0.0242,-0.0199,0.0112,0.004,-0.093,0.0071,-0.0498,0.0786,0.008,0.0243,-0.0193,0.0307,-0.0358,0.0213,0.0484,-0.0202,-0.0143,0.0834,0.0084,0.0337,-0.0362,-0.0419,-0.0422,0.0699,-0.0736,0.0404,-0.0005,0.0314,0.0337,0.0718,0.0057,-0.0225,0.0101,-0.039,0.0289,-0.0642,-0.0184,0.0264,0.0009,-0.2999,0.0336,0.0306,0.0014,-0.0374,-0.0249,0.0529,-0.0008,-0.0609,0.0281,-0.0142,0.0331,0.0535,-0.0274,0.0352,0.0482,0.0762,-0.0399,0.0284,-0.1025,0.0116,0.011,0.1976,-0.0427,-0.0078,0.0187,-0.0101,0.021,0.0278,-0.0465,0.0126,0.0223,0.0873,-0.0925,0.0363,0.0294,-0.04,0.0217,0.0367,-0.0267,0.0312,-0.0335,-0.0582,-0.0101,0.0742,-0.0384,0.0015,-0.025,0.0193,0.0253,0.0128,-0.018,-0.0073,-0.0225,-0.0096,0.0182,-0.0344,-0.0693,0.0128,-0.0157,-0.0215,-0.0579,-0.0528,-0.0194,0.0129]}
{"key":"[Representing Long-Range Context for Graph Neural Networks with Global Attention] Graph neural networks are powerful architectures for structured datasets. However, current methods struggle to represent long-range dependencies. Scaling the depth or width of GNNs is insufficient to broaden receptive fields as larger GNNs encounter optimization instabilities such as vanishing gradients and representation oversmoothing, while pooling-based approaches have yet to become as universally useful as in computer vision. In this work, we propose the use of Transformer-based self-attention to learn long-range pairwise relationships, with a novel \"readout\" mechanism to obtain a global graph embedding. Inspired by recent computer vision results that find position-invariant attention performant in learning long-range relationships, our method, which we call GraphTrans, applies a permutation-invariant Transformer module after a standard GNN module. This simple architecture leads to state-of-the-art results on several graph classification tasks, outperforming methods that explicitly encode graph structure. Our results suggest that purely-learning-based approaches without graph structure may be suitable for learning high-level, long-range relationships on graphs. Code for GraphTrans is available at https://github.com/ucbrise/graphtrans.","layer":2,"vector":[-0.0077,-0.0151,0.0114,-0.0405,0.0474,0.0144,0.0179,0.01,0.0156,-0.021,0.02,-0.0512,0.0494,0.0966,-0.0011,0.0527,-0.0062,0.1027,-0.0576,-0.0386,0.0235,-0.0514,0.0108,-0.0552,0.034,0.0181,-0.0175,-0.0277,-0.0497,-0.2451,-0.0123,-0.0335,0.0269,-0.0187,-0.0287,-0.0522,0.0181,-0.0021,-0.0209,0.0234,0.0137,0.0344,-0.0199,-0.0601,-0.0403,-0.0301,0.001,-0.0395,-0.0281,-0.0804,0.0186,-0.0753,0.0426,0.0173,0.0305,0.0521,0.069,0.0081,0.0362,0.0322,0.032,0.0571,-0.129,0.036,0.0446,0.0552,-0.0587,0.0031,-0.0036,0.0517,0.0253,-0.025,0.0006,-0.0128,0.0098,0.0304,0.0203,0.0052,-0.0057,-0.0149,0.0156,-0.0309,-0.0391,-0.0229,0.0119,-0.034,0.005,-0.0596,0.029,0.0173,-0.0554,-0.0306,-0.0208,0.0096,-0.0584,-0.0355,0.0709,0.0256,-0.0736,0.1813,-0.0749,0.0071,0.0406,-0.0117,0.0469,-0.0266,-0.0369,-0.0294,-0.0273,0.0026,-0.0164,-0.0291,-0.0023,-0.0697,0.0047,0.0055,0.0743,0.0777,-0.0324,0.0226,-0.0465,0.0297,0.0103,-0.0129,0.0331,-0.0598,0.027,0.1079,0.0391,0.0526,0.0578,0.035,-0.0247,0.0016,-0.0112,-0.0003,0.041,-0.0224,0.0124,-0.0009,0.0001,-0.0055,0.044,-0.0264,-0.0316,0.1069,-0.0328,-0.0123,-0.0456,-0.0111,-0.0122,0.0189,-0.0093,-0.0421,-0.0178,0.0338,0.0196,0.0199,-0.0643,-0.0018,-0.0191,-0.0164,-0.0565,0.0753,0.0542,-0.1029,-0.0075,-0.0474,-0.0142,-0.0196,0.0488,0.0595,-0.0061,0.0487,0.0949,0.0648,-0.0839,-0.0118,0.0341,-0.0297,0.0357,-0.0316,-0.0323,0.0345,0.042,-0.0247,0.016,-0.0426,0.0011,0.0643,-0.0267,0.0523,-0.0215,-0.0045,-0.0326,-0.0141,-0.0163,-0.0184,-0.0179,-0.0247,0.0182,-0.005,-0.0205,0.0341,-0.0363,-0.008,-0.039,-0.0017,-0.011,-0.0017,-0.0533,0.0372,0.03,-0.0167,-0.0235,-0.0537,0.0091,0.0246,0.0195,0.0437,0.0473,-0.0871,-0.0507,-0.2257,-0.0279,0.0129,-0.0489,0.0661,-0.0826,0.0369,0.0107,0.0861,0.0954,0.0985,-0.0043,-0.0421,0.0252,-0.0061,0.0797,0.0084,0.0779,-0.0443,-0.031,-0.0214,0.0418,0.0038,-0.0898,0.0473,-0.0089,0.2617,-0.0065,0.0006,-0.0231,0.0103,0.0047,-0.067,-0.0785,0.0574,-0.0017,0.0299,-0.0243,-0.0091,-0.0195,-0.0705,-0.0426,0.012,-0.0814,-0.0115,0.0054,0.0052,0.0383,-0.0104,0.0216,0.0414,-0.0283,0.0406,-0.0017,-0.006,-0.0498,-0.0651,0.0086,-0.0597,0.0164,0.0116,-0.0679,0.0191,-0.0522,0.0434,0.0213,-0.0055,-0.0113,0.0351,0.0052,-0.0292,0.0419,0.0362,-0.0016,0.0394,0.0057,0.0618,0.0034,-0.0517,0.0075,0.0299,-0.0083,0.0268,0.0159,0.093,0.016,0.0583,-0.0021,0.0544,0.0078,0.0207,0.008,-0.0633,-0.057,0.0771,-0.0333,-0.3157,0.0278,0.0343,0.0642,-0.0159,0.0421,0.0605,0.0518,-0.054,-0.0299,-0.0133,0.0209,0.0279,-0.0067,-0.0341,0.0504,0.043,-0.0286,0.0491,-0.0279,0.0234,0.0049,0.2157,-0.0251,0.0474,0.0104,-0.0677,-0.0401,0.0288,-0.0389,0.0305,0.0362,0.0672,-0.051,0.0246,0.0823,-0.0094,0.0632,0.0337,-0.0001,0.0187,-0.0138,-0.0653,-0.0667,0.1095,-0.0095,-0.0068,-0.0463,0.0034,0.0342,-0.028,-0.0117,-0.0166,0.0359,0.0386,0.0299,-0.0451,-0.0067,-0.0388,-0.0508,-0.0111,-0.0926,-0.0145,-0.0282,-0.0202]}
{"key":"[Towards Generalizing Sensorimotor Control Across Weather Conditions] The ability of deep learning models to generalize well across different scenarios depends primarily on the quality and quantity of annotated data. Labeling large amounts of data for all possible scenarios that a model may encounter would not be feasible; if even possible. We propose a framework to deal with limited labeled training data and demonstrate it on the application of vision-based vehicle control. We show how limited steering angle data available for only one condition can be transferred to multiple different weather scenarios. This is done by leveraging unlabeled images in a teacher-student learning paradigm complemented with an image-to-image translation network. The translation network transfers the images to a new domain, whereas the teacher provides soft supervised targets to train the student on this domain. Furthermore, we demonstrate how utilization of auxiliary networks can reduce the size of a model at inference time, without affecting the accuracy. The experiments show that our approach generalizes well across multiple different weather conditions using only ground truth labels from one domain.","layer":2,"vector":[0.0028,-0.0358,0.0595,-0.0097,0.0105,0.0484,0.0226,0.0043,-0.0188,0.0171,0.015,-0.0488,0.0332,0.0853,0.0041,0.0115,0.012,0.0696,0.0009,0.0046,0.0066,-0.0084,0.0014,-0.0336,-0.0316,0.0521,-0.0289,-0.0052,-0.0365,-0.2309,-0.003,-0.0444,0.0132,-0.0231,-0.0115,-0.0164,-0.0689,0.0336,-0.0179,0.0147,0.0307,-0.0063,-0.0505,-0.0635,-0.0179,-0.0602,-0.0369,-0.0393,-0.0182,-0.0616,-0.0009,-0.0362,-0.0052,0.028,0.0293,0.0591,0.0218,0.047,0.0708,0.0229,-0.0253,0.0473,-0.1936,0.0679,0.0603,0.0478,-0.0468,0.0295,0.0129,0.0284,-0.0171,0.013,0.0278,0.0514,0.0018,-0.0401,-0.0631,-0.0412,-0.0202,-0.0429,0.0075,-0.0076,-0.0049,-0.0281,0.0097,-0.0248,0.011,-0.0629,0.0398,0.0192,-0.0655,-0.0541,-0.0267,0.0261,-0.0447,0.0057,0.0497,0.026,-0.0658,0.1996,-0.0649,0.0162,0.0281,-0.0013,0.0403,-0.0151,-0.0303,-0.0329,-0.0718,0.0147,-0.0296,-0.0185,-0.015,-0.0024,0.0316,-0.0081,0.0675,0.0328,0.0143,0.0123,-0.0217,-0.0205,0.0493,-0.0197,0.011,-0.0661,0.0068,0.1453,0.079,0.0149,0.0741,-0.0467,-0.0576,-0.0269,0.0493,0.0034,0.0468,0.0131,-0.0058,0.0257,-0.0056,-0.0351,0.0339,-0.1047,-0.0474,0.1079,-0.0372,0.0161,-0.0317,-0.0059,-0.0245,0.0676,-0.0459,-0.0127,0.0536,0.0557,0.052,0.0186,-0.0633,0.0189,0.0054,-0.0206,-0.0132,0.0481,-0.0057,-0.0848,-0.0042,0.0125,-0.0128,-0.0163,0.0286,0.0152,0.0024,0.0461,0.0799,0.0523,-0.0854,0.0141,-0.0336,0.0216,0.0192,-0.0798,0.0108,0.0241,0.0361,0.0146,-0.012,-0.0622,-0.0413,0.015,-0.0131,-0.0091,-0.0242,-0.0134,-0.0413,0.0058,-0.0209,0.0017,0.0061,-0.0235,0.0015,0.0136,-0.0238,0.0016,-0.0178,-0.019,-0.0226,0.0097,0.0254,0.0111,-0.0258,0.0204,0.0499,-0.0392,-0.0406,0.0026,0.0127,0.0618,-0.0072,-0.0056,0.0582,-0.0366,-0.0212,-0.2524,0.0078,0.0171,-0.0231,0.0741,-0.0854,0.0249,0.0046,0.0781,0.0278,0.0677,-0.0742,-0.0021,0.0023,0.0113,0.0461,0.0087,0.0389,-0.0045,-0.0174,-0.0161,0.0169,-0.0327,-0.0871,0.055,-0.0203,0.2332,0.038,0.0809,-0.0275,-0.0012,0.032,0.0133,-0.0453,0.0645,-0.0022,0.0399,-0.0038,-0.0208,-0.0335,0.0039,0.0128,0.0274,-0.0989,-0.0406,-0.0314,-0.0397,0.0194,-0.0246,-0.0265,0.0298,-0.0598,0.0281,-0.0026,-0.0441,-0.0237,-0.0912,0.057,-0.0322,0.0226,0.0152,-0.0163,0.0038,-0.0624,0.0391,0.016,-0.0027,-0.0955,0.0297,0.0042,-0.0521,0.0766,0.0083,0.0442,0.0677,0.006,0.0112,-0.0009,-0.0564,-0.041,0.0628,-0.0239,0.0025,0.0729,0.0626,0.0476,0.0719,-0.043,0.0421,-0.0308,0.0226,0.0248,-0.0775,-0.0395,0.0834,0.0025,-0.2878,0.0342,0.0247,0.0247,-0.0465,0.0222,0.0281,0.024,-0.096,-0.0002,-0.0097,0.015,0.081,0.0403,0.0143,0.0435,0.0537,-0.0117,0.0768,-0.0477,0.0061,0.0437,0.2003,-0.0335,0.0214,0.0514,-0.0615,-0.0453,0.0408,0.0207,0.0068,0.0113,0.0674,-0.0277,0.0245,0.0915,-0.0421,0.0369,0.0293,0.0421,-0.026,0.0617,0.0264,-0.0465,0.0973,0.0051,-0.0208,-0.0295,-0.0445,0.0355,-0.0065,0.0017,-0.0567,-0.0159,0.0211,0.0007,-0.0288,-0.0627,-0.0554,-0.0636,0.0336,-0.0845,-0.0212,-0.0163,-0.0015]}
{"key":"[Biological Sequence Design with GFlowNets] Design of de novo biological sequences with desired properties, like protein and DNA sequences, often involves an active loop with several rounds of molecule ideation and expensive wet-lab evaluations. These experiments can consist of multiple stages, with increasing levels of precision and cost of evaluation, where candidates are filtered. This makes the diversity of proposed candidates a key consideration in the ideation phase. In this work, we propose an active learning algorithm leveraging epistemic uncertainty estimation and the recently proposed GFlowNets as a generator of diverse candidate solutions, with the objective to obtain a diverse batch of useful (as defined by some utility function, for example, the predicted anti-microbial activity of a peptide) and informative candidates after each round. We also propose a scheme to incorporate existing labeled datasets of candidates, in addition to a reward function, to speed up learning in GFlowNets. We present empirical results on several biological sequence design tasks, and we find that our method generates more diverse and novel batches with high scoring candidates compared to existing approaches.","layer":0,"vector":[-0.0718,0.0016,0.0076,-0.0188,0.0292,-0.0158,0.0404,0.0918,0.0132,-0.0156,-0.0309,-0.042,0.0354,0.0527,-0.0165,-0.0096,-0.0224,0.0548,-0.0344,0.0162,0.0469,-0.0312,0.0235,-0.0531,0.0285,0.0287,-0.0101,-0.0058,-0.0613,-0.2184,0.0049,0.0151,0.0588,-0.0411,0.0069,-0.0513,-0.0468,0.0712,-0.0366,0.036,0.068,0.0283,-0.0263,-0.0288,0.0208,-0.0347,-0.0597,0.0149,-0.0291,-0.0507,-0.009,-0.0517,0.0217,0.0393,-0.0064,-0.0002,0.0096,-0.001,-0.0124,0.0517,0.0486,0.0193,-0.1149,0.0897,0.0244,0.0229,-0.062,-0.0441,0.0139,0.0647,0.0012,0.0479,0.0249,0.0794,0.0093,0.0517,0.0129,-0.0215,-0.0083,0.0413,0.0301,-0.0144,-0.0036,-0.0152,-0.0207,-0.0553,0.0111,-0.02,0.0184,0.0264,-0.0405,-0.0065,-0.0322,0.0165,-0.0767,-0.0041,0.0195,-0.0174,-0.0235,0.2177,-0.0714,0.0081,0.0002,-0.0083,0.0514,-0.0508,-0.0304,-0.0168,-0.0037,-0.0233,-0.0213,0.0321,0.0383,-0.0456,0.0149,0.0277,0.0395,0.0096,-0.0725,-0.0003,-0.0642,-0.0111,-0.007,-0.0206,-0.024,-0.0167,-0.0054,0.1258,0.0163,0.0355,0.037,-0.0281,-0.0204,-0.027,0.0438,0.0252,-0.0151,0.0058,0.0153,-0.0349,-0.0145,-0.0182,0.0118,-0.1242,-0.0819,0.1175,-0.0485,0.0394,-0.0362,-0.0129,-0.0022,0.0163,0.0212,-0.0538,0.0097,0.0736,0.0514,0.016,-0.0528,-0.0009,-0.0726,-0.0323,-0.0099,0.114,-0.0115,-0.0928,-0.0539,-0.0143,0.0143,-0.0046,0.0338,0.0321,-0.0099,0.0444,0.0157,0.0151,-0.033,-0.0045,0.0376,-0.0146,-0.01,-0.0456,0.0082,0.0427,0.0453,-0.0643,0.005,-0.0242,-0.0183,0.0432,-0.0158,0.0171,0.0121,0.0432,-0.0371,-0.0667,-0.0061,0.0018,0.0203,-0.026,0.0331,-0.01,-0.0101,-0.0126,-0.0201,-0.011,-0.0123,0.0155,0.0368,0.0112,-0.0156,0.0429,-0.0188,-0.0363,0.0045,-0.0222,0.0255,0.0493,0.0281,0.0138,0.033,-0.0513,-0.0454,-0.2749,-0.0245,0.011,-0.0499,0.1037,-0.0316,0.0224,-0.0027,0.022,0.0759,0.0672,0.007,-0.0258,-0.006,-0.0284,0.0645,0.0382,-0.0259,-0.0034,0.0226,0.0278,0.0367,0.0458,-0.09,0.0259,-0.0075,0.2278,0.0576,0.0357,-0.0268,0.0338,0.0496,-0.0285,-0.1095,0.0497,0.0505,0.0429,-0.0026,-0.013,-0.0675,0.0197,0.0175,-0.0199,-0.1227,-0.0524,-0.0307,-0.0371,0.0602,-0.0491,0.0097,0.036,-0.0279,0.0726,-0.0242,-0.0065,-0.016,-0.0894,0.0691,-0.0575,0.038,0.028,-0.0378,0.0003,-0.0114,0.0541,-0.0328,-0.0231,-0.0184,0.0693,-0.0444,-0.007,0.0807,0.0458,0.0691,0.0609,0.0031,0.0043,-0.0624,-0.0508,0.0076,0.0241,-0.0673,0.0075,-0.0076,0.0447,0.0314,0.068,-0.0148,0.0348,-0.0162,0.048,0.0122,-0.0394,0.0201,0.0488,-0.0081,-0.2896,0.0574,0.0265,0.0445,-0.0434,0.0277,0.0772,-0.0254,-0.0447,-0.0204,-0.0112,0.0481,0.0395,0.0085,-0.0165,0.0254,0.0556,-0.0768,0.0417,-0.07,0.0585,0.0343,0.2413,-0.0471,0.0182,0.011,-0.0285,0.0008,0.0137,-0.0228,0.0253,0.011,0.0706,-0.0515,0.0715,0.0732,-0.0366,0.0161,-0.0012,-0.0236,-0.0545,0.0075,-0.0309,-0.0434,0.091,-0.0485,-0.0034,-0.0295,-0.0154,0.0247,-0.0467,0.0073,0.0137,-0.0308,-0.0052,0.0175,-0.0193,-0.0471,-0.0531,-0.0297,-0.0122,-0.046,-0.0356,0.031,-0.0233]}
{"key":"[Learning from Multiple Outlooks] We propose a novel problem formulation of learning a single task when the data are provided in different feature spaces. Each such space is called an outlook, and is assumed to contain both labeled and unlabeled data. The objective is to take advantage of the data from all the outlooks to better classify each of the outlooks. We devise an algorithm that computes optimal affine mappings from different outlooks to a target outlook by matching moments of the empirical distributions. We further derive a probabilistic interpretation of the resulting algorithm and a sample complexity bound indicating how many samples are needed to adequately find the mapping. We report the results of extensive experiments on activity recognition tasks that show the value of the proposed approach in boosting performance.","layer":4,"vector":[-0.0223,-0.0153,0.0235,-0.0162,0.0115,0.028,0.0793,0.0313,0.0264,-0.0158,-0.0322,-0.0735,0.0041,0.0427,0.0191,0.0449,0.0125,0.037,-0.0543,-0.0001,0.0356,-0.0191,-0.0071,-0.0355,0.0313,0.0329,-0.045,-0.0398,-0.046,-0.2355,-0.0034,-0.0304,0.0965,-0.0109,0.0212,-0.029,-0.0548,0.0805,-0.0935,0.0251,0.0213,0.0076,-0.0278,-0.0706,-0.0464,-0.0774,-0.0426,-0.0335,-0.0378,-0.0351,0.0176,-0.0568,0.0079,0.0328,0.0098,0.0453,0.0254,0.0349,-0.0159,0.0723,-0.009,0.0284,-0.1485,0.0274,0.0268,-0.0012,-0.0335,-0.0074,0.0233,0.0848,-0.0198,0.0414,0.0469,0.0652,0.0388,-0.0248,-0.0177,-0.0248,0.0018,0.011,-0.0177,-0.011,-0.0328,-0.0002,-0.0099,-0.0234,-0.02,-0.0318,0.07,0.0085,-0.0481,-0.0078,-0.0416,0.0239,-0.0527,-0.0219,0.0052,0.0323,-0.0179,0.1847,-0.0662,0.0499,0.0536,-0.0548,0.0234,-0.0576,-0.0091,-0.0384,-0.0399,-0.0153,-0.025,-0.0402,-0.026,-0.0464,0.0003,0.0282,0.0581,0.0125,-0.0033,-0.0283,-0.012,0.0004,0.091,-0.0421,0.0115,-0.0431,0.0185,0.1691,0.0492,0.0483,0.0657,-0.0172,-0.0173,-0.0242,0.0268,0.04,0.0196,-0.0034,0.0333,0.0118,-0.0022,-0.0379,0.0584,-0.0827,-0.0671,0.183,-0.0009,0.0011,-0.0564,-0.0187,-0.0486,0.0365,-0.0337,-0.0325,0.0191,0.0261,0.0308,0.0031,-0.0639,-0.0169,-0.0369,-0.064,-0.0009,0.098,0.011,-0.0611,-0.0148,0.0207,-0.0303,-0.0302,0.0629,0.0421,-0.001,0.0687,0.0906,0.0339,-0.0541,-0.016,0.0574,-0.0167,0.0491,-0.0681,0.003,0.0593,0.0519,-0.0128,0.0207,-0.0177,0.0221,0.0413,-0.0213,0.0125,0.0198,-0.0021,-0.0665,-0.0792,-0.0516,0.0268,0.0224,-0.0524,0.0131,-0.0084,-0.0322,0.0314,-0.0022,-0.0045,-0.0178,0.0269,0.0662,0.0181,0.0088,0.014,0.0578,-0.0206,-0.0084,-0.0174,0.0401,0.0434,-0.0123,0.0295,0.0417,-0.0246,-0.0091,-0.249,-0.0033,0.0266,0.0129,0.0062,-0.0679,0.0426,0.0265,0.0711,0.0696,0.0388,-0.0569,-0.0238,0.0455,0.0109,0.0578,-0.0048,0.0091,-0.0348,0.0106,0.0198,0.0175,-0.0012,-0.0941,0.0663,-0.0329,0.2188,0.0504,0.0121,-0.0495,0.0218,0.0013,-0.0229,-0.0974,0.0763,0.0013,0.0745,-0.003,-0.0239,-0.0174,-0.0491,0.01,0.033,-0.0741,-0.0369,-0.0335,-0.029,-0.012,-0.0479,0.0251,0.0387,-0.0308,0.0631,-0.0362,-0.0072,-0.0233,-0.1059,0.0117,-0.0437,0.0102,0.0098,-0.0629,0.0328,-0.0355,0.0801,0.0268,-0.0486,-0.0432,-0.0214,-0.0164,-0.0921,0.0426,-0.0383,-0.0092,0.0551,-0.0165,0.0006,0.0028,-0.0314,-0.0259,0.054,-0.0315,0.0165,0.0054,0.0276,-0.0145,0.0953,-0.0237,0.0008,-0.0135,0.0286,0.0044,-0.0192,0.0173,0.0543,0.001,-0.2773,-0.0108,0.0112,0.0187,-0.0312,0.0057,0.025,0.0084,-0.0277,0.0438,-0.0068,0.0426,-0.0097,0.0122,-0.0082,0.0791,0.0841,-0.0485,0.0192,-0.0776,0.0603,0.0854,0.1995,-0.0171,0.0264,0.0088,-0.0346,-0.0103,0.0302,-0.0346,0.022,-0.0247,0.0473,-0.057,0.0434,0.0773,-0.03,0.0421,0.0129,0.0116,-0.0046,0.0165,-0.0874,-0.0144,0.1101,-0.0164,-0.0119,-0.0396,-0.0389,-0.0113,0.0009,-0.0046,-0.0622,0.0364,0.027,0.0306,-0.0148,-0.0304,-0.0018,-0.0373,0.0324,-0.0576,0.0099,-0.033,-0.0254]}
{"key":"[Understanding the Pathologies of Approximate Policy Evaluation when Combined with Greedification in Reinforcement Learning] Despite empirical success, the theory of reinforcement learning (RL) with value function approximation remains fundamentally incomplete. Prior work has identified a variety of pathological behaviours that arise in RL algorithms that combine approximate on-policy evaluation and greedification. One prominent example is policy oscillation, wherein an algorithm may cycle indefinitely between policies, rather than converging to a fixed point. What is not well understood however is the quality of the policies in the region of oscillation. In this paper we present simple examples illustrating that in addition to policy oscillation and multiple fixed points -- the same basic issue can lead to convergence to the worst possible policy for a given approximation. Such behaviours can arise when algorithms optimize evaluation accuracy weighted by the distribution of states that occur under the current policy, but greedify based on the value of states which are rare or nonexistent under this distribution. This means the values used for greedification are unreliable and can steer the policy in undesirable directions. Our observation that this can lead to the worst possible policy shows that in a general sense such algorithms are unreliable. The existence of such examples helps to narrow the kind of theoretical guarantees that are possible and the kind of algorithmic ideas that are likely to be helpful. We demonstrate analytically and experimentally that such pathological behaviours can impact a wide range of RL and dynamic programming algorithms; such behaviours can arise both with and without bootstrapping, and with linear function approximation as well as with more complex parameterized functions like neural networks.","layer":1,"vector":[-0.0769,-0.0146,0.0187,-0.0545,0.0056,0.0342,0.014,0.0564,0.0727,-0.0019,0.0061,-0.0578,0.049,0.0614,0.0066,0.0268,-0.0216,0.0126,-0.0129,0.0069,0.0422,-0.0505,-0.0255,-0.0684,0.0126,0.0355,-0.039,-0.0541,-0.0063,-0.2481,0.0364,-0.0441,-0.0039,-0.0725,0.006,0.0031,-0.0284,0.0421,-0.0181,0.0237,0.0474,0.0401,-0.0459,-0.0721,-0.0292,-0.0486,-0.0312,-0.0147,-0.0216,-0.0271,0.0223,0.0063,0.0254,-0.0022,0.0299,0.0205,0.0449,0.0825,0.0463,0.0777,0.0113,0.0353,-0.1729,0.0234,0.0401,0.0518,-0.0396,-0.0185,0.0021,0.0722,-0.0105,0.0378,0.0183,0.0381,0.0083,-0.0388,-0.0079,-0.0218,-0.0265,0.0129,-0.0015,-0.0647,-0.0352,-0.0108,-0.0043,-0.0619,0.0467,-0.071,0.0246,0.0173,-0.0052,-0.0007,-0.0294,-0.0026,-0.046,0.0013,0.0423,0.0225,-0.1136,0.1917,-0.0032,0.0544,0.025,-0.0003,0.0502,-0.0732,-0.0246,-0.0088,-0.0423,-0.026,-0.0512,-0.0252,0.0374,-0.0483,-0.0152,0.0285,0.0379,0.0339,0.0182,-0.0172,0.0083,0.0066,0.0404,0.0071,0.0268,-0.0722,0.0076,0.1506,0.0007,-0.0015,0.0165,-0.0428,-0.0324,-0.0473,0.034,0.0413,0.003,0.0139,0.0493,0.0114,-0.0311,-0.0219,-0.0033,-0.1376,-0.0304,0.0848,0.0136,0.0365,-0.0289,-0.0113,-0.0011,-0.017,-0.0417,-0.0315,0.0053,0.0418,0.0284,0.0535,-0.0211,0.0016,0.0067,-0.0602,-0.062,0.087,-0.0101,-0.0117,-0.0171,-0.0328,0.0112,-0.0355,-0.0018,0.0383,-0.0398,-0.0015,0.0681,0.0163,-0.0707,0.0184,0.0281,0.0114,0.0442,-0.0511,-0.0428,0.0186,0.0501,-0.0063,-0.0044,-0.0714,0.0348,0.0146,-0.0111,-0.0221,-0.019,-0.0194,-0.0394,-0.0857,0.0237,0.0035,0.0252,-0.0051,0.0063,0.0044,-0.0269,-0.0016,0.0268,0.0066,-0.0298,-0.0286,0.058,-0.0035,-0.0112,0.0657,0.0384,0.0084,-0.0339,0.0332,0.0191,-0.0137,-0.0217,-0.0192,0.049,0.0178,-0.0182,-0.2295,-0.0283,-0.0421,-0.0173,0.0791,-0.0772,0.0332,-0.0491,-0.0393,0.0423,0.0148,-0.0133,-0.0111,0.0555,0.0086,0.0239,0.0512,0.0332,-0.0302,0.0157,-0.0017,-0.0095,-0.0392,-0.1236,0.0264,-0.0421,0.2296,0.006,0.0523,-0.0093,0.0327,0.043,-0.0078,-0.0762,0.04,0.0498,0.1037,-0.0285,0.018,-0.0205,0.0312,0.0425,-0.0478,-0.0744,-0.0253,-0.005,0.001,0.0356,-0.0711,0.0083,0.0767,-0.038,0.0354,-0.0034,0.0116,-0.0338,-0.0811,0.0416,0.0169,0.0296,0.0102,-0.0256,0.0416,-0.0179,0.0853,0.0097,0.0428,-0.022,0.0244,0.0438,-0.0165,0.0852,0.0148,-0.0436,0.0373,0.0028,-0.0084,-0.029,-0.0432,0.0043,0.0532,-0.0434,0.0058,0.0558,-0.0518,-0.0136,0.0366,-0.0022,-0.0005,0.0062,0.0001,-0.0169,-0.0554,-0.0008,0.0656,0.0383,-0.3039,0.0621,0.0263,0.0173,-0.0542,0.0044,0.0228,-0.0109,-0.0348,-0.0085,0.0379,0.1203,0.0522,-0.0141,0.0293,0.0389,0.0611,-0.0752,0.0699,-0.0709,0.0311,0.036,0.2319,-0.0614,0.0454,0.0185,-0.03,0.0267,0.007,-0.0167,0.0113,-0.0048,0.0167,-0.0266,0.0747,0.0756,-0.0282,0.0533,-0.0128,-0.0161,-0.0179,0.033,-0.0191,-0.0347,0.0993,-0.0088,-0.0022,-0.0444,-0.0093,0.0444,-0.0765,0.0107,-0.0098,-0.0418,0.0349,0.0331,-0.0582,-0.041,-0.0513,-0.0555,0.0033,-0.053,0.0277,-0.0101,-0.0155]}
{"key":"[A Multimodal Memes Classification: A Survey and Open Research Issues] Memes are graphics and text overlapped so that together they present concepts that become dubious if one of them is absent. It is spread mostly on social media platforms, in the form of jokes, sarcasm, motivating, etc. After the success of BERT in Natural Language Processing (NLP), researchers inclined to Visual-Linguistic (VL) multimodal problems like memes classification, image captioning, Visual Question Answering (VQA), and many more. Unfortunately, many memes get uploaded each day on social media platforms that need automatic censoring to curb misinformation and hate. Recently, this issue has attracted the attention of researchers and practitioners. State-of-the-art methods that performed significantly on other VL dataset, tends to fail on memes classification. In this context, this work aims to conduct a comprehensive study on memes classification, generally on the VL multimodal problems and cutting edge solutions. We propose a generalized framework for VL problems. We cover the early and next-generation works on VL problems. Finally, we identify and articulate several open research issues and challenges. This is the first study that presents the generalized view of the advanced classification techniques concerning memes classification to the best of our knowledge. We believe this study presents a clear road-map for the Machine Learning (ML) research community to implement and enhance memes classification techniques.","layer":0,"vector":[0.012,-0.0071,-0.0012,-0.0053,0.0545,0.0214,0.0323,0.0264,-0.0029,0.0026,0.0262,-0.0692,0.0318,0.0203,0.0491,0.0096,0.0684,0.0044,-0.0555,0.0075,0.0449,0.0033,-0.0193,-0.0585,0.0312,-0.0227,-0.0229,-0.0325,-0.0233,-0.2156,0.0142,-0.0202,0.0161,-0.0662,0.0104,-0.0139,-0.0235,0.0576,-0.0419,0.0342,-0.0152,-0.0543,-0.0431,-0.079,-0.043,-0.0529,-0.034,-0.0297,-0.0403,-0.0414,0.0461,-0.0287,-0.007,0.0243,-0.0068,0.0387,0.0483,0.0406,0.0411,-0.0034,0.031,0.0472,-0.1453,0.0886,0.041,-0.0019,-0.0206,0.0093,0.004,0.0431,-0.0287,0.0702,0.0279,0.0289,-0.028,0.0063,0.0242,-0.0115,-0.0396,-0.0149,0.0172,-0.0099,0.006,-0.0276,-0.001,-0.0518,0.0068,0.0088,0.0346,0.0001,-0.0451,-0.0118,-0.0029,0.0305,-0.0466,-0.0384,0.0277,0.0427,-0.0485,0.2461,-0.0613,0.01,0.0421,-0.0835,0.044,-0.0025,-0.0234,-0.0576,-0.0352,-0.0197,-0.0392,-0.0206,0.0352,-0.0151,0.0602,-0.0203,0.0583,0.0242,-0.005,0.0093,-0.0206,0.028,0.0251,-0.0449,0.0388,-0.0489,0.051,0.1402,0.0394,-0.0328,0.018,0.0157,-0.0204,-0.0333,-0.0077,0.0238,-0.021,-0.0134,0.039,-0.0063,-0.0152,-0.1137,0.0124,-0.0786,-0.0501,0.0659,-0.0616,0.0076,-0.0499,-0.0203,0.0005,0.0228,-0.0322,0.0161,0.0382,0.0057,0.0438,0.0656,-0.0424,0.027,0.0197,-0.0951,-0.0271,0.0924,0.0112,-0.0966,-0.052,-0.002,0.0217,-0.0545,0.0581,0.0429,-0.0313,0.0163,0.0385,0.0406,-0.0754,0.0124,0.0111,0.0006,0.0019,-0.0619,-0.02,0.0738,0.0431,-0.0559,0.0128,-0.0809,0.0932,0.0359,0.0065,0.0357,-0.0271,-0.0099,-0.0598,-0.0269,-0.0003,-0.0425,0.0101,-0.0444,0.0249,0.0046,-0.0201,0.0406,-0.0153,0.0027,-0.0117,0.0255,0.0807,0.037,-0.0324,-0.0224,-0.0053,-0.0315,-0.0123,-0.0362,0.0315,0.0533,-0.0097,0.0297,0.0802,-0.0142,-0.0339,-0.2449,-0.0393,0.027,-0.045,0.0487,-0.0346,-0.0073,-0.0457,0.1054,0.0728,0.0692,-0.0603,-0.0516,-0.0096,0.0226,-0.0021,0.0019,0.0576,-0.0241,-0.0318,-0.0204,0.0049,-0.0065,-0.0878,0.0657,-0.0345,0.2335,0.094,-0.0055,-0.0182,0.0657,0.0775,-0.0587,-0.1155,0.0552,0.0261,0.0149,-0.0416,-0.0284,0.0205,-0.001,0.0251,-0.0094,-0.0938,-0.0249,-0.0435,-0.0001,-0.0018,-0.0702,0.0433,0.0399,-0.0013,0.0492,0.0163,-0.0025,-0.0234,-0.1111,-0.0116,-0.0308,0.0174,0.0091,-0.0363,0.0508,-0.0711,0.0499,0.0197,-0.0464,-0.0077,0.0369,-0.0355,-0.0175,0.1216,-0.0087,-0.0061,0.0486,0.0437,0.0334,-0.0143,-0.0332,-0.0085,0.0342,0.0241,0.0231,-0.0158,-0.0093,0.0003,0.0509,-0.008,0.0349,0.0189,0.052,0.0389,0.0118,-0.0329,0.0081,-0.001,-0.2941,0.0312,0.0343,0.028,0.0034,0.0493,0.0476,-0.008,-0.0377,-0.0104,-0.016,0.0619,0.0613,-0.0507,-0.0367,0.003,0.0552,-0.0479,0.0498,-0.0311,0.0076,0.0149,0.2206,-0.0442,0.0082,0.0173,0.0085,-0.0164,0.0434,-0.0214,-0.0124,-0.0114,0.0974,-0.0407,0.0435,0.0336,-0.088,0.0269,0.03,0.007,-0.0,0.0235,-0.0302,-0.043,0.0571,0.014,0.0322,-0.0288,0.0244,0.0129,-0.0212,0.0044,-0.0302,0.0479,0.0217,-0.0001,0.0015,-0.0635,-0.0235,-0.04,0.0035,-0.0408,0.002,0.0084,0.0011]}
{"key":"[Learning from partial correction] We introduce a new model of interactive learning in which an expert examines the predictions of a learner and partially fixes them if they are wrong. Although this kind of feedback is not i.i.d., we show statistical generalization bounds on the quality of the learned model.","layer":6,"vector":[-0.0393,-0.0169,0.0601,-0.018,-0.0112,0.0142,0.0214,0.0233,-0.0177,-0.0003,0.008,-0.0558,0.0226,0.0499,-0.0036,0.0137,-0.022,0.0398,-0.0491,0.0086,0.0208,-0.022,-0.0018,-0.0528,-0.0037,0.0167,-0.051,-0.0998,-0.0171,-0.2228,-0.0166,-0.0152,0.0401,-0.0005,-0.0298,-0.0125,-0.0339,0.0172,-0.0002,0.0397,0.0347,-0.0161,-0.0231,-0.0277,-0.0092,-0.0225,-0.0169,-0.0182,-0.0179,-0.0396,0.0117,-0.0413,0.0116,0.0264,0.0434,0.0568,0.059,0.081,0.0375,0.0668,0.0219,0.0701,-0.1688,0.0966,0.0242,0.0444,-0.0423,-0.0446,0.0063,0.0697,-0.0187,0.0412,0.0416,0.0545,0.0288,-0.0044,-0.0218,-0.0069,0.0191,0.0394,0.0075,-0.0789,-0.04,-0.0282,-0.0096,-0.0835,0.0562,-0.0315,0.0733,0.0529,-0.0295,-0.0115,-0.0199,0.0306,-0.0139,0.0167,0.0575,0.0078,-0.0618,0.1844,-0.0184,0.0162,0.0448,-0.0263,0.0297,-0.0205,-0.0108,-0.0027,-0.028,0.0066,-0.0525,-0.0139,0.0612,-0.0369,0.0225,0.0278,0.0931,0.0478,-0.0083,-0.0087,-0.008,0.0182,0.0617,-0.0037,-0.0104,-0.0707,0.0084,0.1121,-0.0093,0.0145,0.0285,-0.0584,-0.0523,-0.0037,-0.0001,0.0251,0.0515,0.0055,0.0675,-0.0012,-0.044,-0.0489,-0.0471,-0.0634,-0.0691,0.1119,-0.0406,0.0576,-0.0067,-0.0493,-0.0141,0.0148,-0.0216,-0.0229,0.0153,0.0332,0.0572,0.0005,-0.0141,0.0026,-0.0249,-0.0686,-0.009,0.0564,0.0355,-0.0092,-0.0598,0.0087,0.051,-0.0248,0.031,0.0457,-0.0314,-0.0096,0.0695,0.0356,-0.1221,0.0119,0.0145,0.0223,0.0702,-0.0387,-0.0146,0.0789,0.0306,-0.01,0.0133,-0.0545,0.0631,0.0444,-0.0023,0.0305,-0.0397,0.0187,-0.0722,-0.0309,-0.013,-0.0296,-0.0178,-0.0447,-0.0196,-0.0018,-0.0584,0.0034,-0.0167,0.0434,-0.0242,-0.0058,0.0858,-0.0063,-0.0396,-0.0088,0.0214,-0.0463,-0.0471,0.0028,0.0474,0.007,0.0026,0.0669,0.0006,-0.0215,-0.0349,-0.244,-0.021,-0.0029,0.0296,0.0481,-0.0932,0.0721,0.0178,0.0269,0.0588,0.0159,-0.0203,-0.0125,0.0237,-0.0122,0.0233,0.0106,-0.0032,-0.028,-0.0091,-0.0745,0.0254,-0.0309,-0.0817,0.0451,0.0035,0.2129,0.0617,0.028,-0.0516,0.0553,0.0087,-0.0427,-0.0822,0.0638,0.051,0.04,-0.0364,-0.0214,-0.0127,0.0073,0.0424,-0.0167,-0.1014,-0.0569,-0.0332,-0.0809,-0.0284,-0.0812,0.0165,0.0141,-0.0088,0.0202,0.0212,-0.0294,-0.0407,-0.1133,0.0355,-0.0386,0.0635,0.0372,-0.0637,0.0148,-0.0703,0.0091,0.0071,-0.0127,-0.05,0.067,-0.0301,-0.0194,0.0696,-0.0282,-0.0116,0.003,0.0248,0.0325,-0.0557,-0.0405,-0.0292,0.0526,-0.0271,0.0233,0.0228,0.0228,-0.0085,0.0478,-0.0213,0.0346,-0.0568,-0.0089,0.0315,-0.0312,0.0165,0.041,-0.0408,-0.2943,0.0433,-0.0018,0.0395,-0.0372,0.0495,0.0395,-0.0072,-0.0386,0.004,0.0058,0.0213,0.0462,0.0204,0.007,0.0212,0.0793,-0.0654,0.0275,-0.054,0.0114,0.0934,0.248,-0.0212,0.0347,0.0046,-0.0257,-0.0318,0.0663,-0.0274,0.0064,0.0195,0.0673,-0.003,0.0466,0.0804,-0.0456,0.0598,0.0078,0.0283,-0.0389,-0.0216,-0.0659,-0.005,0.1054,0.0514,0.0301,-0.0415,-0.0125,0.0222,-0.0058,0.003,-0.0037,-0.0135,-0.0261,0.0288,-0.0378,-0.0431,-0.0363,-0.0438,0.0323,-0.0628,0.0264,-0.0241,-0.0104]}
{"key":"[Model-Based Episodic Memory Induces Dynamic Hybrid Controls] Episodic control enables sample efficiency in reinforcement learning by recalling past experiences from an episodic memory. We propose a new model-based episodic memory of trajectories addressing current limitations of episodic control. Our memory estimates trajectory values, guiding the agent towards good policies. Built upon the memory, we construct a complementary learning model via a dynamic hybrid control unifying model-based, episodic and habitual learning into a single architecture. Experiments demonstrate that our model allows significantly faster and better learning than other strong reinforcement learning agents across a variety of environments including stochastic and non-Markovian settings.","layer":7,"vector":[-0.0483,0.0034,0.0552,-0.0161,-0.0109,0.0306,0.022,0.0308,0.0507,0.0168,0.0283,-0.0282,0.0253,0.0826,0.0182,0.0079,-0.0541,0.0619,-0.0008,-0.015,0.0427,-0.0522,0.0158,0.0078,-0.0158,0.0459,-0.0629,-0.0446,-0.0301,-0.2226,0.0237,-0.0298,0.0007,-0.0563,-0.0222,-0.0415,-0.0798,0.0362,-0.0317,0.0361,0.0362,0.0239,-0.0236,-0.064,-0.0027,-0.0084,-0.0243,0.011,0.0382,-0.0559,-0.008,-0.0035,-0.0239,0.0139,0.0557,0.0689,0.0527,0.1055,0.0633,0.0344,0.0093,-0.0124,-0.1531,0.0773,0.028,0.0673,-0.0572,0.004,0.0417,0.038,-0.0802,0.0739,-0.0215,0.0283,0.0097,-0.0134,-0.005,-0.0379,-0.0001,-0.025,0.0227,-0.0378,-0.0487,-0.0262,-0.0113,-0.0768,-0.013,-0.0605,0.0631,0.0088,-0.0198,0.0106,-0.0344,0.0047,-0.0459,-0.0146,0.0532,0.0359,-0.0714,0.2186,-0.0328,0.0447,0.0202,-0.0159,0.0561,-0.0326,-0.0213,-0.0513,-0.0693,-0.0079,-0.0249,-0.021,0.041,-0.0273,0.0064,0.0079,0.0563,0.0327,-0.0005,-0.0146,-0.0132,0.0154,0.0499,-0.0525,0.0196,-0.0866,0.0259,0.1197,0.0109,0.0045,0.0558,-0.0366,0.0013,-0.0185,-0.042,0.0261,0.0179,-0.037,0.0356,-0.0129,-0.0299,0.004,0.0014,-0.1332,-0.0678,0.1239,0.0549,0.0038,-0.055,0.0075,-0.0467,0.0159,0.0167,-0.0562,-0.0145,0.0177,0.0559,0.053,-0.0481,-0.0126,-0.078,-0.0622,-0.0192,0.0792,-0.0282,-0.0579,-0.0228,-0.013,0.0071,-0.0189,0.0332,0.0307,-0.0574,0.0229,0.0618,0.0087,-0.0729,0.0161,-0.013,0.0149,0.0297,-0.0884,-0.021,0.0669,0.0497,-0.0534,0.0131,-0.0164,0.0191,0.0513,0.0172,0.0465,0.0122,0.0147,-0.0543,-0.0645,0.0211,-0.0379,-0.0018,-0.037,-0.0337,0.0078,-0.0376,0.0154,0.0132,-0.0097,-0.0422,0.0121,0.0575,-0.0121,-0.0406,0.0148,0.0509,-0.0115,-0.0273,0.0035,0.0154,0.0133,0.0388,0.0325,0.0638,0.0149,-0.0288,-0.2075,0.0064,-0.0157,0.0058,0.0608,-0.0659,0.0277,-0.0353,0.049,0.0212,0.0399,-0.0528,-0.0116,0.0426,-0.0343,0.0651,0.0415,0.0185,-0.0041,-0.0014,0.0172,-0.0176,-0.0382,-0.0922,0.0617,-0.0193,0.2354,0.0192,0.0507,-0.0074,0.001,0.0461,0.0091,-0.1033,0.0198,0.0084,0.0621,-0.0246,-0.0241,-0.0787,-0.0343,0.0061,-0.0229,-0.0961,-0.0222,-0.0572,-0.0385,0.0666,-0.0136,0.009,0.0318,-0.0463,0.0172,-0.0313,-0.0444,-0.0472,-0.0687,0.0317,-0.0228,0.0576,0.0154,-0.041,0.0071,-0.0403,0.0461,0.0204,0.0464,-0.0881,0.0305,0.0181,-0.0223,0.0721,-0.0154,-0.0107,0.034,-0.0205,0.0094,-0.0174,-0.0562,-0.015,0.0521,-0.0326,0.0183,0.0773,0.0429,-0.0435,0.0699,-0.0217,0.0443,-0.0258,0.0002,0.0105,-0.0605,-0.0136,0.0551,-0.0125,-0.2932,0.0365,-0.0007,0.014,-0.0196,0.0164,0.0169,0.0077,-0.0462,0.019,-0.0043,0.0511,0.0584,0.0199,0.003,0.0654,0.0798,0.0057,0.0681,-0.0884,0.0132,0.0551,0.2156,-0.0022,0.0401,0.0134,-0.0206,0.0232,0.0458,-0.0169,0.0363,-0.002,0.0628,-0.0417,0.0899,0.0697,-0.0106,0.0356,0.0015,-0.0251,-0.0257,0.0221,-0.0037,-0.0178,0.0956,-0.0112,0.0143,-0.0471,-0.036,0.052,0.0021,-0.0067,-0.0239,0.0027,0.0518,0.0191,-0.046,-0.0582,-0.0478,-0.0662,0.0057,-0.0487,0.0182,-0.0122,0.0073]}
{"key":"[Neural Differential Equations for Learning to Program Neural Nets Through Continuous Learning Rules] Neural ordinary differential equations (ODEs) have attracted much attention as continuous-time counterparts of deep residual neural networks (NNs), and numerous extensions for recurrent NNs have been proposed. Since the 1980s, ODEs have also been used to derive theoretical results for NN learning rules, e.g., the famous connection between Oja's rule and principal component analysis. Such rules are typically expressed as additive iterative update processes which have straightforward ODE counterparts. Here we introduce a novel combination of learning rules and Neural ODEs to build continuous-time sequence processing nets that learn to manipulate short-term memory in rapidly changing synaptic connections of other nets. This yields continuous-time counterparts of Fast Weight Programmers and linear Transformers. Our novel models outperform the best existing Neural Controlled Differential Equation based models on various time series classification tasks, while also addressing their scalability limitations. Our code is public.","layer":1,"vector":[-0.0651,0.0094,0.0198,-0.0238,0.012,0.0564,0.0254,0.0084,0.0463,-0.0084,0.0265,-0.0054,0.063,0.0233,0.0312,-0.0032,-0.0297,0.0477,-0.0595,0.0249,0.0078,-0.0269,-0.0298,-0.0257,-0.0011,-0.0069,-0.0447,-0.0274,-0.0405,-0.2424,0.0046,-0.0346,0.0192,-0.0415,0.0045,-0.0077,-0.043,0.0118,-0.021,0.0361,0.0116,0.0441,0.0146,-0.0628,0.014,-0.041,-0.0414,-0.0087,-0.0081,-0.0559,0.013,-0.0248,0.043,0.032,0.0038,0.0227,0.072,0.0037,0.0665,0.0228,0.062,0.0183,-0.1692,0.0569,0.0117,0.0231,-0.0155,-0.0128,0.0086,0.0757,-0.0327,0.0497,0.0053,0.0299,-0.0116,0.011,-0.0196,-0.0075,-0.0328,0.042,0.0614,-0.0011,0.0052,-0.0757,-0.0009,-0.0182,0.0193,-0.0671,0.0269,-0.0213,-0.0417,-0.0065,-0.0247,0.017,-0.0473,-0.0145,0.0291,0.0455,-0.0386,0.1953,-0.046,0.0191,0.0221,-0.004,0.0289,-0.0082,-0.0307,-0.0141,-0.0279,0.0063,-0.0341,-0.0088,0.0181,-0.0492,0.0284,0.0275,0.0222,0.0389,-0.0074,0.0047,-0.0385,0.0283,0.0239,-0.0544,0.0068,-0.0361,-0.0173,0.1459,0.0612,0.055,0.0631,-0.0279,-0.0478,0.0193,0.0315,0.0474,0.0396,-0.0226,-0.0267,-0.0268,-0.0965,-0.0036,-0.0011,-0.0811,-0.0828,0.1167,-0.0006,0.0196,-0.0451,-0.0072,-0.0551,0.0355,-0.0401,-0.0153,0.0262,0.0226,0.0624,0.0201,-0.0556,0.0339,-0.0443,-0.0536,-0.0285,0.1171,0.0095,-0.0487,-0.0198,-0.0014,0.0135,-0.0264,0.0347,0.0323,-0.0247,0.0046,0.0908,0.0128,-0.0435,-0.025,-0.0243,-0.0019,-0.0115,-0.0546,-0.0332,0.0611,0.0709,-0.0389,0.0391,-0.039,0.0096,0.044,-0.055,0.0226,-0.0138,0.0081,-0.0126,-0.0524,-0.0329,-0.0173,0.0357,-0.0619,-0.0079,-0.0112,-0.0361,0.0011,0.0106,-0.0145,-0.027,0.0153,-0.002,0.02,-0.0305,-0.0008,0.0816,-0.0391,-0.0271,0.0135,-0.0332,0.0252,0.004,0.036,0.0587,-0.0581,-0.0409,-0.2199,0.0083,0.0421,-0.0403,0.0742,-0.1038,0.0185,-0.0333,0.038,0.0732,0.0251,0.0104,0.0094,-0.0283,0.0145,0.066,0.0633,0.0415,-0.0097,-0.0093,-0.0507,-0.0165,0.0476,-0.0892,0.0534,0.0148,0.1968,0.0084,0.1072,-0.0627,0.0316,0.0393,0.0065,-0.0846,0.0522,-0.0147,0.0909,-0.0036,-0.0359,-0.0527,-0.049,0.023,-0.0263,-0.0983,-0.0626,-0.0348,0.0041,-0.0112,-0.0394,0.0046,0.0039,-0.0309,0.03,-0.0088,-0.0163,-0.0715,-0.0838,0.0351,-0.0855,0.024,-0.0008,-0.0366,-0.0172,-0.0252,0.0431,0.0267,0.008,-0.0662,0.0093,-0.0237,-0.005,0.0792,0.0187,0.0104,0.0533,-0.0203,-0.0162,-0.0067,-0.0607,0.0347,0.0563,-0.0368,0.0601,0.0589,0.0309,0.0127,0.0771,-0.0317,-0.0071,-0.0137,0.0191,0.0314,-0.0597,0.0084,0.0443,-0.0109,-0.3046,0.0469,0.0446,0.0262,0.0098,0.0159,0.0248,-0.0058,-0.0674,-0.005,-0.0446,0.0621,0.058,0.0195,0.0527,0.0452,0.0737,-0.0544,0.0577,-0.052,0.0272,0.0719,0.2209,-0.0291,0.0743,-0.0094,-0.0283,-0.0041,0.0836,-0.0274,0.0279,0.0327,0.0645,-0.0399,0.0327,0.0568,-0.082,0.0633,0.0452,-0.0145,0.0191,-0.0211,-0.0329,-0.0467,0.0798,0.0281,-0.0053,-0.0634,-0.0057,0.0255,-0.0476,-0.023,0.0007,0.0135,0.0014,0.0335,-0.0734,-0.0152,-0.0495,-0.0423,0.0245,-0.0939,0.0373,0.0114,-0.0203]}
{"key":"[Contextual Online Learning for Multimedia Content Aggregation] The last decade has witnessed a tremendous growth in the volume as well as the diversity of multimedia content generated by a multitude of sources (news agencies, social media, etc.). Faced with a variety of content choices, consumers are exhibiting diverse preferences for content; their preferences often depend on the context in which they consume content as well as various exogenous events. To satisfy the consumers' demand for such diverse content, multimedia content aggregators (CAs) have emerged which gather content from numerous multimedia sources. A key challenge for such systems is to accurately predict what type of content each of its consumers prefers in a certain context, and adapt these predictions to the evolving consumers' preferences, contexts and content characteristics. We propose a novel, distributed, online multimedia content aggregation framework, which gathers content generated by multiple heterogeneous producers to fulfill its consumers' demand for content. Since both the multimedia content characteristics and the consumers' preferences and contexts are unknown, the optimal content aggregation strategy is unknown a priori. Our proposed content aggregation algorithm is able to learn online what content to gather and how to match content and users by exploiting similarities between consumer types. We prove bounds for our proposed learning algorithms that guarantee both the accuracy of the predictions as well as the learning speed. Importantly, our algorithms operate efficiently even when feedback from consumers is missing or content and preferences evolve over time. Illustrative results highlight the merits of the proposed content aggregation system in a variety of settings.","layer":1,"vector":[-0.0399,-0.0047,0.0023,-0.0178,0.0405,0.0477,0.0211,0.0207,0.0258,0.0002,-0.0082,-0.0465,-0.0039,0.0376,0.0328,-0.0104,0.0315,-0.0118,-0.0374,0.0022,0.0223,-0.0303,-0.0027,-0.0422,0.0269,-0.0281,-0.0478,-0.0478,-0.0665,-0.2114,-0.0206,-0.0588,0.0776,-0.0004,0.0266,-0.0269,0.002,0.0251,-0.0398,0.0716,-0.0114,0.0474,-0.0123,-0.1017,-0.0596,-0.0434,0.0015,-0.0122,-0.0142,-0.0408,0.0355,-0.0152,0.0121,0.0297,-0.0019,0.0764,0.0097,0.0646,0.0144,0.0522,0.004,0.0377,-0.1291,0.0874,0.0074,0.0489,-0.0203,0.0613,-0.0054,0.0408,-0.0232,0.0783,0.01,0.0231,0.0398,-0.0127,0.0067,-0.0276,-0.0375,0.0051,-0.0091,-0.0133,-0.0822,-0.0308,0.0139,-0.0539,0.0178,-0.0709,0.0429,0.0139,-0.0442,0.0351,-0.0212,0.0265,-0.0572,-0.0513,0.0021,0.0025,-0.0227,0.2034,-0.0341,0.0243,0.0777,-0.0507,0.0312,-0.0063,-0.0379,0.0014,-0.0239,-0.0243,-0.0011,-0.033,0.0348,-0.0425,0.0284,-0.0035,0.0375,0.047,0.0183,-0.0416,-0.0508,0.0186,0.0573,-0.0519,0.0352,-0.0846,0.0321,0.1127,0.0042,-0.0143,0.0407,-0.0116,-0.0453,-0.0668,0.0371,0.0286,-0.0165,0.025,0.0032,0.0241,-0.0628,-0.0663,-0.018,-0.0879,-0.0348,0.1762,-0.0003,0.0551,-0.0518,-0.0253,-0.0082,-0.0075,-0.0196,-0.0054,0.0017,0.0369,0.0468,0.0451,-0.0654,0.0207,-0.031,-0.0463,0.0114,0.0971,-0.0003,-0.0752,-0.0338,0.0149,0.0377,0.0003,0.0245,0.0067,-0.0119,0.0167,0.1184,0.0292,-0.0783,0.0195,0.0444,-0.0045,0.0172,-0.0547,-0.0577,0.0561,0.0161,-0.0175,0.0271,-0.0758,0.0579,0.0345,-0.0187,0.0039,-0.0006,-0.0131,0.016,-0.0497,-0.0134,0.0101,0.039,0.0053,-0.0076,-0.0175,-0.053,-0.0287,-0.0171,0.0175,-0.0224,-0.0007,0.0681,-0.0355,-0.0401,-0.0118,0.028,-0.0078,-0.0168,-0.0302,0.0866,0.0689,0.0183,0.0494,0.0372,-0.0216,-0.0094,-0.1955,-0.0,0.0042,-0.0231,0.0389,-0.0389,0.0506,-0.0251,0.0558,0.081,0.0391,-0.0065,-0.0154,-0.0005,0.013,0.0228,0.033,0.0457,0.0041,0.0092,-0.0363,0.0008,0.0151,-0.0914,0.0665,0.0028,0.2101,0.068,-0.0155,-0.0475,0.0754,0.0605,-0.0693,-0.0873,-0.0021,0.0361,0.047,-0.0014,-0.0429,-0.047,0.031,0.0108,-0.0148,-0.1241,-0.0401,-0.0358,-0.0281,0.0455,-0.072,0.0031,0.0467,-0.0538,0.0408,-0.0082,-0.0395,0.0019,-0.0201,0.039,-0.0286,0.0437,0.0248,-0.0376,0.0403,-0.0864,0.0327,0.0046,0.0103,-0.0025,0.011,-0.0454,-0.0037,0.0879,-0.0235,-0.0233,-0.0055,0.0052,0.0403,-0.0257,-0.0029,-0.0356,0.0716,-0.0641,0.0743,0.0571,0.0186,-0.0178,0.0411,-0.0154,0.0237,-0.04,0.0297,0.0186,-0.0563,-0.0456,0.043,-0.0111,-0.3194,0.0594,-0.0132,0.0294,-0.0026,0.0333,0.0665,0.0036,-0.0235,0.0246,0.0275,0.0513,0.0363,-0.0233,0.0037,0.0699,0.0793,-0.0161,0.0378,-0.0695,0.0343,0.0178,0.2482,-0.0193,-0.0299,-0.0202,-0.027,-0.018,0.0349,-0.0026,-0.0212,-0.0171,0.1186,-0.0861,0.0003,0.0385,-0.0677,0.0426,0.006,-0.0164,-0.042,-0.0217,-0.0306,0.0252,0.0261,0.0427,0.0089,-0.0863,-0.0667,0.0285,-0.013,-0.0017,0.0137,0.0153,-0.0095,0.0128,-0.0152,-0.0574,-0.0166,-0.0731,0.0259,-0.0289,-0.0078,0.012,-0.0126]}
{"key":"[Deep Learning on Graphs: A Survey] Deep learning has been shown to be successful in a number of domains, ranging from acoustics, images, to natural language processing. However, applying deep learning to the ubiquitous graph data is non-trivial because of the unique characteristics of graphs. Recently, substantial research efforts have been devoted to applying deep learning methods to graphs, resulting in beneficial advances in graph analysis techniques. In this survey, we comprehensively review the different types of deep learning methods on graphs. We divide the existing methods into five categories based on their model architectures and training strategies: graph recurrent neural networks, graph convolutional networks, graph autoencoders, graph reinforcement learning, and graph adversarial methods. We then provide a comprehensive overview of these methods in a systematic manner mainly by following their development history. We also analyze the differences and compositions of different methods. Finally, we briefly outline the applications in which they have been used and discuss potential future research directions.","layer":4,"vector":[-0.016,-0.0211,-0.0208,-0.0267,0.0573,0.0608,0.0002,0.0327,0.0368,-0.019,0.0187,-0.0396,0.0634,0.0921,0.0503,0.0384,0.0107,0.0788,-0.0387,-0.0354,0.0081,-0.0599,0.002,-0.0585,0.0606,-0.0035,-0.0132,-0.049,-0.0725,-0.212,0.012,-0.0472,0.0673,-0.0682,-0.0295,-0.057,0.012,0.031,-0.0049,0.0452,0.0216,0.0073,-0.0377,-0.0336,0.0086,-0.031,-0.0354,-0.0392,0.0084,-0.0416,0.0315,-0.0511,0.0442,0.0231,0.0442,0.0269,0.0801,0.047,0.0343,0.0873,0.0382,0.0436,-0.1575,0.0395,0.0329,0.0553,-0.0438,0.0129,-0.0086,0.0672,0.0287,0.0229,-0.0056,-0.0209,0.0179,0.0444,-0.0045,-0.0301,-0.0116,-0.0548,0.0027,-0.0341,-0.0555,-0.0254,0.0294,-0.0245,-0.0024,-0.0428,0.0316,0.021,-0.0051,0.007,-0.0215,0.015,-0.0552,-0.0098,0.0536,-0.0144,-0.0853,0.1795,-0.0701,0.0258,0.0196,-0.0105,0.0306,-0.0357,-0.0046,-0.0506,-0.0346,0.0166,-0.0253,-0.0489,0.0311,-0.0397,0.0118,-0.0317,0.059,0.0512,-0.0151,-0.0089,-0.0399,0.0239,0.035,-0.0421,0.0229,-0.0617,0.0039,0.0969,0.0525,0.02,0.0683,0.005,-0.0081,0.0131,-0.0025,0.0341,0.0438,-0.0373,0.005,0.0096,-0.0188,0.0127,0.0017,-0.0524,-0.0554,0.0972,-0.0659,-0.0416,-0.0414,-0.0535,-0.0425,0.0088,-0.0119,-0.0027,-0.0024,0.0254,0.0444,0.0865,-0.063,0.0387,-0.0483,-0.0273,-0.0701,0.103,0.0188,-0.1226,-0.0525,-0.0458,-0.006,-0.0377,0.0281,0.0379,-0.0197,0.0407,0.0731,0.0663,-0.0659,-0.0341,-0.0047,-0.0043,-0.0012,-0.0128,-0.0473,0.0228,0.0205,-0.061,0.0208,-0.0672,0.0137,0.0374,-0.012,0.065,-0.0325,0.0035,-0.0298,-0.0172,-0.0279,-0.0613,-0.0367,-0.0369,-0.0115,-0.0474,-0.0079,-0.0143,-0.0437,0.0297,-0.0552,0.0165,0.0209,-0.019,-0.0609,0.0155,0.0515,-0.0431,-0.016,-0.0007,0.0153,0.0126,0.0017,0.0533,0.0174,-0.0415,-0.0397,-0.1972,-0.037,-0.0007,-0.0437,0.0644,-0.0592,0.0121,0.0016,0.0963,0.0744,0.0621,0.0103,-0.0147,-0.0282,0.0242,0.059,0.0145,0.0615,-0.0229,-0.0089,-0.0016,-0.0006,0.015,-0.1011,0.0179,0.0503,0.2173,0.0224,0.0475,-0.0068,0.0074,0.0287,-0.0469,-0.096,0.0663,0.0309,0.0611,0.0044,-0.0179,-0.0325,-0.0285,0.0123,-0.0147,-0.1059,-0.0296,0.0272,-0.0202,0.006,-0.056,-0.0125,0.0368,-0.021,0.0627,0.0455,-0.0257,-0.0371,-0.0914,0.0599,-0.0509,-0.0027,0.0113,-0.084,0.0207,-0.0622,0.0702,0.0407,-0.0023,-0.056,0.0319,0.0451,0.0088,0.0997,0.0661,0.0002,0.0786,0.0199,0.0198,-0.0261,-0.0435,-0.0206,0.0622,-0.0241,0.0337,0.0103,0.0091,0.0208,0.0919,-0.0279,0.0885,-0.0011,0.0106,0.0146,-0.052,-0.0602,0.0557,-0.0254,-0.2993,0.0636,0.0544,0.0969,0.0034,0.0107,0.0289,0.0375,-0.0326,0.0143,0.0045,0.0548,0.041,-0.0303,-0.0363,0.0364,0.0703,-0.0167,0.0359,-0.0461,0.0425,0.0283,0.1998,-0.0223,0.0671,0.0372,-0.0214,-0.0012,0.0298,-0.044,-0.0456,-0.0122,0.0832,-0.0365,0.0374,0.0808,-0.0615,0.0545,0.0073,0.0224,0.0223,-0.0038,-0.0262,-0.0213,0.0465,-0.0171,-0.0361,-0.0316,0.0163,0.0278,-0.039,-0.0472,-0.0318,0.0208,0.0253,0.0207,-0.0334,-0.0343,-0.0493,-0.0688,0.0137,-0.0744,-0.0105,-0.0378,-0.0391]}
{"key":"[Relative Hausdorff Distance for Network Analysis] Similarity measures are used extensively in machine learning and data science algorithms. The newly proposed graph Relative Hausdorff (RH) distance is a lightweight yet nuanced similarity measure for quantifying the closeness of two graphs. In this work we study the effectiveness of RH distance as a tool for detecting anomalies in time-evolving graph sequences. We apply RH to cyber data with given red team events, as well to synthetically generated sequences of graphs with planted attacks. In our experiments, the performance of RH distance is at times comparable, and sometimes superior, to graph edit distance in detecting anomalous phenomena. Our results suggest that in appropriate contexts, RH distance has advantages over more computationally intensive similarity measures.","layer":0,"vector":[-0.0348,-0.0191,0.002,-0.0446,0.0702,0.0538,0.0158,0.0439,0.0428,-0.0448,0.0321,-0.0637,0.0467,0.0484,-0.0087,0.0187,-0.0575,0.0265,-0.0821,0.0209,0.0291,-0.0322,-0.0073,-0.0381,0.0599,0.0532,-0.0153,-0.0108,-0.0774,-0.2176,0.0288,-0.0543,0.0872,-0.0171,0.0226,-0.029,-0.0195,0.0226,-0.0278,0.0275,0.0158,0.0782,-0.0056,-0.0467,-0.0429,-0.0798,0.0198,0.0205,-0.0316,-0.0656,0.0187,-0.0544,0.0377,0.0204,0.0647,-0.0016,0.0872,0.0194,0.0413,0.0612,0.0413,0.0237,-0.1379,0.0057,0.047,0.0069,-0.0509,-0.039,0.0561,0.0428,-0.0219,0.0077,-0.0332,0.0392,0.0328,0.0227,-0.0501,-0.0537,-0.0368,0.021,-0.0335,-0.0386,-0.0429,-0.0285,-0.0208,-0.0262,0.0317,-0.0538,0.0595,0.0125,-0.0236,0.0385,0.0189,0.0275,-0.0716,-0.0512,0.0227,-0.0003,-0.0289,0.1986,-0.0362,0.0066,0.0195,-0.0062,0.0415,-0.0077,0.0166,-0.0608,0.0195,0.0144,-0.0101,-0.0352,0.0357,-0.0327,0.0473,-0.0039,0.0851,0.0437,-0.0236,-0.0144,-0.0524,0.0485,0.06,-0.045,0.0177,-0.0545,0.0244,0.1323,0.0203,-0.0022,0.0007,0.008,-0.0147,0.0153,-0.01,0.0259,-0.0011,-0.0022,0.0229,-0.0167,-0.0139,-0.0435,0.0192,-0.0627,-0.0528,0.1343,-0.0578,0.0429,-0.0423,-0.0264,-0.0144,0.0136,-0.033,-0.0249,0.0136,0.0485,0.0638,0.027,-0.0345,0.0187,-0.0076,-0.034,-0.0308,0.1209,0.071,-0.1074,0.0106,-0.0014,0.0225,-0.0056,-0.0154,0.0139,0.0179,0.0462,0.0396,0.0182,-0.0751,-0.0602,0.0198,-0.0205,0.0337,-0.0136,-0.0728,0.0452,0.0401,-0.0337,-0.0153,0.0154,0.0539,0.0735,-0.0456,-0.0049,-0.009,0.0141,-0.0328,-0.018,-0.01,-0.0453,-0.0076,-0.0429,0.0424,-0.0425,-0.0502,0.0157,-0.0375,0.0143,-0.0376,-0.0156,-0.007,0.0256,-0.053,-0.0275,-0.0233,-0.0169,-0.0003,0.0195,0.0209,0.0309,-0.0014,0.0262,0.0055,-0.0051,-0.0319,-0.2363,-0.0574,-0.028,-0.0317,0.0528,-0.048,0.0148,-0.0252,0.085,0.0401,0.031,-0.0158,-0.0287,0.0108,0.0065,0.1133,0.0554,0.0452,-0.0277,-0.0031,-0.0317,0.023,-0.0517,-0.0799,0.014,0.0174,0.2077,0.0447,0.0583,-0.0656,-0.005,-0.0111,-0.0152,-0.0802,0.0502,0.0419,0.0463,-0.0255,0.0022,-0.0326,-0.0747,0.0352,-0.0228,-0.0862,-0.0147,-0.0176,-0.0143,-0.0198,-0.01,0.0177,0.0686,0.0097,0.0536,0.0343,0.0076,-0.058,-0.0429,0.0234,-0.024,0.0614,0.019,-0.0641,0.0081,-0.0649,0.0805,-0.0141,-0.0385,-0.0212,0.0299,0.0166,-0.0195,0.1,0.0057,-0.0083,0.0376,0.0266,0.0012,-0.0294,-0.036,-0.0113,0.0496,-0.0339,0.0412,0.015,0.0128,-0.012,0.0694,0.037,0.0688,-0.0284,0.0482,-0.0189,-0.046,-0.0411,0.0062,-0.0222,-0.2781,0.0204,0.01,0.065,-0.0572,-0.0195,0.0724,0.0457,-0.0441,-0.0513,0.0555,0.0434,0.0103,-0.0317,-0.0322,0.0458,0.0378,-0.0628,0.0081,-0.0246,0.0637,0.0635,0.2376,-0.0587,0.0293,0.02,0.0101,0.0211,0.0104,-0.0298,-0.0075,0.0388,0.0421,-0.0632,0.0488,0.0475,0.0045,0.0654,-0.0062,-0.0147,-0.0188,-0.0239,-0.0785,-0.0371,0.1306,-0.0033,-0.0695,-0.0811,0.0362,0.0354,-0.0718,-0.0136,-0.0264,0.0251,0.0291,0.0494,-0.0118,-0.039,-0.0516,-0.0371,-0.0101,-0.0497,-0.0049,-0.0105,0.003]}
{"key":"[Boarding House Renting Price Prediction Using Deep Neural Network Regression on Mobile Apps] Boarding house is the most important requirement, especially for college students who live far away from the city, place of his origin or house. However, the problem we see now is the uneven distribution of study places in Indonesia which 75% of the best top educational institutions come from the island of Java. So, students who are looking for boarding houses rent requires more effort in comparing the various aspects desired. They need to survey one by one to the boarding house they want, even though they can survey online, it still requires more effort to pay attention to the desired facilities one by one. Therefore, we then created an Mobile Application that can predict prices based on student needs by comparing several variables, namely city, area, type of boarding house, and facilities. So, students can easily estimate the ideal price. The results of this study prove that we have succeeded in predicting prices for boarding houses rent well based on the variables we have determined, and modeling that variables using Deep Neural Network Regression.","layer":0,"vector":[0.0234,-0.0101,0.039,-0.0214,0.063,0.0002,0.0277,0.0461,0.029,0.0009,0.0097,-0.0193,0.0132,0.0649,0.0289,-0.0306,-0.0125,0.0109,-0.0225,0.0026,0.0511,-0.0186,-0.0257,-0.0903,0.0617,-0.0204,-0.0101,-0.051,-0.0337,-0.1862,0.0229,-0.0565,0.0827,-0.0295,0.0014,-0.02,-0.0424,0.022,-0.0079,0.0571,0.0334,0.0268,-0.0076,-0.0648,-0.0245,-0.0469,-0.0233,-0.0284,-0.0061,-0.0038,0.0434,-0.0237,-0.0009,0.0191,0.0252,0.0174,0.0437,0.0282,0.0156,0.0293,0.0575,0.0541,-0.2045,0.083,0.0509,-0.0166,-0.0196,-0.0196,0.0278,0.0401,0.0078,0.0405,0.033,0.0492,0.0434,0.0083,0.0182,-0.0188,0.0039,-0.014,0.0156,-0.0211,-0.0097,-0.0259,-0.0163,-0.0365,0.0238,-0.0448,0.0195,-0.0126,-0.0001,-0.0136,-0.0436,0.0204,-0.099,0.009,0.0074,0.0203,-0.0649,0.221,-0.0176,0.0573,0.0456,-0.0589,0.0016,-0.0363,-0.0181,-0.0283,-0.0413,0.0259,-0.0607,-0.0197,0.0166,-0.0386,-0.0157,0.0033,0.0576,0.0193,0.0349,-0.0125,-0.034,0.0117,0.0244,-0.0218,0.0015,-0.0913,-0.0125,0.1238,0.0228,0.0188,0.0057,-0.0703,-0.0851,-0.0378,0.0358,0.0522,0.054,0.0174,0.025,-0.0183,-0.0534,-0.0335,0.043,-0.0715,-0.0593,0.0591,0.0175,0.0422,-0.0262,-0.0354,-0.0298,0.0524,-0.0467,-0.0183,0.0193,0.0297,0.0368,0.0903,-0.0437,-0.0173,-0.0158,-0.0218,-0.0737,0.0962,-0.0047,-0.0929,-0.0015,0.016,-0.0253,-0.0421,0.0302,0.0564,-0.0453,0.0324,0.0887,0.0043,-0.0443,0.0603,-0.0431,0.0054,0.0129,-0.0153,-0.0505,0.031,0.0706,-0.0575,0.0005,-0.0638,0.0219,0.0442,-0.0303,0.0217,-0.0686,0.008,-0.0123,0.0041,-0.0016,-0.022,0.0509,-0.0659,0.0215,-0.0156,-0.0534,0.0282,-0.0124,0.0357,0.0349,0.0135,0.0719,0.0139,0.0052,0.0093,0.0803,-0.0814,-0.0356,-0.0173,0.0347,0.0561,0.0376,0.0645,0.0151,-0.0708,-0.0384,-0.2376,0.0113,0.0185,-0.053,0.0279,-0.058,0.0195,-0.0265,0.0319,0.0576,0.0703,-0.0581,-0.0115,0.061,-0.0025,0.0392,0.049,0.0375,-0.0338,-0.0192,-0.0114,0.0141,0.0245,-0.0706,0.0624,0.018,0.1691,-0.013,0.0556,-0.0307,0.0604,0.0041,-0.029,-0.1118,0.0702,0.0045,0.086,-0.0098,-0.0403,-0.0669,-0.0251,0.0686,0.0045,-0.0724,-0.0569,-0.0093,-0.0098,0.0252,-0.055,0.0089,-0.0059,-0.0263,0.0878,0.0029,-0.0127,-0.0174,-0.0714,0.0659,-0.026,0.0496,0.008,-0.0625,0.0208,-0.0249,0.0315,-0.0017,-0.003,-0.0214,-0.0033,0.0039,-0.0145,0.0718,-0.013,0.0129,0.0482,-0.0213,0.0211,-0.0113,-0.0027,0.0075,0.0274,-0.0263,0.0272,0.016,0.0398,0.0262,0.08,-0.026,0.0293,-0.0485,-0.0281,0.0061,-0.0614,-0.031,-0.0045,0.0109,-0.2977,0.0413,-0.0353,0.0459,-0.024,-0.0027,0.0182,0.0402,-0.039,-0.0005,0.0296,0.0026,0.061,-0.061,0.025,-0.0228,0.0882,-0.0463,0.0511,-0.0563,0.0134,0.025,0.228,-0.0686,0.0452,0.008,-0.013,0.0218,0.057,-0.0271,-0.0199,-0.0356,0.1049,-0.0589,0.0079,0.1017,-0.0125,0.0175,0.0119,0.0203,-0.0198,0.0146,-0.0638,0.0052,0.0885,0.0057,-0.0435,-0.0373,0.0098,0.0472,-0.0635,-0.04,-0.0241,-0.0294,0.0421,-0.0026,-0.1061,-0.031,-0.0696,-0.0561,0.0284,-0.0264,0.0009,-0.0575,0.038]}
{"key":"[Understanding and Overcoming the Challenges of Efficient Transformer Quantization] Transformer-based architectures have become the de-facto standard models for a wide range of Natural Language Processing tasks. However, their memory footprint and high latency are prohibitive for efficient deployment and inference on resource-limited devices. In this work, we explore quantization for transformers. We show that transformers have unique quantization challenges -- namely, high dynamic activation ranges that are difficult to represent with a low bit fixed-point format. We establish that these activations contain structured outliers in the residual connections that encourage specific attention patterns, such as attending to the special separator token. To combat these challenges, we present three solutions based on post-training quantization and quantization-aware training, each with a different set of compromises for accuracy, model size, and ease of use. In particular, we introduce a novel quantization scheme -- per-embedding-group quantization. We demonstrate the effectiveness of our methods on the GLUE benchmark using BERT, establishing state-of-the-art results for post-training quantization. Finally, we show that transformer weights and embeddings can be quantized to ultra-low bit-widths, leading to significant memory savings with a minimum accuracy loss. Our source code is available at~\\url{https://github.com/qualcomm-ai-research/transformer-quantization}.","layer":0,"vector":[-0.0794,-0.0078,0.0024,-0.0362,-0.0135,-0.0086,-0.0187,0.0291,-0.0004,0.0131,0.0192,-0.0745,0.0432,0.0343,0.0351,0.0319,-0.0167,0.0382,-0.0525,-0.0219,0.0708,-0.0402,0.0034,0.0016,0.003,0.0038,-0.0143,-0.0516,0.0057,-0.2161,-0.0128,-0.0512,0.0286,0.0012,0.0006,0.0128,-0.0529,0.0251,0.0126,0.0082,0.0143,0.0175,-0.0012,-0.0722,-0.0281,-0.0437,-0.0108,-0.0383,-0.0529,-0.0468,0.049,-0.0062,0.0436,0.0825,0.0205,0.0242,0.0726,0.0532,0.0506,0.0208,0.0156,0.0546,-0.1365,0.0919,-0.0076,0.0028,-0.0347,-0.0244,-0.0248,0.0176,-0.0086,-0.002,0.0458,0.0169,0.0443,0.0097,0.009,-0.0274,-0.0136,0.027,0.0172,-0.0268,-0.0536,0.0032,-0.0485,-0.0735,0.0381,-0.0209,0.0107,-0.0058,-0.0509,0.0119,-0.0603,0.0369,-0.0336,-0.0699,0.0448,0.0048,-0.0666,0.2176,-0.0418,0.0202,-0.0114,-0.0421,0.041,-0.0111,-0.0514,-0.0212,-0.0676,0.0117,-0.0291,-0.0255,-0.0153,-0.0263,0.0485,-0.0095,0.0813,-0.0009,-0.0383,0.0147,-0.0013,0.0291,0.028,-0.0196,0.0401,-0.0522,0.0623,0.1197,0.0399,0.0677,0.0214,0.0269,-0.0367,-0.0034,0.036,0.0545,0.0318,-0.0486,0.0258,0.0488,-0.0194,-0.0324,-0.0227,-0.0498,-0.0333,0.1096,-0.0641,0.0093,-0.0349,-0.0132,-0.0209,0.0451,-0.0352,-0.0337,0.0442,0.0314,0.0195,0.0354,-0.0588,-0.0335,0.0121,-0.0587,-0.0602,0.081,0.0132,-0.0671,-0.0104,-0.0311,0.0313,-0.0522,0.0561,0.0233,-0.0196,0.045,0.0666,0.0372,-0.028,-0.0064,0.0042,0.0122,0.0288,-0.0194,-0.0037,0.049,0.0445,-0.0455,-0.0082,-0.0518,0.0383,0.0412,-0.0352,0.0448,-0.0481,-0.0323,0.0007,0.004,-0.0364,0.0086,-0.0168,-0.045,-0.009,-0.0192,-0.0453,0.0275,-0.018,0.0088,-0.0004,-0.0018,0.0332,0.0493,0.006,-0.0536,0.101,-0.0291,-0.0337,0.0191,0.0207,0.0497,0.0158,0.0324,-0.0261,-0.0379,-0.0687,-0.236,-0.0543,0.0293,-0.0249,0.0968,-0.0716,-0.0096,0.0148,0.05,0.0911,0.0168,-0.0706,-0.027,0.0508,-0.0227,0.0545,0.0402,0.0484,-0.0074,0.0285,-0.0015,0.0131,-0.0201,-0.087,0.064,-0.016,0.2628,0.0252,0.0729,-0.0126,0.0441,0.0216,-0.0449,-0.0681,0.068,0.0051,0.0385,-0.0108,0.0173,0.0086,-0.0818,-0.0104,-0.0135,-0.1092,0.0104,-0.0479,-0.0284,0.0176,-0.0808,0.0239,0.0641,-0.0442,0.0368,0.0193,-0.0204,-0.0546,-0.0755,0.0183,-0.0145,0.0143,0.0207,-0.0609,0.009,-0.0325,0.0029,0.0134,-0.0225,-0.0137,0.031,-0.0012,-0.0702,0.0688,-0.0054,-0.002,0.0472,0.0454,0.0331,-0.036,-0.0308,-0.0578,0.0682,-0.0351,0.0537,-0.0185,0.0492,-0.0026,0.0759,0.003,0.0366,0.055,-0.005,0.0046,-0.0411,0.0058,0.0216,-0.061,-0.3149,0.0275,0.0525,0.0061,-0.0263,0.0265,0.051,0.0194,-0.0096,-0.006,-0.0774,0.0252,0.0522,0.0074,-0.0052,0.0378,0.0942,-0.0586,0.0305,-0.0326,-0.0224,0.0369,0.2186,-0.013,0.0011,-0.0041,-0.0227,0.0049,0.0704,-0.0309,-0.0068,0.0072,0.0913,-0.0319,0.0373,0.0538,-0.0637,0.052,0.0245,0.0127,0.0179,0.0178,-0.0618,-0.0498,0.0654,-0.0014,0.0095,-0.0357,-0.0194,0.0124,0.0167,0.0378,-0.0452,-0.0058,0.0409,0.0376,-0.0412,-0.0506,-0.007,-0.0233,-0.0079,-0.0647,-0.0483,0.0174,-0.0135]}
{"key":"[Complementary Calibration: Boosting General Continual Learning with Collaborative Distillation and Self-Supervision] General Continual Learning (GCL) aims at learning from non independent and identically distributed stream data without catastrophic forgetting of the old tasks that don't rely on task boundaries during both training and testing stages. We reveal that the relation and feature deviations are crucial problems for catastrophic forgetting, in which relation deviation refers to the deficiency of the relationship among all classes in knowledge distillation, and feature deviation refers to indiscriminative feature representations. To this end, we propose a Complementary Calibration (CoCa) framework by mining the complementary model's outputs and features to alleviate the two deviations in the process of GCL. Specifically, we propose a new collaborative distillation approach for addressing the relation deviation. It distills model's outputs by utilizing ensemble dark knowledge of new model's outputs and reserved outputs, which maintains the performance of old tasks as well as balancing the relationship among all classes. Furthermore, we explore a collaborative self-supervision idea to leverage pretext tasks and supervised contrastive learning for addressing the feature deviation problem by learning complete and discriminative features for all classes. Extensive experiments on four popular datasets show that our CoCa framework achieves superior performance against state-of-the-art methods.","layer":1,"vector":[-0.0335,0.0045,0.0003,-0.0442,0.0282,0.005,0.0272,0.0329,-0.0082,-0.0194,0.0259,-0.0652,0.0603,0.0739,0.0092,0.0189,-0.0107,0.0596,-0.044,-0.0185,0.0012,-0.0313,-0.0235,-0.042,-0.0045,-0.0014,-0.0677,-0.0554,0.0031,-0.2687,-0.001,-0.0378,-0.009,-0.0298,-0.0227,-0.0083,-0.0428,0.06,-0.0455,0.0291,0.0144,0.024,-0.0531,-0.0386,-0.0334,-0.0638,0.0215,-0.0258,0.0088,-0.0519,0.0205,-0.0599,0.0094,0.0483,0.0316,0.0818,0.0599,0.0388,0.0611,0.0178,0.0404,0.0209,-0.1231,0.0382,0.0341,0.0321,-0.0597,0.0197,0.0109,0.0477,-0.0152,0.063,-0.0071,0.0637,-0.007,0.0023,-0.0022,-0.0144,-0.0162,0.0415,0.042,0.0037,-0.0324,-0.0718,-0.0082,-0.0316,0.0117,-0.0406,0.032,-0.0223,-0.0587,-0.0353,-0.0188,0.031,-0.0591,-0.0366,0.0581,0.0439,-0.0601,0.2206,-0.0616,0.0031,0.0449,-0.0001,0.0098,-0.0429,-0.0228,-0.0273,-0.0428,-0.0018,-0.0149,0.0108,0.0186,-0.0852,0.0613,0.01,0.0487,0.0578,-0.0006,0.0059,0.0199,0.0323,0.0724,-0.0328,0.0462,-0.0521,-0.0015,0.1248,0.02,0.0169,0.0431,-0.0349,-0.033,-0.0181,-0.0003,0.0747,0.0203,-0.0483,0.0454,-0.0257,-0.0383,-0.0183,0.0364,-0.093,-0.0522,0.1253,-0.0179,0.0426,-0.0359,-0.0462,-0.0404,-0.0102,-0.0027,-0.0275,0.0506,0.0263,0.0636,0.0398,-0.03,-0.027,-0.0223,-0.0164,-0.0263,0.0783,0.0535,-0.0704,-0.0046,0.0208,-0.0129,-0.0131,0.0632,0.0429,-0.0447,-0.0055,0.0855,-0.0196,-0.0507,0.0012,0.0148,-0.0126,0.0301,-0.0388,-0.0262,0.0576,0.0303,-0.0377,-0.0075,-0.0491,0.0217,0.0667,-0.0046,0.0346,-0.0056,0.0061,-0.0443,0.011,-0.0033,-0.0478,0.0174,-0.0507,-0.0229,0.0257,-0.0499,-0.0191,0.0211,0.0146,-0.0215,-0.003,0.0655,0.0399,-0.0167,-0.0461,0.0224,-0.0204,-0.0037,0.0139,-0.0186,0.0458,-0.0217,0.0391,0.0941,0.0002,-0.0302,-0.224,0.0111,0.0089,-0.0034,0.0545,-0.0878,0.0305,0.0294,-0.0072,0.0642,0.0372,-0.0268,-0.0474,0.0165,-0.0037,0.0308,0.0798,0.0321,-0.0227,0.0427,-0.0393,0.0185,0.0064,-0.0863,0.0542,0.0093,0.2255,0.0279,0.0318,-0.0606,-0.019,0.0457,-0.0147,-0.1222,0.0116,0.0442,0.047,0.0108,-0.0231,-0.0421,-0.0124,0.0477,0.01,-0.1092,-0.0419,-0.0238,-0.0658,0.0045,-0.0887,0.036,0.0596,-0.0165,0.0199,-0.0447,-0.0258,0.0076,-0.1005,0.0371,-0.0587,-0.008,0.0122,-0.0278,-0.0282,-0.0631,0.0688,-0.009,-0.0491,-0.0574,0.0834,0.004,-0.0188,0.0572,-0.0064,-0.0204,0.0352,-0.0087,0.058,-0.0421,-0.068,-0.0186,0.046,-0.0306,0.0371,0.0309,0.0602,0.0129,0.1024,0.0158,0.058,-0.0196,0.0137,0.0086,-0.0251,-0.0275,0.061,-0.0457,-0.2655,0.0168,-0.0502,0.0441,-0.0301,0.0568,0.044,0.0194,-0.0449,-0.0051,0.0052,0.0333,0.0436,0.004,0.0175,0.0487,0.0663,-0.0815,0.0673,-0.055,0.0101,0.0794,0.1991,-0.0078,0.0239,-0.0016,-0.0201,-0.0142,0.0612,-0.0048,-0.0211,0.0278,0.0934,-0.025,0.0014,0.0738,-0.0374,0.0414,0.0261,-0.0094,-0.0262,-0.0144,-0.0379,-0.0737,0.1141,0.0257,-0.0119,-0.0536,-0.0164,-0.0341,0.004,0.0114,0.0076,0.0003,-0.0,0.0185,-0.0468,-0.0242,-0.0341,-0.0686,-0.0157,-0.0157,-0.0119,-0.0148,-0.0294]}
{"key":"[Women, artificial intelligence, and key positions in collaboration networks: Towards a more equal scientific ecosystem] Scientific collaboration in almost every discipline is mainly driven by the need of sharing knowledge, expertise, and pooled resources. Science is becoming more complex which has encouraged scientists to involve more in collaborative research projects in order to better address the challenges. As a highly interdisciplinary field with a rapidly evolving scientific landscape, artificial intelligence calls for researchers with special profiles covering a diverse set of skills and expertise. Understanding gender aspects of scientific collaboration is of paramount importance, especially in a field such as artificial intelligence that has been attracting large investments. Using social network analysis, natural language processing, and machine learning and focusing on artificial intelligence publications for the period from 2000 to 2019, in this work, we comprehensively investigated the effects of several driving factors on acquiring key positions in scientific collaboration networks through a gender lens. It was found that, regardless of gender, scientific performance in terms of quantity and impact plays a crucial in possessing the \"social researcher\" in the network. However, subtle differences were observed between female and male researchers in acquiring the \"local influencer\" role.","layer":3,"vector":[-0.0056,0.0179,-0.0091,-0.0011,0.0509,0.0116,0.0451,0.072,0.0299,-0.0399,-0.0054,-0.0743,0.0603,0.0368,0.0134,0.0155,-0.0459,-0.0102,-0.044,0.0193,0.0033,-0.0256,0.0055,-0.069,0.001,-0.0048,-0.1044,-0.0638,-0.0619,-0.2134,0.0014,-0.0477,0.0695,-0.03,0.0083,0.004,0.042,0.0224,-0.0257,0.0487,0.0359,-0.0343,-0.0085,-0.0221,-0.0342,-0.0137,-0.0584,-0.0038,-0.0741,-0.0491,0.0302,-0.0543,0.0018,0.0557,0.0234,0.0592,0.048,0.0136,0.0083,0.0502,0.0597,0.0493,-0.2298,0.077,0.0299,0.011,-0.0549,-0.0046,0.021,0.036,0.0249,0.059,0.029,0.0327,0.0594,0.0468,0.0082,-0.0805,0.041,0.0107,0.0003,-0.0307,-0.0409,-0.0028,0.0123,-0.0193,0.0175,0.0075,0.0419,0.0004,-0.0261,-0.0145,-0.0202,0.03,-0.0649,-0.0057,0.0333,-0.0081,-0.0156,0.204,-0.0758,0.0199,0.0324,-0.0317,-0.0002,-0.0442,-0.0213,-0.0527,-0.018,0.0164,-0.0202,0.0213,-0.0075,-0.0338,0.0494,0.0223,0.069,0.0491,-0.0145,-0.0337,-0.0568,0.0396,0.0186,0.0138,0.039,-0.0186,0.0089,0.1045,0.0331,0.0182,0.1007,0.0186,-0.0673,0.0176,0.0038,0.0022,-0.0301,-0.0163,-0.0329,0.0071,-0.0028,-0.0525,0.0073,-0.0778,-0.0526,0.1367,-0.0262,-0.0239,0.0056,0.0232,-0.0378,-0.0067,-0.0738,-0.0368,0.0153,0.0264,0.024,0.0541,-0.0509,0.0035,-0.0023,-0.0465,-0.0739,0.099,0.041,-0.0887,-0.0108,0.0197,-0.0154,-0.0282,0.0342,0.0075,-0.0149,0.0558,0.0504,0.0164,-0.0162,0.0255,-0.0074,0.0285,0.0068,-0.003,-0.0171,0.0464,0.0319,-0.0467,-0.0208,-0.033,0.0096,0.0617,-0.0225,0.0996,0.0268,0.023,-0.0158,0.0195,0.0008,-0.0532,-0.0321,-0.0204,-0.019,-0.0039,-0.0568,0.0448,-0.0315,0.0114,0.0202,0.0142,0.0704,-0.0015,-0.0525,-0.0135,0.0358,-0.0155,-0.0654,-0.0048,0.0537,-0.0268,0.0572,0.0795,0.0496,-0.0164,-0.0474,-0.1988,-0.0073,0.0162,-0.0017,0.0677,-0.0497,0.0239,-0.0287,0.0431,0.1048,0.0666,0.0161,-0.0675,0.0129,0.002,0.0425,0.0343,0.0187,-0.0025,-0.0024,-0.0071,-0.0048,-0.0536,-0.071,0.0379,0.0212,0.159,0.0518,0.0004,-0.0052,0.0296,0.0409,-0.0374,-0.1652,0.0543,0.0375,0.0872,-0.0701,-0.0704,-0.0256,-0.0216,0.0504,0.0091,-0.0523,-0.0404,-0.0098,-0.0153,-0.0185,-0.0488,0.0032,0.0148,-0.0195,0.0366,0.0135,-0.0311,-0.0186,-0.0832,0.0647,-0.0066,-0.018,0.0146,-0.0419,-0.0274,-0.0536,0.058,-0.0217,-0.0633,0.0175,0.0314,-0.0319,0.0102,0.148,-0.0014,0.009,0.0516,-0.0136,0.0111,-0.0662,-0.0292,0.0101,0.0808,-0.0563,0.0717,0.034,0.0131,-0.0135,0.0652,-0.0239,0.0279,-0.05,-0.0073,0.018,-0.0698,-0.0309,0.0253,0.0059,-0.291,0.0776,0.0154,0.0416,-0.0414,0.0154,0.0115,-0.0053,-0.0007,0.0171,0.0231,0.0137,-0.0181,0.0022,-0.0201,0.0621,-0.0013,-0.0344,0.0162,-0.0728,0.0157,0.0513,0.2102,-0.0048,0.0424,0.0176,-0.0068,0.0017,0.0186,-0.014,-0.011,0.0206,0.0754,-0.0507,0.0256,0.0842,-0.0195,0.0162,0.0536,-0.0192,-0.0166,-0.0085,-0.0369,-0.0189,0.1224,-0.0451,-0.0245,-0.058,0.0011,0.0194,-0.0558,0.0347,-0.0442,0.0083,-0.0253,0.0101,-0.0172,-0.001,-0.0886,-0.06,-0.0062,-0.049,-0.0562,0.0302,-0.0026]}
{"key":"[Copula-based synthetic data augmentation for machine-learning emulators] Can we improve machine-learning (ML) emulators with synthetic data? If data are scarce or expensive to source and a physical model is available, statistically generated data may be useful for augmenting training sets cheaply. Here we explore the use of copula-based models for generating synthetically augmented datasets in weather and climate by testing the method on a toy physical model of downwelling longwave radiation and corresponding neural network emulator. Results show that for copula-augmented datasets, predictions are improved by up to 62 % for the mean absolute error (from 1.17 to 0.44 W m$^{-2}$).","layer":4,"vector":[-0.0827,-0.0001,0.0637,-0.038,0.0566,0.0399,-0.0268,-0.0021,0.0204,0.0114,0.0117,-0.0731,0.053,0.0394,-0.0002,0.0351,0.0128,0.0852,-0.0386,0.0278,0.0282,-0.0092,-0.04,-0.0673,0.0477,0.0689,-0.0208,-0.0067,-0.0425,-0.2226,0.0161,-0.036,0.0421,-0.0014,0.0089,-0.0334,-0.0083,0.028,-0.021,0.0852,-0.001,-0.0265,-0.0207,-0.0602,-0.0031,-0.0678,-0.0611,0.0434,-0.0311,-0.0316,0.023,0.0026,-0.0214,0.0313,0.0475,0.0136,0.0139,0.035,0.0655,0.053,0.0015,0.1057,-0.1833,0.0199,0.0222,0.0203,-0.0374,-0.0075,0.0318,0.0189,-0.0546,0.0354,0.038,0.0352,0.0401,0.0307,-0.023,-0.025,-0.0111,0.0144,0.0246,-0.0268,-0.0507,-0.0167,-0.0087,-0.0384,0.0266,-0.0035,0.0365,0.0355,-0.0374,-0.0419,-0.0349,0.0481,-0.0858,-0.0023,0.0705,0.0288,-0.0482,0.2124,-0.0998,-0.0025,0.0381,0.0285,0.035,-0.0434,-0.0499,-0.0433,-0.008,-0.0574,0.0034,-0.0242,-0.0095,-0.0232,0.0091,-0.0105,0.0762,-0.013,-0.0257,-0.0128,-0.033,-0.0024,0.0775,0.0284,0.0273,-0.0571,0.0181,0.1214,0.011,0.0257,0.0582,-0.0192,-0.0766,-0.0028,-0.0018,0.0183,-0.0071,0.0054,0.0055,0.0495,-0.0345,-0.0195,0.0333,-0.0981,-0.0421,0.1221,-0.033,0.008,-0.026,-0.042,-0.0597,0.0437,-0.0229,-0.0374,0.0859,0.0588,-0.0122,0.0678,-0.0326,0.0095,-0.0041,-0.0402,-0.006,0.0756,-0.0218,-0.0573,-0.0296,0.0559,0.0324,-0.0054,0.0414,0.0302,-0.0234,0.0301,0.0635,-0.0124,-0.0578,0.0023,0.0328,-0.0191,0.0061,-0.0505,-0.0102,0.0265,0.0243,-0.0257,-0.0052,-0.039,0.0097,0.0528,0.0042,-0.0157,-0.01,-0.0071,-0.0075,-0.0564,-0.0219,0.0095,0.0279,-0.0425,-0.0033,0.0353,-0.0037,0.0053,-0.0085,0.0472,0.0149,-0.0111,0.0477,0.0202,-0.0099,-0.0436,0.0795,-0.0028,-0.0457,0.054,-0.0005,0.0339,0.0154,0.0493,0.0069,-0.0697,-0.0793,-0.2493,-0.0308,0.0025,-0.0478,0.0615,-0.0743,0.0589,0.0217,0.0429,0.0551,0.0575,-0.0597,0.0055,-0.014,-0.0369,0.029,0.0183,0.0195,-0.0558,-0.0217,-0.0506,0.015,-0.0437,-0.0632,0.043,0.0097,0.1711,0.0273,0.0511,-0.0319,0.0269,0.0012,0.0128,-0.1034,0.0589,0.0602,0.0887,0.0378,-0.0516,-0.0007,-0.0063,0.0424,0.0002,-0.0707,-0.0309,-0.0147,-0.0051,0.0496,-0.0947,-0.0047,0.0168,-0.022,0.0297,-0.0193,-0.0162,-0.077,-0.108,0.0275,-0.0278,0.0098,0.0352,-0.0497,0.044,-0.0659,0.0028,-0.0223,-0.0241,-0.0665,0.0481,-0.0029,-0.0428,0.126,-0.0436,0.0133,0.0289,-0.0017,0.0284,-0.0506,-0.0381,-0.0282,0.0321,-0.0298,0.0746,0.0556,0.0286,0.0021,0.0761,-0.017,0.0199,-0.0589,-0.0152,0.0398,-0.0511,-0.0165,0.0641,-0.0063,-0.3156,0.0178,-0.0158,0.0445,-0.0366,0.0048,0.013,0.0453,-0.0496,0.0443,-0.0255,0.0039,0.0521,-0.0146,0.0378,0.0224,0.04,-0.0459,0.022,-0.0291,0.0171,0.0112,0.2037,-0.0445,0.0173,0.0261,-0.0462,-0.0043,0.0479,-0.017,0.0463,0.0419,0.0583,-0.0188,0.0439,0.0549,-0.0442,0.067,0.0161,-0.0247,0.0041,0.0197,-0.0449,-0.0354,0.0687,0.0107,-0.0244,-0.0559,-0.0124,0.0327,-0.042,0.0395,-0.0293,-0.0025,0.0127,0.0129,-0.0345,-0.0231,-0.0059,-0.037,0.0139,-0.0803,-0.005,-0.0371,0.028]}
{"key":"[Locate This, Not That: Class-Conditioned Sound Event DOA Estimation] Existing systems for sound event localization and detection (SELD) typically operate by estimating a source location for all classes at every time instant. In this paper, we propose an alternative class-conditioned SELD model for situations where we may not be interested in localizing all classes all of the time. This class-conditioned SELD model takes as input the spatial and spectral features from the sound file, and also a one-hot vector indicating the class we are currently interested in localizing. We inject the conditioning information at several points in our model using feature-wise linear modulation (FiLM) layers. Through experiments on the DCASE 2020 Task 3 dataset, we show that the proposed class-conditioned SELD model performs better in terms of common SELD metrics than the baseline model that locates all classes simultaneously, and also outperforms specialist models that are trained to locate only a single class of interest. We also evaluate performance on the DCASE 2021 Task 3 dataset, which includes directional interference (sound events from classes we are not interested in localizing) and notice especially strong improvement from the class-conditioned model.","layer":2,"vector":[-0.0256,-0.0509,0.072,-0.0418,0.0494,-0.0112,0.0392,0.007,0.041,-0.0205,0.0604,-0.0324,0.0047,0.0556,0.0572,0.0235,0.0471,0.0373,-0.0202,-0.0064,0.0035,-0.0138,0.0216,-0.0052,0.0256,-0.0023,-0.0481,-0.0001,-0.049,-0.231,0.0375,-0.066,0.0707,-0.0318,0.0095,-0.0588,-0.0323,0.0731,-0.0193,0.0457,0.0459,0.0172,-0.0299,-0.0894,-0.0661,-0.0671,-0.0223,-0.026,-0.0297,-0.0748,0.0535,-0.0852,0.0071,0.0038,-0.0123,0.0373,0.0533,0.0758,0.0382,0.0434,-0.0157,0.0278,-0.1922,0.0635,0.0436,0.0249,-0.0143,-0.0288,0.0276,-0.0185,-0.0244,0.0555,0.0172,0.0331,0.011,-0.0353,0.0441,-0.0149,-0.0053,-0.0184,0.0175,-0.0557,0.0001,-0.0028,-0.0018,-0.0516,-0.0002,-0.0354,-0.0032,-0.0241,-0.0882,0.0177,-0.0292,0.0506,-0.0562,-0.0118,0.0553,0.0678,-0.0002,0.1817,-0.0186,0.0085,0.0204,-0.0496,0.0213,-0.039,-0.0401,-0.035,0.0135,0.0096,0.0301,-0.0098,0.0107,-0.0018,0.043,0.0236,0.0952,0.0549,0.0008,-0.0234,-0.0075,-0.0235,0.0316,-0.0272,0.0419,-0.0935,0.0794,0.1066,0.0381,0.016,0.0627,-0.024,-0.0638,0.008,0.0113,0.0055,0.0394,0.0178,-0.0028,-0.021,-0.0099,-0.0786,0.033,-0.0701,-0.0082,0.0995,-0.0946,0.0034,-0.0424,-0.0416,-0.0011,0.0197,-0.0397,0.027,0.0575,0.0211,0.0008,0.0066,-0.0451,0.04,-0.0264,-0.0508,0.028,0.0888,0.0096,-0.0821,-0.0338,0.0075,-0.0012,-0.0112,0.005,0.0158,-0.0121,0.0423,0.0783,0.0394,-0.0849,0.0383,-0.0072,0.0301,0.0058,-0.0643,-0.0573,-0.0059,0.0463,-0.0357,0.0283,-0.0778,0.0157,0.0582,-0.0499,-0.0069,0.0025,-0.0277,-0.0174,-0.0248,-0.0024,-0.0029,0.0452,0.0221,-0.0172,-0.0043,-0.0337,0.0193,0.0017,0.0376,-0.0039,-0.0119,-0.0145,0.0263,-0.0331,-0.0053,0.1182,-0.0496,-0.0056,-0.0136,0.0294,0.049,-0.0319,0.0538,-0.0039,-0.037,-0.0413,-0.2808,0.0003,0.023,0.006,0.0123,-0.0577,0.0201,0.0152,0.0881,0.0292,0.0638,-0.0051,0.008,0.0299,0.0018,0.0463,0.0238,-0.0007,0.0108,0.022,-0.0111,0.0001,-0.0468,-0.0592,0.0662,-0.042,0.191,0.0192,0.0501,-0.03,-0.007,-0.0042,-0.0438,-0.0648,0.0187,0.0268,0.0681,0.0233,-0.0431,-0.0163,-0.0435,0.0321,-0.014,-0.0936,-0.0253,-0.0591,0.0023,0.0093,-0.0287,0.0323,0.0736,-0.0347,0.0348,0.0,-0.0014,-0.0377,-0.0557,0.0165,-0.0579,0.0459,0.0276,0.0005,0.0786,-0.0502,0.0456,-0.0062,-0.0203,-0.0741,0.0126,0.0088,-0.0383,0.075,0.0112,-0.0002,0.0448,-0.0369,0.0476,-0.0676,-0.042,-0.061,0.0658,-0.0338,0.0315,-0.0031,-0.0046,0.008,0.1077,0.0302,0.0247,-0.0343,-0.0024,-0.0005,-0.0141,-0.0186,0.0174,-0.0128,-0.3107,-0.013,0.0521,0.0415,-0.0492,0.0252,0.0072,-0.0199,-0.071,-0.0174,-0.0197,0.0117,0.0144,0.0092,0.0123,0.0401,0.0737,0.0002,0.044,-0.0467,-0.0209,0.0715,0.1866,0.0249,0.0683,-0.0246,-0.0531,-0.0099,0.0015,-0.0311,0.009,-0.0037,0.0627,-0.0234,-0.0005,0.0654,-0.0071,0.0631,0.0038,-0.0504,-0.0201,-0.0042,-0.0383,-0.0233,0.1051,-0.0004,-0.0149,-0.0172,0.0103,0.0306,0.0293,0.0169,0.0319,-0.0059,0.0232,0.023,-0.0796,-0.0805,-0.0575,-0.0347,0.0783,-0.0736,-0.0102,-0.0031,-0.0078]}
{"key":"[Policy Space Identification in Configurable Environments] We study the problem of identifying the policy space of a learning agent, having access to a set of demonstrations generated by its optimal policy. We introduce an approach based on statistical testing to identify the set of policy parameters the agent can control, within a larger parametric policy space. After presenting two identification rules (combinatorial and simplified), applicable under different assumptions on the policy space, we provide a probabilistic analysis of the simplified one in the case of linear policies belonging to the exponential family. To improve the performance of our identification rules, we frame the problem in the recently introduced framework of the Configurable Markov Decision Processes, exploiting the opportunity of configuring the environment to induce the agent revealing which parameters it can control. Finally, we provide an empirical evaluation, on both discrete and continuous domains, to prove the effectiveness of our identification rules.","layer":7,"vector":[-0.0499,-0.0152,0.041,-0.0274,-0.0063,0.0525,0.0454,0.0655,0.048,0.0141,0.0359,-0.0262,0.0372,0.0638,-0.0103,-0.0083,-0.0411,0.0623,-0.0345,-0.0107,0.0653,-0.0165,-0.022,-0.0663,0.0051,0.0557,0.001,-0.0645,-0.0255,-0.2227,0.0537,-0.0452,0.019,-0.0457,0.0115,-0.0325,-0.0193,0.0103,-0.0281,0.009,0.0428,-0.0072,-0.015,-0.0427,-0.0113,-0.0426,0.0074,-0.0536,-0.0739,-0.0439,-0.0011,-0.0097,0.0697,0.0573,0.0378,0.0184,0.0375,0.031,0.0171,0.0068,0.0047,0.0331,-0.1738,0.0505,0.0401,0.0425,-0.0425,0.0011,0.0418,0.0614,-0.0444,0.0373,0.0125,0.0465,0.0501,-0.0199,-0.0239,-0.0155,0.0408,-0.0235,-0.0344,-0.046,-0.0269,0.0259,-0.04,-0.0569,0.0183,-0.036,0.0528,0.0401,-0.0615,0.0337,-0.0253,0.0061,-0.0618,0.005,0.0054,0.029,-0.0485,0.2163,-0.0283,0.0354,0.0442,0.0219,0.0334,-0.0389,-0.0052,-0.0103,0.009,-0.015,-0.0446,-0.041,0.0192,-0.0401,-0.0217,0.0118,0.0324,0.0478,0.001,-0.0548,0.0185,0.0084,0.0635,-0.0175,0.0124,-0.0746,0.0002,0.1581,0.0099,0.0192,0.018,-0.0596,-0.0352,0.0043,0.0045,0.0402,0.0162,0.0334,0.0413,-0.0003,-0.0258,0.0078,0.0425,-0.1141,-0.0572,0.1428,-0.0258,0.0175,-0.0473,0.0022,0.0191,0.0002,-0.0202,-0.0117,0.0207,0.04,0.0002,0.0275,-0.0366,0.0119,-0.0117,-0.0454,0.0155,0.1113,-0.0363,-0.0646,-0.0693,-0.0074,0.0345,0.0373,0.0294,0.0517,-0.0705,0.0412,0.0268,-0.0386,-0.0747,0.0229,0.0255,-0.0121,0.0265,-0.0668,-0.0263,0.0251,0.0455,-0.0059,-0.018,-0.0438,0.013,0.0513,-0.0181,0.0093,-0.014,-0.0232,-0.0437,-0.0546,0.021,-0.0401,0.0272,-0.0595,-0.0047,0.02,-0.0646,0.0094,-0.0166,-0.0087,-0.0328,0.0072,0.0567,-0.01,-0.0182,0.0389,0.0431,-0.0167,-0.028,0.0116,0.0059,0.0473,0.0225,0.0274,0.0373,-0.0072,-0.0394,-0.2475,-0.0049,-0.0246,0.0029,0.0682,-0.0535,0.025,-0.0118,0.0377,0.0223,0.038,-0.0351,-0.0067,0.0555,-0.0227,0.0171,0.0278,0.0519,-0.0292,0.0037,0.0161,-0.0219,-0.0447,-0.0875,0.0505,-0.0331,0.1934,0.02,0.0508,-0.0113,0.0571,0.0343,-0.044,-0.0986,0.0619,0.0456,0.027,-0.0241,0.0357,-0.0814,0.0313,0.0356,-0.0234,-0.0712,-0.0208,-0.0546,-0.0443,0.0784,-0.046,-0.0207,0.0343,-0.0155,0.0177,-0.0419,-0.0197,-0.0166,-0.0778,0.0031,-0.0133,0.0298,0.0084,-0.016,0.0415,-0.0569,0.0646,-0.0048,0.0054,-0.0527,0.0308,-0.0197,-0.0165,0.0678,0.0462,0.0066,0.045,0.002,0.0322,-0.0081,-0.0599,-0.0472,0.0398,-0.0491,0.0278,0.052,0.0119,-0.0313,0.0709,-0.0081,0.0663,-0.021,-0.0149,0.0306,-0.0598,0.0337,0.024,0.002,-0.3039,0.085,0.0235,0.0451,-0.0321,-0.0355,-0.0034,-0.0185,-0.0552,-0.0087,0.0321,0.0693,0.0306,0.0062,0.0171,0.0438,0.0583,-0.0649,0.0625,-0.0846,0.0158,0.0113,0.2086,-0.0397,0.0641,0.0246,-0.0146,0.0195,0.0419,-0.0115,0.0339,0.0026,0.0593,-0.0877,0.0921,0.0549,-0.0283,0.0491,-0.0227,-0.0081,-0.0255,0.0011,-0.0547,0.0035,0.0929,-0.0098,-0.0099,-0.0853,-0.0302,0.0083,-0.0216,-0.0169,-0.0271,-0.0059,0.0308,-0.0087,-0.0387,-0.0707,-0.0312,-0.0605,0.011,-0.0982,0.0834,-0.0155,0.0169]}
{"key":"[Explaining Algorithmic Fairness Through Fairness-Aware Causal Path Decomposition] Algorithmic fairness has aroused considerable interests in data mining and machine learning communities recently. So far the existing research has been mostly focusing on the development of quantitative metrics to measure algorithm disparities across different protected groups, and approaches for adjusting the algorithm output to reduce such disparities. In this paper, we propose to study the problem of identification of the source of model disparities. Unlike existing interpretation methods which typically learn feature importance, we consider the causal relationships among feature variables and propose a novel framework to decompose the disparity into the sum of contributions from fairness-aware causal paths, which are paths linking the sensitive attribute and the final predictions, on the graph. We also consider the scenario when the directions on certain edges within those paths cannot be determined. Our framework is also model agnostic and applicable to a variety of quantitative disparity measures. Empirical evaluations on both synthetic and real-world data sets are provided to show that our method can provide precise and comprehensive explanations to the model disparities.","layer":1,"vector":[-0.0364,-0.0144,0.0021,-0.0112,0.0442,0.0068,0.0583,0.0492,0.0726,-0.0317,0.0316,-0.0642,0.0042,0.0425,0.0043,0.0144,0.0071,0.0533,-0.0612,-0.0127,0.0199,-0.024,-0.0101,-0.0664,0.0163,0.0507,-0.0258,-0.032,-0.0619,-0.2517,0.0127,-0.0736,0.0586,-0.0305,-0.0126,-0.032,-0.0558,0.0451,-0.0489,0.0109,0.0175,0.0302,-0.0412,-0.0303,-0.0373,-0.0092,0.0153,0.049,-0.0478,-0.074,0.0398,-0.0583,-0.009,0.0466,0.0454,0.0454,0.0975,0.0596,0.0464,0.0693,0.0552,0.0379,-0.1523,0.0512,0.0719,0.0121,-0.0191,-0.0492,-0.0179,0.058,0.0164,0.0365,0.0385,0.0147,0.0191,-0.0232,0.0457,0.0044,-0.0266,0.0027,0.0093,0.0048,-0.047,0.0255,-0.0079,-0.0261,0.037,-0.0544,0.0138,0.0052,-0.0169,-0.001,0.0068,0.0214,-0.0736,-0.0191,0.0126,0.0209,-0.0039,0.1875,-0.0216,0.0314,-0.0189,-0.0287,0.0531,-0.0419,0.0103,-0.0732,-0.0258,0.0029,0.0157,-0.0257,0.0456,-0.0264,0.0012,-0.0054,0.0564,0.0509,-0.0104,-0.0144,-0.0268,0.041,0.0443,-0.0391,-0.0155,-0.0249,0.0289,0.1415,0.0141,0.0165,0.0485,-0.0507,-0.0457,-0.017,0.0036,0.006,0.0098,-0.004,-0.0015,0.0091,-0.068,-0.0026,0.0229,-0.0905,-0.0467,0.1286,-0.049,-0.006,0.0001,-0.0241,-0.0242,0.0168,-0.0636,-0.0283,0.0128,0.0328,0.0272,0.0468,-0.0312,0.0765,0.0333,-0.0609,-0.075,0.1136,-0.0156,-0.0881,-0.0099,0.0482,0.0126,0.0096,0.0457,0.0265,-0.0258,0.0257,0.0763,0.0076,-0.0748,0.0189,-0.0194,-0.0334,0.0584,-0.0334,-0.0189,0.0523,0.063,-0.0052,0.0259,-0.018,0.0347,0.0411,-0.0231,-0.0329,-0.0597,-0.0213,-0.0123,0.0097,0.0129,-0.0247,0.024,-0.0201,-0.0325,0.0028,-0.0568,0.0419,-0.0719,0.0127,-0.0134,-0.0144,0.0421,0.0314,-0.0489,-0.0078,0.02,-0.0271,0.0149,0.0429,0.0716,0.0092,0.0246,0.0762,-0.0052,-0.0259,-0.0739,-0.193,-0.0461,0.011,0.0038,0.0091,-0.0531,0.0082,0.0075,0.0271,0.0938,0.0734,-0.0023,-0.0267,0.0435,-0.0032,0.0802,-0.0163,0.0769,-0.0394,0.0079,-0.026,0.0144,0.0062,-0.094,0.0346,0.0572,0.2334,0.0166,0.0094,-0.0179,0.0095,-0.0044,-0.0351,-0.099,0.0223,0.016,-0.0096,-0.0417,-0.018,-0.028,-0.0266,0.0195,-0.0078,-0.0793,-0.0184,-0.025,-0.0185,-0.0027,-0.0277,0.0309,0.0222,-0.0286,0.077,0.0131,0.0379,-0.053,-0.0571,0.0558,-0.0503,0.0161,0.0114,-0.064,-0.0142,-0.069,0.0615,0.0167,-0.0307,-0.0507,-0.0005,-0.001,0.0189,0.0911,0.0053,-0.0636,0.0619,-0.0035,0.0172,-0.0247,-0.0257,-0.0236,0.1105,-0.0216,-0.0163,0.024,0.0194,-0.0422,0.0369,0.0176,0.039,-0.0246,-0.0367,-0.0202,-0.0513,-0.019,0.0407,-0.0202,-0.278,0.0498,-0.0265,0.0041,-0.0436,0.0042,0.0896,0.0135,-0.037,-0.0342,0.0447,0.0525,-0.0223,-0.0396,-0.0174,0.0249,0.0947,-0.0609,0.0515,-0.0368,0.0432,0.0507,0.2469,-0.0296,0.0002,0.0287,-0.031,-0.0039,0.0351,0.0032,-0.0181,0.0027,0.0858,-0.0795,0.0695,0.0578,-0.0254,-0.0104,0.0243,-0.034,-0.0215,-0.0178,-0.0414,-0.0175,0.1066,0.0106,-0.0471,-0.0556,0.0155,0.0211,-0.0484,0.0104,-0.0545,0.0052,0.0063,0.0442,-0.0646,-0.0138,-0.0362,-0.0684,-0.0084,-0.0533,-0.0005,0.0002,-0.0179]}
{"key":"[Random Forest for Malware Classification] The challenge in engaging malware activities involves the correct identification and classification of different malware variants. Various malwares incorporate code obfuscation methods that alters their code signatures effectively countering antimalware detection techniques utilizing static methods and signature database. In this study, we utilized an approach of converting a malware binary into an image and use Random Forest to classify various malware families. The resulting accuracy of 0.9562 exhibits the effectivess of the method in detecting malware","layer":12,"vector":[-0.0386,-0.0453,0.0259,-0.0345,0.0582,0.0179,0.0449,0.0329,0.0081,0.0295,0.0108,-0.0378,0.0113,-0.0039,0.0398,0.0165,0.017,0.0279,-0.0251,0.0336,0.0235,-0.0103,0.0172,-0.0359,0.0187,0.0395,-0.0098,-0.0026,-0.0884,-0.1795,0.0047,-0.0795,0.048,-0.0472,-0.0065,-0.0088,-0.0402,0.0307,-0.0265,0.0094,-0.0004,0.0242,-0.0336,-0.0334,-0.0499,-0.0759,-0.0334,-0.0138,0.0073,-0.0135,-0.0165,-0.0308,0.0243,0.0324,-0.003,-0.0144,0.039,0.0348,0.043,0.0608,0.0321,0.0749,-0.159,0.0515,0.0683,0.0577,-0.0163,-0.0442,0.0383,0.0418,-0.0544,0.0169,-0.0321,0.0131,0.0123,-0.0037,-0.0285,-0.0213,-0.0204,-0.0063,-0.0377,-0.045,-0.0171,0.0059,-0.0287,-0.0175,-0.0005,-0.042,0.1196,0.0055,-0.0378,0.0195,-0.0337,-0.0257,-0.0502,0.0003,0.0484,-0.0173,-0.0705,0.2084,-0.0699,-0.043,0.045,-0.0195,0.048,-0.0225,0.0115,-0.0236,-0.0256,-0.0033,0.0462,-0.02,0.0618,-0.0302,-0.0208,-0.0377,0.0337,0.0192,0.0013,0.0169,-0.0032,-0.005,0.0503,-0.0469,0.0659,-0.0284,0.0468,0.1282,0.0491,0.0322,0.0114,-0.0264,-0.0282,-0.0247,0.0134,0.0428,-0.0154,0.0642,0.0394,-0.0446,-0.0517,-0.0549,0.0185,-0.0915,-0.0728,0.0895,-0.0676,0.044,-0.0656,-0.034,-0.0163,0.0251,-0.0378,-0.0566,0.03,0.0121,0.0955,0.0701,-0.0668,-0.0228,0.0047,-0.0468,-0.0204,0.1215,0.031,-0.0482,0.0008,-0.0249,-0.0226,0.0029,0.0125,0.0398,-0.0151,0.0076,0.0053,0.0098,-0.0207,-0.0204,0.0031,0.0075,0.0085,-0.0471,-0.0369,0.0516,0.0576,-0.0495,-0.015,-0.0699,0.0474,0.0417,-0.0479,0.0268,-0.0005,-0.0192,0.0067,-0.0247,-0.0214,-0.0221,0.0093,-0.0361,0.0732,0.0307,-0.0172,0.008,-0.0434,0.0151,0.0289,-0.0322,0.0354,0.0494,-0.0115,0.0285,0.0015,-0.0091,-0.0314,0.0055,0.0144,0.0946,0.0373,0.0414,0.028,-0.0578,-0.0699,-0.1987,-0.0028,0.0208,-0.0154,0.0086,-0.0695,-0.0307,-0.0483,0.0461,0.0243,0.0652,-0.0169,0.0069,0.0457,-0.0292,0.0613,0.0454,0.0221,-0.0414,0.029,-0.015,0.0285,0.055,-0.0945,0.0105,0.0231,0.1978,0.0717,0.0617,-0.0135,0.0529,0.0525,-0.0349,-0.1095,0.0726,0.0762,0.0179,-0.026,-0.0054,-0.002,-0.0332,0.0486,0.0103,-0.1164,-0.0263,-0.0505,-0.0585,-0.0179,-0.059,0.0297,0.0473,0.0271,0.0982,0.054,0.0185,-0.0679,-0.0615,0.0446,-0.0038,0.0287,0.0039,-0.0999,-0.0026,-0.1129,0.0359,-0.0241,-0.0511,-0.0029,0.0637,-0.0259,-0.0163,0.1137,0.0042,-0.0834,0.0896,0.0182,0.0518,-0.0097,-0.058,-0.0276,0.028,-0.0044,0.0265,0.0117,0.032,-0.0121,0.0773,-0.012,0.0338,-0.0202,0.0076,0.0187,0.012,-0.0573,0.0293,0.0559,-0.2787,0.0162,0.011,0.0342,-0.0041,-0.0064,0.0611,-0.0061,-0.0449,0.0162,-0.0229,0.0099,0.0238,-0.0655,0.0114,0.0218,0.0301,-0.0462,0.0487,-0.0502,0.0202,-0.0186,0.2582,-0.0031,-0.0136,0.0362,-0.0035,0.0411,0.0133,-0.0109,0.057,0.0424,0.1046,-0.047,0.0272,0.0534,-0.0389,0.0101,-0.0152,-0.0496,-0.0137,0.0061,-0.0579,-0.0467,0.0855,-0.0532,0.0052,-0.0354,0.029,0.0437,-0.047,-0.0326,-0.0609,-0.0301,0.0426,0.0371,-0.037,-0.0178,-0.0697,-0.0369,0.0168,-0.0573,-0.0276,0.0106,-0.0216]}
{"key":"[Query-by-Example Keyword Spotting system using Multi-head Attention and Softtriple Loss] This paper proposes a neural network architecture for tackling the query-by-example user-defined keyword spotting task. A multi-head attention module is added on top of a multi-layered GRU for effective feature extraction, and a normalized multi-head attention module is proposed for feature aggregation. We also adopt the softtriple loss - a combination of triplet loss and softmax loss - and showcase its effectiveness. We demonstrate the performance of our model on internal datasets with different languages and the public Hey-Snips dataset. We compare the performance of our model to a baseline system and conduct an ablation study to show the benefit of each component in our architecture. The proposed work shows solid performance while preserving simplicity.","layer":3,"vector":[-0.0222,-0.003,-0.0007,-0.0186,0.0229,0.0077,0.0704,0.0147,0.0287,-0.0357,-0.0429,-0.0172,0.0156,0.0563,0.027,0.0085,0.031,0.0331,0.0012,-0.0284,0.0208,0.0157,-0.012,-0.0229,-0.0151,-0.051,-0.0612,-0.0437,-0.0627,-0.2319,0.0247,-0.021,0.044,0.0111,0.0133,-0.0102,-0.0376,0.0381,-0.0139,0.0642,0.0181,-0.0053,-0.0077,-0.0293,-0.0398,-0.0186,-0.0331,-0.0192,-0.0418,-0.0084,-0.0014,-0.0437,0.0116,0.0167,0.0706,0.0284,0.0716,0.0123,-0.019,0.0305,0.0215,0.0112,-0.1543,0.0401,0.0533,0.0615,-0.0534,-0.0339,0.0227,0.0127,-0.0049,0.0215,-0.003,0.0677,-0.0179,0.0389,0.0052,-0.023,-0.0002,-0.0163,0.0203,-0.0037,-0.0153,-0.0366,0.0194,-0.0295,0.013,-0.0205,0.0373,-0.0628,-0.0132,-0.0402,-0.0214,0.0392,-0.0168,-0.0184,0.022,0.0103,-0.0453,0.1793,-0.0306,0.0168,0.0216,-0.0919,0.0473,-0.077,-0.0354,-0.0265,-0.0466,-0.0025,0.0158,-0.0324,0.0371,-0.0065,0.0631,-0.0081,0.0576,0.0328,0.0068,-0.0141,-0.0223,-0.0211,0.0148,0.0058,0.0558,-0.0237,0.016,0.1436,0.0391,-0.0075,0.0506,-0.0203,-0.066,-0.0299,0.0105,0.0192,-0.0098,0.0122,0.015,-0.0338,-0.0522,-0.0569,0.0013,-0.0837,-0.0339,0.1205,-0.0765,-0.0047,-0.0313,-0.0413,-0.0111,0.0298,0.0176,-0.0524,0.035,0.05,0.0737,0.0536,-0.0701,0.0104,0.0276,-0.0656,-0.0469,0.0899,0.0215,-0.1398,0.0037,0.0006,-0.014,-0.0285,0.0709,-0.004,-0.0611,0.0444,0.0662,0.0357,-0.0672,0.0321,-0.0079,0.0216,0.0445,-0.0306,-0.0313,0.0203,0.0073,-0.0786,0.0098,-0.0094,0.0361,0.0676,-0.0693,0.0341,0.0003,0.0056,-0.0082,-0.0133,-0.031,-0.0441,0.0321,-0.0383,0.0585,-0.0081,-0.0214,-0.0037,0.0525,0.0452,-0.0235,-0.0083,0.0356,0.0128,-0.025,-0.0203,0.0535,-0.0523,-0.0053,-0.0294,-0.0133,0.0416,-0.0068,0.0503,0.0557,-0.0597,-0.0319,-0.2428,-0.003,0.0211,-0.0134,0.0659,-0.0956,0.0591,0.0146,0.0701,0.0706,0.059,-0.0171,-0.0066,0.012,-0.0081,0.0851,0.0186,0.0133,-0.038,-0.031,0.058,-0.0022,-0.0177,-0.077,0.0358,0.0294,0.2363,0.0767,-0.0027,-0.0407,0.0057,0.0174,-0.0208,-0.1265,0.0329,0.0322,0.0468,-0.0166,-0.0402,-0.0059,-0.0467,0.0122,0.04,-0.0725,-0.0244,-0.0642,-0.0387,0.0029,-0.0492,0.0747,0.0372,-0.0481,0.075,0.0502,0.0094,-0.0729,-0.0721,-0.0227,-0.0721,0.0242,0.031,-0.0592,0.0171,-0.0482,0.039,0.0369,-0.0409,0.0086,-0.0122,-0.0335,-0.0828,0.0509,0.0227,-0.0021,0.021,0.0257,0.0862,-0.0719,-0.0347,-0.0378,0.0653,-0.03,0.0308,-0.0043,0.0318,0.0625,0.1055,-0.0446,0.0319,-0.034,-0.0108,0.0065,-0.0132,-0.0159,0.0105,0.0137,-0.3087,0.0391,0.0259,0.0442,0.002,0.0096,0.0282,0.0203,-0.0157,0.0318,-0.0221,0.04,0.0256,-0.0558,-0.004,0.0587,0.0153,-0.0287,0.0488,-0.0066,0.0569,0.0614,0.1937,-0.0369,0.0367,-0.0439,-0.0427,0.0044,0.0302,0.0153,0.0208,0.0053,0.0999,-0.0378,0.0157,0.1033,-0.0082,0.0253,0.0215,0.0198,-0.0128,0.0324,-0.051,-0.0385,0.0737,-0.0401,-0.0151,-0.0573,-0.0007,0.0401,-0.0352,-0.0212,-0.0264,-0.0228,0.0533,0.0303,-0.0244,-0.019,-0.0494,0.0017,0.0133,-0.0713,-0.0132,-0.0043,0.0015]}
{"key":"[Enhancement of Short Text Clustering by Iterative Classification] Short text clustering is a challenging task due to the lack of signal contained in such short texts. In this work, we propose iterative classification as a method to b o ost the clustering quality (e.g., accuracy) of short texts. Given a clustering of short texts obtained using an arbitrary clustering algorithm, iterative classification applies outlier removal to obtain outlier-free clusters. Then it trains a classification algorithm using the non-outliers based on their cluster distributions. Using the trained classification model, iterative classification reclassifies the outliers to obtain a new set of clusters. By repeating this several times, we obtain a much improved clustering of texts. Our experimental results show that the proposed clustering enhancement method not only improves the clustering quality of different clustering methods (e.g., k-means, k-means--, and hierarchical clustering) but also outperforms the state-of-the-art short text clustering methods on several short text datasets by a statistically significant margin.","layer":0,"vector":[-0.0094,-0.0046,0.0124,-0.0056,0.045,-0.0409,0.0124,0.0325,-0.0047,-0.0161,0.0529,-0.0541,0.0231,0.0294,0.0051,0.0182,0.0681,0.0154,-0.027,0.0253,-0.0114,-0.0451,-0.0153,-0.0185,0.0628,0.0093,-0.0116,-0.0678,-0.088,-0.2276,-0.0139,-0.0429,0.081,-0.0235,0.0174,0.03,-0.0379,0.0315,-0.0144,0.0285,0.0271,-0.0172,-0.0053,-0.0494,-0.0356,-0.0483,-0.0661,-0.0095,-0.02,-0.0707,0.0221,-0.025,0.0339,0.0441,0.0291,0.0295,0.0144,0.0165,0.0194,0.0142,0.0658,0.049,-0.1738,0.0568,0.0292,-0.0035,-0.0507,0.0255,0.0336,0.0554,0.027,0.081,0.0234,0.0537,0.0019,0.0316,0.0012,-0.0198,-0.0181,0.0226,0.004,-0.077,-0.0077,0.0276,-0.0128,-0.0773,0.0016,-0.0793,0.0215,-0.0011,-0.0059,0.0199,-0.0196,0.05,-0.1238,-0.05,0.0098,0.0101,0.0036,0.1884,-0.0441,0.047,0.0397,-0.0447,0.0256,-0.0405,-0.0025,-0.0686,-0.0115,-0.0205,-0.0029,0.023,-0.0418,-0.0219,0.0141,-0.014,0.1057,0.0064,-0.0059,0.0009,-0.0256,-0.0112,0.0451,-0.0212,0.0844,-0.049,0.0493,0.1039,0.0454,0.0057,0.0252,0.0128,-0.0501,-0.0127,-0.0133,0.0233,-0.0208,0.0146,0.0506,-0.0581,0.0127,-0.1041,0.0036,-0.092,-0.057,0.1217,-0.0298,0.0021,-0.042,-0.0219,-0.0119,-0.0137,-0.0374,-0.015,0.0194,-0.0203,0.0894,0.0457,-0.0149,0.0105,-0.0017,-0.0532,-0.034,0.0968,0.0349,-0.0716,-0.0191,0.0099,0.0154,-0.0206,0.0651,0.0394,-0.0384,0.0752,0.0746,0.0347,-0.022,-0.0063,0.0003,0.0008,0.0138,-0.0002,-0.0582,0.0556,0.0779,-0.0612,-0.0314,-0.0028,0.0602,0.0286,-0.064,0.0356,-0.0001,-0.0429,-0.0221,-0.0532,-0.0037,-0.0135,0.0023,-0.0436,0.0171,0.0379,-0.0432,0.028,-0.0067,0.0233,0.0309,-0.0318,0.0436,0.0257,-0.0243,-0.0358,0.0476,-0.0349,-0.0128,-0.0151,0.0022,0.0355,-0.0366,0.0423,0.0669,-0.0316,-0.0728,-0.212,-0.0224,0.007,-0.0348,0.0154,-0.0878,-0.0108,0.0097,0.0954,0.057,0.0235,-0.0152,-0.0341,0.0233,-0.0147,0.0456,0.0492,0.0444,-0.0273,0.0295,-0.0118,0.0495,-0.0135,-0.0588,0.0102,0.0092,0.1928,0.059,0.036,-0.02,0.0676,0.0551,-0.0358,-0.0869,0.0641,0.0376,0.0579,-0.0086,-0.0672,0.0101,-0.0108,0.0128,0.0194,-0.0985,-0.0414,-0.0472,-0.0103,-0.0179,-0.0431,0.0456,0.0166,0.0128,0.0632,0.0205,0.0365,-0.0355,-0.0917,0.0337,-0.0493,-0.0176,0.058,-0.0707,-0.005,-0.0736,0.0566,0.0164,-0.0624,-0.0141,0.0385,-0.0484,-0.0444,0.1129,-0.0056,-0.0003,0.0292,0.0018,0.0376,0.0076,-0.0394,0.0013,0.0735,-0.0304,0.0311,0.0336,-0.0121,-0.0113,0.047,-0.0188,0.0051,0.0017,0.0178,0.0489,-0.016,0.0031,0.0178,-0.0128,-0.2735,0.0129,-0.0121,0.0284,-0.0288,-0.0219,0.0085,0.0043,0.0083,-0.0072,-0.0249,0.0364,-0.008,-0.0822,-0.0032,0.0784,0.0272,-0.0675,0.0251,-0.0767,0.0478,0.0623,0.2386,-0.0485,-0.0049,0.0339,-0.0139,-0.0273,0.0277,-0.062,0.0133,-0.001,0.1153,0.0156,0.0355,0.0507,0.0093,0.0054,0.0469,-0.0083,0.0037,0.0279,-0.0503,-0.043,0.0992,-0.0225,-0.0358,-0.0846,0.0099,0.0351,-0.06,0.0035,-0.0297,0.0211,0.0087,0.0555,-0.0591,-0.0255,-0.0695,-0.0598,-0.0132,-0.0845,0.0088,0.044,0.0221]}
{"key":"[Finite Hilbert Transform in Weighted L2 Spaces] Several new properties of weighted Hilbert transform are obtained. If mu is zero, two Plancherel-like equations and the isotropic properties are derived. For mu is real number, a coerciveness is derived and two iterative sequences are constructed to find the inversion. The proposed iterative sequences are applicable to the case of pure imaginary constant mu=i*eta with |eta|<pi/4 . For mu=0.0 and 3.0 , we present the computer simulation results by using the Chebyshev series representation of finite Hilbert transform. The results in this paper are useful to the half scan in several imaging applications.","layer":2,"vector":[-0.0499,-0.0214,0.0041,-0.0391,-0.0318,0.0429,0.0077,0.0432,0.0024,-0.0006,0.028,-0.0913,0.055,0.0201,0.0182,0.0006,0.0194,0.072,-0.0395,0.0103,0.0879,0.0002,0.0026,-0.0377,-0.0039,0.0264,0.0267,-0.0112,-0.036,-0.1978,-0.0213,-0.0294,0.1006,-0.01,0.0121,-0.0559,-0.0399,0.0332,-0.0114,0.0411,-0.0184,-0.0333,0.0266,-0.0365,-0.049,-0.0975,-0.0641,-0.0335,0.0236,-0.0227,0.0153,0.0609,0.0532,0.0399,0.0555,0.0022,0.0465,-0.0017,0.0317,0.0166,0.0413,0.0089,-0.2214,0.0791,0.0396,0.0169,-0.0095,-0.0983,0.0783,0.0441,-0.0384,0.0461,0.0032,-0.0002,0.0433,-0.0329,-0.0125,-0.0066,-0.054,0.0244,-0.023,-0.0517,-0.0187,0.0143,-0.0737,-0.0267,-0.0108,-0.0885,0.038,0.0513,-0.0292,-0.0469,-0.0679,0.0331,-0.0223,-0.0235,-0.0178,0.0543,-0.0116,0.1927,-0.0381,0.0159,0.0951,-0.0404,0.021,-0.031,-0.0276,-0.0553,-0.01,-0.0124,-0.0286,-0.0075,0.0622,-0.0117,0.0038,-0.0051,0.0219,0.0889,-0.013,-0.0133,-0.0212,0.0283,0.023,-0.0031,0.0341,-0.0821,0.0223,0.0974,0.028,0.0705,0.019,-0.0326,-0.0479,0.0085,-0.0328,0.0481,0.0069,-0.0033,0.0255,0.0102,-0.0277,-0.0934,-0.0358,-0.0471,-0.0349,0.0717,-0.0473,0.0404,-0.0625,-0.0017,0.006,0.0328,-0.0419,-0.0142,0.0373,0.0455,0.002,0.0254,-0.0422,0.0664,-0.0255,-0.0711,-0.0101,0.1326,0.0251,-0.0169,-0.0467,0.0204,0.0455,-0.0027,0.0056,0.0186,-0.0547,0.0211,0.0962,-0.021,-0.0269,-0.0097,-0.0146,-0.0107,0.0083,-0.0476,-0.0548,0.0703,0.0311,-0.0739,0.0168,-0.0215,-0.001,0.0439,-0.0654,-0.0042,-0.0716,-0.005,-0.0293,-0.0451,-0.0226,-0.0182,0.039,-0.0314,0.0733,-0.0237,-0.0517,0.0267,0.0301,-0.0207,0.0404,-0.0114,-0.0447,0.0504,-0.0082,-0.0177,0.0956,-0.0133,-0.0322,-0.0253,0.0442,0.0081,0.0279,0.058,0.0087,-0.0811,-0.0764,-0.2039,-0.0303,-0.0127,-0.0099,0.0639,-0.0666,0.0508,-0.0027,0.071,0.0439,0.077,-0.0038,-0.0354,0.0348,-0.0321,0.0266,0.0022,0.0232,0.0014,-0.058,0.0047,0.0295,0.0084,-0.0812,0.1122,-0.0449,0.1697,0.018,0.0345,-0.0012,0.0179,0.0061,-0.0013,-0.0188,0.0352,0.0198,0.0704,-0.0031,-0.0095,-0.0159,0.0046,-0.0185,0.0165,-0.0344,0.0239,0.0143,-0.0464,0.0118,-0.0323,0.0003,0.0242,-0.0401,0.016,-0.0407,0.0291,-0.0198,-0.091,-0.0304,-0.021,0.0407,-0.0215,-0.0445,-0.0042,-0.0349,0.0395,0.0036,0.0244,-0.0377,0.0026,-0.0319,0.0058,0.0673,0.0002,0.0252,0.0514,-0.0168,0.0642,0.0476,0.0147,-0.0433,0.0495,-0.0197,0.0733,0.0405,0.0444,0.0402,0.085,-0.0255,-0.0079,0.0058,-0.0165,0.0615,-0.0358,0.0164,-0.0227,-0.0323,-0.3254,-0.0087,-0.0218,-0.0091,-0.0178,0.0237,-0.0216,0.0038,-0.0693,-0.0007,-0.071,0.0155,0.0231,0.0049,0.0095,0.03,0.026,-0.0613,0.04,-0.0489,-0.0171,0.0369,0.244,-0.023,-0.0095,0.0121,-0.0307,-0.0058,-0.0104,0.0021,-0.0049,-0.0345,0.0564,-0.0807,0.0421,0.0749,-0.0221,0.0835,-0.0285,0.0044,0.0424,0.0489,-0.0591,-0.0251,0.1166,0.0252,0.0015,0.0008,-0.0043,0.0232,-0.0086,0.0454,0.0028,-0.0447,0.0339,0.0374,-0.0597,-0.0384,-0.0529,-0.0058,0.0449,-0.0297,-0.0299,0.0425,0.0353]}
{"key":"[Joint universal lossy coding and identification of i.i.d. vector sources] The problem of joint universal source coding and modeling, addressed by Rissanen in the context of lossless codes, is generalized to fixed-rate lossy coding of continuous-alphabet memoryless sources. We show that, for bounded distortion measures, any compactly parametrized family of i.i.d. real vector sources with absolutely continuous marginals (satisfying appropriate smoothness and Vapnik--Chervonenkis learnability conditions) admits a joint scheme for universal lossy block coding and parameter estimation, and give nonasymptotic estimates of convergence rates for distortion redundancies and variational distances between the active source and the estimated source. We also present explicit examples of parametric sources admitting such joint universal compression and modeling schemes.","layer":5,"vector":[-0.0608,-0.02,0.0288,-0.0176,0.0071,0.0678,-0.0237,0.0575,0.0176,0.0083,0.0145,-0.0232,0.051,0.0197,0.0262,-0.0013,-0.014,0.0571,-0.0439,0.011,0.0946,-0.0552,0.0039,-0.0274,-0.0097,0.0414,-0.0264,-0.0632,-0.0056,-0.2381,0.0145,-0.0082,0.0499,-0.017,0.0294,-0.0152,-0.0311,0.0249,-0.0316,0.0288,0.0312,0.0217,-0.022,-0.0374,-0.0197,-0.0005,-0.0392,-0.0091,-0.0152,-0.0144,0.0377,0.0361,0.0335,0.0427,-0.0139,0.0047,0.0248,0.0631,0.0072,0.0219,-0.0092,0.045,-0.1897,0.0384,0.0467,0.0518,-0.0368,-0.003,0.0353,0.0484,-0.0524,0.0174,0.009,0.0578,-0.0035,-0.0224,0.0181,-0.0171,-0.0184,-0.0018,-0.0227,-0.0745,-0.0439,0.0088,-0.0179,-0.061,0.0291,-0.0484,0.0222,-0.0217,-0.0516,0.0029,0.0255,0.0298,-0.066,-0.0258,0.0023,0.0523,-0.0306,0.1962,-0.0534,0.0182,0.0496,-0.0669,0.0316,-0.0367,-0.0206,-0.0182,0.0186,-0.0061,-0.0348,-0.0338,0.057,-0.033,0.0854,0.0098,0.0418,0.0219,0.0143,-0.0007,-0.0286,0.0213,0.032,-0.0326,0.0268,-0.0541,0.0012,0.1419,0.0651,0.0513,0.0545,-0.0176,-0.0382,-0.0323,-0.0002,0.025,0.0554,0.0095,0.0332,0.0026,-0.0187,-0.077,0.0035,-0.0569,-0.0635,0.1154,-0.037,0.0141,-0.0588,0.0299,0.0183,-0.0283,0.0353,0.0066,0.0432,0.0363,0.0217,0.0304,-0.0289,0.0032,0.0001,-0.0595,-0.0307,0.1018,-0.0315,-0.0392,-0.0006,0.0231,0.0959,0.0008,0.0486,0.0242,-0.046,-0.0138,0.0576,-0.004,-0.0959,-0.0024,0.047,0.0206,0.0054,-0.0716,-0.013,0.0452,0.002,-0.0165,0.0139,-0.0551,0.037,0.0128,-0.0153,-0.0287,-0.026,0.011,-0.0257,-0.0141,-0.0021,-0.0283,0.0139,-0.054,0.0141,-0.0156,-0.058,0.0549,0.0348,0.0041,-0.0346,-0.0132,-0.0158,0.0256,-0.0518,-0.0443,0.0791,-0.0172,-0.0352,-0.0052,0.0057,0.0334,0.0086,0.0636,-0.0065,-0.076,-0.0626,-0.2489,-0.0078,-0.0116,-0.0417,0.0784,-0.0633,0.0088,0.0132,0.0264,0.0472,0.0591,0.0197,-0.019,0.0675,-0.0309,0.0519,0.0496,0.0437,-0.0019,0.0315,-0.0534,0.0571,-0.0292,-0.0653,0.0663,0.0113,0.1933,-0.0041,0.0615,-0.046,-0.0066,0.055,0.0123,-0.1011,0.0284,0.0183,0.0266,0.0121,0.0234,-0.0133,-0.029,-0.0174,0.0167,-0.0522,-0.0307,-0.0393,-0.0737,0.0071,-0.0799,0.0065,0.0478,-0.0274,0.0966,-0.0204,0.0039,-0.0327,-0.0597,-0.0425,-0.0359,0.0515,0.0018,-0.0584,0.0376,-0.0901,0.0702,0.0213,-0.0328,-0.0604,0.0075,-0.0092,-0.0159,0.0513,0.0199,-0.0012,0.0467,-0.012,0.0148,-0.0358,-0.0543,-0.0528,0.058,-0.0238,0.0567,0.0175,0.0551,0.0348,0.0722,0.0125,0.0459,-0.027,-0.0333,-0.0328,-0.0392,-0.0048,0.0194,-0.0564,-0.311,0.0164,-0.0084,-0.0017,-0.057,-0.0051,0.0775,-0.0304,-0.0729,0.0028,-0.0117,0.0603,-0.014,-0.0112,0.0744,0.0894,0.0804,-0.0426,0.0461,-0.0269,0.0313,0.0249,0.1908,-0.0525,0.0146,-0.0006,-0.0432,0.0413,0.0016,-0.0309,0.0241,0.0287,0.0939,-0.0209,0.0325,0.0678,-0.0542,0.0562,0.0139,-0.0367,0.0056,-0.0285,-0.0656,0.002,0.1181,0.0148,-0.0075,-0.0557,-0.0494,-0.0047,-0.0271,0.0258,0.0402,0.0061,-0.0027,0.0139,-0.0948,-0.0476,-0.0107,-0.0217,0.0119,-0.0596,-0.0292,0.0128,-0.0102]}
{"key":"[Implicitly Regularized RL with Implicit Q-Values] The $Q$-function is a central quantity in many Reinforcement Learning (RL) algorithms for which RL agents behave following a (soft)-greedy policy w.r.t. to $Q$. It is a powerful tool that allows action selection without a model of the environment and even without explicitly modeling the policy. Yet, this scheme can only be used in discrete action tasks, with small numbers of actions, as the softmax cannot be computed exactly otherwise. Especially the usage of function approximation, to deal with continuous action spaces in modern actor-critic architectures, intrinsically prevents the exact computation of a softmax. We propose to alleviate this issue by parametrizing the $Q$-function implicitly, as the sum of a log-policy and of a value function. We use the resulting parametrization to derive a practical off-policy deep RL algorithm, suitable for large action spaces, and that enforces the softmax relation between the policy and the $Q$-value. We provide a theoretical analysis of our algorithm: from an Approximate Dynamic Programming perspective, we show its equivalence to a regularized version of value iteration, accounting for both entropy and Kullback-Leibler regularization, and that enjoys beneficial error propagation results. We then evaluate our algorithm on classic control tasks, where its results compete with state-of-the-art methods.","layer":4,"vector":[-0.0857,0.0164,0.0273,-0.0131,-0.0163,0.0513,0.0164,0.0369,0.058,-0.0032,0.007,-0.0149,0.0362,0.0692,0.0506,0.0094,0.0013,0.0448,-0.0505,-0.0135,0.0527,-0.0616,-0.0198,-0.0553,0.0134,-0.0143,-0.0421,-0.0631,-0.0094,-0.2421,0.0173,-0.0214,0.0043,-0.0374,0.0241,0.0115,-0.0544,0.0309,-0.0315,0.0402,0.0467,0.0429,-0.0151,-0.043,-0.0297,-0.052,-0.0472,-0.0361,-0.0164,-0.0157,-0.0024,0.0207,0.0054,0.0221,0.0629,0.0383,0.0653,0.0279,0.0696,0.0252,0.0226,0.0373,-0.1321,0.0337,0.0096,0.058,-0.0213,-0.0292,0.0621,0.0753,-0.0304,0.0637,0.0167,0.0215,0.0413,-0.0677,-0.0208,-0.022,0.016,0.0097,0.0324,-0.0466,-0.0762,0.0078,-0.0236,-0.0606,0.0517,-0.0649,0.0442,0.0201,-0.0348,0.0087,-0.0132,0.0088,-0.0453,0.0043,0.0181,0.0048,-0.0712,0.1934,0.0125,0.0564,-0.0058,0.0096,0.0623,-0.05,-0.0521,-0.0171,-0.0366,0.0034,-0.0569,-0.0089,0.049,-0.0385,-0.001,0.0146,0.0499,0.0795,-0.0071,-0.0279,0.0273,0.0004,0.0522,-0.0277,0.0228,-0.0492,-0.0041,0.1393,0.0236,0.0297,0.0382,-0.0458,-0.027,-0.0329,0.0009,0.0074,0.0365,0.0046,0.0522,0.0455,-0.0567,-0.0093,-0.0105,-0.1169,-0.0456,0.0799,-0.0152,0.0042,-0.0263,-0.0163,0.0171,0.0168,-0.0216,-0.0123,0.043,0.0297,0.0107,0.0685,-0.0397,-0.0102,-0.0517,-0.0753,-0.0158,0.098,-0.0459,-0.0313,-0.0078,-0.0444,0.0014,-0.0099,0.046,0.0101,-0.0579,0.0119,0.0743,0.0032,-0.0586,-0.0018,-0.009,0.0132,0.0114,-0.0825,-0.0463,0.0188,0.0224,-0.0082,0.0234,-0.0414,0.0594,0.0169,-0.0071,0.0123,-0.0468,-0.0161,-0.0388,-0.0374,0.019,0.0005,0.0059,-0.0321,-0.0035,0.0145,-0.053,-0.0259,-0.0023,-0.0087,-0.0525,-0.0131,0.03,0.0299,-0.013,0.0343,0.0495,-0.0098,-0.042,0.0332,0.0266,-0.0297,-0.0244,0.0162,0.0287,-0.0135,-0.0418,-0.2429,-0.0036,-0.0346,-0.0396,0.0573,-0.069,0.0586,-0.0221,0.0268,0.0768,0.0412,-0.0564,-0.0282,0.0539,-0.0137,0.0889,0.0707,0.0327,-0.0369,-0.0048,-0.0125,0.0082,-0.0356,-0.1085,0.0644,-0.0006,0.223,0.0228,0.0972,-0.0022,0.008,0.0352,-0.0021,-0.0764,0.0252,0.0311,0.0929,0.0094,0.0299,-0.0644,0.006,0.0212,-0.0851,-0.1098,-0.0006,-0.0206,-0.0368,0.0193,-0.077,-0.0003,0.0596,-0.0405,0.0386,0.01,-0.0181,-0.0416,-0.0498,0.0157,-0.0318,0.0438,0.0216,-0.0419,-0.0084,-0.0232,0.0822,-0.0036,0.0659,-0.0222,0.0382,-0.0013,-0.0548,0.0455,0.012,0.0084,0.0542,0.0072,0.0098,-0.0063,-0.0627,-0.0376,0.0653,-0.0309,-0.001,-0.0077,-0.0301,-0.0402,0.0457,-0.0261,0.034,-0.0263,-0.019,-0.0043,-0.0768,-0.0104,0.0489,-0.018,-0.2965,0.0353,0.0169,0.001,-0.0229,-0.0092,0.0708,-0.0245,-0.0411,0.0148,0.0158,0.0992,0.0245,0.0313,0.0151,0.0046,0.0924,-0.0083,0.0695,-0.073,0.0469,0.0052,0.2064,-0.0276,0.0619,0.011,-0.0505,-0.0428,0.0399,-0.0276,0.0217,0.05,0.0803,-0.0736,0.0551,0.0671,-0.0273,0.0347,0.0239,0.0302,-0.017,-0.0033,0.0072,-0.0235,0.1074,-0.0118,-0.0366,-0.0495,-0.0231,0.0425,-0.0425,0.0232,-0.0495,0.009,0.0346,0.0091,-0.052,-0.0518,-0.0463,-0.0358,-0.0135,-0.0651,0.0358,0.0237,0.0053]}
{"key":"[Deep Innovation Protection: Confronting the Credit Assignment Problem in Training Heterogeneous Neural Architectures] Deep reinforcement learning approaches have shown impressive results in a variety of different domains, however, more complex heterogeneous architectures such as world models require the different neural components to be trained separately instead of end-to-end. While a simple genetic algorithm recently showed end-to-end training is possible, it failed to solve a more complex 3D task. This paper presents a method called Deep Innovation Protection (DIP) that addresses the credit assignment problem in training complex heterogenous neural network models end-to-end for such environments. The main idea behind the approach is to employ multiobjective optimization to temporally reduce the selection pressure on specific components in multi-component network, allowing other components to adapt. We investigate the emergent representations of these evolved networks, which learn to predict properties important for the survival of the agent, without the need for a specific forward-prediction loss.","layer":0,"vector":[-0.0492,-0.0199,0.0018,-0.017,0.0388,0.009,0.0046,0.0368,0.0323,0.0006,-0.01,-0.0318,0.0435,0.0647,-0.0192,0.0073,-0.0241,0.0096,-0.0246,0.0047,0.0276,-0.0359,-0.0095,-0.0375,0.028,0.0276,-0.0295,-0.0155,-0.0491,-0.2495,0.0259,-0.0468,0.0406,-0.0405,0.03,-0.0061,-0.033,0.0296,-0.0287,0.0298,0.0258,0.0259,-0.0184,-0.0504,0.002,-0.0479,0.0153,-0.0051,0.0144,-0.0416,0.0478,-0.0567,0.0017,0.0499,0.0271,-0.003,0.092,0.0557,0.0321,0.0409,0.0348,0.0313,-0.1716,0.0452,0.0532,0.0533,-0.03,-0.0143,0.0316,0.0565,0.0027,0.0385,0.0269,-0.0089,0.0091,0.0118,0.0137,-0.0384,0.0097,0.0096,0.0384,-0.0593,-0.0667,0.0042,-0.0103,-0.0761,0.0139,-0.0374,0.035,0.0286,-0.036,0.0391,0.0051,-0.0086,-0.0586,0.0004,0.0018,0.0279,-0.0753,0.2015,-0.0195,0.0113,0.0419,-0.0034,0.0432,0.0036,-0.0345,-0.0353,-0.0336,-0.0032,-0.0146,0.0115,0.0116,-0.0071,0.0056,0.0316,0.024,0.0141,-0.0023,-0.0113,0.0015,0.0152,0.0235,-0.011,0.0032,-0.0729,-0.0063,0.1502,-0.0028,0.0016,0.0159,0.0123,-0.0189,-0.0376,0.0463,0.031,-0.0514,-0.0058,-0.0188,-0.0099,-0.046,0.0017,0.0236,-0.1091,-0.0275,0.0873,0.0125,-0.0084,-0.0041,-0.0452,-0.0156,0.0178,-0.0012,-0.0404,0.0039,0.0414,0.0266,0.0666,-0.0438,-0.0024,-0.0413,-0.0176,-0.0617,0.1555,-0.0107,-0.0799,-0.0172,0.0228,0.0119,-0.0276,-0.0084,0.0542,-0.0398,-0.0051,0.0748,0.0081,-0.1193,0.0332,0.0007,0.0274,-0.0002,-0.0733,-0.0674,-0.0144,0.0338,-0.0372,0.0137,-0.0888,-0.0445,0.0121,-0.0251,0.0829,-0.0311,0.0439,0.0075,-0.0024,-0.0185,0.0001,0.0161,-0.0235,-0.0422,0.0313,-0.0425,-0.0106,-0.042,0.0226,0.0378,0.0118,0.0601,0.04,-0.0575,0.0097,0.0657,-0.0005,-0.0161,0.0188,0.009,0.0409,0.0171,0.0647,0.0169,-0.0332,-0.0164,-0.241,0.0243,-0.0209,-0.045,0.0571,-0.053,0.0453,-0.0369,0.0008,0.0457,0.0556,-0.0231,-0.0091,0.0333,-0.0117,0.0515,0.0442,0.0252,-0.0118,0.008,-0.004,-0.0155,0.0109,-0.0839,0.031,-0.0156,0.2462,0.026,0.0049,0.0167,0.0261,0.0675,-0.028,-0.1299,0.0618,0.0004,0.0791,-0.0234,-0.0687,-0.0802,-0.007,0.0135,-0.0122,-0.0981,0.0028,-0.0204,-0.051,0.0714,-0.0679,-0.0003,0.035,-0.047,0.0466,0.0065,-0.0252,-0.0439,-0.0622,0.0869,-0.0571,0.0104,0.0133,-0.0518,0.0117,-0.041,0.0461,-0.0126,-0.0027,-0.0339,0.0576,-0.0474,-0.0035,0.074,0.0106,-0.0103,0.0281,-0.0324,0.0355,-0.0109,-0.0217,0.0031,0.0553,-0.0713,0.0331,0.0528,0.0054,-0.002,0.0669,-0.0206,0.0546,-0.0221,0.0064,-0.0238,-0.0409,-0.0089,0.0601,-0.0338,-0.3004,0.0332,0.0388,0.0028,-0.0174,-0.0036,0.0133,-0.0096,-0.0412,0.0087,-0.0092,0.0278,0.0582,0.0073,0.0073,-0.0137,0.0946,-0.0294,0.0571,-0.0521,0.0154,0.0433,0.2461,-0.0075,0.077,-0.0098,-0.0566,0.0072,0.0173,-0.0054,-0.0085,0.0447,0.0608,-0.0841,0.0653,0.1022,-0.0194,0.0065,-0.0039,0.0258,-0.0206,-0.0234,-0.009,0.0026,0.0827,0.0058,-0.0584,-0.0332,-0.0289,0.0266,-0.0116,0.0031,-0.0374,-0.0414,0.0354,0.0127,-0.0114,-0.0444,-0.0366,-0.0075,0.035,-0.0405,-0.0183,-0.0097,-0.0152]}
{"key":"[Score-Based Generative Modeling through Stochastic Differential Equations] Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time SDE that transforms the prior distribution back into the data distribution by slowly removing the noise. Crucially, the reverse-time SDE depends only on the time-dependent gradient field (\\aka, score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical SDE solvers to generate samples. We show that this framework encapsulates previous approaches in score-based generative modeling and diffusion probabilistic modeling, allowing for new sampling procedures and new modeling capabilities. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time SDE. We also derive an equivalent neural ODE that samples from the same distribution as the SDE, but additionally enables exact likelihood computation, and improved sampling efficiency. In addition, we provide a new way to solve inverse problems with score-based models, as demonstrated with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 2.99 bits/dim, and demonstrate high fidelity generation of 1024 x 1024 images for the first time from a score-based generative model.","layer":2,"vector":[-0.0322,-0.0209,0.0218,-0.0447,0.0443,0.0439,-0.0145,-0.0278,0.0417,0.0371,0.0066,-0.0413,0.0438,0.0441,-0.0077,0.0111,-0.0254,0.0184,-0.0292,-0.0061,0.0333,-0.013,0.036,-0.0561,-0.0027,-0.0036,-0.0196,-0.058,-0.0464,-0.254,-0.0058,-0.0607,0.0499,-0.0346,0.0233,-0.0382,-0.0779,0.0694,-0.0414,0.0354,0.0036,0.0374,-0.0129,-0.0644,-0.001,-0.042,-0.0331,-0.0324,-0.003,-0.023,0.0169,-0.0289,0.0091,0.06,0.0327,0.0559,0.1006,0.0428,0.0435,0.06,-0.0139,0.0825,-0.1577,0.0332,0.0466,0.0074,-0.0396,-0.0132,0.0025,0.028,-0.0445,0.0345,0.0029,0.0679,0.052,-0.0171,-0.0227,-0.0711,-0.0343,0.0469,0.0312,-0.0328,-0.0535,-0.0312,-0.0048,-0.0327,0.0205,-0.019,0.0566,0.0293,-0.0275,-0.0462,-0.0783,0.0594,-0.0681,-0.0057,-0.0259,0.0336,0.0009,0.1845,-0.046,0.0304,0.0754,-0.0317,0.0276,-0.0442,-0.0409,-0.0126,-0.0278,-0.0091,-0.0037,-0.0099,0.057,-0.0414,0.0166,-0.0071,0.0454,0.0459,0.0094,-0.0052,-0.0395,0.0335,0.0242,-0.0203,0.016,-0.0802,-0.0082,0.1404,0.0306,0.0406,0.0446,-0.0198,-0.0658,-0.0036,0.0212,-0.027,0.013,-0.0268,0.012,-0.0492,-0.0307,-0.0055,-0.0052,-0.0542,-0.0849,0.1003,0.0075,0.0765,-0.0743,-0.0456,0.0245,0.0353,-0.0038,-0.0279,0.0351,0.0492,-0.0195,0.0152,-0.0436,0.0281,-0.0013,-0.0672,-0.0257,0.1065,-0.0051,-0.0453,-0.0006,0.0747,0.0182,0.017,-0.013,0.0222,-0.0369,0.0288,0.0864,0.025,-0.0304,-0.028,0.015,0.0298,0.0011,-0.0517,-0.0349,0.0329,0.0484,-0.0627,-0.0035,-0.0407,0.0164,0.0524,0.0139,-0.0133,-0.003,0.0075,-0.0085,-0.044,-0.0435,-0.0165,0.0117,-0.0435,-0.0132,-0.036,-0.0501,0.0087,-0.0209,-0.0173,0.0024,-0.0007,0.007,0.0617,-0.0266,-0.0109,0.0954,0.0008,0.0056,0.0022,0.0079,0.0175,0.0034,0.0403,0.0263,-0.0558,-0.0315,-0.2107,0.0245,0.027,-0.0043,0.0416,-0.0822,0.0015,-0.0375,0.0528,0.0865,0.0519,0.015,0.0327,0.0222,0.0071,0.039,-0.0138,0.0378,0.0183,-0.0445,-0.0081,-0.0145,0.0226,-0.1181,0.0658,-0.0239,0.2214,0.0513,0.0535,0.0098,0.0608,0.0263,-0.0253,-0.0711,0.0608,0.0221,0.109,-0.027,-0.0602,-0.0358,-0.0257,0.0335,0.0023,-0.1089,-0.0437,-0.0294,-0.0189,0.0381,-0.0287,0.0305,0.0091,-0.0794,0.0797,-0.0303,-0.0014,-0.0608,-0.0828,0.0299,-0.0509,-0.0108,0.0074,-0.0444,0.0059,-0.0599,0.0154,0.0027,-0.0184,-0.0987,0.0346,-0.0479,0.0309,0.0538,-0.0172,0.0297,0.0597,0.0097,0.0166,-0.0215,-0.034,-0.0485,0.0422,-0.0186,0.0399,0.0387,0.0467,0.0105,0.0549,-0.0123,-0.0016,-0.0347,0.0086,0.0355,-0.0673,0.0039,0.0294,-0.0188,-0.3106,0.0204,-0.0055,0.0242,0.0349,0.0186,0.0681,0.023,-0.0455,-0.0041,-0.0435,0.05,0.0444,0.0197,0.0177,-0.0113,0.0545,-0.1013,0.101,-0.0615,-0.0042,0.0267,0.2084,-0.0576,0.0275,-0.0171,0.0227,0.0137,0.0631,-0.0096,0.0072,0.0249,0.0726,-0.0432,0.0392,0.0924,-0.0432,0.0281,0.0409,-0.0569,0.001,-0.0066,-0.0087,-0.0139,0.0335,0.0136,0.0165,-0.0147,0.0272,0.0222,-0.0347,0.0352,-0.0035,-0.0098,0.0104,0.0343,-0.065,-0.041,-0.0128,-0.0377,0.0195,-0.0664,-0.0131,0.0269,-0.0363]}
{"key":"[Interpretable Learning for Self-Driving Cars by Visualizing Causal Attention] Deep neural perception and control networks are likely to be a key component of self-driving vehicles. These models need to be explainable - they should provide easy-to-interpret rationales for their behavior - so that passengers, insurance companies, law enforcement, developers etc., can understand what triggered a particular behavior. Here we explore the use of visual explanations. These explanations take the form of real-time highlighted regions of an image that causally influence the network's output (steering control). Our approach is two-stage. In the first stage, we use a visual attention model to train a convolution network end-to-end from images to steering angle. The attention model highlights image regions that potentially influence the network's output. Some of these are true influences, but some are spurious. We then apply a causal filtering step to determine which input regions actually influence the output. This produces more succinct visual explanations and more accurately exposes the network's behavior. We demonstrate the effectiveness of our model on three datasets totaling 16 hours of driving. We first show that training with attention does not degrade the performance of the end-to-end network. Then we show that the network causally cues on a variety of features that are used by humans while driving.","layer":0,"vector":[0.0009,-0.0116,0.0202,-0.0272,0.0331,0.0555,0.0775,0.0225,0.0118,-0.0537,0.0409,-0.055,0.0427,0.081,-0.0091,0.0333,-0.0086,0.0093,-0.0163,-0.0312,0.0045,-0.041,-0.0178,-0.0505,-0.03,0.0519,-0.0102,-0.0096,-0.0535,-0.2285,0.0197,-0.0704,0.019,-0.0099,-0.0248,-0.0449,-0.0613,0.0545,-0.0219,-0.0104,0.0473,0.0053,-0.0594,-0.0408,-0.0518,-0.0448,0.0008,-0.0183,-0.0109,-0.0666,-0.0029,-0.0189,0.0439,0.0245,0.0187,0.0144,0.0627,0.0612,0.059,0.0027,0.0314,0.029,-0.1828,0.0674,0.0777,0.0422,-0.0328,0.0053,0.0005,0.0377,-0.0101,-0.0046,-0.0335,0.0283,0.0072,-0.0195,0.006,-0.0161,-0.0129,-0.0211,0.0121,0.0098,-0.0187,-0.0097,0.019,-0.0387,0.0072,-0.0414,0.0592,0.0061,-0.0793,-0.0298,-0.074,-0.0205,-0.0388,0.0036,0.037,-0.0325,-0.027,0.1715,-0.038,-0.0006,0.0293,-0.0212,0.0257,-0.0092,-0.0062,-0.0082,-0.0364,0.0411,-0.0011,-0.011,0.014,0.0035,0.0525,0.0244,0.0532,0.0472,0.0087,-0.0142,0.0017,-0.0237,0.0454,-0.0689,0.02,-0.0757,0.0383,0.1597,0.0383,0.0017,0.0541,-0.0384,-0.0475,-0.0145,0.0005,-0.0196,0.035,0.0135,-0.0081,-0.0039,-0.0211,-0.0502,0.0151,-0.1086,-0.0566,0.0723,-0.0286,0.0079,0.0102,-0.0058,-0.0171,-0.0028,-0.0458,-0.0511,0.0081,0.037,0.064,0.0325,-0.0763,0.0475,-0.0235,-0.0499,-0.0526,0.0712,0.0254,-0.087,0.0051,0.0207,0.021,-0.0211,-0.0032,0.0272,-0.0419,0.0408,0.0828,0.0628,-0.1026,0.0372,-0.0231,-0.0128,0.0207,-0.08,-0.04,0.0502,0.0544,0.0133,0.0043,-0.0381,-0.026,0.0683,0.0084,0.0258,-0.0493,0.0374,-0.0492,-0.0366,0.0201,-0.0036,-0.0192,-0.0153,-0.0263,0.0113,-0.0176,0.0169,-0.06,0.0046,-0.0583,0.0495,0.0607,0.0195,-0.0477,0.0365,0.0521,-0.0149,-0.0311,-0.0122,-0.0082,0.0396,-0.0225,0.0282,0.0475,-0.0464,-0.0392,-0.2242,-0.0217,0.0175,-0.0341,-0.0099,-0.086,0.0301,0.0224,0.0516,0.0707,0.0668,-0.0542,-0.0232,-0.015,0.0107,0.0513,-0.026,0.0616,-0.034,-0.0138,-0.0017,0.0136,0.0185,-0.0864,0.0392,0.0299,0.2395,0.0246,0.054,-0.0024,0.0307,0.0433,-0.0116,-0.099,0.0519,-0.0246,0.0695,0.0028,-0.0527,-0.0285,-0.0363,0.0305,-0.0328,-0.0833,-0.0054,-0.02,-0.005,0.055,-0.0204,0.0217,0.0492,-0.055,0.0593,0.024,-0.021,-0.0341,-0.0884,0.0201,-0.0564,0.0126,-0.0178,-0.0013,0.0111,-0.0925,0.0523,0.0507,-0.0383,-0.0733,0.0014,0.0057,-0.0211,0.1122,-0.0178,-0.0027,0.0632,0.0139,0.0528,-0.0132,-0.0386,-0.0137,0.048,0.0343,0.0201,0.021,0.0593,0.0219,0.0588,-0.0223,0.0126,-0.0203,0.0039,0.0561,-0.0353,-0.0424,0.0499,-0.002,-0.2917,0.0504,0.005,0.0493,-0.0165,0.0357,0.0567,0.0502,-0.0281,-0.0377,-0.0545,0.0389,0.0776,0.026,-0.0022,0.0415,0.0467,-0.0117,0.0432,-0.0517,0.015,0.0441,0.2192,-0.0326,0.027,0.0269,-0.0808,-0.0349,0.0622,-0.0014,0.0061,0.0311,0.0725,-0.0511,0.0118,0.0672,-0.0564,0.0401,0.0431,0.0031,0.0091,0.0363,0.0092,-0.0709,0.1018,-0.0119,-0.0213,-0.0634,0.0184,0.0232,0.0105,-0.0089,-0.0472,-0.018,0.0266,0.0001,-0.0704,-0.0306,-0.0181,-0.047,0.0331,-0.0823,0.0124,-0.0013,0.0065]}
{"key":"[Extreme Precipitation Seasonal Forecast Using a Transformer Neural Network] An impact of climate change is the increase in frequency and intensity of extreme precipitation events. However, confidently predicting the likelihood of extreme precipitation at seasonal scales remains an outstanding challenge. Here, we present an approach to forecasting the quantiles of the maximum daily precipitation in each week up to six months ahead using the temporal fusion transformer (TFT) model. Through experiments in two regions, we compare TFT predictions with those of two baselines: climatology and a calibrated ECMWF SEAS5 ensemble forecast (S5). Our results show that, in terms of quantile risk at six month lead time, the TFT predictions significantly outperform those from S5 and show an overall small improvement compared to climatology. The TFT also responds positively to departures from normal that climatology cannot.","layer":8,"vector":[-0.0562,-0.0154,0.0588,0.0023,0.0645,-0.007,-0.0202,0.0379,0.0116,0.0207,0.023,-0.0339,0.0359,0.0079,0.009,0.0149,-0.0267,0.0339,-0.0216,-0.0207,0.0443,-0.0048,-0.0328,-0.0548,0.0187,0.0381,0.0029,-0.0074,-0.0516,-0.2395,-0.0165,-0.0538,-0.0083,-0.0331,-0.0272,-0.0131,-0.017,0.0284,-0.0026,0.0656,-0.0101,0.0552,-0.0241,-0.0728,-0.035,-0.0712,-0.0017,-0.0019,-0.0427,-0.0399,0.0607,-0.0297,0.029,0.0617,0.0447,0.0397,0.0469,0.0557,0.0406,0.0184,-0.0083,0.0696,-0.1951,0.0539,0.0551,0.0518,-0.0611,0.0063,-0.0066,-0.0212,-0.0125,0.0336,0.0349,0.015,0.0464,0.0078,-0.0347,-0.0237,-0.0019,0.0036,0.0404,-0.0223,-0.0479,-0.0669,0.0017,-0.0552,0.0067,-0.0379,0.0498,-0.0254,-0.0233,-0.015,-0.0312,0.0391,-0.0615,0.0413,0.0725,-0.014,-0.07,0.2222,-0.0909,0.0117,0.0172,-0.0135,0.0379,-0.055,-0.0332,-0.0509,-0.0358,-0.027,-0.0335,-0.0699,0.0241,-0.0305,0.0314,-0.0151,0.0336,0.036,-0.0139,-0.0259,-0.0358,0.0486,0.0681,0.0253,-0.0091,-0.0089,0.0638,0.1028,0.0059,0.011,0.092,-0.022,-0.0479,-0.0124,0.0365,-0.0029,0.0141,-0.0314,-0.0106,0.0069,-0.0074,-0.0431,0.0012,-0.0977,-0.0084,0.1092,-0.0506,-0.0167,-0.035,0.0008,-0.0488,0.0512,-0.0193,-0.0627,0.0767,0.0444,0.0149,0.0577,-0.0583,-0.0473,-0.0205,-0.0434,-0.0317,0.0746,0.0327,-0.0569,0.0213,0.0073,0.0424,0.0118,0.0072,-0.0151,0.0355,0.022,0.1045,0.0061,-0.0258,-0.0073,-0.0067,0.0331,0.0139,-0.0251,-0.0105,0.0612,0.0322,-0.0396,0.0192,-0.0709,-0.0095,0.0348,-0.0369,0.0053,0.0256,0.0168,-0.0073,-0.0332,-0.0117,0.0369,0.0139,-0.1054,-0.0196,-0.0168,-0.0148,-0.0336,0.0025,0.0501,-0.0066,-0.017,0.0599,0.0411,0.0216,-0.024,0.1168,-0.0292,-0.0194,0.0516,0.0224,0.0558,0.0007,0.0107,0.07,-0.0571,-0.0507,-0.189,-0.0413,0.0369,-0.0217,0.1011,-0.0406,0.0037,-0.0454,0.0571,0.0571,0.0671,-0.0373,-0.0313,0.0079,0.0045,0.0369,0.0144,0.0397,0.001,0.0365,-0.052,0.0245,-0.0058,-0.0865,0.065,0.0007,0.1856,0.0167,0.0694,-0.0534,0.0496,0.0061,-0.0133,-0.0693,0.0654,0.0099,0.0267,-0.0289,-0.0675,-0.0507,-0.0176,0.0252,-0.0065,-0.051,-0.0427,-0.0187,-0.0111,0.1052,-0.0724,0.0185,0.0156,-0.0586,0.0546,-0.028,0.0556,-0.0764,-0.0957,0.0301,-0.0104,0.0126,0.0235,-0.0417,0.0044,-0.0853,-0.0065,0.0081,0.0054,-0.046,0.0248,-0.0489,-0.0575,0.0792,-0.046,0.0102,0.0748,-0.0082,0.0297,-0.0358,-0.0236,-0.0397,0.0766,-0.066,0.0623,0.0198,0.0485,-0.0017,0.0393,0.0111,0.0256,0.0136,-0.009,-0.0231,-0.0292,-0.0293,0.0358,-0.008,-0.3032,0.0353,0.0033,0.007,-0.0043,-0.0144,0.0031,0.0243,-0.054,0.0086,-0.0338,0.0414,0.0581,0.0245,0.0059,0.0189,0.062,-0.0397,0.0231,-0.0117,-0.0111,0.0333,0.2359,-0.0133,0.0088,0.0709,-0.0467,-0.0176,0.0222,-0.0294,0.0484,0.0199,0.0427,-0.0585,0.0086,0.0438,-0.0031,0.0701,0.0485,-0.0197,0.007,0.0425,-0.0271,-0.0425,0.0922,-0.0222,0.0104,-0.0674,-0.0237,0.0393,-0.0369,0.0098,-0.0236,-0.0266,-0.0091,0.0499,-0.0279,0.0015,-0.0319,-0.0617,0.0277,-0.0616,-0.0231,-0.0206,0.0092]}
{"key":"[Defense-GAN: Protecting Classifiers Against Adversarial Attacks Using Generative Models] In recent years, deep neural network approaches have been widely adopted for machine learning tasks, including classification. However, they were shown to be vulnerable to adversarial perturbations: carefully crafted small perturbations can cause misclassification of legitimate images. We propose Defense-GAN, a new framework leveraging the expressive capability of generative models to defend deep neural networks against such attacks. Defense-GAN is trained to model the distribution of unperturbed images. At inference time, it finds a close output to a given image which does not contain the adversarial changes. This output is then fed to the classifier. Our proposed method can be used with any classification model and does not modify the classifier structure or training procedure. It can also be used as a defense against any attack as it does not assume knowledge of the process for generating the adversarial examples. We empirically show that Defense-GAN is consistently effective against different attack methods and improves on existing defense strategies. Our code has been made publicly available at https://github.com/kabkabm/defensegan","layer":0,"vector":[-0.0226,-0.0187,0.0082,-0.0145,0.0508,0.0158,0.056,-0.0271,-0.0108,-0.0139,-0.0074,-0.0385,0.0507,0.0506,0.0136,0.0177,0.0695,0.0351,-0.0483,-0.0096,0.0269,0.003,0.0386,-0.0113,0.0071,0.0191,0.0126,-0.0006,-0.0535,-0.2325,0.0242,-0.094,0.0148,-0.038,0.0004,-0.0402,-0.0448,0.0575,-0.0184,0.0289,0.005,0.06,-0.0532,-0.0588,-0.0076,-0.0099,-0.0324,-0.0215,-0.001,-0.0604,0.0562,-0.0384,0.029,0.0257,-0.0038,-0.041,0.0896,0.0498,0.0533,0.0958,0.0211,0.0774,-0.127,0.028,0.0096,0.0341,0.0024,0.0117,0.0172,0.0428,-0.0037,0.0222,-0.0263,0.0204,-0.0011,0.0074,-0.0205,-0.0397,-0.0235,-0.0088,0.0407,0.0083,-0.042,0.0342,0.0145,-0.0038,0.0198,-0.0222,0.0824,0.0193,-0.0217,0.0047,-0.0235,0.0256,-0.0271,-0.0085,0.0491,0.0307,-0.1038,0.2122,-0.0759,-0.0028,0.0354,-0.0541,0.013,-0.037,-0.0483,-0.0595,-0.0705,-0.0169,-0.0009,0.0079,0.0015,-0.0049,0.0073,-0.0324,0.0701,0.0029,-0.0558,-0.0321,-0.027,0.0121,0.0283,-0.0226,0.0637,-0.0855,0.0213,0.1424,0.0494,0.0227,-0.0021,-0.0229,-0.0268,-0.0006,0.0207,0.0687,-0.0203,0.0499,0.0148,0.0021,-0.0415,-0.034,0.0108,-0.0491,-0.0155,0.0826,-0.0714,0.023,0.0028,-0.0516,-0.0428,0.0064,-0.0444,-0.0388,-0.0126,0.0419,0.0077,0.0887,-0.0489,0.0028,-0.0144,-0.0487,-0.0484,0.1046,0.0161,-0.0692,-0.0253,0.0063,0.0044,-0.0267,-0.0009,0.0356,0.0077,0.027,0.046,0.0053,-0.101,-0.0227,-0.0182,0.023,-0.0153,-0.0949,-0.0207,0.007,0.0589,0.0027,-0.0105,-0.0551,0.0334,0.0315,-0.0501,0.0493,-0.0632,-0.0245,-0.0111,-0.0361,-0.0474,-0.0043,0.0026,-0.0251,-0.0074,0.0127,-0.0101,-0.02,-0.0338,0.0491,0.0151,0.0013,0.0215,0.0458,-0.0233,0.0191,0.0423,-0.047,-0.037,0.0111,-0.0074,0.073,-0.0004,0.0351,0.0281,-0.0288,-0.0211,-0.2469,0.0064,-0.0082,-0.0714,0.0362,-0.1081,0.0276,-0.0327,0.0099,0.0251,0.04,0.0237,-0.0188,0.0213,-0.0144,0.0506,0.0157,0.0274,-0.0406,0.0062,-0.0096,0.0373,0.0142,-0.1203,0.0122,0.0529,0.224,0.0668,0.0556,-0.0235,0.0478,0.0378,-0.0487,-0.106,0.082,0.0031,0.0456,-0.0089,-0.0361,-0.0288,-0.0133,0.052,0.0192,-0.1214,0.0213,-0.0407,-0.0412,0.032,-0.0298,0.0317,0.0382,-0.0015,0.0622,0.005,0.022,-0.0407,-0.1164,0.058,-0.0409,0.0364,-0.0005,-0.0648,0.0472,-0.1179,0.0594,0.0014,-0.0468,-0.0728,0.0307,-0.0198,0.0041,0.0752,0.0205,0.0168,0.068,0.0092,0.0259,-0.0336,-0.0976,-0.0101,0.0454,-0.0031,0.0276,0.0064,0.0335,-0.0069,0.0859,0.0194,0.0459,-0.0222,0.006,0.0231,-0.0666,-0.0194,0.0376,0.0026,-0.2768,0.046,0.0268,0.079,-0.0306,0.0192,0.0623,0.0231,-0.0738,0.0325,-0.0062,0.013,0.0463,-0.0347,0.0092,-0.0034,0.0561,-0.0532,0.0328,-0.0078,0.0401,0.0313,0.2174,-0.0364,0.0023,-0.0238,0.0098,0.0574,-0.0079,-0.0132,-0.016,0.0057,0.0927,-0.0134,-0.0048,0.1039,-0.0134,-0.0137,-0.0048,-0.0438,0.0095,-0.0136,-0.0424,0.0572,0.0585,-0.0078,-0.0027,-0.002,-0.0048,0.0173,-0.0451,-0.0294,0.0009,0.0052,0.0477,-0.0134,-0.0455,-0.0515,-0.0429,0.02,0.019,-0.0462,-0.0169,0.0081,-0.0158]}
{"key":"[Xplique: A Deep Learning Explainability Toolbox] Today's most advanced machine-learning models are hardly scrutable. The key challenge for explainability methods is to help assisting researchers in opening up these black boxes, by revealing the strategy that led to a given decision, by characterizing their internal states or by studying the underlying data representation. To address this challenge, we have developed Xplique: a software library for explainability which includes representative explainability methods as well as associated evaluation metrics. It interfaces with one of the most popular learning libraries: Tensorflow as well as other libraries including PyTorch, scikit-learn and Theano. The code is licensed under the MIT license and is freely available at github.com/deel-ai/xplique.","layer":0,"vector":[-0.047,0.0031,0.0408,0.0127,0.0302,0.0279,0.0143,0.0339,0.0201,-0.0122,-0.0056,-0.0827,0.0206,0.0346,0.0221,0.0182,-0.0589,0.0429,-0.0709,-0.0095,0.0506,-0.032,-0.0216,-0.0319,0.0167,0.0347,0.0034,-0.0178,-0.0545,-0.2399,0.0387,-0.0241,0.0379,-0.0453,0.0001,-0.0184,-0.0225,0.0204,-0.0538,0.0435,0.0469,0.0308,-0.0337,-0.0241,-0.0239,-0.0787,-0.0149,-0.0467,-0.0493,-0.024,0.0294,-0.044,0.0088,0.0202,0.0036,0.0409,0.062,0.0843,0.0362,0.0101,0.0205,0.0418,-0.1345,0.0713,0.0719,0.0185,-0.0285,-0.0197,0.0118,0.0395,-0.0105,0.0454,0.015,0.0621,0.0089,0.0091,-0.0108,-0.0439,0.0197,0.0102,0.0202,-0.0122,-0.0325,-0.0259,0.0029,-0.0334,0.019,-0.0267,0.0162,-0.0061,-0.0194,0.0174,-0.0622,-0.0013,-0.0332,-0.0184,0.0423,-0.0087,-0.0743,0.1994,-0.0379,0.0082,0.0026,0.0016,0.0295,-0.0334,-0.0401,-0.059,0.0105,-0.0433,-0.044,0.009,0.0482,-0.0428,0.0118,0.0185,0.0334,-0.0041,-0.0198,0.0057,-0.0396,-0.0082,0.0422,0.0161,0.0104,-0.0413,0.0269,0.1377,-0.0063,0.014,0.0362,-0.0333,-0.0483,0.0242,0.0434,0.0168,0.0119,-0.0006,0.0075,-0.0077,-0.0382,-0.034,0.0265,-0.112,-0.0663,0.0984,-0.082,-0.0029,-0.047,-0.014,-0.0275,0.0086,-0.0435,-0.0413,0.0746,0.0692,0.0351,0.0319,-0.06,0.0542,0.0203,-0.0725,-0.0486,0.0843,0.0329,-0.0905,-0.0237,0.0041,0.0097,-0.0032,0.0738,0.012,-0.0728,0.0103,0.0414,0.0104,-0.0321,-0.0316,0.0002,-0.0108,0.0335,-0.0729,-0.029,0.091,0.0329,-0.0263,0.0007,-0.0481,0.0411,0.0705,-0.0007,0.0345,-0.0257,0.001,-0.0403,0.0179,0.0135,-0.0054,0.0016,0.0006,0.0095,0.0321,-0.0428,0.0385,-0.0533,0.0222,-0.0061,0.0317,0.0767,0.0281,-0.0208,0.0178,-0.0162,-0.0509,-0.0097,0.0241,0.028,0.0313,-0.0221,0.0527,0.0186,-0.0522,-0.0644,-0.2565,0.0113,0.008,-0.0459,0.0453,-0.09,0.0648,-0.0167,0.0284,0.0965,0.0878,0.0037,-0.0351,-0.0216,-0.0234,0.0413,0.0361,0.0129,-0.0386,-0.0055,-0.0051,0.0238,0.0073,-0.1132,-0.0035,0.0226,0.2242,0.033,0.0613,-0.0142,-0.0145,0.031,-0.0458,-0.1277,0.0712,0.0085,0.0822,-0.0357,-0.0066,-0.0219,-0.0125,0.0584,-0.0078,-0.1052,-0.0271,0.0111,-0.05,0.0538,-0.0456,0.055,0.0069,-0.0379,0.0467,-0.0033,0.0118,-0.0492,-0.1007,0.019,-0.0232,0.0336,-0.0514,-0.0321,0.0279,-0.0743,0.0706,-0.0109,-0.0324,-0.0651,0.0158,-0.0305,-0.0301,0.0983,0.021,-0.0264,0.095,0.0712,0.0587,-0.0312,-0.0248,-0.0223,0.0637,-0.0048,0.0153,0.0375,-0.0092,-0.0044,0.0685,-0.0218,0.0348,-0.0167,-0.0119,0.0206,-0.058,-0.0606,0.04,-0.0277,-0.2932,0.0261,0.0287,0.038,-0.0586,-0.0166,0.0623,0.0178,-0.0214,0.0383,0.0165,0.032,0.053,0.0075,-0.0166,0.0192,0.0899,-0.0454,0.0617,-0.0172,0.0321,0.0279,0.2003,-0.0136,0.0048,0.0109,-0.0392,-0.0395,0.0547,0.0085,0.0206,-0.0182,0.0754,-0.0229,0.0272,0.0296,-0.0174,0.0194,0.0822,-0.037,0.0391,-0.028,-0.0379,-0.0525,0.071,-0.0119,-0.0058,-0.0506,-0.013,0.0222,0.0091,-0.0086,-0.0568,-0.0085,0.0206,-0.0099,-0.012,-0.0398,-0.0032,-0.0254,0.016,-0.0732,0.0346,0.0069,-0.0046]}
{"key":"[Latent Space Oddity: Exploring Latent Spaces to Design Guitar Timbres] We introduce a novel convolutional network architecture with an interpretable latent space for modeling guitar amplifiers. Leveraging domain knowledge of popular amplifiers spanning a range of styles, the proposed system intuitively combines or subtracts characteristics of different amplifiers, allowing musicians to design entirely new guitar timbres.","layer":1,"vector":[-0.0231,-0.0157,0.0428,-0.0574,-0.0068,0.0053,0.0055,0.0165,0.0255,-0.0265,0.0357,-0.0399,0.0341,0.0436,0.0353,0.0433,0.0277,0.0501,-0.0307,0.0024,0.04,0.0106,-0.0102,-0.0435,0.0276,-0.0166,-0.0079,-0.0273,-0.0555,-0.2329,-0.0141,-0.029,0.0674,-0.0418,-0.0151,-0.0416,-0.0399,0.0014,-0.0463,0.0293,0.0517,0.0062,-0.0347,-0.0373,-0.0481,-0.0528,-0.0431,-0.0386,0.0016,-0.0339,0.0295,-0.0992,0.0349,0.0294,0.0492,0.0422,0.0548,0.053,0.0304,0.0444,0.0512,0.0303,-0.1413,0.0928,0.0053,0.0324,-0.0466,0.0011,0.0014,0.0528,-0.0019,0.0228,0.0138,0.0248,0.037,0.0065,0.0268,-0.0227,-0.0351,0.0141,0.0279,-0.0313,-0.0428,-0.0008,-0.0241,-0.0607,-0.0082,-0.0369,-0.0235,0.0098,-0.0865,-0.0357,-0.0425,-0.0071,-0.0501,-0.025,0.0251,-0.0087,-0.031,0.2271,-0.0207,0.0027,0.0732,-0.0011,0.0386,-0.0152,-0.0275,-0.0102,-0.0058,-0.0044,-0.011,-0.0071,-0.0159,-0.037,0.0283,0.005,0.0262,0.0674,0.0373,-0.0243,-0.0621,0.023,0.0283,-0.0043,0.0435,-0.0522,-0.0088,0.1369,0.0426,0.0397,0.0355,-0.0192,-0.0147,-0.0251,0.0015,0.0448,0.0267,-0.0017,0.0314,-0.0268,-0.0345,-0.0318,0.0151,-0.0616,-0.0365,0.1054,-0.0274,-0.0244,-0.0293,0.0366,-0.0378,0.034,-0.036,-0.0265,0.0718,0.0552,0.0305,0.0073,-0.0338,0.0269,-0.0196,-0.0646,-0.0342,0.1149,-0.0189,-0.0939,-0.0461,0.013,0.0073,-0.0068,0.0182,-0.0009,-0.0879,0.0543,0.0852,0.0309,-0.0715,0.0059,0.0103,-0.0064,0.0219,-0.0363,-0.0031,0.0536,0.0298,-0.0304,0.0269,-0.0661,-0.0272,0.0768,0.0171,0.0531,-0.059,0.0281,-0.0308,-0.0553,0.0094,-0.0038,-0.0092,-0.0112,-0.0151,0.0165,-0.0506,0.0201,-0.0038,0.028,0.0342,0.0174,0.0269,0.0479,-0.0549,-0.0033,0.0648,-0.0071,0.0113,-0.0581,0.0023,0.0363,-0.0042,0.0397,-0.0068,-0.0943,-0.0445,-0.2424,0.0078,0.0413,0.0115,0.0912,-0.0342,0.0197,0.0014,0.0418,0.062,0.0538,0.0109,-0.0053,-0.0128,-0.0238,0.0802,0.0096,0.007,0.0031,-0.0261,0.0245,0.0087,-0.0434,-0.0785,0.0353,-0.0053,0.2242,0.0252,0.0375,-0.0183,0.0343,0.0279,-0.0574,-0.0891,0.0392,0.0356,0.1126,0.0348,-0.0554,-0.0656,-0.0738,0.0128,0.0042,-0.0676,-0.0298,-0.027,-0.0391,0.0108,-0.0494,-0.0,0.0549,-0.0512,0.0561,-0.0254,0.0042,-0.0663,-0.1164,0.0468,-0.0456,0.0088,0.0509,-0.0551,0.0546,-0.088,0.0252,0.0074,-0.0417,-0.0381,0.0441,-0.0508,0.0076,0.0421,0.003,0.023,0.0534,-0.0479,0.0424,-0.0488,-0.044,0.0136,0.0255,-0.014,0.0369,-0.0301,0.0232,0.019,0.0776,-0.0006,0.0779,-0.0514,0.0279,0.0194,-0.0386,-0.0148,0.0055,0.0129,-0.3003,0.0361,0.0385,0.047,-0.0168,0.0085,0.0233,0.0227,-0.0248,-0.0399,-0.0093,0.0481,0.0203,-0.0006,0.011,0.0007,0.0857,-0.0512,0.0618,-0.0537,0.0298,0.0388,0.2458,-0.0247,0.0135,0.0075,-0.0029,-0.0284,0.0507,-0.0175,0.0056,0.0438,0.0839,-0.0508,0.0312,0.0292,-0.0578,0.0245,-0.0282,-0.0282,-0.0511,0.0096,-0.0678,0.0074,0.0846,-0.0334,-0.0227,-0.0227,0.0107,0.0286,0.0222,0.0135,-0.0366,0.0108,0.0205,0.0197,-0.031,-0.0141,0.0205,-0.007,-0.0063,-0.077,0.0023,0.0032,-0.0041]}
{"key":"[Disentangling group and link persistence in Dynamic Stochastic Block models] We study the inference of a model of dynamic networks in which both communities and links keep memory of previous network states. By considering maximum likelihood inference from single snapshot observations of the network, we show that link persistence makes the inference of communities harder, decreasing the detectability threshold, while community persistence tends to make it easier. We analytically show that communities inferred from single network snapshot can share a maximum overlap with the underlying communities of a specific previous instant in time. This leads to time-lagged inference: the identification of past communities rather than present ones. Finally we compute the time lag and propose a corrected algorithm, the Lagged Snapshot Dynamic (LSD) algorithm, for community detection in dynamic networks. We analytically and numerically characterize the detectability transitions of such algorithm as a function of the memory parameters of the model and we make a comparison with a full dynamic inference.","layer":7,"vector":[-0.0561,0.0067,0.0319,0.0046,0.0832,0.022,0.0582,-0.0061,0.035,-0.0274,0.0347,-0.003,0.0513,0.0696,-0.0097,0.0116,-0.0378,0.028,0.0098,-0.0243,0.001,-0.0508,0.0195,-0.0184,0.0316,0.0353,-0.0084,-0.0224,-0.0496,-0.2248,-0.0054,-0.0526,0.0553,-0.0537,0.0096,-0.0006,0.0003,0.0277,-0.0055,0.0741,0.0356,0.0474,-0.0073,-0.0574,-0.0426,-0.0053,0.0098,-0.0225,-0.0336,-0.026,0.0054,-0.0122,0.0402,0.0446,0.0287,0.0716,0.0481,0.0753,0.0581,0.0239,0.0501,0.0487,-0.1552,0.0469,0.0676,0.0041,-0.044,0.0283,0.0339,0.0156,-0.0187,0.0536,-0.0312,0.0596,0.0548,0.0474,-0.0122,0.0032,0.0025,0.0137,-0.0313,-0.0277,-0.0502,-0.0304,-0.0398,-0.067,0.0611,-0.0587,0.0288,-0.0177,-0.025,0.0266,0.0324,-0.0134,-0.0837,-0.0268,0.0466,0.0291,-0.0119,0.219,-0.0824,0.02,0.033,0.0007,0.0125,-0.0616,-0.0323,-0.0064,0.0187,0.0456,-0.0153,-0.0432,0.0383,-0.0602,0.028,-0.0135,0.0636,0.0477,-0.0133,-0.0045,-0.0193,0.0174,0.0703,-0.0618,0.0176,-0.0376,0.0153,0.1497,0.0863,0.0296,0.0039,-0.0026,-0.0579,-0.0283,-0.0283,0.0265,0.0414,-0.0389,0.0111,-0.0519,-0.0256,-0.0578,0.0252,-0.1016,-0.052,0.1094,-0.0138,0.0074,-0.0552,-0.0006,-0.0074,-0.011,-0.0659,-0.0376,0.0036,0.0174,0.0505,0.0674,-0.0445,0.0035,-0.052,-0.0483,-0.0513,0.0822,0.0192,-0.0538,-0.0057,-0.0205,-0.0127,-0.0417,0.0503,0.0226,-0.0064,0.027,0.0427,0.0034,-0.1037,0.016,0.0037,0.0267,0.0411,-0.0136,-0.0268,0.0473,0.017,-0.0255,-0.0344,-0.0161,0.0192,0.0356,-0.0397,-0.0343,-0.0218,0.0065,-0.0289,-0.0296,0.0072,-0.0578,0.022,-0.053,0.0078,-0.0022,-0.0443,-0.008,-0.0354,0.0075,-0.0152,0.0215,0.0247,-0.0136,0.0046,-0.0252,0.0291,-0.0265,-0.0032,-0.0073,0.0349,0.0042,0.0083,0.0383,0.0531,-0.0485,-0.0493,-0.2444,-0.0159,0.0206,0.0192,0.0557,-0.0629,0.0472,-0.0179,0.0481,0.0691,0.0418,0.003,0.0149,0.0249,0.0213,0.0543,0.0162,0.0313,-0.0301,0.0169,-0.0119,-0.0329,-0.069,-0.0686,0.0722,0.0135,0.2323,0.0027,0.022,-0.0068,-0.0319,0.0574,-0.0605,-0.0885,0.0413,0.0618,0.0739,-0.0105,-0.0015,-0.0287,-0.0497,0.0285,-0.0183,-0.0997,-0.048,-0.0131,0.0023,0.0132,-0.0498,-0.0338,0.0332,-0.0231,0.0641,0.0384,-0.0164,-0.0466,-0.0799,-0.0099,-0.0092,0.0503,0.0065,-0.0035,-0.0462,-0.0471,0.1244,-0.0061,-0.0665,-0.0485,0.0076,-0.0336,0.0088,0.0844,-0.0143,-0.0091,0.0622,-0.017,0.0068,-0.0798,-0.0798,-0.0453,0.0859,-0.0737,0.0368,0.0489,0.0029,-0.0152,0.0641,0.0117,0.0209,0.0098,0.0172,0.0052,-0.0679,-0.0271,-0.0227,-0.0456,-0.2964,0.0482,-0.0087,0.0293,-0.0273,0.0442,0.0643,0.0309,-0.0228,-0.0256,0.0335,0.0742,0.0599,-0.0051,-0.0311,0.0636,0.0346,-0.0598,0.0012,-0.0472,-0.0008,0.0344,0.2012,-0.0155,0.0161,0.0281,0.0028,0.0309,0.0182,-0.0165,0.0061,0.0266,0.0489,-0.0618,0.0286,0.048,-0.0365,0.0389,0.0023,-0.0127,-0.0119,0.0046,-0.0409,-0.0109,0.098,-0.0605,-0.0047,-0.0652,-0.0039,0.0705,-0.0462,-0.0216,-0.0115,0.0087,0.0366,-0.0055,-0.0308,-0.0326,-0.0098,-0.0505,-0.0222,-0.0319,-0.0004,-0.0117,0.0067]}
{"key":"[ConAML: Constrained Adversarial Machine Learning for Cyber-Physical Systems] Recent research demonstrated that the superficially well-trained machine learning (ML) models are highly vulnerable to adversarial examples. As ML techniques are becoming a popular solution for cyber-physical systems (CPSs) applications in research literatures, the security of these applications is of concern. However, current studies on adversarial machine learning (AML) mainly focus on pure cyberspace domains. The risks the adversarial examples can bring to the CPS applications have not been well investigated. In particular, due to the distributed property of data sources and the inherent physical constraints imposed by CPSs, the widely-used threat models and the state-of-the-art AML algorithms in previous cyberspace research become infeasible. We study the potential vulnerabilities of ML applied in CPSs by proposing Constrained Adversarial Machine Learning (ConAML), which generates adversarial examples that satisfy the intrinsic constraints of the physical systems. We first summarize the difference between AML in CPSs and AML in existing cyberspace systems and propose a general threat model for ConAML. We then design a best-effort search algorithm to iteratively generate adversarial examples with linear physical constraints. We evaluate our algorithms with simulations of two typical CPSs, the power grids and the water treatment system. The results show that our ConAML algorithms can effectively generate adversarial examples which significantly decrease the performance of the ML models even under practical constraints.","layer":2,"vector":[-0.0357,-0.0309,0.0545,-0.0433,0.0468,0.0153,0.0343,-0.012,-0.0033,-0.0061,0.0062,-0.0375,0.014,0.0641,0.0283,0.0166,0.0056,0.0483,-0.0288,0.0957,0.0353,-0.0078,-0.0145,-0.0535,0.0032,-0.0056,-0.0225,-0.0418,-0.0613,-0.2281,0.0016,-0.0357,0.0116,-0.0094,0.0052,-0.0292,-0.0316,0.0699,-0.0,0.0413,-0.0193,0.0291,0.0239,-0.039,-0.0264,-0.0467,-0.0133,-0.0017,0.0087,-0.0634,0.0414,-0.0552,0.0236,0.0398,0.0622,-0.0003,0.053,0.0267,0.0325,0.0344,0.013,0.0386,-0.1686,0.0424,0.0632,0.05,-0.0256,0.0179,0.0633,0.0281,0.0188,0.0496,-0.0033,0.0355,-0.018,0.0628,-0.0068,-0.0275,0.0138,-0.0051,0.0659,-0.0251,-0.0593,0.0488,-0.0873,-0.0579,0.0402,-0.019,0.0532,-0.0018,-0.0819,-0.017,0.002,0.0385,-0.0571,-0.0461,0.0391,-0.0071,-0.0724,0.1935,-0.0481,0.0018,0.0215,-0.0065,0.0523,-0.032,-0.0256,-0.0508,-0.002,-0.0179,-0.0344,0.0042,0.0183,0.0337,0.0204,-0.0047,0.0539,0.0155,-0.0283,-0.0433,-0.0501,0.0339,0.0771,0.0342,0.0647,-0.0754,0.0384,0.1739,0.0038,0.0251,0.0016,-0.0508,-0.011,-0.0202,0.0387,0.0243,0.0226,0.0265,0.0165,0.0355,-0.0362,-0.0295,0.02,-0.0817,-0.0236,0.107,-0.0346,0.0118,-0.0631,-0.0097,0.0263,0.0505,0.0041,-0.0134,0.0119,0.0127,0.0138,0.0527,-0.0181,-0.0137,-0.0331,-0.017,-0.0121,0.1536,0.0187,-0.0867,-0.0023,0.0024,0.0066,-0.0277,-0.0046,0.0493,-0.039,0.0238,0.0288,0.0087,-0.0624,-0.0335,-0.0778,0.0482,-0.0415,-0.0367,-0.0297,0.0176,0.0582,-0.0345,0.0096,-0.0268,-0.0177,0.039,-0.0587,0.0222,-0.0305,-0.0076,-0.0291,-0.0037,0.0102,0.0126,0.0215,-0.038,0.01,0.0051,-0.0645,0.0165,-0.0027,0.0202,0.0016,-0.0411,-0.0146,0.0513,-0.0438,-0.0437,0.0616,-0.0619,-0.043,0.0121,0.0056,0.0307,-0.0124,0.0439,0.0412,0.0095,-0.0668,-0.2473,-0.0129,-0.0482,-0.0231,0.0553,-0.0762,0.0294,-0.042,0.0369,0.0266,0.0919,-0.0292,-0.0433,-0.03,0.0064,0.0599,0.0244,0.0116,-0.0384,0.0291,0.0252,0.026,-0.0175,-0.0714,0.0116,0.0285,0.2113,0.0334,0.037,-0.0315,0.0239,0.0019,-0.0152,-0.097,0.0504,0.01,0.069,0.0066,-0.0281,-0.0005,-0.0169,0.0515,0.0191,-0.0932,-0.0339,-0.0573,-0.0237,0.0359,-0.0683,0.0352,0.028,0.0155,0.0355,-0.0189,-0.0017,-0.0284,-0.1162,0.0439,-0.0226,0.0335,-0.021,-0.0521,-0.0253,-0.0985,0.0669,-0.0125,-0.0539,-0.0311,0.0548,-0.0272,-0.018,0.1077,0.0466,0.0092,0.0701,-0.0182,0.0138,-0.0276,-0.0169,-0.0513,0.0274,0.0082,0.036,0.0433,0.0266,-0.0377,0.0372,0.0202,0.0251,-0.0498,-0.0301,0.0248,-0.0358,0.0096,0.052,0.0267,-0.289,0.0206,0.0428,0.0857,-0.0419,-0.0402,0.0575,0.0443,-0.0649,0.024,0.0033,0.0414,-0.015,-0.0029,0.023,0.0031,0.0515,-0.0771,0.0095,-0.0611,-0.0065,0.0428,0.2252,-0.0427,0.0265,0.043,-0.0222,0.0323,0.0192,-0.0397,0.0167,-0.003,0.0526,-0.0732,-0.0005,0.0371,-0.0355,0.0173,0.0307,-0.0314,-0.0236,0.0709,-0.0342,0.0112,0.0765,0.0047,-0.0273,-0.0429,0.0092,0.0072,-0.0459,0.0028,0.0113,0.0164,0.062,0.0079,-0.0507,-0.0718,0.0071,-0.038,0.0349,-0.0361,-0.0377,-0.041,-0.0151]}
{"key":"[Automatic Tuberculosis and COVID-19 cough classification using deep learning] We present a deep learning based automatic cough classifier which can discriminate tuberculosis (TB) coughs from COVID-19 coughs and healthy coughs. Both TB and COVID-19 are respiratory disease, have cough as a predominant symptom and claim thousands of lives each year. The cough audio recordings were collected at both indoor and outdoor settings and also uploaded using smartphones from subjects around the globe, thus contain various levels of noise. This cough data include 1.68 hours of TB coughs, 18.54 minutes of COVID-19 coughs and 1.69 hours of healthy coughs from 47 TB patients, 229 COVID-19 patients and 1498 healthy patients and were used to train and evaluate a CNN, LSTM and Resnet50. These three deep architectures were also pre-trained on 2.14 hours of sneeze, 2.91 hours of speech and 2.79 hours of noise for improved performance. The class-imbalance in our dataset was addressed by using SMOTE data balancing technique and using performance metrics such as F1-score and AUC. Our study shows that the highest F1-scores of 0.9259 and 0.8631 have been achieved from a pre-trained Resnet50 for two-class (TB vs COVID-19) and three-class (TB vs COVID-19 vs healthy) cough classification tasks, respectively. The application of deep transfer learning has improved the classifiers' performance and makes them more robust as they generalise better over the cross-validation folds. Their performances exceed the TB triage test requirements set by the world health organisation (WHO). The features producing the best performance contain higher order of MFCCs suggesting that the differences between TB and COVID-19 coughs are not perceivable by the human ear. This type of cough audio classification is non-contact, cost-effective and can easily be deployed on a smartphone, thus it can be an excellent tool for both TB and COVID-19 screening.","layer":1,"vector":[-0.0329,-0.0065,0.0135,-0.0115,0.0761,0.0034,0.069,-0.0048,-0.0143,-0.0044,-0.0135,-0.0466,0.0068,0.0214,0.0245,0.0066,0.0369,-0.0193,-0.0529,0.0312,0.0036,0.0387,0.0061,-0.0264,0.0608,-0.0215,-0.0313,-0.0923,-0.0906,-0.2112,0.052,-0.0413,-0.0047,-0.0555,0.0223,-0.0529,-0.0058,0.0912,-0.0865,0.0227,0.0174,0.0128,0.0304,-0.0977,-0.0462,-0.081,-0.0707,-0.047,-0.0031,-0.0492,0.0396,-0.0584,0.0043,0.0301,-0.0288,0.0099,0.0468,0.0394,0.0247,0.0835,0.0265,0.0315,-0.1871,0.048,0.023,0.0272,-0.0469,-0.0335,0.02,0.0578,-0.0237,0.0297,0.0328,0.0467,0.0244,0.0306,0.0075,-0.0293,-0.0264,0.0466,0.0067,0.0506,-0.0139,-0.0377,-0.0196,-0.0358,0.0208,-0.0656,-0.0149,-0.0223,-0.0469,0.0006,-0.0157,0.0606,-0.0719,0.0237,0.0324,0.0273,-0.0596,0.1937,-0.0475,-0.0182,0.0142,-0.0288,0.0542,-0.0016,-0.0307,-0.0865,-0.0248,-0.0028,-0.0003,-0.0434,0.0361,-0.0014,0.0043,0.0167,0.0676,0.0032,0.0194,0.0172,-0.0078,-0.0147,0.0543,-0.0108,0.0372,-0.0247,0.0073,0.1614,-0.023,0.0135,0.0715,-0.016,-0.067,-0.0119,0.0058,0.0025,0.011,-0.0038,0.0121,0.0006,-0.028,-0.1019,0.0396,-0.0839,-0.1172,0.0845,-0.0771,0.0402,-0.0258,-0.0597,-0.0088,0.0137,-0.0099,-0.0151,0.0708,0.0318,0.0473,0.0609,-0.0112,0.037,-0.0111,-0.0781,-0.0344,0.0963,0.0077,-0.0898,-0.0382,-0.0161,-0.0163,-0.0381,0.0506,0.0459,-0.0446,0.051,0.074,0.0388,-0.0608,-0.0303,-0.0161,-0.0149,0.0115,-0.035,0.0167,0.016,0.0535,-0.0349,0.0025,-0.0832,0.043,0.0207,-0.007,0.0116,-0.0186,0.0075,-0.0044,-0.0171,-0.001,0.0175,0.0415,0.0166,0.0247,0.0053,0.0186,0.0151,0.0085,0.0114,-0.0039,0.0156,0.0441,0.0487,0.0004,0.0196,0.0483,-0.0298,-0.0241,-0.025,-0.0056,0.0291,-0.0401,0.0573,0.0434,-0.016,-0.053,-0.2134,0.0129,0.0636,-0.0382,0.0572,-0.0417,0.0545,0.0015,0.0684,0.075,0.0867,-0.01,0.0051,0.0118,-0.0169,0.0871,0.0211,0.0159,-0.0315,-0.0252,0.0149,0.0022,-0.0044,-0.1069,0.0463,-0.0056,0.2504,-0.0095,0.0547,0.0148,-0.0147,0.001,0.0332,-0.1477,0.0412,0.0219,0.0346,-0.0004,-0.0749,-0.0201,-0.0736,0.0229,0.0395,-0.1048,-0.0265,-0.0073,-0.0042,0.0122,-0.0879,-0.0092,-0.0059,0.0086,0.038,0.0046,0.001,-0.0427,-0.0958,0.0501,-0.0436,0.0146,-0.0254,-0.0321,0.0256,-0.0546,-0.0227,-0.0223,-0.0166,-0.032,0.0257,0.004,0.0068,0.0934,-0.0432,0.0049,0.0589,0.0006,0.0793,-0.0361,-0.034,-0.0476,0.0627,-0.0014,0.0323,0.0049,0.0155,0.0179,0.0833,0.0143,0.0093,-0.0189,0.0151,0.0145,-0.0593,-0.0331,0.0346,-0.0131,-0.2639,0.0182,0.0041,0.0507,-0.0327,0.0084,-0.0073,0.0118,-0.0263,0.0187,0.0101,0.0115,0.0335,-0.0272,0.0111,0.0232,0.0746,-0.0889,0.1046,-0.038,-0.0225,0.0303,0.1887,-0.0419,0.0258,0.0281,-0.0029,0.0113,0.0199,-0.0084,0.028,-0.016,0.0945,-0.0253,-0.0199,0.0943,-0.0381,0.0158,0.0057,0.0238,-0.006,0.0219,0.0071,-0.039,0.0561,-0.0304,-0.0308,-0.0477,0.0239,0.0474,-0.0035,0.0223,-0.005,0.0157,-0.009,0.0291,-0.0033,-0.0279,-0.0015,-0.0229,0.0326,-0.0852,-0.0406,0.0076,0.0063]}
{"key":"[Sisyphus: A Cautionary Tale of Using Low-Degree Polynomial Activations in Privacy-Preserving Deep Learning] Privacy concerns in client-server machine learning have given rise to private inference (PI), where neural inference occurs directly on encrypted inputs. PI protects clients' personal data and the server's intellectual property. A common practice in PI is to use garbled circuits to compute nonlinear functions privately, namely ReLUs. However, garbled circuits suffer from high storage, bandwidth, and latency costs. To mitigate these issues, PI-friendly polynomial activation functions have been employed to replace ReLU. In this work, we ask: Is it feasible to substitute all ReLUs with low-degree polynomial activation functions for building deep, privacy-friendly neural networks? We explore this question by analyzing the challenges of substituting ReLUs with polynomials, starting with simple drop-and-replace solutions to novel, more involved replace-and-retrain strategies. We examine the limitations of each method and provide commentary on the use of polynomial activation functions for PI. We find all evaluated solutions suffer from the escaping activation problem: forward activation values inevitably begin to expand at an exponential rate away from stable regions of the polynomials, which leads to exploding values (NaNs) or poor approximations.","layer":0,"vector":[-0.0612,-0.017,-0.0132,-0.0563,-0.0033,0.0482,0.0498,0.0188,0.0735,-0.0537,0.019,-0.0408,0.0272,0.0078,0.029,0.0116,-0.004,0.0689,-0.0651,0.0223,0.0514,-0.053,-0.0077,-0.0483,-0.0008,-0.0124,-0.0387,-0.0159,-0.0551,-0.2249,0.0148,-0.0546,0.0437,-0.0202,0.0285,-0.045,-0.0356,0.043,-0.0345,0.0424,0.0271,0.0081,-0.0376,-0.0299,-0.0356,-0.0294,-0.0205,-0.0219,-0.0416,-0.0172,0.0223,0.0126,0.0222,0.0637,0.0329,-0.0008,0.0614,0.0413,0.0308,0.0603,0.0253,0.0791,-0.1411,0.0421,0.0323,0.0552,-0.0328,-0.0561,0.0316,0.0517,-0.0333,0.0653,-0.0063,0.0096,-0.0004,0.0165,0.0008,-0.0144,-0.0079,0.0267,0.0016,-0.0393,-0.0439,0.0052,-0.0512,-0.0287,-0.0027,-0.0454,0.0408,-0.0288,-0.0293,0.0081,-0.0025,0.0179,-0.0379,0.0132,-0.0013,0.0404,-0.0813,0.1923,-0.0549,0.0517,-0.016,-0.0108,0.0217,-0.0325,-0.0291,-0.018,-0.0508,-0.0012,-0.0079,-0.0788,0.0408,-0.0117,0.002,0.036,0.0523,0.0113,-0.0527,-0.0118,0.0029,0.0131,0.0049,0.0025,0.0257,-0.0767,0.0117,0.1651,-0.0171,0.088,0.0402,-0.0334,-0.0009,0.0064,0.0502,0.0421,0.0363,0.0151,0.0435,-0.0036,-0.0705,-0.0091,0.0273,-0.0231,-0.0167,0.109,-0.0399,0.0549,-0.0271,-0.0377,0.002,0.0752,-0.0187,-0.0435,0.0406,-0.0006,0.0186,0.0348,-0.0589,0.0123,-0.0168,-0.0341,-0.0275,0.1195,0.0685,-0.0789,0.0053,0.0286,-0.0029,-0.0306,0.0131,0.0104,-0.021,0.008,0.0886,0.0235,-0.0331,0.0018,-0.0067,0.0041,-0.0365,-0.0877,-0.015,0.0214,-0.0126,-0.0491,0.0067,-0.038,-0.019,0.055,-0.0859,0.0413,-0.0679,0.0175,-0.0288,-0.0274,-0.0145,-0.0198,-0.0097,-0.0255,-0.0016,-0.0258,0.0026,0.0134,-0.021,0.0462,0.0319,0.0145,0.0399,0.0301,-0.0246,-0.0041,0.0234,-0.0829,-0.0769,-0.0141,-0.0106,0.0086,0.0098,0.0105,0.0505,-0.0572,-0.0691,-0.2255,-0.0057,0.0062,-0.0316,0.1045,-0.1063,0.0554,-0.0249,0.0723,0.0537,0.0573,-0.0397,-0.0414,0.0625,-0.0036,0.0295,0.052,-0.0063,-0.0057,0.0047,-0.0091,0.0378,-0.0071,-0.099,0.0673,0.0258,0.2368,0.0196,0.0562,-0.0281,0.0135,0.0224,-0.0026,-0.1033,0.0572,0.0129,0.0643,-0.0126,0.0023,-0.0386,-0.0282,0.0209,-0.0244,-0.0992,-0.0048,0.0112,-0.0539,0.0104,-0.0571,0.0242,0.0311,-0.0159,0.0582,0.0185,-0.001,-0.0522,-0.0652,0.0065,-0.0542,0.0659,0.0116,-0.0384,-0.0171,-0.0746,0.065,-0.028,-0.0187,-0.0665,0.0842,0.0008,0.0122,0.0685,0.0321,0.0239,0.0506,0.0327,0.0346,-0.0169,-0.0815,-0.0319,0.0716,0.0081,0.0243,0.0158,0.0101,-0.0215,0.1176,0.0017,0.024,-0.0396,0.0021,-0.0125,-0.0589,-0.0134,0.0749,-0.0197,-0.2982,0.0013,-0.0135,0.0383,-0.053,0.0402,0.0511,0.0175,-0.0777,-0.0076,0.0155,0.0617,0.0555,-0.0087,0.0043,-0.0174,0.0554,-0.0559,0.009,-0.0531,0.0186,0.0317,0.1968,-0.0245,0.0308,0.0013,0.016,0.0025,0.0458,-0.0132,0.0082,0.0282,0.0374,-0.0405,0.0181,0.0452,-0.0533,0.0373,0.0617,-0.0112,-0.0122,-0.0215,-0.0665,0.0019,0.0709,-0.0314,-0.0229,-0.0186,0.0204,0.0432,-0.0152,0.0183,-0.0369,-0.0048,0.0778,0.0492,-0.0787,-0.0438,-0.0243,0.0218,-0.0191,-0.0533,-0.0118,0.0246,-0.0546]}
{"key":"[ConvPath: A Software Tool for Lung Adenocarcinoma Digital Pathological Image Analysis Aided by Convolutional Neural Network] The spatial distributions of different types of cells could reveal a cancer cell growth pattern, its relationships with the tumor microenvironment and the immune response of the body, all of which represent key hallmarks of cancer. However, manually recognizing and localizing all the cells in pathology slides are almost impossible. In this study, we developed an automated cell type classification pipeline, ConvPath, which includes nuclei segmentation, convolutional neural network-based tumor, stromal and lymphocytes classification, and extraction of tumor microenvironment related features for lung cancer pathology images. The overall classification accuracy is 92.9% and 90.1% in training and independent testing datasets, respectively. By identifying cells and classifying cell types, this pipeline can convert a pathology image into a spatial map of tumor, stromal and lymphocyte cells. From this spatial map, we can extracted features that characterize the tumor micro-environment. Based on these features, we developed an image feature-based prognostic model and validated the model in two independent cohorts. The predicted risk group serves as an independent prognostic factor, after adjusting for clinical variables that include age, gender, smoking status, and stage.","layer":3,"vector":[-0.0427,-0.0395,0.0126,0.0029,0.0444,0.0414,0.0599,0.0176,0.0258,-0.0178,0.0196,-0.0499,0.0427,0.0439,0.0161,0.0415,0.0298,0.0441,-0.0394,-0.0021,-0.0081,-0.0005,0.0088,-0.0678,0.0494,-0.0387,-0.0123,-0.0782,-0.0636,-0.2049,0.023,-0.053,0.0376,-0.0151,-0.0288,-0.0667,-0.0481,0.0451,-0.0497,-0.002,-0.0208,-0.0211,-0.0286,-0.0228,0.0239,-0.065,-0.0331,-0.0753,-0.0138,-0.0441,0.0441,-0.0525,-0.0194,0.0405,-0.0011,0.0277,0.0599,0.0927,0.0287,0.0328,0.0429,0.0316,-0.1606,0.0777,0.0593,-0.0317,-0.0605,-0.0134,0.0442,0.0285,-0.0183,0.0663,0.0292,0.0674,0.0008,-0.0054,-0.0,-0.0255,0.0142,0.0387,0.0266,0.0252,-0.0615,-0.0102,-0.018,-0.0626,-0.0351,-0.0754,0.029,0.0011,-0.0323,0.0093,-0.052,0.0528,-0.0689,-0.0028,0.0659,0.0039,-0.0066,0.1842,-0.0301,-0.002,0.0107,-0.0297,0.0058,-0.0074,-0.0347,-0.0468,-0.0367,0.0119,-0.0029,-0.0075,0.0141,-0.0171,-0.0045,-0.0233,0.0575,0.0421,0.0001,-0.0116,-0.0072,-0.0348,0.0349,-0.0259,0.0555,-0.0445,0.0292,0.1297,0.0352,0.0107,0.0794,0.0401,-0.0398,0.0107,-0.029,0.0224,0.0436,0.0144,-0.0587,-0.0308,-0.0422,-0.0541,0.0199,-0.0719,-0.0264,0.1001,-0.0785,0.0018,-0.027,-0.0482,-0.003,0.022,-0.0418,-0.0339,0.014,0.0137,-0.0168,0.043,-0.0392,-0.0064,-0.0014,-0.0677,-0.0192,0.1378,0.0143,-0.0624,-0.0329,-0.0204,0.0285,0.0112,0.0429,-0.0006,-0.0248,-0.0036,0.0585,0.0408,-0.0743,-0.0282,-0.0256,0.0314,0.0052,-0.0281,-0.0301,-0.0058,0.0582,-0.0245,-0.0096,-0.0565,0.0715,0.088,-0.0274,0.0614,-0.065,-0.0163,-0.007,-0.06,-0.0397,-0.0255,0.0118,-0.0452,0.0331,-0.0077,-0.0304,0.0211,-0.0249,0.0083,-0.0332,0.0234,0.0181,-0.0188,-0.0808,-0.037,0.0369,0.0082,-0.0101,-0.0153,0.0223,0.0031,0.0443,0.0622,0.0296,0.0019,-0.0795,-0.2078,-0.0089,0.0205,-0.0251,0.0054,-0.0526,0.0176,0.0259,0.0782,0.0425,0.0873,0.0379,0.0319,0.0177,0.0311,0.0535,0.0551,0.02,-0.0471,-0.0213,0.0065,-0.001,0.006,-0.0784,0.0705,0.0188,0.217,0.0397,0.0298,0.0468,0.0191,0.0106,-0.0437,-0.1066,0.0759,-0.0085,0.0112,-0.0133,-0.0758,-0.0119,-0.0345,0.0058,0.0086,-0.092,-0.0359,-0.0254,0.0105,0.1137,-0.0706,-0.0122,0.0308,-0.0592,0.0446,-0.0037,0.0234,0.0314,-0.0801,0.0824,-0.043,0.0099,-0.0187,-0.0472,0.0106,-0.0625,0.0454,-0.0175,-0.0633,-0.0653,0.0105,-0.0631,0.0141,0.0922,0.0111,-0.0217,0.104,-0.0039,0.0455,0.004,-0.0513,-0.0369,0.054,-0.0355,0.0299,0.0457,0.0291,0.0238,0.0729,-0.0122,0.0026,-0.0419,0.0141,0.0079,-0.0582,-0.0027,0.0184,0.0141,-0.2781,0.0674,0.0365,0.0225,-0.0249,-0.0186,0.0783,0.0287,-0.031,0.0009,0.0454,0.0056,0.0655,-0.0725,0.0331,-0.0023,0.0484,-0.0906,0.083,-0.0368,0.0043,0.0118,0.2128,-0.0629,0.0376,0.0212,-0.0321,0.0086,0.0195,-0.006,0.071,0.0126,0.0719,-0.0677,0.0562,0.0807,-0.0238,0.0755,0.0037,-0.0237,0.0201,0.0356,-0.0167,-0.0139,0.0643,-0.0469,-0.0436,-0.0237,0.0287,-0.009,-0.0394,0.0211,-0.0343,0.0547,0.0118,0.0108,-0.0184,-0.0552,-0.0221,-0.0025,0.038,-0.0589,-0.0305,0.051,-0.0087]}
{"key":"[Fine-Tuning Pretrained Language Models With Label Attention for Biomedical Text Classification] The massive scale and growth of textual biomedical data have made its indexing and classification increasingly important. However, existing research on this topic mainly utilized convolutional and recurrent neural networks, which generally achieve inferior performance than the novel transformers. On the other hand, systems that apply transformers only focus on the target documents, overlooking the rich semantic information that label descriptions contain. To address this gap, we develop a transformer-based biomedical text classifier that considers label information. The system achieves this with a label attention module incorporated into the fine-tuning process of pretrained language models (PTMs). Our results on two public medical datasets show that the proposed fine-tuning scheme outperforms the vanilla PTMs and state-of-the-art models.","layer":1,"vector":[-0.034,0.0097,-0.0055,-0.0138,0.0017,-0.0127,0.0276,0.082,0.0454,-0.0022,-0.0258,-0.0513,0.0196,0.0547,0.0185,0.0301,0.04,0.0211,-0.0623,0.0022,0.0378,0.021,0.0369,-0.0099,0.0163,0.0204,-0.0033,-0.0189,-0.0584,-0.2155,-0.0004,-0.0605,0.0476,-0.0174,-0.016,-0.016,-0.0589,0.0225,-0.0207,0.0432,-0.0095,0.0094,-0.0207,-0.0127,-0.0059,-0.0669,-0.0401,-0.0373,-0.0331,-0.0654,0.0126,-0.0449,0.0235,0.0361,0.021,0.0231,0.0348,-0.0093,0.0723,0.0515,0.0326,0.0635,-0.1727,0.0937,0.0311,0.0029,-0.0804,-0.0143,-0.0009,0.0597,-0.0071,0.0238,0.0408,0.0224,0.001,-0.0164,0.0177,-0.0285,0.0581,0.0279,0.0494,-0.0253,-0.0355,-0.0721,0.0111,-0.0661,0.0053,-0.0823,0.0046,0.0163,-0.0362,-0.0217,-0.0367,0.0575,-0.0771,-0.0335,0.0393,0.0195,-0.0742,0.188,-0.0586,0.0057,0.0178,-0.059,0.0571,-0.0242,-0.0177,0.0225,-0.0284,0.0055,-0.0144,-0.0007,0.0162,-0.0212,0.0489,0.0178,0.1141,0.0309,0.0108,0.0005,-0.0109,-0.0064,0.0353,-0.0266,0.0455,-0.0328,0.0649,0.1394,0.0374,0.0159,0.0744,0.0226,-0.0451,-0.0303,0.0005,0.0166,0.0057,-0.0465,0.0187,-0.0111,-0.0044,-0.0512,-0.0272,-0.073,-0.0592,0.1032,-0.0388,-0.0176,-0.0384,-0.0638,0.0014,0.0071,-0.0155,-0.0129,0.0382,0.0208,0.0631,0.0167,-0.0434,-0.0173,0.0312,-0.0473,-0.0631,0.1013,0.0129,-0.0588,-0.0352,-0.0438,-0.0199,-0.0112,0.0734,0.0267,0.0045,0.058,0.0494,0.0818,-0.0356,-0.0033,-0.0134,-0.0171,0.065,-0.0264,0.0032,0.0236,0.0223,-0.0545,-0.0042,-0.0778,0.0517,0.0439,-0.0177,0.0525,0.0044,-0.0427,-0.0112,-0.0257,-0.037,-0.0045,0.0087,-0.0632,0.0035,0.0178,0.0073,0.0155,-0.003,-0.0258,-0.0553,0.0291,0.0252,0.0055,-0.0321,0.0405,0.0614,-0.0159,-0.0516,-0.0259,-0.004,0.0414,-0.0154,0.047,0.0307,-0.0204,-0.0332,-0.2237,-0.0138,0.0428,-0.0249,0.0379,-0.0377,0.0158,0.0136,0.0635,0.0773,0.0365,0.0072,-0.0881,-0.0058,-0.0398,0.0562,0.0423,0.0057,-0.0109,-0.0,0.05,0.0117,0.0377,-0.0833,0.0615,-0.0106,0.2215,0.0297,0.0248,-0.0527,0.0442,0.0255,-0.0573,-0.1017,0.0875,0.0054,0.0115,0.0096,-0.0471,0.0045,-0.0205,0.0078,0.0072,-0.1151,-0.0298,-0.0247,-0.0075,-0.0083,-0.0628,0.0547,0.0284,-0.0267,0.0466,0.0244,-0.0122,-0.0496,-0.0977,-0.0033,-0.0906,-0.0179,0.0043,-0.0346,0.0289,-0.0568,-0.0305,0.0219,-0.0647,-0.017,0.0307,-0.0563,-0.0571,0.0836,0.0152,0.0179,0.0502,0.0639,-0.0063,-0.0328,-0.0496,-0.0173,0.0517,0.0201,0.0472,-0.0091,0.0173,-0.009,0.1088,0.0126,0.0315,0.0131,0.0082,0.0582,-0.0335,-0.0122,0.0069,-0.032,-0.3034,0.0447,0.0282,0.0217,0.0035,0.0222,0.0358,0.0269,-0.0185,0.0231,-0.0158,-0.0083,0.0563,-0.0177,-0.0552,0.0392,0.0812,-0.0455,0.053,-0.0418,0.0294,-0.0067,0.1914,-0.0005,0.0118,-0.0322,-0.0427,0.0236,0.017,0.0062,0.0488,0.0025,0.0941,-0.0279,0.097,0.057,-0.0269,0.0224,0.0249,-0.0042,0.015,0.0309,-0.0729,-0.0726,0.0762,0.0185,-0.0106,-0.0459,-0.0328,-0.0129,-0.028,0.0513,-0.0095,0.0098,0.0684,0.0341,-0.038,-0.0468,-0.033,-0.0682,-0.0203,-0.0937,-0.0546,0.0463,-0.031]}
{"key":"[Learning Dynamical Systems from Noisy Sensor Measurements using Multiple Shooting] Modeling dynamical systems plays a crucial role in capturing and understanding complex physical phenomena. When physical models are not sufficiently accurate or hardly describable by analytical formulas, one can use generic function approximators such as neural networks to capture the system dynamics directly from sensor measurements. As for now, current methods to learn the parameters of these neural networks are highly sensitive to the inherent instability of most dynamical systems of interest, which in turn prevents the study of very long sequences. In this work, we introduce a generic and scalable method based on multiple shooting to learn latent representations of indirectly observed dynamical systems. We achieve state-of-the-art performances on systems observed directly from raw images. Further, we demonstrate that our method is robust to noisy measurements and can handle complex dynamical systems, such as chaotic ones.","layer":1,"vector":[-0.0588,-0.0163,0.0347,-0.0505,0.0228,0.0352,0.0294,-0.0175,0.0213,-0.0201,0.0125,-0.0961,0.0569,0.0679,0.0149,-0.0129,0.0016,0.0388,-0.0229,0.0076,0.0347,-0.0162,0.0175,-0.0203,-0.0075,-0.0027,-0.0145,-0.0473,-0.056,-0.2443,-0.0064,-0.0572,0.011,-0.0613,0.0065,-0.0598,-0.0411,0.04,-0.005,0.0354,0.02,0.0511,0.0444,-0.0847,-0.0081,-0.0666,0.0223,-0.0345,-0.0116,-0.0474,0.0153,-0.0168,0.0219,0.0214,0.0361,0.0389,0.0555,0.0803,0.0629,0.0517,0.0555,0.0368,-0.1787,0.0434,0.0483,0.0241,-0.0151,-0.0154,0.0774,0.0321,-0.0088,0.048,0.0053,0.0515,0.0126,-0.0124,-0.0132,-0.0676,-0.029,0.005,0.0109,-0.0181,0.0063,-0.0367,-0.015,-0.0227,0.0203,-0.0409,0.0435,0.0109,-0.069,-0.0415,-0.045,0.0031,-0.0506,-0.0098,0.0532,0.0411,0.0049,0.203,-0.0288,-0.0095,0.078,-0.0167,0.0258,-0.0474,-0.0623,-0.0414,-0.0543,0.0263,-0.001,-0.0148,0.0446,-0.0465,0.036,-0.0119,0.0661,0.0209,0.028,-0.0017,0.0018,-0.024,0.035,-0.0202,0.0398,-0.0684,0.0068,0.1725,0.0472,0.0506,0.0312,-0.0218,-0.0304,-0.0605,0.0191,0.0585,0.0313,-0.0278,0.0436,0.0187,-0.0514,-0.0681,0.0472,-0.1088,-0.062,0.0903,-0.0047,0.0397,-0.0366,-0.02,-0.0495,0.0243,-0.0089,0.0101,0.0209,0.0544,0.0473,0.0263,-0.049,0.0217,-0.0753,-0.0488,-0.025,0.0766,0.0067,-0.0465,0.0436,-0.0121,0.0084,-0.048,0.0488,0.0059,-0.0504,-0.0197,0.0771,0.0182,-0.049,-0.0176,-0.0089,0.0582,0.0121,-0.0622,-0.0268,0.0616,0.04,-0.0444,0.0015,-0.0262,0.0194,0.0548,-0.0003,-0.016,-0.0086,0.0174,-0.0167,-0.0029,-0.0179,-0.015,-0.0224,-0.0656,0.0349,-0.0242,-0.0239,0.0675,-0.0012,-0.0028,-0.0129,0.0124,-0.0091,0.0019,-0.0017,0.0067,0.0436,-0.059,-0.0355,0.0139,0.0079,0.0474,-0.0162,0.019,0.0285,-0.0499,-0.0616,-0.2661,-0.003,-0.0073,-0.0583,0.0819,-0.0597,0.0204,-0.0402,0.0637,0.0483,0.0578,0.0009,-0.0026,0.0263,0.0021,0.0672,0.0042,0.0603,-0.032,-0.0269,-0.0185,0.019,-0.0188,-0.0974,0.0615,0.0174,0.2458,0.0211,0.0419,-0.0274,0.0116,0.0212,-0.0276,-0.015,0.0638,-0.0152,0.0917,-0.0161,-0.0262,-0.0455,-0.0298,-0.0202,-0.0025,-0.0534,-0.0066,-0.0263,-0.036,0.0199,-0.0552,-0.0059,0.0764,-0.0167,0.0179,-0.0618,-0.0082,-0.0203,-0.0921,0.0686,-0.011,0.0449,-0.0337,-0.0466,-0.026,-0.0403,0.0757,0.0009,-0.0367,-0.0672,0.0284,-0.0224,-0.0003,0.1075,0.0103,-0.0236,0.0837,0.0022,0.0087,-0.0378,-0.0511,0.0071,0.0529,-0.0513,-0.0042,0.0546,0.0444,-0.0363,0.0334,-0.0213,0.0273,-0.0147,-0.013,0.0256,-0.0414,0.0006,0.0295,0.0003,-0.2892,0.0182,0.0377,0.0254,-0.0065,-0.0159,0.019,-0.0089,-0.0157,0.013,-0.0279,0.039,0.0385,0.0118,0.0248,0.0234,0.012,-0.061,0.0756,-0.0886,0.0404,0.0606,0.195,-0.047,0.0347,0.0322,0.0223,0.0029,-0.0061,-0.0294,0.0341,0.0027,0.0272,-0.0516,0.0279,0.0787,-0.0024,0.0238,-0.0235,0.0297,0.0154,0.0137,-0.0091,-0.0395,0.0684,-0.0166,-0.0125,-0.0265,-0.0334,0.0392,-0.0184,0.0484,-0.0236,-0.0022,0.0382,0.031,-0.0291,-0.0355,-0.0371,-0.0217,-0.0037,-0.0813,0.0152,-0.0476,-0.0072]}
{"key":"[Detecting organized eCommerce fraud using scalable categorical clustering] Online retail, eCommerce, frequently falls victim to fraud conducted by malicious customers (fraudsters) who obtain goods or services through deception. Fraud coordinated by groups of professional fraudsters that place several fraudulent orders to maximize their gain is referred to as organized fraud. Existing approaches to fraud detection typically analyze orders in isolation and they are not effective at identifying groups of fraudulent orders linked to organized fraud. These also wrongly identify many legitimate orders as fraud, which hinders their usage for automated fraud cancellation. We introduce a novel solution to detect organized fraud by analyzing orders in bulk. Our approach is based on clustering and aims to group together fraudulent orders placed by the same group of fraudsters. It selectively uses two existing techniques, agglomerative clustering and sampling to recursively group orders into small clusters in a reasonable amount of time. We assess our clustering technique on real-world orders placed on the Zalando website, the largest online apparel retailer in Europe1. Our clustering processes 100,000s of orders in a few hours and groups 35-45% of fraudulent orders together. We propose a simple technique built on top of our clustering that detects 26.2% of fraud while raising false alarms for only 0.1% of legitimate orders.","layer":2,"vector":[-0.0105,-0.0275,0.0103,-0.0036,0.0464,0.0156,0.0676,0.0107,0.0125,-0.0173,0.0431,-0.0428,-0.0204,0.0718,0.0194,-0.0147,0.0358,0.0157,-0.0397,-0.0076,0.0062,-0.0416,-0.0595,-0.0502,0.0338,0.033,-0.0311,-0.0406,-0.0963,-0.2212,0.0023,-0.0337,0.0622,-0.0203,0.0404,-0.0232,-0.0213,0.0426,-0.0619,-0.0028,0.0041,0.0267,-0.0696,-0.0369,-0.0285,-0.0305,-0.0057,0.0267,-0.0313,-0.0373,0.0184,-0.0395,-0.0108,0.0465,0.0176,0.0295,0.0407,0.0223,0.0847,0.022,0.0675,0.01,-0.1256,0.0491,0.0342,0.0352,-0.0275,-0.0038,0.0171,0.0388,-0.0,0.0543,-0.0308,0.0576,0.0371,0.0286,0.0008,-0.0181,-0.0067,0.0226,-0.0537,-0.0596,-0.0023,-0.0182,-0.0104,-0.0403,0.0114,-0.0386,0.0959,-0.0069,0.0085,0.0397,-0.0218,0.0117,-0.0812,-0.0416,-0.0205,0.0109,-0.0178,0.2138,-0.0261,0.0241,0.0613,-0.0405,0.0016,-0.0703,-0.0188,-0.0093,0.0053,-0.0115,-0.0405,-0.031,0.0246,-0.0569,0.005,0.0473,0.0485,0.0011,0.0192,-0.0012,0.0072,0.013,0.0714,0.0059,0.0381,-0.0648,0.0447,0.1654,0.0571,0.0336,-0.0024,-0.0032,-0.0478,0.0135,0.0259,0.0258,-0.0128,0.0423,0.0209,0.002,-0.0849,-0.0716,0.005,-0.086,-0.0126,0.1185,-0.0199,0.0057,-0.0467,-0.0117,-0.0428,0.0058,-0.055,-0.0651,-0.0247,0.0014,0.0851,0.0441,-0.0243,-0.0227,-0.013,-0.0191,-0.0093,0.1419,0.0247,-0.1039,0.0159,-0.0123,0.0,-0.0227,0.0124,0.032,-0.0537,0.0593,0.0564,-0.0295,-0.0397,0.0094,0.0092,0.0141,0.0581,-0.0014,-0.0635,0.0493,0.0744,-0.0494,-0.0093,-0.0096,0.004,0.0459,-0.0541,0.0307,-0.0462,-0.033,-0.0243,-0.0488,-0.0267,0.0003,0.0143,0.0083,0.0557,0.0107,-0.0738,0.0188,0.0195,0.0668,-0.0024,-0.0313,0.0368,0.0233,0.0001,0.026,0.0443,-0.0176,0.0039,-0.0126,0.0205,0.0723,-0.0115,0.0347,0.0546,-0.052,-0.044,-0.2216,-0.0144,0.0318,-0.0399,0.0039,-0.0505,0.044,-0.0195,0.06,0.0747,0.0735,-0.0045,-0.0185,0.0543,-0.0173,0.0685,0.0213,0.0285,-0.0384,0.0197,-0.0242,0.0237,-0.0494,-0.0673,0.0081,0.0001,0.2433,0.0598,-0.0176,-0.0242,0.0453,0.0161,-0.0274,-0.0674,0.0498,0.0395,0.0128,-0.0301,0.0129,0.0247,-0.0599,0.0034,0.0047,-0.0569,-0.0251,-0.0219,-0.0463,0.0594,-0.0266,0.0302,0.032,-0.0429,0.0592,0.0273,0.0014,-0.0683,-0.0433,0.015,-0.0339,0.014,0.0277,-0.0521,0.0004,-0.0409,0.0837,-0.0391,-0.048,-0.0076,0.031,-0.0331,-0.0172,0.1167,-0.0024,-0.0498,0.0506,-0.008,0.01,-0.0507,-0.016,-0.0358,0.0661,-0.0082,0.0053,0.0321,-0.0029,0.0053,0.0503,0.0048,0.0483,-0.0063,0.0008,0.0164,-0.0793,0.0155,0.0117,0.0188,-0.2765,0.0091,-0.0489,0.0574,0.0124,0.004,0.0065,0.0187,-0.0011,-0.0482,0.0005,0.0452,0.0563,-0.0394,0.01,0.0451,0.0309,-0.0635,0.043,-0.0292,0.0002,0.0262,0.2508,-0.0271,-0.0242,0.0247,0.0227,-0.019,0.0153,-0.017,-0.0042,0.0142,0.0977,-0.0349,0.0304,0.011,-0.0383,0.0549,0.0332,-0.0514,-0.0743,-0.0187,-0.0676,-0.0198,0.091,-0.0321,-0.0323,-0.0799,0.0407,0.0849,-0.0561,-0.0289,-0.0621,-0.0151,0.0019,0.0501,-0.0688,-0.0278,-0.0605,-0.0357,-0.0299,-0.0545,-0.0218,0.0111,0.0099]}
{"key":"[MIDAS: Microcluster-Based Detector of Anomalies in Edge Streams] Given a stream of graph edges from a dynamic graph, how can we assign anomaly scores to edges in an online manner, for the purpose of detecting unusual behavior, using constant time and memory? Existing approaches aim to detect individually surprising edges. In this work, we propose MIDAS, which focuses on detecting microcluster anomalies, or suddenly arriving groups of suspiciously similar edges, such as lockstep behavior, including denial of service attacks in network traffic data. MIDAS has the following properties: (a) it detects microcluster anomalies while providing theoretical guarantees about its false positive probability; (b) it is online, thus processing each edge in constant time and constant memory, and also processes the data 162-644 times faster than state-of-the-art approaches; (c) it provides 42%-48% higher accuracy (in terms of AUC) than state-of-the-art approaches.","layer":0,"vector":[-0.074,-0.0285,0.0287,-0.0179,0.0389,0.0039,0.0485,0.0269,0.0159,-0.0116,0.0424,-0.0392,0.0034,0.023,-0.0295,0.0111,-0.0207,0.014,-0.006,0.0104,0.0047,-0.0115,-0.003,-0.0669,0.0318,0.0641,-0.0249,-0.0489,-0.0883,-0.2226,0.0025,-0.0549,0.0305,-0.0671,0.0262,-0.0651,-0.0116,0.0521,0.0037,0.014,-0.0053,0.0407,-0.0269,-0.0617,-0.0484,-0.0324,0.0121,0.0224,-0.0012,-0.045,-0.0084,-0.0321,0.04,0.0345,0.0317,0.0113,0.0466,0.0204,0.07,0.0522,0.0395,0.0358,-0.1262,0.0418,0.077,-0.0095,-0.0181,-0.0096,0.0022,0.0331,-0.0052,0.0614,-0.0022,0.058,0.0167,-0.0043,-0.0196,-0.0156,-0.038,0.0207,-0.0397,-0.0619,-0.003,0.0077,-0.0185,-0.0374,-0.0181,-0.0278,0.0753,0.0164,-0.0453,0.0046,0.0234,0.0355,-0.0479,-0.0042,0.0157,0.0389,-0.03,0.2073,-0.0413,0.0196,-0.0007,-0.0093,0.0684,-0.0123,0.0112,-0.0621,-0.0005,-0.0059,0.0291,-0.0764,0.0784,-0.0564,0.0205,-0.0333,0.0762,0.0523,-0.022,0.0273,-0.0447,0.009,0.034,-0.0538,0.0487,-0.0644,0.0233,0.1062,0.0068,0.0251,-0.0327,0.0105,-0.0151,0.0084,-0.0115,-0.0158,-0.027,-0.0036,0.0085,-0.0502,-0.0539,-0.0299,0.0278,-0.0633,-0.0444,0.1314,-0.017,0.0203,-0.0155,-0.0378,-0.0242,0.0193,-0.0746,-0.0458,0.0048,0.0177,0.0613,0.0742,-0.0355,0.0428,-0.018,-0.0229,-0.0353,0.129,0.0271,-0.0977,0.0037,-0.0302,0.0142,-0.0167,0.0173,0.0419,-0.0294,-0.0044,-0.0074,0.0129,-0.0617,-0.0124,-0.021,0.0037,0.0314,-0.0391,-0.0653,0.0494,0.0441,-0.0374,-0.0057,-0.0078,0.0494,0.0121,-0.0823,0.0083,-0.0302,-0.0339,-0.0218,-0.0198,-0.0065,-0.0457,0.0054,-0.0653,0.0823,0.0025,-0.045,0.0092,-0.0331,0.0229,-0.0371,-0.007,0.0274,0.0211,0.0044,-0.0032,0.0061,-0.0185,0.0113,-0.0114,0.0456,0.0759,-0.0036,0.0018,0.0114,-0.0172,-0.0399,-0.2518,-0.0297,0.0038,-0.0036,0.0517,-0.0323,0.0367,-0.0089,0.066,0.0517,0.041,-0.0077,-0.0249,-0.0072,-0.016,0.0908,0.0279,0.0099,-0.0259,0.0454,-0.0192,0.0022,-0.0552,-0.0833,0.0222,0.0273,0.2261,0.0607,0.0152,-0.0373,-0.0064,-0.0082,-0.0234,-0.0637,0.0731,0.0471,0.0553,0.0119,-0.0137,-0.026,-0.0419,0.0275,-0.0034,-0.0657,-0.0236,-0.0468,-0.0457,0.0352,-0.0395,-0.0255,0.0594,-0.0135,0.0866,0.051,0.0381,-0.0522,-0.0499,0.0345,-0.0282,0.0324,0.0062,-0.0719,0.0389,-0.0852,0.0911,0.015,-0.0499,-0.0544,0.0153,-0.0108,-0.0028,0.1212,0.0399,-0.0244,0.0437,0.0456,0.0661,-0.0683,-0.0447,-0.0259,0.0388,-0.0583,0.0056,0.0075,0.007,-0.007,0.0445,0.0302,0.0596,0.0002,0.058,-0.0045,-0.0823,-0.0365,0.025,-0.0053,-0.3247,0.0339,-0.0011,0.0258,-0.0215,0.0028,0.038,0.0753,-0.0534,-0.0027,0.0118,0.0479,0.018,-0.0066,0.012,0.0573,0.0562,-0.0349,-0.0241,0.0152,0.0355,0.0355,0.2412,-0.0134,0.0134,0.0354,0.0122,-0.009,0.0291,-0.0029,0.0094,-0.0139,0.0748,-0.0445,0.044,0.0239,-0.0543,0.0342,0.027,-0.0233,0.017,-0.0243,-0.051,-0.0262,0.0943,-0.0358,-0.0351,-0.0428,0.0536,0.0506,-0.0211,-0.0148,-0.0181,-0.0287,0.0244,0.0652,-0.0766,-0.032,-0.0726,-0.0269,0.0161,-0.0212,0.02,-0.0077,0.0065]}
{"key":"[Communication Lower Bounds for Distributed Convex Optimization: Partition Data on Features] Recently, there has been an increasing interest in designing distributed convex optimization algorithms under the setting where the data matrix is partitioned on features. Algorithms under this setting sometimes have many advantages over those under the setting where data is partitioned on samples, especially when the number of features is huge. Therefore, it is important to understand the inherent limitations of these optimization problems. In this paper, with certain restrictions on the communication allowed in the procedures, we develop tight lower bounds on communication rounds for a broad class of non-incremental algorithms under this setting. We also provide a lower bound on communication rounds for a class of (randomized) incremental algorithms.","layer":4,"vector":[-0.0508,-0.0051,0.051,-0.015,-0.003,0.0564,0.0148,0.0389,0.039,-0.0323,0.0462,-0.0323,0.0028,0.0567,0.0083,0.0615,0.0245,0.0236,-0.009,-0.0064,0.0131,-0.0963,-0.0437,-0.0453,0.0566,-0.0147,-0.0422,-0.0602,-0.0399,-0.2576,0.033,-0.0227,0.0486,-0.0096,0.0109,-0.0141,0.0223,0.0683,-0.0981,0.0486,0.0048,0.0576,-0.0496,-0.0408,-0.0124,-0.0383,0.0002,-0.0281,-0.0286,-0.0535,0.0119,-0.0593,-0.0012,0.0407,0.0341,0.075,0.0196,0.0437,0.0214,0.0609,0.0096,0.0397,-0.1568,0.0544,0.0496,0.0077,-0.0277,-0.0287,-0.0047,0.0861,0.0277,0.0346,0.0651,0.0247,0.0474,-0.0259,0.002,-0.044,0.0278,0.0049,0.0142,-0.0531,-0.0312,0.0332,-0.0614,-0.0416,0.0608,-0.0218,0.0119,0.0117,-0.036,0.0012,-0.0158,-0.0134,-0.0904,-0.0137,0.0276,0.0044,-0.0471,0.1941,-0.0916,0.0321,0.0209,-0.0092,0.0153,-0.0375,-0.0296,-0.0479,-0.0127,0.041,-0.0179,-0.0249,0.026,-0.0119,0.0036,0.0097,0.0541,0.0529,-0.0036,-0.0079,-0.0576,0.0338,0.0616,-0.0126,0.0453,-0.0442,0.0042,0.0972,0.0205,0.0528,0.0564,-0.0077,-0.0016,-0.027,0.0378,0.0092,0.0092,-0.0028,0.0164,-0.0072,-0.0523,-0.0751,0.0238,-0.0994,0.0023,0.1922,-0.0122,0.0322,-0.0326,-0.0789,0.0165,-0.0068,0.0045,-0.0245,-0.0105,-0.0194,0.0267,0.0337,-0.0674,0.0262,-0.0229,-0.0305,0.0255,0.1221,-0.0109,-0.0765,-0.0045,0.0233,0.0471,-0.0305,0.0408,0.0584,-0.0057,0.0453,0.0755,0.0523,-0.0951,0.0003,-0.0092,-0.0062,0.0283,0.0084,-0.0226,0.0071,0.0093,-0.0559,0.0276,-0.0204,0.0386,0.0182,-0.0456,-0.0217,-0.0425,0.0376,-0.0336,-0.0549,0.0091,-0.0024,-0.0017,0.0006,0.0274,0.0111,-0.0341,0.0567,0.0151,-0.0184,-0.0316,-0.0082,0.0245,0.0471,-0.0578,-0.0198,0.0637,-0.0022,-0.0438,0.0106,0.0489,0.0251,-0.0024,0.056,0.042,0.0097,-0.0794,-0.2188,-0.0317,-0.008,-0.0215,0.0215,-0.0767,0.0458,-0.0001,0.0445,0.0612,0.0948,-0.025,-0.0517,0.028,-0.0122,0.0292,0.0264,0.0355,-0.0126,0.0089,0.0005,0.0037,-0.0149,-0.0298,0.0565,-0.0153,0.1967,-0.0136,0.0399,-0.0227,0.0196,0.0504,-0.0071,-0.0682,-0.005,0.0382,0.0231,0.0144,-0.0425,-0.0117,-0.0023,0.0316,0.02,-0.088,-0.0401,-0.0659,-0.0467,0.0174,-0.0868,0.0055,0.0361,0.0003,0.0262,-0.0088,0.0477,-0.0308,-0.1141,0.0087,-0.0206,0.0242,-0.0121,-0.0515,-0.0181,-0.0409,0.0708,-0.0201,0.0094,-0.0255,-0.0097,-0.0079,-0.0166,0.0916,0.0005,0.0082,0.0255,0.0225,0.0566,0.0059,-0.0333,-0.0139,0.0666,-0.0933,0.0239,0.0059,0.0244,0.0055,0.0537,0.0202,-0.01,-0.0026,-0.0069,0.0294,-0.0405,0.0274,0.0379,-0.0076,-0.3049,-0.021,0.0067,-0.0351,-0.0501,0.0241,0.0677,0.0196,-0.0843,-0.0076,0.0461,0.0488,-0.0008,0.0044,0.0234,0.0425,0.0439,-0.0189,0.0282,-0.0838,0.0055,0.0276,0.2206,-0.0739,0.0226,0.0269,-0.0519,-0.0085,-0.0218,-0.0317,0.0035,-0.0167,0.0782,-0.0688,0.009,0.1002,-0.0363,0.0356,0.0113,-0.006,-0.0433,0.0204,0.0063,0.0157,0.0929,-0.0026,-0.0369,-0.0466,0.0142,-0.0064,-0.0337,0.0285,-0.0016,-0.0095,0.0167,0.0144,-0.047,-0.0762,-0.036,-0.0056,0.0405,-0.0597,-0.0639,-0.0618,0.0263]}
{"key":"[Conversation Group Detection With Spatio-Temporal Context] In this work, we propose an approach for detecting conversation groups in social scenarios like cocktail parties and networking events, from overhead camera recordings. We posit the detection of conversation groups as a learning problem that could benefit from leveraging the spatial context of the surroundings, and the inherent temporal context in interpersonal dynamics which is reflected in the temporal dynamics in human behavior signals, an aspect that has not been addressed in recent prior works. This motivates our approach which consists of a dynamic LSTM-based deep learning model that predicts continuous pairwise affinity values indicating how likely two people are in the same conversation group. These affinity values are also continuous in time, since relationships and group membership do not occur instantaneously, even though the ground truths of group membership are binary. Using the predicted affinity values, we apply a graph clustering method based on Dominant Set extraction to identify the conversation groups. We benchmark the proposed method against established methods on multiple social interaction datasets. Our results showed that the proposed method improves group detection performance in data that has more temporal granularity in conversation group labels. Additionally, we provide an analysis in the predicted affinity values in relation to the conversation group detection. Finally, we demonstrate the usability of the predicted affinity values in a forecasting framework to predict group membership for a given forecast horizon.","layer":1,"vector":[-0.0195,0.0269,0.0021,-0.036,0.0526,-0.0087,0.075,-0.0318,0.0177,-0.0487,0.0108,-0.0232,0.0478,0.0477,-0.006,-0.0019,0.0128,0.0119,-0.0391,-0.0064,-0.0171,-0.0631,-0.0218,-0.0244,0.0373,0.0433,-0.0202,-0.0803,-0.0506,-0.1766,0.0195,-0.0421,0.0607,-0.0509,-0.0113,-0.0396,0.0152,0.0044,-0.0465,0.0477,-0.0214,0.0015,-0.018,-0.0798,-0.0487,-0.022,-0.0446,-0.0404,-0.0711,-0.0682,0.0294,-0.055,0.0506,-0.0061,0.0006,0.0549,0.0616,0.021,0.0576,0.0417,0.0408,0.0032,-0.1745,0.0634,0.0304,0.027,-0.0276,0.066,0.0305,0.0132,-0.0007,0.0749,0.0314,0.0106,0.0371,0.0411,-0.0126,-0.037,-0.0105,0.0247,0.01,0.0264,-0.0328,-0.0276,-0.0011,-0.0774,0.0181,-0.0432,0.022,0.04,-0.0994,0.0143,-0.042,0.0394,-0.0648,-0.0291,0.0231,0.0095,-0.0108,0.2061,-0.0705,0.0647,0.0521,-0.02,0.0023,-0.0448,-0.0128,-0.0606,-0.0054,0.0073,-0.0374,-0.0007,0.0493,-0.0722,0.0699,-0.0112,0.1402,0.0501,-0.0439,-0.0199,0.0132,0.0411,0.0688,-0.0654,0.0235,-0.0557,0.0298,0.1302,0.0785,-0.0024,0.0462,0.028,-0.0589,0.0008,0.0024,0.0421,0.0376,0.0009,0.0157,-0.0192,-0.0127,-0.057,0.0342,-0.0517,-0.018,0.1525,-0.0621,-0.0565,-0.0546,-0.0222,-0.0457,0.0036,-0.0403,-0.0224,0.0161,-0.0212,0.0532,0.0423,-0.0555,0.0158,-0.033,-0.0199,-0.0404,0.0775,0.0374,-0.1207,-0.0474,-0.0065,0.0096,-0.0035,0.0564,0.0623,-0.0064,0.0317,0.0565,0.0594,-0.0925,0.0238,-0.0298,-0.0171,0.0294,0.0034,-0.0513,0.0179,0.0156,-0.0482,-0.0195,-0.0394,0.0458,0.0455,-0.045,-0.0143,-0.0256,0.0239,-0.0328,0.0341,0.0277,-0.0397,0.0157,-0.023,0.0041,-0.0254,-0.0439,0.0119,-0.0236,0.0059,-0.0142,0.0034,0.0287,0.0207,-0.0185,0.0088,0.0574,-0.0208,0.0018,-0.0638,0.0445,0.0158,-0.0521,0.0444,0.0314,-0.0125,-0.0398,-0.234,0.0209,0.026,-0.0113,0.0288,-0.0543,0.0466,-0.0123,0.0957,0.085,0.0645,0.0069,-0.0174,0.0171,0.0074,0.0655,0.0099,0.0613,-0.0364,0.0121,-0.0124,0.0289,-0.0363,-0.0481,0.0402,0.0261,0.2054,0.0564,0.0122,-0.0246,0.0274,0.0404,-0.0445,-0.0868,0.0628,0.0581,0.0768,-0.0213,-0.025,-0.0277,-0.0359,0.0224,-0.0191,-0.0905,-0.073,-0.0179,0.0057,0.0102,-0.051,-0.0302,0.0764,-0.0504,0.0575,0.0145,-0.0193,-0.066,-0.0779,0.0024,-0.0309,0.037,0.0144,-0.0049,0.005,-0.0487,0.0712,0.0029,-0.0594,-0.0472,-0.0348,-0.0039,-0.0198,0.0882,-0.0067,0.025,0.057,-0.0168,0.0196,-0.0507,-0.0857,-0.028,0.0923,-0.0248,0.029,0.051,0.0171,-0.0167,0.0569,0.0436,0.0672,-0.0257,0.0099,0.0104,-0.0601,-0.0119,0.0281,-0.0229,-0.3024,0.0432,-0.0092,0.0203,-0.0183,0.0551,0.0272,0.035,-0.0332,-0.016,0.0236,0.0561,0.0317,-0.0027,0.0128,0.0485,0.0697,-0.0413,-0.0214,-0.0343,0.0167,0.009,0.1943,0.0019,0.0561,0.0086,-0.009,-0.02,0.0401,-0.0299,-0.0421,-0.0275,0.0903,-0.0702,0.0118,0.0314,0.0046,-0.0093,-0.0044,0.0023,-0.0149,0.0147,-0.0279,-0.038,0.0758,-0.0031,0.0039,-0.0439,0.0284,0.0345,-0.0275,-0.0219,-0.0297,0.0142,0.0333,0.0362,-0.0331,-0.0086,0.0019,-0.0197,-0.0168,-0.0617,-0.0106,-0.0053,-0.0277]}
{"key":"[Out-of-distribution Generalization with Causal Invariant Transformations] In real-world applications, it is important and desirable to learn a model that performs well on out-of-distribution (OOD) data. Recently, causality has become a powerful tool to tackle the OOD generalization problem, with the idea resting on the causal mechanism that is invariant across domains of interest. To leverage the generally unknown causal mechanism, existing works assume a linear form of causal feature or require sufficiently many and diverse training domains, which are usually restrictive in practice. In this work, we obviate these assumptions and tackle the OOD problem without explicitly recovering the causal feature. Our approach is based on transformations that modify the non-causal feature but leave the causal part unchanged, which can be either obtained from prior knowledge or learned from the training data in the multi-domain scenario. Under the setting of invariant causal mechanism, we theoretically show that if all such transformations are available, then we can learn a minimax optimal model across the domains using only single domain data. Noticing that knowing a complete set of these causal invariant transformations may be impractical, we further show that it suffices to know only a subset of these transformations. Based on the theoretical findings, a regularized training procedure is proposed to improve the OOD generalization capability. Extensive experimental results on both synthetic and real datasets verify the effectiveness of the proposed algorithm, even with only a few causal invariant transformations.","layer":2,"vector":[0.0238,0.0059,0.0161,-0.0473,0.0539,-0.0108,0.0124,0.0171,0.0437,-0.0173,0.0361,-0.0493,0.0108,0.095,0.0292,0.0406,0.0146,0.0642,-0.0566,-0.008,0.003,-0.0651,-0.0002,-0.0313,0.0198,0.008,-0.0239,-0.0079,-0.0495,-0.27,0.0194,-0.0313,0.0346,-0.0224,0.0316,-0.0431,-0.0591,0.0223,-0.0117,0.0478,0.0071,0.0299,-0.0344,-0.0953,-0.0197,-0.0525,-0.0255,-0.0125,-0.0396,-0.0143,0.0074,-0.0047,0.009,0.0396,0.0518,0.0266,0.0232,0.0362,0.0434,0.0519,-0.0147,0.0547,-0.1723,0.0191,0.0862,0.0421,-0.02,0.02,-0.0029,0.0592,-0.0168,0.0425,0.0066,0.0295,0.0089,0.0221,0.0073,0.0142,-0.0163,0.0195,0.0554,-0.0133,0.0143,-0.0263,-0.0052,-0.0759,0.036,-0.0758,0.0373,0.0447,-0.0904,0.0022,0.0245,0.0118,-0.0652,0.0059,0.0585,0.0035,-0.0121,0.2013,-0.0378,0.0061,0.0138,-0.0087,0.0525,-0.0218,-0.0459,-0.0217,0.0028,0.0069,0.0004,-0.0006,-0.0061,-0.0476,0.0052,-0.0152,0.0789,0.024,-0.0082,-0.0032,-0.0296,-0.0171,0.0086,-0.0164,0.0246,-0.0532,0.0334,0.123,0.0627,-0.0019,0.055,-0.0382,-0.0729,-0.0212,0.0563,0.0046,0.0336,0.016,0.0279,0.0381,-0.0019,-0.0067,0.0358,-0.0799,-0.0612,0.1275,-0.0199,0.0274,-0.0244,-0.0076,-0.0131,0.0369,-0.0263,-0.0322,0.0075,0.0334,0.0118,0.0305,-0.0472,0.0457,-0.064,-0.0451,-0.0161,0.0785,-0.0336,-0.056,-0.0234,0.0443,0.0378,-0.0145,0.0233,0.0443,-0.0271,0.0376,0.1358,0.0261,-0.0343,-0.0296,-0.002,0.0142,0.0472,-0.0529,-0.0148,0.0536,0.0326,-0.0169,0.0302,0.0015,0.0021,0.0039,0.0048,-0.0147,-0.0834,-0.0254,-0.0236,0.0078,-0.0214,-0.0031,0.024,-0.0265,-0.0213,0.0,-0.0674,-0.0001,-0.079,0.0086,0.0105,0.0019,0.0278,0.0341,0.0122,0.0079,0.0643,-0.0055,-0.0237,0.0401,0.0214,0.0358,-0.0182,0.0318,0.0282,-0.0642,-0.0188,-0.223,-0.0398,0.0178,0.0051,0.0136,-0.096,0.0432,0.0112,0.0596,0.1014,0.0578,-0.0213,-0.0241,0.0233,-0.0199,0.0364,0.0232,0.0579,-0.032,0.0253,-0.0051,-0.0091,0.0064,-0.1202,0.0747,-0.0103,0.2057,0.0339,0.0482,-0.0379,0.0096,0.0172,0.0272,-0.0978,0.0588,0.0006,0.0568,-0.006,-0.0292,-0.0209,0.0019,0.019,0.0069,-0.0995,-0.0161,-0.0194,-0.0463,0.0186,-0.0596,0.0525,0.0198,-0.0397,0.0628,-0.0108,-0.0024,-0.0627,-0.0916,0.0038,-0.0577,-0.0109,0.0087,-0.0027,0.0053,-0.0919,0.0128,-0.0176,0.0086,-0.0823,0.0349,-0.0241,-0.0158,0.0625,-0.0037,0.0172,0.0427,0.014,-0.006,0.0107,-0.0735,-0.0416,0.0627,-0.0287,0.0045,0.0154,0.0406,0.0,0.08,-0.0233,0.0256,0.0057,-0.0223,0.0198,-0.0421,-0.008,0.0479,0.0083,-0.304,0.0352,0.0134,0.054,-0.0084,0.0203,0.0165,0.0181,-0.057,-0.0259,-0.0307,0.0054,0.0399,0.012,0.0241,0.0317,0.0567,-0.0788,0.0266,-0.0889,0.0095,0.0511,0.2324,-0.0437,-0.014,0.0102,-0.0326,-0.0208,0.014,-0.0233,0.0345,0.0461,0.0683,-0.0628,0.0371,0.0682,-0.0745,0.0412,0.0003,-0.0527,-0.0086,-0.0081,-0.0221,-0.0447,0.1072,0.0123,-0.0275,-0.0688,0.0164,0.0347,-0.0367,-0.0023,-0.0617,0.0101,0.0189,0.0162,-0.0362,-0.0404,-0.0216,-0.0772,0.016,-0.0336,-0.048,-0.0092,-0.035]}
{"key":"[Specification-Guided Learning of Nash Equilibria with High Social Welfare] Reinforcement learning has been shown to be an effective strategy for automatically training policies for challenging control problems. Focusing on non-cooperative multi-agent systems, we propose a novel reinforcement learning framework for training joint policies that form a Nash equilibrium. In our approach, rather than providing low-level reward functions, the user provides high-level specifications that encode the objective of each agent. Then, guided by the structure of the specifications, our algorithm searches over policies to identify one that provably forms an $\\epsilon$-Nash equilibrium (with high probability). Importantly, it prioritizes policies in a way that maximizes social welfare across all agents. Our empirical evaluation demonstrates that our algorithm computes equilibrium policies with high social welfare, whereas state-of-the-art baselines either fail to compute Nash equilibria or compute ones with comparatively lower social welfare.","layer":9,"vector":[-0.0464,0.0074,0.0275,-0.0265,0.0193,0.0433,0.0291,0.0362,0.0455,0.0191,0.0182,-0.0327,0.0273,0.0648,0.0219,0.033,-0.0383,0.074,-0.0294,0.0246,0.0381,-0.0445,-0.0531,-0.0884,0.0092,0.037,-0.039,-0.059,-0.0295,-0.2104,0.0448,-0.0424,-0.0258,-0.0029,0.0038,-0.0347,-0.0216,0.0379,-0.0333,0.0064,0.0232,0.0332,-0.018,-0.0776,-0.0395,-0.0283,-0.025,-0.0221,-0.0457,-0.052,0.0003,-0.0233,0.0116,0.011,0.0531,0.0306,0.0596,0.0839,0.0122,0.0481,0.0103,0.024,-0.1708,0.0722,0.0628,0.0758,-0.0541,0.0264,0.0285,0.0661,-0.0103,0.0574,0.0369,0.0057,0.0644,0.0128,0.0099,-0.0528,-0.011,0.0066,-0.0202,-0.0572,-0.0534,0.0119,-0.0019,-0.0821,0.0355,-0.0506,0.0609,0.0096,-0.0368,0.0231,-0.0131,0.0071,-0.0454,0.0327,0.0185,-0.0072,-0.0958,0.2066,0.001,0.028,-0.0177,-0.0536,0.0402,-0.0275,-0.0457,0.0011,-0.0193,-0.0261,-0.0526,-0.0247,0.0446,0.0054,0.0024,0.025,0.056,0.0323,0.0121,-0.0085,0.0187,0.012,0.0467,-0.0018,0.0312,-0.0709,0.008,0.1423,0.0444,0.0244,0.0123,-0.0433,-0.0347,-0.0282,0.0206,0.047,0.0135,0.0171,0.0156,0.0417,-0.0256,-0.0227,-0.0141,-0.1417,-0.0813,0.0832,0.0108,0.0214,-0.0647,-0.0154,-0.0203,-0.0228,-0.0039,-0.0104,-0.0188,0.0137,0.0571,0.0534,-0.0507,-0.0085,-0.0341,-0.0611,-0.0276,0.0837,-0.0187,-0.0566,-0.0399,-0.0353,0.0108,-0.0235,0.015,0.0268,-0.0494,0.0122,0.0955,0.0386,-0.0888,0.0469,0.0298,0.016,0.0292,-0.0637,-0.0595,0.0334,0.0162,-0.0063,-0.0148,-0.0536,-0.0098,-0.0108,-0.0197,0.0212,-0.0449,-0.0085,-0.031,-0.012,-0.0026,-0.032,0.0282,-0.0307,-0.0021,0.0319,-0.0567,0.0455,-0.0092,0.0242,-0.0211,-0.0261,0.0477,-0.0157,-0.0266,0.0112,0.0564,0.0173,-0.0904,0.0377,0.0187,0.0624,-0.0329,0.0103,0.0559,0.0091,-0.047,-0.1653,0.0196,-0.0136,-0.0327,0.0353,-0.0243,0.0539,-0.0369,-0.0097,0.0654,0.0914,-0.0192,-0.0525,0.0384,0.0146,0.0335,0.0231,0.0189,0.0341,0.0569,-0.0079,-0.0025,0.0036,-0.0998,0.1058,-0.0215,0.249,0.0038,0.021,-0.0258,0.0075,-0.0028,-0.0715,-0.0903,0.0443,0.0189,0.043,-0.0483,-0.0118,-0.0659,0.0163,0.0473,-0.0728,-0.0961,-0.0037,-0.0244,-0.0216,0.0614,-0.0626,-0.0229,0.0581,-0.0037,-0.0018,-0.0323,-0.0425,-0.0429,-0.062,0.0503,-0.0279,0.0482,0.0082,-0.0058,0.0187,-0.0595,0.0914,-0.0026,0.0013,-0.0578,0.026,0.0093,-0.0122,0.0441,-0.002,-0.0004,0.0116,0.017,0.0243,-0.0067,-0.0399,-0.0333,0.0562,-0.0517,0.0313,0.0453,0.0235,-0.0644,0.0694,-0.0011,0.05,-0.007,-0.0105,0.0263,-0.0614,0.0106,0.0615,-0.0291,-0.308,0.0408,0.0245,0.0218,-0.0438,0.0062,0.05,-0.0043,-0.0711,0.0096,0.0556,0.0562,0.0323,0.0589,0.0007,-0.0067,0.0745,-0.0215,0.0465,-0.0571,-0.0029,0.0129,0.2262,-0.0338,0.0287,0.0185,-0.0474,0.0078,0.0289,-0.0051,-0.0065,0.0295,0.0731,-0.0638,0.042,0.0988,-0.0322,0.0331,-0.0266,0.045,-0.0721,-0.0054,0.0614,-0.0108,0.0527,0.0472,-0.0603,-0.0498,-0.0104,0.0221,-0.0284,0.0243,-0.0151,-0.0089,0.0138,0.0162,-0.0489,-0.0261,-0.0579,-0.0432,-0.015,0.0011,-0.0003,-0.0084,-0.0132]}
{"key":"[Generalized Shape Metrics on Neural Representations] Understanding the operation of biological and artificial networks remains a difficult and important challenge. To identify general principles, researchers are increasingly interested in surveying large collections of networks that are trained on, or biologically adapted to, similar tasks. A standardized set of analysis tools is now needed to identify how network-level covariates -- such as architecture, anatomical brain region, and model organism -- impact neural representations (hidden layer activations). Here, we provide a rigorous foundation for these analyses by defining a broad family of metric spaces that quantify representational dissimilarity. Using this framework we modify existing representational similarity measures based on canonical correlation analysis to satisfy the triangle inequality, formulate a novel metric that respects the inductive biases in convolutional layers, and identify approximate Euclidean embeddings that enable network representations to be incorporated into essentially any off-the-shelf machine learning method. We demonstrate these methods on large-scale datasets from biology (Allen Institute Brain Observatory) and deep learning (NAS-Bench-101). In doing so, we identify relationships between neural representations that are interpretable in terms of anatomical features and model performance.","layer":0,"vector":[-0.0159,-0.0316,0.0238,0.0103,0.0051,0.0359,0.0512,0.0283,0.0357,-0.0413,0.0022,-0.0963,0.049,0.0699,0.0018,-0.0206,-0.0045,0.0593,-0.0581,0.0425,-0.0004,-0.0302,0.0209,-0.0266,0.01,-0.001,-0.0342,0.0118,-0.0456,-0.2757,0.0138,-0.0283,0.0662,-0.02,0.0144,0.0034,-0.0183,0.035,-0.0355,0.008,0.0422,0.0342,-0.0141,-0.0379,-0.0424,-0.0492,-0.0361,0.0069,0.0005,-0.0791,0.0506,-0.0422,0.0202,0.0592,0.0335,0.051,0.0884,0.0534,0.0291,0.0527,0.021,0.0101,-0.1498,0.0228,0.0323,0.0273,-0.0889,-0.0537,-0.004,0.0281,-0.0054,0.0179,-0.0021,0.0281,-0.0111,0.0198,0.0168,-0.0162,-0.0287,0.015,0.0515,0.0198,-0.0497,-0.0276,0.0099,-0.0509,0.0158,-0.0406,0.0071,-0.0118,-0.0759,-0.0112,-0.0562,0.0077,-0.0492,-0.0123,0.042,-0.014,-0.0485,0.2005,-0.0159,0.02,0.0394,0.0072,0.0655,-0.0147,-0.0158,-0.0386,-0.0137,0.0436,-0.0178,-0.0095,-0.0318,-0.0259,0.0468,-0.0219,0.0589,0.0483,-0.0018,-0.0047,-0.063,0.0332,0.0391,-0.0356,0.0301,-0.0613,0.0261,0.1358,0.062,0.0187,0.0359,0.0409,-0.0347,0.0199,0.0105,0.049,0.0246,-0.0168,-0.052,-0.0008,-0.0166,-0.0162,0.0431,-0.0518,-0.0672,0.1708,-0.0722,-0.0222,-0.0603,0.0297,-0.0242,0.0494,-0.0611,-0.0431,0.0176,0.0331,-0.0229,0.0272,-0.0569,0.0091,-0.0355,-0.0502,-0.0045,0.1074,0.0842,-0.0632,-0.0086,0.0147,0.0175,-0.006,0.0563,0.0031,-0.0139,0.0374,0.0775,0.0171,-0.0758,-0.0312,0.0629,-0.002,0.0457,-0.0875,-0.0525,0.0271,0.0452,-0.0431,0.0105,-0.0273,-0.0093,0.0413,-0.0097,0.0074,-0.0489,-0.0083,-0.0277,-0.0423,0.0037,-0.0329,-0.004,-0.0082,-0.0027,-0.0115,-0.0283,-0.0018,-0.0123,0.0002,-0.0053,-0.0123,0.0539,0.0314,-0.0682,-0.0155,0.03,-0.0309,-0.0037,0.0143,0.0124,0.0384,0.0223,0.0266,0.0178,-0.0807,-0.0719,-0.222,-0.0163,-0.011,-0.047,0.0701,-0.0786,0.0096,0.023,0.0306,0.0502,0.0372,-0.0149,0.0005,0.0146,-0.0252,0.1004,0.0274,0.0463,-0.0655,-0.0215,0.0011,0.038,-0.0098,-0.0613,0.0327,-0.0114,0.2092,0.0451,0.0676,-0.0067,-0.0336,0.0218,-0.0416,-0.081,0.0539,0.0242,0.0548,-0.0024,-0.0371,-0.022,-0.0792,0.0101,0.0078,-0.0953,0.0039,-0.0041,-0.0242,0.0113,-0.0614,-0.0197,0.0181,-0.0345,0.0835,0.0011,-0.0164,-0.012,-0.0634,0.0399,-0.0686,0.0456,0.015,-0.0671,-0.0191,-0.1007,0.0427,-0.0115,-0.0563,-0.0217,0.0271,-0.0091,-0.0095,0.0752,-0.0117,-0.0089,0.0961,-0.0252,0.055,-0.0087,-0.01,0.0451,0.0788,-0.003,0.014,-0.0264,0.0338,0.0108,0.083,-0.0229,0.0664,-0.0685,0.0197,-0.0157,-0.0623,-0.0496,0.0159,0.0025,-0.2747,0.0536,0.0384,0.034,-0.0097,-0.0109,0.0206,0.0007,-0.0179,-0.0642,0.0541,0.0257,0.0403,-0.0103,-0.0045,0.0366,0.0727,-0.0685,0.0697,-0.0237,-0.0064,0.0159,0.2126,-0.0413,0.029,-0.0008,-0.0207,-0.0033,0.021,-0.0184,0.0248,0.0107,0.0777,-0.0571,0.0435,0.1036,-0.0345,0.024,0.0281,-0.0053,0.0568,-0.0365,-0.0766,0.0034,0.0815,0.0156,-0.02,-0.0176,0.0125,0.0059,-0.0309,0.037,-0.0018,-0.0215,0.0227,0.0167,-0.0241,-0.0426,-0.0196,-0.0251,-0.0036,-0.0523,-0.0334,0.0186,-0.0037]}
{"key":"[A block coordinate descent optimizer for classification problems exploiting convexity] Second-order optimizers hold intriguing potential for deep learning, but suffer from increased cost and sensitivity to the non-convexity of the loss surface as compared to gradient-based approaches. We introduce a coordinate descent method to train deep neural networks for classification tasks that exploits global convexity of the cross-entropy loss in the weights of the linear layer. Our hybrid Newton/Gradient Descent (NGD) method is consistent with the interpretation of hidden layers as providing an adaptive basis and the linear layer as providing an optimal fit of the basis to data. By alternating between a second-order method to find globally optimal parameters for the linear layer and gradient descent to train the hidden layers, we ensure an optimal fit of the adaptive basis to data throughout training. The size of the Hessian in the second-order step scales only with the number weights in the linear layer and not the depth and width of the hidden layers; furthermore, the approach is applicable to arbitrary hidden layer architecture. Previous work applying this adaptive basis perspective to regression problems demonstrated significant improvements in accuracy at reduced training cost, and this work can be viewed as an extension of this approach to classification problems. We first prove that the resulting Hessian matrix is symmetric semi-definite, and that the Newton step realizes a global minimizer. By studying classification of manufactured two-dimensional point cloud data, we demonstrate both an improvement in validation error and a striking qualitative difference in the basis functions encoded in the hidden layer when trained using NGD. Application to image classification benchmarks for both dense and convolutional architectures reveals improved training accuracy, suggesting possible gains of second-order methods over gradient descent.","layer":2,"vector":[-0.0181,-0.0191,0.0259,0.0044,0.0139,0.0882,0.0235,0.036,0.0487,0.0043,0.0233,-0.0289,0.0491,0.0575,0.0264,0.0156,0.0441,0.0688,-0.0581,-0.0008,0.0214,-0.0164,-0.0073,-0.0623,0.0266,0.0124,-0.0448,-0.0106,-0.0425,-0.2679,0.0069,-0.0731,0.0282,-0.0186,0.0141,-0.0292,-0.0153,0.0581,-0.0384,-0.0007,0.0009,-0.0277,-0.0425,-0.0407,0.0246,0.0,-0.0509,-0.0163,-0.0245,-0.0509,0.0079,-0.0037,0.011,0.0183,0.0303,0.0604,0.0403,0.0277,0.0225,0.0581,0.0233,0.0344,-0.1617,0.0564,0.0228,0.0363,-0.0398,-0.0425,-0.0225,0.0563,0.0191,0.0276,0.0211,0.0074,0.0105,-0.0057,-0.0323,0.0047,-0.0144,-0.0306,0.0651,-0.0115,-0.0202,0.0025,0.001,-0.0418,0.0656,0.0009,0.042,0.0149,-0.0236,-0.0353,-0.0293,0.0171,-0.0546,-0.0097,0.0569,0.0525,-0.0625,0.1875,-0.0356,0.0212,0.0132,-0.0317,0.0636,-0.0351,-0.0662,-0.0523,0.0003,0.0092,-0.0189,-0.0122,0.0061,-0.0217,0.0226,0.0073,0.0392,0.0378,-0.0236,-0.0028,-0.0174,-0.0149,0.0709,-0.027,0.0605,-0.0657,-0.0135,0.1333,0.0453,0.0309,0.0607,-0.0443,-0.0296,-0.0375,0.027,0.0286,0.0035,0.0093,0.0117,-0.0236,-0.0161,-0.0318,0.0197,-0.0606,-0.0761,0.146,-0.0727,0.0097,-0.0439,-0.0463,-0.0127,0.0067,-0.0073,-0.0455,-0.0013,0.0418,0.014,0.0305,-0.0606,0.0008,-0.0214,-0.0471,-0.005,0.1176,-0.0109,-0.0825,-0.0153,-0.0315,0.0373,-0.0213,0.045,0.055,-0.0273,-0.0332,0.0726,0.0436,-0.1009,-0.0124,0.0111,0.0146,0.0135,-0.059,-0.0262,0.0376,0.0522,-0.0173,0.0352,-0.0493,0.0068,0.0364,-0.0717,0.0204,-0.0356,-0.0152,-0.0054,-0.0296,-0.0164,0.0085,0.0113,-0.0305,0.0367,0.0266,-0.0071,0.0331,-0.0044,0.0222,0.0276,-0.0039,0.0241,0.0355,-0.0428,-0.0152,0.0248,-0.0588,-0.0238,-0.0,-0.0019,0.0307,-0.0592,0.0018,0.0335,-0.041,-0.0548,-0.21,0.0009,-0.0005,-0.0361,0.0365,-0.0739,0.0331,0.0057,0.0094,0.0479,0.0752,-0.0002,-0.0505,0.0103,-0.0054,0.0414,0.088,0.0065,-0.0001,0.0084,0.0493,0.0466,-0.0004,-0.0944,0.061,0.0074,0.2239,-0.0004,0.079,0.0125,0.0512,0.0424,0.0069,-0.0911,0.0728,-0.0152,0.0991,-0.0178,-0.0614,-0.0662,-0.0326,-0.0062,0.028,-0.085,-0.0402,-0.0174,-0.0329,0.0365,-0.0657,-0.0226,0.0822,-0.019,0.0334,-0.0409,-0.0168,-0.0268,-0.0786,0.003,-0.0341,0.0024,-0.0053,-0.082,0.0039,-0.0398,0.0291,0.0078,-0.0327,-0.0154,0.0131,-0.0429,0.0116,0.0625,0.0045,-0.0028,0.0558,-0.0047,0.0416,0.0286,-0.0398,-0.0356,0.0627,0.0069,0.0352,0.0117,0.0321,0.0218,0.1211,-0.0462,0.0087,-0.0073,-0.0581,0.0157,-0.0923,0.0056,0.0316,-0.0085,-0.2902,-0.0157,0.0054,0.0138,-0.0312,-0.0214,0.057,0.0205,-0.0341,0.0219,-0.0091,0.0473,0.0533,0.0043,0.0201,0.0341,0.0404,-0.0301,0.0463,-0.0513,-0.0037,0.0652,0.235,-0.0584,0.0178,-0.0023,-0.0115,-0.0118,0.0364,0.0102,0.0191,-0.0073,0.0547,-0.0593,0.0171,0.1085,-0.0472,0.0394,0.0482,-0.0248,0.0389,-0.0023,-0.0686,-0.0308,0.0713,-0.02,-0.031,-0.0107,0.0117,-0.0024,-0.0265,0.0148,0.0246,0.0337,0.048,0.0085,-0.079,-0.0486,-0.0269,-0.0367,0.0815,-0.0922,-0.0464,-0.0505,-0.0395]}
{"key":"[SHARKS: Smart Hacking Approaches for RisK Scanning in Internet-of-Things and Cyber-Physical Systems based on Machine Learning] Cyber-physical systems (CPS) and Internet-of-Things (IoT) devices are increasingly being deployed across multiple functionalities, ranging from healthcare devices and wearables to critical infrastructures, e.g., nuclear power plants, autonomous vehicles, smart cities, and smart homes. These devices are inherently not secure across their comprehensive software, hardware, and network stacks, thus presenting a large attack surface that can be exploited by hackers. In this article, we present an innovative technique for detecting unknown system vulnerabilities, managing these vulnerabilities, and improving incident response when such vulnerabilities are exploited. The novelty of this approach lies in extracting intelligence from known real-world CPS/IoT attacks, representing them in the form of regular expressions, and employing machine learning (ML) techniques on this ensemble of regular expressions to generate new attack vectors and security vulnerabilities. Our results show that 10 new attack vectors and 122 new vulnerability exploits can be successfully generated that have the potential to exploit a CPS or an IoT ecosystem. The ML methodology achieves an accuracy of 97.4% and enables us to predict these attacks efficiently with an 87.2% reduction in the search space. We demonstrate the application of our method to the hacking of the in-vehicle network of a connected car. To defend against the known attacks and possible novel exploits, we discuss a defense-in-depth mechanism for various classes of attacks and the classification of data targeted by such attacks. This defense mechanism optimizes the cost of security measures based on the sensitivity of the protected resource, thus incentivizing its adoption in real-world CPS/IoT by cybersecurity practitioners.","layer":1,"vector":[-0.0592,-0.0203,0.0325,-0.0302,0.0249,-0.002,0.0775,0.0206,0.0287,0.0053,0.0372,-0.0368,0.0463,0.0081,0.0191,0.0097,0.0069,0.0042,-0.022,0.0378,0.0498,-0.01,0.0033,-0.0779,0.0224,0.0536,-0.0017,-0.0124,-0.0734,-0.2034,-0.001,-0.0607,0.0693,-0.0189,-0.0036,-0.0457,-0.0391,0.0335,0.0136,0.0402,0.0194,0.0019,-0.0264,-0.0397,-0.0146,-0.0698,0.0103,-0.0037,-0.0122,-0.0652,0.0232,-0.0299,0.0264,0.0481,0.0523,-0.0422,0.044,0.0459,0.0693,0.0407,0.0576,0.0702,-0.1725,0.0543,0.0835,0.0527,-0.0426,0.001,0.0593,0.0132,-0.0016,0.0082,-0.0012,0.061,0.0148,0.0827,0.0121,-0.0502,0.0152,0.0286,0.0189,-0.0505,-0.0599,0.0432,-0.0526,-0.0444,0.0276,-0.0292,0.0833,0.0075,-0.0951,-0.0159,0.045,0.0289,-0.0525,-0.0193,0.0336,0.0039,-0.0627,0.2357,-0.0637,0.0139,-0.0242,-0.0434,0.0634,-0.0397,-0.0195,-0.0744,-0.0122,-0.0225,-0.0058,-0.0223,0.0303,-0.0357,0.0193,0.0289,0.0282,0.0559,0.0173,-0.0097,-0.0542,-0.0109,0.0661,0.0152,0.038,-0.0672,0.0399,0.1413,0.0592,0.0632,0.0152,-0.0332,-0.0317,0.01,0.0304,0.0476,-0.0315,0.0307,0.0201,0.0011,-0.0012,-0.0394,0.0307,-0.0777,-0.0263,0.0781,-0.0369,0.0482,-0.0195,-0.0553,-0.0349,0.036,-0.0028,-0.0079,-0.0055,0.0202,0.0349,0.0493,-0.008,-0.0035,-0.0294,0.0031,-0.0601,0.1223,0.0206,-0.0894,0.0154,0.0004,-0.0406,-0.0243,0.0117,0.0629,-0.0283,0.0482,0.0403,0.0271,-0.0694,0.013,-0.0611,0.0418,-0.0465,-0.0199,-0.062,0.0118,0.0519,-0.0481,-0.0199,-0.0242,-0.0097,0.0353,-0.042,0.0117,-0.0408,-0.032,-0.0397,-0.0052,-0.0138,0.0086,-0.0018,-0.0253,0.0255,-0.0223,-0.0427,0.0389,-0.039,-0.0111,-0.0216,-0.0354,-0.026,0.0265,-0.0052,-0.0133,0.0407,-0.0322,-0.0374,-0.0259,-0.015,0.0554,0.0022,0.0083,-0.0076,0.0013,-0.0623,-0.2207,-0.0653,-0.0259,-0.0226,0.0309,-0.0724,0.037,-0.0675,0.0273,0.0191,0.0723,-0.019,-0.0382,-0.0033,0.0008,0.0857,0.0382,0.0105,-0.0165,0.0552,-0.0099,0.0395,-0.0155,-0.0523,0.0112,0.0327,0.2268,0.054,0.0076,-0.0182,0.0369,0.0326,-0.0181,-0.0905,0.0805,-0.0205,0.0474,0.02,-0.0404,0.0048,-0.0394,0.0047,-0.0259,-0.0898,-0.0141,-0.0745,-0.0555,0.0079,-0.0134,0.0143,0.029,0.0389,0.0279,0.007,0.0082,-0.0424,-0.0584,0.0409,-0.0346,0.0079,-0.012,-0.0485,-0.0046,-0.0893,0.0492,-0.0265,-0.0286,-0.0461,0.0206,-0.0243,-0.0525,0.1186,0.0553,-0.0333,0.0362,0.0172,0.034,-0.0437,-0.0145,-0.0416,0.0892,-0.0371,0.0603,0.0198,0.0279,0.0005,0.0375,0.0136,0.0554,-0.0332,0.0037,-0.0165,-0.0394,-0.0174,0.0421,0.0112,-0.2894,0.0603,0.0205,0.0447,-0.0488,-0.0161,0.0871,-0.0153,-0.0475,0.034,-0.0029,0.0247,0.0025,-0.0223,0.0188,0.0051,0.0528,-0.0619,0.028,-0.0659,0.0352,0.0632,0.2349,-0.0039,0.0396,0.0246,-0.0272,0.0422,0.0324,-0.0073,0.0146,-0.0109,0.0402,-0.0573,0.005,0.0194,-0.0153,-0.0025,0.0083,-0.0589,0.0106,0.0137,-0.041,0.0021,0.083,-0.0168,-0.033,-0.0842,0.0456,0.0396,-0.0129,-0.0219,-0.0329,-0.0184,0.0814,0.0198,-0.0263,-0.0693,-0.0429,-0.0305,0.0329,-0.0472,-0.0168,0.0116,0.0196]}
{"key":"[Diet deep generative audio models with structured lottery] Deep learning models have provided extremely successful solutions in most audio application fields. However, the high accuracy of these models comes at the expense of a tremendous computation cost. This aspect is almost always overlooked in evaluating the quality of proposed models. However, models should not be evaluated without taking into account their complexity. This aspect is especially critical in audio applications, which heavily relies on specialized embedded hardware with real-time constraints. In this paper, we build on recent observations that deep models are highly overparameterized, by studying the lottery ticket hypothesis on deep generative audio models. This hypothesis states that extremely efficient small sub-networks exist in deep models and would provide higher accuracy than larger models if trained in isolation. However, lottery tickets are found by relying on unstructured masking, which means that resulting models do not provide any gain in either disk size or inference time. Instead, we develop here a method aimed at performing structured trimming. We show that this requires to rely on global selection and introduce a specific criterion based on mutual information. First, we confirm the surprising result that smaller models provide higher accuracy than their large counterparts. We further show that we can remove up to 95% of the model weights without significant degradation in accuracy. Hence, we can obtain very light models for generative audio across popular methods such as Wavenet, SING or DDSP, that are up to 100 times smaller with commensurate accuracy. We study the theoretical bounds for embedding these models on Raspberry Pi and Arduino, and show that we can obtain generative models on CPU with equivalent quality as large GPU models. Finally, we discuss the possibility of implementing deep generative audio models on embedded platforms.","layer":1,"vector":[-0.0419,-0.0586,0.0562,-0.0524,-0.0,0.0479,-0.0452,-0.0396,0.0686,-0.0114,0.0145,-0.0405,0.0607,0.0187,0.0457,0.0053,0.0232,0.0153,-0.0166,0.005,0.0483,-0.0535,-0.0456,-0.0555,0.0238,-0.0227,-0.02,-0.0565,-0.0574,-0.2731,0.0254,-0.0211,0.0811,-0.0544,0.0062,-0.0472,-0.0202,0.0227,-0.0384,0.0307,0.0444,0.0337,-0.0562,-0.0953,0.002,-0.0341,-0.082,-0.0428,-0.0259,-0.031,0.0058,-0.0252,0.0333,0.0399,0.0206,0.0167,0.0664,0.0671,0.0515,0.0144,-0.0056,0.0576,-0.1434,0.0394,0.0288,0.0507,-0.0067,-0.0021,0.0103,0.0205,-0.0117,0.0086,0.0263,0.0289,0.0148,-0.0393,0.0139,-0.0202,-0.0223,0.0155,-0.0234,-0.054,-0.0504,-0.0443,-0.0162,-0.0085,-0.0,-0.0086,-0.0444,-0.0074,-0.0624,-0.0031,-0.0531,0.0025,-0.0442,0.004,0.0349,-0.0073,-0.038,0.2221,-0.0081,0.0202,0.0034,-0.0108,0.0205,-0.0284,-0.0724,0.0017,-0.0315,0.0362,0.0239,-0.001,0.0393,-0.0122,0.0238,0.0411,0.0537,0.0358,0.0166,-0.0052,-0.0801,0.041,0.0189,-0.0415,0.0388,-0.042,0.0414,0.1132,0.0493,0.0615,0.0427,-0.0074,-0.0637,-0.0336,0.061,0.0153,0.0065,0.0074,0.0117,-0.0166,-0.0101,-0.0314,0.0128,-0.0633,-0.0447,0.1222,-0.0378,0.0444,-0.0333,-0.0308,0.0154,-0.0051,0.0029,-0.0621,0.041,0.0241,0.0084,0.0292,-0.0471,0.0486,-0.007,-0.0384,0.0183,0.1074,-0.001,-0.0748,-0.0625,0.0171,0.0162,-0.0262,-0.0105,-0.0035,-0.0546,0.033,0.0469,0.015,-0.0869,-0.0119,-0.0176,0.0122,0.0122,-0.0691,-0.0186,-0.0013,0.0478,-0.0689,0.0142,-0.0679,0.0078,0.0603,-0.0111,0.0161,-0.0251,0.0369,0.0051,-0.0276,-0.0183,-0.0008,0.0271,0.0027,-0.0365,0.0161,-0.0324,0.009,-0.0207,0.0065,-0.0007,0.0022,0.0327,0.0236,-0.0553,-0.0177,0.0926,0.0112,-0.0362,-0.0294,-0.0129,0.0054,0.0022,0.0273,0.0029,-0.0728,-0.0583,-0.2195,0.0311,0.0078,-0.0232,0.0561,-0.0801,0.0164,-0.0048,0.0499,0.0544,0.0401,-0.0175,0.0302,0.0369,-0.0357,0.0551,-0.016,0.034,0.0117,0.0028,-0.0033,0.0203,-0.0672,-0.0845,0.0455,-0.0056,0.2384,0.0309,0.0528,-0.0123,0.0183,0.0398,-0.0162,-0.0866,0.0276,0.084,0.0994,0.0034,-0.0229,-0.0033,-0.0758,0.0025,-0.0143,-0.1267,-0.0432,-0.0213,-0.0383,0.0359,-0.0458,0.043,0.0661,-0.0289,0.0541,0.0399,-0.0019,-0.0736,-0.1288,0.0419,-0.0629,0.05,0.0212,-0.0452,0.0289,-0.0537,-0.0001,0.0223,-0.0262,-0.0485,0.0288,-0.0271,0.0321,0.0832,-0.0125,0.0305,0.0604,0.0051,0.0415,-0.0768,-0.0454,-0.0288,0.088,0.0113,0.0408,0.0364,0.0075,0.0303,0.0675,0.0006,0.0121,-0.0225,0.0129,0.0132,-0.0319,0.0025,0.0121,-0.007,-0.2702,0.027,-0.0305,0.065,-0.0498,-0.0154,0.0028,0.0314,-0.0645,0.0395,0.0325,0.0747,0.0529,-0.0409,0.0219,0.0413,0.0942,-0.0405,0.0591,-0.0126,-0.0072,0.0541,0.2116,-0.0397,0.0043,-0.007,-0.0203,0.0224,0.0501,-0.0233,-0.0115,0.0165,0.1103,-0.0429,-0.0153,0.0627,-0.0308,0.0333,0.033,-0.0114,-0.0045,-0.0113,-0.0404,-0.0154,0.0513,-0.0295,0.0062,-0.0216,0.0234,0.0735,0.0097,-0.0327,0.0155,-0.0004,0.0099,0.0326,-0.0465,-0.0231,0.0101,-0.008,0.0496,-0.0497,-0.0164,0.0064,-0.0073]}
{"key":"[Reimagining an autonomous vehicle] The self driving challenge in 2021 is this century's technological equivalent of the space race, and is now entering the second major decade of development. Solving the technology will create social change which parallels the invention of the automobile itself. Today's autonomous driving technology is laudable, though rooted in decisions made a decade ago. We argue that a rethink is required, reconsidering the autonomous vehicle (AV) problem in the light of the body of knowledge that has been gained since the DARPA challenges which seeded the industry. What does AV2.0 look like? We present an alternative vision: a recipe for driving with machine learning, and grand challenges for research in driving.","layer":5,"vector":[-0.0452,-0.0473,0.0337,-0.0078,0.0159,0.0987,0.0516,0.0623,0.0077,-0.0269,0.0321,-0.0661,0.0153,0.0162,0.0219,0.008,-0.0103,0.0081,0.004,-0.0386,0.0075,-0.0501,-0.0176,-0.0623,-0.023,0.0266,-0.0225,-0.0175,-0.0595,-0.2583,-0.007,-0.0726,0.0563,0.0006,-0.0044,0.0051,-0.022,0.0564,-0.0278,0.0157,0.0619,-0.0154,-0.0488,-0.0499,-0.0316,-0.041,-0.0128,-0.0057,-0.0227,-0.0693,-0.0276,-0.0371,0.0238,0.0077,0.0349,0.0529,0.0496,0.0372,0.0558,0.0159,0.0245,0.0401,-0.2014,0.0713,0.0697,0.036,-0.048,-0.0319,0.018,0.0397,0.0104,0.0217,0.0029,0.0095,-0.0146,0.0066,-0.009,-0.0386,-0.0072,-0.006,0.0029,0.0154,-0.0758,0.0137,-0.0371,-0.0532,-0.0158,-0.0319,0.0676,0.002,-0.0702,0.0151,0.0012,-0.0206,-0.0631,-0.0011,0.0117,-0.0194,-0.0402,0.1759,-0.0498,0.0565,0.0268,-0.0425,0.0124,-0.032,-0.0114,-0.0562,-0.0509,0.0196,0.0182,-0.0006,0.0222,0.0267,-0.0093,0.0306,0.0262,0.0885,0.0249,-0.0101,-0.0146,-0.0157,0.0451,-0.0453,0.0017,-0.0737,0.0685,0.1198,0.0041,0.0309,0.0432,-0.0212,-0.0504,-0.0236,0.0236,0.018,0.0009,0.0007,0.0067,-0.0175,-0.0296,-0.0365,-0.0001,-0.0766,-0.0386,0.0865,-0.0131,0.0271,-0.0031,-0.0129,0.0008,-0.0085,-0.0703,-0.0262,0.0252,0.0448,0.0405,0.0465,-0.1006,0.053,0.0024,-0.0237,-0.0654,0.0874,0.0275,-0.0855,-0.0231,0.0069,0.032,0.038,0.0378,0.0207,-0.0193,0.0636,0.1058,0.0391,-0.0967,0.0525,-0.0382,0.013,-0.0196,-0.0742,-0.0305,0.0518,0.0792,-0.0153,-0.0102,-0.0551,-0.0173,0.0624,0.0287,0.0509,-0.0084,0.023,0.0054,-0.0086,0.0067,0.0081,0.0295,-0.0242,-0.0423,0.0204,0.0275,0.01,-0.0114,-0.0266,-0.0134,0.0057,0.0747,0.0508,-0.0398,-0.0238,0.0421,-0.0125,-0.0542,-0.0405,-0.0412,0.0229,-0.007,0.0256,0.0244,0.0024,-0.0398,-0.2058,-0.0065,-0.0165,-0.0344,0.0359,-0.038,0.011,-0.0074,0.0491,0.0558,0.0777,-0.0415,-0.0189,0.06,0.0233,0.0397,-0.0268,0.0285,-0.0455,0.0095,-0.0111,0.0223,0.0041,-0.0569,0.0245,0.0241,0.2324,0.0285,0.0555,0.011,0.0302,0.0407,-0.0236,-0.0935,0.0731,-0.0328,0.0625,0.0021,-0.0941,-0.0468,-0.0291,0.0307,-0.0226,-0.0878,-0.0431,-0.0406,-0.0427,0.0133,-0.0121,0.0076,0.0483,-0.0218,0.0277,-0.0257,-0.0172,-0.0402,-0.084,0.0162,-0.0028,0.0167,-0.0049,0.0088,0.0116,-0.0943,0.0478,0.0026,-0.0174,-0.0713,0.01,-0.013,-0.0019,0.1077,0.0099,-0.047,0.0655,-0.0196,0.0518,-0.0274,-0.032,-0.0489,0.0628,-0.0117,0.0765,0.0446,0.0283,0.023,0.0609,-0.0369,-0.0062,-0.0592,0.0328,0.0401,-0.0575,-0.0492,0.0774,0.0276,-0.2589,0.0265,0.0041,0.0658,-0.0355,-0.0033,0.0504,0.0381,-0.0514,-0.004,-0.012,0.0607,0.0616,0.0438,0.029,0.0182,0.1144,-0.0362,0.0312,-0.0771,0.0338,0.0722,0.2081,-0.0797,0.0233,0.0472,-0.0604,-0.0173,0.0327,-0.0236,-0.0048,-0.0143,0.084,-0.0722,0.0132,0.0349,-0.0674,0.0313,0.0237,0.0129,0.0065,0.0296,0.0164,-0.003,0.0865,-0.0227,-0.0393,-0.0592,-0.0043,0.0357,0.0098,-0.0119,-0.0576,0.0049,0.0534,0.0046,-0.0315,-0.0472,-0.0193,-0.0804,0.0374,-0.0325,0.0046,-0.0025,0.0086]}
{"key":"[On Computation and Generalization of Generative Adversarial Imitation Learning] Generative Adversarial Imitation Learning (GAIL) is a powerful and practical approach for learning sequential decision-making policies. Different from Reinforcement Learning (RL), GAIL takes advantage of demonstration data by experts (e.g., human), and learns both the policy and reward function of the unknown environment. Despite the significant empirical progresses, the theory behind GAIL is still largely unknown. The major difficulty comes from the underlying temporal dependency of the demonstration data and the minimax computational formulation of GAIL without convex-concave structure. To bridge such a gap between theory and practice, this paper investigates the theoretical properties of GAIL. Specifically, we show: (1) For GAIL with general reward parameterization, the generalization can be guaranteed as long as the class of the reward functions is properly controlled; (2) For GAIL, where the reward is parameterized as a reproducing kernel function, GAIL can be efficiently solved by stochastic first order optimization algorithms, which attain sublinear convergence to a stationary solution. To the best of our knowledge, these are the first results on statistical and computational guarantees of imitation learning with reward/policy function approximation. Numerical experiments are provided to support our analysis.","layer":1,"vector":[-0.0399,-0.0496,0.035,-0.0552,-0.0039,0.0424,0.0694,0.0227,0.0478,0.0056,-0.0057,-0.0144,0.0405,0.0801,-0.0235,0.0278,0.0147,0.0351,-0.0543,-0.0065,0.052,-0.0649,-0.0087,-0.0685,-0.0158,-0.0009,-0.0385,-0.0666,-0.0097,-0.2221,0.0351,-0.059,0.0117,-0.0064,-0.038,-0.0179,-0.0333,0.0811,-0.0305,0.0557,-0.0029,0.0134,-0.0646,-0.0803,-0.0115,-0.0291,-0.0328,-0.0308,-0.0222,-0.0331,0.0211,-0.0213,-0.0278,0.0194,0.0675,0.0103,0.0606,0.0673,0.0301,0.0529,-0.0262,0.0509,-0.1567,0.0573,-0.0031,0.0801,-0.0334,-0.035,-0.0023,0.0747,-0.0251,0.0588,0.0135,0.0509,-0.0119,0.0075,-0.0222,-0.0487,-0.0463,0.0299,0.0203,-0.03,-0.05,0.0211,-0.0108,-0.0134,0.0221,-0.0467,0.0575,0.0337,-0.025,0.0049,-0.0308,0.0407,-0.0692,0.0281,0.0296,0.0538,-0.0695,0.1851,-0.01,0.0835,0.0706,0.023,0.0401,-0.0119,-0.0693,-0.0465,-0.0308,0.0233,-0.0839,-0.0105,0.0471,-0.0248,0.0108,0.0157,0.0482,0.0201,-0.0164,-0.0505,0.0027,0.0306,0.0488,-0.0073,0.0305,-0.0595,0.0117,0.1473,0.019,0.0539,0.0248,-0.044,-0.0587,0.0029,0.0135,0.0322,-0.0173,0.0114,-0.001,0.0138,-0.0073,0.0121,0.0004,-0.0462,-0.0188,0.0814,-0.0133,0.0341,-0.0226,0.007,0.0031,-0.0005,-0.0066,-0.0407,0.0112,0.0476,0.0362,0.0678,-0.0527,-0.0171,-0.0511,-0.0315,-0.0041,0.0801,-0.0176,-0.0925,-0.046,0.0364,0.0048,-0.0001,0.0077,0.0364,-0.0585,0.0458,0.0776,0.0495,-0.0869,-0.0172,0.0012,0.0103,0.0197,-0.1003,0.0003,-0.0073,0.0425,-0.0333,0.0129,-0.0349,0.0196,0.0472,-0.0009,0.0414,-0.0535,-0.0168,-0.0309,-0.0586,-0.0029,-0.0061,0.0157,-0.0303,-0.0154,-0.0274,-0.0537,-0.0029,-0.0168,0.0063,0.0081,0.0006,0.0762,0.0504,-0.0191,0.0178,0.0253,0.0073,-0.0786,0.0171,0.0021,0.0458,-0.023,0.0238,0.0101,0.0293,-0.0174,-0.2121,-0.014,-0.0517,0.0279,0.0342,-0.1002,0.0526,-0.0364,0.0505,0.0092,0.0501,-0.0128,-0.0063,0.0315,-0.0041,0.0344,0.0334,0.0087,-0.0111,0.0022,-0.0088,0.0182,0.0126,-0.0972,0.0402,-0.029,0.2046,0.0298,0.0659,-0.0052,0.0441,0.0537,-0.0366,-0.0884,0.0257,0.0097,0.1106,-0.0391,-0.0201,-0.0472,0.0128,0.0401,-0.0084,-0.0845,-0.02,-0.0301,-0.0389,0.0766,-0.079,0.011,0.0325,-0.0266,0.0746,-0.0287,-0.0222,-0.0318,-0.1243,0.0494,-0.0423,0.036,0.0117,-0.0469,0.02,-0.0608,0.0613,-0.0037,0.0102,-0.0586,0.0573,0.0369,0.0058,0.0341,0.0038,0.0207,0.0339,0.0301,-0.0037,-0.013,-0.0609,-0.02,0.0484,-0.0208,0.0223,0.0402,0.0066,-0.0213,0.0572,-0.0154,0.0293,-0.0158,-0.0068,0.0245,-0.0587,0.0006,0.0551,-0.0088,-0.3193,0.049,0.0052,0.0812,-0.0104,0.01,0.0322,-0.0182,-0.0512,-0.0102,0.0161,0.0397,0.0718,0.0245,0.0171,0.0241,0.076,-0.0805,0.0203,-0.0686,-0.0096,0.0672,0.2187,-0.0838,0.022,-0.0181,-0.0093,-0.0086,0.0204,-0.0836,-0.0196,0.0293,0.0611,-0.0608,0.0306,0.101,-0.0884,0.0469,0.0003,-0.0265,-0.0601,-0.0032,-0.0187,0.0126,0.0661,0.0102,0.0132,-0.0422,-0.0299,0.0198,-0.0509,0.0304,0.0265,-0.0123,0.0293,0.0097,-0.0482,-0.0395,-0.0184,-0.0153,0.0138,-0.0248,-0.0038,0.0021,-0.0273]}
{"key":"[Optimal and Efficient Algorithms for General Mixable Losses against Switching Oracles] We investigate the problem of online learning, which has gained significant attention in recent years due to its applicability in a wide range of fields from machine learning to game theory. Specifically, we study the online optimization of mixable loss functions in a dynamic environment. We introduce online mixture schemes that asymptotically achieves the performance of the best dynamic estimation sequence of the switching oracle with optimal regret redundancies. The best dynamic estimation sequence that we compete against is selected in hindsight with full observation of the loss functions and is allowed to select different optimal estimations in different time intervals (segments). We propose two mixtures in our work. Firstly, we propose a tractable polynomial time complexity algorithm that can achieve the optimal redundancy of the intractable brute force approach. Secondly, we propose an efficient logarithmic time complexity algorithm that can achieve the optimal redundancy up to a constant multiplicity gap. Our results are guaranteed to hold in a strong deterministic sense in an individual sequence manner.","layer":5,"vector":[-0.071,-0.0214,0.033,-0.0422,-0.0258,0.0466,0.0494,0.0354,0.0642,-0.0227,0.0022,-0.0416,-0.01,0.0531,0.0067,0.0266,-0.027,0.0141,-0.0051,0.0073,0.0365,-0.0292,-0.0279,-0.0583,0.0274,-0.0014,-0.0183,-0.0722,-0.0511,-0.2431,0.0496,-0.0307,0.0027,-0.0381,0.003,-0.0139,-0.0244,0.0611,-0.0564,0.0494,0.0264,0.0568,-0.0042,-0.0284,-0.0251,-0.0461,-0.0161,-0.0133,0.0251,-0.0121,0.0165,0.0135,0.0541,0.0428,0.0344,0.0384,0.0433,0.0988,0.0206,0.0384,0.0313,0.0128,-0.1029,0.0191,0.0301,0.0147,-0.0186,0.0115,0.0074,0.0883,-0.045,0.0437,-0.0191,0.0522,0.0223,0.0048,-0.0088,-0.0443,-0.0303,0.0129,0.009,-0.0315,-0.0462,-0.0008,-0.0406,-0.0776,-0.0023,-0.0312,0.0573,-0.0142,-0.0374,0.0244,0.0053,-0.0024,-0.022,0.0015,0.0435,0.0314,-0.0088,0.2051,-0.0266,0.0637,0.0194,-0.0601,-0.0113,-0.0593,-0.0269,-0.0337,0.0028,-0.0588,-0.0055,-0.0321,0.0789,-0.0297,-0.0062,0.056,0.0599,0.0249,0.0035,0.0386,-0.0183,0.0023,0.0432,-0.0146,0.0462,-0.0532,-0.026,0.137,-0.0201,0.0431,0.015,-0.0469,-0.0337,-0.0407,0.0421,0.0286,0.0336,0.0005,0.0266,-0.0237,-0.0574,-0.054,0.0339,-0.106,0.0244,0.149,0.0092,0.0516,-0.0517,-0.0542,-0.0019,-0.0202,-0.0306,-0.0225,0.0018,0.0166,0.0861,0.0589,-0.0664,-0.0139,-0.0436,-0.0267,0.0132,0.131,-0.0221,-0.087,-0.0039,-0.02,0.0011,-0.0106,0.0252,0.0416,-0.0518,-0.0244,0.0866,-0.004,-0.0741,-0.0321,0.0369,0.0081,0.0129,-0.0114,-0.0476,0.0307,0.0443,-0.0083,0.0236,-0.0286,0.0258,0.0346,-0.0122,0.008,-0.0213,0.0259,-0.0304,-0.016,0.0017,-0.0389,0.0291,0.0234,0.005,-0.0191,-0.0478,0.0228,0.0314,0.0162,0.0059,-0.0006,0.0534,0.0219,-0.0303,0.0032,0.0442,0.0217,-0.0043,0.0183,0.0556,0.019,0.0132,0.0297,0.0239,-0.0289,-0.0049,-0.2356,-0.0394,-0.0252,0.0015,0.0701,-0.0448,0.0262,-0.0448,0.0613,0.0641,0.0482,-0.0003,-0.0101,0.0606,-0.0209,0.0367,0.0166,0.0042,-0.0433,-0.0086,-0.055,0.0245,-0.0132,-0.0756,0.0745,0.0047,0.2496,-0.0127,0.0309,-0.0378,0.0298,0.0404,0.0081,-0.0408,0.0756,0.0235,0.0652,-0.0541,-0.0362,-0.0401,-0.0178,-0.0209,0.0101,-0.1054,-0.0188,-0.0249,-0.0679,-0.0015,-0.0616,0.0141,0.0773,-0.0056,0.0545,-0.0493,0.0116,-0.0524,-0.0807,0.0198,-0.0277,0.0386,-0.0071,-0.0304,-0.0304,-0.0879,0.0834,-0.0256,0.0053,-0.0447,0.0118,-0.0299,-0.0144,0.0309,0.0024,-0.0107,0.0612,-0.0039,0.0147,-0.0451,-0.0734,-0.0104,0.0818,-0.0566,0.0186,0.0562,0.0068,-0.0064,0.0856,0.0113,0.0196,-0.0196,-0.0226,-0.0296,-0.0601,-0.0112,0.0477,-0.0075,-0.3015,0.0536,0.0085,0.0317,-0.0454,0.0513,0.0217,0.0106,-0.0285,-0.0174,0.0145,0.0514,0.0079,-0.0312,0.0512,0.0642,0.071,-0.054,0.0129,-0.0311,0.0128,0.0656,0.2017,-0.064,0.032,-0.0002,-0.0261,0.0283,0.0245,-0.0353,0.0228,0.0085,0.1044,-0.0393,0.03,0.0356,-0.0108,0.0398,0.0044,-0.0506,-0.027,-0.0107,-0.0499,-0.0056,0.1027,-0.0024,-0.0648,-0.0741,-0.0538,0.0225,-0.0509,0.0109,0.0072,-0.0311,-0.0055,0.0257,-0.0478,-0.0586,-0.0258,-0.0632,0.054,-0.0659,-0.0154,-0.0131,-0.0034]}
{"key":"[MiniHack the Planet: A Sandbox for Open-Ended Reinforcement Learning Research] Progress in deep reinforcement learning (RL) is heavily driven by the availability of challenging benchmarks used for training agents. However, benchmarks that are widely adopted by the community are not explicitly designed for evaluating specific capabilities of RL methods. While there exist environments for assessing particular open problems in RL (such as exploration, transfer learning, unsupervised environment design, or even language-assisted RL), it is generally difficult to extend these to richer, more complex environments once research goes beyond proof-of-concept results. We present MiniHack, a powerful sandbox framework for easily designing novel RL environments. MiniHack is a one-stop shop for RL experiments with environments ranging from small rooms to complex, procedurally generated worlds. By leveraging the full set of entities and environment dynamics from NetHack, one of the richest grid-based video games, MiniHack allows designing custom RL testbeds that are fast and convenient to use. With this sandbox framework, novel environments can be designed easily, either using a human-readable description language or a simple Python interface. In addition to a variety of RL tasks and baselines, MiniHack can wrap existing RL benchmarks and provide ways to seamlessly add additional complexity.","layer":1,"vector":[-0.0394,-0.0048,0.0454,-0.0147,0.0192,0.0725,0.0012,0.0225,0.0406,-0.0126,0.0174,-0.0574,0.0501,0.0567,0.0191,0.0141,-0.0349,0.0268,-0.014,-0.0106,0.0242,-0.0556,0.0074,-0.0731,-0.0371,0.0427,-0.0341,-0.0449,-0.0508,-0.2619,0.0355,-0.0382,0.0002,-0.0499,0.0142,-0.0331,-0.0064,0.0422,-0.0537,0.0129,0.047,0.0007,0.0004,-0.0175,-0.0237,-0.067,-0.0146,-0.0799,-0.0148,-0.0365,0.006,-0.047,-0.0005,0.0307,0.045,-0.0064,0.0815,0.0529,0.0539,0.0073,0.0301,0.0203,-0.1537,0.0632,0.0393,0.0612,-0.0587,0.0008,0.0403,0.0445,-0.0244,0.066,0.0205,0.0269,0.0332,0.0102,-0.0053,-0.0583,0.0364,-0.0175,0.003,-0.0425,-0.0428,0.004,-0.0068,-0.0585,-0.0108,-0.0394,0.0328,0.0017,-0.0187,0.0279,0.0108,0.0209,-0.061,0.0348,0.0599,-0.0119,-0.0886,0.222,-0.0185,0.061,-0.0104,0.0229,0.0502,0.0075,-0.0185,-0.0244,0.004,0.0164,-0.0759,-0.0065,0.0577,-0.0024,0.0415,-0.0015,0.0788,0.018,-0.019,-0.0218,-0.0241,-0.0041,0.0479,-0.029,0.0138,-0.0503,0.0376,0.1363,0.0196,0.0103,0.059,-0.0382,-0.0281,-0.0054,0.0194,0.0266,0.0457,0.0305,0.0051,0.0032,-0.0333,0.0386,0.0197,-0.121,-0.0761,0.0826,0.0068,0.0096,-0.0233,-0.0114,0.0027,0.0355,-0.0502,-0.0068,0.0046,0.0449,0.0472,0.0641,-0.0499,0.0026,-0.0176,-0.0188,-0.0358,0.0744,-0.0192,-0.1049,-0.0616,0.0167,-0.0141,-0.0205,0.0473,0.0418,-0.0781,-0.0133,0.0479,0.0163,-0.1045,0.0291,-0.0057,0.0198,0.0191,-0.0221,-0.0351,0.0204,0.0428,-0.0371,0.0189,-0.0457,0.0111,-0.0064,-0.003,0.0323,-0.0132,0.0233,0.0011,-0.0207,0.0067,-0.0408,-0.0163,-0.0121,-0.0086,-0.0161,-0.0381,0.02,-0.0241,0.0001,-0.0153,-0.018,0.071,0.0192,-0.0645,0.0309,0.0305,-0.0339,-0.0496,0.0022,0.0117,-0.0084,-0.0417,0.0218,0.0074,-0.022,-0.0209,-0.2167,0.0304,-0.0273,-0.0431,0.0428,-0.0373,0.0601,-0.0097,0.0358,0.0284,0.0452,-0.0588,-0.0173,0.048,-0.013,0.053,0.0066,0.0009,-0.0126,0.0163,0.0086,-0.0205,-0.0325,-0.1069,0.0464,-0.0213,0.2456,0.0224,0.0337,-0.0135,0.0245,0.0427,-0.0099,-0.1467,0.0512,0.0393,0.1048,0.0173,-0.0401,-0.0315,-0.0285,0.0484,0.0003,-0.1312,-0.0284,-0.0024,-0.0633,0.056,-0.0581,-0.0197,-0.0073,-0.0167,0.0177,-0.0043,-0.0179,-0.0248,-0.054,0.0394,-0.0319,0.0207,-0.01,-0.0214,0.0305,-0.0291,0.0657,-0.0073,0.0074,-0.052,0.0815,-0.017,-0.0301,0.0307,-0.0346,0.0215,0.0322,-0.0073,0.025,0.0093,-0.0342,-0.013,0.0494,-0.0245,0.0433,0.0298,-0.0034,-0.0125,0.0674,-0.0146,0.0668,-0.0192,0.0044,0.0489,-0.0504,-0.0241,0.0574,-0.0032,-0.3008,0.054,0.0373,0.0203,-0.0174,0.0004,0.026,0.0448,-0.0422,-0.008,0.0312,0.0277,0.0406,0.0368,0.0138,0.0123,0.1041,-0.0287,0.0506,-0.0848,0.0089,0.07,0.2205,-0.0551,0.0173,-0.007,-0.0417,-0.0072,0.0314,-0.0377,-0.0215,0.0288,0.0886,-0.0606,0.0322,0.0787,-0.0255,-0.002,0.0297,0.0239,-0.011,0.0025,0.0238,0.0085,0.0635,-0.0341,-0.0191,-0.0559,-0.0148,0.0243,-0.0317,0.0073,-0.0061,-0.0223,0.0609,0.0235,-0.0046,-0.0583,-0.0419,-0.0537,0.0069,-0.0685,0.0434,-0.048,-0.0224]}
{"key":"[A Unified Analysis of First-Order Methods for Smooth Games via Integral Quadratic Constraints] The theory of integral quadratic constraints (IQCs) allows the certification of exponential convergence of interconnected systems containing nonlinear or uncertain elements. In this work, we adapt the IQC theory to study first-order methods for smooth and strongly-monotone games and show how to design tailored quadratic constraints to get tight upper bounds of convergence rates. Using this framework, we recover the existing bound for the gradient method~(GD), derive sharper bounds for the proximal point method~(PPM) and optimistic gradient method~(OG), and provide \\emph{for the first time} a global convergence rate for the negative momentum method~(NM) with an iteration complexity $\\mathcal{O}(\\kappa^{1.5})$, which matches its known lower bound. In addition, for time-varying systems, we prove that the gradient method with optimal step size achieves the fastest provable worst-case convergence rate with quadratic Lyapunov functions. Finally, we further extend our analysis to stochastic games and study the impact of multiplicative noise on different algorithms. We show that it is impossible for an algorithm with one step of memory to achieve acceleration if it only queries the gradient once per batch (in contrast with the stochastic strongly-convex optimization setting, where such acceleration has been demonstrated). However, we exhibit an algorithm which achieves acceleration with two gradient queries per batch.","layer":1,"vector":[-0.0885,-0.0282,0.0414,-0.0105,-0.0055,0.0331,0.022,0.0163,0.0805,0.0276,-0.0002,-0.0251,0.0429,0.086,-0.0167,0.0552,0.007,0.0166,-0.0472,0.0521,0.0411,-0.063,-0.024,-0.0931,0.0533,0.0117,-0.0652,-0.0826,-0.0065,-0.2707,0.0515,-0.0527,0.016,-0.025,0.0035,-0.0205,-0.0323,0.0522,-0.0413,0.0194,0.03,0.074,-0.0229,-0.0404,-0.0173,-0.0841,-0.0402,-0.0009,-0.0345,-0.0262,-0.0018,-0.0376,0.0235,-0.0355,0.0091,0.0322,0.0339,0.0575,0.0543,0.0384,0.0047,0.006,-0.156,0.0838,0.0818,0.0133,-0.0146,-0.023,0.0141,0.0693,-0.0126,0.04,0.0258,0.0553,0.0156,-0.0257,0.008,-0.05,-0.0105,0.0532,0.0196,-0.0505,-0.0447,-0.034,0.0244,-0.0534,0.0014,-0.0451,0.0535,-0.0005,-0.0015,-0.0154,0.0114,-0.0075,-0.0465,-0.0014,0.0103,0.0254,-0.041,0.1851,-0.0162,0.0464,-0.0136,-0.0125,0.0269,-0.0158,-0.032,-0.0598,-0.0295,-0.0065,-0.0483,-0.0168,0.0877,-0.0305,-0.0004,0.0669,0.0228,0.0124,0.0036,-0.0173,-0.0492,0.0123,0.0333,0.0332,0.0264,-0.0569,-0.017,0.1244,0.0019,0.0394,0.0133,-0.0146,-0.0281,-0.0332,0.0035,0.0166,-0.003,0.0247,0.0837,-0.0028,-0.0377,-0.0956,0.0062,-0.1781,-0.0531,0.1041,0.0246,0.0401,-0.0239,-0.0154,0.0329,-0.0257,0.0078,-0.008,0.013,-0.0064,0.0498,0.0401,-0.0713,-0.0098,-0.0788,-0.0293,0.0139,0.1137,-0.016,-0.0659,-0.0106,0.013,-0.0005,-0.0249,0.0471,0.0388,-0.019,-0.0143,0.0706,0.019,-0.0738,0.002,-0.0141,0.0149,-0.0062,-0.0216,-0.0128,0.0239,0.0226,-0.0347,0.0043,-0.0236,0.0246,0.0063,-0.0715,-0.0273,-0.0316,0.0544,-0.0102,-0.0053,0.0188,-0.031,0.0469,-0.0396,-0.0127,-0.0227,-0.0655,0.0664,0.0048,-0.0396,-0.0222,-0.0006,0.0004,0.0276,-0.0795,-0.0294,0.0531,-0.0582,-0.0423,0.0382,0.0516,0.0378,-0.0249,0.0772,0.0226,0.0219,-0.0321,-0.1959,-0.0295,-0.0633,-0.0117,0.064,-0.063,0.0287,-0.0726,0.0416,0.041,0.0691,-0.0206,-0.034,0.0545,0.0071,0.0432,0.0083,0.0021,0.0075,-0.0216,0.0116,-0.0173,-0.0485,-0.0751,0.0531,0.0056,0.2351,0.0036,0.0222,-0.0163,0.0217,0.0203,-0.0039,-0.029,0.0558,0.06,0.0701,-0.0579,-0.0073,0.0071,-0.0053,0.0167,-0.012,-0.0779,-0.0324,-0.0133,-0.0575,0.0299,-0.0509,-0.0203,0.0705,-0.0128,0.0741,-0.0247,-0.0088,-0.0398,-0.062,0.029,0.0092,0.0146,-0.0296,-0.0217,-0.0041,-0.0206,0.0787,0.0356,-0.0155,0.0211,0.0622,-0.0254,0.0157,0.0264,0.0192,0.0229,0.0482,0.0398,0.0311,0.0013,-0.04,-0.0404,0.0389,-0.0543,0.0609,0.0175,0.0065,-0.0356,0.0585,-0.0113,0.034,0.007,-0.0013,0.0186,-0.079,0.0029,0.0347,-0.0142,-0.2927,0.0447,0.0297,0.0112,-0.0526,0.019,0.0545,-0.0304,-0.0631,0.0155,0.0089,0.1013,0.0004,0.0431,0.0079,-0.0188,0.0201,-0.0253,0.0802,-0.09,0.0366,0.0196,0.2347,-0.0462,0.0169,0.0415,-0.0227,0.0155,0.023,-0.0233,0.0192,-0.0055,0.0546,-0.0335,0.0469,0.0644,-0.0238,0.0485,0.0245,0.0037,-0.0606,0.0244,-0.0023,-0.0011,0.0374,-0.0041,-0.0517,-0.0428,0.025,0.0093,-0.0652,0.0205,0.0383,-0.0418,0.0264,0.011,-0.0471,-0.0554,-0.0155,-0.0111,0.0229,-0.0493,0.0036,-0.0007,-0.0141]}
{"key":"[Order in the Court: Explainable AI Methods Prone to Disagreement] By computing the rank correlation between attention weights and feature-additive explanation methods, previous analyses either invalidate or support the role of attention-based explanations as a faithful and plausible measure of salience. To investigate whether this approach is appropriate, we compare LIME, Integrated Gradients, DeepLIFT, Grad-SHAP, Deep-SHAP, and attention-based explanations, applied to two neural architectures trained on single- and pair-sequence language tasks. In most cases, we find that none of our chosen methods agree. Based on our empirical observations and theoretical objections, we conclude that rank correlation does not measure the quality of feature-additive methods. Practitioners should instead use the numerous and rigorous diagnostic methods proposed by the community.","layer":3,"vector":[-0.0841,-0.0116,0.01,0.001,0.0092,-0.0268,0.0762,0.0344,0.0776,-0.0504,0.0134,-0.0417,0.0328,0.0495,0.0387,-0.0047,0.0118,0.0011,-0.0678,-0.023,0.021,0.0049,-0.0192,-0.0507,-0.039,0.0313,-0.0438,-0.0152,-0.0352,-0.2335,0.0402,-0.0496,0.0536,-0.0363,-0.018,0.0102,0.001,0.0303,-0.0598,0.0128,0.007,0.0118,-0.0443,-0.0193,-0.0549,-0.0542,0.0053,-0.001,-0.0297,-0.036,0.0267,-0.0349,-0.0156,0.0164,0.0246,0.0347,0.0667,0.0353,0.032,0.0224,0.0391,0.0647,-0.1823,0.0609,0.0823,0.0221,-0.0358,-0.0565,0.0167,0.0711,-0.0131,0.0475,0.0051,0.0486,0.0344,-0.0175,-0.0138,-0.0205,0.0197,0.0036,0.0329,-0.0097,-0.0736,-0.0161,0.0233,-0.0134,0.0188,-0.0042,0.0261,-0.0394,0.0002,0.0064,-0.0313,-0.0313,-0.0152,0.0349,0.0279,0.0069,-0.0771,0.1482,-0.0446,0.013,0.0354,-0.0363,0.043,-0.028,-0.008,-0.0448,-0.0115,0.0005,-0.0494,0.0166,0.0196,-0.0111,0.0311,0.0373,0.0763,0.0054,0.014,-0.0407,-0.0237,-0.0072,0.0356,-0.0425,0.0159,-0.0832,0.0272,0.1166,0.0158,-0.0122,0.0564,-0.0443,-0.0456,0.0079,0.0093,-0.0027,0.0419,0.0159,0.0392,0.0008,-0.0221,-0.0696,-0.0105,-0.0741,-0.0424,0.1357,-0.0667,0.0181,-0.0148,0.031,-0.0183,-0.0055,-0.0178,-0.0957,-0.0124,0.0073,0.0072,0.0122,-0.0568,0.0296,0.0517,-0.0428,-0.0502,0.0746,0.038,-0.0584,-0.0326,0.0071,0.006,-0.0346,0.053,0.017,-0.0415,0.026,0.0517,0.0194,-0.0728,0.0213,0.0084,-0.017,0.0363,-0.0739,-0.0201,0.0602,0.0651,-0.0638,0.037,-0.0699,0.0388,0.0488,-0.0265,-0.005,-0.0313,0.0426,-0.0444,0.0032,-0.0091,-0.0283,0.0086,-0.0183,-0.0325,0.0094,-0.0342,-0.0006,-0.0199,0.0381,-0.0122,0.0166,0.0781,0.0587,-0.0348,0.0243,0.0018,-0.0279,0.0056,-0.0012,0.0307,0.0026,-0.015,0.0231,0.0176,-0.0464,-0.0587,-0.2712,0.0076,0.0158,-0.0511,0.0617,-0.1147,0.0744,0.0088,0.0319,0.0689,0.0401,-0.0123,0.0079,-0.0029,-0.0134,0.0766,-0.0204,0.0103,-0.0402,0.0182,0.0114,0.0066,0.0182,-0.0936,0.0192,-0.005,0.2101,0.0482,0.0356,0.0015,-0.005,0.0458,-0.0243,-0.1457,0.0696,0.0099,0.0532,-0.0376,0.007,0.0157,-0.0325,0.0525,0.0173,-0.1011,-0.0243,0.015,0.0026,0.031,-0.0125,0.0338,0.0404,-0.0267,0.046,0.0157,-0.0044,-0.0194,-0.1217,-0.0091,-0.0369,0.0097,-0.0107,-0.0219,0.0621,-0.0383,0.0575,0.0013,-0.0322,-0.0309,0.0288,0.0144,-0.0342,0.1152,0.0023,-0.0115,0.0456,0.0276,0.0653,-0.0093,-0.0484,0.0068,0.0569,-0.0057,0.0228,-0.0129,0.0403,0.005,0.0597,-0.0491,0.0227,-0.0207,-0.0286,0.0574,-0.0279,-0.0802,0.0324,-0.0264,-0.3074,0.0216,-0.0103,0.0182,-0.0223,0.0171,0.0314,0.0077,-0.0206,-0.0233,-0.0102,0.0186,0.0314,-0.0312,-0.0182,0.0545,0.0332,-0.0571,0.0764,-0.0297,-0.015,0.0564,0.2179,0.0085,0.021,-0.0233,-0.0386,-0.086,0.0048,-0.0159,0.0434,-0.0004,0.0478,-0.051,0.0481,0.0842,-0.0064,0.0546,0.0178,0.0082,-0.0063,0.0062,-0.0396,-0.0676,0.1611,0.0245,-0.0182,-0.0592,0.0132,0.0275,-0.0026,-0.0093,-0.0387,-0.0252,0.0274,0.0088,-0.0142,0.0035,-0.0131,-0.0086,-0.0035,-0.055,0.019,0.0447,0.0253]}
{"key":"[Performance Evaluation of Deep Learning Networks for Semantic Segmentation of Traffic Stereo-Pair Images] Semantic image segmentation is one the most demanding task, especially for analysis of traffic conditions for self-driving cars. Here the results of application of several deep learning architectures (PSPNet and ICNet) for semantic image segmentation of traffic stereo-pair images are presented. The images from Cityscapes dataset and custom urban images were analyzed as to the segmentation accuracy and image inference time. For the models pre-trained on Cityscapes dataset, the inference time was equal in the limits of standard deviation, but the segmentation accuracy was different for various cities and stereo channels even. The distributions of accuracy (mean intersection over union - mIoU) values for each city and channel are asymmetric, long-tailed, and have many extreme outliers, especially for PSPNet network in comparison to ICNet network. Some statistical properties of these distributions (skewness, kurtosis) allow us to distinguish these two networks and open the question about relations between architecture of deep learning networks and statistical distribution of the predicted results (mIoU here). The results obtained demonstrated the different sensitivity of these networks to: (1) the local street view peculiarities in different cities that should be taken into account during the targeted fine tuning the models before their practical applications, (2) the right and left data channels in stereo-pairs. For both networks, the difference in the predicted results (mIoU here) for the right and left data channels in stereo-pairs is out of the limits of statistical error in relation to mIoU values. It means that the traffic stereo pairs can be effectively used not only for depth calculations (as it is usually used), but also as an additional data channel that can provide much more information about scene objects than simple duplication of the same street view images.","layer":1,"vector":[-0.0247,-0.0159,0.0395,-0.0005,0.0429,0.0429,0.0573,0.0076,-0.0049,-0.0091,0.0108,-0.0746,0.0354,0.0441,0.0314,0.0016,0.0071,0.0091,-0.0031,0.0207,0.02,-0.0996,-0.0044,-0.0551,0.0064,0.0465,0.0379,-0.0145,-0.0371,-0.2067,0.0203,-0.0403,0.032,-0.0032,-0.0414,-0.0535,-0.0374,0.0472,-0.0029,-0.001,0.0446,-0.0029,-0.0092,-0.0256,-0.009,-0.0164,-0.0009,-0.0247,-0.0032,-0.0474,0.048,-0.0345,0.0643,0.0322,-0.0153,0.0433,0.0687,0.0717,0.0304,0.0714,0.0143,0.0372,-0.1901,0.0405,0.0507,0.0114,-0.0315,-0.062,-0.0046,0.0357,0.0064,0.0236,0.0206,0.0286,0.036,-0.011,-0.0096,-0.0287,-0.0206,-0.0072,-0.0065,-0.0017,-0.038,-0.0362,-0.0006,-0.0206,-0.004,-0.0387,0.0133,-0.0373,-0.0267,-0.0186,-0.007,0.0271,-0.036,0.0244,0.0059,-0.0176,-0.0137,0.1929,-0.0675,0.0538,0.0701,-0.0181,0.017,0.0015,-0.0252,-0.0337,-0.0264,-0.0073,0.0171,-0.0008,0.033,0.0058,0.0133,0.0079,0.061,0.0591,-0.005,0.0102,-0.0554,0.0187,0.0436,-0.0474,0.035,-0.0548,0.0252,0.1165,0.0653,0.0534,0.0235,0.0059,-0.0405,0.0049,-0.0052,-0.0091,0.013,-0.01,-0.0142,-0.0329,0.0047,-0.0929,0.04,-0.0468,0.0089,0.0925,-0.0846,0.0462,-0.0418,-0.0107,-0.0009,0.0408,-0.0533,-0.031,0.0279,-0.0145,0.0322,0.0665,-0.0266,0.0369,-0.0036,-0.0506,-0.0622,0.1311,-0.0013,-0.0787,-0.0424,0.0149,-0.0155,-0.0302,0.0244,0.0089,-0.0192,0.0506,0.0835,0.0439,-0.0821,0.0292,-0.041,0.0072,0.0031,-0.0169,-0.0155,0.0441,0.0188,-0.0675,-0.0513,-0.0454,-0.0189,0.0819,-0.004,0.0057,-0.0404,-0.0043,-0.0085,-0.0331,-0.0077,-0.0018,-0.0389,-0.0234,-0.0136,0.0029,0.0142,-0.0234,-0.0423,0.0092,-0.0469,0.0269,-0.015,0.0182,-0.0115,-0.0196,0.0608,0.0037,0.0006,-0.0221,0.0215,0.0128,-0.009,0.0688,0.0322,-0.0539,-0.053,-0.1841,0.015,0.0183,-0.0317,0.0602,-0.0421,0.0088,0.0319,0.005,0.0634,0.0974,-0.0157,0.0002,0.0153,-0.006,0.0519,-0.0161,0.0917,-0.0509,-0.0002,-0.0387,0.056,-0.0199,-0.1009,0.0753,0.0007,0.1924,-0.0073,0.0604,-0.0109,0.0002,0.02,-0.0108,-0.0878,0.0573,-0.0155,0.0887,0.0081,-0.073,-0.0598,-0.0485,-0.0129,0.0169,-0.0636,-0.0496,-0.0471,-0.0015,0.0581,0.0015,0.0042,-0.0233,-0.0647,-0.0011,0.0182,-0.005,-0.0043,-0.0705,0.0369,-0.0519,-0.0301,0.001,-0.0314,0.0373,-0.0906,0.0765,0.0065,-0.0276,-0.088,0.0012,0.0318,-0.046,0.1376,-0.0014,0.0017,0.0757,-0.0086,0.0474,-0.0032,-0.0223,-0.0191,0.0569,-0.0571,0.0284,0.0175,0.0515,0.0297,0.0755,0.0062,-0.016,-0.0273,0.0795,0.0288,-0.0503,-0.0412,0.0569,-0.0128,-0.3036,0.0667,-0.0161,0.0438,-0.0234,0.0014,0.0641,0.0054,-0.0507,-0.056,-0.0098,0.0407,0.0285,-0.0298,-0.0163,0.005,0.0364,-0.0062,0.0797,0.0023,0.0099,0.034,0.2448,-0.0818,0.0352,0.0407,-0.0399,-0.0053,0.0155,-0.0239,0.0051,-0.0037,0.0945,-0.0834,0.0182,0.107,-0.052,0.0131,0.0049,0.0287,-0.0216,0.0087,-0.0215,-0.0403,0.1157,-0.0007,0.0143,-0.0338,0.0017,0.0387,-0.0304,-0.0141,-0.0333,0.0072,0.0647,-0.0091,-0.07,-0.0709,-0.0798,0.001,0.0021,-0.1331,-0.0102,0.0148,-0.0188]}
{"key":"[Asymmetric Impurity Functions, Class Weighting, and Optimal Splits for Binary Classification Trees] We investigate how asymmetrizing an impurity function affects the choice of optimal node splits when growing a decision tree for binary classification. In particular, we relax the usual axioms of an impurity function and show how skewing an impurity function biases the optimal splits to isolate points of a particular class when splitting a node. We give a rigorous definition of this notion, then give a necessary and sufficient condition for such a bias to hold. We also show that the technique of class weighting is equivalent to applying a specific transformation to the impurity function, and tie all these notions together for a class of impurity functions that includes the entropy and Gini impurity. We also briefly discuss cost-insensitive impurity functions and give a characterization of such functions.","layer":5,"vector":[-0.0195,-0.0177,0.0312,-0.0665,0.034,0.0111,0.0578,0.0212,0.0442,0.0206,0.0203,-0.0375,0.0078,0.0373,0.0414,0.0245,0.0249,0.0125,-0.0632,0.045,0.0445,-0.0379,-0.0128,-0.0604,0.066,0.0123,-0.0445,-0.0111,-0.0632,-0.2354,0.0338,-0.0434,0.0302,-0.0232,-0.0048,-0.0665,-0.0147,0.0284,-0.066,0.0266,0.0266,0.0205,-0.0301,-0.0438,-0.0145,-0.0595,-0.004,-0.0266,-0.0304,-0.027,0.0021,-0.0401,0.0068,0.038,0.0131,0.0332,0.0471,0.0384,0.0288,0.0657,0.0097,0.0478,-0.1102,0.0278,0.0555,0.0248,-0.0541,-0.0594,0.0214,0.067,-0.001,0.0453,0.0001,0.0494,-0.0218,-0.0055,-0.0143,-0.0144,-0.0233,-0.0015,-0.0188,-0.0555,-0.04,0.0043,0.0105,-0.0503,0.0146,-0.0687,0.0282,0.0234,-0.008,-0.0192,0.0042,0.0154,-0.0616,0.0068,0.0262,0.0298,-0.0898,0.2009,-0.0426,0.0274,0.0283,-0.0838,0.0115,-0.0274,-0.0246,-0.0815,-0.0357,-0.0303,0.0044,-0.0297,0.0498,-0.0058,-0.0151,0.0068,0.054,0.0491,0.0025,0.0065,0.007,0.0008,0.0573,-0.0134,0.0163,-0.0351,-0.0309,0.1681,-0.0059,0.0391,0.008,-0.0528,-0.0358,0.0037,0.0172,-0.0032,0.0167,0.0463,0.0448,-0.0054,-0.0248,-0.0513,0.0292,-0.0728,-0.0258,0.1181,-0.0499,0.0774,-0.0054,-0.0004,-0.0073,-0.0285,-0.0209,-0.0525,0.0122,0.0279,0.022,0.0289,-0.0665,0.015,0.0216,-0.0188,0.0164,0.124,0.0203,-0.058,-0.0215,0.0405,-0.0113,-0.0559,0.0367,0.0263,-0.0195,0.0452,0.0749,0.0067,-0.0411,-0.0607,-0.0081,-0.0004,0.0316,-0.0459,-0.0626,0.0341,0.0252,-0.0039,-0.0242,-0.0629,0.0316,0.0824,-0.0668,0.0068,-0.0155,-0.0404,-0.0261,-0.0514,-0.0139,0.0078,-0.0033,-0.0114,0.0267,0.037,-0.0076,0.0069,-0.0229,0.0519,-0.0074,-0.0044,0.0291,0.0172,-0.0324,0.0145,0.0266,0.0031,-0.0449,-0.0191,0.0647,0.0141,0.0322,0.007,0.035,-0.0654,-0.0332,-0.2319,-0.0313,-0.0102,0.0052,0.0748,-0.0655,0.0148,-0.0101,0.0062,0.0415,0.064,0.0063,-0.0292,0.0201,-0.0543,0.0411,0.074,0.0158,-0.0391,0.0065,-0.0182,0.0541,0.0332,-0.0887,0.0051,0.0166,0.213,-0.0058,0.0582,0.0039,0.0013,-0.0099,-0.0079,-0.0375,0.0699,0.0211,0.0261,-0.0097,-0.0693,-0.0468,-0.0032,0.0267,-0.0321,-0.1096,-0.0044,-0.0484,-0.051,0.0191,-0.0547,0.0409,0.0375,0.0045,0.0536,0.0323,0.0151,-0.0531,-0.0499,0.0015,-0.0162,0.0492,0.0389,-0.0442,-0.0166,-0.0469,0.0411,0.0039,-0.0584,-0.0216,0.0212,-0.0381,-0.031,0.0733,-0.0249,-0.0883,0.0618,0.0584,0.0393,-0.0019,-0.0361,-0.0445,0.0877,-0.0452,0.0202,-0.0018,0.0222,0.0296,0.0682,-0.016,0.0194,0.0056,0.0039,0.0269,-0.0536,0.0425,0.0397,-0.0113,-0.3076,0.0494,-0.003,0.026,-0.0338,-0.0047,0.0788,-0.0438,-0.0527,0.0025,0.0583,0.061,0.0155,-0.023,-0.0013,0.0219,0.0656,-0.0619,0.0246,-0.0194,0.0343,0.0539,0.2471,-0.0276,0.0707,0.0218,-0.0187,0.0095,-0.0019,-0.0149,0.0632,-0.0108,0.0519,-0.0621,0.0417,0.0985,-0.0103,0.0498,0.0509,-0.0453,-0.0236,-0.0133,-0.0889,-0.0385,0.1203,-0.0415,-0.0037,-0.0586,0.0284,0.0129,-0.0336,0.0546,-0.018,0.0062,0.0302,0.0423,-0.0452,-0.0469,-0.0641,-0.0344,-0.0056,-0.079,-0.0139,0.0065,0.0017]}
{"key":"[Lessons from the AdKDD'21 Privacy-Preserving ML Challenge] Designing data sharing mechanisms providing performance and strong privacy guarantees is a hot topic for the Online Advertising industry. Namely, a prominent proposal discussed under the Improving Web Advertising Business Group at W3C only allows sharing advertising signals through aggregated, differentially private reports of past displays. To study this proposal extensively, an open Privacy-Preserving Machine Learning Challenge took place at AdKDD'21, a premier workshop on Advertising Science with data provided by advertising company Criteo. In this paper, we describe the challenge tasks, the structure of the available datasets, report the challenge results, and enable its full reproducibility. A key finding is that learning models on large, aggregated data in the presence of a small set of unaggregated data points can be surprisingly efficient and cheap. We also run additional experiments to observe the sensitivity of winning methods to different parameters such as privacy budget or quantity of available privileged side information. We conclude that the industry needs either alternate designs for private data sharing or a breakthrough in learning with aggregated data only to keep ad relevance at a reasonable level.","layer":3,"vector":[-0.0034,-0.0342,0.0109,-0.0063,0.0191,0.0405,0.0512,0.0449,0.0287,-0.0395,0.0385,-0.0306,0.0445,0.0229,0.0342,0.0322,0.0051,0.0252,-0.0534,0.0224,0.0242,-0.0137,-0.0259,-0.0585,0.0403,0.0215,-0.0317,-0.0337,-0.0505,-0.2296,0.008,-0.0757,0.0662,-0.0176,0.0453,-0.0258,-0.0402,0.0479,-0.0579,0.0403,0.0081,0.0198,-0.0194,-0.0379,-0.0402,-0.035,-0.0025,-0.0392,-0.0486,-0.01,0.0324,-0.0199,0.028,0.0608,0.0465,0.0123,0.0376,0.0065,-0.0009,0.0603,0.0313,0.0274,-0.1649,0.055,0.0173,0.089,-0.0333,-0.0006,-0.0196,0.0408,-0.0044,0.0234,0.0119,0.0091,-0.0322,0.0438,-0.019,-0.0406,-0.0438,-0.0257,-0.0171,-0.0232,0.0027,0.0096,-0.0352,-0.0763,0.0733,-0.0792,0.0583,-0.0312,-0.0433,0.0119,-0.0219,0.0193,-0.0593,-0.0123,0.0058,0.0344,-0.0371,0.189,-0.0829,0.0766,0.0048,-0.0429,0.0094,-0.0447,-0.0129,0.004,-0.045,0.0342,-0.0163,-0.0395,0.0483,-0.0351,0.0262,0.0309,0.037,0.0457,0.0006,-0.0644,-0.0004,0.0204,0.0516,0.0274,0.019,-0.0203,0.0116,0.1402,0.0082,0.0344,0.0097,-0.0295,-0.0178,-0.0113,0.0192,-0.0297,0.0023,0.0436,0.0617,0.0032,-0.0589,-0.0321,0.0193,-0.0729,-0.0462,0.1653,0.0117,0.0603,-0.0404,-0.0177,0.0303,0.0175,-0.0106,0.0073,0.001,0.0068,0.0405,0.0727,-0.0578,0.0392,0.0045,-0.0556,-0.01,0.1048,0.0085,-0.1135,-0.0058,0.0243,0.0096,-0.0185,0.0667,0.0386,-0.0497,0.0344,0.0267,-0.0143,-0.0769,0.0112,-0.0127,-0.0098,-0.0205,-0.0604,0.0038,0.0287,0.0491,-0.0601,-0.0017,-0.027,0.0174,0.0854,-0.0344,0.014,-0.049,-0.0043,-0.0541,-0.0437,0.0003,0.0124,0.0557,-0.0177,-0.0468,-0.0136,-0.0591,0.0052,-0.0152,0.0246,0.0244,-0.0432,0.0699,-0.0109,-0.0503,0.011,0.0231,-0.0437,-0.0364,-0.0023,0.0043,0.0341,-0.0014,0.0454,0.0514,-0.0098,-0.0422,-0.2496,-0.0267,0.0118,0.0197,0.0192,-0.0814,0.0588,-0.0087,0.0611,0.0752,0.0721,-0.0317,-0.0545,0.0198,-0.022,0.0453,0.0286,0.0566,-0.0319,-0.0111,-0.0012,0.0149,0.021,-0.06,0.0469,0.0299,0.2292,0.0298,0.0134,-0.0401,0.038,0.0327,-0.0503,-0.1345,0.0038,0.0073,0.0269,0.0147,-0.0456,-0.0126,-0.0152,0.0093,-0.0113,-0.1241,-0.0143,-0.0395,-0.0381,0.0211,-0.0458,0.0497,0.0137,0.0062,0.0756,-0.0113,-0.019,-0.0344,-0.0362,0.0116,-0.021,0.0802,0.0232,-0.0496,0.0033,-0.0605,0.0438,0.0037,-0.0577,-0.0777,0.0168,-0.0211,-0.0334,0.0497,0.0276,-0.0241,0.0337,0.0201,0.026,-0.0564,-0.0426,-0.0064,0.0762,0.0036,0.0388,0.0326,0.0354,0.0325,0.0991,0.0094,0.0195,-0.0174,0.0013,0.0025,-0.0483,-0.0364,0.0511,-0.0077,-0.3198,-0.0053,-0.0436,0.0505,-0.0259,0.0524,0.0407,0.0471,-0.0859,0.0031,-0.0249,0.0524,0.0348,-0.0122,0.0216,0.0273,0.0551,-0.024,0.0106,-0.0137,0.0478,0.0386,0.1992,-0.0253,-0.0065,-0.0077,0.0075,0.0099,0.0115,-0.0437,0.017,0.0056,0.0836,-0.0292,0.0466,0.0603,-0.0597,0.0158,0.0453,-0.0012,-0.0019,-0.0087,-0.0467,0.0031,0.0884,0.0225,-0.0254,-0.0289,-0.0213,0.0135,-0.0139,-0.0299,-0.0295,-0.0185,0.0514,0.0149,-0.0772,-0.0381,-0.0165,-0.0428,-0.0166,-0.031,-0.045,0.0129,-0.0226]}
{"key":"[Multi-camera Torso Pose Estimation using Graph Neural Networks] Estimating the location and orientation of humans is an essential skill for service and assistive robots. To achieve a reliable estimation in a wide area such as an apartment, multiple RGBD cameras are frequently used. Firstly, these setups are relatively expensive. Secondly, they seldom perform an effective data fusion using the multiple camera sources at an early stage of the processing pipeline. Occlusions and partial views make this second point very relevant in these scenarios. The proposal presented in this paper makes use of graph neural networks to merge the information acquired from multiple camera sources, achieving a mean absolute error below 125 mm for the location and 10 degrees for the orientation using low-resolution RGB images. The experiments, conducted in an apartment with three cameras, benchmarked two different graph neural network implementations and a third architecture based on fully connected layers. The software used has been released as open-source in a public repository (https://github.com/vangiel/WheresTheFellow).","layer":2,"vector":[0.0083,-0.0442,0.0451,-0.0515,0.0519,0.0656,0.0207,0.0446,-0.0068,-0.0138,0.0634,-0.1233,0.0307,0.0846,-0.0008,-0.0064,0.008,0.0517,0.0024,-0.0109,-0.0089,-0.0367,0.0085,-0.0503,0.0178,0.0022,-0.0258,-0.0771,-0.0425,-0.2074,0.0205,-0.037,0.0818,-0.0513,-0.0083,-0.033,0.0196,0.0596,-0.0068,0.0305,-0.0101,-0.0153,-0.0211,-0.0788,0.0136,0.0178,-0.0087,0.0247,-0.0246,-0.0205,0.0466,-0.02,0.0472,0.0278,0.0449,0.0175,0.0282,0.0313,-0.0002,0.048,0.0224,0.0324,-0.1798,0.0447,0.056,0.0459,-0.0281,-0.0215,0.0419,0.0196,-0.0099,0.026,0.0503,0.0192,-0.0291,0.0,0.042,-0.0326,-0.0095,-0.0325,-0.0071,0.0052,-0.0019,0.0155,-0.0039,0.0158,0.0162,-0.0387,-0.0138,0.0263,-0.0363,0.0038,-0.0582,0.0218,-0.0747,-0.0377,0.0294,0.0146,-0.0398,0.2176,-0.0101,0.0205,0.0801,-0.0079,0.0432,-0.0439,-0.0309,-0.0085,-0.0705,0.0095,-0.0312,-0.0502,0.0075,-0.0155,0.044,0.0205,0.0359,0.0545,0.0104,-0.0386,-0.0123,0.0256,0.0633,-0.0142,0.0026,-0.0796,0.0094,0.131,0.0455,0.032,0.0638,-0.0114,0.0039,-0.0407,-0.0121,0.024,0.0685,0.0337,0.0277,-0.0169,-0.0059,-0.0722,0.0281,-0.0567,-0.0398,0.1059,-0.0744,0.0112,-0.0368,-0.0262,-0.0474,0.0732,-0.0122,0.0122,0.0087,0.0276,0.0186,0.0426,-0.0725,0.0298,-0.043,-0.0023,-0.0845,0.0852,0.0352,-0.1072,-0.0463,0.0157,-0.0284,-0.0159,0.0551,0.033,-0.0182,0.0043,0.0975,0.053,-0.0913,-0.0177,0.0126,0.0159,-0.012,-0.0414,-0.0477,-0.0123,0.0044,-0.0214,0.0259,-0.0289,0.019,0.0801,-0.0252,0.0363,-0.0358,0.0036,-0.0341,0.001,-0.0253,-0.0168,-0.0044,-0.0226,0.0423,-0.0321,-0.0089,-0.0174,-0.0394,0.0001,0.0053,0.0,0.0034,0.0503,-0.0262,0.0002,0.0822,-0.026,-0.0202,0.0253,0.0142,0.0219,-0.0088,0.0236,0.0288,-0.0532,-0.0144,-0.2338,0.0468,0.0194,0.0157,0.0411,-0.08,-0.0104,0.0228,0.0344,0.0315,0.0898,-0.0205,0.0036,0.0379,0.0158,0.0754,0.0479,0.0406,-0.0542,-0.0653,-0.0065,0.0087,-0.0013,-0.0771,0.0586,-0.005,0.2335,0.0077,0.0204,-0.0167,0.0167,0.0399,-0.0324,-0.0957,0.0946,0.0734,0.0304,-0.0136,-0.0013,-0.0289,-0.081,0.0177,0.0426,-0.0527,-0.0352,-0.0292,-0.0528,0.0298,-0.0334,-0.0213,0.0017,-0.0364,0.0179,-0.0009,-0.0419,-0.0441,-0.0719,0.0227,-0.0114,0.0245,0.0057,-0.095,0.0571,-0.0438,0.082,0.0064,0.0016,-0.0632,0.0127,-0.0172,-0.0099,0.0786,0.0408,-0.0007,0.0571,-0.0188,0.0581,-0.0088,-0.018,-0.0301,0.0004,-0.0445,-0.011,0.0118,0.0674,-0.0313,0.0662,-0.0245,0.0211,-0.0538,0.0119,0.0063,-0.0433,0.0065,0.0442,0.0047,-0.296,0.0304,0.0109,0.0258,-0.0423,0.0073,0.0062,0.0001,-0.0251,0.0051,-0.0066,0.0287,0.0423,-0.0158,-0.0097,0.0583,0.0141,-0.0455,0.0618,-0.0779,0.0165,0.0495,0.194,-0.0603,-0.009,0.0782,-0.0569,-0.0409,-0.0111,-0.0239,-0.0271,0.0135,0.0348,-0.04,0.0386,0.0945,-0.006,0.0339,0.0244,0.015,-0.0151,-0.0104,-0.0082,-0.0461,0.1249,0.0206,-0.0333,-0.0124,0.0134,0.0058,-0.0465,-0.0023,-0.0372,0.0112,0.0405,-0.0137,-0.0638,-0.0437,-0.1138,-0.0639,0.0532,-0.0688,-0.014,-0.0374,-0.0101]}
{"key":"[A Numerical Transform of Random Forest Regressors corrects Systematically-Biased Predictions] Over the past decade, random forest models have become widely used as a robust method for high-dimensional data regression tasks. In part, the popularity of these models arises from the fact that they require little hyperparameter tuning and are not very susceptible to overfitting. Random forest regression models are comprised of an ensemble of decision trees that independently predict the value of a (continuous) dependent variable; predictions from each of the trees are ultimately averaged to yield an overall predicted value from the forest. Using a suite of representative real-world datasets, we find a systematic bias in predictions from random forest models. We find that this bias is recapitulated in simple synthetic datasets, regardless of whether or not they include irreducible error (noise) in the data, but that models employing boosting do not exhibit this bias. Here we demonstrate the basis for this problem, and we use the training data to define a numerical transformation that fully corrects it. Application of this transformation yields improved predictions in every one of the real-world and synthetic datasets evaluated in our study.","layer":5,"vector":[-0.0107,-0.0212,0.0394,-0.009,0.0906,-0.0004,-0.0029,0.0344,0.0286,-0.0007,0.0171,-0.0604,0.0048,0.0243,0.0404,0.0456,0.0018,0.0314,-0.0737,0.0027,0.021,-0.0269,-0.0097,-0.0466,0.0305,-0.0011,-0.0345,-0.0302,-0.0844,-0.2594,0.0085,-0.0881,0.0287,-0.0073,0.0003,0.0332,-0.0216,0.051,-0.0206,0.0697,-0.0186,0.0082,-0.0255,-0.0271,-0.0236,-0.0524,-0.0268,-0.0022,-0.0539,0.0017,-0.0041,-0.0189,-0.0025,0.0152,0.0053,0.0193,0.0509,0.0474,0.0573,0.0366,0.0346,0.035,-0.1604,0.0516,0.0494,0.0361,-0.0481,-0.0452,-0.0131,0.0469,-0.012,0.0242,0.0163,-0.011,0.0302,-0.0085,0.0141,-0.0087,0.0314,0.0126,0.0142,-0.0339,-0.0555,-0.0042,0.008,-0.0507,0.0203,-0.0539,0.0698,0.0329,-0.0243,0.0055,-0.0641,0.0194,-0.0595,0.0151,0.0682,0.0136,-0.047,0.1894,-0.0617,0.0273,-0.01,-0.0231,0.0205,-0.0669,-0.0199,-0.0127,-0.0108,-0.0012,0.0099,-0.0395,0.0241,-0.0268,-0.0414,-0.0096,0.0771,0.0661,-0.0189,0.007,-0.0383,0.0233,0.0399,-0.0337,0.0369,-0.0525,0.0201,0.1483,0.0528,0.004,0.032,-0.0296,-0.0781,-0.045,-0.015,0.0281,0.0379,0.0195,0.0282,0.0447,-0.0385,-0.0264,-0.0215,-0.04,-0.0925,0.1201,-0.0511,0.0148,-0.0452,-0.0194,-0.0494,0.0245,-0.0311,-0.0399,0.035,0.0314,0.011,0.0463,-0.0451,0.0175,-0.0037,-0.0337,0.0003,0.0427,-0.0121,-0.0337,-0.0257,0.0222,0.0194,-0.0001,0.0234,0.0086,0.0003,0.0586,0.0758,0.0123,-0.0381,0.0221,-0.0012,0.0087,0.0508,-0.0308,-0.0329,0.0826,0.0248,-0.021,-0.0232,-0.0467,0.047,0.0605,-0.0306,0.0178,-0.0512,-0.0017,0.0288,-0.0582,-0.0214,-0.0116,0.0566,-0.0277,0.0081,0.0243,-0.0211,0.0033,-0.0166,0.0251,0.0117,-0.0082,0.075,0.0137,-0.017,0.012,0.0772,-0.0199,-0.0716,0.0183,0.0355,0.0928,0.0124,0.072,0.0067,-0.036,-0.0633,-0.2319,-0.0115,0.0524,-0.0113,0.0391,-0.0471,-0.0113,-0.0209,0.0247,0.1021,0.0161,-0.0017,-0.0405,0.0698,-0.0026,0.0464,-0.0104,-0.0116,-0.0669,0.0261,-0.0184,0.0127,0.0146,-0.0826,0.0491,0.0123,0.2069,0.0135,0.0407,-0.008,0.0747,0.0037,-0.0063,-0.1129,0.0906,0.0354,0.0468,-0.0368,-0.0326,0.0016,0.0077,0.0423,0.0052,-0.1234,-0.072,0.0019,-0.0556,0.017,-0.0878,0.0201,0.0059,-0.0058,0.1062,-0.012,0.0339,-0.0412,-0.121,0.0645,-0.0106,-0.0138,0.0263,-0.0832,0.0156,-0.0952,0.0157,-0.0227,-0.0155,-0.022,0.042,-0.025,-0.0479,0.1092,-0.0147,-0.0247,0.0568,0.008,-0.0077,-0.0186,-0.0468,-0.0225,0.046,-0.0387,0.0408,0.0092,0.0335,-0.0144,0.1065,-0.0388,0.0509,-0.0577,-0.0147,0.0099,-0.0642,-0.0258,0.0568,0.0431,-0.2657,0.0016,-0.0009,0.0249,0.0117,-0.007,0.0429,0.0275,-0.0286,0.0213,-0.0127,0.0342,0.0857,-0.0096,0.0301,0.018,0.0592,-0.0232,0.05,-0.0643,0.0195,-0.0095,0.2011,-0.0149,0.0187,0.05,-0.0356,-0.027,0.0384,-0.0342,0.0151,0.0389,0.0882,-0.0435,0.0532,0.0608,-0.0546,-0.0027,0.0086,-0.031,0.0195,-0.0013,-0.0301,-0.0175,0.0922,-0.04,-0.0106,-0.0434,0.019,0.0492,-0.0814,0.0112,-0.0318,-0.0274,0.0029,0.0233,-0.0273,-0.0077,-0.0702,-0.0211,0.0031,-0.0543,-0.0834,-0.0311,-0.0235]}
{"key":"[Lipschitz Continuity of Mahalanobis Distances and Bilinear Forms] Many theoretical results in the machine learning domain stand only for functions that are Lipschitz continuous. Lipschitz continuity is a strong form of continuity that linearly bounds the variations of a function. In this paper, we derive tight Lipschitz constants for two families of metrics: Mahalanobis distances and bounded-space bilinear forms. To our knowledge, this is the first time the Mahalanobis distance is formally proved to be Lipschitz continuous and that such tight Lipschitz constants are derived.","layer":0,"vector":[-0.0488,-0.0321,0.0225,0.0059,-0.0076,0.044,0.0404,0.0651,0.0288,-0.0538,-0.0213,-0.0836,0.0388,0.0249,0.0387,0.0163,0.0296,0.0552,-0.073,0.0549,0.0872,0.0055,-0.0028,-0.0546,0.0414,0.0044,-0.0196,-0.0206,-0.0321,-0.2308,-0.0016,-0.0552,0.0459,-0.0535,0.018,-0.0001,-0.0211,0.0455,-0.0232,0.0451,0.038,0.0324,0.0118,-0.0583,-0.0306,-0.0505,0.0139,0.0055,-0.0563,-0.0388,0.0117,-0.0566,0.0433,0.027,0.0431,0.0437,0.0356,0.0083,0.0184,0.0173,0.0226,-0.0122,-0.1567,0.0336,0.0085,-0.0091,-0.0247,-0.0476,-0.0039,0.0863,-0.0076,0.0286,-0.0109,0.0611,-0.0207,-0.0163,0.0103,-0.0257,-0.067,0.0443,0.0132,-0.0465,-0.0645,0.0093,-0.0276,-0.0369,0.0462,-0.062,0.0118,0.0192,-0.0612,-0.0233,0.0034,0.0473,-0.0962,-0.0418,0.0202,0.0206,-0.0232,0.189,-0.0645,0.033,0.0309,-0.0394,0.0397,0.0394,-0.0245,-0.0295,-0.0191,-0.0242,-0.0324,-0.0418,0.0036,-0.0048,-0.0347,0.0014,0.0616,0.0315,-0.0037,0.0076,-0.0665,0.0535,0.0132,-0.0106,0.0239,-0.0477,0.0217,0.1184,0.0537,0.0548,0.0588,-0.0188,-0.0676,-0.0166,0.0247,0.0593,0.01,0.0242,0.0421,0.0037,-0.0599,-0.0385,0.0446,-0.0656,-0.0444,0.1511,-0.0357,0.0762,-0.0426,-0.0026,-0.0057,-0.0124,-0.0157,0.001,0.0231,0.0582,0.0229,0.0343,-0.0568,0.049,-0.0111,-0.0329,0.0124,0.1277,0.012,-0.0711,-0.0197,-0.001,0.0157,0.0331,0.0448,0.0494,-0.0345,0.0484,0.0737,-0.0015,-0.0643,0.0054,0.0209,0.0065,-0.0134,-0.0434,-0.0489,0.0292,0.0439,-0.0499,0.0045,-0.0494,0.0653,0.1009,-0.0422,0.0101,-0.0176,-0.0359,-0.0183,-0.0512,-0.0208,-0.033,0.0131,-0.0305,0.0323,-0.0019,-0.0073,0.0216,0.024,-0.0436,0.0247,0.0073,0.0502,0.0348,-0.0399,-0.0243,0.0034,-0.0469,-0.0309,0.0168,0.0808,0.015,-0.0113,0.0323,0.0277,0.0126,-0.0675,-0.2397,-0.0386,0.0258,-0.0175,0.0588,-0.0823,0.0348,0.021,0.0234,0.0395,0.0601,-0.0214,-0.0482,0.0361,-0.0788,0.0548,0.0373,0.0112,-0.0534,-0.0147,-0.0601,0.0258,-0.0482,-0.0805,0.0809,-0.0079,0.2048,0.0104,0.0683,-0.0498,-0.0012,0.0224,-0.0253,-0.0661,0.0319,-0.0028,0.0335,-0.0427,0.007,-0.0577,0.0017,0.0199,-0.0138,-0.0322,-0.0066,-0.0072,-0.0395,0.0152,-0.0586,-0.0002,0.0312,-0.0352,0.0685,-0.0324,0.0267,-0.0277,-0.0725,-0.0127,-0.0333,0.0564,-0.0652,-0.0672,0.0186,-0.0136,0.0686,0.0209,-0.0233,-0.0214,0.0139,-0.0239,-0.0135,0.0554,-0.0059,-0.0735,0.064,-0.0344,0.0649,0.0179,-0.0447,-0.0187,0.0503,-0.0449,0.0497,0.0098,0.0242,-0.0002,0.0894,-0.0273,0.0352,-0.0071,-0.0025,-0.006,-0.0609,0.0091,0.0426,-0.0059,-0.2678,0.0244,-0.0071,-0.0004,-0.0393,0.0181,0.0657,0.0107,-0.0954,-0.014,0.0227,0.076,0.0301,-0.0187,0.0156,0.0046,0.0579,-0.0602,0.0428,-0.0882,0.0039,0.066,0.2315,-0.011,0.0222,0.0233,-0.0192,-0.0008,0.0567,-0.0415,-0.0056,-0.0049,0.0683,-0.0535,0.0315,0.1072,-0.0378,0.0688,-0.0043,-0.0201,0.0134,0.0133,-0.0639,-0.0396,0.094,0.0052,-0.0145,-0.0337,-0.0068,0.0173,-0.0172,0.0186,0.0226,0.04,0.0207,0.0504,-0.0311,-0.0504,-0.0342,-0.0404,0.0037,-0.0769,-0.0177,-0.0418,0.0049]}
{"key":"[Clustering as an Evaluation Protocol for Knowledge Embedding Representation of Categorised Multi-relational Data in the Clinical Domain] Learning knowledge representation is an increasingly important technology applicable in many domain-specific machine learning problems. We discuss the effectiveness of traditional Link Prediction or Knowledge Graph Completion evaluation protocol when embedding knowledge representation for categorised multi-relational data in the clinical domain. Link prediction uses to split the data into training and evaluation subsets, leading to loss of information along training and harming the knowledge representation model accuracy. We propose a Clustering Evaluation Protocol as a replacement alternative to the traditionally used evaluation tasks. We used embedding models trained by a knowledge embedding approach which has been evaluated with clinical datasets. Experimental results with Pearson and Spearman correlations show strong evidence that the novel proposed evaluation protocol is pottentially able to replace link prediction.","layer":5,"vector":[-0.046,-0.0304,0.029,0.0411,0.0128,0.013,0.0086,0.0481,0.0203,-0.0071,-0.017,-0.0643,0.0183,0.0828,0.0429,0.0317,0.0105,0.0883,-0.0505,0.0125,0.0045,-0.0167,0.0059,-0.0565,0.0423,0.0509,-0.0029,-0.0372,-0.0499,-0.1817,0.0273,-0.0519,0.0362,0.0155,-0.0226,-0.0404,-0.0022,0.0699,-0.0166,0.0529,0.0373,-0.0041,0.0125,-0.0397,-0.0071,-0.0459,-0.0489,-0.0127,0.0069,-0.0318,0.0216,-0.0312,-0.0318,0.0302,0.0215,0.0405,0.0275,0.0163,0.0157,0.0709,0.0722,0.0797,-0.1764,0.0417,0.0323,0.0341,-0.0254,-0.0119,0.0252,0.0778,0.0234,0.0135,0.0321,0.0713,0.0123,0.0263,-0.0003,0.0089,-0.0381,0.0154,0.0034,-0.0383,-0.0353,-0.0252,0.011,-0.0464,-0.0152,-0.0903,-0.0273,0.0205,-0.0433,-0.0297,-0.0598,0.0176,-0.0835,-0.0236,0.0426,0.0299,-0.0669,0.1834,-0.0471,0.0158,0.0483,-0.0577,0.0054,-0.0828,0.0049,-0.0414,-0.0289,-0.0241,-0.0189,-0.0213,0.0064,-0.0536,0.0509,0.0432,0.0786,0.0535,-0.0297,-0.0319,-0.0094,0.0117,0.0259,0.0093,0.0347,-0.0588,0.0188,0.109,0.0372,0.0148,0.0422,-0.002,-0.0465,0.031,-0.0091,0.0166,-0.0017,-0.0361,-0.0046,-0.01,0.0216,-0.0825,0.0091,-0.0803,-0.0633,0.1415,-0.0522,-0.0057,-0.0236,0.0004,-0.0074,0.0358,-0.0311,-0.0364,-0.015,0.0067,0.0382,0.0097,-0.0392,0.0295,0.0067,-0.0922,-0.0632,0.1154,0.0203,-0.1034,-0.0315,-0.0211,0.0213,-0.0596,0.0551,0.0659,-0.0442,0.0606,0.0707,0.0038,-0.047,-0.0132,0.0107,-0.0537,0.0512,-0.0035,-0.0216,0.0725,0.0299,-0.0371,-0.0278,0.0045,0.0539,0.0302,-0.0314,-0.0169,-0.0344,-0.0262,-0.0272,-0.0102,-0.0149,-0.028,-0.0098,-0.023,0.0185,0.011,0.0021,0.0259,-0.0472,0.0326,-0.0095,0.031,0.0634,-0.0018,-0.0347,0.0001,0.0771,-0.0332,-0.011,0.004,0.0141,0.0255,0.0081,0.0525,0.0325,0.0036,-0.0338,-0.2228,0.0031,0.0343,0.0044,0.0327,-0.0605,-0.0134,0.0378,0.0082,0.0627,0.0731,-0.0087,-0.0306,-0.0288,-0.0685,0.0123,0.0885,0.0331,-0.0605,0.0328,-0.0172,0.0418,-0.0011,-0.0567,0.012,-0.0201,0.2423,0.0123,-0.0094,-0.0037,0.0194,0.0308,-0.0727,-0.162,0.0815,-0.0261,0.0117,0.0032,-0.0151,-0.0113,-0.0821,0.0207,-0.0146,-0.0935,-0.0056,-0.0382,-0.0035,0.0526,-0.0502,0.0498,0.0118,-0.0473,0.0186,0.0138,-0.0513,-0.0301,-0.1069,0.0292,-0.0505,-0.0055,0.0522,-0.033,0.002,-0.0628,0.0833,-0.0463,-0.0411,0.0104,0.0478,-0.0643,-0.0273,0.091,-0.0129,-0.02,0.0365,0.0021,0.0135,-0.0187,-0.0513,0.0044,0.0615,-0.0566,0.0456,0.0477,0.0234,0.0107,0.074,0.0081,0.0409,-0.0297,0.0166,-0.0183,-0.0344,-0.0075,0.033,0.0048,-0.2912,0.0579,0.0232,0.0337,-0.0416,0.0028,0.0288,-0.0044,-0.0667,-0.0219,0.02,0.0191,0.047,-0.0088,-0.0306,0.0194,0.0444,-0.0618,0.0341,-0.0303,0.0229,0.0371,0.2032,-0.0047,0.0191,-0.0026,-0.0566,-0.0075,0.0248,0.0306,0.0311,0.0159,0.0781,-0.0125,0.0673,0.0507,-0.0273,0.0298,0.0747,-0.0005,0.0518,-0.0109,-0.0615,-0.0358,0.0998,0.044,0.0172,-0.0793,-0.0004,0.034,-0.0189,0.0092,-0.005,-0.0108,0.0488,0.0255,-0.0117,-0.0109,-0.0458,-0.0741,-0.0486,-0.0571,-0.0335,0.0456,-0.0078]}
{"key":"[Denoising Auto-encoding Priors in Undecimated Wavelet Domain for MR Image Reconstruction] Compressive sensing is an impressive approach for fast MRI. It aims at reconstructing MR image using only a few under-sampled data in k-space, enhancing the efficiency of the data acquisition. In this study, we propose to learn priors based on undecimated wavelet transform and an iterative image reconstruction algorithm. At the stage of prior learning, transformed feature images obtained by undecimated wavelet transform are stacked as an input of denoising autoencoder network (DAE). The highly redundant and multi-scale input enables the correlation of feature images at different channels, which allows a robust network-driven prior. At the iterative reconstruction, the transformed DAE prior is incorporated into the classical iterative procedure by the means of proximal gradient algorithm. Experimental comparisons on different sampling trajectories and ratios validated the great potential of the presented algorithm.","layer":4,"vector":[-0.0482,-0.0002,0.0379,-0.0296,0.0436,0.0391,0.0303,0.0242,0.0526,-0.0258,0.0298,-0.0971,0.054,0.0488,-0.0057,0.0055,-0.0021,0.079,-0.0269,0.0078,0.022,-0.0444,0.03,-0.0297,0.0243,-0.0143,-0.0038,-0.0413,-0.0455,-0.2377,0.0279,-0.0635,0.073,-0.034,0.0435,-0.0392,-0.024,0.0293,-0.0688,0.062,0.0362,0.0463,-0.0182,-0.0511,0.0048,-0.0441,0.0177,-0.0631,0.0364,-0.0407,0.1045,-0.0024,0.0165,0.0653,0.0482,0.0125,0.0439,0.0333,0.0203,0.0576,0.0567,0.0585,-0.1936,0.0597,0.0526,0.0144,-0.0393,-0.0521,0.0187,0.0629,-0.0029,0.0374,0.0005,0.0663,0.0309,-0.0016,-0.0213,-0.0284,-0.0561,0.0068,0.0469,-0.0119,-0.0166,0.0147,-0.051,-0.0603,0.0259,-0.09,0.0245,0.0133,-0.047,-0.0225,-0.0352,0.0082,-0.0832,-0.0377,-0.0413,0.0556,-0.0182,0.1978,-0.0575,0.0375,0.083,-0.04,0.0104,-0.0587,-0.049,0.0171,-0.0304,0.0228,-0.0329,-0.0276,0.0697,-0.066,0.0525,0.0005,0.0218,0.0009,-0.0215,0.0069,-0.0229,-0.0041,0.0383,-0.0392,0.0433,-0.0746,-0.01,0.1251,0.041,0.0078,0.0402,-0.0106,-0.0066,-0.0157,0.0134,-0.0201,0.0181,-0.0371,0.0215,-0.0171,-0.0205,-0.0686,0.0182,-0.0423,-0.0845,0.0904,-0.033,0.0394,-0.0495,-0.0354,-0.0129,0.0292,-0.0474,-0.006,0.0065,0.0195,-0.0108,0.0468,-0.0643,0.0025,-0.0038,-0.1018,-0.0459,0.1188,0.0549,-0.0813,-0.0486,-0.0192,0.037,-0.0244,0.0021,-0.0129,-0.0307,0.0245,0.1198,0.0266,-0.0257,0.0257,-0.0024,0.0042,0.0315,-0.0262,-0.011,0.025,0.0476,-0.0388,0.0088,0.0029,0.0358,0.0163,-0.0136,-0.0001,-0.0209,-0.0028,-0.0554,-0.0141,-0.0206,-0.006,-0.0272,-0.0243,0.0395,-0.0311,-0.0134,0.0414,0.0159,0.0177,0.0099,0.0077,0.0057,0.0259,-0.0026,-0.0382,0.0844,-0.0344,0.0024,0.003,0.0269,0.0411,0.0101,0.0653,0.0793,-0.08,-0.0471,-0.2136,-0.0459,0.0116,0.012,0.0391,-0.0669,0.0136,-0.0224,0.1151,0.0596,0.0511,0.0472,0.0243,0.0253,-0.0203,0.0362,0.0406,0.0355,0.0114,-0.0008,-0.0242,0.0444,0.0216,-0.0402,0.0698,-0.0079,0.1897,-0.0117,0.0368,0.0056,-0.027,0.0279,-0.0226,-0.0819,0.0648,0.0252,0.0492,0.0,-0.0459,-0.0365,-0.0304,-0.0463,-0.0236,-0.0703,-0.0323,-0.0402,-0.0634,0.0457,-0.0367,0.0141,0.0536,-0.0298,0.0099,-0.083,-0.0246,-0.0419,-0.1074,0.0092,-0.0322,0.0123,0.0572,-0.063,-0.0486,-0.0651,0.0423,0.0322,0.0014,-0.0167,-0.0006,-0.025,0.002,0.0941,0.0314,0.0269,0.0563,0.0283,0.0521,-0.0175,-0.0521,-0.0347,0.068,-0.0183,0.0095,0.0644,0.06,0.0518,0.0414,0.0,-0.0639,-0.0349,-0.0421,0.0131,-0.0535,-0.064,0.0235,-0.022,-0.2865,-0.0209,0.0429,0.0103,-0.0072,0.0443,0.0706,-0.0019,-0.0626,-0.0252,-0.0578,0.0253,0.0181,-0.0298,0.0144,0.0372,0.0605,-0.0837,0.0568,-0.0817,0.0115,0.0397,0.1476,-0.0087,0.0234,0.0479,-0.0279,-0.0025,-0.0118,0.0155,0.0167,0.0279,0.0382,-0.0551,0.0533,0.101,-0.0644,0.0649,-0.0192,-0.0121,0.0351,0.0326,-0.0242,-0.0294,0.073,-0.0019,-0.0324,-0.038,-0.0262,-0.0065,-0.0029,0.025,0.0156,-0.0281,0.0361,0.056,-0.0207,-0.0331,-0.0419,-0.0194,-0.0076,-0.0531,-0.0372,0.0013,-0.0125]}
{"key":"[Adversarial Robustness Study of Convolutional Neural Network for Lumbar Disk Shape Reconstruction from MR images] Machine learning technologies using deep neural networks (DNNs), especially convolutional neural networks (CNNs), have made automated, accurate, and fast medical image analysis a reality for many applications, and some DNN-based medical image analysis systems have even been FDA-cleared. Despite the progress, challenges remain to build DNNs as reliable as human expert doctors. It is known that DNN classifiers may not be robust to noises: by adding a small amount of noise to an input image, a DNN classifier may make a wrong classification of the noisy image (i.e., in-distribution adversarial sample), whereas it makes the right classification of the clean image. Another issue is caused by out-of-distribution samples that are not similar to any sample in the training set. Given such a sample as input, the output of a DNN will become meaningless. In this study, we investigated the in-distribution (IND) and out-of-distribution (OOD) adversarial robustness of a representative CNN for lumbar disk shape reconstruction from spine MR images. To study the relationship between dataset size and robustness to IND adversarial attacks, we used a data augmentation method to create training sets with different levels of shape variations. We utilized the PGD-based algorithm for IND adversarial attacks and extended it for OOD adversarial attacks to generate OOD adversarial samples for model testing. The results show that IND adversarial training can improve the CNN robustness to IND adversarial attacks, and larger training datasets may lead to higher IND robustness. However, it is still a challenge to defend against OOD adversarial attacks.","layer":5,"vector":[0.0117,-0.0414,-0.0014,-0.0219,0.0355,0.0127,0.0586,0.0245,0.0114,-0.0029,-0.0002,-0.0563,0.026,0.0904,-0.0308,0.0412,0.0132,0.0279,-0.0455,0.0486,0.0181,-0.0322,0.0165,-0.0657,0.0072,0.004,0.0151,-0.0512,-0.0695,-0.2615,0.006,-0.0407,0.0543,-0.0431,0.0345,-0.0155,-0.0531,0.0457,-0.0455,0.0059,0.0262,0.0184,-0.0419,-0.0754,0.0197,-0.0438,-0.0204,-0.0003,0.0286,-0.0184,0.0734,-0.0339,0.0256,0.0236,0.0363,-0.0,0.0495,0.0111,-0.001,0.1091,0.0323,0.064,-0.1807,0.0498,0.0368,0.0238,-0.0465,-0.0382,0.0114,0.0283,0.0129,0.0331,0.0168,0.0437,0.0066,0.0336,0.0075,-0.0367,-0.004,0.0345,0.0677,-0.0164,-0.0219,0.0167,-0.0221,-0.0607,0.0258,-0.0688,0.0452,0.0016,-0.0278,0.0169,-0.0383,0.0199,-0.0698,-0.0368,0.0183,0.0226,-0.0756,0.2017,-0.0491,-0.0046,0.0577,-0.0092,0.0217,-0.0287,-0.0075,-0.037,-0.0269,0.0261,-0.0313,-0.0347,0.0214,-0.0171,-0.0129,-0.0023,0.051,0.0189,-0.0329,-0.0278,-0.0362,-0.0056,0.0608,-0.0149,0.0518,-0.077,0.0245,0.1394,0.0579,0.0253,-0.0139,-0.0534,-0.042,-0.0086,0.0192,0.003,0.0095,0.0022,0.0206,-0.014,0.001,-0.0739,0.0484,-0.0516,-0.0235,0.0968,-0.0471,0.0503,-0.0282,-0.0051,-0.0016,0.0484,-0.0656,-0.0126,0.0216,0.0033,-0.012,0.0295,-0.0501,-0.0238,-0.0166,-0.0791,-0.0494,0.1169,0.0176,-0.0541,-0.0263,0.006,0.0645,0.0264,0.0787,0.0175,-0.0162,0.0164,0.087,0.0007,-0.09,-0.0212,-0.0346,0.0079,0.0236,-0.0327,-0.0209,0.0164,0.0489,-0.0657,-0.002,-0.0052,0.0386,0.0536,-0.0128,0.0232,-0.1182,-0.0251,-0.0457,-0.0569,-0.0309,-0.0032,-0.033,-0.0106,-0.0183,-0.0153,-0.0281,0.0296,-0.0041,0.0161,-0.0198,-0.0158,-0.0233,0.0868,-0.051,-0.0108,0.048,-0.0115,-0.0332,-0.0026,0.0292,0.0243,-0.0201,0.0344,0.041,-0.052,-0.0266,-0.2463,-0.0141,-0.0154,0.0111,0.0628,-0.0776,0.0477,-0.008,0.0495,0.0251,0.0403,0.029,-0.0267,-0.018,0.0062,0.0725,0.0187,0.0038,-0.0248,-0.0329,-0.0251,0.0563,-0.0001,-0.0459,0.0386,0.0358,0.2006,0.0123,0.0419,0.0181,-0.0145,0.0406,-0.0106,-0.0777,0.0539,-0.0052,0.0418,0.0199,-0.0413,-0.0082,-0.0423,0.0227,0.0262,-0.0909,-0.0384,-0.0366,-0.06,0.0341,-0.0637,0.0412,0.0361,-0.0224,0.0717,0.0175,0.0236,-0.0241,-0.0954,0.0542,-0.0389,-0.0065,-0.024,-0.0691,0.0191,-0.0998,0.0276,0.0011,-0.0189,-0.0294,0.0679,-0.0264,0.029,0.0997,0.0385,0.0273,0.0742,0.0033,0.0445,-0.0226,-0.0629,-0.0445,0.0327,0.011,0.0188,0.0343,0.0743,-0.0284,0.0396,0.0159,0.0175,-0.059,0.0402,0.0449,-0.0507,-0.0289,0.0279,0.0094,-0.2819,0.0338,0.0107,0.0583,-0.0262,0.0361,0.0076,0.0363,-0.0053,-0.0149,-0.0212,0.0283,0.0165,-0.0734,0.0138,0.0109,0.0579,-0.0726,0.0659,-0.0468,-0.0145,0.0275,0.2104,-0.0516,0.0154,0.0119,-0.0043,0.0243,-0.0252,-0.0182,0.0093,-0.0187,0.0451,-0.0176,0.0173,0.1543,-0.0664,0.0335,-0.0051,-0.022,-0.0094,0.0103,-0.027,0.053,0.0889,-0.0044,-0.0292,-0.018,0.0186,0.034,-0.0493,-0.0204,-0.0189,0.0033,0.0259,0.0459,-0.0101,-0.0601,-0.0198,-0.0329,0.0332,-0.0357,-0.0267,0.0166,-0.0175]}
{"key":"[Minimizing Negative Transfer of Knowledge in Multivariate Gaussian Processes: A Scalable and Regularized Approach] Recently there has been an increasing interest in the multivariate Gaussian process (MGP) which extends the Gaussian process (GP) to deal with multiple outputs. One approach to construct the MGP and account for non-trivial commonalities amongst outputs employs a convolution process (CP). The CP is based on the idea of sharing latent functions across several convolutions. Despite the elegance of the CP construction, it provides new challenges that need yet to be tackled. First, even with a moderate number of outputs, model building is extremely prohibitive due to the huge increase in computational demands and number of parameters to be estimated. Second, the negative transfer of knowledge may occur when some outputs do not share commonalities. In this paper we address these issues. We propose a regularized pairwise modeling approach for the MGP established using CP. The key feature of our approach is to distribute the estimation of the full multivariate model into a group of bivariate GPs which are individually built. Interestingly pairwise modeling turns out to possess unique characteristics, which allows us to tackle the challenge of negative transfer through penalizing the latent function that facilitates information sharing in each bivariate model. Predictions are then made through combining predictions from the bivariate models within a Bayesian framework. The proposed method has excellent scalability when the number of outputs is large and minimizes the negative transfer of knowledge between uncorrelated outputs. Statistical guarantees for the proposed method are studied and its advantageous features are demonstrated through numerical studies.","layer":0,"vector":[-0.0253,-0.0665,0.008,-0.0224,0.0268,0.0076,0.0457,-0.003,0.0456,-0.0343,0.0099,-0.0538,0.0005,0.063,0.041,0.0386,0.0075,0.0467,-0.0694,0.0244,0.0198,-0.0491,-0.0174,-0.0248,0.0403,0.0074,-0.0196,-0.052,-0.0669,-0.2719,0.0232,-0.0424,0.0481,0.0375,0.0054,-0.0105,-0.0202,0.0544,-0.0077,0.0444,0.0243,0.011,-0.0441,-0.0142,-0.014,-0.0322,-0.0425,-0.0216,-0.0399,-0.0584,0.022,-0.0348,-0.0135,0.0245,0.0513,0.0521,0.0416,-0.0061,0.0243,0.0231,0.0037,0.0567,-0.1965,0.0445,0.0526,0.0371,-0.0709,-0.0226,0.0156,0.0192,-0.0487,0.0472,-0.014,0.0424,0.0039,-0.0188,-0.0218,0.0203,-0.061,0.0154,0.0107,0.0216,-0.0098,-0.0189,-0.0197,-0.0434,0.007,-0.0278,0.0459,0.0016,-0.0297,0.0016,-0.0493,0.0699,-0.0485,0.0003,0.0413,0.0308,-0.0074,0.1868,-0.0733,0.0118,0.0868,-0.0151,0.0289,0.0234,-0.0675,-0.0288,0.011,0.0046,-0.0274,-0.0318,0.0021,-0.0507,0.0334,0.0175,0.0494,0.0209,-0.012,-0.0501,-0.0337,0.0462,0.0566,-0.002,0.025,-0.0306,0.0089,0.1422,0.0471,0.0125,0.0788,-0.012,-0.0647,0.0082,0.0174,0.0036,0.0323,-0.0145,0.0331,-0.0102,-0.0324,-0.0344,0.0197,-0.0797,-0.0056,0.1676,-0.0404,0.0111,-0.08,-0.0244,-0.0019,-0.0079,-0.0088,-0.0155,0.0296,-0.0097,-0.0014,0.0046,-0.0556,0.0389,-0.018,-0.0001,-0.0244,0.0783,0.0408,-0.0736,-0.0473,-0.0045,0.0387,0.0152,0.0409,0.0327,-0.0354,-0.0196,0.095,0.0171,-0.0742,0.0107,0.0399,-0.0231,0.0111,-0.057,-0.0036,0.0383,0.0658,-0.0253,0.0304,-0.0477,-0.0113,0.0598,-0.0554,-0.009,-0.0494,0.0088,0.0109,-0.0099,-0.0508,-0.0117,0.0338,-0.0518,-0.0079,0.0111,-0.079,-0.0072,-0.0205,0.033,-0.0098,0.0417,0.0394,0.0361,0.0055,-0.0264,0.0883,0.0063,-0.0065,0.0119,-0.0384,0.096,0.0034,0.0334,0.0193,-0.0589,-0.0526,-0.2489,-0.0082,0.0264,0.0274,0.0641,-0.0491,0.0266,-0.0105,0.0392,0.0729,0.0936,-0.023,-0.0402,0.0067,-0.0294,0.0407,0.035,0.0726,-0.0016,0.0148,-0.0389,-0.003,-0.0237,-0.0854,0.0754,0.0051,0.1757,0.0316,0.0236,-0.0319,-0.0107,0.0301,-0.0354,-0.0752,0.0345,0.0543,0.058,-0.0123,-0.0191,-0.0052,-0.0472,-0.0315,0.0152,-0.0862,-0.0582,-0.0044,-0.0101,0.0341,-0.0829,0.0076,0.0348,-0.0357,0.0637,-0.0432,-0.0032,-0.0463,-0.0939,0.0752,-0.0398,0.0173,0.0691,-0.0316,0.0484,-0.0771,0.0484,-0.0248,-0.0413,-0.0462,-0.0055,-0.0081,-0.0092,0.1047,0.0231,0.0205,0.0481,0.0021,0.0356,-0.052,-0.0872,-0.0305,0.0835,-0.0603,0.0255,0.0399,0.0326,-0.0172,0.0693,0.0143,0.0113,-0.0734,-0.0323,-0.0473,-0.0331,0.0221,0.02,-0.0118,-0.2778,0.0238,0.0153,0.0298,-0.0081,-0.0072,0.0032,0.0155,-0.0322,0.012,-0.0325,0.0225,0.0497,-0.0199,0.0276,-0.0054,0.039,-0.0488,0.0028,-0.0604,0.0061,0.0433,0.1998,-0.0304,0.0243,0.0235,-0.0252,0.0217,0.0368,-0.0242,0.0385,0.0469,0.0812,-0.0529,0.0237,0.0927,-0.0172,0.0859,0.0191,-0.0122,-0.0035,0.0123,0.0023,-0.0361,0.1065,-0.0067,-0.0199,-0.033,-0.0376,0.0502,-0.0243,0.0346,0.0128,-0.0142,0.0305,0.0038,-0.0214,-0.0514,-0.0159,-0.0103,-0.0264,-0.0654,-0.0417,-0.0001,-0.0438]}
{"key":"[Resolving the Human Subjects Status of Machine Learning's Crowdworkers] In recent years, machine learning (ML) has come to rely more heavily on crowdworkers, both for building bigger datasets and for addressing research questions requiring human interaction or judgment. Owing to the diverse tasks performed by crowdworkers, and the myriad ways the resulting datasets are used, it can be difficult to determine when these individuals are best thought of as workers, versus as human subjects. These difficulties are compounded by conflicting policies, with some institutions and researchers treating all ML crowdwork as human subjects research, and other institutions holding that ML crowdworkers rarely constitute human subjects. Additionally, few ML papers involving crowdwork mention IRB oversight, raising the prospect that many might not be in compliance with ethical and regulatory requirements. In this paper, we focus on research in natural language processing to investigate the appropriate designation of crowdsourcing studies and the unique challenges that ML research poses for research oversight. Crucially, under the U.S. Common Rule, these judgments hinge on determinations of \"aboutness\", both whom (or what) the collected data is about and whom (or what) the analysis is about. We highlight two challenges posed by ML: (1) the same set of workers can serve multiple roles and provide many sorts of information; and (2) compared to the life sciences and social sciences, ML research tends to embrace a dynamic workflow, where research questions are seldom stated ex ante and data sharing opens the door for future studies to ask questions about different targets from the original study. In particular, our analysis exposes a potential loophole in the Common Rule, where researchers can elude research ethics oversight by splitting data collection and analysis into distinct studies. We offer several policy recommendations to address these concerns.","layer":1,"vector":[-0.022,-0.0114,0.0312,-0.0053,0.0281,0.0044,0.058,0.0043,0.0173,-0.0124,0.0089,-0.0287,0.0194,0.0441,0.0409,0.0256,-0.0133,-0.0015,-0.054,0.0366,0.0238,-0.0606,-0.0243,-0.065,-0.0166,0.0162,-0.0746,-0.0732,-0.0559,-0.2152,0.0396,-0.0435,0.0768,0.0088,0.0274,-0.0107,0.0377,0.0473,-0.0155,0.0513,0.0053,-0.0419,-0.0296,-0.035,-0.0068,-0.018,-0.0359,-0.0372,-0.0908,-0.0171,0.0304,-0.061,0.0268,0.07,0.0093,0.0076,0.0792,0.0297,0.0841,0.0385,0.0413,0.0257,-0.2083,0.0582,0.0122,0.0565,-0.0565,-0.0376,-0.025,-0.0045,0.0048,0.0367,0.032,0.0782,0.0486,0.026,-0.0044,-0.0362,0.0221,0.021,0.0078,0.0087,-0.0008,0.0156,-0.0237,-0.0564,0.0308,0.0025,0.0262,0.0314,-0.0282,-0.037,-0.0075,0.0713,-0.0452,-0.026,0.0166,0.0481,-0.054,0.205,-0.0435,0.0718,-0.0118,-0.0406,0.0015,-0.0661,-0.0107,-0.0367,-0.0241,0.008,-0.0274,0.0235,0.0345,-0.001,0.0634,0.022,0.0654,0.0244,-0.0182,0.0083,-0.0463,0.0203,0.0433,0.0277,0.0045,-0.0382,0.0437,0.1413,0.0626,-0.0037,0.0724,-0.0112,-0.0875,-0.0396,0.001,0.0478,0.0124,0.033,0.0348,-0.0139,-0.0205,-0.0473,-0.0179,-0.0926,-0.0595,0.1108,-0.0129,0.0372,-0.0143,-0.0198,-0.0091,0.0549,-0.0623,-0.0503,0.0281,0.0436,0.0477,0.0309,-0.0484,0.0252,0.0549,-0.0232,-0.0406,0.1088,-0.0088,-0.0665,-0.0102,0.0352,0.0091,-0.0085,0.0319,-0.0144,-0.0342,0.0645,0.0507,0.0276,-0.0739,0.0192,-0.0342,0.0156,0.0401,-0.0439,-0.0405,0.0382,-0.0004,-0.0748,-0.0205,-0.0353,0.0081,0.0582,0.0026,0.0503,-0.0389,-0.0085,-0.0187,-0.0333,-0.0071,-0.0262,-0.0063,-0.0331,-0.067,0.0518,-0.0306,0.0575,-0.0067,0.0392,0.0182,0.0135,0.0884,0.0128,-0.0299,0.0254,-0.0388,-0.0224,-0.0307,0.0071,0.0305,0.015,0.0146,-0.0042,0.0053,-0.0035,-0.0794,-0.2214,-0.0309,-0.0216,0.0061,0.0573,-0.0096,-0.0056,-0.0271,0.0056,0.1053,0.0519,-0.047,-0.0628,0.022,-0.0104,0.0412,0.0148,0.0332,-0.0548,-0.0094,0.0162,0.0007,-0.0079,-0.0918,0.0118,0.0085,0.2283,0.0819,-0.0119,-0.0087,0.0212,0.0204,-0.0287,-0.1824,0.0498,0.0235,0.0567,-0.0216,-0.0788,0.0021,-0.0486,-0.0071,0.0183,-0.1276,-0.0273,-0.0193,-0.0517,-0.0224,-0.0604,-0.0043,0.0148,-0.0479,0.0254,-0.0044,-0.0019,-0.0556,-0.0848,0.0319,-0.0199,0.0305,-0.0046,-0.0197,0.0293,-0.0457,0.0532,-0.0036,-0.0287,0.0018,0.0194,-0.0227,-0.0168,0.0929,0.0049,-0.0097,0.0395,0.0396,0.0214,-0.0778,-0.0449,-0.0064,0.0733,-0.0058,0.0075,0.0312,0.0575,-0.0157,0.0246,-0.0093,0.0443,-0.0435,0.0146,0.0195,-0.0602,-0.0282,0.0432,0.0002,-0.2785,0.0345,0.0006,0.0447,-0.0293,0.0227,0.0521,-0.0082,-0.0449,-0.0029,0.0478,0.0597,0.032,-0.0072,0.0259,0.0372,0.0813,-0.024,0.0266,-0.0821,0.0158,0.0455,0.1806,-0.0544,0.015,0.0186,-0.0276,0.03,0.0262,-0.0366,0.0102,-0.0358,0.0631,-0.0537,0.0248,0.0131,-0.0471,-0.0093,0.0389,-0.0223,-0.0361,0.02,-0.0121,-0.0432,0.093,0.0569,-0.0011,-0.0653,0.0417,0.0527,-0.0231,-0.0178,-0.077,0.0149,0.0251,0.0188,-0.0054,0.0025,-0.035,-0.0379,-0.0092,-0.0393,-0.0119,0.0103,-0.016]}
{"key":"[Enhancing Pure-Pixel Identification Performance via Preconditioning] In this paper, we analyze different preconditionings designed to enhance robustness of pure-pixel search algorithms, which are used for blind hyperspectral unmixing and which are equivalent to near-separable nonnegative matrix factorization algorithms. Our analysis focuses on the successive projection algorithm (SPA), a simple, efficient and provably robust algorithm in the pure-pixel algorithm class. Recently, a provably robust preconditioning was proposed by Gillis and Vavasis (arXiv:1310.2273) which requires the resolution of a semidefinite program (SDP) to find a data points-enclosing minimum volume ellipsoid. Since solving the SDP in high precisions can be time consuming, we generalize the robustness analysis to approximate solutions of the SDP, that is, solutions whose objective function values are some multiplicative factors away from the optimal value. It is shown that a high accuracy solution is not crucial for robustness, which paves the way for faster preconditionings (e.g., based on first-order optimization methods). This first contribution also allows us to provide a robustness analysis for two other preconditionings. The first one is pre-whitening, which can be interpreted as an optimal solution of the same SDP with additional constraints. We analyze robustness of pre-whitening which allows us to characterize situations in which it performs competitively with the SDP-based preconditioning. The second one is based on SPA itself and can be interpreted as an optimal solution of a relaxation of the SDP. It is extremely fast while competing with the SDP-based preconditioning on several synthetic data sets.","layer":1,"vector":[-0.0464,-0.009,0.0347,-0.0234,0.0467,0.0272,0.0192,0.0037,-0.0056,0.0188,0.0216,-0.0858,0.0278,0.0243,0.008,0.0479,0.0379,0.0628,-0.0217,0.0341,0.0324,0.009,-0.0289,-0.0705,0.0737,0.0179,-0.0138,-0.0667,-0.0347,-0.2898,-0.0012,-0.0085,0.082,-0.045,0.0558,-0.0107,-0.0086,0.0656,-0.0579,0.0261,-0.0063,0.0167,-0.0094,-0.0291,-0.0357,-0.0667,-0.007,-0.0072,-0.0194,-0.0899,0.0319,-0.0043,0.0247,0.0279,-0.0056,0.0134,0.0395,0.0153,0.0354,0.0463,0.0428,0.0174,-0.16,0.0497,0.0803,0.023,-0.0479,-0.0544,0.0409,0.0396,-0.0118,0.0668,0.018,-0.0119,0.0262,-0.0292,0.0284,-0.0576,-0.02,-0.0226,0.0495,-0.0223,-0.0265,0.0121,0.0065,-0.0493,0.0004,-0.0502,0.0542,0.0215,-0.024,-0.0215,-0.013,0.029,-0.08,-0.0386,0.0324,0.0363,-0.0253,0.1807,-0.0435,0.0499,0.0418,-0.0209,-0.0013,-0.0469,-0.0471,0.0012,-0.0442,-0.0253,0.0284,-0.0255,0.0658,0.0193,-0.0109,-0.0093,0.0274,0.0126,-0.0183,0.0131,-0.0467,0.0099,0.0419,-0.0151,0.0474,-0.0722,0.0369,0.1323,-0.0094,0.0823,0.06,-0.0314,-0.0277,-0.0129,0.0153,0.023,0.0173,0.004,0.0156,0.024,-0.0524,-0.0361,-0.0007,-0.0756,-0.034,0.131,-0.0452,0.0325,-0.061,-0.0689,-0.0135,-0.0105,-0.0193,0.0141,0.0131,0.0251,0.0264,0.0298,-0.0623,0.0462,-0.0151,-0.0625,-0.0008,0.1113,-0.0423,-0.0776,0.0241,-0.0105,0.0394,0.0107,-0.0132,0.0308,0.0096,-0.0072,0.0619,-0.0119,-0.0762,0.0448,-0.019,-0.0199,0.0026,-0.0275,-0.024,0.0369,0.0464,-0.045,-0.0069,0.0042,0.0144,0.0096,-0.0705,-0.0386,-0.0664,0.0186,-0.0288,-0.0133,0.0112,-0.015,0.0272,-0.0311,0.0157,0.0136,-0.0316,0.0503,0.044,0.0209,0.0483,0.0041,-0.0091,0.057,-0.0023,0.0157,0.0544,-0.0265,-0.0259,0.0175,0.0229,0.0408,0.0019,0.0887,0.0067,-0.0471,-0.1032,-0.2063,-0.0021,-0.0039,0.0084,0.0494,-0.0811,0.0398,-0.0081,0.0729,0.0589,0.047,-0.0206,-0.0208,0.059,-0.035,0.0152,0.0505,0.0181,0.0309,-0.0209,-0.0275,0.0126,-0.0499,-0.0349,0.0714,-0.0097,0.177,0.0323,0.0062,-0.0188,0.0154,-0.0127,-0.0349,-0.0773,0.0301,-0.0135,0.0658,-0.0152,-0.0143,-0.0235,0.0453,0.0021,-0.009,-0.0473,-0.0366,-0.0535,-0.0424,0.0384,-0.0075,0.0163,0.0599,0.013,0.0455,-0.0353,0.0399,-0.0272,-0.0845,0.013,-0.0481,0.0109,0.0228,-0.0871,-0.0257,-0.0737,0.092,0.0316,0.003,-0.0865,0.0315,-0.0697,-0.038,0.0806,0.0096,0.0304,0.0573,0.0174,0.0314,-0.0126,-0.0218,-0.026,0.0915,-0.0341,0.0392,0.0136,0.0674,-0.0153,0.0893,0.0037,-0.0158,-0.0412,-0.0186,0.0079,-0.0835,0.0058,0.0629,-0.0077,-0.3012,-0.0092,0.009,-0.0444,-0.0472,0.0014,0.0459,0.0165,-0.0868,0.0049,-0.0827,0.0803,0.0256,-0.0306,0.0142,0.0217,0.0359,-0.045,0.0487,-0.0625,-0.0383,0.015,0.2187,-0.0712,-0.0078,0.0576,0.0128,0.0239,0.003,-0.03,0.0202,0.0461,0.0631,-0.0836,0.049,0.0755,-0.0052,0.0337,0.0009,-0.0058,-0.0001,-0.0013,-0.0095,-0.0033,0.1056,-0.0172,0.0009,0.0254,-0.027,-0.0326,-0.0645,0.0152,0.0149,-0.0063,-0.0201,0.0022,-0.0917,-0.0409,-0.0117,-0.0234,0.011,-0.0172,-0.0259,0.0151,0.0057]}
{"key":"[A survey of machine learning-based physics event generation] Event generators in high-energy nuclear and particle physics play an important role in facilitating studies of particle reactions. We survey the state-of-the-art of machine learning (ML) efforts at building physics event generators. We review ML generative models used in ML-based event generators and their specific challenges, and discuss various approaches of incorporating physics into the ML model designs to overcome these challenges. Finally, we explore some open questions related to super-resolution, fidelity, and extrapolation for physics event generation based on ML technology.","layer":6,"vector":[-0.0662,-0.0074,0.0015,-0.0451,0.0352,0.0064,-0.0198,0.0142,-0.0155,-0.0151,0.0085,-0.0445,0.0348,0.01,0.0183,0.0169,0.0017,-0.0054,-0.0892,0.0068,-0.0049,-0.0125,0.0253,-0.0446,-0.002,0.026,-0.0336,-0.0323,-0.0411,-0.2211,0.0324,-0.0427,0.042,-0.0172,0.0096,-0.0105,-0.0344,0.0543,-0.0096,0.0697,0.0361,-0.0273,0.0255,-0.0905,0.019,-0.0613,-0.011,-0.0552,-0.0548,-0.009,0.0271,-0.0265,0.0271,0.0261,0.0457,0.0179,0.0569,0.0275,0.0397,0.0107,-0.0105,0.066,-0.1749,0.0269,0.0806,0.0146,-0.0343,0.0364,0.0482,0.0707,-0.0479,0.0334,0.0035,0.0643,-0.0066,-0.0067,0.0002,-0.045,-0.0019,0.0197,-0.0155,-0.0446,-0.0548,-0.039,-0.016,-0.0489,0.0158,0.0075,0.0393,0.0085,-0.0871,0.0058,-0.0283,0.0746,-0.051,0.0222,0.0425,0.0065,-0.0121,0.1899,-0.0575,0.0372,-0.0134,-0.0136,0.0399,-0.0458,-0.0492,-0.036,-0.0469,-0.0425,0.0338,-0.0247,0.0284,-0.0705,0.0534,0.0046,0.0783,0.0394,-0.0304,-0.0357,0.0252,0.0298,0.0103,-0.0114,0.0339,-0.0756,0.0179,0.137,0.0143,0.0216,0.0413,-0.0251,-0.0719,-0.0244,0.0294,-0.0232,0.0343,0.005,0.0214,0.0321,-0.0081,-0.0172,-0.0416,-0.0818,-0.0417,0.12,-0.0769,0.0143,-0.0223,-0.0203,-0.0358,0.0294,-0.0661,-0.0218,0.0387,0.0655,-0.0115,0.0608,-0.0446,0.0339,-0.0245,-0.0348,-0.0157,0.1196,0.0054,-0.0879,-0.0098,0.0212,0.0251,-0.0356,0.0194,0.0547,-0.037,0.0265,0.0862,0.0181,-0.0638,-0.0271,0.0145,0.0478,0.0147,-0.0516,-0.0487,0.0407,0.0374,-0.0988,0.0468,-0.0526,0.0419,0.0408,0.0,0.029,-0.0088,-0.0336,0.0084,-0.0314,-0.0154,-0.0089,0.016,-0.0461,-0.0322,0.0026,-0.0595,0.0613,-0.0395,0.0097,-0.0128,-0.0021,0.0528,0.0226,-0.0264,-0.0272,0.076,-0.0287,-0.0695,0.0108,-0.0136,0.0404,-0.034,0.0403,0.027,-0.0387,-0.0534,-0.2612,0.0151,0.0002,-0.0211,0.0512,-0.0754,0.0245,-0.0252,0.0261,0.0319,0.0685,-0.0008,-0.0181,-0.0551,-0.0037,0.0319,-0.0004,0.0205,-0.028,0.0235,-0.0314,0.0212,-0.0072,-0.0931,-0.004,-0.007,0.2052,0.0915,0.0081,-0.0054,0.0361,0.0018,-0.0456,-0.0614,0.0507,0.0549,0.061,0.065,-0.005,0.0199,-0.0371,0.0508,-0.0442,-0.1043,0.0017,-0.0546,-0.0037,0.0478,0.0049,0.0116,0.0399,-0.0382,0.0503,-0.0263,-0.0185,-0.0287,-0.0773,0.0369,-0.0268,-0.0079,0.0179,-0.0206,0.0457,-0.0428,0.0265,-0.0216,-0.0278,-0.0238,0.015,-0.036,0.0026,0.1177,-0.0153,-0.015,0.076,0.0345,-0.0071,-0.05,-0.0616,0.0205,0.0725,0.0044,0.0625,0.0188,-0.0015,0.0051,0.0521,0.0045,0.0184,-0.012,-0.0206,0.0069,-0.0296,0.0317,-0.0063,-0.0052,-0.3198,0.0549,0.013,0.0333,-0.0014,-0.0179,0.0354,0.002,-0.0721,0.0111,0.0032,0.0072,0.0267,0.0058,-0.0213,0.0646,0.0429,-0.0273,-0.0032,-0.0447,0.0498,0.051,0.2427,-0.0136,0.0296,0.0349,-0.0256,0.0066,0.0565,-0.0559,-0.0111,0.006,0.0467,-0.0375,0.0129,0.0895,-0.0248,0.0198,0.0257,-0.0032,-0.0033,0.0178,-0.0155,-0.0753,0.0982,-0.0241,0.0052,-0.0357,0.006,0.0173,-0.0134,0.0347,-0.0276,0.0329,0.0463,0.0282,-0.038,-0.0386,-0.0015,0.0039,0.0436,-0.013,0.0194,-0.0005,-0.0247]}
{"key":"[Online Preconditioning of Experimental Inkjet Hardware by Bayesian Optimization in Loop] High-performance semiconductor optoelectronics such as perovskites have high-dimensional and vast composition spaces that govern the performance properties of the material. To cost-effectively search these composition spaces, we utilize a high-throughput experimentation method of rapidly printing discrete droplets via inkjet deposition, in which each droplet is comprised of a unique permutation of semiconductor materials. However, inkjet printer systems are not optimized to run high-throughput experimentation on semiconductor materials. Thus, in this work, we develop a computer vision-driven Bayesian optimization framework for optimizing the deposited droplet structures from an inkjet printer such that it is tuned to perform high-throughput experimentation on semiconductor materials. The goal of this framework is to tune to the hardware conditions of the inkjet printer in the shortest amount of time using the fewest number of droplet samples such that we minimize the time and resources spent on setting the system up for material discovery applications. We demonstrate convergence on optimum inkjet hardware conditions in 10 minutes using Bayesian optimization of computer vision-scored droplet structures. We compare our Bayesian optimization results with stochastic gradient descent.","layer":2,"vector":[-0.0678,0.0135,0.031,0.0057,0.0221,0.033,0.0289,0.0582,0.0115,-0.0241,0.0191,-0.0212,0.014,0.0598,0.0287,0.0154,-0.0215,-0.0088,-0.0383,-0.0011,0.041,-0.0534,-0.0383,-0.1218,0.0513,0.0298,-0.0016,-0.0239,-0.058,-0.2665,-0.0089,-0.0237,0.0177,-0.0493,0.0404,-0.022,-0.041,0.057,-0.0287,0.0382,0.0105,0.0025,-0.0419,-0.0508,-0.018,-0.0305,-0.0006,-0.0331,-0.0157,-0.0729,0.0157,-0.043,0.0322,-0.0056,-0.0129,0.012,0.0886,0.0405,0.0355,0.0121,-0.0339,0.0274,-0.1688,0.1066,0.0744,0.0039,-0.0272,0.0067,0.0047,0.0486,-0.0185,0.063,0.0266,0.0708,0.0344,-0.0189,0.0022,-0.0638,-0.0337,-0.0013,0.0194,-0.0179,-0.0497,0.0189,-0.0485,-0.0037,-0.0201,-0.0123,0.0355,0.0036,-0.0353,0.0149,-0.0802,-0.0071,-0.0783,-0.0044,0.0414,-0.0214,-0.0087,0.2202,-0.0492,0.0315,0.0455,-0.0226,0.0179,-0.0585,-0.035,-0.016,-0.0397,-0.0278,0.0138,0.0014,0.0323,-0.0108,-0.0217,0.0088,0.0127,0.0232,-0.0015,0.0014,-0.0152,0.0364,0.0175,0.0075,0.0041,-0.0971,0.0132,0.1456,-0.0188,0.0438,0.0505,-0.0233,-0.0327,-0.0296,0.0437,0.0358,0.0135,-0.0166,0.023,-0.0394,-0.0349,-0.0157,0.0491,-0.1046,-0.0263,0.0969,-0.0221,0.084,-0.0241,-0.0813,0.0219,0.0273,-0.007,0.0128,-0.0003,0.0314,-0.023,0.0452,-0.0777,0.0317,-0.0096,-0.0238,-0.0196,0.11,-0.0361,-0.068,-0.0218,0.0071,-0.0012,-0.0047,-0.0124,0.0884,-0.0126,0.0217,0.0589,0.0365,-0.0622,0.0287,-0.0009,0.0294,0.06,-0.02,-0.0412,-0.0068,0.1027,-0.0549,0.0137,-0.0257,0.0031,0.0814,-0.0284,0.0114,-0.0025,0.0038,-0.0324,-0.0501,-0.0196,0.0181,0.0238,-0.0107,0.0768,-0.0275,-0.0603,0.0641,-0.0032,0.0246,0.0134,-0.0073,0.0263,0.0019,-0.0306,-0.0197,0.0436,-0.0213,-0.0066,0.0061,-0.0102,0.0405,0.0204,0.0255,0.0435,-0.0688,-0.0679,-0.2199,0.0222,0.0138,-0.0199,0.0671,-0.0366,-0.0122,-0.0612,0.0655,0.035,0.056,-0.0044,0.0076,-0.0096,-0.0429,0.02,0.0263,0.0027,-0.0271,-0.0006,-0.0272,-0.0066,-0.0403,-0.0651,0.0666,0.0029,0.206,0.0159,0.0211,-0.0126,-0.0055,0.0368,-0.0402,-0.0768,0.0105,0.0704,0.0382,0.0127,-0.055,-0.0261,-0.0344,0.0087,-0.0018,-0.0871,-0.011,-0.0369,-0.0596,0.0145,-0.0633,0.0435,0.0232,-0.0343,0.0345,-0.0403,0.0293,-0.0297,-0.0671,0.0614,-0.0374,0.026,0.0297,-0.0431,-0.0156,-0.0031,0.0518,-0.0289,-0.0297,-0.0347,0.0522,-0.0555,0.0051,0.0901,0.017,0.0069,0.0636,-0.0021,0.0546,-0.0198,-0.0021,-0.0584,0.0601,0.0022,0.0462,0.0625,0.0157,0.0214,0.0799,-0.0358,-0.008,-0.0032,-0.0248,-0.0116,-0.05,0.0673,0.0375,0.0201,-0.281,0.0355,-0.0136,0.0302,-0.0308,0.0171,0.1148,0.011,-0.0236,-0.0009,-0.0374,0.0534,-0.0005,0.0313,0.0014,0.0325,0.0554,-0.066,0.0415,-0.0674,-0.0014,0.0355,0.2357,-0.0714,-0.0402,0.0224,0.0107,-0.0003,-0.0059,-0.006,0.0215,0.0135,0.0546,-0.0636,0.0357,0.1238,-0.0699,0.006,0.0109,-0.0291,0.0083,-0.0078,-0.0384,-0.0519,0.0923,0.0011,-0.0619,-0.0317,0.0162,-0.0043,-0.0145,0.0567,-0.0184,-0.0192,0.0178,0.0389,-0.0273,-0.0008,0.007,0.0003,0.0187,-0.0514,-0.0301,0.0072,-0.0059]}
{"key":"[Model Selection, Adaptation, and Combination for Deep Transfer Learning through Neural Networks in Renewable Energies] There is recent interest in using model hubs, a collection of pre-trained models, in computer vision tasks. To utilize the model hub, we first select a source model and then adapt the model for the target to compensate for differences. While there is yet limited research on a model selection and adaption for computer vision tasks, this holds even more for the field of renewable power. At the same time, it is a crucial challenge to provide forecasts for the increasing demand for power forecasts based on weather features from a numerical weather prediction. We close these gaps by conducting the first thorough experiment for model selection and adaptation for transfer learning in renewable power forecast, adopting recent results from the field of computer vision on six datasets. We adopt models based on data from different seasons and limit the amount of training data. As an extension of the current state of the art, we utilize a Bayesian linear regression for forecasting the response based on features extracted from a neural network. This approach outperforms the baseline with only seven days of training data. We further show how combining multiple models through ensembles can significantly improve the model selection and adaptation approach. In fact, with more than 30 days of training data, both proposed model combination techniques achieve similar results to those models trained with a full year of training data.","layer":0,"vector":[-0.0239,0.0145,0.0142,0.0195,0.0631,0.0616,0.0165,-0.0321,0.0238,0.0132,0.0315,-0.0747,0.0074,0.0365,0.0336,0.0188,0.0027,0.0282,-0.0236,-0.0139,0.0267,-0.0102,-0.044,-0.0372,0.055,-0.0062,0.0022,0.0201,-0.0698,-0.2455,-0.0153,-0.0429,-0.0155,-0.0207,0.0125,-0.0131,-0.0381,0.0262,-0.0347,0.0562,-0.03,-0.0008,0.0086,-0.0727,-0.0319,-0.0687,0.0098,0.0096,-0.0191,-0.047,0.0793,-0.0408,0.0451,0.0416,0.0209,0.0141,0.0402,0.0526,0.0654,0.0344,0.0037,0.0705,-0.2012,0.0735,0.0294,0.0357,-0.0148,0.0163,-0.0197,-0.0029,-0.0161,0.0326,0.0201,0.0007,-0.0039,0.0327,-0.038,0.0,-0.0368,-0.019,0.0355,-0.0222,-0.0483,-0.0364,0.001,-0.0025,0.0293,-0.0237,0.0554,0.0134,-0.0614,-0.007,-0.0695,0.0236,-0.0575,-0.0043,0.0832,-0.0194,-0.0314,0.195,-0.0363,0.0509,0.0375,-0.0235,0.014,-0.0519,-0.0681,-0.0271,-0.0445,-0.0446,-0.0321,-0.0144,-0.0112,-0.0027,0.0205,0.0016,0.0452,0.0507,-0.0326,0.0011,-0.0266,0.022,0.1183,-0.0179,0.0104,-0.0415,0.0184,0.1848,0.028,0.0057,0.0755,-0.0059,-0.0498,-0.0544,0.0494,-0.0212,0.0647,-0.0367,-0.0138,0.0095,-0.0188,-0.0532,0.0423,-0.087,-0.0424,0.0851,-0.0392,0.0197,-0.0581,-0.0277,0.0011,0.0445,0.0064,-0.0246,0.0593,0.0247,0.0073,0.0409,-0.0193,0.0107,-0.0057,0.0066,-0.0435,0.0705,0.0304,-0.0759,-0.0273,-0.0138,-0.0016,-0.0158,0.0567,0.043,0.0196,-0.0152,0.0968,0.0535,-0.0683,-0.0169,-0.0096,0.0006,0.0161,-0.0217,-0.0338,0.0116,0.0402,-0.0094,-0.0094,-0.0701,-0.0159,0.0608,-0.0314,0.0253,0.0007,0.04,-0.0112,0.0172,-0.0532,-0.0245,0.0441,-0.0613,-0.0183,0.002,-0.0649,0.0168,-0.0384,0.0023,-0.0094,0.0116,0.0293,0.0312,-0.0229,-0.0112,0.0882,-0.0298,-0.0528,0.0572,0.0375,0.0211,0.0174,0.0371,0.0532,-0.0389,-0.0346,-0.2305,0.023,0.017,-0.019,0.0784,-0.0748,0.0428,-0.0288,0.0235,0.0705,0.0571,-0.0188,-0.0402,0.0041,-0.0367,0.0292,0.0089,0.0108,-0.0455,0.0059,-0.0304,0.0204,-0.018,-0.0916,0.0495,-0.0313,0.1764,0.025,0.0512,-0.0131,0.0073,0.0334,-0.02,-0.0783,0.0415,0.0215,0.0771,-0.0101,-0.0571,-0.0307,0.0178,0.025,-0.0231,-0.0884,-0.0484,-0.0228,-0.0301,0.0354,-0.0621,0.0075,0.0334,-0.0345,0.0231,-0.0462,0.0049,-0.0113,-0.1147,0.0958,0.005,0.0424,0.0155,-0.037,-0.0018,-0.0544,0.0235,0.0026,-0.0337,-0.0598,0.0042,-0.0015,-0.0403,0.0673,0.0266,0.0162,0.0623,0.0044,0.0206,-0.0217,-0.0197,-0.0654,0.0583,-0.0669,0.0382,0.0726,0.0481,0.0255,0.0579,-0.0191,0.0133,-0.0242,-0.021,-0.0388,-0.0475,-0.009,0.0845,0.0123,-0.2969,0.0496,0.0144,0.0346,-0.039,0.0103,0.0521,0.0252,-0.0117,-0.0116,-0.0132,0.0188,0.0668,0.008,-0.0168,0.0296,0.0687,-0.0769,0.0409,-0.0376,-0.0145,0.0256,0.2152,-0.029,0.0494,0.032,-0.0271,-0.0155,0.0332,-0.0228,0.0185,0.021,0.0848,-0.0582,0.0428,0.1055,0.0061,0.0069,-0.0113,-0.0388,-0.0221,0.0518,-0.0174,-0.0423,0.0923,-0.007,-0.0072,-0.0138,-0.0545,0.0463,0.0008,0.0101,-0.0607,-0.0161,0.0252,0.0163,-0.0347,-0.0308,-0.0534,-0.0576,0.0518,-0.0483,-0.0117,-0.0608,-0.0282]}
{"key":"[How Robust are Randomized Smoothing based Defenses to Data Poisoning?] Predictions of certifiably robust classifiers remain constant in a neighborhood of a point, making them resilient to test-time attacks with a guarantee. In this work, we present a previously unrecognized threat to robust machine learning models that highlights the importance of training-data quality in achieving high certified adversarial robustness. Specifically, we propose a novel bilevel optimization-based data poisoning attack that degrades the robustness guarantees of certifiably robust classifiers. Unlike other poisoning attacks that reduce the accuracy of the poisoned models on a small set of target points, our attack reduces the average certified radius (ACR) of an entire target class in the dataset. Moreover, our attack is effective even when the victim trains the models from scratch using state-of-the-art robust training methods such as Gaussian data augmentation\\cite{cohen2019certified}, MACER\\cite{zhai2020macer}, and SmoothAdv\\cite{salman2019provably} that achieve high certified adversarial robustness. To make the attack harder to detect, we use clean-label poisoning points with imperceptible distortions. The effectiveness of the proposed method is evaluated by poisoning MNIST and CIFAR10 datasets and training deep neural networks using previously mentioned training methods and certifying the robustness with randomized smoothing. The ACR of the target class, for models trained on generated poison data, can be reduced by more than 30\\%. Moreover, the poisoned data is transferable to models trained with different training methods and models with different architectures.","layer":0,"vector":[0.0005,-0.0512,0.0075,0.0014,0.0296,0.0196,0.0639,0.0221,-0.0172,-0.0212,0.0168,-0.0343,0.0298,0.0385,-0.0252,0.0365,0.0186,0.0554,-0.055,0.0435,0.0133,-0.0058,-0.0109,-0.0405,0.0176,0.0085,-0.0178,-0.0415,-0.0841,-0.2737,0.0023,-0.0457,0.0363,-0.0492,-0.0015,-0.0183,-0.0172,0.0782,-0.02,0.0225,0.0303,0.0462,-0.0186,-0.075,-0.0302,-0.0375,0.0166,0.0076,-0.0037,-0.067,0.0157,-0.0459,0.0199,0.0225,0.0628,-0.04,0.0596,0.0314,0.0337,0.0817,0.0065,0.064,-0.1293,0.023,0.0192,0.047,-0.0579,-0.0441,-0.005,0.0238,0.0077,0.0052,0.0108,0.0379,-0.0055,-0.0047,-0.0328,-0.0547,-0.0159,0.0421,0.0278,-0.0534,-0.0333,0.028,-0.018,-0.0247,0.0214,-0.0453,0.0512,0.0037,0.0104,-0.0266,0.0057,0.0365,-0.0221,-0.0218,0.076,0.0364,-0.1043,0.1933,-0.0912,-0.0097,-0.0438,-0.0204,0.0533,-0.0272,-0.0361,-0.0706,0.0136,-0.0491,0.0138,0.0023,0.0101,-0.0278,-0.0007,0.0303,0.0603,-0.0186,-0.0603,-0.0026,-0.0284,0.016,0.0863,-0.0254,0.0823,-0.0562,0.0284,0.1449,0.0338,0.0504,-0.0004,-0.023,-0.0463,0.0025,0.0316,0.0353,0.0012,0.0264,0.0442,-0.0099,-0.0539,-0.036,0.012,-0.0864,-0.0286,0.0876,-0.0467,0.0483,-0.0501,-0.0623,0.0126,-0.0054,-0.0106,0.0011,0.0113,0.0166,0.0247,0.0638,-0.0485,-0.0328,-0.0158,-0.0419,-0.0156,0.1068,-0.0087,-0.0414,-0.0153,0.0198,0.0231,-0.0082,0.038,0.0005,-0.015,0.0612,0.0134,0.0162,-0.0671,0.0067,-0.0359,0.0224,-0.0043,-0.0392,-0.0204,0.0363,0.0425,-0.0144,0.0074,-0.0182,0.045,0.0559,-0.0959,0.021,-0.0466,-0.0329,-0.0518,-0.0462,-0.0141,-0.0003,-0.0109,-0.0277,-0.0094,-0.0033,-0.0452,0.0308,0.0302,0.0292,0.027,-0.0167,0.0279,0.0577,-0.0144,-0.0218,0.0474,-0.0752,-0.0048,-0.0072,0.0052,0.0421,-0.0397,0.037,0.0244,-0.0057,-0.0665,-0.2526,-0.0124,-0.0242,-0.0282,0.082,-0.0985,0.0565,-0.0049,0.0627,0.061,0.0398,0.0175,-0.0427,0.0087,-0.0103,0.0609,0.0263,0.0059,-0.0197,0.033,-0.05,0.0439,-0.0317,-0.0364,0.0501,0.0256,0.2327,0.0226,0.0136,-0.0126,0.0219,0.0282,-0.0164,-0.0716,0.0575,-0.0107,0.0717,-0.0014,-0.052,-0.009,-0.0237,0.0074,0.0223,-0.1605,-0.0313,-0.052,-0.0461,0.0191,-0.0676,0.0638,0.056,0.031,0.069,-0.0222,0.0366,-0.0282,-0.0849,0.0769,-0.0432,0.0418,-0.0043,-0.0636,0.0102,-0.095,0.0357,0.0214,-0.0057,-0.0753,0.0642,-0.0339,-0.024,0.0839,0.012,-0.0165,0.0601,0.0143,-0.0333,-0.0106,-0.0475,0.0076,0.0547,0.0135,0.0365,0.0201,0.0328,-0.0061,0.0618,0.0185,0.0566,-0.0341,0.0188,-0.0108,-0.075,-0.0276,0.0478,0.0313,-0.2943,-0.0307,0.0056,0.0384,-0.0033,-0.0011,0.0289,0.0081,-0.0607,0.0162,-0.0119,0.0207,0.0495,-0.0215,0.0312,0.0103,0.0744,-0.0802,0.0499,-0.0247,0.0378,0.0232,0.2083,-0.021,-0.0124,-0.003,-0.0089,0.0686,0.0366,-0.0438,0.0177,-0.0116,0.0469,-0.0014,0.0267,0.047,-0.0431,0.0211,0.0303,0.0007,-0.03,0.0117,-0.0423,-0.0102,0.0413,0.001,-0.0167,-0.0315,0.0105,0.0346,0.0106,-0.0191,0.0012,0.0308,0.0216,0.0433,-0.0564,-0.0343,-0.0402,-0.0176,0.0113,-0.0179,-0.0066,0.0489,-0.0451]}
{"key":"[Embedding of FRPN in CNN architecture] This paper extends the fully recursive perceptron network (FRPN) model for vectorial inputs to include deep convolutional neural networks (CNNs) which can accept multi-dimensional inputs. A FRPN consists of a recursive layer, which, given a fixed input, iteratively computes an equilibrium state. The unfolding realized with this kind of iterative mechanism allows to simulate a deep neural network with any number of layers. The extension of the FRPN to CNN results in an architecture, which we call convolutional-FRPN (C-FRPN), where the convolutional layers are recursive. The method is evaluated on several image classification benchmarks. It is shown that the C-FRPN consistently outperforms standard CNNs having the same number of parameters. The gap in performance is particularly large for small networks, showing that the C-FRPN is a very powerful architecture, since it allows to obtain equivalent performance with fewer parameters when compared with deep CNNs.","layer":6,"vector":[-0.0359,-0.0422,0.0035,-0.0179,0.0546,0.0469,0.0081,0.0308,0.0492,0.0023,0.013,-0.0653,0.0239,0.0822,0.0202,0.0237,0.0138,0.0555,-0.0205,-0.0231,0.0315,-0.0297,0.0004,-0.0948,0.0138,-0.0206,-0.0132,-0.0594,-0.0725,-0.2361,0.0174,-0.0602,0.0484,-0.0197,-0.0014,-0.0276,-0.0279,-0.0112,-0.0462,0.0319,0.0136,-0.0027,-0.0242,-0.0583,-0.0062,-0.006,-0.0306,-0.0172,-0.0002,-0.0611,0.0628,-0.0498,0.0273,0.0314,-0.0126,0.0545,0.0641,0.0041,0.0275,0.0323,0.0171,0.0595,-0.1816,0.0528,0.0159,0.0362,-0.0599,-0.0025,0.0202,0.0769,-0.018,0.0537,0.0024,0.0042,0.0003,-0.0375,-0.0017,-0.0286,-0.0078,0.0153,0.0239,0.0071,-0.0366,-0.0228,0.0549,-0.047,0.0308,-0.0399,0.026,0.0086,-0.0648,-0.0166,-0.0115,0.0269,-0.0546,-0.0194,0.05,0.0111,-0.0805,0.1864,-0.0507,0.0352,0.0638,-0.0639,0.0093,-0.0058,-0.0225,0.0082,-0.0783,0.003,-0.0348,-0.0542,0.0257,-0.0238,0.0039,0.0123,0.0579,0.0163,-0.0229,0.0165,-0.0044,0.0034,0.02,-0.0137,0.0396,-0.0502,0.0469,0.1149,0.0275,0.0442,0.0519,-0.0404,-0.0419,-0.0134,0.0341,0.0665,0.04,-0.0284,-0.0398,-0.0231,-0.0351,-0.0778,0.009,-0.0756,-0.0875,0.0953,-0.0246,0.0641,-0.0234,-0.0091,-0.0403,-0.0076,-0.0063,-0.0271,0.0463,0.0446,0.0226,0.0242,-0.0661,0.0223,-0.0199,-0.0727,-0.0178,0.0826,0.0508,-0.0701,-0.0252,-0.0319,0.0055,-0.0283,0.0384,0.0093,-0.0445,0.0067,0.0594,0.0874,-0.0986,-0.0257,0.0028,0.0059,-0.0036,-0.0368,0.0082,0.0269,0.0316,-0.0593,0.0034,-0.0683,0.0042,0.0585,-0.0555,0.047,-0.0516,0.0089,-0.0478,-0.0193,-0.0224,-0.0096,0.0114,-0.0308,-0.0051,0.0159,-0.0207,0.0243,-0.0298,-0.0098,-0.0282,0.0381,-0.009,0.0262,-0.0247,0.038,0.0761,-0.0823,-0.0431,0.0161,0.0034,0.016,0.0006,0.0357,0.0345,-0.0839,-0.0315,-0.216,-0.0055,0.0015,-0.035,0.0363,-0.069,0.0104,-0.0081,0.0562,0.0063,0.0506,0.0211,0.0005,0.0355,0.0397,0.0375,0.0509,0.0482,0.0188,-0.0294,-0.0006,0.0691,0.0222,-0.111,0.0718,0.0291,0.2184,0.0141,0.0793,-0.0112,0.0664,0.0512,-0.064,-0.0766,0.0426,0.0075,0.0679,0.0122,-0.0316,-0.0429,-0.0605,0.0313,0.0282,-0.0982,-0.0281,-0.0344,-0.0311,0.0223,-0.0401,0.0089,0.0009,-0.0494,0.0342,0.0067,-0.0106,-0.0333,-0.107,0.0366,-0.084,0.0164,0.0308,-0.0476,-0.0115,-0.0385,0.0641,0.0084,-0.0562,-0.0072,0.0315,-0.0016,-0.0237,0.0724,0.0049,0.016,0.066,-0.0434,0.024,0.0068,-0.0129,-0.004,0.0683,0.0105,-0.0001,0.0591,0.0416,0.0169,0.0656,-0.0189,0.0518,-0.012,0.0072,0.0448,-0.0422,-0.0026,0.0205,0.0278,-0.305,0.022,0.0015,0.0506,-0.0276,0.0273,0.0358,0.0354,-0.0203,-0.0123,0.0058,0.0512,0.0968,0.0018,0.0346,-0.0297,0.0345,-0.0397,0.0756,-0.0326,0.0195,0.0644,0.2265,-0.0984,-0.0028,-0.015,-0.0137,-0.0009,0.0349,0.0001,0.0616,-0.0197,0.0858,-0.0247,0.0165,0.0964,-0.0008,0.0546,0.0467,-0.0287,0.0073,-0.034,-0.0317,-0.0049,0.0328,-0.0243,-0.0079,-0.014,0.0057,0.0482,-0.0266,0.0111,-0.007,-0.0329,0.0417,-0.0208,-0.0613,-0.0582,-0.0524,0.0062,0.0284,-0.0383,-0.0327,0.0002,-0.0225]}
{"key":"[FedProc: Prototypical Contrastive Federated Learning on Non-IID data] Federated learning allows multiple clients to collaborate to train high-performance deep learning models while keeping the training data locally. However, when the local data of all clients are not independent and identically distributed (i.e., non-IID), it is challenging to implement this form of efficient collaborative learning. Although significant efforts have been dedicated to addressing this challenge, the effect on the image classification task is still not satisfactory. In this paper, we propose FedProc: prototypical contrastive federated learning, which is a simple and effective federated learning framework. The key idea is to utilize the prototypes as global knowledge to correct the local training of each client. We design a local network architecture and a global prototypical contrastive loss to regulate the training of local models, which makes local objectives consistent with the global optima. Eventually, the converged global model obtains a good performance on non-IID data. Experimental results show that, compared to state-of-the-art federated learning methods, FedProc improves the accuracy by $1.6\\%\\sim7.9\\%$ with acceptable computation cost.","layer":2,"vector":[-0.0157,-0.0332,0.0056,-0.0183,0.0404,0.0031,-0.0065,0.0251,0.0139,-0.0327,0.0309,-0.0551,0.0335,0.0711,-0.0134,-0.0055,-0.0015,0.0166,-0.0339,-0.0173,0.0146,-0.0474,-0.039,-0.068,-0.0116,0.0269,-0.0046,-0.0067,-0.0506,-0.2328,0.0273,-0.0282,0.0004,0.009,0.0236,-0.0215,-0.0183,0.0337,-0.0423,0.0485,0.0101,-0.0051,-0.0179,-0.0312,-0.0368,-0.0415,-0.0195,-0.0227,-0.0235,-0.0076,0.0952,-0.0557,0.0365,0.0243,-0.0348,0.0605,0.067,0.0742,0.0668,-0.0112,0.0292,0.0824,-0.1301,0.0756,0.0177,0.0516,0.0039,0.0132,0.0395,0.0437,0.0338,0.0399,0.014,0.0138,-0.0062,-0.0339,-0.0061,-0.0138,-0.0272,-0.0131,0.0355,-0.0007,-0.0512,-0.0422,-0.0043,-0.0215,-0.012,-0.0588,0.0036,-0.0112,-0.0602,-0.0006,-0.0351,0.0114,-0.0413,-0.0283,0.0138,-0.0105,-0.0559,0.2026,-0.0744,0.0611,0.0437,-0.0658,0.0022,-0.0583,-0.0302,0.0083,0.0043,-0.0152,-0.0088,-0.0271,-0.0067,-0.0429,0.0357,0.019,0.0922,0.0437,0.0021,0.0093,-0.0116,-0.0336,0.065,-0.0439,0.0316,-0.0561,0.0141,0.1131,0.0067,0.0146,0.0362,-0.0114,-0.0191,-0.0146,0.057,0.0527,0.0206,-0.0359,-0.0153,0.0267,0.0137,-0.041,-0.0254,-0.071,-0.035,0.1024,-0.0143,0.0338,-0.0867,-0.0119,-0.0596,-0.0038,-0.0368,0.0111,0.0302,0.0288,0.0699,0.048,-0.0529,-0.0394,-0.0211,-0.0959,0.0083,0.1063,0.0292,-0.0968,-0.027,-0.0097,0.0312,-0.0265,0.0273,0.0246,-0.02,0.0635,0.061,0.0341,-0.0937,0.0041,0.0368,-0.0214,0.0103,-0.0372,-0.0131,0.0271,0.0248,-0.0245,0.0191,-0.0637,0.0122,0.0087,-0.0369,0.041,-0.0445,-0.011,-0.0134,0.0108,0.0275,-0.0304,0.0129,-0.0464,0.0173,-0.0053,-0.026,0.0001,-0.0157,-0.0064,0.0055,0.0052,0.0258,0.0093,-0.0534,0.0046,0.0386,0.0123,-0.0508,-0.0206,0.0206,0.0602,0.0004,0.0452,0.0542,0.011,-0.0177,-0.1842,-0.0203,0.0338,-0.0545,0.0508,-0.072,0.041,0.0011,0.0112,0.0529,0.0704,-0.0015,-0.0308,0.0481,-0.0244,0.0614,0.0956,0.0173,0.0017,-0.0417,-0.0314,0.0693,-0.0002,-0.0834,0.0747,0.0217,0.2125,0.0199,0.0066,-0.0631,-0.0001,0.0272,-0.0529,-0.1316,0.0254,-0.0197,0.0708,-0.005,-0.0576,-0.0137,0.0034,0.0349,0.0206,-0.151,-0.0107,-0.0409,-0.042,0.0229,-0.1012,0.0253,0.0308,-0.0365,0.0066,0.0185,0.0111,-0.0344,-0.0793,0.0641,-0.0772,0.0628,0.0106,-0.0432,0.0111,-0.0702,0.113,-0.0074,-0.0317,-0.0445,0.0571,-0.0578,0.0118,0.0654,-0.0108,-0.0008,0.0445,0.0311,0.058,-0.0082,-0.0559,-0.0156,0.1324,0.0207,0.0677,0.0494,0.0053,0.0432,0.0681,0.0348,0.0223,-0.0398,0.0058,0.0175,-0.0327,-0.0266,0.0291,-0.0137,-0.3103,0.0053,0.002,0.061,-0.0458,0.0383,0.059,0.0211,-0.0165,0.0165,0.041,0.0205,0.0492,-0.0248,0.01,0.0181,0.0195,-0.0422,0.0879,-0.0745,0.0053,0.0471,0.2296,-0.0618,0.0107,0.0134,-0.0221,0.0043,0.0298,-0.0015,-0.0176,0.0239,0.0898,-0.0581,0.0284,0.0916,-0.0184,0.0265,0.0121,-0.04,-0.0202,0.0016,0.0116,-0.017,0.0695,-0.0034,-0.0247,0.0043,-0.0499,0.02,-0.0185,-0.0279,0.0036,-0.0084,0.0058,-0.0164,-0.0566,-0.0283,-0.0555,-0.0263,-0.0048,-0.0439,-0.0377,-0.0201,-0.0236]}
{"key":"[Functional Tensors for Probabilistic Programming] It is a significant challenge to design probabilistic programming systems that can accommodate a wide variety of inference strategies within a unified framework. Noting that the versatility of modern automatic differentiation frameworks is based in large part on the unifying concept of tensors, we describe a software abstraction for integration --functional tensors-- that captures many of the benefits of tensors, while also being able to describe continuous probability distributions. Moreover, functional tensors are a natural candidate for generalized variable elimination and parallel-scan filtering algorithms that enable parallel exact inference for a large family of tractable modeling motifs. We demonstrate the versatility of functional tensors by integrating them into the modeling frontend and inference backend of the Pyro programming language. In experiments we show that the resulting framework enables a large variety of inference strategies, including those that mix exact and approximate inference.","layer":0,"vector":[-0.0509,0.008,0.0028,-0.0093,0.0378,0.018,0.0407,0.0466,0.0339,-0.0158,0.029,-0.0564,0.0316,0.0427,0.001,0.0343,-0.0515,0.0741,-0.0325,0.0009,0.0601,-0.0828,-0.0556,-0.0851,0.022,0.038,0.0116,0.0154,-0.0203,-0.2347,0.0138,-0.0501,0.0351,-0.0761,0.0072,0.0006,-0.0378,0.018,-0.0349,0.0584,0.0031,-0.005,-0.0239,-0.0403,-0.012,-0.0439,-0.023,-0.0143,-0.0748,0.0368,0.0047,-0.0602,0.0246,0.0018,0.0458,0.0248,0.031,0.0297,0.048,0.0816,0.0067,0.008,-0.1529,0.0745,0.0616,0.0236,-0.0251,-0.0428,0.0436,0.095,-0.0237,0.0787,-0.0117,0.0664,-0.005,-0.0463,0.0294,-0.0348,-0.0073,0.0471,-0.0023,-0.0605,-0.0408,-0.0157,-0.0083,-0.0153,0.0434,-0.0048,0.0395,0.0008,0.0004,-0.0105,-0.0215,0.0483,-0.0218,0.0001,0.0619,0.0282,-0.0203,0.2274,-0.057,0.0012,0.0104,-0.0091,0.0464,-0.0475,0.0061,-0.0314,-0.0223,-0.0272,-0.0207,-0.0186,0.0486,-0.0372,0.01,-0.0042,0.0358,0.0031,0.0097,0.031,-0.0457,-0.0254,0.0617,0.0151,-0.0226,-0.0525,-0.0273,0.1566,0.0214,0.0525,0.0641,-0.019,-0.0518,-0.042,0.0232,0.005,-0.0036,-0.0217,-0.0085,0.0144,-0.0631,-0.0024,0.0184,-0.1036,-0.0486,0.1238,-0.0645,0.0224,-0.0452,0.0159,-0.0102,0.0605,-0.0051,-0.0188,0.0611,0.0514,-0.0137,0.0178,-0.0663,0.0428,-0.0464,-0.067,0.0075,0.0609,0.0027,-0.0466,-0.0376,0.0256,0.0237,-0.0141,0.0404,0.0061,0.0009,0.0064,0.0476,0.0322,-0.0588,0.0221,0.0333,0.0067,0.0187,-0.0315,-0.0152,0.0414,-0.0328,-0.079,-0.0181,-0.0446,-0.0112,0.0137,0.0084,0.0075,-0.021,-0.0872,-0.0361,-0.0684,-0.0099,-0.0221,0.0247,-0.0454,0.0273,0.0275,-0.0622,0.0161,-0.0362,0.0321,-0.0662,0.0108,-0.0022,0.0547,-0.0279,-0.0188,0.0476,0.0175,-0.0226,-0.0028,0.0352,0.0425,0.0474,0.0532,0.0213,-0.0517,-0.032,-0.2459,-0.0035,0.0127,-0.0389,0.0546,-0.0572,0.0345,-0.0218,-0.0077,0.0655,0.0559,0.0119,-0.0378,0.0281,-0.0036,0.0229,0.0124,-0.0299,-0.0202,0.0125,0.0098,0.0192,-0.0269,-0.1122,0.028,0.0189,0.2264,0.0345,0.0498,-0.0275,0.0426,-0.0064,-0.0073,-0.0761,0.0906,0.0728,0.0613,-0.0384,0.0014,-0.018,-0.0154,0.0601,-0.0237,-0.1327,-0.0343,-0.0403,-0.0225,0.0034,0.003,0.0269,0.0383,-0.0537,0.04,-0.0134,-0.0183,-0.0648,-0.0524,0.0122,-0.0011,0.0201,0.012,-0.0308,-0.0074,-0.0194,0.081,-0.018,-0.016,-0.0398,0.0268,-0.0295,-0.0104,0.0674,-0.0468,-0.0204,0.0379,0.0338,0.0488,-0.0165,-0.0266,-0.0247,0.0859,-0.0442,0.0167,0.0481,0.0211,-0.0055,0.0499,0.0001,0.0368,-0.0104,-0.0186,0.0349,-0.0375,-0.0051,0.0592,0.0069,-0.2917,0.0239,0.006,-0.0013,-0.0483,0.0026,0.0736,0.0031,-0.0528,-0.0111,0.0205,0.0336,0.0879,0.017,-0.0334,0.0268,0.0598,-0.0613,0.0504,-0.0705,0.0267,0.0207,0.2193,-0.0259,0.0423,0.0404,0.001,-0.0048,0.0631,0.0185,0.0074,0.0201,0.0632,-0.0642,0.0383,0.0564,-0.0143,0.0386,0.0256,-0.07,-0.0244,-0.0269,-0.0526,-0.0347,0.0906,-0.0371,-0.0662,-0.046,0.0128,0.0199,0.0031,-0.0011,-0.0001,-0.0234,0.0105,0.0241,0.0117,-0.0609,-0.0466,0.0077,-0.0182,-0.0614,0.0122,-0.0018,-0.0355]}
{"key":"[Learning the Beauty in Songs: Neural Singing Voice Beautifier] We are interested in a novel task, singing voice beautifying (SVB). Given the singing voice of an amateur singer, SVB aims to improve the intonation and vocal tone of the voice, while keeping the content and vocal timbre. Current automatic pitch correction techniques are immature, and most of them are restricted to intonation but ignore the overall aesthetic quality. Hence, we introduce Neural Singing Voice Beautifier (NSVB), the first generative model to solve the SVB task, which adopts a conditional variational autoencoder as the backbone and learns the latent representations of vocal tone. In NSVB, we propose a novel time-warping approach for pitch correction: Shape-Aware Dynamic Time Warping (SADTW), which ameliorates the robustness of existing time-warping approaches, to synchronize the amateur recording with the template pitch curve. Furthermore, we propose a latent-mapping algorithm in the latent space to convert the amateur vocal tone to the professional one. To achieve this, we also propose a new dataset containing parallel singing recordings of both amateur and professional versions. Extensive experiments on both Chinese and English songs demonstrate the effectiveness of our methods in terms of both objective and subjective metrics. Audio samples are available at~\\url{https://neuralsvb.github.io}. Codes: \\url{https://github.com/MoonInTheRiver/NeuralSVB}.","layer":0,"vector":[-0.0139,-0.058,0.0442,-0.0327,0.0011,0.0192,-0.014,-0.027,0.055,-0.0394,0.0349,-0.0602,0.0679,0.0336,0.0382,0.0298,0.032,0.09,-0.0665,0.0301,0.0277,0.0197,-0.0155,0.0005,0.0274,-0.0487,-0.0543,-0.0422,-0.0444,-0.2153,0.0289,-0.0264,0.0579,-0.0298,-0.0313,0.0072,-0.0292,0.0375,-0.0389,0.0516,0.0392,0.0458,-0.0422,-0.084,0.0039,-0.0622,-0.0854,0.0156,-0.0209,0.0257,0.0097,-0.0659,0.0389,0.0747,0.0372,0.056,0.1174,0.1018,0.0416,0.0506,-0.0267,0.0349,-0.1347,0.0777,-0.0008,0.0268,-0.0092,-0.0425,-0.0246,0.0379,-0.0184,0.0423,0.0278,-0.0053,-0.0066,-0.0284,0.0451,-0.0397,-0.0284,0.0149,0.0289,0.002,0.0054,-0.0332,0.0139,-0.0381,-0.0262,-0.0327,0.0014,0.0136,-0.0811,-0.0167,-0.0603,0.0345,-0.0636,-0.0474,0.0365,0.0251,-0.0419,0.2375,-0.0574,0.0388,0.0292,-0.0559,0.0168,-0.0495,-0.0466,-0.0021,-0.0504,0.0122,-0.0084,-0.0213,0.0063,-0.0388,0.0498,0.0086,0.0251,0.0304,0.0007,-0.0497,-0.0102,0.0018,0.0425,-0.0025,0.0462,-0.058,0.0438,0.1108,0.0302,0.037,0.077,0.0129,-0.0634,0.008,-0.0173,0.0471,0.0018,-0.0018,0.0396,-0.0231,-0.0553,-0.1002,-0.0049,-0.0573,-0.0271,0.1078,-0.0277,0.0397,-0.0488,0.0059,-0.0563,0.0101,-0.0638,-0.0369,0.0561,0.0264,0.0747,0.044,-0.0751,0.0009,-0.0233,-0.0353,-0.0322,0.0737,0.0346,-0.075,-0.0473,-0.0212,0.0068,0.0113,-0.018,-0.0112,-0.0452,0.034,0.0677,0.0541,-0.0287,0.011,0.0013,-0.0146,0.0399,-0.0513,0.0031,0.0296,0.0619,-0.0655,-0.0024,-0.0619,0.0202,0.0347,-0.0297,0.0105,-0.0091,0.0038,-0.0346,-0.0451,-0.0155,-0.0404,0.0007,-0.0634,-0.0145,-0.0096,-0.0197,0.0049,0.0128,0.0184,-0.0026,0.027,0.0507,0.0731,-0.0441,-0.0039,0.0619,-0.0199,-0.0525,-0.0058,0.0554,0.0398,0.01,0.0374,-0.0128,-0.0486,-0.1099,-0.2121,0.0042,0.0309,-0.0078,0.0641,-0.0512,0.0221,0.0001,0.1128,0.035,0.0159,-0.0072,0.0039,0.0428,-0.0105,0.0223,0.024,0.0391,0.0054,-0.0284,-0.003,0.012,-0.0267,-0.0984,0.0664,-0.0116,0.2235,0.0243,0.0643,-0.0209,0.0256,-0.0368,-0.0329,-0.0888,0.0719,0.015,0.0943,0.0071,-0.0609,-0.0264,-0.0261,-0.0,-0.0095,-0.0865,-0.0222,-0.018,-0.0302,-0.0122,-0.0395,-0.011,0.0309,-0.0008,0.006,0.0038,0.0208,-0.072,-0.0883,0.0291,-0.0615,0.0618,0.0334,-0.0418,0.069,-0.0715,0.0078,-0.018,0.0014,-0.0357,0.0137,-0.0286,-0.0036,0.0633,0.0146,0.0089,0.0558,0.0072,0.0158,-0.0302,-0.0657,-0.0111,0.0336,-0.0051,0.0609,0.0061,0.0015,0.003,0.0581,-0.025,0.0375,-0.0361,0.0117,0.0315,-0.0184,-0.0049,0.022,0.0346,-0.2824,0.0135,0.0229,0.0361,-0.058,-0.0024,-0.0323,0.058,-0.0665,-0.0191,-0.0281,0.0463,0.019,-0.0304,0.0016,0.0362,0.115,-0.0426,0.0724,-0.0534,-0.0054,0.0512,0.2248,-0.0365,-0.0234,-0.0039,-0.0083,-0.0099,0.065,-0.0364,-0.0052,-0.0077,0.0862,-0.0266,-0.0037,0.0643,-0.0628,0.0123,-0.0202,0.0042,-0.029,0.0359,-0.0039,-0.0431,0.0632,0.0097,0.0007,-0.0169,0.0085,0.029,0.0252,0.0022,0.0009,0.0204,0.0063,0.0751,-0.0425,-0.0275,0.0116,-0.0061,0.0488,-0.0404,-0.0283,0.0028,-0.0163]}
{"key":"[Converged Algorithms for Orthogonal Nonnegative Matrix Factorizations] This paper proposes uni-orthogonal and bi-orthogonal nonnegative matrix factorization algorithms with robust convergence proofs. We design the algorithms based on the work of Lee and Seung [1], and derive the converged versions by utilizing ideas from the work of Lin [2]. The experimental results confirm the theoretical guarantees of the convergences.","layer":0,"vector":[-0.1278,-0.0132,-0.0101,-0.0403,0.0077,0.0026,0.0532,0.0067,0.036,0.0217,0.0018,-0.0702,0.0478,0.0421,0.005,0.028,0.0521,0.057,-0.0388,0.0031,0.0171,-0.0176,-0.0054,-0.0715,0.0512,-0.0004,-0.0467,-0.0558,-0.0213,-0.2322,-0.0289,-0.0072,0.0973,-0.0096,0.0305,0.0231,-0.0197,0.0125,-0.0422,0.0133,0.0207,0.0308,-0.0447,-0.0186,-0.0414,-0.0675,-0.0326,-0.0009,0.01,-0.039,0.0657,-0.001,0.0009,0.0198,0.0643,0.0138,0.0458,0.0374,0.0062,0.0557,0.0284,0.0204,-0.1966,0.0817,0.0605,-0.0131,-0.0293,-0.0152,0.0374,0.0815,-0.0436,0.0359,0.0594,0.0165,0.015,0.0402,-0.0018,-0.0448,-0.0075,0.0091,0.0195,-0.0201,-0.0446,0.009,-0.0148,-0.0487,-0.0086,-0.0839,0.0063,0.0015,-0.0202,-0.0071,0.0228,0.0078,-0.0664,-0.0765,0.0127,0.052,-0.0234,0.2171,-0.0799,0.0397,0.0698,-0.0266,0.0038,-0.0007,-0.0411,-0.0505,-0.0386,-0.0309,-0.031,-0.0122,0.0213,-0.0787,0.013,-0.0038,0.0783,0.0305,-0.0184,-0.0209,-0.0521,0.0221,0.0372,-0.0257,0.0436,-0.0993,0.0229,0.109,0.0558,0.0611,0.0339,-0.0298,-0.029,-0.0454,0.0149,-0.0071,-0.0016,0.0224,0.0312,0.0104,-0.0191,-0.1144,0.0359,-0.0514,-0.0101,0.1142,0.0015,0.0105,-0.0345,-0.0441,0.0268,-0.0162,-0.0319,-0.0502,0.0335,0.0153,0.0394,0.0347,-0.044,0.0304,-0.0094,-0.0483,-0.013,0.0969,0.0079,-0.0989,-0.0072,-0.0152,0.0821,0.0172,0.0311,0.0517,-0.0048,0.021,0.0676,-0.0089,-0.0461,0.0178,0.0481,-0.0031,0.0276,-0.0576,-0.0527,0.0375,0.0097,0.0038,-0.0149,-0.0672,0.0201,-0.0424,-0.0796,-0.0102,-0.0357,0.0638,-0.0238,-0.0645,-0.0046,-0.0397,0.0067,-0.0252,0.04,0.0115,-0.0517,0.0441,0.0187,0.0246,-0.0266,-0.0335,0.0263,0.0569,-0.0258,-0.0589,0.0603,0.0003,-0.0313,-0.0155,0.0138,0.0409,0.0117,0.0396,0.0208,-0.0232,-0.0839,-0.2136,-0.045,0.0059,0.0024,0.0382,-0.066,0.0052,-0.0054,0.0679,0.148,0.0368,0.0055,-0.0349,0.036,-0.0193,0.0508,0.0305,0.0058,0.0191,-0.019,-0.0655,0.049,0.0121,-0.033,0.0665,-0.0146,0.1724,0.0536,0.0122,0.0069,0.08,0.0096,-0.0219,-0.0821,0.067,0.0247,0.044,-0.0184,-0.0157,-0.0206,0.0496,0.0096,0.0032,-0.0441,-0.0282,-0.0784,-0.027,-0.0013,-0.0408,-0.0053,0.0363,-0.0473,0.0627,-0.0265,-0.003,-0.0658,-0.0479,0.0117,-0.0341,0.0364,-0.0126,-0.091,0.0169,-0.0411,0.0535,0.0158,-0.0245,0.0101,0.0269,-0.042,-0.0267,0.0981,0.0367,0.0249,0.0847,0.0124,0.024,-0.0306,-0.0233,-0.0047,0.0704,-0.037,0.0215,-0.0074,0.0374,0.0158,0.0612,0.004,-0.0023,-0.0419,0.0346,-0.0043,-0.0713,0.0204,0.0462,-0.0004,-0.3072,-0.0013,0.0176,-0.0364,-0.0362,0.0249,0.0268,-0.008,-0.0631,-0.0084,-0.0425,0.0813,0.0251,-0.0437,0.0287,0.0138,0.0645,-0.0275,0.0628,-0.0527,-0.0069,0.0386,0.1973,-0.0158,0.0034,-0.0061,-0.001,0.0456,-0.005,-0.0452,-0.019,-0.0168,0.0892,-0.0497,0.0491,0.0307,0.0049,0.058,0.026,0.0301,-0.0545,0.0047,-0.0305,-0.0152,0.0954,-0.0265,-0.0174,-0.0235,0.023,-0.0117,-0.0053,0.0182,0.0624,-0.0052,-0.0376,0.0046,-0.0392,-0.0435,-0.0276,-0.0313,0.0086,-0.0371,-0.0303,0.0387,0.0008]}
{"key":"[Scaling Laws and Interpretability of Learning from Repeated Data] Recent large language models have been trained on vast datasets, but also often on repeated data, either intentionally for the purpose of upweighting higher quality data, or unintentionally because data deduplication is not perfect and the model is exposed to repeated data at the sentence, paragraph, or document level. Some works have reported substantial negative performance effects of this repeated data. In this paper we attempt to study repeated data systematically and to understand its effects mechanistically. To do this, we train a family of models where most of the data is unique but a small fraction of it is repeated many times. We find a strong double descent phenomenon, in which repeated data can lead test loss to increase midway through training. A predictable range of repetition frequency leads to surprisingly severe degradation in performance. For instance, performance of an 800M parameter model can be degraded to that of a 2x smaller model (400M params) by repeating 0.1% of the data 100 times, despite the other 90% of the training tokens remaining unique. We suspect there is a range in the middle where the data can be memorized and doing so consumes a large fraction of the model's capacity, and this may be where the peak of degradation occurs. Finally, we connect these observations to recent mechanistic interpretability work - attempting to reverse engineer the detailed computations performed by the model - by showing that data repetition disproportionately damages copying and internal structures associated with generalization, such as induction heads, providing a possible mechanism for the shift from generalization to memorization. Taken together, these results provide a hypothesis for why repeating a relatively small fraction of data in large language models could lead to disproportionately large harms to performance.","layer":2,"vector":[-0.0177,0.0043,0.0313,-0.0118,-0.0028,0.021,0.0028,0.0122,0.0478,-0.0469,0.0035,-0.0337,0.0867,0.0442,0.0149,0.0274,0.0191,-0.0133,-0.0625,-0.0113,0.0393,-0.0177,-0.0325,0.0129,-0.0123,0.0401,-0.0475,0.0008,0.0024,-0.3029,0.01,-0.0358,0.0065,-0.0156,-0.0017,-0.0102,-0.0426,0.0361,-0.0186,0.0564,0.0018,0.0206,-0.038,-0.0442,-0.0331,-0.0149,-0.0622,-0.0235,-0.0113,-0.0097,0.0099,-0.0266,0.039,0.028,0.0486,0.0392,0.0718,0.0431,0.0451,0.0044,0.0206,0.0489,-0.1497,0.0393,0.0682,0.0293,-0.0634,-0.014,-0.0341,0.0583,-0.0272,0.0082,0.013,0.0417,0.0224,0.0193,-0.0193,-0.0242,0.0045,0.0143,0.0517,-0.0521,0.0056,-0.0383,-0.0192,-0.0788,-0.0228,-0.0091,0.0391,-0.0117,-0.0001,0.0042,-0.0204,0.0237,-0.0384,-0.0232,0.0441,0.0574,-0.0587,0.2116,-0.043,0.0089,-0.0028,0.003,0.0085,-0.0263,-0.0368,-0.0012,0.0285,-0.0095,-0.0362,-0.0299,0.0091,-0.0253,0.01,0.0446,0.0767,0.0215,-0.0413,0.0384,-0.0318,-0.0068,0.0316,-0.0312,0.0197,-0.067,0.0167,0.1403,0.0485,-0.0045,0.0198,-0.0193,-0.068,-0.0241,0.0229,0.024,0.03,-0.0357,0.0419,-0.0125,-0.0377,-0.0405,0.0097,-0.0589,-0.0535,0.1384,-0.044,0.0315,-0.0337,0.0007,-0.0264,0.0325,0.0242,-0.0854,0.0546,0.0489,0.0234,0.0071,-0.0354,-0.0112,-0.0093,-0.0294,-0.0468,0.0555,0.0193,-0.007,-0.047,0.0085,0.0089,-0.015,0.069,0.0437,-0.0388,0.0088,0.0293,0.0174,-0.0765,-0.0345,0.0048,0.0255,0.0456,-0.0547,-0.0525,0.0765,0.0231,-0.05,-0.0145,-0.0214,0.0624,0.0446,-0.0309,0.0439,-0.0283,-0.0177,-0.0495,-0.0102,0.0026,-0.0149,0.0195,-0.0522,-0.007,0.0126,-0.0373,0.0456,0.0014,0.042,0.02,0.0217,0.0403,0.0221,-0.022,-0.0158,0.0297,-0.0349,0.0057,-0.0404,-0.0161,0.0316,0.0313,0.0324,0.0075,-0.0509,-0.0452,-0.235,-0.0243,0.0301,-0.0336,0.088,-0.049,0.0481,0.0204,0.0496,0.0572,-0.0159,-0.0471,-0.0484,0.0086,-0.0304,0.0294,0.0032,0.0439,-0.0248,0.0138,0.0091,0.0131,-0.0109,-0.1099,0.0255,-0.0396,0.2305,0.03,0.0175,-0.0548,0.0516,0.0468,-0.0197,-0.0892,0.0902,-0.0007,0.0342,0.0081,-0.0049,0.0005,-0.0271,0.0156,0.0259,-0.0837,-0.0501,-0.0051,-0.0426,-0.0174,-0.0753,0.0431,0.0263,-0.0343,0.0298,0.01,-0.0222,-0.0201,-0.109,0.0269,-0.0329,0.0191,0.0446,-0.0231,0.0261,-0.0777,0.0509,0.0234,-0.0202,-0.0465,-0.0074,0.0035,-0.0168,0.0712,-0.0244,-0.0597,0.0212,0.0306,-0.0093,-0.0779,-0.0628,-0.0114,0.0579,0.0015,0.0463,0.0219,0.0582,-0.0089,0.0749,-0.0028,0.0541,-0.035,0.002,0.0206,-0.0935,-0.037,0.029,-0.0379,-0.2898,0.0263,-0.0284,0.0632,-0.0158,0.0715,-0.0115,0.0169,-0.0168,-0.0192,0.0218,0.0875,0.0504,-0.0376,0.0303,0.0607,0.074,-0.036,0.0305,-0.0447,0.0314,0.0756,0.1988,-0.0208,-0.0079,-0.002,-0.0247,0.0029,0.0565,0.0039,-0.0068,-0.0096,0.0869,-0.015,0.0314,0.0561,-0.0459,0.0383,0.0595,-0.0238,0.0201,0.0192,-0.0948,-0.0652,0.1215,0.0102,0.0035,-0.0853,-0.0443,0.0506,0.0035,-0.0254,0.0061,-0.0022,0.036,0.0448,-0.0529,-0.0301,-0.0103,-0.041,-0.0059,-0.0453,-0.0404,0.0358,-0.0047]}
{"key":"[Auditing ML Models for Individual Bias and Unfairness] We consider the task of auditing ML models for individual bias/unfairness. We formalize the task in an optimization problem and develop a suite of inferential tools for the optimal value. Our tools permit us to obtain asymptotic confidence intervals and hypothesis tests that cover the target/control the Type I error rate exactly. To demonstrate the utility of our tools, we use them to reveal the gender and racial biases in Northpointe's COMPAS recidivism prediction instrument.","layer":6,"vector":[-0.0451,-0.0287,0.0335,-0.0135,0.0286,0.0435,0.0382,0.0439,0.0362,-0.0034,0.0517,-0.0236,-0.0221,0.021,0.0135,0.0311,-0.0176,0.0462,-0.0346,0.0299,0.0007,-0.0445,-0.0451,-0.0685,0.0072,0.0049,-0.0356,-0.0457,-0.0692,-0.2179,0.0094,-0.0728,0.0574,-0.0116,-0.0004,-0.0293,-0.0398,0.0277,-0.0342,0.0502,0.0446,0.0209,-0.0191,-0.0466,-0.0354,-0.0444,-0.0362,0.035,-0.0518,-0.0335,-0.0056,-0.0453,0.0047,0.0441,-0.0005,0.0026,0.0827,0.0384,0.0401,0.0256,0.0161,0.0236,-0.1691,0.0567,0.0353,0.0739,-0.0356,-0.0512,0.0087,0.0175,-0.0253,0.0087,0.0154,0.0539,0.0038,-0.01,0.0415,-0.0114,0.0002,0.026,-0.049,-0.0356,-0.0195,0.0065,0.0053,-0.0511,0.0269,0.0006,0.0525,0.0022,-0.0083,-0.0202,0.0075,0.0059,-0.0441,0.0108,0.0453,0.0101,-0.0604,0.1962,-0.0288,0.0216,0.0167,-0.0177,0.0661,-0.0283,-0.0236,-0.0677,-0.0343,-0.0281,0.0328,-0.0035,0.0277,-0.0542,0.0067,0.0554,0.0645,0.0078,-0.0044,-0.0052,-0.055,0.0361,0.0447,-0.0256,0.0107,-0.0703,0.0149,0.1736,0.0265,0.0039,-0.009,-0.0649,-0.0472,-0.0294,0.0089,0.0184,0.017,0.0201,0.0514,-0.0118,-0.0626,-0.0916,-0.0194,-0.0802,-0.0615,0.1403,-0.0546,0.0299,-0.0234,-0.0179,-0.0318,0.0532,-0.0282,-0.0359,0.0376,0.0244,-0.0051,0.0464,-0.0557,0.0757,0.0259,-0.0307,-0.0498,0.1118,-0.0018,-0.0512,0.0133,0.0467,-0.0066,-0.015,0.0569,-0.0024,-0.0306,0.0516,0.0157,-0.032,-0.0495,0.0296,-0.0117,-0.0018,0.0259,-0.0578,-0.0595,0.0537,0.0533,-0.0248,0.0106,-0.0485,0.0435,0.0905,-0.0485,0.0253,-0.0501,0.0166,-0.0179,-0.0528,-0.0115,-0.0187,0.0112,-0.0107,-0.0196,0.0236,-0.0543,0.0224,0.0354,0.0389,-0.0214,-0.0246,0.0594,0.0008,-0.0259,-0.0206,0.0435,-0.0182,-0.0176,0.0073,0.0559,0.0201,0.0731,0.0489,-0.0287,-0.0493,-0.0335,-0.2431,-0.0054,-0.0026,0.0245,0.0345,-0.0515,0.0526,0.003,0.0352,0.1014,0.0507,-0.0274,-0.0522,0.0763,0.0254,0.0377,-0.0348,0.0385,-0.0248,0.0052,-0.019,0.0187,-0.0052,-0.032,0.0594,-0.0136,0.1994,-0.002,0.0154,-0.0457,0.0369,-0.0142,0.0103,-0.1103,0.0938,0.0153,0.0168,0.0003,-0.0357,0.0008,-0.0029,0.03,0.0196,-0.1008,-0.0801,-0.0405,-0.0274,0.0337,-0.0592,0.0441,-0.0015,-0.0724,0.0559,-0.0043,0.0026,-0.0608,-0.1136,0.0353,-0.0399,0.0416,0.0501,-0.0493,0.0348,-0.0797,0.064,0.0019,-0.0049,-0.0496,0.0321,-0.0401,-0.007,0.0895,-0.0013,-0.0353,0.0366,0.0221,-0.0154,-0.108,-0.0587,-0.0227,0.0651,-0.0188,0.0185,0.0141,0.0463,0.0049,0.047,0.0231,0.0693,-0.0241,-0.0309,-0.0361,-0.0744,-0.0166,0.0019,0.0074,-0.2875,0.0285,-0.0168,0.0443,-0.043,0.0013,0.0591,-0.0003,-0.0188,-0.0235,0.0327,0.0712,0.0439,0.0015,0.0137,0.0015,0.0668,-0.0274,0.0412,-0.0687,0.0389,0.0398,0.1853,-0.0506,0.0297,0.044,0.036,-0.0389,0.0452,-0.0419,0.0075,0.0122,0.0767,-0.0186,0.0528,0.0678,-0.0458,0.0298,0.0431,-0.0193,-0.0512,0.0031,-0.0237,0.0224,0.0979,-0.0079,-0.0323,-0.0497,0.0457,0.0341,-0.049,0.0196,-0.0148,0.0158,0.037,0.0571,-0.072,-0.0112,-0.0127,-0.0645,0.0061,-0.024,0.0051,-0.0071,-0.0062]}
{"key":"[Tensor Graph Convolutional Networks for Multi-relational and Robust Learning] The era of \"data deluge\" has sparked renewed interest in graph-based learning methods and their widespread applications ranging from sociology and biology to transportation and communications. In this context of graph-aware methods, the present paper introduces a tensor-graph convolutional network (TGCN) for scalable semi-supervised learning (SSL) from data associated with a collection of graphs, that are represented by a tensor. Key aspects of the novel TGCN architecture are the dynamic adaptation to different relations in the tensor graph via learnable weights, and the consideration of graph-based regularizers to promote smoothness and alleviate over-parameterization. The ultimate goal is to design a powerful learning architecture able to: discover complex and highly nonlinear data associations, combine (and select) multiple types of relations, scale gracefully with the graph size, and remain robust to perturbations on the graph edges. The proposed architecture is relevant not only in applications where the nodes are naturally involved in different relations (e.g., a multi-relational graph capturing family, friendship and work relations in a social network), but also in robust learning setups where the graph entails a certain level of uncertainty, and the different tensor slabs correspond to different versions (realizations) of the nominal graph. Numerical tests showcase that the proposed architecture achieves markedly improved performance relative to standard GCNs, copes with state-of-the-art adversarial attacks, and leads to remarkable SSL performance over protein-to-protein interaction networks.","layer":0,"vector":[-0.0194,-0.034,-0.0066,-0.017,0.0113,0.0271,0.0183,0.0204,-0.0143,-0.0189,0.019,-0.0737,0.0712,0.0991,0.0298,0.0384,0.0364,0.063,-0.0523,0.0261,0.0048,-0.0753,0.0215,-0.0499,0.0721,0.0394,0.0115,-0.0234,-0.0863,-0.2501,0.0123,-0.0623,0.0368,-0.0065,-0.0064,-0.023,0.0111,0.0125,-0.0176,0.0396,0.0152,-0.0067,-0.0269,-0.0708,-0.0368,-0.0234,-0.0093,-0.0133,-0.0281,-0.0272,0.0063,-0.0565,0.0078,0.0181,0.0576,0.0325,0.063,0.0016,0.0415,0.071,0.0403,0.0339,-0.1481,0.0494,0.017,0.0338,-0.056,0.0277,0.0298,0.063,0.0171,0.0794,0.0292,0.0096,0.0369,0.0115,0.0138,-0.0156,0.0046,0.0093,0.0384,-0.0314,-0.0418,-0.0347,0.0138,-0.037,0.005,-0.0364,0.0529,0.0351,-0.0644,-0.048,-0.0423,0.0472,-0.028,-0.0213,0.0562,0.0247,-0.0547,0.1778,-0.0782,0.0478,0.0449,0.0149,0.0006,-0.0296,0.0006,-0.0403,-0.0237,0.0317,-0.0014,-0.0296,0.018,-0.068,0.0385,-0.0062,0.0602,0.0422,-0.0283,-0.0103,-0.0407,0.0236,0.0321,-0.0617,0.0016,-0.0507,-0.016,0.1444,0.0405,0.041,0.0386,0.0346,-0.0491,-0.0088,0.0125,0.0342,0.014,-0.0068,-0.0237,0.0011,-0.0329,-0.0371,0.0273,-0.0778,-0.0554,0.1576,-0.0413,-0.0273,-0.0219,-0.008,-0.0151,0.0206,-0.0175,0.0335,-0.0018,0.0465,0.0533,0.043,-0.0518,0.0224,-0.0352,-0.0653,-0.0435,0.0885,0.0555,-0.1165,-0.0355,-0.0087,-0.0003,0.0145,0.0489,0.0369,-0.0353,0.0075,0.0596,0.0136,-0.0905,-0.008,-0.0127,0.0051,-0.0056,-0.0142,-0.0177,0.0608,0.0155,0.0003,0.0211,-0.0316,-0.0065,0.0099,-0.0388,0.0541,-0.0538,-0.0358,-0.0508,0.0046,-0.0122,-0.0204,-0.0484,-0.0282,-0.0257,-0.0283,-0.0259,0.016,-0.0168,0.0189,-0.0476,0.0072,-0.0326,-0.0039,-0.0239,-0.0224,0.0026,-0.0115,-0.0423,0.0165,0.0232,0.0377,-0.032,0.0507,0.0472,-0.0561,-0.0493,-0.2256,-0.0106,-0.0065,-0.0056,0.0634,-0.0717,0.0628,0.0042,0.0638,0.0908,0.0826,0.0366,-0.0455,-0.0062,-0.0073,0.0423,0.0557,0.0329,0.0155,-0.0225,-0.0114,0.0341,0.0137,-0.0807,0.0531,0.0397,0.2403,0.0504,0.0452,-0.0265,0.0078,0.0598,-0.0343,-0.0917,0.0676,0.0473,0.0649,-0.0217,-0.0412,-0.0358,-0.0347,-0.0045,0.0352,-0.1109,-0.037,-0.0316,-0.0058,0.0063,-0.0427,0.0094,0.0323,-0.0382,0.0649,-0.0049,-0.0259,-0.0499,-0.0989,0.0628,-0.0505,-0.0027,0.0276,-0.0666,-0.011,-0.0379,0.077,-0.001,-0.0501,-0.0043,0.0056,-0.0106,-0.0047,0.0703,0.0485,-0.0017,0.076,-0.0175,0.0443,0.0007,-0.0102,-0.0084,0.084,-0.0527,0.0173,0.0261,0.0439,0.0081,0.0732,0.0095,0.0638,-0.0508,-0.0005,-0.0042,-0.0566,-0.0324,0.0554,-0.0017,-0.307,0.0049,0.0367,0.0421,-0.0593,-0.0011,0.0188,0.044,-0.0217,-0.0565,0.0265,0.0376,0.0475,-0.01,-0.0333,0.0278,0.0665,-0.0508,0.0216,-0.0419,-0.0023,0.0108,0.178,-0.0027,0.0388,0.0196,-0.0182,0.021,0.0365,-0.0119,-0.0397,0.0327,0.0768,-0.0707,0.0414,0.0405,-0.0211,0.0114,0.0219,-0.0038,-0.0074,0.0114,-0.0659,-0.0369,0.0679,-0.0147,-0.0354,-0.0629,0.0299,-0.0083,0.0374,-0.0204,-0.0349,0.0188,0.0263,-0.0027,-0.0213,-0.0395,-0.0399,0.0032,-0.0217,-0.0554,-0.0446,-0.0257,-0.0572]}
{"key":"[k-Same-Siamese-GAN: k-Same Algorithm with Generative Adversarial Network for Facial Image De-identification with Hyperparameter Tuning and Mixed Precision Training] For a data holder, such as a hospital or a government entity, who has a privately held collection of personal data, in which the revealing and/or processing of the personal identifiable data is restricted and prohibited by law. Then, \"how can we ensure the data holder does conceal the identity of each individual in the imagery of personal data while still preserving certain useful aspects of the data after de-identification?\" becomes a challenge issue. In this work, we propose an approach towards high-resolution facial image de-identification, called k-Same-Siamese-GAN, which leverages the k-Same-Anonymity mechanism, the Generative Adversarial Network, and the hyperparameter tuning methods. Moreover, to speed up model training and reduce memory consumption, the mixed precision training technique is also applied to make kSS-GAN provide guarantees regarding privacy protection on close-form identities and be trained much more efficiently as well. Finally, to validate its applicability, the proposed work has been applied to actual datasets - RafD and CelebA for performance testing. Besides protecting privacy of high-resolution facial images, the proposed system is also justified for its ability in automating parameter tuning and breaking through the limitation of the number of adjustable parameters.","layer":2,"vector":[-0.0332,0.016,0.0564,-0.0227,0.0064,0.0325,0.0686,-0.0443,0.0077,0.0207,0.0285,-0.0471,0.0594,0.0509,0.0207,-0.0216,0.0377,0.0108,-0.0313,0.038,0.0105,-0.0204,-0.0288,-0.0357,0.0098,-0.0173,0.0228,-0.054,-0.02,-0.2419,0.0656,-0.0369,0.0316,-0.0299,0.0098,-0.0288,-0.0379,0.0821,-0.0056,0.0426,-0.0396,0.0276,0.0028,-0.0585,0.0142,0.0453,-0.057,-0.0013,-0.0053,-0.0469,0.0414,-0.0254,0.0272,0.0623,0.0187,0.0177,0.0768,0.0065,0.0436,0.0587,0.0435,0.0429,-0.1389,0.0483,0.034,0.0576,-0.0177,-0.0596,0.0169,0.0189,-0.0236,0.0198,0.0228,0.0007,-0.0138,-0.0122,-0.0494,-0.0319,-0.0483,0.0103,0.0169,-0.0161,-0.0075,0.009,-0.0383,0.0087,0.0053,-0.0683,0.0372,-0.0286,-0.0306,0.0035,-0.0043,0.0033,-0.0559,-0.0429,-0.0262,0.0732,-0.0653,0.236,-0.0722,0.0171,0.0267,-0.0067,0.0159,-0.0433,-0.065,-0.0233,-0.0315,-0.0057,-0.0024,-0.0542,0.0128,-0.0421,0.008,0.0037,0.0182,0.0421,-0.0063,-0.052,0.0111,0.047,0.0175,-0.0181,0.0396,-0.0603,0.0181,0.1405,0.0213,0.0395,0.0158,-0.0217,-0.0374,-0.0126,-0.0326,-0.0034,0.0071,0.022,-0.0086,-0.0407,-0.0605,-0.0619,0.0127,-0.0481,-0.0183,0.0939,-0.0181,0.0532,-0.0054,-0.0484,-0.0067,0.0227,-0.0455,0.0165,0.0348,0.0263,0.0116,0.0769,-0.0155,0.057,0.0011,-0.0866,-0.0567,0.0909,0.0143,-0.1032,-0.0252,0.0218,-0.0031,0.0296,0.0212,0.0223,-0.0128,0.0384,0.0434,0.0416,-0.0342,-0.0061,-0.0365,0.0064,0.0023,-0.0685,-0.0639,0.0082,0.0221,-0.0502,0.0249,-0.0422,0.0258,0.0188,-0.0798,0.0272,-0.0304,-0.0597,-0.0169,-0.0316,-0.0103,-0.0454,-0.0042,-0.0239,0.0253,0.0146,-0.0198,0.0235,-0.0072,0.0371,0.0003,-0.0153,0.0502,0.029,-0.0593,0.0421,0.0598,-0.0274,-0.0156,-0.0145,0.0057,0.0825,-0.019,0.0563,0.0414,-0.0422,-0.0386,-0.2694,0.0247,-0.0208,-0.0005,0.0561,-0.0782,0.0703,-0.0055,0.0726,0.0456,0.0564,0.0267,-0.0421,0.0616,0.0162,0.0582,0.0427,0.044,-0.0388,-0.0247,0.0111,0.0433,0.0117,-0.1008,0.0565,-0.0093,0.1967,0.0139,0.0273,-0.0057,0.0094,0.0432,-0.0462,-0.1316,0.0521,0.0169,0.0215,0.024,-0.0328,-0.0432,-0.0172,0.0526,-0.0122,-0.0745,-0.0002,-0.007,-0.042,0.0227,-0.0532,0.0166,0.0503,-0.0042,0.0376,-0.0404,0.0035,-0.0497,-0.1155,0.0063,-0.0626,0.0413,-0.0276,-0.07,0.0091,-0.0662,0.0762,-0.0041,-0.0727,-0.0373,0.054,-0.0023,-0.0336,0.0726,0.0126,-0.0056,0.0733,0.0267,0.0578,-0.0175,-0.0588,-0.0157,0.063,0.0247,0.0647,0.0574,0.0192,-0.0084,0.091,-0.0012,0.0351,-0.057,-0.0112,0.017,-0.0377,-0.0586,0.0433,0.0044,-0.3056,0.0416,-0.0278,0.0651,-0.042,0.024,0.0281,0.0305,-0.0472,-0.0189,-0.0023,0.0509,0.0637,-0.0393,-0.0009,0.0189,0.0018,-0.0458,0.0302,-0.01,0.0277,-0.0289,0.1791,-0.0367,-0.0349,-0.0067,0.0179,0.0308,0.0238,-0.0285,-0.0197,0.0495,0.0625,-0.027,0.0386,0.0782,-0.0603,0.0342,0.0236,-0.0082,-0.0093,0.0102,-0.0321,0.0185,0.1161,0.0049,-0.0317,-0.0321,-0.0104,-0.0188,-0.0404,0.0202,-0.0332,0.036,0.0375,0.0162,-0.0704,-0.014,-0.0196,-0.0368,0.0227,0.0115,-0.0653,0.0065,-0.0162]}
{"key":"[Neural Structured Prediction for Inductive Node Classification] This paper studies node classification in the inductive setting, i.e., aiming to learn a model on labeled training graphs and generalize it to infer node labels on unlabeled test graphs. This problem has been extensively studied with graph neural networks (GNNs) by learning effective node representations, as well as traditional structured prediction methods for modeling the structured output of node labels, e.g., conditional random fields (CRFs). In this paper, we present a new approach called the Structured Proxy Network (SPN), which combines the advantages of both worlds. SPN defines flexible potential functions of CRFs with GNNs. However, learning such a model is nontrivial as it involves optimizing a maximin game with high-cost inference. Inspired by the underlying connection between joint and marginal distributions defined by Markov networks, we propose to solve an approximate version of the optimization problem as a proxy, which yields a near-optimal solution, making learning more efficient. Extensive experiments on two settings show that our approach outperforms many competitive baselines.","layer":1,"vector":[-0.0165,-0.0052,-0.0127,0.0187,0.0428,0.0323,0.0307,0.0026,0.0215,-0.0194,0.0255,-0.0333,0.0225,0.0575,0.0076,0.0308,-0.0088,0.097,-0.0226,-0.0091,0.0243,-0.0055,-0.0323,-0.0722,0.0533,0.0208,-0.0161,-0.0196,-0.0261,-0.242,0.0439,-0.068,0.009,-0.0114,0.0044,-0.0646,0.0007,-0.0034,-0.024,0.0644,0.0293,-0.0474,-0.024,-0.054,0.0015,0.0014,-0.0067,-0.0616,-0.0486,-0.0747,0.0018,-0.0453,0.0167,0.0373,0.0357,0.0459,0.0331,0.042,0.0181,0.0771,0.007,0.0444,-0.1454,0.0467,0.0617,0.054,-0.0875,0.0174,0.0044,0.0953,0.0145,0.043,0.0122,0.0525,0.019,0.0248,0.0103,-0.0199,-0.0311,-0.0135,-0.0051,-0.0361,-0.045,-0.0238,0.0213,0.0016,0.0215,-0.0425,-0.0059,0.0328,-0.0249,-0.0081,-0.0197,0.0219,-0.0757,-0.0051,0.0568,0.0123,-0.0831,0.2096,-0.0628,0.0329,-0.0045,-0.021,0.0182,-0.0175,-0.0262,-0.0229,-0.0453,-0.0123,-0.0028,-0.0327,-0.0137,-0.0669,-0.0108,0.0091,0.0877,0.082,0.0019,-0.0055,-0.0339,0.0373,0.0323,-0.045,0.0427,-0.0388,-0.0086,0.0956,0.0171,0.0573,0.0077,-0.0264,-0.0369,-0.0304,0.0344,-0.0273,0.028,0.0138,-0.0054,0.0225,-0.0107,-0.0212,0.0343,-0.0993,-0.0838,0.1015,-0.0548,0.0073,-0.0272,-0.0508,-0.0222,0.0116,-0.0481,-0.0361,-0.021,0.0308,0.0223,0.0557,-0.0725,-0.0014,-0.0206,-0.0392,-0.0748,0.1065,0.0372,-0.0785,-0.0316,-0.0021,-0.0097,-0.0459,0.0358,0.033,-0.0216,-0.0016,0.0547,0.0633,-0.0748,0.0115,-0.0015,-0.0188,0.0165,-0.0015,-0.0248,0.0308,0.029,-0.0418,0.0199,-0.0094,-0.004,0.0432,-0.0247,0.0377,0.0017,0.0225,-0.0548,-0.0096,-0.0315,0.001,0.0293,-0.0507,0.0154,-0.0398,-0.0312,0.0231,-0.045,-0.004,-0.0226,0.02,0.0408,-0.0062,-0.0257,-0.0132,0.0758,-0.0055,-0.0171,-0.0075,0.0172,0.0213,0.0179,0.0608,0.0258,-0.0551,-0.0349,-0.2152,0.0175,-0.0041,-0.011,0.072,-0.067,0.0077,-0.0034,0.0786,0.0758,0.0655,0.0007,-0.0348,-0.0006,-0.0216,0.0264,0.0221,-0.0012,-0.0049,0.0035,0.0007,-0.0136,0.0003,-0.0655,0.0619,0.0041,0.2684,0.0322,0.0394,-0.0042,-0.0063,0.0085,-0.0475,-0.0771,0.0761,0.0204,0.0237,-0.0122,-0.0792,-0.0304,-0.0364,0.0018,0.0057,-0.1128,-0.0075,-0.0251,-0.0538,0.0235,-0.0482,0.0073,0.0418,-0.0099,0.0648,0.0054,-0.0329,-0.0346,-0.0844,0.0047,-0.0316,0.0353,0.0337,-0.0641,-0.0238,-0.0515,0.0528,0.017,-0.0219,-0.0323,0.0092,-0.0107,-0.0218,0.0531,0.0126,-0.0143,0.0577,0.0233,0.0253,-0.0037,-0.0532,0.0084,0.0842,-0.0324,0.0618,0.0148,0.0178,0.0098,0.0643,-0.0125,0.0859,-0.0025,0.0348,0.0371,-0.0498,0.0167,0.0502,-0.0616,-0.2973,0.0259,-0.0017,0.0435,-0.0155,0.0159,0.0479,0.0444,-0.0527,0.0014,0.0502,0.0417,0.0723,-0.0322,-0.0447,0.0489,0.0434,-0.0552,0.0653,-0.0491,0.0365,0.046,0.2262,-0.0688,0.0731,0.0333,-0.0211,-0.0041,0.0132,-0.0274,0.0541,0.0168,0.0772,-0.0307,0.0487,0.0781,0.0043,0.0053,0.0137,-0.0341,0.0023,-0.0433,-0.0674,-0.0297,0.0796,0.0098,-0.0125,-0.0439,0.0015,0.0411,-0.0301,0.0128,-0.0211,-0.0144,0.026,0.0155,0.0028,-0.0477,-0.0814,-0.0486,-0.0006,-0.1056,0.0429,-0.0146,-0.0072]}
{"key":"[How Much Over-parameterization Is Sufficient to Learn Deep ReLU Networks?] A recent line of research on deep learning focuses on the extremely over-parameterized setting, and shows that when the network width is larger than a high degree polynomial of the training sample size $n$ and the inverse of the target error $\\epsilon^{-1}$, deep neural networks learned by (stochastic) gradient descent enjoy nice optimization and generalization guarantees. Very recently, it is shown that under certain margin assumptions on the training data, a polylogarithmic width condition suffices for two-layer ReLU networks to converge and generalize (Ji and Telgarsky, 2019). However, whether deep neural networks can be learned with such a mild over-parameterization is still an open question. In this work, we answer this question affirmatively and establish sharper learning guarantees for deep ReLU networks trained by (stochastic) gradient descent. In specific, under certain assumptions made in previous work, our optimization and generalization guarantees hold with network width polylogarithmic in $n$ and $\\epsilon^{-1}$. Our results push the study of over-parameterized deep neural networks towards more practical settings.","layer":2,"vector":[-0.0218,-0.0004,0.0389,-0.0035,-0.0176,0.0413,0.0039,0.0167,0.0329,-0.0497,-0.0033,-0.0422,0.0646,0.086,0.0228,0.0291,0.0342,0.0415,-0.0907,0.0083,0.0721,-0.0304,0.0242,-0.071,0.0276,-0.0318,-0.0014,-0.0545,-0.0241,-0.2681,-0.0184,-0.0421,0.0139,-0.0581,0.005,-0.0235,-0.0388,0.0474,-0.0477,0.0521,-0.0055,0.0685,-0.0494,-0.0567,-0.0165,-0.0314,-0.0164,-0.0381,-0.031,-0.0343,0.0554,-0.0167,0.029,0.0544,0.0116,0.0191,0.0329,0.0296,0.0606,0.055,-0.0044,0.0578,-0.1428,0.025,-0.0016,0.0043,-0.0551,-0.0211,0.0278,0.0385,0.009,0.0294,0.0045,0.0298,-0.0245,0.0226,0.012,-0.0694,0.0043,0.0515,0.0469,-0.0307,-0.0156,-0.0297,0.0084,-0.0355,0.0015,-0.0386,0.0284,-0.0293,-0.0123,-0.0242,-0.0221,0.0,-0.0626,0.0039,0.0604,0.0404,-0.0484,0.1658,-0.0263,0.0161,0.0689,0.0076,-0.0103,-0.0226,-0.052,0.0024,-0.0149,0.0113,-0.0253,-0.0527,0.0407,-0.0394,0.0168,0.0511,0.038,0.0493,-0.0276,-0.0017,-0.0187,-0.0203,0.015,-0.0118,0.0131,-0.0554,-0.0228,0.1187,0.0253,0.0535,0.0252,-0.0118,-0.0298,-0.048,0.0582,0.0253,0.0095,0.0081,0.0421,-0.0042,-0.0409,-0.0398,0.0036,-0.0572,-0.0541,0.1025,-0.0643,0.0437,-0.0242,-0.063,-0.0183,0.0261,-0.0172,-0.0538,0.0502,0.0271,0.029,0.0127,-0.0461,-0.0023,-0.0188,-0.0498,-0.0041,0.119,0.0029,-0.0722,-0.0584,-0.0054,0.0107,0.0015,0.0626,0.0129,-0.0431,0.0188,0.0485,0.0378,-0.1022,-0.0295,-0.0345,0.035,-0.0051,-0.0605,-0.0106,0.0488,0.0341,-0.0572,-0.0249,-0.0482,0.0072,0.0233,-0.0517,0.0196,-0.083,-0.003,-0.0245,-0.0275,-0.0171,-0.0188,0.0096,-0.0389,-0.0454,0.0206,-0.0307,0.0217,0.025,0.038,-0.0148,0.0087,0.0224,0.0329,-0.0561,0.0067,0.0223,-0.0678,-0.0055,0.0103,0.0164,0.0365,-0.0217,0.0667,0.0496,-0.0388,-0.0392,-0.2288,-0.0225,0.0113,-0.0648,0.0954,-0.0776,0.0526,0.0352,0.0456,0.0593,0.0289,-0.013,0.0033,-0.0185,-0.0046,0.0546,0.0486,0.0176,-0.0142,0.0124,0.013,0.0288,-0.0184,-0.0656,0.0721,0.0049,0.235,-0.0311,0.0893,-0.0157,0.0454,0.0461,-0.0175,-0.0734,0.0827,0.041,0.0798,0.0149,-0.0388,-0.0452,-0.0074,0.0278,-0.0024,-0.1201,-0.0305,-0.0333,-0.0626,-0.0057,-0.045,-0.0022,0.0246,-0.0335,0.0516,0.0021,0.012,-0.0446,-0.1293,0.04,-0.052,0.0022,0.0124,-0.0654,-0.0021,-0.0581,0.0516,0.0345,-0.0121,-0.0118,0.026,0.0237,0.0313,0.0719,-0.0091,0.0297,0.0488,-0.0162,0.0202,-0.0103,-0.0523,-0.0243,0.0754,-0.0382,0.0389,0.0049,0.0592,0.0215,0.111,-0.0307,0.0154,-0.0098,-0.0213,-0.0265,-0.0754,-0.0358,0.0283,-0.0086,-0.2794,0.0482,0.0187,0.0658,-0.0177,0.0145,0.0238,0.0214,-0.0456,-0.0118,0.0534,0.0638,0.0625,-0.0024,0.0345,0.0047,0.0445,-0.0287,0.0745,-0.0471,-0.0085,0.0131,0.1822,-0.0661,0.0313,-0.0261,-0.0249,-0.0199,0.0463,-0.0023,0.0097,0.0107,0.0813,-0.0612,0.0918,0.1129,-0.043,0.0429,0.0383,-0.0038,0.0236,-0.0459,-0.0318,-0.0376,0.117,-0.015,-0.007,-0.0167,0.0164,0.0342,-0.0294,-0.0024,-0.005,0.0065,0.0375,0.0462,-0.0523,-0.0583,-0.0569,-0.0427,0.0195,-0.0541,-0.0256,0.0041,-0.0235]}
{"key":"[Geometric Rates of Convergence for Kernel-based Sampling Algorithms] The rate of convergence of weighted kernel herding (WKH) and sequential Bayesian quadrature (SBQ), two kernel-based sampling algorithms for estimating integrals with respect to some target probability measure, is investigated. Under verifiable conditions on the chosen kernel and target measure, we establish a near-geometric rate of convergence for target measures that are nearly atomic. Furthermore, we show these algorithms perform comparably to the theoretical best possible sampling algorithm under the maximum mean discrepancy. An analysis is also conducted in a distributed setting. Our theoretical developments are supported by empirical observations on simulated data as well as a real world application.","layer":0,"vector":[-0.0525,-0.0243,0.039,-0.0177,0.0225,-0.0074,0.0386,0.0512,0.069,-0.0023,0.0096,-0.0377,0.039,0.0351,0.0202,0.0259,0.009,0.0135,-0.0149,-0.0008,0.0313,-0.0725,-0.0076,-0.0985,0.0349,0.0562,-0.0299,-0.0416,-0.0472,-0.2454,0.0247,-0.0462,0.0741,-0.0387,-0.0414,-0.0171,-0.0364,0.0455,0.0029,0.0583,0.0209,0.0117,-0.0434,-0.044,-0.0441,-0.0647,-0.045,-0.0298,-0.0031,0.0128,0.0145,-0.0197,0.0428,0.0062,0.0556,0.0329,0.0653,0.0609,0.1004,0.0726,-0.0034,0.0183,-0.1685,0.0458,0.0218,0.0291,-0.0518,-0.0424,0.0225,0.063,-0.0494,0.061,-0.0163,0.045,0.0048,-0.0263,-0.0067,-0.0479,-0.0528,0.0284,-0.0106,-0.0339,-0.0516,-0.0059,-0.0463,0.0045,0.015,-0.0559,0.0703,0.0448,-0.0184,0.0098,-0.0305,0.0191,-0.0657,-0.0061,0.0282,-0.0192,-0.0011,0.1847,-0.0092,0.0184,0.0417,-0.021,0.0294,-0.0754,-0.071,-0.0625,-0.0245,0.0183,-0.0227,-0.0151,0.0298,-0.0568,0.079,0.0044,0.0472,0.0431,0.0074,-0.0237,-0.0219,0.0608,0.0503,0.0134,0.0317,-0.0279,-0.0028,0.1458,0.0323,0.0089,0.0572,-0.0352,-0.0633,-0.0679,-0.0268,0.0252,0.003,0.0299,0.0271,0.0236,-0.0438,-0.1154,0.0018,-0.0909,-0.0059,0.1518,0.0012,0.0402,-0.0529,-0.0348,0.0368,0.0417,-0.0207,-0.0435,0.0369,-0.0022,0.0253,0.0705,-0.0564,0.0279,-0.0669,-0.0472,-0.0234,0.1037,-0.0124,-0.0551,-0.0071,0.0271,0.0129,-0.0021,0.0231,0.0256,-0.0113,0.0307,0.0952,-0.0208,-0.0813,0.0461,0.0412,0.002,0.0281,-0.0067,-0.0591,0.0171,0.0244,-0.0443,-0.0411,-0.0114,0.0294,0.0296,-0.0221,-0.0293,-0.021,-0.0192,-0.0244,-0.0431,0.0094,0.0118,0.0304,-0.0253,-0.0057,-0.0066,-0.0756,0.0091,0.0103,0.0342,-0.0194,-0.0036,0.0383,0.014,-0.0209,-0.0067,0.0347,-0.0195,-0.0633,0.0477,0.0073,0.0189,-0.0111,0.0401,0.022,-0.046,-0.0599,-0.2167,-0.0476,0.0112,0.0279,0.0593,-0.0498,0.0551,0.0001,0.0413,0.0864,0.0276,0.0067,-0.0598,0.0389,-0.0104,0.0528,-0.0178,0.0126,-0.014,-0.0121,-0.0164,0.0131,-0.0367,-0.0234,0.0289,-0.0372,0.1945,0.0345,0.0184,-0.031,0.0397,0.0348,-0.007,-0.0506,0.0474,0.0628,0.048,-0.0255,-0.0295,-0.0091,-0.0329,-0.0151,0.0003,-0.0904,-0.0438,-0.0226,-0.0487,0.0054,-0.0261,-0.0102,0.0226,-0.0416,0.0944,-0.0536,0.0063,-0.0508,-0.0696,-0.0055,-0.0269,0.0394,0.0444,-0.0476,0.0312,-0.0146,0.0699,-0.0394,0.0105,0.0013,0.0113,-0.0206,-0.0176,0.0846,0.0162,-0.0008,0.0462,0.0066,-0.0138,-0.0124,-0.0042,-0.0458,0.075,-0.0609,0.0166,0.0028,0.0411,0.0246,0.0569,0.0102,0.0113,-0.023,-0.0567,0.0381,-0.0556,-0.0135,0.0128,0.0276,-0.3073,0.0693,-0.0126,0.0052,-0.0419,-0.0101,0.0724,-0.0278,-0.0439,-0.038,-0.0119,0.1049,0.0475,-0.0008,-0.0074,0.0387,0.0429,-0.0449,0.0349,-0.0653,-0.0078,0.0256,0.2294,0.0018,0.0045,0.0357,-0.0198,0.0392,0.0055,-0.0497,0.0124,-0.0598,0.0531,-0.0681,0.0364,0.0846,0.0233,0.0448,0.0105,-0.0668,-0.0027,-0.0119,-0.0467,-0.0096,0.0782,0.0107,-0.0381,-0.056,0.0124,0.031,-0.0301,0.0225,0.037,-0.0327,0.0458,0.0423,-0.0415,0.0099,-0.0283,-0.0481,0.0578,-0.0377,-0.006,-0.0046,-0.0069]}
{"key":"[Phase transitions in nonparametric regressions: a curse of exploiting higher degree smoothness assumptions in finite samples] When the regression function belongs to the smooth classes consisting of univariate functions with derivatives up to the $(\\gamma+1)$th order bounded in absolute values by a common constant everywhere or a.e., it is generally viewed that exploiting higher degree smoothness assumption helps reduce the estimation error. This paper shows that the minimax optimal mean integrated squared error (MISE) rate increases in $\\gamma$ when the sample size $n$ is small relative to $\\left(\\gamma+1\\right)^{2\\gamma+3}$ (e.g., $\\left(\\gamma+1\\right)^{2\\gamma+3}=262144$ when $\\gamma=3$), and decreases in $\\gamma$ when $n$ is large relative to $\\left(\\gamma+1\\right)^{2\\gamma+3}$. In particular, this phase transition property is shown to be achieved by common nonparametric procedures. Consider $\\gamma_{1}$ and $\\gamma_{2}$ such that $\\gamma_{1}<\\gamma_{2}$, where the $(\\gamma_{2}+1)$th degree smoothness class is a subset of the $(\\gamma_{1}+1)$th degree class. What is interesting about our results is that they imply, if $n$ is small relative to $\\left(\\gamma_{1}+1\\right)^{2\\gamma_{1}+3}$, the optimal rate achieved by the estimator constrained to be in the smoother class is larger. In data sets with fewer than hundreds-of-thousands observations, our results suggest that one should not exploit beyond the third degree of smoothness. To some extent, our results provide a theoretical basis for the widely adopted practical recommendation given by Gelman and Imbens (2019). The building blocks of our minimax optimality results are a set of metric entropy bounds we develop in this paper for smooth function classes. Some of our bounds are original, and some of them refine and/or generalize the ones in the literature.","layer":1,"vector":[0.0249,-0.0423,0.0576,-0.0387,0.0166,0.0503,0.0171,0.0737,0.0577,-0.0319,0.0422,-0.0457,0.0234,0.0259,-0.0195,0.0432,0.0255,0.0378,-0.0478,0.01,0.0469,-0.0745,0.0077,-0.0601,0.0352,0.0208,-0.0463,-0.0553,-0.0363,-0.271,0.0374,-0.0518,0.0365,-0.0258,0.0202,-0.0178,-0.0211,0.0281,-0.0274,0.0469,0.0065,0.0452,-0.0345,-0.0879,-0.0215,-0.0524,0.0076,0.0062,-0.0689,-0.0279,-0.0043,-0.0082,0.0308,0.0244,0.0376,0.0171,0.0069,0.0131,0.0461,0.06,0.0158,0.0253,-0.1865,0.0095,0.0347,0.0098,-0.0191,-0.029,-0.0366,0.0463,-0.0158,0.0389,-0.0016,0.0503,-0.0029,-0.0301,-0.0126,0.0012,-0.0,0.0176,0.0254,-0.0357,-0.0159,-0.0255,-0.0128,-0.0684,0.0348,-0.0731,0.0222,0.0053,0.0206,-0.0329,-0.029,-0.0012,-0.0268,0.0107,0.053,0.0673,-0.0031,0.1684,-0.0188,0.0494,0.0043,-0.0444,0.0171,-0.0374,-0.0015,-0.0523,-0.0111,0.0192,-0.0196,0.0082,0.0376,-0.0347,0.0201,-0.0076,0.0509,-0.0029,0.0076,0.0081,-0.0131,0.0363,0.0679,-0.0282,0.0051,-0.0595,-0.0441,0.1305,0.0517,0.0277,0.0073,-0.0098,-0.0415,-0.0076,-0.0127,0.0045,0.0302,0.0255,0.0188,0.0048,-0.0312,-0.0788,0.0283,-0.1087,-0.0447,0.147,-0.0646,0.0384,-0.0859,-0.0288,-0.0005,0.0303,-0.0455,-0.0128,0.0242,0.0469,0.0488,0.0323,-0.0819,0.0114,-0.038,-0.0284,-0.0038,0.0825,-0.0177,-0.0375,0.0121,0.0178,0.0002,0.0053,0.0446,0.0526,-0.0576,-0.0055,0.0984,0.0225,-0.0531,0.021,-0.0239,0.0456,0.03,-0.016,-0.0537,0.0287,-0.0059,-0.0102,0.0064,-0.0649,0.0555,0.0923,-0.0156,0.0026,-0.0359,-0.0015,0.002,-0.0283,-0.0041,-0.0033,0.023,-0.0551,0.0292,-0.0093,-0.0304,0.0257,0.032,0.0522,-0.0068,-0.0356,0.0211,0.0147,-0.0331,-0.0336,0.0713,-0.0038,-0.0388,0.0583,0.0401,0.0303,-0.0052,0.085,0.0263,-0.0286,-0.0691,-0.2593,-0.0322,-0.0113,-0.0213,0.1199,-0.0716,0.0606,0.0476,0.0438,0.0876,0.0314,0.0361,-0.0479,0.0523,-0.0132,0.0569,0.0534,0.0225,-0.0377,-0.0061,-0.0279,0.0127,-0.0473,-0.0748,0.0449,-0.0488,0.1697,-0.0323,0.0006,-0.0496,0.0127,0.0141,-0.0045,-0.0497,0.0387,0.0577,0.0582,-0.0647,-0.0748,-0.0649,0.01,0.0085,0.0038,-0.0228,-0.0674,-0.0601,-0.0354,0.0246,-0.1147,0.0256,0.0358,-0.0336,0.0962,-0.0322,0.0328,-0.0334,-0.0873,0.0261,-0.0005,0.0512,-0.0215,-0.0391,0.0299,-0.0451,0.0253,-0.0019,-0.0171,-0.0603,0.0423,-0.0343,0.0057,0.0712,-0.0424,-0.0127,0.0649,0.0102,0.0224,-0.0261,-0.0591,-0.0417,0.077,-0.047,0.0278,-0.0257,-0.024,-0.0001,0.0935,-0.0282,0.0571,-0.0164,0.0316,0.0039,-0.0632,-0.0014,0.0395,-0.0249,-0.257,0.0117,-0.0186,0.0071,-0.0311,0.0149,0.0422,-0.0155,-0.0625,0.0466,0.0024,0.0512,0.0528,0.0112,0.0279,-0.0048,0.0543,-0.0371,0.0285,-0.0651,0.0544,-0.0059,0.1988,-0.0423,0.0316,-0.0054,-0.0283,0.0249,0.0288,-0.0219,0.0332,-0.0081,0.0727,-0.0336,0.0385,0.0783,-0.0546,0.0507,0.0313,-0.0067,0.0323,0.0132,-0.0558,-0.0205,0.1205,-0.0512,-0.0196,-0.0437,-0.0091,0.0247,-0.0187,0.0375,0.0167,0.0302,0.0268,0.0374,-0.0915,-0.0369,-0.0351,-0.0244,0.0105,-0.0541,-0.041,0.009,0.0051]}
{"key":"[Fast Training Algorithms for Deep Convolutional Fuzzy Systems with Application to Stock Index Prediction] A deep convolutional fuzzy system (DCFS) on a high-dimensional input space is a multi-layer connection of many low-dimensional fuzzy systems, where the input variables to the low-dimensional fuzzy systems are selected through a moving window across the input spaces of the layers. To design the DCFS based on input-output data pairs, we propose a bottom-up layer-by-layer scheme. Specifically, by viewing each of the first-layer fuzzy systems as a weak estimator of the output based only on a very small portion of the input variables, we design these fuzzy systems using the WM Method. After the first-layer fuzzy systems are designed, we pass the data through the first layer to form a new data set and design the second-layer fuzzy systems based on this new data set in the same way as designing the first-layer fuzzy systems. Repeating this process layer-by-layer we design the whole DCFS. We also propose a DCFS with parameter sharing to save memory and computation. We apply the DCFS models to predict a synthetic chaotic plus random time-series and the real Hang Seng Index of the Hong Kong stock market.","layer":3,"vector":[-0.0415,-0.0437,0.0139,-0.0041,0.0486,0.0722,0.0387,0.014,0.0651,-0.0487,0.0192,-0.058,0.0605,0.0535,0.0314,0.0008,-0.0088,0.002,-0.0773,0.0386,0.0697,-0.049,-0.0447,-0.0491,0.0232,-0.025,-0.0101,-0.0442,-0.0673,-0.2264,0.002,-0.0664,0.0599,0.0124,0.0034,-0.0672,-0.0586,0.0589,-0.0112,0.0366,-0.0204,0.0082,0.0272,-0.0251,-0.003,-0.0767,-0.0178,-0.0263,-0.0126,-0.0244,0.0234,-0.0238,0.0459,0.0127,0.0402,0.0354,0.0349,0.0073,0.0584,0.0188,0.058,0.0079,-0.1692,0.0485,-0.0059,0.0146,-0.0197,-0.0039,0.0468,0.0245,-0.0011,0.0549,0.0003,0.0555,0.0251,-0.0473,0.031,-0.0165,-0.043,-0.003,0.0028,0.0021,-0.0192,-0.059,0.0238,-0.0407,0.0549,-0.0461,0.0146,0.0197,-0.055,-0.0149,-0.0312,0.0427,-0.0611,0.0058,0.0203,0.0279,-0.0568,0.1754,-0.0696,0.0399,0.0502,-0.057,0.0208,-0.012,-0.0475,-0.0268,-0.0176,0.005,-0.0077,-0.0414,0.0238,0.0051,-0.0117,-0.0016,0.0386,0.072,0.001,-0.0114,0.007,-0.0116,0.0787,-0.0096,-0.0033,-0.0247,0.0127,0.1821,0.0172,0.0089,0.0192,0.005,-0.0482,-0.017,0.0122,0.043,0.0107,0.0088,0.0389,-0.0465,-0.0724,-0.0335,0.0142,-0.0823,-0.0304,0.0783,-0.0302,-0.0019,-0.0761,-0.0378,-0.0484,-0.0025,-0.0347,-0.0571,0.0436,0.0022,-0.0128,0.0441,-0.0244,0.0092,-0.0779,-0.0443,-0.0524,0.0903,0.0009,-0.1068,-0.0363,-0.0013,0.0309,-0.0141,0.0411,0.0177,-0.0372,0.0081,0.0876,0.0301,-0.0539,-0.0172,0.0236,0.0184,0.0013,-0.0392,-0.0204,0.0109,0.0348,-0.0545,-0.0019,-0.0454,-0.0068,0.0234,-0.0468,0.0199,-0.0527,0.0538,-0.0237,-0.0169,-0.0224,0.012,0.0224,-0.0512,-0.0268,-0.0259,-0.0811,0.0178,-0.0055,0.0109,-0.0087,0.0415,0.0535,0.0327,0.0328,0.0285,0.0595,-0.011,-0.0404,0.0276,0.0188,0.0695,-0.0301,0.0287,0.0543,-0.068,-0.0646,-0.2517,-0.0249,-0.0078,-0.0638,0.027,-0.0469,0.0206,0.0097,0.0769,0.0489,0.0739,-0.0025,0.0129,0.0046,0.015,0.0734,0.0271,0.056,-0.0334,-0.0028,0.007,0.0367,-0.0039,-0.0854,0.0678,0.0543,0.1796,-0.01,0.0457,0.0121,0.0504,0.0133,-0.0179,-0.079,0.085,-0.0034,0.0647,0.0014,-0.0484,-0.0552,-0.0483,0.0112,0.0206,-0.0696,-0.0293,-0.0123,-0.0273,0.0035,-0.0542,0.0192,0.0292,-0.0569,0.0318,-0.0141,0.0201,-0.0105,-0.1136,0.0328,-0.036,0.0043,-0.0075,-0.0551,-0.0047,-0.0201,-0.0052,-0.0158,-0.0003,-0.0241,0.0188,0.0036,-0.0495,0.0983,0.0356,0.0508,0.0672,0.0212,0.0153,-0.0059,-0.0089,0.0041,0.1007,-0.0215,0.0334,0.0804,0.0325,0.0152,0.0915,-0.0125,0.0318,-0.0355,-0.0123,0.0099,-0.0964,0.0134,0.0086,-0.0134,-0.2944,0.0273,0.009,0.049,0.0116,-0.0404,0.014,0.027,-0.0152,-0.0003,0.0026,0.0236,0.0387,-0.0584,0.0509,-0.0408,0.0454,-0.051,0.0538,-0.0208,0.0413,0.0502,0.2403,0.0027,0.0258,-0.0383,-0.0323,-0.0218,0.0245,-0.0105,0.0493,0.0668,0.0833,-0.0689,0.0338,0.0568,-0.0306,0.058,0.032,0.0365,-0.0401,0.0395,-0.0484,-0.0019,0.0773,-0.0185,-0.0345,-0.055,0.0145,0.0375,-0.0433,0.005,-0.0084,-0.0204,0.0561,0.0116,-0.0351,-0.0226,-0.0101,-0.0322,0.0238,-0.0752,-0.0088,-0.0553,-0.0208]}
{"key":"[Root Mean Square Layer Normalization] Layer normalization (LayerNorm) has been successfully applied to various deep neural networks to help stabilize training and boost model convergence because of its capability in handling re-centering and re-scaling of both inputs and weight matrix. However, the computational overhead introduced by LayerNorm makes these improvements expensive and significantly slows the underlying network, e.g. RNN in particular. In this paper, we hypothesize that re-centering invariance in LayerNorm is dispensable and propose root mean square layer normalization, or RMSNorm. RMSNorm regularizes the summed inputs to a neuron in one layer according to root mean square (RMS), giving the model re-scaling invariance property and implicit learning rate adaptation ability. RMSNorm is computationally simpler and thus more efficient than LayerNorm. We also present partial RMSNorm, or pRMSNorm where the RMS is estimated from p% of the summed inputs without breaking the above properties. Extensive experiments on several tasks using diverse network architectures show that RMSNorm achieves comparable performance against LayerNorm but reduces the running time by 7%~64% on different models. Source code is available at https://github.com/bzhangGo/rmsnorm.","layer":0,"vector":[-0.0295,-0.0387,0.0372,-0.0152,0.0369,0.0582,-0.0084,0.0178,0.0062,-0.0363,0.015,-0.0294,0.0732,0.0791,0.0456,0.0219,0.0534,0.0331,-0.0374,0.0318,0.0351,-0.0253,-0.0025,-0.0333,0.0453,-0.0349,-0.0619,-0.0314,-0.0289,-0.2914,0.0282,-0.0382,0.0686,-0.0178,-0.0298,-0.0227,-0.081,0.0239,0.0472,0.0499,0.0222,0.0327,0.0008,-0.0845,0.0185,-0.0112,-0.0668,-0.0163,-0.0046,-0.0434,0.0447,-0.0676,0.0152,0.0298,0.0373,0.0308,0.042,0.0468,0.0801,0.0567,-0.0088,0.0363,-0.16,0.0218,0.043,0.0028,-0.0487,-0.0382,0.0027,0.0536,-0.0009,0.0355,0.0099,0.0172,0.0247,0.006,0.0067,-0.0239,-0.0236,0.0277,0.0371,-0.0092,-0.025,-0.0259,0.0245,-0.0311,0.0219,-0.0536,0.015,0.0034,-0.0623,-0.0049,-0.0376,0.0388,-0.0218,-0.018,0.0274,0.0071,-0.075,0.2041,-0.045,0.0396,0.0215,-0.0224,0.0635,-0.0221,-0.0243,0.0011,-0.022,0.0426,-0.0215,-0.0369,0.0006,-0.0333,0.0407,0.0149,0.0621,0.0284,-0.0457,-0.0195,-0.0344,-0.0136,-0.0214,-0.0373,0.0554,-0.0783,0.0261,0.1383,0.0525,0.0514,0.0397,-0.0381,-0.0441,-0.0356,0.0303,0.0323,0.0271,0.0012,-0.0371,-0.0315,-0.0341,-0.024,0.022,-0.0631,-0.0515,0.1034,-0.0465,-0.0337,-0.0349,-0.0136,-0.0205,0.0027,-0.0347,-0.0237,0.0633,0.0282,0.0244,0.0354,-0.0683,-0.0437,-0.0216,-0.0817,-0.044,0.0976,0.0078,-0.0634,-0.0308,-0.0055,0.0152,0.0032,0.0482,-0.0026,-0.0064,0.0047,0.1041,0.058,-0.0587,-0.0279,-0.0027,-0.0059,0.0316,-0.0344,-0.0288,0.0242,0.0365,-0.0374,0.0145,-0.0497,-0.0022,0.011,-0.0649,0.0004,-0.089,-0.0002,0.0066,0.0003,-0.0161,0.0327,0.0005,-0.0296,0.0218,0.044,-0.0373,-0.052,-0.0203,0.0667,-0.0265,-0.0122,0.0304,0.0651,-0.036,-0.033,0.0776,-0.064,-0.0556,0.0171,0.0167,0.0697,-0.0131,0.0451,0.0618,0.0059,-0.067,-0.1953,-0.0038,0.0197,-0.0278,0.0771,-0.0359,0.0545,0.0054,0.0465,0.0256,0.0258,0.0291,-0.011,0.025,0.0004,0.0686,0.0529,0.0195,-0.0009,-0.0203,-0.0272,0.0435,-0.0014,-0.1012,0.0661,0.0119,0.1928,-0.0266,0.0358,-0.032,0.0043,0.0195,0.0032,-0.1021,0.0405,0.0136,0.0951,0.0226,-0.0463,0.0102,-0.0189,0.0153,0.0301,-0.1455,-0.0556,-0.0131,-0.0474,-0.0152,-0.049,-0.0383,-0.0032,-0.0641,0.0557,0.0017,-0.0025,-0.0444,-0.1087,-0.0046,-0.0707,0.0187,0.0019,-0.068,0.0224,-0.0646,0.0585,-0.013,0.0013,-0.0319,0.0232,-0.0108,-0.0331,0.0198,0.0301,-0.0096,0.0366,0.0036,0.045,0.0015,-0.0263,-0.0089,0.067,-0.0374,0.027,0.004,0.0582,0.0475,0.0877,-0.0041,0.0322,0.0213,-0.0438,-0.0074,-0.0412,-0.0246,0.0245,0.0013,-0.294,0.0182,0.0187,0.0555,-0.0034,0.007,0.023,0.0286,-0.0456,-0.0012,0.0047,0.0695,0.0461,0.0126,0.0,0.0223,0.05,-0.0235,0.0567,-0.0534,0.0243,0.036,0.2161,-0.0458,-0.0132,0.0048,-0.0189,0.0015,0.0652,0.0021,0.0057,0.0391,0.0721,-0.0195,0.0649,0.0631,-0.0067,0.0238,0.0457,0.0138,-0.04,-0.0195,-0.0708,-0.0171,0.0881,-0.0472,-0.0179,-0.0061,-0.0092,0.0137,-0.0138,-0.0015,0.0076,-0.0193,0.0669,0.0192,-0.0675,-0.055,-0.0381,-0.0327,0.0301,-0.0829,0.0039,0.0052,-0.0292]}
{"key":"[SHREC 2020 track: 6D Object Pose Estimation] 6D pose estimation is crucial for augmented reality, virtual reality, robotic manipulation and visual navigation. However, the problem is challenging due to the variety of objects in the real world. They have varying 3D shape and their appearances in captured images are affected by sensor noise, changing lighting conditions and occlusions between objects. Different pose estimation methods have different strengths and weaknesses, depending on feature representations and scene contents. At the same time, existing 3D datasets that are used for data-driven methods to estimate 6D poses have limited view angles and low resolution. To address these issues, we organize the Shape Retrieval Challenge benchmark on 6D pose estimation and create a physically accurate simulator that is able to generate photo-realistic color-and-depth image pairs with corresponding ground truth 6D poses. From captured color and depth images, we use this simulator to generate a 3D dataset which has 400 photo-realistic synthesized color-and-depth image pairs with various view angles for training, and another 100 captured and synthetic images for testing. Five research groups register in this track and two of them submitted their results. Data-driven methods are the current trend in 6D object pose estimation and our evaluation results show that approaches which fully exploit the color and geometric features are more robust for 6D pose estimation of reflective and texture-less objects and occlusion. This benchmark and comparative evaluation results have the potential to further enrich and boost the research of 6D object pose estimation and its applications.","layer":0,"vector":[-0.0187,-0.0204,0.0694,-0.0328,0.0386,0.0793,0.0046,-0.0062,0.0011,0.0162,0.0472,-0.0727,-0.0059,0.0451,-0.0392,0.0185,0.029,0.0613,-0.0426,0.0141,0.0169,-0.074,-0.0066,-0.0401,-0.0056,0.0346,-0.0487,-0.0111,-0.0327,-0.2369,-0.0337,-0.0614,0.0588,-0.0218,-0.0075,-0.0292,-0.0433,0.0497,-0.0292,0.0099,0.0236,-0.0081,-0.0431,-0.0087,-0.0194,-0.0084,-0.0092,0.0106,0.0098,-0.0504,0.0283,-0.0425,0.0271,0.0027,0.0429,0.0834,0.0634,0.0132,0.0569,0.0139,0.049,0.0201,-0.1598,0.0506,0.0475,0.0038,-0.038,-0.0421,-0.0101,-0.0033,-0.0422,0.0457,0.0256,0.0687,0.0021,-0.0552,-0.0074,-0.0545,0.0001,-0.0706,-0.0118,0.0215,-0.0435,0.0178,0.0163,-0.0321,0.0122,-0.0308,0.0237,0.0435,-0.0609,0.0329,-0.0666,0.0056,-0.066,-0.0456,-0.0029,0.0069,-0.0021,0.2014,-0.0467,0.0458,0.0509,-0.0297,0.0062,-0.0623,-0.0117,-0.0051,-0.0269,-0.0345,0.021,-0.0061,0.0247,-0.0456,0.0371,0.0034,0.0642,0.0392,-0.0077,-0.0263,-0.0074,0.0328,0.0501,-0.0265,0.0235,-0.0586,-0.0071,0.1447,0.0419,0.0553,0.0566,-0.0194,-0.053,-0.0469,-0.0236,0.0584,0.0287,-0.0333,0.0176,0.0086,-0.0286,-0.0538,0.041,-0.0837,-0.0327,0.1213,-0.0788,0.0315,-0.0656,-0.0082,-0.0049,0.0503,-0.0054,0.0187,-0.0006,0.0265,-0.0148,0.0744,-0.0694,0.0299,-0.0299,-0.0212,-0.0303,0.0429,0.0142,-0.0978,-0.023,0.0058,0.0079,-0.0049,0.0071,0.0558,-0.0177,-0.0102,0.1172,0.0062,-0.1079,0.0016,0.0234,0.0067,-0.0204,-0.0511,0.0116,0.0083,0.0529,-0.042,0.0259,0.0053,0.0102,0.0624,0.0107,0.0103,-0.0885,0.0113,0.0082,0.0068,-0.0114,-0.0332,0.0444,-0.0032,0.0424,0.0103,-0.0444,-0.0047,0.0143,0.0177,-0.0425,-0.0032,-0.0008,0.0476,-0.0666,-0.0001,0.0502,-0.044,-0.0589,-0.0289,0.0512,0.0339,-0.0353,0.0129,0.0013,-0.065,0.0152,-0.237,0.0595,0.0073,-0.0148,0.0474,-0.0785,-0.0315,-0.0003,0.072,0.0284,0.0512,-0.0289,0.0073,0.0449,-0.0096,0.0324,0.0208,0.0028,-0.0372,-0.0126,-0.0279,0.01,-0.0368,-0.0587,0.0582,0.0197,0.2129,0.0338,0.006,0.0176,0.0674,0.0593,-0.0623,-0.096,0.0539,0.0597,0.032,-0.011,-0.029,-0.0164,-0.0512,0.0477,0.0408,-0.0878,0.0019,-0.0273,-0.0468,0.0587,-0.0089,0.0026,-0.0062,-0.024,0.017,-0.0138,-0.0378,-0.0379,-0.0678,-0.0025,-0.0243,0.0351,-0.0312,-0.0642,0.0409,-0.0532,0.0668,-0.0184,-0.0121,-0.0916,0.0469,-0.0541,-0.0258,0.0783,0.0185,0.036,0.0452,0.0025,0.0733,-0.0307,0.0313,-0.081,0.0178,0.0195,0.0025,0.0425,0.026,-0.0205,0.0982,0.0166,0.0238,-0.0707,0.0384,0.011,-0.0545,-0.0061,0.0708,0.0234,-0.3066,0.0187,0.0095,0.0875,-0.046,0.0181,0.0469,-0.0007,-0.0075,0.0135,-0.0097,0.0038,0.0478,-0.003,0.0176,0.0331,0.0332,-0.0083,0.0646,-0.0658,-0.0169,0.0744,0.1822,-0.0698,-0.0136,0.0426,-0.0162,-0.0276,0.0107,-0.0338,-0.0258,-0.0075,0.0556,-0.0472,-0.0251,0.1155,-0.0103,0.052,-0.0086,0.0018,0.0115,0.0154,-0.0063,0.0021,0.1142,0.0475,-0.017,-0.0065,0.025,-0.0218,-0.0526,-0.021,-0.0181,-0.0081,0.0614,0.0158,-0.0331,-0.024,-0.0607,0.0071,0.0364,-0.0378,-0.0442,-0.001,0.0008]}
{"key":"[Malware Classification Using Long Short-Term Memory Models] Signature and anomaly based techniques are the quintessential approaches to malware detection. However, these techniques have become increasingly ineffective as malware has become more sophisticated and complex. Researchers have therefore turned to deep learning to construct better performing model. In this paper, we create four different long-short term memory (LSTM) based models and train each to classify malware samples from 20 families. Our features consist of opcodes extracted from malware executables. We employ techniques used in natural language processing (NLP), including word embedding and bidirection LSTMs (biLSTM), and we also use convolutional neural networks (CNN). We find that a model consisting of word embedding, biLSTMs, and CNN layers performs best in our malware classification experiments.","layer":10,"vector":[-0.0137,-0.005,-0.0025,0.0003,0.0529,0.0058,0.0368,0.0219,0.0292,-0.0475,-0.004,0.0016,0.0482,0.0483,0.0639,0.003,0.0423,0.0264,-0.0349,0.0253,0.0512,-0.0425,0.0195,-0.0202,-0.0143,0.0328,0.0025,0.0045,-0.0808,-0.1845,0.0042,-0.0432,0.0267,-0.0425,0.02,0.0072,-0.0491,0.0117,-0.02,0.0246,-0.0112,0.0512,-0.023,-0.0497,-0.0568,-0.0706,-0.0403,-0.0312,0.0136,-0.0418,0.001,-0.023,0.0428,0.0273,0.0209,0.011,0.0647,0.0213,0.0417,0.049,0.0406,0.0399,-0.1536,0.0506,-0.0014,0.049,-0.0625,0.012,0.0176,0.0508,-0.047,0.0349,-0.0137,0.0396,-0.0271,0.0437,0.0051,0.0289,-0.0208,-0.0405,0.0114,-0.0071,-0.0311,-0.0479,-0.0036,-0.0626,-0.0192,-0.0367,0.0735,0.0009,-0.015,-0.0125,0.0128,0.0165,-0.0537,0.0014,0.0449,0.0122,-0.098,0.1985,-0.0915,-0.0224,0.0124,-0.0545,0.05,0.0158,0.0055,-0.0209,-0.0404,-0.0166,-0.0091,0.0205,0.0418,-0.0236,0.0508,-0.0153,0.0821,0.0238,-0.0208,0.0111,-0.0225,-0.0016,0.0544,-0.0392,0.0649,-0.0125,0.0411,0.166,0.0551,0.0143,0.0079,-0.0164,-0.0172,0.0085,0.0275,0.0851,-0.0264,0.001,0.0367,-0.0826,-0.0831,-0.0355,0.0397,-0.0751,-0.0668,0.1085,-0.0518,-0.0019,-0.044,-0.0208,-0.0263,0.025,-0.0062,-0.0831,0.0368,0.0058,0.0581,0.0618,-0.0729,-0.0092,0.0034,-0.027,-0.0297,0.1204,0.0228,-0.052,-0.0381,-0.0499,0.0056,-0.0235,0.091,0.0314,-0.0436,-0.0158,0.0053,0.022,-0.0627,-0.0241,-0.0093,0.001,-0.0242,-0.0776,-0.045,0.048,0.0691,-0.0307,0.0156,-0.0778,0.0366,0.0309,-0.0613,0.0498,-0.0026,-0.0427,-0.0058,-0.0305,-0.0041,-0.0041,0.0076,-0.0492,0.0146,0.0423,-0.0218,0.004,-0.0063,0.0007,0.0067,0.0088,0.0239,0.0256,-0.0327,0.0138,0.0314,-0.0442,-0.0246,-0.0504,-0.0292,0.0606,0.022,0.0415,0.0087,-0.048,-0.0397,-0.2186,-0.0261,-0.0014,-0.0651,0.0481,-0.09,0.0369,-0.0167,0.0766,0.0298,0.0658,-0.0254,0.0057,-0.0007,0.0011,0.0793,0.0475,0.0675,-0.0278,0.0476,-0.0287,-0.013,0.049,-0.0926,0.0262,0.0156,0.1989,0.0703,0.0602,-0.0707,0.0223,0.001,-0.0218,-0.123,0.062,0.0327,0.0572,0.0142,-0.0297,-0.0098,-0.08,0.0538,-0.0075,-0.0892,-0.0268,-0.0262,-0.0261,0.0201,-0.0244,0.0223,0.0223,-0.0317,0.0565,0.0496,0.0054,-0.0785,-0.1006,0.0339,-0.0435,0.0269,0.0037,-0.0611,0.0321,-0.0865,0.0407,-0.0012,-0.0545,-0.0202,0.0635,-0.0291,-0.0619,0.1006,0.0082,-0.0295,0.0662,-0.013,0.0427,-0.0543,-0.0758,-0.0271,0.0492,-0.0163,0.0299,0.0149,0.0475,0.0073,0.0741,0.0062,0.0548,-0.0117,0.0375,0.0043,-0.0307,-0.0221,-0.0219,0.0187,-0.2707,0.0546,-0.0023,0.0693,0.0012,-0.0132,0.0339,-0.0145,-0.0226,0.0178,-0.0187,0.03,0.0404,-0.0425,0.0129,0.0215,0.0516,-0.0732,0.0498,-0.0153,0.012,0.0223,0.24,0.0029,0.0113,-0.0011,0.0076,0.0411,0.0026,-0.0603,0.0544,0.0064,0.1008,-0.0389,0.0279,0.0767,-0.0208,0.0137,0.0306,-0.0183,-0.0015,0.0058,-0.0501,-0.0301,0.0438,-0.042,0.0313,-0.0594,0.0174,0.0634,-0.0485,-0.0094,-0.0107,0.0241,0.0529,0.0102,-0.0659,-0.0368,-0.0476,-0.0098,0.0064,-0.0516,-0.0066,0.0206,-0.0476]}
{"key":"[Phraseformer: Multimodal Key-phrase Extraction using Transformer and Graph Embedding] Background: Keyword extraction is a popular research topic in the field of natural language processing. Keywords are terms that describe the most relevant information in a document. The main problem that researchers are facing is how to efficiently and accurately extract the core keywords from a document. However, previous keyword extraction approaches have utilized the text and graph features, there is the lack of models that can properly learn and combine these features in a best way. Methods: In this paper, we develop a multimodal Key-phrase extraction approach, namely Phraseformer, using transformer and graph embedding techniques. In Phraseformer, each keyword candidate is presented by a vector which is the concatenation of the text and structure learning representations. Phraseformer takes the advantages of recent researches such as BERT and ExEm to preserve both representations. Also, the Phraseformer treats the key-phrase extraction task as a sequence labeling problem solved using classification task. Results: We analyze the performance of Phraseformer on three datasets including Inspec, SemEval2010 and SemEval 2017 by F1-score. Also, we investigate the performance of different classifiers on Phraseformer method over Inspec dataset. Experimental results demonstrate the effectiveness of Phraseformer method over the three datasets used. Additionally, the Random Forest classifier gain the highest F1-score among all classifiers. Conclusions: Due to the fact that the combination of BERT and ExEm is more meaningful and can better represent the semantic of words. Hence, Phraseformer significantly outperforms single-modality methods.","layer":1,"vector":[-0.0605,-0.0249,-0.0137,-0.0598,0.0211,0.0219,0.0125,0.0476,-0.0157,0.0041,0.0069,-0.0486,0.0586,0.0249,0.0565,0.0214,-0.0043,0.0553,-0.0493,-0.0109,0.0665,-0.029,0.0387,-0.0332,0.0298,-0.0053,-0.0361,-0.0334,-0.0495,-0.2202,0.0002,-0.0379,0.0397,-0.0174,-0.0391,0.0069,-0.0173,0.0393,-0.039,0.0488,-0.0098,-0.0285,-0.0107,-0.0453,-0.027,-0.0503,-0.0128,-0.0457,-0.0116,-0.0476,0.0131,-0.0438,0.04,0.0252,-0.0057,0.0125,0.0299,0.0321,0.0319,0.0317,0.0224,0.0443,-0.1791,0.0719,0.0547,0.0224,-0.0704,-0.0366,0.0191,0.0626,0.013,0.0069,0.0309,0.0198,0.027,0.0044,-0.0075,-0.0223,-0.019,0.0089,0.0038,-0.0233,-0.0198,-0.0082,-0.0248,-0.0418,0.0232,-0.0299,0.0437,0.004,-0.0466,-0.0351,-0.0357,0.0558,-0.0562,-0.0342,0.0738,0.0114,-0.0237,0.1603,-0.0364,0.0376,0.0027,-0.0997,-0.0029,-0.0539,0.0122,-0.0177,-0.0364,0.0034,-0.0087,-0.006,0.0194,-0.0352,0.0172,-0.0174,0.0989,0.0495,-0.0057,-0.0114,-0.0409,0.0288,0.0034,-0.0074,0.0019,-0.0601,0.0609,0.0696,0.0476,-0.0235,0.0733,0.0442,-0.043,0.0159,-0.0113,-0.0041,-0.0223,0.0227,0.0032,-0.0182,-0.011,-0.0724,-0.0055,-0.0751,-0.0792,0.1529,-0.0349,-0.0233,-0.076,-0.0305,-0.0152,0.0324,0.0306,-0.0343,0.0715,0.0413,0.0547,0.0229,-0.0443,0.0291,0.0306,-0.0427,-0.0364,0.1076,0.0533,-0.1357,-0.053,-0.035,0.005,-0.0517,0.0536,0.0378,-0.0552,0.047,0.0607,0.0538,-0.0213,-0.0173,-0.011,-0.0024,0.0341,-0.0246,-0.0391,0.0506,0.0507,-0.0556,-0.0009,-0.0494,0.0456,0.0325,0.0012,0.058,0.0045,0.0008,-0.024,-0.0068,-0.0183,-0.0531,0.0167,-0.0594,0.0195,0.0238,-0.0102,0.0201,-0.0407,-0.0204,-0.0179,-0.0065,0.0111,0.0209,-0.0667,-0.0048,0.0647,-0.0095,-0.0274,0.0251,0.0284,0.0658,0.0086,0.0428,0.0104,-0.067,-0.0451,-0.2335,-0.0139,0.0133,-0.0243,0.0604,-0.0744,-0.0148,-0.0092,0.088,0.0914,0.029,-0.0304,-0.0298,0.0474,-0.008,0.0688,0.0233,0.0104,-0.0095,0.011,0.0232,-0.0023,0.0247,-0.0624,0.0489,-0.0212,0.2134,0.0567,0.0176,-0.0306,0.0504,0.0512,-0.0558,-0.1416,0.081,-0.0044,0.0512,-0.0182,-0.0055,-0.0125,0.0005,0.0057,-0.0136,-0.0822,-0.0781,-0.0439,-0.0417,0.0286,-0.0673,0.0417,0.0769,0.0155,0.0543,0.024,-0.0385,-0.0168,-0.0625,0.036,-0.0264,-0.0264,0.004,-0.0313,0.0376,-0.0762,0.0466,0.0313,-0.0341,0.0177,0.0024,-0.0383,-0.0551,0.0999,0.0283,-0.0008,0.0541,0.0419,0.0859,-0.0293,-0.0717,-0.029,0.0703,-0.0486,0.0334,-0.0003,0.0014,-0.0021,0.0989,0.0178,0.0657,0.0083,0.0516,0.0051,-0.0338,-0.0116,0.0472,-0.019,-0.3046,0.0339,0.0493,0.028,-0.0173,0.0162,-0.0072,0.0352,-0.0215,-0.0012,-0.049,0.0375,-0.0142,-0.0647,-0.0251,0.0196,0.1034,-0.0193,0.0129,-0.0165,0.001,0.0392,0.2219,-0.0319,0.0356,0.0225,-0.0149,-0.0085,0.0165,-0.0061,0.0215,-0.016,0.1043,0.0013,0.0403,0.0207,0.0062,0.0294,0.0484,0.0029,0.0303,0.0232,-0.0443,-0.0656,0.037,-0.0477,0.0459,-0.0596,0.0217,0.0294,-0.0342,-0.0496,-0.0095,-0.0201,0.0219,0.0409,-0.0108,-0.0313,-0.0212,-0.0042,-0.0202,-0.0582,-0.0445,-0.0092,-0.0098]}
{"key":"[Gaussian Processes for Nonlinear Signal Processing] Gaussian processes (GPs) are versatile tools that have been successfully employed to solve nonlinear estimation problems in machine learning, but that are rarely used in signal processing. In this tutorial, we present GPs for regression as a natural nonlinear extension to optimal Wiener filtering. After establishing their basic formulation, we discuss several important aspects and extensions, including recursive and adaptive algorithms for dealing with non-stationarity, low-complexity solutions, non-Gaussian noise models and classification scenarios. Furthermore, we provide a selection of relevant applications to wireless digital communications.","layer":0,"vector":[-0.0415,-0.0475,0.0397,-0.0544,0.0317,-0.0326,0.0207,0.0331,0.0463,-0.0403,0.0595,-0.0597,-0.0036,0.0379,0.002,-0.0058,0.0264,0.0456,-0.0724,0.0205,0.044,-0.0129,-0.0233,-0.0003,0.0316,-0.0066,-0.0185,-0.0376,-0.0381,-0.2232,0.0152,-0.0742,0.0691,-0.0104,-0.0285,-0.0187,-0.022,0.0553,-0.029,0.0683,0.0271,0.0061,-0.0609,-0.0419,0.0098,-0.0655,-0.0227,-0.043,-0.0475,-0.0518,0.0061,0.027,-0.0075,0.0023,0.0248,0.0406,0.054,0.0413,0.0604,0.033,-0.0137,0.0592,-0.1975,0.0719,0.0192,0.0232,-0.0179,-0.0593,0.0037,0.0423,-0.0315,0.0557,-0.0116,0.0534,0.0105,-0.0116,-0.0231,0.0083,-0.0437,0.0561,0.0325,-0.0147,0.0008,-0.0135,-0.0077,-0.076,-0.006,-0.0518,0.0334,0.0123,-0.0457,0.0081,-0.0654,0.0394,-0.0609,0.005,0.0168,0.041,0.002,0.1936,-0.037,0.0401,0.023,-0.0428,0.0505,-0.0443,-0.0482,-0.0425,-0.0254,0.0277,0.0055,-0.054,0.0138,-0.0188,-0.0045,-0.0232,0.0326,0.0431,-0.0216,-0.0219,-0.0143,-0.007,0.0715,-0.0161,0.0657,-0.047,0.0107,0.1662,0.0399,0.0239,0.0389,-0.0293,-0.0674,0.0249,0.0212,0.0077,0.0461,0.018,0.0386,0.019,-0.0413,-0.0516,0.0263,-0.0783,-0.0646,0.1196,-0.0416,-0.0117,-0.0967,-0.0394,-0.025,0.0127,0.0384,-0.0155,0.0272,0.0521,0.0008,0.0233,-0.0858,0.0639,-0.0324,-0.0271,-0.0316,0.1061,0.0314,-0.0841,-0.0361,0.0108,0.0288,0.0016,0.0622,0.0452,-0.0242,-0.03,0.0444,0.0151,-0.0258,0.037,-0.0293,-0.0196,-0.0118,-0.0395,-0.0162,0.0037,0.0778,-0.0261,0.0151,-0.0501,0.0423,0.0327,-0.0421,0.0018,-0.0161,-0.0363,-0.0006,-0.0052,-0.0302,-0.0232,0.0132,-0.0225,0.0288,-0.0231,-0.027,0.0226,0.0134,0.0326,-0.0288,-0.0061,0.0377,0.0193,0.0195,0.0067,0.0993,-0.0362,-0.0453,-0.0246,-0.0221,0.0814,0.0112,0.0289,0.0474,-0.0098,-0.0956,-0.2131,-0.0334,0.0252,0.0191,0.0337,-0.0608,0.0152,-0.0182,0.0859,0.1017,0.0682,-0.0048,-0.0453,0.0082,-0.0028,0.0409,0.0286,0.0171,0.0035,0.0129,-0.001,-0.007,-0.0281,-0.0865,0.0376,-0.0321,0.1518,-0.0029,0.0568,-0.0439,0.0388,-0.0052,-0.0313,-0.0721,0.041,0.0896,0.0963,-0.0033,-0.0155,-0.0487,-0.0364,0.0004,0.0404,-0.0589,-0.0455,-0.0158,-0.0369,0.0059,-0.0625,-0.0147,0.0616,-0.0423,0.0818,-0.03,0.0124,-0.0268,-0.0646,0.034,-0.0135,0.0581,0.0178,-0.0629,0.0388,-0.0505,0.0842,0.0016,-0.0074,-0.0228,-0.0134,0.0214,0.0073,0.0919,0.0174,0.016,0.0605,-0.014,0.0283,-0.0428,-0.0569,-0.0276,0.063,-0.044,0.0422,0.0102,0.0058,-0.0281,0.1102,-0.0403,0.0115,-0.0517,-0.0397,-0.0082,-0.044,-0.0024,0.0209,0.0031,-0.2738,0.0104,0.0259,0.0146,-0.0185,-0.0141,0.0167,0.0214,-0.0863,0.0318,-0.0439,0.04,0.0302,-0.0202,0.0478,0.0402,0.0286,-0.047,0.0134,-0.0705,0.0406,0.0531,0.2129,-0.0217,0.0497,0.0307,-0.0297,0.026,0.0348,-0.0632,0.042,0.0292,0.097,-0.0342,0.0567,0.0699,-0.0266,0.0861,-0.0302,-0.0509,0.0058,-0.0069,-0.001,-0.032,0.1009,-0.0449,-0.0902,-0.0381,-0.012,0.0577,-0.0325,0.0229,0.0126,0.0236,0.0138,-0.0013,-0.0569,-0.0478,-0.0167,-0.0461,0.0456,-0.0876,-0.0105,-0.0053,0.0028]}
{"key":"[Iterative Residual Refinement for Joint Optical Flow and Occlusion Estimation] Deep learning approaches to optical flow estimation have seen rapid progress over the recent years. One common trait of many networks is that they refine an initial flow estimate either through multiple stages or across the levels of a coarse-to-fine representation. While leading to more accurate results, the downside of this is an increased number of parameters. Taking inspiration from both classical energy minimization approaches as well as residual networks, we propose an iterative residual refinement (IRR) scheme based on weight sharing that can be combined with several backbone networks. It reduces the number of parameters, improves the accuracy, or even achieves both. Moreover, we show that integrating occlusion prediction and bi-directional flow estimation into our IRR scheme can further boost the accuracy. Our full network achieves state-of-the-art results for both optical flow and occlusion estimation across several standard datasets.","layer":3,"vector":[-0.0303,-0.0439,0.0313,-0.0355,0.0344,0.0818,0.0261,0.0205,0.0288,-0.0119,0.0444,-0.0958,0.0021,0.0216,0.0095,0.0018,0.0067,0.0634,-0.0219,-0.0089,0.024,-0.075,-0.0236,-0.0595,0.0252,-0.0057,-0.0606,-0.0942,-0.0208,-0.2639,0.005,-0.0237,0.0183,-0.002,-0.0038,-0.0437,-0.0607,0.0666,-0.0542,0.0419,-0.0132,0.0309,-0.0204,-0.0423,-0.0077,-0.0026,-0.0128,0.0051,0.0009,-0.0107,0.0729,-0.0251,0.0422,0.0394,0.0015,0.0211,0.0339,0.0386,0.0594,0.0411,0.0266,0.029,-0.1763,0.045,0.0203,-0.0017,-0.0448,-0.0285,0.0119,0.0282,-0.0265,0.0645,0.0365,-0.0011,-0.0048,-0.0064,0.0003,-0.0034,-0.029,-0.0256,0.0193,-0.0142,-0.0283,-0.002,0.0541,-0.0287,0.0299,-0.0437,0.0424,-0.0187,-0.0839,-0.0394,-0.0561,0.0048,-0.0701,0.0022,0.0274,0.0009,-0.0153,0.2177,-0.0358,0.0165,0.0608,0.025,0.031,-0.0218,-0.0082,-0.0077,-0.0078,0.0478,-0.0172,-0.0397,0.0704,-0.0233,0.0583,0.0131,0.0074,0.0511,0.0047,0.0191,-0.0273,-0.0121,0.0312,-0.0207,0.0198,-0.0605,0.0359,0.1367,0.0601,0.0518,0.0333,-0.0105,0.0017,-0.0301,0.0325,0.0613,0.0236,-0.0178,0.0421,-0.0306,-0.027,-0.0544,0.0027,-0.1028,-0.0611,0.118,-0.0335,0.051,-0.0126,-0.0749,-0.0039,0.0109,-0.006,0.0115,-0.0071,0.0101,0.0376,0.0594,-0.0782,0.0656,-0.0198,-0.0376,-0.0738,0.0474,-0.0074,-0.1094,-0.0205,0.0062,-0.0196,-0.0069,-0.0098,-0.0041,-0.0265,-0.0209,0.0964,0.0005,-0.1102,0.03,-0.0092,0.0013,0.0227,-0.0455,-0.0425,0.0,0.0507,-0.011,-0.0059,-0.044,-0.0242,0.0687,-0.0492,0.0142,-0.0276,0.02,0.0181,0.0164,-0.0532,-0.0175,-0.0165,-0.0357,0.0325,-0.0193,-0.0183,0.0041,-0.0201,-0.0063,0.0098,0.0391,0.0048,0.0667,-0.066,-0.0414,0.0571,-0.0093,-0.0228,-0.0212,0.0046,0.0086,-0.037,0.0375,0.0569,-0.0514,-0.0435,-0.2275,0.0148,0.0143,-0.0317,0.0706,-0.0884,0.0312,0.0318,0.0305,0.0723,0.0463,0.0436,0.0119,0.0189,0.0087,0.0372,0.0554,0.0351,-0.0198,0.0027,-0.0146,0.0344,-0.0158,-0.0716,0.0646,0.0094,0.2329,0.0179,0.0184,0.0089,0.0105,0.0352,-0.0265,-0.1073,0.0214,0.0389,0.0839,0.0058,-0.0141,-0.0216,-0.041,-0.0138,-0.016,-0.1045,-0.0229,-0.0189,-0.0368,0.0136,-0.0335,0.0019,-0.0004,-0.0653,0.0867,-0.0427,-0.0108,-0.0209,-0.0867,0.0597,-0.0493,0.0345,-0.0096,-0.0404,-0.0001,-0.0697,0.0772,0.0175,-0.0022,-0.0726,0.0187,-0.017,-0.019,0.0732,-0.005,0.0346,0.0648,0.0319,0.0481,-0.0246,-0.0377,-0.0225,0.0948,-0.0514,0.068,0.0514,0.0705,0.0357,0.0297,-0.0423,-0.0246,-0.0687,0.0306,-0.0039,-0.0488,-0.017,0.001,0.0214,-0.2846,0.0093,0.0064,0.0385,-0.0259,0.0006,0.0682,0.0266,-0.0263,-0.0434,-0.0265,0.0069,0.0135,-0.0094,0.0315,0.0281,0.0878,-0.0341,0.0681,-0.0237,-0.0147,0.0474,0.1776,-0.039,0.0088,0.0145,-0.0527,0.0194,0.0188,-0.0633,0.0154,0.0396,0.0935,-0.0447,0.0027,0.0659,-0.0304,0.0278,0.0437,-0.007,0.0038,-0.0186,0.0039,-0.0333,0.0679,0.0109,-0.0248,0.053,-0.0179,0.0308,-0.0161,0.024,0.0159,0.0174,0.0239,0.024,-0.078,-0.0257,-0.069,-0.0047,-0.001,-0.0563,-0.0172,0.0095,-0.0171]}
{"key":"[Extreme Bandits using Robust Statistics] We consider a multi-armed bandit problem motivated by situations where only the extreme values, as opposed to expected values in the classical bandit setting, are of interest. We propose distribution free algorithms using robust statistics and characterize the statistical properties. We show that the provided algorithms achieve vanishing extremal regret under weaker conditions than existing algorithms. Performance of the algorithms is demonstrated for the finite-sample setting using numerical experiments. The results show superior performance of the proposed algorithms compared to the well known algorithms.","layer":0,"vector":[-0.0434,0.0129,0.0065,-0.0301,0.0427,-0.0005,0.0403,0.0127,0.0222,0.0019,0.0305,-0.013,0.0345,0.038,-0.0053,0.0487,0.0288,0.0243,-0.0352,0.0094,0.0314,-0.0831,0.0004,-0.0871,0.0436,-0.0051,-0.0279,-0.0882,-0.0318,-0.2044,-0.0016,-0.0455,0.0439,-0.0694,0.0059,-0.0301,-0.0567,0.0244,-0.0068,0.0896,0.0243,0.0348,-0.0392,-0.0844,-0.028,-0.0343,0.0185,-0.0074,0.0131,-0.0397,0.0024,-0.0284,0.0441,0.0256,0.0623,-0.0002,0.0537,0.0419,0.0357,0.0777,-0.0062,0.0659,-0.1316,0.0125,0.0332,0.0354,-0.0689,-0.0166,0.0219,0.0228,0.016,0.0371,0.0248,0.0755,0.047,-0.0166,0.0124,-0.035,-0.0156,0.0077,-0.0066,-0.0493,-0.032,0.0328,-0.0317,-0.0482,0.0295,-0.0515,0.0703,0.0237,-0.0092,0.0109,-0.0171,0.0132,-0.0391,-0.01,0.048,0.0148,-0.0335,0.2128,0.0075,0.0257,0.019,-0.0404,0.0299,-0.0569,-0.0287,-0.0434,-0.0373,-0.0216,-0.0127,0.0118,0.0717,-0.0186,-0.0089,0.0096,0.0295,0.034,-0.02,-0.0199,-0.0693,0.0177,0.0948,-0.0102,0.0349,-0.0295,-0.0007,0.1674,0.0196,0.0186,0.0156,-0.0515,-0.0634,-0.0665,0.0028,0.0097,-0.03,0.0234,0.0408,-0.005,-0.0162,-0.0536,0.0331,-0.1145,-0.0168,0.0844,0.0144,0.0239,-0.0266,-0.059,-0.0094,-0.0268,0.001,-0.0388,0.0164,0.0306,0.0411,0.0747,-0.0398,0.0214,-0.028,-0.0823,0.0394,0.0897,-0.009,-0.0718,-0.0218,-0.0015,-0.0142,0.0483,-0.0114,0.0003,-0.0436,0.0507,0.0859,-0.0358,-0.0773,-0.0218,-0.0035,-0.0025,-0.0229,0.0088,-0.0259,0.0451,0.0302,-0.0182,-0.0009,0.0002,0.0452,0.0597,-0.0488,-0.0128,-0.0086,-0.024,-0.0475,-0.0393,-0.0154,0.0086,0.0014,-0.0213,0.0228,-0.0065,-0.0612,0.0196,0.0288,0.0141,0.0387,-0.0233,0.046,0.0099,-0.0272,-0.0119,0.0604,-0.011,-0.0766,-0.0152,0.0574,0.0791,-0.023,0.0419,0.0559,-0.0338,-0.0339,-0.2412,-0.0001,-0.0265,0.0415,0.0493,-0.0226,0.0655,-0.0514,0.0276,0.0849,0.0824,-0.0699,-0.0332,0.0609,0.0106,0.0761,-0.0061,0.0087,-0.0144,0.0314,-0.0022,0.0495,-0.03,-0.0467,0.0523,0.0186,0.2137,0.0229,0.0232,-0.0311,0.0526,0.0012,0.0137,-0.0432,0.0289,0.0414,0.044,-0.0358,0.0126,-0.0268,-0.0051,0.0155,0.0052,-0.0762,-0.0909,-0.0707,-0.0361,0.0237,-0.0516,0.0516,0.0131,0.0143,0.0519,-0.0286,0.0024,-0.0335,-0.0771,0.0453,-0.0317,0.0642,0.0561,-0.0589,0.0197,-0.0852,0.048,0.0147,0.0135,-0.0376,0.0454,-0.0498,-0.0312,0.0405,0.0067,0.0061,0.0182,0.0068,0.0321,-0.051,-0.0689,-0.0486,0.0151,-0.0005,0.0326,-0.0039,0.0228,-0.0027,0.0795,0.0195,-0.0102,0.0015,0.0237,-0.0336,-0.0322,0.0053,0.0591,0.0112,-0.3285,0.0228,0.0167,-0.0027,-0.0273,0.0035,-0.0031,-0.0034,-0.033,-0.0232,0.0072,0.0823,0.0125,-0.0386,0.0265,-0.0353,0.0522,-0.0242,0.0728,-0.0493,0.0317,0.0314,0.2339,-0.0136,0.0282,0.0435,-0.013,0.0035,-0.0212,-0.0759,0.0025,-0.0263,0.081,-0.0547,0.0306,0.1019,-0.034,0.0267,0.008,-0.0054,-0.0501,0.0097,-0.0248,-0.0116,0.1354,-0.0241,-0.0308,-0.0436,-0.0093,0.0104,-0.0559,-0.0168,-0.0221,0.0065,0.0028,-0.0054,-0.0423,-0.0524,-0.0367,-0.0389,-0.0016,-0.0177,0.0025,-0.0031,-0.0204]}
{"key":"[Addressing the Real-world Class Imbalance Problem in Dermatology] Class imbalance is a common problem in medical diagnosis, causing a standard classifier to be biased towards the common classes and perform poorly on the rare classes. This is especially true for dermatology, a specialty with thousands of skin conditions but many of which have low prevalence in the real world. Motivated by recent advances, we explore few-shot learning methods as well as conventional class imbalance techniques for the skin condition recognition problem and propose an evaluation setup to fairly assess the real-world utility of such approaches. We find the performance of few-show learning methods does not reach that of conventional class imbalance techniques, but combining the two approaches using a novel ensemble improves model performance, especially for rare classes. We conclude that ensembling can be useful to address the class imbalance problem, yet progress can further be accelerated by real-world evaluation setups for benchmarking new methods.","layer":5,"vector":[-0.0485,-0.011,0.0362,0.0027,0.0651,-0.0088,0.0354,0.0337,-0.0254,-0.0102,0.0312,-0.0679,0.018,0.0532,0.0243,-0.0047,0.0247,0.0266,-0.0478,0.0274,-0.0299,-0.009,-0.0473,-0.0213,0.0219,0.0025,0.0056,-0.0732,-0.0738,-0.2508,0.0191,-0.0141,0.025,-0.0142,0.0171,-0.0486,-0.0496,0.0574,-0.0382,0.0635,0.0161,0.0192,-0.0309,-0.0561,-0.0074,-0.0499,-0.0492,-0.0331,-0.0022,-0.0291,0.0406,-0.0656,0.0379,0.0392,0.0055,0.0223,0.0321,0.0527,0.0373,0.0669,0.028,0.0146,-0.1308,0.0844,0.053,0.0155,0.0072,-0.0552,0.0164,0.0191,-0.0016,0.0446,0.043,0.0534,-0.0285,0.004,0.0408,0.0161,-0.0352,0.0088,0.0065,-0.0137,-0.0102,-0.0088,0.0015,-0.0292,0.0099,-0.0637,0.038,0.0225,-0.0751,-0.0148,-0.0008,0.0392,-0.0271,-0.0026,0.0304,0.0337,-0.0581,0.2167,-0.0399,-0.0076,0.0175,-0.0327,0.0277,-0.0381,-0.0239,-0.0388,-0.0774,-0.0634,0.0179,0.0124,0.0398,-0.0231,-0.0131,-0.0266,0.085,0.0497,-0.0097,-0.0059,0.0266,-0.0286,0.034,-0.0508,-0.0044,-0.0402,0.1033,0.1508,0.0399,0.0156,0.0194,-0.0283,-0.0392,-0.0348,-0.027,0.0121,0.0017,0.031,0.0259,0.0073,-0.0349,-0.0489,0.03,-0.0831,-0.0543,0.1333,-0.0402,0.0718,-0.045,-0.0339,0.0026,0.0188,-0.0691,0.0079,0.0229,0.0164,0.0459,0.0182,0.0051,-0.0058,0.0111,-0.0636,-0.0094,0.1057,0.0107,-0.0493,-0.0205,-0.0071,-0.0093,-0.0332,0.0615,0.0523,-0.0197,0.0782,0.0627,0.0148,-0.066,-0.0079,-0.0055,0.0004,0.0554,-0.0151,-0.0689,0.0409,0.0391,0.0057,0.0206,-0.0371,0.0535,0.0322,-0.0595,-0.0142,-0.0433,-0.0061,-0.0427,-0.026,-0.0042,-0.0417,0.0136,-0.0342,0.0014,0.0414,0.0018,0.0304,-0.0049,0.0452,-0.0297,-0.0024,0.0786,0.0096,-0.0444,0.0293,0.0596,-0.0287,-0.0463,-0.0004,0.0264,0.0586,0.0189,0.0442,0.0438,-0.0041,-0.0784,-0.2183,0.028,0.0294,-0.0367,0.0566,-0.059,0.0242,0.0009,0.0744,0.0763,0.076,-0.0027,-0.002,0.0112,-0.061,0.0544,0.0354,-0.0002,-0.0137,-0.0189,-0.0163,0.029,0.0386,-0.0861,0.0868,-0.0098,0.1947,0.0212,0.0066,-0.0544,0.0197,0.0143,-0.0577,-0.0536,0.0812,0.0021,0.042,-0.0505,-0.0539,-0.0077,-0.0138,-0.0032,0.0064,-0.1079,-0.0449,-0.009,-0.0385,0.0254,-0.0638,0.0056,0.0654,-0.0356,0.0151,-0.005,-0.006,-0.0599,-0.1085,0.0556,-0.0194,0.036,0.0363,-0.0376,0.0289,-0.0816,0.0417,0.0045,-0.069,-0.0826,0.0478,-0.0413,-0.0377,0.0869,0.0149,-0.0474,0.0119,0.0324,0.1116,-0.0286,-0.0415,-0.0683,0.0604,-0.0326,0.0158,0.0344,0.0291,0.0333,0.073,-0.0142,0.0148,-0.0272,0.0135,0.0235,-0.0309,0.042,0.0169,-0.0338,-0.2703,0.022,-0.0139,0.062,-0.0456,0.041,0.0249,-0.0223,-0.0381,-0.0358,0.0386,-0.0141,0.03,-0.0125,-0.0217,0.0161,0.047,-0.1072,0.041,-0.0346,0.0355,0.0316,0.2231,-0.0556,0.0507,0.0392,-0.015,0.0021,0.0196,-0.0331,0.0105,-0.0149,0.0707,-0.0301,0.0343,0.0628,-0.0246,-0.0148,0.0143,-0.0111,0.0325,-0.0266,-0.0145,-0.0073,0.0858,-0.0288,-0.0265,-0.0557,0.0156,0.042,-0.0085,0.0166,-0.0083,-0.025,0.0309,0.0084,-0.064,-0.026,-0.0584,-0.0365,0.0344,-0.076,-0.0568,0.0031,0.0053]}
{"key":"[Efficient estimation of the ANOVA mean dimension, with an application to neural net classification] The mean dimension of a black box function of $d$ variables is a convenient way to summarize the extent to which it is dominated by high or low order interactions. It is expressed in terms of $2^d-1$ variance components but it can be written as the sum of $d$ Sobol' indices that can be estimated by leave one out methods. We compare the variance of these leave one out methods: a Gibbs sampler called winding stairs, a radial sampler that changes each variable one at a time from a baseline, and a naive sampler that never reuses function evaluations and so costs about double the other methods. For an additive function the radial and winding stairs are most efficient. For a multiplicative function the naive method can easily be most efficient if the factors have high kurtosis. As an illustration we consider the mean dimension of a neural network classifier of digits from the MNIST data set. The classifier is a function of $784$ pixels. For that problem, winding stairs is the best algorithm. We find that inputs to the final softmax layer have mean dimensions ranging from $1.35$ to $2.0$.","layer":12,"vector":[-0.0541,-0.0441,0.0346,-0.0229,0.0124,0.0229,0.0492,0.0262,0.0273,-0.0465,0.036,-0.0151,0.0394,0.0482,-0.0202,0.0123,0.0251,0.0211,-0.0874,0.0184,0.0497,-0.0005,-0.0142,-0.0524,0.0271,0.0141,-0.0409,-0.0682,-0.0197,-0.2587,0.0396,-0.0218,0.0621,-0.0537,-0.0168,-0.0058,-0.0441,0.0473,0.0135,0.01,0.006,0.0301,-0.0109,-0.0512,-0.0215,-0.0325,-0.0313,-0.0038,-0.0401,-0.023,0.0005,-0.0414,0.0032,-0.0003,0.0315,0.0358,0.0604,0.0014,0.063,0.0295,0.0156,0.0246,-0.1649,0.0416,0.0349,0.0334,-0.0616,-0.0449,0.0071,0.0506,-0.0077,-0.0141,0.0113,0.0375,0.019,-0.0313,-0.0059,-0.0239,-0.0144,0.0236,0.0057,-0.0166,-0.0471,-0.0318,-0.0075,-0.0515,0.0474,-0.0185,0.0489,0.0016,-0.028,0.0265,-0.0447,0.0087,-0.0447,-0.0195,0.0462,0.0031,-0.0132,0.169,-0.054,-0.0018,0.011,-0.0396,0.0214,-0.0477,-0.0447,-0.0096,-0.0684,0.0146,-0.0232,0.0084,-0.0147,-0.0456,0.0467,0.0009,0.0478,0.0157,-0.0334,-0.0154,-0.0189,0.0049,0.0464,-0.0221,0.0232,-0.0359,-0.0132,0.187,0.0324,0.0404,0.0387,-0.006,-0.0407,0.0032,-0.005,0.0216,0.0075,0.0581,0.0005,-0.016,-0.0691,-0.0611,0.0335,-0.0433,-0.0439,0.1229,-0.0697,0.005,-0.0163,-0.0208,0.016,0.0351,-0.014,-0.0404,0.0462,0.0163,0.0122,0.0367,-0.0542,0.0099,-0.0543,-0.0564,-0.0128,0.104,0.0469,-0.0719,-0.0357,0.014,0.0169,-0.0155,0.0368,0.0342,-0.0581,0.0146,0.0689,0.0038,-0.0675,-0.0077,0.029,0.0107,0.0269,-0.025,-0.031,0.0307,0.0385,0.0175,0.0133,-0.0019,0.0423,0.0561,-0.0512,0.0167,-0.026,0.0073,-0.0305,-0.0336,-0.0369,0.0268,0.0245,-0.0403,0.0348,0.0028,-0.0218,0.0185,0.0123,0.0619,-0.0191,-0.0039,0.058,0.0177,-0.036,-0.0062,0.0712,-0.0311,-0.0304,0.0364,0.0389,0.0294,-0.0058,0.0708,0.0676,-0.0792,-0.0811,-0.2329,-0.0202,0.0271,-0.0286,0.0546,-0.0811,0.0638,-0.0117,0.0301,0.0841,0.0407,0.0003,-0.0075,0.0342,0.0005,0.1057,0.0307,0.0071,-0.037,0.0083,-0.0449,0.0067,0.0182,-0.0534,0.028,0.0189,0.1922,0.001,0.025,-0.045,0.0201,0.0642,0.0161,-0.0653,0.0664,0.0139,0.0468,0.0116,-0.0636,-0.0706,-0.0704,-0.0137,0.0366,-0.0973,-0.0187,-0.0218,-0.0069,-0.0337,-0.0926,0.0094,-0.0049,-0.0074,0.0681,-0.0355,0.0358,-0.0459,-0.1168,0.0229,-0.0327,0.0235,0.0226,-0.0403,0.0495,-0.0956,-0.0216,-0.0155,0.016,0.0049,0.0344,-0.0482,-0.0594,0.0635,0.0079,-0.0311,0.0823,0.0002,0.0167,-0.0121,-0.0506,0.0088,0.0471,-0.037,0.0042,-0.0055,0.0344,0.0074,0.072,0.0216,0.0305,-0.0129,0.0176,0.0194,-0.0219,-0.0422,0.0183,0.0539,-0.2881,0.0435,-0.001,0.032,-0.0159,0.0038,0.0556,0.019,0.0034,-0.0032,0.0303,0.0336,0.0582,-0.0194,0.0242,0.049,0.0649,-0.0599,0.0781,-0.0858,0.0221,0.0139,0.2434,-0.0541,0.0211,0.0083,-0.01,-0.0307,0.0003,-0.0736,0.0236,0.0016,0.0813,-0.0692,0.0515,0.093,-0.0195,0.01,0.0085,-0.0392,0.0135,-0.004,-0.0639,-0.0033,0.1443,-0.0376,-0.0178,-0.0391,0.0089,0.0224,-0.0179,0.033,0.0315,0.0462,-0.003,0.0462,-0.0497,-0.0142,-0.0396,-0.0392,0.0044,-0.1042,-0.0012,-0.011,0.0043]}
{"key":"[Development of a hybrid machine-learning and optimization tool for performance-based solar shading design] Solar shading design should be done for the desired Indoor Environmental Quality (IEQ) in the early design stages. This field can be very challenging and time-consuming also requires experts, sophisticated software, and a large amount of money. The primary purpose of this research is to design a simple tool to study various models of solar shadings and make decisions easier and faster in the early stages. Database generation methods, artificial intelligence, and optimization have been used to achieve this goal. This tool includes two main parts of 1. predicting the performance of the user-selected model along with proposing effective parameters and 2. proposing optimal pre-prepared models to the user. In this regard, initially, a side-lit shoebox model with variable parameters was modeled parametrically, and five common solar shading models with their variables were applied to the space. For each solar shadings and the state without shading, metrics related to daylight and glare, view, and initial costs were simulated. The database generated in this research includes 87912 alternatives and six calculated metrics introduced to optimized machine learning models, including neural network, random Forrest, support vector regression, and k nearest neighbor. According to the results, the most accurate and fastest estimation model was Random Forrest, with an r2_score of 0.967 to 1. Then, sensitivity analysis was performed to identify the most influential parameters for each shading model and the state without it. This analysis distinguished the most effective parameters, including window orientation, WWR, room width, length, and shading depth. Finally, by optimizing the estimation function of machine learning models with the NSGA II algorithm, about 7300 optimal models were identified. The developed tool can evaluate various design alternatives in less than a few seconds for each.","layer":0,"vector":[-0.044,0.0096,0.0727,0.0295,0.0592,0.0356,0.0116,0.0335,0.0126,0.0218,0.0078,-0.0352,0.0237,0.0573,0.0212,0.004,0.0306,0.0338,-0.0206,0.0088,0.0447,-0.0037,-0.0349,-0.0792,0.015,0.0244,-0.0119,-0.0268,-0.0465,-0.2082,0.0088,-0.0644,0.068,-0.0511,-0.018,-0.0658,-0.0334,0.0434,-0.0421,0.0527,0.0146,-0.0311,-0.0106,-0.027,-0.0059,-0.023,0.0187,-0.0325,-0.0229,-0.0163,0.0195,-0.0532,-0.0217,0.0484,0.0633,0.0499,0.0361,0.0616,0.0441,0.0585,0.0221,0.0403,-0.2391,0.0404,0.0341,0.0354,-0.0317,-0.0468,0.0002,0.0541,-0.0365,0.0403,-0.0041,-0.0009,-0.0032,0.0324,-0.016,-0.0582,0.0309,-0.0183,-0.0117,-0.0339,-0.0656,0.0119,-0.0154,0.0164,-0.041,-0.0266,0.0557,0.037,-0.0011,0.0137,-0.0581,-0.002,-0.0611,-0.0184,0.03,-0.0148,-0.0498,0.1938,-0.0524,0.0253,0.0374,-0.0472,0.0118,-0.0601,-0.0128,-0.0515,-0.0397,-0.0313,-0.0604,-0.004,0.0069,-0.0033,0.0276,-0.0035,0.038,0.0529,0.0245,-0.0216,-0.0378,0.0081,0.082,0.0165,0.041,-0.0731,0.0293,0.1001,0.005,0.0566,0.0504,-0.0359,0.0025,-0.0055,0.0122,0.0623,0.0305,0.0052,0.0238,-0.0238,-0.0004,-0.0274,0.0455,-0.0996,-0.0354,0.116,-0.0584,0.0227,-0.052,-0.03,-0.0171,0.0719,-0.0375,-0.0232,0.0339,0.0296,-0.0288,0.0747,-0.0078,0.0056,0.0267,-0.0215,-0.0415,0.0611,-0.0471,-0.1199,-0.0406,0.0297,-0.0189,0.0333,0.0865,0.0756,-0.0653,0.0761,0.0931,0.0146,-0.0825,0.0037,-0.0234,0.0356,0.0404,-0.0039,-0.044,0.0167,0.1027,-0.0693,0.0095,-0.0427,0.0026,0.0538,-0.0616,-0.0311,-0.0405,0.0429,-0.0177,-0.0247,-0.0104,-0.0492,0.0422,-0.0299,0.0264,0.0125,-0.0495,0.0079,0.0009,0.0124,0.0342,-0.0001,0.0355,-0.0028,-0.0453,-0.0262,0.0443,-0.018,-0.0308,0.0367,0.0044,0.0281,0.0491,0.0568,0.0239,-0.0546,-0.1209,-0.2103,0.0524,0.016,0.0238,0.0701,-0.0536,0.0392,-0.0184,0.0112,0.041,0.0886,-0.0131,-0.0021,0.003,-0.0347,0.0156,0.0299,0.0529,-0.0589,-0.053,-0.0051,0.0472,0.0112,-0.126,0.0418,-0.0239,0.156,-0.018,0.0178,-0.0319,0.0223,-0.0059,-0.0334,-0.1222,0.0515,0.0845,0.0919,0.0111,-0.0674,-0.0395,0.0132,0.0314,-0.0209,-0.0873,-0.023,-0.0477,-0.0266,0.0377,-0.0371,-0.0158,0.0216,-0.0359,0.0552,-0.0324,-0.0058,-0.018,-0.1011,0.0699,-0.011,0.0124,-0.0056,-0.0935,0.0485,-0.0403,0.0382,-0.0141,-0.0394,-0.034,0.0088,-0.0216,-0.0419,0.1227,-0.0075,-0.0227,0.0615,0.0114,0.0296,-0.0014,-0.0009,-0.0362,0.0549,-0.0586,0.0656,0.0266,0.0245,-0.0111,0.0334,-0.0227,0.02,-0.0516,-0.0106,-0.016,-0.062,0.0061,0.0502,-0.0031,-0.2716,0.017,-0.0226,0.0212,-0.0363,-0.0422,0.0321,0.0158,0.0042,-0.0153,-0.0058,-0.0059,-0.0099,-0.0121,0.0436,-0.0285,0.0756,-0.0025,0.043,-0.0307,0.0524,0.0274,0.2084,-0.0699,0.0368,0.0509,-0.0108,0.0183,0.0306,-0.0028,0.018,0.0354,0.1102,-0.0286,0.0349,0.0495,-0.0008,0.0063,0.0025,-0.0107,0.0156,0.0312,-0.0298,0.0006,0.098,-0.0099,-0.0044,-0.0373,-0.0073,-0.0293,-0.0133,0.0184,-0.0456,-0.033,-0.0025,-0.0195,-0.0647,-0.0485,-0.0387,0.0227,0.0426,-0.0292,-0.0074,-0.0355,0.0433]}
{"key":"[Belittling the Source: Trustworthiness Indicators to Obfuscate Fake News on the Web] With the growth of the internet, the number of fake-news online has been proliferating every year. The consequences of such phenomena are manifold, ranging from lousy decision-making process to bullying and violence episodes. Therefore, fact-checking algorithms became a valuable asset. To this aim, an important step to detect fake-news is to have access to a credibility score for a given information source. However, most of the widely used Web indicators have either been shut-down to the public (e.g., Google PageRank) or are not free for use (Alexa Rank). Further existing databases are short-manually curated lists of online sources, which do not scale. Finally, most of the research on the topic is theoretical-based or explore confidential data in a restricted simulation environment. In this paper we explore current research, highlight the challenges and propose solutions to tackle the problem of classifying websites into a credibility scale. The proposed model automatically extracts source reputation cues and computes a credibility factor, providing valuable insights which can help in belittling dubious and confirming trustful unknown websites. Experimental results outperform state of the art in the 2-classes and 5-classes setting.","layer":3,"vector":[-0.0366,-0.0383,-0.0055,-0.006,0.0504,0.0175,0.0487,0.0301,0.0198,-0.0156,0.0234,-0.0255,0.0252,0.0417,0.0656,0.008,0.0013,0.0249,-0.0095,-0.0055,0.0611,-0.0213,0.0058,-0.0291,0.0436,0.0315,-0.02,-0.0456,-0.0662,-0.198,-0.0061,-0.1163,0.0525,-0.0235,0.0071,-0.0283,0.0024,0.0157,-0.0181,0.024,0.0059,0.0202,0.0119,-0.016,-0.0259,-0.0434,-0.0021,0.0804,-0.048,-0.0267,0.0385,-0.014,0.008,0.0494,0.0394,-0.0142,0.0408,0.0334,0.0498,0.0322,0.0306,0.0735,-0.1326,0.0596,0.0102,0.0339,-0.0466,-0.0351,-0.0257,0.0345,-0.0271,0.0279,0.0024,0.0687,0.0246,-0.0173,-0.002,-0.0013,-0.0249,0.0338,-0.016,-0.0453,-0.0086,0.0036,-0.0165,-0.0662,0.0613,-0.0183,0.0284,-0.0398,-0.0214,-0.0164,0.0084,0.0002,-0.0523,-0.034,-0.0356,0.024,-0.0385,0.1874,-0.0936,0.0433,0.0458,-0.0437,0.0572,-0.0565,0.0053,-0.0336,0.0108,0.0089,-0.0233,-0.0036,0.0465,-0.0155,0.0231,0.0473,0.0738,0.011,-0.0299,-0.0183,-0.0232,0.005,0.0704,-0.0202,0.0274,-0.0366,0.0244,0.1356,0.0144,0.0391,-0.024,-0.036,-0.0583,-0.0107,0.0462,0.034,-0.0473,0.0402,0.0553,0.0057,-0.0534,-0.0733,-0.0092,-0.0514,-0.0978,0.113,-0.0414,0.0272,-0.023,0.0149,-0.0252,0.0423,-0.033,-0.0311,0.0713,-0.0213,0.0066,0.0542,-0.0143,0.0049,0.0073,-0.028,-0.0375,0.0854,-0.012,-0.0844,0.0126,0.0516,0.025,-0.0217,0.0015,0.0121,-0.0272,0.0252,0.037,-0.0271,-0.0316,0.0045,0.0549,0.0231,0.0671,-0.0455,-0.0786,0.0993,0.0012,-0.0178,-0.0025,-0.0585,0.0806,0.05,-0.068,-0.0053,-0.0345,0.0069,-0.0058,-0.0354,-0.0175,0.0058,0.002,-0.0391,0.0225,-0.0408,-0.0806,0.0049,-0.025,0.0201,0.0006,-0.0314,0.0365,0.0218,-0.0546,0.0112,-0.0098,-0.084,-0.0376,-0.0237,0.0466,0.0475,0.0136,0.0367,0.0199,-0.0571,-0.0465,-0.2479,-0.0326,-0.0145,0.0276,0.0708,-0.0548,0.0727,-0.0341,0.0646,0.108,0.0728,-0.0155,-0.0123,0.0408,0.0152,0.0734,0.0245,-0.0089,-0.0147,0.0102,-0.0242,0.0268,-0.0498,-0.0873,0.042,0.0085,0.1981,0.0669,-0.0404,-0.0429,0.0483,-0.0027,-0.0271,-0.1311,0.0973,0.0217,0.059,-0.0474,-0.01,-0.0042,-0.008,0.0234,-0.0335,-0.1008,-0.0229,-0.0494,-0.0053,0.0406,-0.0437,0.0654,0.0523,0.0053,0.0665,0.0156,-0.0304,-0.0551,-0.0524,0.0185,-0.0299,0.0254,-0.017,-0.0517,-0.0099,-0.0756,0.0727,-0.0163,-0.0614,-0.0223,0.05,-0.0182,-0.0047,0.1384,0.0356,-0.0039,0.027,-0.011,0.0286,-0.0698,-0.0013,-0.0129,0.0648,-0.0143,0.021,0.0507,0.0015,-0.0017,0.0391,0.0017,0.0343,-0.032,0.0161,0.0096,-0.0684,-0.039,0.055,-0.0107,-0.3246,0.0224,-0.0405,0.0471,-0.0124,-0.032,0.0666,0.0213,-0.0512,-0.0187,0.0184,0.0306,-0.0132,-0.0226,0.0055,0.0615,0.0077,-0.0581,0.0232,0.0129,0.033,0.0054,0.2296,-0.0165,0.0092,0.028,-0.0199,-0.0061,0.0069,-0.0314,-0.0007,0.0056,0.0828,-0.0333,0.0119,0.0245,-0.039,0.0301,0.0061,-0.0642,-0.0288,-0.013,-0.0355,0.0312,0.0876,-0.0255,-0.0086,-0.0484,0.0166,0.0376,-0.062,-0.0188,0.0005,0.0148,-0.0126,0.0286,-0.0296,-0.0128,0.0122,-0.0544,-0.0484,-0.0397,0.0128,0.0439,0.0324]}
{"key":"[Deep Active Learning with a Neural Architecture Search] We consider active learning of deep neural networks. Most active learning works in this context have focused on studying effective querying mechanisms and assumed that an appropriate network architecture is a priori known for the problem at hand. We challenge this assumption and propose a novel active strategy whereby the learning algorithm searches for effective architectures on the fly, while actively learning. We apply our strategy using three known querying techniques (softmax response, MC-dropout, and coresets) and show that the proposed approach overwhelmingly outperforms active learning using fixed architectures.","layer":0,"vector":[-0.0502,0.0243,0.0256,0.0208,0.0138,0.0413,0.0512,0.0335,0.0323,-0.0228,0.0081,-0.0133,0.0423,0.0102,0.0189,0.0065,0.0202,0.0429,-0.0065,0.0157,0.0084,-0.0072,-0.0146,-0.0427,0.005,-0.0246,-0.0218,-0.0284,-0.0641,-0.235,-0.0069,-0.05,0.0952,-0.0573,0.0078,-0.0054,-0.0194,0.0554,0.0024,0.0407,0.049,0.0077,0.0083,-0.038,0.0141,0.0075,0.0014,-0.0286,-0.0052,-0.0249,-0.0091,-0.0264,0.0262,0.0172,0.0468,0.0385,0.0481,0.0409,-0.0072,0.0211,0.0238,0.0615,-0.1325,0.0625,0.0205,0.0129,-0.0327,-0.0358,-0.0029,0.0608,0.0169,0.0402,-0.0049,0.0413,0.0072,0.0393,-0.0084,-0.0154,0.0245,-0.0388,-0.0012,-0.0248,-0.0546,-0.0358,-0.0176,-0.0637,-0.0109,-0.0131,0.0488,-0.0125,-0.043,0.0198,-0.0187,0.015,-0.0349,0.0156,0.04,0.039,-0.0441,0.2301,-0.0466,0.0324,0.0053,-0.0088,-0.0012,-0.0384,-0.0327,0.0234,-0.0302,-0.03,-0.0518,-0.0017,0.0213,-0.0026,0.0408,0.0064,0.053,0.0338,-0.0287,-0.0254,-0.0232,-0.0218,0.0455,0.0107,0.0254,-0.0718,0.0217,0.1346,0.0021,0.022,0.0181,-0.0552,-0.056,-0.03,0.0343,0.0492,0.0296,-0.0232,-0.0055,-0.0108,-0.084,-0.0366,0.0716,-0.0986,-0.063,0.1141,-0.036,0.0026,-0.0541,-0.0588,-0.0292,0.0419,-0.0121,-0.0244,0.0044,0.0556,0.0372,0.077,-0.0671,-0.0378,-0.035,-0.0732,-0.0365,0.1501,-0.0136,-0.0871,-0.0084,-0.0094,-0.0114,-0.0243,0.0698,0.012,-0.0268,0.0376,0.0659,0.0301,-0.0605,0.0028,-0.0358,0.0253,-0.0145,-0.0562,0.0193,0.0323,0.0278,-0.0567,0.0257,-0.0696,0.027,0.023,-0.0428,0.0458,-0.0407,0.004,-0.0348,-0.0315,0.0182,-0.0196,0.0353,0.0229,0.0195,-0.0309,-0.0199,-0.0075,-0.0166,0.0304,-0.0601,0.0377,0.0507,0.0222,-0.0116,0.0226,0.0426,-0.0702,-0.0492,-0.0626,-0.0042,0.0277,-0.0078,0.0464,0.0611,-0.0505,-0.0381,-0.2365,-0.0196,-0.0427,-0.0456,0.0593,-0.0892,0.0508,-0.0009,0.0113,0.0807,0.0232,-0.0094,-0.0043,0.0291,0.0037,0.0913,0.0509,0.036,-0.0506,-0.0233,0.0299,0.0319,0.0265,-0.0869,0.0582,0.0301,0.2241,0.0404,0.0613,-0.044,0.0274,0.0142,-0.0154,-0.0954,0.0613,0.0053,0.1116,0.0098,-0.0198,-0.0471,-0.0701,0.0512,-0.0344,-0.1236,-0.0287,-0.0082,-0.0047,0.0556,-0.0568,0.0157,-0.0298,-0.0261,0.041,0.0092,-0.0117,-0.0429,-0.1055,0.0035,-0.0383,0.0648,0.018,-0.0283,-0.0544,-0.0642,0.0258,0.0125,-0.0386,-0.0502,0.0445,-0.0276,-0.0158,0.0397,-0.0027,0.0244,0.0618,0.0141,0.0348,-0.0459,0.0043,0.0033,0.0495,-0.051,0.0276,-0.0241,0.0193,0.044,0.0829,-0.0593,0.0318,-0.0129,-0.0016,0.0377,-0.0511,-0.0094,0.0371,-0.0211,-0.2514,0.0338,0.0353,0.028,-0.0633,0.028,0.0687,0.0141,-0.0482,0.0391,-0.0074,0.0422,0.0248,0.0028,-0.0473,0.0305,0.0779,-0.056,0.042,-0.0632,0.0106,0.0475,0.2257,-0.0743,0.0276,-0.0085,-0.0277,-0.0394,0.002,-0.0489,0.0283,0.0115,0.1145,-0.0206,0.0142,0.0819,0.0036,0.0294,0.0285,0.0107,-0.0193,-0.0042,-0.0616,-0.0132,0.0985,-0.0126,-0.0052,-0.0358,-0.0243,0.0249,0.0016,-0.0292,0.0316,0.0006,0.0069,0.0018,-0.0568,-0.0251,-0.0516,-0.0046,0.0185,-0.079,0.007,-0.0015,0.0213]}
{"key":"[Neural Modular Control for Embodied Question Answering] We present a modular approach for learning policies for navigation over long planning horizons from language input. Our hierarchical policy operates at multiple timescales, where the higher-level master policy proposes subgoals to be executed by specialized sub-policies. Our choice of subgoals is compositional and semantic, i.e. they can be sequentially combined in arbitrary orderings, and assume human-interpretable descriptions (e.g. 'exit room', 'find kitchen', 'find refrigerator', etc.). We use imitation learning to warm-start policies at each level of the hierarchy, dramatically increasing sample efficiency, followed by reinforcement learning. Independent reinforcement learning at each level of hierarchy enables sub-policies to adapt to consequences of their actions and recover from errors. Subsequent joint hierarchical training enables the master policy to adapt to the sub-policies. On the challenging EQA (Das et al., 2018) benchmark in House3D (Wu et al., 2018), requiring navigating diverse realistic indoor environments, our approach outperforms prior work by a significant margin, both in terms of navigation and question answering.","layer":4,"vector":[-0.0425,-0.0167,0.0325,-0.0104,-0.0223,0.007,0.025,-0.0039,0.0183,0.0139,0.0222,-0.0697,0.0567,0.0823,0.0535,-0.0196,-0.0276,0.0474,0.0055,0.0268,0.0389,-0.0506,-0.0419,-0.0654,-0.0179,0.0205,-0.0541,-0.0643,-0.0271,-0.2275,0.0334,-0.043,-0.0049,-0.0308,0.0032,0.0036,-0.0184,0.062,0.0009,0.0326,0.0265,0.0126,0.0072,-0.0786,0.0075,-0.0308,0.0072,0.0015,-0.0303,-0.052,-0.0055,-0.0474,0.0091,0.0052,0.0701,0.0566,0.0735,0.034,0.0531,0.0399,-0.0026,-0.003,-0.1423,0.1054,0.0286,0.0685,-0.0421,0.008,0.0077,0.0518,-0.0242,0.0321,0.0435,0.0465,0.0328,-0.0305,0.0005,-0.0468,0.0239,-0.0374,0.0035,-0.0234,-0.0495,-0.0155,-0.0174,-0.0474,-0.0292,-0.024,0.0124,0.0011,-0.0575,-0.0394,-0.0514,0.0314,-0.0297,-0.0422,0.0128,0.0551,-0.0682,0.2382,-0.0155,0.0291,0.0377,-0.0282,0.0029,-0.0423,-0.0241,0.0052,-0.0266,0.0065,-0.0806,-0.0231,0.0156,-0.004,0.0172,0.0207,0.1061,0.0049,-0.0223,-0.05,0.0213,0.0251,0.0684,-0.0364,0.0384,-0.0817,0.053,0.138,-0.0089,0.037,0.0712,-0.0321,-0.0186,-0.0244,0.0074,0.0302,0.0444,-0.0132,0.0479,-0.0131,-0.0526,0.0009,0.0367,-0.0828,-0.0125,0.1037,-0.0223,-0.0146,-0.0501,0.0054,-0.0451,0.019,0.0094,-0.0712,-0.0007,0.0476,0.0203,0.0033,-0.0521,-0.0344,-0.0056,-0.0558,-0.0241,0.0836,-0.0105,-0.063,-0.0611,-0.0005,-0.0217,-0.0421,0.0486,0.0237,-0.037,0.0449,0.0851,0.0378,-0.093,0.0431,0.0133,0.0102,0.0225,-0.1093,-0.0138,0.0289,0.0509,-0.0209,0.0313,-0.0298,0.0416,0.0423,-0.0041,0.0567,-0.0165,-0.0062,-0.0181,-0.0113,0.0343,0.0149,0.0266,-0.0349,-0.0014,-0.0306,-0.0655,0.0238,-0.0125,0.0241,0.0011,0.0185,0.084,0.0047,-0.0055,0.0347,0.0545,-0.0118,0.0096,-0.034,-0.0182,0.0029,-0.0134,0.0355,0.0104,-0.0364,-0.0009,-0.1997,0.0218,0.0099,0.0062,0.0133,-0.0846,0.0074,-0.0225,0.0137,0.0393,0.0394,-0.0382,-0.0158,0.037,-0.0157,0.0456,0.0642,0.045,-0.0209,-0.0402,0.027,-0.0069,-0.0312,-0.0954,0.0499,-0.0299,0.2621,0.0526,0.0379,-0.0172,0.0526,0.0274,-0.0367,-0.1,0.0459,0.0187,0.0817,-0.0196,-0.0215,-0.0813,-0.0436,0.0594,-0.0271,-0.0834,-0.0339,-0.0152,-0.0251,0.0285,-0.0194,-0.0069,0.0229,-0.0162,-0.0243,0.0008,-0.0606,-0.0449,-0.0495,-0.0061,-0.0634,0.0648,0.0176,0.0155,0.0109,-0.0285,0.0476,-0.0012,0.032,-0.047,0.0386,-0.0142,-0.0427,0.0286,-0.0094,-0.0042,0.0224,0.043,0.0327,0.0144,-0.0599,-0.0375,0.082,-0.0442,0.0268,0.0238,0.0397,-0.0211,0.0496,-0.0438,0.031,0.0065,0.0041,0.0569,-0.0759,-0.0367,0.0409,-0.0052,-0.294,0.0467,0.0366,0.0391,-0.0304,0.0091,-0.0103,0.0078,-0.0191,0.0332,-0.0143,0.0717,0.052,0.0403,-0.0042,0.0224,0.0981,0.0004,0.0578,-0.0962,0.0088,0.074,0.2263,-0.0106,0.0427,-0.01,-0.0263,-0.0211,0.0373,-0.0068,0.005,-0.0031,0.0838,-0.0596,0.0285,0.0179,-0.0326,0.0272,0.0123,0.0133,-0.0536,0.0105,-0.0149,-0.0463,0.07,0.0065,-0.0469,-0.021,-0.0281,0.0221,-0.0114,0.0076,-0.0197,0.0002,0.009,0.002,-0.0204,-0.0778,-0.0842,-0.0295,-0.0083,-0.0402,0.0578,0.0047,-0.0434]}
{"key":"[Exploring the Semantic Content of Unsupervised Graph Embeddings: An Empirical Study] Graph embeddings have become a key and widely used technique within the field of graph mining, proving to be successful across a broad range of domains including social, citation, transportation and biological. Graph embedding techniques aim to automatically create a low-dimensional representation of a given graph, which captures key structural elements in the resulting embedding space. However, to date, there has been little work exploring exactly which topological structures are being learned in the embeddings process. In this paper, we investigate if graph embeddings are approximating something analogous with traditional vertex level graph features. If such a relationship can be found, it could be used to provide a theoretical insight into how graph embedding approaches function. We perform this investigation by predicting known topological features, using supervised and unsupervised methods, directly from the embedding space. If a mapping between the embeddings and topological features can be found, then we argue that the structural information encapsulated by the features is represented in the embedding space. To explore this, we present extensive experimental evaluation from five state-of-the-art unsupervised graph embedding techniques, across a range of empirical graph datasets, measuring a selection of topological features. We demonstrate that several topological features are indeed being approximated by the embedding space, allowing key insight into how graph embeddings create good representations.","layer":2,"vector":[-0.0033,-0.027,-0.0184,0.0099,0.0527,0.0251,0.0274,0.0486,0.0027,-0.0154,0.0156,-0.0718,0.0498,0.0547,0.0505,0.0136,-0.0077,0.0908,-0.0638,-0.0117,0.027,-0.0597,0.0112,-0.0455,0.0403,0.0399,-0.0172,-0.0118,-0.0313,-0.228,0.01,-0.0683,0.0817,-0.0252,-0.0182,-0.0564,0.0092,0.0525,-0.0116,0.0497,0.0295,-0.0019,-0.0301,-0.0354,-0.0153,-0.0289,-0.0097,-0.0297,-0.0193,-0.0671,0.0325,-0.0361,0.0363,0.0342,0.0311,0.0647,0.0823,0.02,-0.0088,0.0684,0.0657,0.0533,-0.1133,0.0426,0.0552,0.0356,-0.0803,0.0335,-0.0066,0.0907,0.0437,-0.0004,-0.0454,0.0036,0.0595,0.0324,-0.0029,-0.0045,-0.017,-0.031,-0.0174,-0.0341,-0.0547,-0.0123,0.004,-0.0108,0.0219,-0.068,0.0059,0.0114,0.0029,-0.0408,-0.0457,0.0073,-0.0598,-0.0186,0.0533,0.0178,-0.026,0.1835,-0.0934,0.0471,0.0517,-0.0269,0.0285,-0.0741,0.023,-0.0381,-0.0428,-0.0087,-0.0194,-0.0115,0.0104,-0.067,0.0204,-0.0445,0.0796,0.065,-0.0187,-0.0249,-0.0634,0.0326,0.0357,-0.0281,0.0202,-0.0229,-0.0085,0.0794,0.053,0.007,0.0453,0.0488,-0.0132,0.0225,-0.0426,0.0613,0.0219,-0.0135,0.0167,0.0193,-0.0094,-0.01,0.0132,-0.0598,-0.1021,0.1369,-0.0776,-0.0579,-0.0361,-0.0116,-0.0497,-0.0053,-0.0178,-0.0194,-0.0183,0.0032,0.0245,0.0813,-0.0682,0.0129,-0.0208,-0.0288,-0.0358,0.1174,0.0264,-0.128,-0.0305,-0.0088,-0.0051,-0.053,0.0519,0.0459,-0.015,0.0721,0.1001,0.0339,-0.0639,-0.0513,0.0219,-0.0067,0.0289,-0.016,-0.0613,0.0374,0.035,-0.0255,-0.0328,-0.0356,0.0439,0.0535,-0.0267,0.0474,-0.0225,-0.0047,-0.0661,-0.0294,-0.0112,-0.0463,-0.008,-0.0475,0.034,-0.0166,-0.0398,0.0348,-0.0721,0.0002,-0.0228,0.0152,0.036,-0.0066,-0.0397,0.0359,0.005,-0.0382,-0.0128,-0.0053,0.0213,0.026,0.034,0.0354,-0.0101,-0.0519,-0.0501,-0.1934,-0.0545,0.0239,-0.0255,0.0263,-0.0513,0.0184,-0.0129,0.0699,0.0593,0.0521,-0.0103,-0.036,0.0238,0.0064,0.0604,0.0363,0.0299,-0.0279,0.0059,-0.0159,-0.0109,-0.0132,-0.0583,0.0111,-0.0009,0.2314,0.0372,0.0318,-0.0517,0.0119,-0.0022,-0.0662,-0.1128,0.0782,0.0308,0.0037,0.0004,-0.0217,-0.0117,-0.0544,0.0027,0.0155,-0.0813,-0.0088,-0.011,-0.024,-0.0239,-0.0547,0.0394,0.0457,-0.0063,0.0765,0.026,-0.0231,-0.0239,-0.045,0.0216,-0.0269,0.0212,0.0241,-0.0911,0.0184,-0.0714,0.0587,-0.0071,-0.0653,0.0015,0.0102,-0.0271,-0.0322,0.1022,-0.0051,-0.0318,0.0696,-0.0023,0.0337,-0.0091,-0.0287,-0.0049,0.0584,-0.0498,0.0502,0.023,0.0209,0.0211,0.0894,-0.0351,0.0612,-0.0121,0.0142,0.0025,-0.0267,-0.0613,0.0493,-0.0171,-0.2954,0.0577,0.0514,0.0468,0.0027,-0.0211,0.0582,0.0444,-0.0175,0.0001,0.0875,0.0271,0.0263,-0.0101,-0.0463,0.0442,0.0471,-0.0461,0.0459,-0.0235,0.0598,0.0406,0.2227,-0.0019,0.0598,0.0439,-0.009,-0.0031,0.0208,-0.0014,-0.0337,0.0184,0.0972,-0.0514,0.0317,0.0747,-0.0384,0.0193,0.0064,-0.0232,0.0213,-0.0284,-0.0744,-0.0058,0.0625,-0.0135,-0.0314,-0.0142,0.0368,0.0234,-0.0479,-0.0165,-0.0731,0.0004,0.0236,0.0194,-0.0362,-0.0286,-0.0367,-0.0369,-0.0092,-0.0488,-0.029,0.0067,-0.0159]}
{"key":"[Vanilla Feature Distillation for Improving the Accuracy-Robustness Trade-Off in Adversarial Training] Adversarial training has been widely explored for mitigating attacks against deep models. However, most existing works are still trapped in the dilemma between higher accuracy and stronger robustness since they tend to fit a model towards robust features (not easily tampered with by adversaries) while ignoring those non-robust but highly predictive features. To achieve a better robustness-accuracy trade-off, we propose the Vanilla Feature Distillation Adversarial Training (VFD-Adv), which conducts knowledge distillation from a pre-trained model (optimized towards high accuracy) to guide adversarial training towards higher accuracy, i.e., preserving those non-robust but predictive features. More specifically, both adversarial examples and their clean counterparts are forced to be aligned in the feature space by distilling predictive representations from the pre-trained/clean model, while previous works barely utilize predictive features from clean models. Therefore, the adversarial training model is updated towards maximally preserving the accuracy as gaining robustness. A key advantage of our method is that it can be universally adapted to and boost existing works. Exhaustive experiments on various datasets, classification models, and adversarial training algorithms demonstrate the effectiveness of our proposed method.","layer":1,"vector":[-0.0084,-0.0622,0.0082,-0.0206,0.0327,0.014,0.0288,-0.0281,-0.0214,-0.0378,-0.011,-0.0411,0.0521,0.0796,0.0025,-0.0028,0.0108,0.0517,-0.045,0.0682,0.0167,-0.0246,-0.0013,-0.0077,0.0291,-0.0149,-0.0096,-0.041,-0.0207,-0.2726,0.0113,-0.0418,-0.0088,-0.0305,-0.0053,-0.0273,-0.0777,0.0577,-0.0529,0.0423,0.019,-0.0,-0.0296,-0.0865,-0.0198,-0.0336,-0.0114,0.0063,0.0032,-0.0777,0.0648,-0.0531,0.001,0.024,0.0501,-0.0007,0.0699,0.0282,0.0398,0.095,0.0096,0.0352,-0.1481,0.0484,0.0122,0.0235,-0.0722,-0.0173,0.0199,0.0528,0.0374,0.0415,0.0184,0.0217,-0.0042,0.0302,-0.0132,-0.0248,-0.0091,0.006,0.0587,-0.0284,-0.03,0.0055,-0.018,-0.0338,0.0391,-0.038,0.0561,-0.0272,-0.0349,-0.0165,-0.0129,0.0429,-0.0157,-0.011,0.055,0.0222,-0.0639,0.2057,-0.0358,-0.0152,0.0019,-0.0424,0.045,-0.0093,-0.0643,-0.03,-0.0357,-0.003,-0.0093,0.0139,0.0051,-0.0112,0.033,0.0062,0.0837,0.0115,-0.0603,-0.0203,0.0035,0.0054,0.0712,-0.0252,0.0407,-0.0601,0.0021,0.1348,0.0095,0.0378,0.0241,-0.0256,-0.0248,-0.0061,0.002,0.0451,0.0051,0.0045,0.0233,-0.0252,-0.0867,-0.0187,0.0033,-0.0859,-0.0289,0.1156,-0.0414,-0.0008,0.0038,-0.0329,-0.0133,0.0012,-0.0292,-0.0311,0.0046,0.0041,0.0057,0.0838,-0.0553,-0.0326,-0.0083,-0.0372,-0.0482,0.0871,0.0073,-0.0901,-0.0178,0.0047,-0.0081,-0.0046,0.0339,0.0069,-0.0295,0.0093,0.0656,-0.0031,-0.09,-0.0338,-0.0111,0.0264,0.0052,-0.0724,-0.0366,0.0294,0.0721,-0.0114,0.0263,-0.079,0.0396,0.0395,-0.0561,0.0406,-0.0487,-0.044,-0.03,-0.0093,-0.0462,-0.0206,-0.0026,-0.04,-0.0275,0.0397,-0.0486,-0.0124,0.0089,0.0265,-0.0076,-0.025,0.026,0.0365,-0.0376,-0.001,0.0413,-0.0422,-0.0005,-0.0087,0.0174,0.0631,-0.018,0.0548,0.0361,-0.0406,-0.0291,-0.2581,-0.004,-0.0125,-0.0226,0.0696,-0.0732,0.0396,0.0139,0.0464,0.0045,0.0505,0.0046,-0.0249,0.0144,-0.0133,0.0931,0.034,0.0166,-0.0247,0.0432,-0.0403,0.023,0.016,-0.0867,0.0477,0.0371,0.203,0.0359,0.055,-0.0558,0.0162,0.0503,-0.0307,-0.1141,0.051,-0.01,0.0457,0.008,-0.021,-0.0026,0.0007,0.0408,0.0174,-0.1344,-0.0648,-0.0291,-0.0467,0.0273,-0.0947,0.0784,0.0723,-0.015,0.0758,0.0058,0.0095,0.0015,-0.1145,0.0699,-0.053,0.0369,0.0036,-0.0493,-0.0005,-0.0786,0.0346,0.0006,-0.0313,-0.0745,0.0949,-0.0023,-0.0155,0.0754,0.021,0.0299,0.0671,-0.018,0.0053,-0.0078,-0.0808,-0.0068,0.064,0.001,0.0171,0.0051,0.0679,-0.013,0.0648,0.027,0.0632,-0.0206,-0.0225,-0.0028,-0.0535,-0.0432,0.0564,0.03,-0.2783,0.0437,0.0222,0.0462,-0.0044,-0.0093,0.0757,-0.0116,-0.0327,-0.0048,-0.0092,0.0033,0.0472,-0.0342,0.0027,0.0222,0.0511,-0.043,0.0348,-0.0201,0.0355,0.0842,0.2115,0.002,-0.028,-0.0066,0.0081,0.0224,0.0254,-0.0344,0.0126,-0.0045,0.0909,0.0371,-0.0184,0.0495,-0.0306,-0.0169,0.0244,-0.019,-0.0226,0.0065,-0.0035,0.0245,0.0737,-0.0053,-0.0112,-0.0129,0.0207,-0.0194,-0.0291,0.0069,0.0108,0.031,0.0217,0.0387,-0.0359,-0.0512,-0.0135,-0.0162,0.0203,-0.016,-0.0145,0.028,-0.0317]}
{"key":"[Scaling Bayesian inference of mixed multinomial logit models to very large datasets] Variational inference methods have been shown to lead to significant improvements in the computational efficiency of approximate Bayesian inference in mixed multinomial logit models when compared to standard Markov-chain Monte Carlo (MCMC) methods without compromising accuracy. However, despite their demonstrated efficiency gains, existing methods still suffer from important limitations that prevent them to scale to very large datasets, while providing the flexibility to allow for rich prior distributions and to capture complex posterior distributions. In this paper, we propose an Amortized Variational Inference approach that leverages stochastic backpropagation, automatic differentiation and GPU-accelerated computation, for effectively scaling Bayesian inference in Mixed Multinomial Logit models to very large datasets. Moreover, we show how normalizing flows can be used to increase the flexibility of the variational posterior approximations. Through an extensive simulation study, we empirically show that the proposed approach is able to achieve computational speedups of multiple orders of magnitude over traditional MSLE and MCMC approaches for large datasets without compromising estimation accuracy.","layer":1,"vector":[-0.0131,0.0362,0.0147,-0.0368,0.0361,0.0572,0.0019,0.0213,0.0573,-0.0119,0.0196,-0.0621,-0.0002,0.0635,0.0473,0.0146,-0.0141,0.0182,-0.0502,-0.0129,0.0073,-0.0437,0.0017,-0.0716,0.0998,-0.0087,0.0081,-0.0297,-0.0562,-0.2657,0.0423,-0.0064,0.0321,-0.0349,0.0139,-0.013,-0.028,0.0593,0.0165,0.0621,0.0149,0.0201,-0.0149,-0.0213,-0.0414,-0.0532,-0.0553,0.0109,-0.054,-0.0057,0.0235,-0.0214,0.04,0.054,0.0453,0.0424,0.0181,0.0111,0.0667,0.0538,0.0173,0.0654,-0.1898,0.0609,0.0732,0.0128,-0.0284,0.0201,0.022,0.0255,-0.0457,0.0318,0.0083,0.064,0.0402,-0.0309,-0.0004,-0.027,-0.054,0.0302,-0.0067,-0.021,-0.0103,-0.0108,-0.0059,-0.0108,0.0018,-0.058,0.0404,-0.0312,-0.0012,-0.0234,-0.0239,-0.02,-0.0609,-0.033,0.0507,0.0632,-0.0173,0.2081,0.0062,0.0601,0.0317,0.011,0.0151,-0.048,-0.0185,-0.0231,-0.014,0.0356,0.0183,-0.0164,0.0735,-0.0342,-0.0014,0.0126,0.052,0.0161,-0.0012,0.0269,-0.0184,0.0085,0.0079,0.0111,-0.0042,-0.0694,-0.0109,0.1423,0.0176,0.0027,0.0677,0.0192,-0.0768,-0.0449,0.0419,-0.0166,-0.0035,-0.0071,0.0163,0.0266,-0.0304,-0.0326,0.0166,-0.1067,-0.0524,0.1351,-0.0083,0.0294,-0.0518,-0.0142,0.0146,0.0222,-0.0252,-0.0408,0.0338,0.0024,0.011,0.0424,-0.0335,0.005,0.0111,-0.0534,-0.0187,0.0673,0.0135,-0.0711,-0.0194,-0.0256,-0.0001,-0.0188,0.0471,0.017,0.0185,-0.0054,0.0938,0.005,-0.0791,0.0403,0.0037,0.0156,0.0117,-0.0472,-0.0358,0.0265,0.0121,-0.0608,-0.0139,-0.0336,0.0175,-0.0066,-0.0099,-0.0089,-0.0532,-0.0458,0.0247,-0.0233,-0.0156,-0.0328,0.0291,-0.02,0.0432,-0.0305,-0.073,0.0153,-0.0251,0.0374,-0.0302,0.0124,0.0646,0.0202,-0.0166,-0.0153,0.1143,0.0076,0.0024,0.0499,0.0353,0.0015,0.0178,0.0106,0.0426,-0.0664,0.0035,-0.2205,-0.024,0.0261,-0.026,0.0422,-0.0522,0.0546,0.0023,0.0472,0.1038,0.0218,0.0021,-0.0265,0.0603,0.0047,0.0548,-0.0251,0.0383,-0.0179,0.0567,-0.0062,-0.0126,-0.0722,-0.0945,0.0676,0.0154,0.2164,0.0074,0.0005,-0.0165,-0.011,0.0502,-0.0065,-0.0841,0.0809,0.036,0.0527,-0.033,-0.0538,-0.0346,-0.0451,0.0241,-0.0293,-0.1253,-0.0734,-0.0401,-0.0527,0.0254,-0.0606,0.0061,0.047,-0.0099,0.0246,-0.0308,0.0011,-0.0541,-0.1145,0.0394,-0.0436,0.0034,0.0387,-0.0461,0.0119,-0.0416,0.034,-0.0565,-0.0416,-0.0862,-0.0333,-0.0659,0.0029,0.0511,-0.0439,0.0171,0.0782,0.0227,0.0324,-0.0366,-0.0434,-0.0574,0.0631,-0.0385,0.0096,0.0307,0.0029,0.0239,0.0749,0.0085,0.0168,-0.0245,0.0023,-0.0374,-0.0313,0.0088,0.0008,0.009,-0.2837,0.0231,-0.0031,0.0312,-0.035,0.0123,0.0476,0.02,-0.0143,-0.0169,0.027,0.0853,0.0826,0.0033,0.0112,-0.0098,0.0755,-0.0466,0.0326,-0.0443,0.0101,0.0111,0.1901,-0.0009,0.0249,0.0249,-0.018,0.0366,0.0485,-0.0439,0.009,-0.008,0.0725,-0.0589,0.0552,0.0815,-0.0261,0.0871,0.0055,-0.0558,-0.0068,-0.0371,-0.0278,-0.0367,0.0928,-0.0066,-0.0058,-0.0372,-0.0138,0.0424,-0.0115,0.0271,-0.0077,-0.0015,-0.0231,0.012,-0.033,-0.046,-0.0481,-0.071,-0.0026,-0.0287,-0.019,-0.0155,-0.0375]}
{"key":"[Training Uncertainty-Aware Classifiers with Conformalized Deep Learning] Deep neural networks are powerful tools to detect hidden patterns in data and leverage them to make predictions, but they are not designed to understand uncertainty and estimate reliable probabilities. In particular, they tend to be overconfident. We address this problem by developing a novel training algorithm that can lead to more dependable uncertainty estimates, without sacrificing predictive power. The idea is to mitigate overconfidence by minimizing a loss function, inspired by advances in conformal inference, that quantifies model uncertainty by carefully leveraging hold-out data. Experiments with synthetic and real data demonstrate this method leads to smaller conformal prediction sets with higher conditional coverage, after exact calibration with hold-out data, compared to state-of-the-art alternatives.","layer":0,"vector":[-0.0244,-0.0215,0.0136,0.0004,0.0318,0.0648,0.0468,0.0071,0.0214,-0.0509,-0.0139,-0.0728,0.0136,0.0462,0.0147,0.0077,-0.0167,0.0703,-0.0189,0.0122,0.0591,-0.0299,0.0325,-0.0514,0.0324,0.0297,-0.0125,-0.0319,-0.0539,-0.233,0.0231,-0.0453,0.0265,-0.0547,0.0342,-0.041,-0.0608,0.0097,0.0093,0.0576,0.0099,0.0141,-0.0395,-0.0362,-0.0116,-0.0574,0.0014,-0.0121,-0.0131,-0.0452,0.0242,-0.043,0.0067,-0.0003,0.0406,0.032,0.051,0.056,0.038,0.0866,0.048,0.0224,-0.1295,0.066,0.0242,0.0133,-0.0391,-0.0449,0.0153,0.047,-0.0132,0.0193,0.0224,0.0802,0.0431,0.0215,0.0248,-0.0615,-0.0451,0.0201,0.0129,-0.0136,-0.0199,-0.0104,-0.0146,-0.0635,0.0377,-0.0139,0.0288,-0.0196,-0.0726,-0.0199,-0.0531,-0.0209,-0.0592,0.0489,0.0281,0.0347,-0.0551,0.1815,-0.049,0.005,0.0091,0.0358,0.0265,-0.0373,-0.0578,-0.0098,-0.0224,-0.0417,-0.0168,-0.0209,0.0531,-0.0367,0.0008,0.0179,0.0665,0.0319,-0.0121,-0.0164,-0.0372,0.034,0.0633,-0.0026,0.0147,-0.0818,0.0253,0.1559,-0.0056,0.018,0.0287,-0.0539,-0.0494,-0.0564,0.0747,0.0026,0.0091,0.0286,0.0608,-0.0038,-0.0159,-0.0054,-0.0203,-0.0633,-0.102,0.1156,-0.0659,0.0022,-0.008,-0.0432,-0.0146,0.0316,-0.0499,-0.0422,0.0625,-0.0038,0.0059,0.0252,-0.063,-0.0061,0.001,-0.0191,-0.0318,0.0709,-0.0099,-0.0525,-0.0099,-0.0175,0.0354,-0.0046,0.0345,0.0249,-0.019,0.0156,0.0296,0.0513,-0.0691,0.0142,0.003,0.0298,-0.0197,-0.052,-0.0334,0.0585,0.0234,-0.0196,-0.0316,-0.0447,0.0202,0.0303,-0.0121,-0.0004,-0.0336,-0.0156,-0.0181,-0.0057,-0.0415,-0.0106,0.0153,-0.0238,-0.0133,-0.0015,-0.0437,0.0453,-0.0025,0.0622,0.0299,0.0252,0.0544,0.0415,-0.0457,0.0121,0.0443,-0.061,-0.0064,0.0357,0.0426,0.0572,-0.0209,0.0156,0.0363,-0.0494,-0.0239,-0.245,-0.0108,0.0152,-0.0229,0.0544,-0.0857,0.0344,0.0068,0.034,0.0656,0.0005,0.0118,-0.0443,0.0322,-0.0516,0.0418,0.0231,-0.0196,-0.0355,0.0391,-0.0156,0.0562,-0.0492,-0.1036,0.0616,0.0311,0.2403,-0.0007,0.0345,0.0021,0.0163,0.0196,-0.0144,-0.0719,0.0968,0.024,0.0475,-0.0155,-0.0419,-0.022,-0.0209,0.0343,0.015,-0.1208,-0.0365,-0.0327,-0.0365,0.0466,-0.0809,0.0037,0.0395,-0.0215,0.0836,-0.0173,-0.0228,-0.0508,-0.0445,0.023,-0.0051,0.0047,-0.0112,-0.0291,0.0145,-0.0518,0.0417,-0.0183,-0.0107,-0.0452,0.0702,-0.0666,-0.0016,0.1111,-0.0469,0.0121,0.0719,-0.0042,0.0023,-0.0207,-0.0438,0.0035,0.0859,0.0026,0.0263,0.0192,0.0289,0.0349,0.0886,-0.0232,0.0936,0.0164,0.0249,0.0137,-0.0347,-0.0302,0.0601,-0.0126,-0.2948,0.0208,-0.0087,0.0069,-0.0647,-0.0329,0.0606,0.0084,-0.0504,-0.0145,-0.0031,0.0355,0.0434,-0.0099,0.0174,0.0276,0.0693,-0.0803,0.0768,-0.031,0.0327,0.0418,0.226,-0.0557,0.0133,0.0214,-0.0578,0.004,0.0609,-0.0322,0.0351,-0.0153,0.0553,-0.0557,0.0187,0.0641,-0.0327,0.0314,0.0149,-0.0087,0.0483,-0.0066,0.0018,-0.038,0.0957,-0.0646,-0.0296,-0.0146,-0.0081,-0.0267,-0.0178,0.0073,-0.0477,-0.0006,0.0066,0.0471,-0.0417,-0.0512,-0.0038,-0.0378,0.0451,-0.0718,-0.0446,-0.0014,-0.0198]}
{"key":"[GraphAD: A Graph Neural Network for Entity-Wise Multivariate Time-Series Anomaly Detection] In recent years, the emergence and development of third-party platforms have greatly facilitated the growth of the Online to Offline (O2O) business. However, the large amount of transaction data raises new challenges for retailers, especially anomaly detection in operating conditions. Thus, platforms begin to develop intelligent business assistants with embedded anomaly detection methods to reduce the management burden on retailers. Traditional time-series anomaly detection methods capture underlying patterns from the perspectives of time and attributes, ignoring the difference between retailers in this scenario. Besides, similar transaction patterns extracted by the platforms can also provide guidance to individual retailers and enrich their available information without privacy issues. In this paper, we pose an entity-wise multivariate time-series anomaly detection problem that considers the time-series of each unique entity. To address this challenge, we propose GraphAD, a novel multivariate time-series anomaly detection model based on the graph neural network. GraphAD decomposes the Key Performance Indicator (KPI) into stable and volatility components and extracts their patterns in terms of attributes, entities and temporal perspectives via graph neural networks. We also construct a real-world entity-wise multivariate time-series dataset from the business data of Ele.me. The experimental results on this dataset show that GraphAD significantly outperforms existing anomaly detection methods.","layer":0,"vector":[-0.0082,-0.0518,0.0417,-0.0206,0.0503,-0.0058,0.0428,-0.0052,0.048,-0.0233,0.0264,-0.0021,0.0179,0.0713,0.007,0.0043,0.0022,0.0172,-0.02,-0.0114,0.0103,-0.0195,-0.0048,-0.0606,0.0471,0.0139,-0.0174,-0.0307,-0.1045,-0.219,0.0125,-0.0514,0.0782,0.0142,0.0637,-0.0132,-0.0396,0.0504,-0.0037,0.0271,0.0204,-0.0046,-0.0241,-0.0736,-0.0143,-0.0682,0.0094,0.0132,-0.0406,-0.0175,0.0115,-0.0315,0.0249,0.0083,0.0507,0.0335,0.0432,0.0015,0.0426,0.0902,0.0605,0.0492,-0.1665,0.0344,0.0486,0.0245,-0.0027,0.0044,0.0148,0.0086,0.0161,0.0212,-0.0128,0.0248,-0.0178,0.0565,0.007,-0.0225,-0.0338,0.0246,-0.0123,-0.0203,-0.0114,-0.0246,-0.0326,-0.0348,0.0017,-0.0204,0.0906,0.013,-0.046,-0.0068,0.0248,-0.0,-0.0634,-0.0455,0.0458,0.0453,-0.048,0.2062,-0.0388,0.0356,0.0358,-0.0174,0.021,-0.0592,-0.028,-0.0661,-0.0231,-0.0293,-0.006,-0.0492,0.0367,-0.0929,-0.0083,0.0283,0.0417,0.0327,0.0061,-0.0151,-0.0207,0.0369,0.0762,-0.0337,0.0016,-0.0704,0.0336,0.117,-0.0001,0.0049,0.0061,0.0124,-0.0787,0.0158,0.0126,0.0225,-0.0072,0.0298,-0.005,-0.0263,-0.0796,-0.0167,0.029,-0.071,-0.0618,0.1098,0.014,-0.0116,-0.0373,-0.0178,-0.0604,0.058,-0.0418,-0.0603,0.0085,0.0193,0.0571,0.0169,-0.0457,0.0085,-0.0235,-0.0147,-0.0357,0.099,0.0113,-0.1222,-0.0394,0.0055,0.012,-0.0156,0.0449,0.0193,-0.0637,0.0193,0.0913,-0.0245,-0.0407,-0.0084,-0.0042,-0.0169,0.0465,-0.0211,-0.0529,0.0448,0.0551,-0.047,0.0298,-0.0183,-0.0045,0.0416,-0.0546,-0.0055,-0.0539,0.0294,0.0089,-0.012,-0.0282,-0.0109,0.0346,-0.0759,0.034,-0.0043,-0.0392,0.0161,-0.0174,0.0527,-0.0266,-0.0016,-0.0099,0.007,-0.0257,0.0275,0.0525,-0.0253,-0.011,0.0028,0.0224,0.083,0.0077,0.0526,0.0459,-0.0161,-0.0523,-0.2722,0.0047,0.0079,0.0114,0.0398,-0.0394,0.0167,-0.0336,0.0681,0.0743,0.063,-0.0025,-0.0253,0.0057,-0.0116,0.1043,0.035,0.0432,-0.0669,-0.0156,-0.0601,-0.0008,-0.0062,-0.0801,0.0284,0.0569,0.1872,0.0199,0.0339,-0.0751,0.0322,0.031,-0.023,-0.0654,0.0893,0.0468,0.0343,-0.0094,-0.0498,-0.029,-0.0585,0.0264,0.0032,-0.0314,-0.0286,-0.0173,0.0215,0.0364,-0.0793,0.0457,0.0592,-0.0116,0.0717,0.0314,0.0421,-0.0616,-0.0313,0.0466,-0.0459,-0.0034,0.0064,-0.0436,0.0204,-0.0611,0.0856,-0.0298,-0.035,-0.0425,0.0157,-0.0218,-0.0099,0.1357,-0.0079,-0.0263,0.0627,-0.0109,0.0342,-0.0268,-0.0474,0.0094,0.0608,-0.0594,0.0381,0.0359,0.0383,-0.0077,0.0725,-0.0093,0.0814,-0.0247,-0.0143,-0.0008,-0.0521,-0.0448,0.0282,-0.0204,-0.2987,0.0084,-0.003,0.0328,-0.0058,0.0246,-0.0293,0.038,-0.0354,-0.0315,-0.0088,0.0078,0.026,-0.0582,-0.0067,0.0625,0.0354,-0.0627,0.0435,-0.0254,0.0347,0.0524,0.2193,0.0258,-0.0088,0.0026,-0.0033,-0.0134,0.0261,0.0004,0.0332,0.0171,0.0723,-0.0317,0.0393,0.0594,-0.0273,0.0732,0.0285,-0.0258,-0.0148,0.0002,-0.0504,-0.0452,0.0761,-0.0177,-0.0144,-0.051,0.0327,0.0539,-0.0194,-0.0203,-0.0354,0.0344,0.0053,0.0163,-0.0212,-0.0386,-0.0457,-0.0816,-0.0115,-0.0637,-0.0443,-0.0139,-0.0214]}
{"key":"[Learning Effective Embeddings From Crowdsourced Labels: An Educational Case Study] Learning representation has been proven to be helpful in numerous machine learning tasks. The success of the majority of existing representation learning approaches often requires a large amount of consistent and noise-free labels. However, labels are not accessible in many real-world scenarios and they are usually annotated by the crowds. In practice, the crowdsourced labels are usually inconsistent among crowd workers given their diverse expertise and the number of crowdsourced labels is very limited. Thus, directly adopting crowdsourced labels for existing representation learning algorithms is inappropriate and suboptimal. In this paper, we investigate the above problem and propose a novel framework of \\textbf{R}epresentation \\textbf{L}earning with crowdsourced \\textbf{L}abels, i.e., \"RLL\", which learns representation of data with crowdsourced labels by jointly and coherently solving the challenges introduced by limited and inconsistent labels. The proposed representation learning framework is evaluated in two real-world education applications. The experimental results demonstrate the benefits of our approach on learning representation from limited labeled data from the crowds, and show RLL is able to outperform state-of-the-art baselines. Moreover, detailed experiments are conducted on RLL to fully understand its key components and the corresponding performance.","layer":1,"vector":[-0.0304,-0.0281,0.0134,0.012,0.0186,0.0229,0.0001,0.0345,-0.0153,-0.0315,0.0191,-0.0474,0.0326,0.0566,0.0415,0.0067,-0.0024,0.0823,-0.0597,-0.0085,0.0065,-0.0377,-0.0075,-0.0511,0.0299,0.0458,-0.0551,-0.0476,-0.0205,-0.2404,0.0735,-0.0574,0.0696,0.0261,0.001,-0.0306,-0.0151,0.0489,0.0022,0.011,-0.0091,-0.0163,-0.03,-0.0651,-0.0304,-0.036,-0.0463,-0.0342,-0.0156,-0.0034,0.0119,-0.0348,-0.019,0.033,0.0113,0.0379,0.0654,0.0235,0.0732,0.049,0.009,0.0141,-0.1575,0.0636,0.0266,0.0265,-0.0171,0.0156,0.0037,0.0226,0.0244,0.02,0.007,0.0627,-0.0114,0.0129,0.0081,-0.0529,-0.0156,-0.0154,0.0198,0.0197,-0.0037,0.0044,0.0092,-0.0434,0.0633,-0.0647,0.0674,0.0309,-0.0171,-0.009,-0.0184,0.0539,-0.07,-0.0463,0.0475,0.0635,-0.0504,0.2108,-0.0572,0.0692,0.0292,-0.03,0.0397,-0.0855,-0.0061,0.0136,-0.0385,-0.0066,-0.0319,-0.0176,0.0266,-0.038,0.0506,-0.0067,0.1124,0.0308,-0.0232,-0.0072,-0.0236,-0.0151,0.0299,-0.0137,0.0455,-0.0243,0.0427,0.1351,0.0368,0.0522,0.0231,-0.0126,-0.0687,-0.0239,-0.0151,0.0755,0.0119,0.0153,0.0089,-0.0185,-0.0173,0.0053,-0.0067,-0.0921,-0.0764,0.1311,-0.0315,0.0364,-0.0292,0.0025,0.0031,0.01,-0.0412,-0.0172,0.0511,0.0311,0.0964,0.0417,-0.0483,0.0243,0.0238,-0.0671,-0.0055,0.1006,0.0224,-0.1143,-0.0118,0.0026,0.0247,-0.0842,0.0492,0.0345,-0.0144,0.0798,0.0559,0.0638,-0.0715,-0.0438,0.0314,0.0151,0.0559,-0.0653,-0.0226,0.019,0.0159,-0.0546,-0.029,-0.0499,0.0428,0.0434,0.0417,0.0159,-0.0294,-0.0119,-0.0361,-0.0485,0.0395,-0.0432,0.026,-0.0059,-0.0046,0.0185,-0.021,0.0329,-0.004,-0.0212,0.0295,0.0001,0.0634,-0.0064,-0.0472,0.0071,0.019,-0.0136,-0.0529,-0.0265,0.0451,0.0321,0.0642,0.0183,0.0198,-0.0335,-0.0709,-0.2435,0.0181,0.0186,-0.0184,0.0352,-0.051,0.0185,0.0003,0.027,0.0774,0.0828,-0.0511,-0.0322,0.0522,-0.0343,0.0294,0.0398,0.0211,-0.0177,-0.0113,-0.0191,0.0255,-0.0309,-0.0758,0.0232,-0.0125,0.2162,0.059,-0.0084,-0.0432,0.0285,0.0666,-0.0663,-0.1353,0.0448,0.023,0.029,0.0084,-0.03,-0.0244,-0.0046,0.0085,-0.0044,-0.1282,-0.0342,-0.0512,-0.0589,-0.0187,-0.061,0.0233,0.0172,-0.0309,0.0868,-0.0227,-0.0072,0.0157,-0.0722,0.0483,-0.043,0.0009,0.0084,-0.0722,0.0184,-0.0556,0.0319,0.0265,-0.0601,-0.0654,-0.0081,-0.0214,-0.0605,0.0851,-0.0114,-0.0166,0.0452,-0.0249,0.0307,-0.0443,-0.0452,0.0036,0.0398,-0.0069,0.0266,-0.0064,0.0831,0.0232,0.0588,-0.0191,0.021,-0.0267,0.0266,0.0155,-0.053,0.0106,0.0264,-0.0343,-0.2542,0.0415,0.0125,0.0492,-0.0573,0.0257,0.03,-0.0001,-0.0526,-0.0335,-0.0158,0.0262,0.0367,-0.047,0.0086,0.0511,0.0764,-0.0659,0.0321,-0.0509,0.013,0.0492,0.195,-0.0491,0.0127,-0.0403,-0.0465,0.0091,0.041,-0.0085,0.0221,-0.037,0.0718,-0.0514,0.0168,0.0456,-0.0196,-0.025,0.033,0.0053,0.031,-0.0044,-0.0633,-0.0596,0.042,0.0525,0.0573,-0.0423,-0.0067,0.0347,-0.0288,0.0179,-0.0517,0.0106,-0.0076,0.0348,-0.049,-0.0128,-0.0272,-0.0248,0.0034,-0.058,-0.0164,0.0182,0.0031]}
{"key":"[Tackling unsupervised multi-source domain adaptation with optimism and consistency] It has been known for a while that the problem of multi-source domain adaptation can be regarded as a single source domain adaptation task where the source domain corresponds to a mixture of the original source domains. Nonetheless, how to adjust the mixture distribution weights remains an open question. Moreover, most existing work on this topic focuses only on minimizing the error on the source domains and achieving domain-invariant representations, which is insufficient to ensure low error on the target domain. In this work, we present a novel framework that addresses both problems and beats the current state of the art by using a mildly optimistic objective function and consistency regularization on the target samples.","layer":2,"vector":[-0.0325,-0.0489,-0.0082,-0.0233,0.0571,0.0151,-0.0286,-0.0128,0.0008,0.0097,0.0014,-0.0828,0.0097,0.0481,0.0389,0.0379,0.0193,0.0638,-0.0212,-0.0125,0.0233,0.0097,0.0277,-0.0186,0.007,0.0012,0.0114,-0.0413,-0.0332,-0.2723,0.0242,-0.0307,0.0288,-0.0216,0.0268,-0.0204,-0.0394,0.0468,-0.0183,0.053,-0.0031,0.0,-0.0149,-0.0961,-0.0518,-0.0521,-0.0263,0.0427,-0.0048,-0.0209,0.0451,-0.0328,0.0397,0.0209,0.0127,0.0123,0.0261,0.0602,0.0389,0.056,-0.0061,0.0694,-0.1495,0.059,0.0357,0.0165,-0.0495,-0.0207,-0.0192,0.0311,-0.0262,0.0766,0.0284,0.0335,0.0113,-0.0153,0.0187,0.0071,-0.041,0.0378,0.0611,0.0067,-0.0434,0.0101,0.0195,-0.0397,0.0143,-0.0499,0.055,0.0055,-0.0755,-0.0283,0.0099,0.0195,-0.0551,-0.0355,0.0343,0.0369,-0.0185,0.2023,-0.0327,-0.0006,0.0658,-0.0281,-0.0057,0.0134,-0.0037,0.0089,0.0044,0.0061,-0.0119,-0.0166,-0.0008,-0.0365,0.0399,-0.0142,0.0569,0.0175,-0.0162,-0.0391,-0.037,-0.0174,0.0465,-0.044,0.0297,-0.0343,0.0361,0.1213,0.0391,0.0173,0.0465,-0.0068,-0.0614,-0.0472,0.0345,0.0229,0.0243,-0.001,0.0333,0.0216,-0.0119,-0.107,0.0091,-0.0382,-0.0474,0.1437,-0.0369,0.0118,-0.0564,0.0151,-0.0303,-0.0021,-0.0323,-0.0048,0.0247,0.0023,0.076,0.014,-0.05,0.0224,0.0155,-0.062,0.0195,0.0804,-0.0324,-0.0762,-0.0446,-0.0169,0.0059,-0.0373,0.0262,0.0326,-0.0198,0.0511,0.0985,-0.0037,-0.0775,-0.0044,0.0564,0.0247,0.0543,-0.071,-0.0573,0.0849,0.015,-0.0422,-0.0177,-0.0296,0.0294,0.0437,-0.049,-0.0102,-0.0276,0.0185,-0.0032,-0.0324,-0.0463,-0.0064,0.0411,-0.0521,0.0076,0.0114,-0.0513,-0.0158,-0.0536,0.0076,-0.0154,-0.0089,0.013,0.0482,-0.0431,0.021,0.0539,0.0032,-0.003,0.0192,0.0447,0.0387,0.0253,0.0441,0.0265,-0.0374,-0.0357,-0.2303,0.0068,0.0212,-0.0248,0.031,-0.0511,0.0542,0.0448,0.0752,0.0791,0.0491,-0.0306,-0.0442,0.0463,-0.0318,0.0137,0.0577,-0.0134,-0.0139,-0.0085,-0.0204,0.0205,0.0197,-0.1253,0.0822,-0.0293,0.1838,0.0182,0.0275,-0.048,0.0109,-0.0027,0.0412,-0.1346,0.0369,0.031,0.0578,-0.0537,-0.063,0.0282,0.0145,0.0191,0.007,-0.1094,-0.0576,-0.0527,-0.0411,0.0299,-0.0542,0.0187,0.0509,-0.0155,0.0807,-0.0271,-0.0379,0.0076,-0.0686,0.0383,-0.0179,0.03,0.0437,-0.0497,0.0639,-0.0779,0.048,-0.0114,-0.0202,-0.0441,0.0495,-0.0057,-0.062,0.0366,0.0107,-0.0009,0.0122,-0.0191,0.0146,-0.02,-0.0563,-0.0229,0.0584,-0.0351,0.0444,0.0262,0.0576,0.0474,0.1172,-0.0136,0.0222,-0.034,0.0032,-0.0281,-0.0303,-0.0122,0.0587,-0.0074,-0.2965,-0.0021,0.015,0.0281,-0.0281,-0.0125,0.0553,0.0483,-0.0363,-0.004,-0.0328,0.0238,0.0074,-0.0289,0.0232,0.0658,0.0748,-0.0575,0.027,-0.0776,-0.0024,0.0589,0.2246,-0.0532,-0.0022,-0.006,-0.0621,-0.0083,0.0106,-0.0172,-0.0177,0.0029,0.0846,-0.0595,0.007,0.1088,-0.0575,0.0068,-0.0017,-0.0083,0.0029,0.0096,-0.023,-0.0048,0.078,0.0229,0.0162,-0.023,-0.0295,0.0429,-0.0396,0.0048,0.0023,-0.0056,0.0209,-0.0108,-0.0271,-0.033,-0.0637,-0.0337,0.0319,-0.0417,-0.037,0.0054,-0.0358]}
{"key":"[Adaptive wavelet distillation from neural networks through interpretations] Recent deep-learning models have achieved impressive prediction performance, but often sacrifice interpretability and computational efficiency. Interpretability is crucial in many disciplines, such as science and medicine, where models must be carefully vetted or where interpretation is the goal itself. Moreover, interpretable models are concise and often yield computational efficiency. Here, we propose adaptive wavelet distillation (AWD), a method which aims to distill information from a trained neural network into a wavelet transform. Specifically, AWD penalizes feature attributions of a neural network in the wavelet domain to learn an effective multi-resolution wavelet transform. The resulting model is highly predictive, concise, computationally efficient, and has properties (such as a multi-scale structure) which make it easy to interpret. In close collaboration with domain experts, we showcase how AWD addresses challenges in two real-world settings: cosmological parameter inference and molecular-partner prediction. In both cases, AWD yields a scientifically interpretable and concise model which gives predictive performance better than state-of-the-art neural networks. Moreover, AWD identifies predictive features that are scientifically meaningful in the context of respective domains. All code and models are released in a full-fledged package available on Github (https://github.com/Yu-Group/adaptive-wavelets).","layer":6,"vector":[-0.0791,-0.0222,-0.0186,0.0037,0.0492,0.0246,0.0331,0.0099,0.0315,-0.0189,-0.0036,-0.0789,0.0638,0.047,0.0269,-0.0078,-0.0088,0.0516,-0.0764,0.0236,0.0472,0.0193,-0.0236,-0.046,0.0321,0.0473,-0.0357,-0.0236,-0.0566,-0.242,0.0186,-0.0505,0.0169,-0.0241,0.0278,-0.003,-0.003,0.0443,-0.0513,0.0656,0.0576,0.0062,-0.054,-0.0546,-0.0199,-0.0686,0.0045,0.0049,-0.0196,-0.0629,0.0324,-0.0599,0.0094,0.0611,0.0087,0.0254,0.0488,0.0117,0.0535,0.031,-0.0136,0.0469,-0.1612,0.0642,0.0272,-0.0051,-0.0472,-0.0032,-0.0188,0.0187,-0.0033,-0.0081,-0.0162,0.0316,0.0321,0.0238,-0.0118,-0.013,-0.023,-0.0225,0.0251,-0.0123,-0.0753,-0.0478,-0.0673,-0.0336,0.0279,-0.0214,0.0327,0.0038,-0.0523,-0.0567,-0.0538,0.0156,-0.1013,0.0182,0.0409,-0.0084,0.0041,0.208,-0.0407,-0.0209,0.0168,-0.0134,0.0802,-0.0488,-0.0332,-0.0091,-0.0022,0.0306,0.0212,-0.0314,0.0458,-0.0285,0.0275,-0.0159,0.0432,0.041,0.0107,-0.0011,-0.0419,-0.0171,0.0574,-0.0345,0.0236,-0.0278,0.0222,0.1209,0.0323,0.0125,0.0822,0.0,-0.0442,0.0258,0.0127,0.0298,0.0069,-0.0142,0.0295,-0.0264,-0.083,-0.0325,0.0091,-0.0836,-0.0497,0.1072,-0.029,0.0127,-0.0539,-0.0222,-0.0335,0.0375,-0.0923,-0.0247,0.0333,0.052,0.0212,0.0496,-0.0656,0.0279,0.0226,-0.0559,-0.0475,0.1099,0.0354,-0.0445,-0.0089,0.0116,0.0338,-0.0309,0.0428,-0.0042,-0.0389,-0.0008,0.1169,0.0011,-0.038,0.0025,0.0407,0.0488,-0.0135,-0.0766,-0.0529,0.0343,0.0347,-0.066,-0.0158,-0.045,0.0093,0.0188,-0.0098,0.0439,-0.018,0.0276,-0.0029,-0.0354,0.0208,-0.0111,0.0045,-0.058,0.0255,-0.0047,-0.0251,0.0293,-0.0111,-0.0058,0.0004,0.0365,-0.002,-0.0371,-0.0136,0.0037,0.045,-0.0498,0.0032,0.006,0.0137,-0.0031,0.0108,0.0739,0.0756,-0.0752,-0.0525,-0.242,-0.0077,0.0405,-0.02,0.0825,-0.0452,0.0575,-0.004,0.0331,0.0462,0.0705,0.0352,0.0141,-0.0271,0.0039,0.0344,0.0304,0.035,-0.0423,0.0031,0.0036,0.0373,0.0218,-0.0884,0.0283,0.0019,0.2038,0.0471,0.0579,0.0018,0.0058,0.0427,0.0283,-0.1202,0.0483,0.0496,0.0771,-0.0242,-0.0074,-0.0575,0.0079,-0.009,0.0036,-0.0774,-0.0553,-0.0205,-0.0091,0.0347,-0.0624,0.0292,0.0412,-0.0359,0.0407,-0.0355,-0.0093,-0.0097,-0.0886,0.0338,-0.001,0.0285,0.0096,-0.0498,-0.0044,-0.0317,0.0339,-0.0017,-0.0249,-0.0326,0.0511,-0.0218,0.0036,0.0893,-0.0187,0.0203,0.0688,-0.0108,0.0299,-0.0605,-0.0892,-0.0013,0.0629,-0.0228,0.051,0.0256,0.0345,0.0158,0.0385,-0.0123,-0.0062,-0.0308,-0.0044,-0.0262,-0.0601,-0.0266,0.0285,-0.0295,-0.2944,0.0323,-0.0104,0.0358,-0.0145,0.0052,0.0393,-0.0003,-0.0402,-0.0409,-0.0407,-0.0083,0.0144,-0.0217,0.0073,0.0475,0.0708,-0.067,0.0544,-0.0391,0.0392,0.0682,0.2288,0.0388,0.0008,0.0311,-0.0144,0.0082,0.0373,0.0219,0.0089,0.0603,0.0856,-0.0406,0.0378,0.0754,-0.0247,0.0071,0.0553,-0.0181,0.0638,0.0049,-0.0431,-0.0703,0.0979,-0.0296,-0.0538,-0.0587,-0.0326,0.0198,-0.0333,0.019,0.0151,0.0121,0.0062,0.0236,-0.0539,-0.0322,-0.0041,-0.0221,0.0153,-0.0638,-0.0115,-0.014,-0.0329]}
{"key":"[Single Trajectory Nonparametric Learning of Nonlinear Dynamics] Given a single trajectory of a dynamical system, we analyze the performance of the nonparametric least squares estimator (LSE). More precisely, we give nonasymptotic expected $l^2$-distance bounds between the LSE and the true regression function, where expectation is evaluated on a fresh, counterfactual, trajectory. We leverage recently developed information-theoretic methods to establish the optimality of the LSE for nonparametric hypotheses classes in terms of supremum norm metric entropy and a subgaussian parameter. Next, we relate this subgaussian parameter to the stability of the underlying process using notions from dynamical systems theory. When combined, these developments lead to rate-optimal error bounds that scale as $T^{-1/(2+q)}$ for suitably stable processes and hypothesis classes with metric entropy growth of order $\\delta^{-q}$. Here, $T$ is the length of the observed trajectory, $\\delta \\in \\mathbb{R}_+$ is the packing granularity and $q\\in (0,2)$ is a complexity term. Finally, we specialize our results to a number of scenarios of practical interest, such as Lipschitz dynamics, generalized linear models, and dynamics described by functions in certain classes of Reproducing Kernel Hilbert Spaces (RKHS).","layer":0,"vector":[-0.0215,-0.0438,0.027,-0.0313,-0.0253,0.0185,0.0548,0.0272,0.0454,-0.0285,0.0045,-0.0308,0.0426,0.0501,-0.011,0.0209,-0.0013,0.0455,-0.0335,-0.0103,0.0642,-0.0504,0.0035,0.0039,0.0218,0.0123,-0.0391,-0.0493,-0.0299,-0.2596,0.0302,-0.0655,0.0318,-0.0466,0.0179,-0.0008,-0.04,0.0443,0.0137,0.0291,0.0121,0.0486,-0.0208,-0.0769,-0.0031,-0.0443,-0.0102,-0.0096,-0.055,-0.0393,-0.0213,-0.0106,0.0199,0.0145,0.0365,0.0566,0.0066,0.026,0.0899,0.0363,0.023,0.0072,-0.182,0.0131,0.0084,0.0104,-0.0477,-0.0349,0.0301,0.0402,0.0032,0.0406,-0.0017,0.0314,0.0165,-0.0288,-0.0036,-0.0281,-0.0269,0.0145,0.0235,-0.0429,-0.0372,-0.0298,-0.0417,-0.0796,0.0086,-0.0444,0.0622,0.0137,-0.0292,-0.0334,0.0011,-0.0436,-0.0679,-0.0048,0.0101,0.0559,0.0047,0.1648,-0.0281,0.0663,0.0592,0.0051,0.0062,-0.0651,-0.0228,-0.0477,-0.0223,0.018,-0.0075,-0.018,0.0217,-0.0486,0.0106,0.0041,0.0406,0.0433,-0.008,0.0026,-0.0332,0.0233,0.0664,-0.0429,0.0123,-0.042,0.0005,0.1557,0.044,0.0394,0.0226,-0.0002,-0.0399,-0.0288,-0.0229,0.0279,0.0303,0.0389,0.0342,0.0031,-0.026,-0.0616,0.0361,-0.0979,-0.0755,0.155,-0.0171,0.0468,-0.07,0.0091,0.0099,0.0185,-0.0356,-0.0466,0.0231,0.0426,0.0246,-0.0011,-0.0857,0.0479,-0.0991,-0.0713,0.021,0.1068,-0.0262,-0.0248,0.0185,0.0045,0.039,0.0132,0.0647,0.0264,-0.0349,0.0053,0.0729,0.0267,-0.0581,-0.0074,-0.0028,0.0409,0.0522,-0.0555,-0.0207,0.0156,0.0716,-0.0153,0.0021,-0.0229,0.0593,0.0497,-0.0241,-0.0176,-0.0035,0.0132,-0.0276,-0.05,-0.0008,-0.0114,0.0299,-0.0513,0.0049,0.012,-0.0314,0.0215,0.009,-0.0036,-0.012,-0.0191,0.0014,0.0248,-0.031,-0.0047,0.0583,-0.0323,-0.0536,0.0282,0.0041,0.0224,-0.005,0.0463,0.0014,-0.0063,-0.0643,-0.2243,0.0281,-0.0171,-0.0117,0.0952,-0.0361,0.0416,0.0012,0.0798,0.089,0.0345,-0.0031,0.0064,0.0223,-0.0143,0.0472,0.0319,0.0419,-0.0437,-0.0188,-0.046,-0.0095,-0.0503,-0.0927,0.0623,-0.0614,0.1735,0.0056,0.0786,-0.0327,-0.0037,0.0434,-0.0138,-0.0433,0.0513,0.0355,0.0527,-0.0492,-0.0434,-0.0303,-0.0112,-0.0157,0.0166,-0.0618,-0.0583,-0.0319,-0.0212,0.0164,-0.0636,-0.0244,0.0687,-0.0372,0.1247,-0.0726,0.0074,-0.0482,-0.046,0.0319,-0.0196,0.0715,-0.0532,-0.0532,0.0178,-0.0039,0.0571,-0.0116,-0.0146,-0.0638,0.0371,-0.0367,-0.0301,0.0886,0.0229,0.0165,0.0513,-0.0045,0.0175,-0.0441,-0.0913,-0.0103,0.0905,-0.0446,0.0433,0.0308,-0.0198,-0.0134,0.0673,-0.0571,0.0273,-0.0091,-0.006,0.0209,-0.0869,-0.0085,0.0255,0.017,-0.2804,0.0257,0.0267,0.0077,-0.0348,-0.0167,0.0546,-0.0153,-0.0447,0.018,0.0149,0.0698,0.0343,-0.001,0.0297,0.0218,0.0511,-0.0364,0.031,-0.0748,0.0165,0.0479,0.2032,-0.02,0.0383,-0.0046,-0.022,0.026,0.0169,-0.0826,0.0226,-0.0189,0.0781,-0.0325,0.0452,0.0866,-0.0507,0.074,-0.0073,0.0029,0.0145,0.0105,0.0015,0.0066,0.0971,-0.0433,-0.012,-0.0577,-0.0299,0.0352,0.0248,0.0642,0.0137,0.0594,0.0658,0.0495,-0.0659,-0.0681,-0.0204,-0.0533,-0.0056,-0.0556,-0.0311,-0.0553,0.016]}
{"key":"[Actionable Interpretation of Machine Learning Models for Sequential Data: Dementia-related Agitation Use Case] Machine learning has shown successes for complex learning problems in which data/parameters can be multidimensional and too complex for a first-principles based analysis. Some applications that utilize machine learning require human interpretability, not just to understand a particular result (classification, detection, etc.) but also for humans to take action based on that result. Black-box machine learning model interpretation has been studied, but recent work has focused on validation and improving model performance. In this work, an actionable interpretation of black-box machine learning models is presented. The proposed technique focuses on the extraction of actionable measures to help users make a decision or take an action. Actionable interpretation can be implemented in most traditional black-box machine learning models. It uses the already trained model, used training data, and data processing techniques to extract actionable items from the model outcome and its time-series inputs. An implementation of the actionable interpretation is shown with a use case: dementia-related agitation prediction and the ambient environment. It is shown that actionable items can be extracted, such as the decreasing of in-home light level, which is triggering an agitation episode. This use case of actionable interpretation can help dementia caregivers take action to intervene and prevent agitation.","layer":1,"vector":[-0.0688,0.0036,0.0302,-0.0071,0.0226,0.0216,0.0928,0.0106,0.0475,-0.0208,0.0033,-0.023,-0.0195,0.0435,-0.0103,-0.0166,-0.0152,0.0489,-0.0164,0.0373,0.0006,-0.0019,-0.004,-0.0394,0.0132,0.0434,-0.0566,-0.0272,-0.0425,-0.2168,0.0194,-0.0415,0.0756,-0.0442,0.0141,-0.0266,-0.013,0.0801,-0.0365,0.0348,0.0456,-0.0063,-0.0022,-0.0802,-0.0604,-0.0502,0.0071,-0.0464,-0.0154,-0.0425,-0.0085,-0.0298,0.0038,0.0683,0.0212,0.0038,0.0546,0.0457,0.0569,0.0273,0.0285,0.0557,-0.1699,0.0513,0.0108,0.0472,-0.034,-0.0209,0.0247,0.0702,-0.0144,-0.0092,0.0037,0.0516,0.0158,-0.0186,0.0031,-0.0257,0.0274,0.0117,0.0172,0.0139,-0.0251,-0.0304,-0.0183,-0.0437,-0.0403,-0.0618,0.0285,-0.004,-0.0478,0.013,-0.0394,0.0328,-0.0466,-0.0331,0.0566,-0.0083,-0.0601,0.1987,-0.0243,0.0276,0.0107,-0.0296,0.0101,-0.0455,-0.0297,-0.0344,0.0091,-0.0087,-0.0246,-0.0121,0.046,-0.0254,0.0605,0.0423,0.0441,0.0165,0.0195,-0.0139,0.0168,0.0285,0.0997,-0.0225,0.0407,-0.0392,0.0344,0.1931,0.0288,-0.0251,0.0307,-0.0558,-0.044,-0.032,0.0269,0.0285,0.0579,-0.0154,0.0531,-0.0155,-0.0368,-0.0446,-0.0079,-0.0934,-0.0747,0.1389,-0.0549,0.0053,-0.0504,0.0224,-0.0204,0.0581,-0.035,-0.0341,0.0102,0.0426,0.0391,0.0218,-0.0507,0.0162,-0.029,-0.1105,-0.0562,0.094,0.0036,-0.0507,-0.0394,-0.02,0.0034,0.0068,0.0586,0.0631,-0.0618,0.0155,0.0421,0.0445,-0.0195,-0.035,-0.0262,0.0013,0.0259,-0.0459,-0.0378,0.0548,0.0593,-0.0473,0.0284,-0.0034,0.0789,0.0525,-0.0004,0.0022,-0.0072,0.0314,-0.043,-0.0317,-0.0335,-0.0155,0.0066,-0.049,-0.0089,-0.0002,-0.0482,0.0349,0.002,0.0483,-0.0247,0.0308,0.0543,0.0326,-0.0068,0.0143,0.0619,-0.0326,-0.0159,0.0586,-0.0365,0.0104,0.0173,0.0584,0.0384,-0.009,-0.0733,-0.234,-0.026,0.0524,-0.0081,0.0162,-0.0589,0.0339,-0.0312,0.0159,0.0713,0.0782,-0.0321,-0.0307,0.0059,-0.0097,0.0423,0.0291,0.0334,-0.0811,-0.0095,-0.0381,-0.0231,0.0123,-0.0821,0.0178,0.0282,0.2053,0.0058,0.0256,-0.0535,-0.0138,0.0095,-0.0121,-0.1235,0.0669,0.013,0.0653,0.0028,-0.0248,-0.0233,-0.0719,0.0464,-0.0051,-0.0735,-0.0212,0.0006,0.009,0.0201,-0.0666,0.0055,0.0348,-0.0285,0.0567,-0.0215,0.0024,0.0118,-0.0801,0.0321,-0.0263,0.0225,-0.0452,-0.0755,0.0562,-0.0661,0.0636,-0.0161,-0.0388,-0.0445,0.0222,-0.0336,-0.0073,0.11,-0.0147,-0.0295,0.0796,0.042,0.0229,-0.0432,-0.047,0.0222,0.0157,-0.0536,0.0341,0.0383,0.0126,-0.0443,0.0451,-0.0577,0.005,-0.0073,-0.0101,0.0326,-0.0503,-0.0344,0.0249,0.0255,-0.2899,0.031,-0.0267,0.0502,-0.0192,0.0105,0.0418,0.0015,-0.0177,0.0046,-0.0297,0.0343,0.0661,-0.0127,-0.0003,0.0251,0.0712,-0.0448,0.0427,-0.077,0.0076,0.0572,0.1961,-0.0493,0.0183,0.0082,0.0157,-0.0099,0.0384,-0.0228,0.0213,-0.0554,0.1067,-0.0369,0.0599,0.0217,-0.0354,0.0146,0.0116,-0.0243,0.0043,0.023,-0.0657,-0.0476,0.1037,0.0143,-0.0118,-0.0524,-0.0464,0.0502,-0.0002,0.0064,-0.0032,0.038,0.0318,0.0344,-0.0287,-0.032,-0.0025,-0.0275,0.0295,-0.0733,0.0248,-0.0236,-0.0063]}
{"key":"[An Overview on Application of Machine Learning Techniques in Optical Networks] Today's telecommunication networks have become sources of enormous amounts of widely heterogeneous data. This information can be retrieved from network traffic traces, network alarms, signal quality indicators, users' behavioral data, etc. Advanced mathematical tools are required to extract meaningful information from these data and take decisions pertaining to the proper functioning of the networks from the network-generated data. Among these mathematical tools, Machine Learning (ML) is regarded as one of the most promising methodological approaches to perform network-data analysis and enable automated network self-configuration and fault management. The adoption of ML techniques in the field of optical communication networks is motivated by the unprecedented growth of network complexity faced by optical networks in the last few years. Such complexity increase is due to the introduction of a huge number of adjustable and interdependent system parameters (e.g., routing configurations, modulation format, symbol rate, coding schemes, etc.) that are enabled by the usage of coherent transmission/reception technologies, advanced digital signal processing and compensation of nonlinear effects in optical fiber propagation. In this paper we provide an overview of the application of ML to optical communications and networking. We classify and survey relevant literature dealing with the topic, and we also provide an introductory tutorial on ML for researchers and practitioners interested in this field. Although a good number of research papers have recently appeared, the application of ML to optical networks is still in its infancy: to stimulate further work in this area, we conclude the paper proposing new possible research directions.","layer":1,"vector":[-0.0996,-0.0148,0.0368,-0.0132,0.0469,0.0458,0.0519,0.0617,0.0139,-0.009,0.04,-0.0558,0.0303,0.0272,0.0183,0.0302,-0.0055,0.0186,-0.0359,-0.0287,0.0655,-0.0478,-0.0766,-0.0545,0.0454,-0.017,0.0009,-0.0363,-0.0526,-0.2115,-0.0011,-0.0436,0.0764,-0.0309,-0.005,-0.0602,-0.0455,0.0129,-0.0389,0.0728,0.0791,-0.0287,-0.0064,-0.0212,-0.0201,-0.0722,0.0133,-0.0262,-0.027,-0.0395,-0.0119,-0.0084,0.0129,0.0334,0.0136,0.0231,0.0542,0.0701,0.0584,0.0588,0.0403,0.07,-0.206,0.0239,0.0292,0.045,-0.0369,-0.0322,0.0249,0.0741,-0.0353,0.0312,-0.0049,0.0176,0.0015,0.0594,0.0048,-0.0185,-0.0123,0.049,0.0097,-0.0637,-0.0212,-0.0115,0.0061,-0.0474,0.0091,-0.061,-0.0096,0.0072,-0.0233,0.0327,-0.0048,0.014,-0.0666,-0.0179,0.0218,-0.0299,-0.0341,0.1712,-0.0425,-0.0073,-0.0043,-0.0358,0.0709,-0.031,-0.0109,-0.0259,-0.0157,-0.0191,-0.022,-0.0323,0.0126,-0.0335,0.0102,0.0208,0.0124,0.0433,0.0278,-0.0155,-0.0329,0.0032,0.0586,-0.0097,0.0464,-0.0448,0.0129,0.1104,0.031,0.0406,-0.0206,-0.0146,-0.0372,-0.0188,0.0147,0.0616,0.0309,-0.0246,-0.0139,0.0148,-0.0139,-0.0774,0.0357,-0.0528,-0.0476,0.1382,-0.0573,0.0267,0.0179,-0.0431,-0.0149,0.0133,-0.0252,-0.0447,-0.0214,0.0454,0.0559,0.054,-0.0678,0.0401,-0.0193,-0.0522,-0.0536,0.1367,0.0174,-0.08,-0.0393,-0.0294,-0.0287,-0.0148,0.0289,0.047,-0.021,0.0422,0.0179,-0.0077,-0.0703,-0.0272,-0.0332,0.0202,0.0067,-0.007,-0.012,0.0551,0.0778,-0.0037,-0.011,-0.0361,-0.0018,0.001,-0.0543,-0.0087,-0.0311,-0.0095,-0.0363,-0.0398,-0.0408,0.0227,0.0042,-0.0323,0.0138,0.0126,-0.0175,0.0024,-0.0671,0.0267,-0.0025,0.0436,0.0319,0.0263,-0.0306,-0.0398,0.0782,-0.0171,-0.0439,-0.0003,-0.0215,0.0822,0.0191,0.0521,0.04,-0.0134,-0.1062,-0.1953,-0.0308,0.0095,-0.0284,0.0795,-0.0646,0.0357,0.0105,0.0394,0.0621,0.0776,0.0509,-0.0245,-0.0413,-0.0134,0.0067,0.0087,0.0414,-0.0535,0.0057,-0.0211,0.0063,-0.0056,-0.0822,0.0228,0.0085,0.1696,-0.0117,0.0341,-0.0552,0.0584,0.0301,-0.0462,-0.0737,0.0416,0.0086,0.0765,-0.0154,-0.0138,-0.0007,-0.0222,0.0181,-0.031,-0.0949,-0.011,-0.0317,-0.0321,0.029,-0.042,-0.0071,0.0185,-0.055,0.0991,0.0076,0.0205,-0.0158,-0.0617,0.0325,-0.0276,-0.0055,0.0213,-0.0449,0.0309,-0.1251,0.0701,-0.0041,-0.0306,-0.0205,0.0148,0.0271,0.0041,0.1397,0.0183,0.011,0.0621,-0.02,0.014,-0.0465,-0.0209,-0.0074,0.1074,-0.0702,0.0567,0.0219,0.0053,0.0111,0.0642,0.0379,0.0446,0.0081,-0.046,-0.0215,0.0034,-0.0102,0.038,-0.0049,-0.2912,0.0579,0.0223,0.0434,-0.0224,0.0037,0.0594,0.0195,-0.006,-0.0179,-0.0005,0.0046,0.0223,-0.0276,0.0452,0.0666,0.042,-0.0482,0.0022,-0.0373,0.0652,0.0315,0.2369,-0.0469,0.0728,0.0533,0.0118,0.0633,-0.0206,-0.0545,0.0389,0.0225,0.0947,-0.056,0.0546,0.0289,-0.0416,-0.0147,0.0234,-0.0269,0.0158,0.0205,-0.0547,0.0024,0.1032,0.006,-0.0381,-0.0321,0.0062,0.0146,-0.0055,0.0271,0.0043,0.0255,-0.0128,-0.0098,-0.095,-0.058,-0.064,-0.0251,0.0127,-0.0823,0.0114,-0.0541,0.011]}
{"key":"[A Self-consistent-field Iteration for Orthogonal Canonical Correlation Analysis] We propose an efficient algorithm for solving orthogonal canonical correlation analysis (OCCA) in the form of trace-fractional structure and orthogonal linear projections. Even though orthogonality has been widely used and proved to be a useful criterion for pattern recognition and feature extraction, existing methods for solving OCCA problem are either numerical unstable by relying on a deflation scheme, or less efficient by directly using generic optimization methods. In this paper, we propose an alternating numerical scheme whose core is the sub-maximization problem in the trace-fractional form with an orthogonal constraint. A customized self-consistent-field (SCF) iteration for this sub-maximization problem is devised. It is proved that the SCF iteration is globally convergent to a KKT point and that the alternating numerical scheme always converges. We further formulate a new trace-fractional maximization problem for orthogonal multiset CCA (OMCCA) and then propose an efficient algorithm with an either Jacobi-style or Gauss-Seidel-style updating scheme based on the same SCF iteration. Extensive experiments are conducted to evaluate the proposed algorithms against existing methods including two real world applications: multi-label classification and multi-view feature extraction. Experimental results show that our methods not only perform competitively to or better than baselines but also are more efficient.","layer":0,"vector":[-0.0781,-0.0347,0.0195,-0.0214,0.0118,-0.0015,0.0634,0.0083,0.0239,0.0079,0.0254,-0.0909,-0.0003,0.0911,0.0394,0.035,0.0411,0.081,-0.0276,-0.0152,0.0174,-0.0339,0.0046,-0.047,0.0323,-0.006,-0.0294,-0.0457,-0.0639,-0.2847,0.0378,0.0113,0.0525,0.005,0.0002,-0.0235,-0.0306,0.07,0.0003,0.0456,0.0183,-0.0065,-0.0044,-0.0532,-0.0491,-0.0707,-0.0244,0.0207,0.0007,-0.0254,0.0171,0.012,0.0183,0.0099,0.0129,0.0645,0.0559,-0.0197,0.023,0.0049,0.0728,0.0613,-0.166,0.0779,0.0674,-0.0105,-0.0109,-0.0319,0.0439,0.0204,-0.0335,0.0178,0.028,0.002,0.0308,-0.0057,-0.0169,-0.0156,-0.0517,0.07,0.0089,0.0332,-0.0391,-0.0091,-0.0323,-0.0492,0.0031,-0.0595,0.0406,0.0333,-0.0545,-0.0701,0.0052,-0.0123,-0.0755,0.0238,0.0046,-0.0096,-0.0388,0.1941,-0.0568,0.036,0.0417,-0.0287,0.0488,-0.0327,-0.0257,-0.0322,-0.0055,-0.0176,0.0204,0.006,-0.0093,-0.05,0.0431,-0.0058,0.0624,0.0568,0.036,-0.0054,-0.0181,-0.0008,0.0216,-0.0159,0.0011,-0.0641,0.0461,0.1304,0.0395,0.0577,0.0197,0.0207,-0.035,-0.0538,-0.0092,0.0187,0.0119,0.0206,0.0133,0.0218,-0.0152,-0.042,0.0377,-0.0568,-0.0037,0.1696,-0.0859,0.01,-0.0522,-0.0191,-0.0009,-0.0149,-0.0226,0.0087,-0.0094,0.0333,0.0048,0.0538,-0.0569,0.0199,-0.0366,-0.0498,-0.003,0.1099,0.0115,-0.1259,-0.0304,-0.0011,0.0053,-0.0266,0.0516,0.0465,-0.0607,-0.0056,0.074,0.0183,-0.0632,0.0081,0.0317,-0.0049,0.0411,-0.073,-0.0504,0.0564,0.0639,-0.0289,0.0442,-0.0422,0.0041,0.002,-0.016,0.0009,-0.0213,0.0387,-0.0427,-0.0331,0.0119,-0.0137,0.0064,-0.042,0.0585,0.0302,-0.0352,0.0303,0.034,-0.0006,0.0158,-0.0139,0.0267,0.0257,-0.0547,-0.0185,0.0576,-0.018,-0.0021,0.0168,-0.0262,0.0602,-0.0208,0.0185,0.0665,-0.0737,-0.0818,-0.2643,0.0118,0.0005,-0.0194,0.0301,-0.0701,0.037,0.0125,0.0768,0.1077,0.0264,0.0402,-0.0347,-0.0027,-0.0315,0.0629,0.0253,0.0366,-0.0272,0.0003,-0.024,0.0352,0.0157,-0.0349,0.0566,-0.0481,0.1718,0.0339,0.0065,-0.0118,0.0495,0.0076,0.0,-0.0598,0.0309,0.0337,0.0385,-0.027,-0.0222,-0.0497,0.0121,0.0078,0.027,-0.0668,-0.0435,-0.0337,-0.0505,-0.0014,-0.0378,-0.0074,0.0626,-0.013,0.0122,-0.0151,0.0346,-0.0156,-0.0427,-0.0093,-0.0265,0.0045,0.0081,-0.0828,-0.0116,-0.0239,0.0434,0.0014,-0.0373,-0.0334,0.033,-0.0062,-0.0443,0.0778,0.0074,0.0051,0.0801,-0.0043,0.0378,-0.0244,-0.041,0.0307,0.1155,-0.0368,0.0259,-0.0057,0.0149,0.0105,0.0663,-0.0245,-0.0233,-0.0424,0.0189,0.0215,-0.027,0.0,0.0314,0.0078,-0.3132,0.0058,-0.0047,0.0209,0.0063,-0.0083,0.0404,0.0381,-0.0444,-0.0053,-0.0603,0.0221,0.0299,-0.0731,0.0049,0.0565,0.0381,-0.0597,0.0997,-0.0381,0.0154,0.0323,0.1871,-0.047,-0.0064,-0.0284,-0.0293,0.0016,-0.0038,-0.0389,0.0336,0.0285,0.0659,-0.0739,0.0444,0.0541,-0.0229,0.0302,-0.0116,-0.0437,0.0286,0.0274,-0.0772,-0.0433,0.0821,-0.023,-0.0199,-0.0314,0.0309,0.0209,-0.0256,-0.001,0.0004,-0.0167,-0.0051,0.0194,-0.0425,-0.0184,-0.0339,-0.0316,0.0104,-0.0038,-0.0741,0.0362,0.0006]}
{"key":"[A Riemannian Block Coordinate Descent Method for Computing the Projection Robust Wasserstein Distance] The Wasserstein distance has become increasingly important in machine learning and deep learning. Despite its popularity, the Wasserstein distance is hard to approximate because of the curse of dimensionality. A recently proposed approach to alleviate the curse of dimensionality is to project the sampled data from the high dimensional probability distribution onto a lower-dimensional subspace, and then compute the Wasserstein distance between the projected data. However, this approach requires to solve a max-min problem over the Stiefel manifold, which is very challenging in practice. The only existing work that solves this problem directly is the RGAS (Riemannian Gradient Ascent with Sinkhorn Iteration) algorithm, which requires to solve an entropy-regularized optimal transport problem in each iteration, and thus can be costly for large-scale problems. In this paper, we propose a Riemannian block coordinate descent (RBCD) method to solve this problem, which is based on a novel reformulation of the regularized max-min problem over the Stiefel manifold. We show that the complexity of arithmetic operations for RBCD to obtain an $\\epsilon$-stationary point is $O(\\epsilon^{-3})$. This significantly improves the corresponding complexity of RGAS, which is $O(\\epsilon^{-12})$. Moreover, our RBCD has very low per-iteration complexity, and hence is suitable for large-scale problems. Numerical results on both synthetic and real datasets demonstrate that our method is more efficient than existing methods, especially when the number of sampled data is very large.","layer":0,"vector":[-0.0236,-0.0411,0.0149,0.0274,-0.0165,0.0503,-0.0114,0.0422,0.0479,-0.0211,0.0258,-0.0646,0.0114,0.0343,0.0136,0.0419,0.0367,0.0696,-0.0594,0.0495,0.037,-0.0693,-0.0164,-0.1026,0.0411,0.0051,-0.0225,-0.0578,-0.0245,-0.2985,0.0278,-0.0597,0.0488,-0.0406,0.0281,-0.0164,-0.0411,0.0585,-0.0205,0.0427,0.0033,0.0396,-0.0241,-0.0461,-0.0137,-0.0286,-0.0547,-0.0038,-0.0111,-0.034,-0.0283,-0.0373,0.004,0.0219,0.0351,0.026,0.0472,0.009,0.0457,0.0482,0.0316,0.045,-0.1623,0.0267,0.0274,0.0152,-0.0378,-0.0436,0.0272,0.0555,-0.003,0.0004,0.0239,0.0265,0.0347,0.0163,0.0011,-0.0058,-0.0328,0.0189,0.0194,-0.0106,-0.074,0.0258,-0.0413,-0.0277,0.0418,-0.0416,0.0109,0.003,-0.0289,-0.0533,-0.0335,0.0474,-0.0539,-0.0573,0.0694,0.0406,0.0067,0.1928,-0.0338,0.0714,0.0343,0.0004,0.0177,-0.0305,-0.0153,0.0078,0.0011,-0.0322,-0.0238,-0.0021,0.0323,-0.0106,0.01,0.0349,0.084,0.0734,-0.0517,0.0253,-0.0403,-0.0093,0.0491,-0.0119,0.0208,-0.0555,-0.0172,0.1334,0.0479,0.082,0.05,0.0041,-0.0184,-0.0219,-0.0065,-0.009,-0.0138,-0.0069,0.0146,-0.0061,-0.0379,-0.0637,0.0273,-0.0912,-0.0289,0.1346,-0.0566,0.039,-0.0523,-0.0647,0.0249,-0.007,-0.0283,-0.0282,0.0213,0.0074,-0.0055,0.0494,-0.0656,0.0087,-0.0384,-0.077,0.0063,0.1078,0.0263,-0.0724,-0.0158,0.0107,0.0154,-0.032,0.0117,0.0298,0.0219,0.0456,0.0868,0.0201,-0.0842,-0.04,0.0031,0.0093,0.0281,-0.0573,-0.0259,-0.0609,0.0463,-0.0143,0.0249,-0.0167,0.0129,0.0385,-0.0043,0.0052,-0.0335,-0.0189,-0.0225,-0.0087,-0.0285,0.0064,0.0137,0.0013,0.031,0.0002,-0.0621,0.0453,0.0108,0.0448,0.0316,-0.0239,0.011,0.0417,-0.0299,-0.0386,0.0295,-0.0432,0.0001,0.0022,-0.003,-0.0144,-0.0227,0.0429,0.0523,-0.0177,-0.0708,-0.2037,-0.0202,0.0098,0.0104,0.0213,-0.0876,0.0601,-0.0185,0.0419,0.078,0.0275,-0.0088,-0.035,0.0213,-0.0048,0.0521,0.0612,0.0277,-0.0308,-0.0154,-0.0145,0.0111,-0.0206,-0.0282,0.0319,-0.0365,0.2329,0.0271,0.0312,-0.0422,0.0105,0.0177,-0.0039,-0.0553,0.0469,-0.0181,0.0731,0.0161,-0.0171,-0.0473,-0.0477,0.0136,0.0247,-0.0879,-0.024,-0.0315,-0.0653,-0.0094,-0.0319,-0.0091,0.0822,-0.014,0.0453,-0.0441,-0.002,-0.0,-0.0724,-0.0258,-0.0271,0.0576,-0.0089,-0.0687,-0.0097,-0.0203,0.0735,0.02,0.0114,-0.0333,0.0331,-0.0309,-0.0127,0.0488,-0.0207,0.0091,0.0792,-0.015,0.0515,0.0475,-0.0248,-0.0518,0.0296,-0.0176,0.0074,0.0045,0.0355,0.0173,0.0884,-0.0212,-0.0268,-0.0113,0.0127,-0.0207,-0.0758,-0.0061,0.0379,0.0023,-0.2952,0.0183,-0.0353,0.0161,-0.0325,-0.027,0.062,0.0308,-0.0577,-0.0059,0.0213,0.0511,0.0287,-0.0126,0.0526,0.0132,0.0759,-0.0308,0.0687,-0.0845,0.0085,0.0451,0.2083,-0.0283,0.0225,0.0168,-0.0044,0.0233,0.0333,-0.017,-0.0364,-0.0051,0.0669,-0.0532,0.0451,0.1067,-0.0277,0.0342,0.0268,-0.0287,0.0725,-0.0007,-0.0072,-0.0101,0.103,-0.0082,-0.0315,-0.0376,0.04,0.0145,-0.0275,0.0084,-0.0123,0.0352,0.0173,0.0439,-0.042,-0.0603,-0.076,-0.0737,0.0323,-0.0732,-0.0691,-0.0454,0.0091]}
{"key":"[Preventing Failures Due to Dataset Shift: Learning Predictive Models That Transport] Classical supervised learning produces unreliable models when training and target distributions differ, with most existing solutions requiring samples from the target domain. We propose a proactive approach which learns a relationship in the training domain that will generalize to the target domain by incorporating prior knowledge of aspects of the data generating process that are expected to differ as expressed in a causal selection diagram. Specifically, we remove variables generated by unstable mechanisms from the joint factorization to yield the Surgery Estimator---an interventional distribution that is invariant to the differences across environments. We prove that the surgery estimator finds stable relationships in strictly more scenarios than previous approaches which only consider conditional relationships, and demonstrate this in simulated experiments. We also evaluate on real world data for which the true causal diagram is unknown, performing competitively against entirely data-driven approaches.","layer":5,"vector":[0.0166,-0.0034,0.0146,-0.0137,0.0223,0.0119,0.0286,0.0571,0.0259,-0.0337,0.018,-0.0478,0.0169,0.0653,-0.0018,0.0302,-0.0306,0.1001,-0.0409,0.0047,0.0108,-0.0274,-0.0062,-0.0569,-0.0018,0.0463,-0.0027,-0.0194,-0.0202,-0.2548,-0.034,-0.0601,0.0067,-0.0222,0.0046,-0.0213,-0.0424,0.0605,-0.0044,0.0403,0.0258,0.0191,-0.0456,-0.0562,-0.0048,-0.0399,-0.0004,-0.0188,-0.0287,-0.0155,-0.0123,-0.0439,0.0302,0.0671,0.0639,0.0209,0.0761,0.0595,0.0242,0.0354,0.0123,0.0785,-0.1591,0.0525,0.0734,0.0063,-0.0525,-0.0159,0.0176,0.0412,0.0154,0.0195,0.0045,0.0442,0.0074,0.0136,-0.0131,0.0096,0.0049,0.0432,0.0441,-0.0053,-0.0493,-0.0039,-0.0309,-0.0672,0.0417,-0.0713,0.0151,0.0281,-0.0951,-0.0036,-0.0206,0.0235,-0.0582,0.0061,0.0402,0.0267,-0.0197,0.1587,-0.0606,0.0461,-0.0022,-0.0048,-0.0011,-0.0738,-0.0257,0.0049,-0.0113,-0.0096,-0.0061,-0.0073,0.0159,-0.0692,0.0198,0.0098,0.0762,0.0493,-0.0226,0.0019,-0.0332,0.0089,0.0543,-0.0202,-0.0043,-0.0921,0.0128,0.1552,0.0308,-0.0327,0.0435,-0.0141,-0.0785,-0.0062,0.0133,0.0127,0.0488,0.0079,0.0266,-0.0004,-0.0156,-0.0132,0.013,-0.0936,-0.0995,0.1655,-0.0214,0.029,-0.0477,-0.0242,-0.0456,0.013,-0.0149,-0.0286,-0.0076,0.0802,0.0072,0.018,-0.0356,0.0582,-0.0276,-0.0729,-0.045,0.0886,0.0046,-0.053,-0.017,-0.0041,0.0544,-0.0014,0.0397,0.0479,-0.0091,0.0246,0.0336,0.0251,-0.0686,-0.0247,0.0047,-0.0126,0.0689,-0.0519,-0.0013,0.0594,0.038,-0.0214,0.0093,0.0067,0.0437,0.0264,0.0286,0.0136,-0.0212,-0.0198,-0.037,-0.0253,-0.0068,-0.042,-0.0109,-0.0186,-0.0114,0.0192,-0.0445,-0.0069,-0.0531,-0.0151,-0.0543,-0.0056,0.0395,0.0199,-0.0179,0.0079,0.0638,-0.0347,-0.0355,0.0215,0.0186,0.0351,-0.0054,0.0498,0.0618,-0.0031,0.0149,-0.2813,-0.0269,0.0029,-0.0006,0.0361,-0.0629,0.0677,-0.0132,-0.0185,0.1121,0.0395,-0.004,-0.0509,0.0255,-0.0011,0.0264,0.0192,0.042,-0.0221,-0.0161,-0.0484,0.0067,-0.0018,-0.0874,0.0485,0.0181,0.2229,0.0163,0.0505,-0.0352,0.034,0.0014,-0.0379,-0.082,0.0594,0.0081,0.0362,-0.0121,-0.0305,-0.0544,-0.0327,0.0102,-0.0298,-0.1006,-0.0257,-0.0237,-0.0282,0.054,-0.0512,0.0504,0.0093,-0.0073,0.0496,-0.0178,-0.0147,-0.0317,-0.0441,0.046,-0.0635,0.0183,0.0342,-0.0409,0.0061,-0.0381,0.0245,-0.0041,-0.0034,-0.0876,0.0386,-0.0129,-0.0011,0.0834,0.004,-0.016,0.0291,0.0066,-0.016,-0.0244,-0.0614,-0.0201,0.0627,-0.0287,0.0113,0.0421,0.043,0.0117,0.0708,-0.0122,0.0459,-0.0266,0.0054,0.023,-0.0961,-0.0063,0.059,-0.025,-0.2821,0.015,0.0054,0.018,-0.0328,-0.0002,0.0467,0.0004,-0.0286,-0.0266,-0.017,0.0065,0.0528,0.0166,-0.0224,0.0439,0.1056,-0.0661,0.0265,-0.0777,0.032,0.0183,0.2244,-0.0001,0.0349,0.0132,-0.0152,-0.0144,0.0288,-0.0054,0.0259,0.0474,0.0462,-0.0428,0.0476,0.0739,-0.0448,0.042,0.0257,-0.022,-0.0304,0.0418,-0.0264,-0.0412,0.1389,-0.016,-0.0071,-0.031,-0.0225,0.014,0.0132,0.0035,-0.0336,0.003,0.0323,0.0304,-0.0387,-0.0462,-0.0514,-0.0609,0.0116,-0.058,-0.0368,-0.0073,-0.0283]}
{"key":"[Machine Learning in Population and Public Health] Research in population and public health focuses on the mechanisms between different cultural, social, and environmental factors and their effect on the health, of not just individuals, but communities as a whole. We present here a very brief introduction into research in these fields, as well as connections to existing machine learning work to help activate the machine learning community on such topics and highlight specific opportunities where machine learning, public and population health may synergize to better achieve health equity.","layer":0,"vector":[-0.0013,0.0082,0.0192,0.0024,0.0563,0.0489,0.0507,0.0635,0.036,-0.0191,0.0151,-0.08,0.0125,0.0178,0.0232,0.055,-0.0183,0.031,-0.0354,0.0305,-0.0325,-0.0124,-0.0156,-0.0697,0.0098,0.0292,-0.0392,-0.0481,-0.0591,-0.167,-0.0041,-0.0493,0.1043,-0.0305,-0.0008,-0.031,0.004,0.0558,-0.0626,0.0377,0.0125,-0.0244,-0.0271,-0.0492,-0.0176,-0.0364,-0.0335,-0.0235,-0.0317,-0.0427,0.0176,-0.0379,0.0283,0.0896,0.0487,0.0069,0.0618,0.0389,0.0297,0.0748,0.0441,0.036,-0.2161,0.0931,0.0385,0.0417,-0.0341,0.0055,-0.0045,0.0284,-0.0153,0.0614,0.0706,0.0569,-0.0176,0.0388,-0.0018,-0.0334,0.0173,0.0237,0.0471,-0.0055,0.0098,-0.0184,-0.0103,-0.041,-0.0079,-0.0507,-0.0022,0.0337,-0.0604,-0.015,0.0098,0.0416,-0.0488,0.0226,0.0456,0.0143,-0.0462,0.1836,-0.0996,0.0062,0.0094,-0.0325,0.0563,-0.0561,-0.0371,-0.0575,0.0008,-0.0225,-0.0113,-0.0249,0.0213,-0.006,0.0201,-0.0096,0.0703,0.0203,0.0181,-0.0466,-0.0277,-0.0089,0.0223,-0.0043,0.0389,-0.029,0.0157,0.1654,0.0425,0.007,0.0746,-0.0455,-0.0746,-0.0348,-0.0174,-0.0033,0.0109,-0.0059,0.0196,-0.0007,0.005,-0.0455,-0.0037,-0.1314,-0.0555,0.1243,-0.0321,0.0198,-0.0285,-0.0226,-0.0184,0.0343,-0.0661,-0.0244,0.035,0.0558,0.0569,0.0467,-0.0201,0.0203,0.0236,-0.0641,-0.0549,0.0945,-0.0057,-0.0786,-0.0141,0.0073,0.0022,-0.0058,0.0713,0.0615,-0.0064,0.0596,0.0782,0.0263,-0.0697,0.0095,-0.0025,-0.0062,0.0082,-0.0147,-0.0183,0.0219,0.056,-0.0534,-0.0045,-0.0768,0.0463,0.0362,0.0104,-0.0023,-0.0233,-0.0388,-0.01,-0.0489,-0.0361,-0.0219,0.0391,0.0104,-0.0403,0.0273,-0.0051,0.0508,0.0022,0.0047,-0.0045,-0.009,0.0384,0.0118,-0.0088,0.0102,0.0473,-0.0298,-0.0455,0.0046,0.0341,0.0239,0.0175,0.0995,0.0398,-0.0226,-0.082,-0.197,-0.0137,0.021,-0.0032,0.0011,-0.0659,0.0158,-0.024,0.0283,0.0963,0.0992,0.0033,-0.0599,0.0726,-0.0082,0.0405,0.015,0.019,-0.038,-0.0126,0.0085,-0.004,0.009,-0.099,0.0607,-0.0124,0.2006,0.0264,0.0051,0.0072,-0.0008,0.0007,-0.0448,-0.1415,0.0723,0.0003,0.041,-0.0054,-0.089,0.0077,-0.0341,0.0257,-0.0026,-0.074,-0.0548,-0.0297,-0.0233,0.0528,-0.0617,-0.004,0.0056,-0.0187,0.0337,-0.0342,-0.016,-0.0387,-0.1193,0.0244,-0.0304,0.0411,-0.0048,-0.062,0.053,-0.0538,0.0446,-0.0339,-0.0499,-0.0074,0.0063,-0.0088,-0.0321,0.151,-0.026,-0.0535,0.03,0.0191,0.0238,-0.0662,-0.0521,-0.0182,0.0799,-0.0239,0.0101,0.0228,0.0183,-0.03,0.0598,-0.0125,0.0212,-0.0277,-0.0191,-0.0075,-0.0565,-0.0114,0.0133,0.0018,-0.2557,0.063,-0.0174,0.0648,-0.0402,0.0081,0.0697,0.0391,0.0155,0.0011,0.0575,0.0214,0.0901,-0.0151,-0.0171,0.0181,0.0496,-0.0694,0.0495,-0.07,0.0167,0.0257,0.1914,-0.0437,0.0366,0.0277,-0.0035,-0.0127,0.0297,-0.0665,-0.0444,-0.0196,0.0753,-0.0632,0.0472,0.0564,-0.0606,-0.0221,-0.0015,-0.0083,-0.0041,0.0269,-0.0194,-0.0092,0.0788,-0.0059,-0.0325,-0.0927,0.0208,0.0522,-0.0278,-0.0055,-0.0428,0.0392,0.0562,0.0072,-0.0152,-0.0271,-0.0188,-0.0285,0.0354,-0.0512,-0.0359,0.0082,-0.0202]}
{"key":"[Vehicle Shape and Color Classification Using Convolutional Neural Network] This paper presents a module of vehicle reidentification based on make/model and color classification. It could be used by the Automated Vehicular Surveillance (AVS) or by the fast analysis of video data. Many of problems, that are related to this topic, had to be addressed. In order to facilitate and accelerate the progress in this subject, we will present our way to collect and to label a large scale data set. We used deeper neural networks in our training. They showed a good classification accuracy. We show the results of make/model and color classification on controlled and video data set. We demonstrate with the help of a developed application the re-identification of vehicles on video images based on make/model and color classification. This work was partially funded under the grant.","layer":0,"vector":[-0.0096,-0.0364,0.0477,-0.0135,0.0473,0.0622,0.0583,0.0151,0.0125,-0.0249,0.0272,-0.0505,-0.0243,0.0526,0.0145,0.0438,0.0568,0.0586,-0.0352,-0.0179,-0.0067,-0.0443,-0.035,-0.0619,0.0355,0.0023,-0.0174,-0.0144,-0.0572,-0.2153,0.0067,-0.0319,0.0695,-0.0259,-0.0126,-0.0257,-0.0535,0.0546,-0.0498,0.0268,0.0303,-0.0205,-0.0216,-0.0592,-0.0455,-0.0431,-0.0107,-0.0112,-0.0043,-0.0794,0.0325,-0.0495,0.0609,0.0219,0.0125,0.0772,0.0566,0.0389,0.0569,0.0165,0.0549,0.0358,-0.1905,0.0304,0.0131,0.0159,-0.0234,-0.0123,0.0368,0.0236,-0.0193,0.0421,-0.0132,0.0259,-0.0788,-0.0197,0.0133,-0.0491,-0.0282,-0.0144,-0.0146,0.0055,-0.009,-0.0014,0.001,-0.0671,-0.0064,-0.0224,0.0561,0.0137,-0.0639,-0.0124,-0.0199,-0.0113,-0.0718,-0.0217,0.0032,-0.0199,-0.0227,0.1955,-0.0528,0.0355,0.0695,-0.0457,0.0224,0.0105,-0.0188,-0.0126,-0.0691,0.0319,0.0204,-0.0091,-0.0033,-0.0031,-0.0272,-0.0129,0.0288,0.0792,0.0031,-0.0181,0.0194,-0.0113,0.0653,-0.0439,0.0142,-0.0572,0.0615,0.1605,0.032,0.0478,0.0218,-0.0144,-0.0794,-0.0311,0.0275,0.0359,0.0479,0.0271,0.0003,-0.0284,-0.0366,-0.0489,0.0102,-0.0768,-0.0101,0.0457,-0.0585,0.0137,-0.0252,-0.0227,-0.029,0.0372,-0.0532,-0.0372,0.0051,0.0378,0.0346,0.0422,-0.0583,0.0524,-0.028,-0.0612,-0.0248,0.1136,-0.0057,-0.1042,-0.0217,-0.0289,0.0255,0.0235,0.0062,0.0481,-0.0278,0.0419,0.0987,0.0636,-0.0934,0.0477,0.0115,-0.0121,-0.0,-0.0565,-0.0504,0.0207,0.0689,-0.0294,-0.0065,-0.059,-0.0104,0.0743,-0.0088,0.0446,-0.0407,-0.0227,-0.0357,-0.007,-0.0165,-0.0256,-0.0215,-0.0273,-0.0031,-0.0125,0.0011,0.0245,-0.0415,0.0023,-0.0394,-0.011,0.011,0.0454,-0.0373,-0.0104,0.0311,-0.064,-0.0402,-0.0207,0.0134,0.0273,0.0159,0.0384,0.0361,-0.0673,-0.0278,-0.1993,-0.016,0.0155,-0.0036,0.0232,-0.0689,0.048,0.0431,0.0573,0.0188,0.0675,-0.0251,0.0139,0.0062,0.0489,0.0766,0.017,0.0838,-0.0219,-0.014,-0.0408,0.029,-0.0299,-0.0795,0.0599,0.0199,0.2105,-0.0037,0.0877,0.0043,0.0294,0.0227,-0.0391,-0.0548,0.0813,-0.0555,0.0249,0.0608,-0.0612,-0.043,-0.0134,0.0042,0.018,-0.0984,0.0272,-0.0345,-0.0296,0.0437,-0.0444,0.0198,-0.0111,0.0075,0.0445,0.0409,-0.02,0.0034,-0.0689,0.0521,-0.002,-0.008,-0.0059,-0.0888,0.0395,-0.0979,0.0474,0.0373,-0.0781,-0.083,0.0019,-0.0301,-0.0116,0.1156,0.0218,-0.0332,0.086,0.0217,0.0365,-0.0171,-0.0055,-0.0251,0.0473,-0.0082,0.0636,-0.002,0.0751,0.006,0.0328,-0.0103,0.0162,0.0079,0.0439,0.0026,-0.0186,0.0083,0.0355,0.0031,-0.27,0.0391,0.0024,0.0445,-0.025,0.0091,0.0073,0.01,-0.0096,0.0158,0.0071,0.0298,0.0713,-0.0338,0.0482,0.012,0.0493,-0.0493,0.0386,-0.0631,-0.0206,0.0139,0.2336,-0.0511,0.0407,0.0534,-0.0255,-0.0157,0.0082,-0.003,-0.0073,-0.0025,0.084,-0.0886,0.0047,0.0818,-0.046,0.0465,0.0196,0.0073,-0.0005,0.0246,-0.0036,-0.0114,0.1043,-0.0331,0.0028,-0.0787,0.0157,0.0037,-0.0153,-0.0338,-0.0325,-0.0121,0.0612,0.0096,-0.0868,-0.026,-0.009,-0.0523,0.0191,-0.0937,-0.0079,0.0305,-0.0118]}
{"key":"[Boosting the Predictive Accurary of Singer Identification Using Discrete Wavelet Transform For Feature Extraction] Facing the diversity and growth of the musical field nowadays, the search for precise songs becomes more and more complex. The identity of the singer facilitates this search. In this project, we focus on the problem of identifying the singer by using different methods for feature extraction. Particularly, we introduce the Discrete Wavelet Transform (DWT) for this purpose. To the best of our knowledge, DWT has never been used this way before in the context of singer identification. This process consists of three crucial parts. First, the vocal signal is separated from the background music by using the Robust Principal Component Analysis (RPCA). Second, features from the obtained vocal signal are extracted. Here, the goal is to study the performance of the Discrete Wavelet Transform (DWT) in comparison to the Mel Frequency Cepstral Coefficient (MFCC) which is the most used technique in audio signals. Finally, we proceed with the identification of the singer where two methods have experimented: the Support Vector Machine (SVM), and the Gaussian Mixture Model (GMM). We conclude that, for a dataset of 4 singers and 200 songs, the best identification system consists of the DWT (db4) feature extraction introduced in this work combined with a linear support vector machine for identification resulting in a mean accuracy of 83.96%.","layer":0,"vector":[-0.0475,-0.0278,0.0452,-0.0656,0.0417,0.0185,0.0466,-0.0183,0.0505,-0.0142,0.0489,-0.022,0.05,-0.0316,0.0232,0.0429,0.0646,0.0879,-0.0557,0.0198,-0.0037,0.0429,-0.0527,-0.0292,0.0639,-0.0089,-0.0127,-0.0436,-0.0395,-0.1952,0.0222,-0.0581,0.0894,-0.0538,-0.0323,-0.0204,-0.03,0.0693,-0.0438,0.0214,0.0257,-0.0095,-0.0478,-0.0843,-0.0157,-0.0254,-0.0125,-0.0028,-0.0239,-0.0055,0.0294,-0.034,0.0283,0.0992,0.0007,0.0286,0.0589,0.0322,0.0528,0.0464,-0.0109,0.0479,-0.1614,0.0481,0.0051,0.0444,-0.0211,-0.0423,0.0193,0.0298,-0.0449,0.0335,0.0314,-0.0035,-0.0254,-0.0293,0.0077,-0.0435,-0.048,0.0483,0.0399,-0.0142,-0.005,-0.0493,-0.0714,-0.0203,-0.0023,-0.0518,0.0093,-0.0201,-0.0349,0.0227,-0.0531,0.0244,-0.0966,-0.0252,0.0296,0.0213,-0.0012,0.2214,-0.0065,0.0209,0.058,-0.0414,0.0243,-0.0493,-0.0516,-0.0083,0.0112,0.0017,0.0227,-0.0281,0.0131,-0.0271,0.0311,-0.013,0.0295,0.0049,0.0057,-0.0297,-0.0146,0.0126,0.0436,-0.0302,0.0778,-0.034,0.0489,0.112,-0.0067,0.051,0.0694,-0.0053,-0.0726,-0.0121,0.0161,0.0469,0.0115,0.0297,0.024,-0.0312,-0.0643,-0.0905,0.0049,-0.0433,-0.0106,0.1038,-0.031,0.0183,-0.0522,-0.01,-0.0019,0.0247,-0.0516,-0.0207,0.0684,0.0601,0.014,0.0605,-0.0545,0.0099,0.0335,-0.0592,-0.0206,0.0938,0.0019,-0.0926,-0.06,-0.0124,-0.0377,-0.0033,-0.0322,0.0,-0.0625,0.0424,0.0858,0.009,-0.0255,0.0041,-0.0105,-0.0153,0.0174,-0.0394,-0.0495,0.0212,0.0688,-0.0395,-0.0158,-0.0641,0.0386,0.0358,-0.0181,0.007,0.0281,-0.022,-0.0409,-0.0483,-0.0101,-0.0535,0.0139,-0.067,0.021,0.0414,0.0139,0.0257,0.0149,0.0352,0.0224,-0.0006,-0.0083,0.0284,0.0099,-0.0097,0.1005,-0.0408,0.0004,-0.0202,0.0342,0.0868,0.0332,0.0907,0.0523,-0.0246,-0.1097,-0.2306,-0.0157,0.0174,0.006,0.0529,-0.0373,0.0485,-0.0523,0.088,0.046,0.0704,0.0133,-0.0302,0.0303,-0.017,0.058,0.0245,0.02,0.0081,0.0218,-0.0277,-0.0052,-0.0178,-0.0322,0.0632,-0.04,0.1641,0.0284,0.0214,-0.0078,-0.0078,0.0173,-0.0732,-0.1177,0.066,-0.0035,0.0546,-0.0178,0.0075,-0.0346,-0.0121,-0.0023,0.0146,-0.0351,-0.0393,-0.0421,-0.0192,0.0213,-0.073,0.0196,0.0649,0.022,0.0176,-0.0231,0.0079,-0.0586,-0.0838,0.0153,-0.0485,0.0337,0.0227,-0.0765,0.0303,-0.0602,0.025,-0.007,0.0024,-0.0177,0.0361,-0.0176,-0.0307,0.0842,0.0383,-0.0193,0.0898,-0.0421,0.0459,-0.0903,-0.0367,-0.0246,0.0287,-0.0135,0.021,0.026,0.0018,0.0062,0.0733,-0.0002,-0.03,-0.0489,0.0201,-0.0157,0.0095,0.0016,-0.0105,0.0349,-0.304,0.0205,0.0127,0.044,-0.0501,-0.0162,-0.0217,-0.0202,-0.0922,0.0172,-0.0242,0.0073,0.0147,-0.0452,0.0117,0.0695,0.0627,-0.0515,0.054,-0.0305,0.018,0.051,0.2287,-0.0287,0.0063,0.0025,-0.0043,0.018,0.0129,-0.0257,0.0115,-0.0211,0.1192,-0.0732,0.0194,0.0499,-0.0373,0.0148,-0.0277,-0.0052,0.0301,0.0098,-0.0653,-0.0225,0.1143,-0.021,0.0007,-0.0642,0.0024,0.0123,-0.0167,0.0054,-0.0253,0.0098,0.029,0.0593,-0.051,0.0142,0.0114,-0.0171,0.0604,-0.06,-0.029,0.0114,0.0055]}
{"key":"[When do GANs replicate? On the choice of dataset size] Do GANs replicate training images? Previous studies have shown that GANs do not seem to replicate training data without significant change in the training procedure. This leads to a series of research on the exact condition needed for GANs to overfit to the training data. Although a number of factors has been theoretically or empirically identified, the effect of dataset size and complexity on GANs replication is still unknown. With empirical evidence from BigGAN and StyleGAN2, on datasets CelebA, Flower and LSUN-bedroom, we show that dataset size and its complexity play an important role in GANs replication and perceptual quality of the generated images. We further quantify this relationship, discovering that replication percentage decays exponentially with respect to dataset size and complexity, with a shared decaying factor across GAN-dataset combinations. Meanwhile, the perceptual image quality follows a U-shape trend w.r.t dataset size. This finding leads to a practical tool for one-shot estimation on minimal dataset size to prevent GAN replication which can be used to guide datasets construction and selection.","layer":6,"vector":[-0.0238,-0.0277,0.0201,-0.0181,0.0404,0.0649,0.0131,-0.0139,0.0141,-0.0093,0.008,-0.0564,0.0599,0.0865,0.0102,0.0272,-0.0027,-0.0431,-0.0492,0.0184,0.0143,-0.0349,0.0445,-0.0437,-0.0252,0.0017,-0.0276,-0.063,-0.0134,-0.2787,0.0151,-0.0414,0.0463,-0.0053,-0.0154,-0.0354,-0.0253,0.0657,-0.0177,0.0425,-0.0115,0.062,-0.0495,-0.074,-0.0292,-0.0374,-0.0331,-0.0053,-0.0143,-0.034,0.0394,-0.0882,0.0245,0.0347,0.0299,0.0141,0.0784,0.0517,0.0544,0.0322,0.0102,0.0605,-0.1669,0.0342,0.0401,-0.0061,-0.0056,-0.0511,-0.0107,0.0592,-0.0096,0.0237,-0.0033,0.0406,0.0122,-0.0034,-0.0203,-0.0466,-0.0505,0.0258,-0.0053,-0.0004,-0.0549,0.0307,-0.0038,-0.027,-0.008,-0.0082,0.0496,-0.0104,0.0009,0.0332,-0.0481,-0.0014,-0.0442,-0.0327,0.0188,0.0362,-0.0403,0.2033,-0.0553,0.036,0.0529,0.0146,0.0214,-0.0197,-0.0522,-0.0319,-0.0388,0.0219,-0.0338,-0.0256,-0.024,-0.0119,0.0333,-0.0024,0.0284,0.0042,-0.014,0.0093,-0.0444,0.0281,0.0468,-0.0195,0.0161,-0.0703,0.0209,0.1167,0.0378,0.0129,0.0108,-0.0011,-0.0581,-0.0317,-0.0257,0.032,0.0834,0.0319,-0.0187,-0.0286,0.002,-0.0154,0.057,-0.0438,-0.0406,0.1365,-0.0526,0.0515,-0.0255,-0.0299,-0.0434,0.0324,-0.0604,-0.0317,0.0285,0.0589,-0.0086,0.0383,-0.0575,0.0151,-0.0045,-0.02,-0.0317,0.0679,0.0179,-0.0786,-0.0062,-0.0033,0.0157,0.0264,0.0006,0.0132,-0.0529,0.0782,0.0535,0.0188,-0.1082,-0.0458,-0.0325,0.0516,0.0552,-0.0505,-0.0051,0.0489,0.0077,-0.0192,-0.0101,-0.0292,0.0265,0.0572,-0.0034,0.0236,-0.0361,0.011,-0.032,-0.024,-0.0303,0.0076,-0.0013,-0.03,-0.0187,-0.0048,-0.0115,-0.015,-0.0056,0.0219,-0.007,0.0001,0.0407,0.0571,-0.0446,-0.0147,0.0256,-0.0025,-0.0137,0.0067,0.014,0.0653,-0.033,0.0721,0.0321,-0.0691,-0.0426,-0.2551,0.0134,0.0024,-0.0314,0.0877,-0.0906,0.0474,0.0242,0.0668,0.0559,0.0382,0.0081,-0.0185,0.0024,-0.0229,0.0398,0.0194,0.034,-0.0472,-0.0466,-0.0245,0.0162,0.0118,-0.1129,0.055,-0.013,0.2127,0.0002,0.0306,-0.0142,0.0327,0.0532,-0.0793,-0.0756,0.0447,0.0641,0.0707,0.0005,-0.0518,-0.0077,-0.0258,0.0255,-0.0139,-0.1195,-0.019,-0.0019,-0.0244,0.0433,-0.0769,0.0037,0.0326,-0.0371,0.0666,-0.027,0.0098,-0.0353,-0.138,0.0341,-0.0177,0.0159,0.0253,-0.0686,0.0154,-0.087,0.052,-0.0002,-0.0274,-0.0213,0.0473,0.0252,-0.0111,0.0709,-0.0417,0.0028,0.0469,-0.0082,0.0229,-0.0367,-0.0367,-0.0176,0.0348,-0.0423,0.0362,0.0446,0.0574,0.0133,0.0703,-0.01,0.0635,-0.0178,0.0534,0.0227,-0.075,-0.0438,0.0175,-0.0209,-0.2818,0.0274,-0.0007,0.0902,-0.0307,0.0395,0.0214,0.054,-0.0153,-0.0026,0.0334,0.0315,0.0639,-0.0395,0.0076,0.0466,0.0509,-0.0463,0.0575,-0.0462,0.0038,0.0138,0.1991,-0.0453,-0.039,0.0095,-0.0076,0.0218,0.0119,-0.0005,-0.0018,0.0284,0.0855,-0.0221,0.0215,0.1148,-0.0691,0.0187,0.0429,-0.0001,-0.0054,-0.0002,-0.0474,0.0291,0.0735,-0.0356,-0.0051,0.0017,-0.0387,0.0083,-0.0546,-0.0001,-0.015,-0.0088,0.0109,0.0196,-0.0475,-0.0345,-0.0231,-0.0096,0.0134,-0.0242,-0.0151,0.0093,0.0092]}
{"key":"[Merlion: A Machine Learning Library for Time Series] We introduce Merlion, an open-source machine learning library for time series. It features a unified interface for many commonly used models and datasets for anomaly detection and forecasting on both univariate and multivariate time series, along with standard pre/post-processing layers. It has several modules to improve ease-of-use, including visualization, anomaly score calibration to improve interpetability, AutoML for hyperparameter tuning and model selection, and model ensembling. Merlion also provides a unique evaluation framework that simulates the live deployment and re-training of a model in production. This library aims to provide engineers and researchers a one-stop solution to rapidly develop models for their specific time series needs and benchmark them across multiple time series datasets. In this technical report, we highlight Merlion's architecture and major functionalities, and we report benchmark numbers across different baseline models and ensembles.","layer":5,"vector":[-0.0272,0.0003,0.0281,0.0343,0.0978,0.0093,-0.0027,0.037,0.0337,0.027,0.0069,-0.0467,0.0096,0.0444,-0.0011,-0.0287,-0.0065,-0.0057,-0.0379,0.0105,0.0212,-0.0126,-0.0214,-0.048,0.0425,0.0377,-0.0241,-0.0125,-0.0921,-0.2162,-0.0036,-0.1007,0.0409,-0.0359,-0.0216,0.0139,-0.0241,0.0362,-0.0034,0.0673,0.0274,0.0132,-0.0139,-0.1035,-0.0499,-0.0908,-0.0029,0.0053,-0.0251,-0.0019,0.0062,-0.017,0.0334,0.028,0.055,0.0253,0.0346,0.0319,0.0386,0.0676,0.0636,0.046,-0.1974,0.0392,0.0395,-0.0033,0.026,0.0238,0.0019,-0.0043,0.0143,0.0217,-0.0053,0.0429,-0.0027,-0.0069,-0.0174,-0.0705,0.0171,-0.0064,0.0011,-0.0453,-0.0576,-0.0502,0.0085,-0.0111,0.0175,0.005,0.0575,-0.0194,-0.0263,-0.0205,-0.0363,0.04,-0.062,-0.0131,0.0582,0.0338,-0.0733,0.222,-0.0375,0.0434,0.0233,-0.0029,0.0759,-0.0231,-0.043,-0.0827,-0.0224,-0.0152,0.0144,-0.0119,0.0612,-0.0428,0.0054,-0.0089,0.0132,0.0165,-0.0287,-0.0003,0.0358,0.004,0.0813,-0.0344,0.0237,-0.0343,0.054,0.1424,-0.0063,-0.0089,0.0266,-0.0103,-0.0692,0.0107,-0.0043,0.0342,-0.0056,-0.0037,0.0325,-0.0224,-0.0792,-0.0391,0.0333,-0.079,-0.0577,0.1101,-0.0114,0.0032,-0.0318,-0.0063,-0.0707,0.0263,-0.033,-0.0523,0.0035,0.0391,0.0355,0.0524,-0.0724,0.0199,-0.0241,-0.0363,-0.0329,0.1004,0.0179,-0.0854,-0.0031,0.0185,0.0289,-0.0012,0.0394,0.0192,0.0105,0.0441,0.1073,0.0192,-0.0309,0.0036,-0.0337,-0.0005,0.0209,0.0051,-0.0474,0.0639,0.0244,-0.0427,0.0226,-0.0322,0.0272,0.0483,-0.0197,0.0135,-0.0128,0.0276,-0.0218,-0.0106,-0.0206,0.0074,0.0503,-0.0691,-0.008,-0.0058,-0.0005,0.0206,-0.0272,0.063,-0.0585,0.026,0.0459,0.0162,-0.0013,0.0183,0.0631,-0.0381,-0.0294,0.0047,0.0215,0.0538,0.019,0.0527,0.0049,-0.0201,-0.0512,-0.2469,-0.0035,0.0108,-0.022,0.0296,-0.0338,0.0063,-0.036,0.0961,0.0539,0.0645,0.0054,-0.0291,-0.0026,-0.0343,0.0597,0.0195,0.0427,-0.0525,0.0066,-0.0398,0.0139,-0.0073,-0.0984,0.0073,0.0267,0.1382,0.0368,0.0534,-0.0522,0.0157,0.0071,-0.0025,-0.0567,0.0605,0.053,0.075,-0.0115,-0.0754,-0.0794,-0.0602,0.0555,-0.0,-0.0509,-0.0225,-0.0278,-0.0256,-0.0014,-0.0569,0.0258,0.0357,-0.0486,0.1216,-0.0121,0.0217,-0.0457,-0.0697,0.0462,-0.0028,0.0076,0.0239,-0.0318,0.0516,-0.0976,0.0275,-0.0434,-0.0455,-0.0107,-0.001,-0.0175,-0.093,0.0862,-0.0298,-0.0066,0.0616,-0.0206,0.0179,-0.0541,-0.0374,-0.0439,0.0799,-0.0561,0.0546,0.0484,0.0164,-0.0169,0.065,-0.0029,0.072,-0.0287,-0.0095,-0.0026,-0.041,-0.0325,0.0508,0.0541,-0.2985,0.0306,-0.0045,0.0356,-0.0436,-0.0532,0.0165,0.0203,-0.0314,0.0142,0.0071,0.0278,0.0767,-0.0159,-0.0047,0.0235,0.0447,-0.077,0.027,-0.0548,0.0114,0.0101,0.2105,-0.0054,0.0109,0.0362,0.008,-0.0405,0.0782,-0.0121,0.0228,0.0149,0.0755,-0.0293,0.0272,0.0815,-0.0539,0.0586,0.0177,-0.0479,-0.0004,0.0049,-0.0359,-0.0206,0.0682,-0.0282,-0.031,-0.0662,0.0137,0.0712,-0.0269,-0.0165,-0.0129,0.0537,-0.012,0.01,-0.04,-0.0125,-0.0379,-0.0369,0.022,-0.0512,-0.0125,-0.0137,-0.0203]}
{"key":"[Dodging DeepFake Detection via Implicit Spatial-Domain Notch Filtering] The current high-fidelity generation and high-precision detection of DeepFake images are at an arms race. We believe that producing DeepFakes that are highly realistic and ``detection evasive'' can serve the ultimate goal of improving future generation DeepFake detection capabilities. In this paper, we propose a simple yet powerful pipeline to reduce the artifact patterns of fake images without hurting image quality by performing implicit spatial-domain notch filtering. We first demonstrate that frequency-domain notch filtering, although famously shown to be effective in removing periodic noise in the spatial domain, is infeasible for our task at hand due to manual designs required for the notch filters. We, therefore, resort to a learning-based approach to reproduce the notch filtering effects, but solely in the spatial domain. We adopt a combination of adding overwhelming spatial noise for breaking the periodic noise pattern and deep image filtering to reconstruct the noise-free fake images, and we name our method DeepNotch. Deep image filtering provides a specialized filter for each pixel in the noisy image, producing filtered images with high fidelity compared to their DeepFake counterparts. Moreover, we also use the semantic information of the image to generate an adversarial guidance map to add noise intelligently. Our large-scale evaluation on 3 representative state-of-the-art DeepFake detection methods (tested on 16 types of DeepFakes) has demonstrated that our technique significantly reduces the accuracy of these 3 fake image detection methods, 36.79% on average and up to 97.02% in the best case.","layer":6,"vector":[0.0019,-0.0217,0.0458,-0.0085,0.0381,0.0457,0.0285,-0.0145,-0.0199,-0.0042,0.038,-0.0754,0.0534,0.0626,0.0142,0.0106,-0.0245,0.0214,-0.0147,0.009,0.0756,-0.0033,0.0043,-0.0423,0.0069,0.0342,-0.0135,-0.0497,-0.0746,-0.2504,0.0403,-0.003,0.0262,-0.0567,0.0493,-0.0895,-0.0715,0.0395,-0.0302,0.0002,0.0063,0.0223,-0.0076,-0.057,-0.0259,-0.06,-0.012,-0.0357,0.0112,-0.0436,0.0168,-0.0449,0.0555,0.0349,0.0102,-0.0112,0.0993,0.0371,0.0717,0.0393,0.0168,0.0581,-0.1299,0.0616,0.0149,0.0053,-0.0138,-0.0846,0.0139,0.0345,-0.0209,0.0545,-0.0034,0.0336,0.0158,-0.002,-0.0063,-0.0163,-0.0358,-0.0008,0.0079,0.021,0.029,-0.0095,-0.009,-0.068,0.0389,-0.0238,0.0769,-0.025,-0.0712,-0.0334,-0.0068,0.0136,-0.0308,0.0348,-0.0168,0.0413,-0.0355,0.2038,-0.075,0.0195,0.046,-0.0225,0.0382,-0.0057,-0.0464,-0.0055,0.0028,0.0055,-0.0078,-0.0086,0.0717,-0.0145,-0.0007,-0.0121,0.0376,0.0411,-0.014,-0.0253,-0.0371,-0.0055,0.0384,0.0024,-0.0116,-0.063,0.0043,0.1552,0.0649,0.028,-0.0136,-0.032,-0.0387,0.032,-0.0015,0.047,-0.0111,0.0164,0.0282,0.0029,-0.0761,-0.0227,0.0225,-0.0691,-0.0386,0.0421,-0.0533,0.0294,-0.0538,-0.0594,-0.0158,0.0289,-0.0723,0.0015,0.0475,0.0162,0.0013,0.065,-0.0699,0.0416,-0.0238,-0.0335,-0.0555,0.0983,0.0258,-0.0705,-0.0525,0.0098,-0.0019,-0.0503,0.0189,0.0262,-0.0334,0.0176,0.0253,0.0229,-0.0849,-0.0091,-0.0264,-0.0081,0.0402,-0.0532,-0.0422,0.0398,0.0774,-0.0575,0.0163,-0.0354,0.0238,0.0507,-0.0242,0.0481,-0.0324,-0.018,0.0135,-0.0196,-0.0369,-0.005,-0.0114,-0.0037,0.0298,-0.0353,-0.0337,0.0361,-0.0153,0.033,-0.0086,-0.0099,0.0501,0.053,-0.0377,0.0014,0.0464,-0.0566,-0.0173,-0.0433,0.0388,0.003,-0.0425,0.0149,0.0416,-0.0436,-0.0599,-0.2742,0.0191,0.0055,0.0091,0.0565,-0.0684,0.0274,0.0027,0.0679,0.0379,0.0365,-0.0376,-0.0258,0.0076,0.0197,0.0995,0.0059,0.0258,-0.0292,-0.0417,-0.0346,0.0702,-0.002,-0.0827,0.0642,0.0017,0.2278,0.0221,0.0242,-0.0547,0.0047,0.0472,0.039,-0.0796,0.0222,0.0008,0.0579,0.0091,-0.0198,-0.0306,-0.0492,0.033,-0.0134,-0.1395,0.0044,-0.0135,-0.0253,0.0247,-0.0472,0.0238,0.0503,-0.0451,0.032,0.0085,-0.0126,-0.0624,-0.067,0.0203,-0.0567,0.0596,-0.0003,-0.063,-0.0034,-0.0452,0.096,0.0283,-0.0437,-0.0253,0.0605,0.0151,0.0241,0.102,-0.0058,0.005,0.0671,0.011,0.0417,-0.0363,-0.0434,-0.0329,0.0712,0.0189,-0.0207,0.0175,-0.0084,0.0266,0.0585,0.0192,0.056,-0.0555,0.0219,0.0461,-0.0683,-0.046,0.0053,-0.0114,-0.2869,-0.0137,0.0317,0.0368,0.0162,-0.0091,0.0739,0.0458,-0.0227,0.0025,-0.0858,0.0387,0.0389,-0.0178,-0.0142,0.0266,0.0041,-0.0364,0.0622,-0.0377,-0.0195,0.0179,0.2109,-0.0367,-0.0466,-0.027,0.0034,-0.0208,0.0376,-0.0099,0.0052,0.0313,0.0841,-0.0279,0.0182,0.0388,-0.0381,0.0482,0.009,-0.0446,-0.0198,0.0545,-0.001,0.0176,0.042,-0.0402,0.0156,0.0118,0.0379,0.0657,-0.0272,0.0101,-0.0306,-0.0339,0.0358,0.0229,-0.0432,-0.0465,-0.0161,-0.0073,0.0161,0.0037,-0.0263,0.0521,-0.0512]}
{"key":"[Decentralized Federated Learning for UAV Networks: Architecture, Challenges, and Opportunities] Unmanned aerial vehicles (UAVs), or say drones, are envisioned to support extensive applications in next-generation wireless networks in both civil and military fields. Empowering UAVs networks intelligence by artificial intelligence (AI) especially machine learning (ML) techniques is inevitable and appealing to enable the aforementioned applications. To solve the problems of traditional cloud-centric ML for UAV networks such as privacy concern, unacceptable latency, and resource burden, a distributed ML technique, \\textit(i.e.), federated learning (FL), has been recently proposed to enable multiple UAVs to collaboratively train ML model without letting out raw data. However, almost all existing FL paradigms are still centralized, \\textit{i.e.}, a central entity is in charge of ML model aggregation and fusion over the whole network, which could result in the issue of a single point of failure and are inappropriate to UAV networks with both unreliable nodes and links. Thus motivated, in this article, we propose a novel architecture called DFL-UN (\\underline{D}ecentralized \\underline{F}ederated \\underline{L}earning for \\underline{U}AV \\underline{N}etworks), which enables FL within UAV networks without a central entity. We also conduct a preliminary simulation study to validate the feasibility and effectiveness of the DFL-UN architecture. Finally, we discuss the main challenges and potential research directions in the DFL-UN.","layer":1,"vector":[-0.0268,-0.0578,0.0403,-0.0021,0.0793,0.0481,0.0212,0.0722,0.0467,-0.016,0.0339,-0.0919,0.0531,0.0562,0.0526,0.0375,-0.0202,0.0257,-0.0395,-0.0032,0.0434,-0.0019,-0.0096,-0.031,0.016,0.0628,-0.0508,-0.0295,-0.0489,-0.2128,0.0244,-0.0509,0.0503,0.0047,0.0025,-0.0005,-0.0121,0.0423,-0.0104,0.036,0.0507,-0.0002,-0.0333,-0.0787,-0.0046,-0.0358,-0.0068,-0.0196,-0.0227,-0.0616,0.0779,-0.0513,-0.0153,0.0026,0.0208,0.0547,0.0181,0.0399,0.0353,-0.0247,0.0243,0.0665,-0.152,0.0622,0.0447,0.052,-0.0429,-0.0312,0.0259,0.0036,0.0047,0.0402,0.0007,0.023,0.0119,0.0372,-0.014,-0.0431,0.0111,0.0205,0.04,-0.0041,-0.0388,-0.0207,-0.0362,-0.0271,0.034,-0.0723,0.0348,-0.0005,-0.0543,0.053,0.0,0.0216,-0.0644,-0.0043,0.0215,-0.0286,-0.1077,0.2074,-0.0521,0.0501,0.0555,-0.0494,0.0102,-0.0481,-0.0309,-0.0641,-0.0378,0.0046,-0.0293,0.0189,0.0214,-0.0276,-0.0006,0.0371,0.0591,0.0555,0.012,-0.0319,-0.0376,-0.0316,0.0677,-0.0192,0.0251,-0.0766,-0.0029,0.1383,0.0225,0.0415,0.0502,-0.0314,-0.0249,-0.0745,0.052,0.0539,0.0481,-0.0201,-0.0151,0.0236,-0.0532,-0.0397,0.0302,-0.0611,-0.05,0.0948,0.0112,0.0468,-0.0123,-0.0099,-0.0488,0.0203,0.0128,-0.0223,0.0508,0.0309,0.0551,0.0636,-0.0311,0.0154,-0.0433,-0.0409,-0.0298,0.1456,0.04,-0.1235,-0.0467,-0.0046,0.0158,-0.0101,0.0171,-0.0021,-0.0057,0.0591,0.0582,0.0112,-0.0649,-0.0279,-0.0328,0.0079,-0.0504,-0.0435,-0.0028,0.0342,0.0529,-0.0086,0.0074,-0.0415,0.0171,0.0052,-0.0838,0.0461,-0.0289,-0.0167,-0.0153,-0.0281,-0.0237,-0.0081,0.0251,-0.0175,0.0082,0.02,-0.0231,0.0083,-0.063,0.0174,-0.0112,-0.0006,-0.0079,0.0191,-0.014,-0.0135,0.0469,-0.0628,-0.0443,-0.0059,0.0135,0.0468,-0.0552,0.0177,0.019,0.0126,-0.02,-0.1736,-0.0176,0.0056,-0.0392,0.0532,-0.0375,0.056,0.0252,0.0483,0.0656,0.1082,-0.0071,-0.0619,0.0169,-0.0344,0.0895,-0.0065,0.0461,-0.0311,0.0146,-0.0136,0.0418,0.0015,-0.0612,0.0289,0.0081,0.1933,0.0098,0.0395,-0.0222,0.0317,0.0352,-0.0316,-0.0961,0.0211,0.0092,0.0465,-0.0215,-0.0296,-0.036,-0.0244,0.0574,0.0208,-0.1309,-0.025,-0.0679,-0.0699,0.0221,-0.0666,0.0006,0.0006,-0.0166,0.0191,-0.0056,-0.0223,-0.0093,-0.0719,0.0369,-0.0026,0.0509,-0.005,0.0011,0.0014,-0.1034,0.0867,-0.0429,-0.0459,-0.036,0.0177,-0.0434,0.0088,0.1042,0.0161,0.0144,0.0759,0.0067,0.0454,-0.0338,-0.0451,-0.0218,0.1018,-0.0432,0.0408,0.0166,-0.0075,0.0097,0.0471,0.0186,0.0343,-0.0266,0.0235,-0.0032,-0.0298,-0.0069,0.0639,-0.0117,-0.2862,0.0156,0.0491,0.0225,-0.06,-0.0131,0.0967,0.0412,-0.0636,0.0061,0.046,0.0728,0.0132,-0.0324,0.0205,0.0586,0.046,-0.0702,0.02,-0.0634,0.0517,0.0097,0.2044,-0.0802,0.0123,0.0352,-0.0656,0.0398,0.0142,0.0059,0.0105,-0.0167,0.105,-0.062,0.0255,0.077,-0.0243,0.0148,0.0252,-0.039,-0.0421,-0.0263,0.049,-0.008,0.0317,0.0381,-0.0292,-0.039,-0.0417,0.0107,0.0086,-0.0469,-0.0407,0.0036,0.0236,0.0175,-0.0482,-0.0206,-0.0796,-0.0381,-0.0021,-0.0588,-0.0439,-0.0253,0.0066]}
{"key":"[Constraints on Hebbian and STDP learned weights of a spiking neuron] We analyse mathematically the constraints on weights resulting from Hebbian and STDP learning rules applied to a spiking neuron with weight normalisation. In the case of pure Hebbian learning, we find that the normalised weights equal the promotion probabilities of weights up to correction terms that depend on the learning rate and are usually small. A similar relation can be derived for STDP algorithms, where the normalised weight values reflect a difference between the promotion and demotion probabilities of the weight. These relations are practically useful in that they allow checking for convergence of Hebbian and STDP algorithms. Another application is novelty detection. We demonstrate this using the MNIST dataset.","layer":1,"vector":[-0.052,-0.0016,0.0845,-0.0467,0.0011,0.0438,0.0702,0.0167,0.0299,-0.0304,0.0191,-0.0408,0.0076,0.0814,0.021,0.0301,-0.0511,0.0113,-0.0398,0.0221,0.048,-0.0048,-0.0027,-0.0134,0.0044,-0.0124,-0.0449,-0.0392,-0.0277,-0.2485,0.0294,-0.0543,0.0645,-0.0697,0.0179,-0.0148,-0.0291,0.057,-0.0234,0.0456,0.02,-0.0031,0.0251,-0.0925,-0.0253,-0.0351,-0.0149,-0.0158,-0.0221,-0.0384,0.0497,0.0033,0.0682,0.0024,0.0079,0.0316,0.0372,0.03,0.0451,0.032,-0.0346,0.0925,-0.158,0.0272,0.0552,0.0028,-0.0721,-0.0526,0.0334,0.0677,-0.0112,0.0304,0.0256,0.0247,0.0011,-0.0021,-0.0051,-0.0442,0.0043,0.0442,-0.0215,-0.0205,-0.0511,-0.0014,-0.0108,-0.0125,0.0436,-0.0273,0.0041,-0.0012,-0.0312,0.0136,-0.0577,0.0206,-0.089,-0.024,-0.0022,0.006,-0.0515,0.1826,-0.0312,0.0357,0.0439,0.0162,0.0267,-0.0427,-0.0345,-0.029,-0.0022,0.0031,0.0233,-0.0301,0.0099,-0.0349,0.025,-0.0058,0.0458,0.0461,-0.0256,-0.0044,-0.04,0.001,0.0664,-0.009,0.0422,-0.0339,-0.0093,0.1444,0.0184,0.0341,0.0056,-0.0472,-0.0305,-0.0003,0.0316,0.0215,0.0323,-0.005,0.0092,-0.0518,-0.0066,-0.0449,0.0216,-0.0822,-0.0671,0.0891,-0.0616,0.0313,-0.0399,0.0114,0.0007,-0.0229,-0.0159,-0.0885,0.0186,0.0582,0.0126,0.0216,-0.0521,0.0237,-0.0485,-0.0707,0.0281,0.0933,0.0354,-0.0456,0.0002,-0.036,-0.0131,-0.009,0.0293,0.0253,-0.0372,0.0362,0.0495,-0.0065,-0.077,-0.0458,0.0099,0.0024,0.0246,-0.0743,-0.0118,-0.0014,0.047,-0.0542,0.0245,-0.0206,0.0452,0.0486,-0.0286,-0.0076,-0.0403,-0.0173,-0.022,-0.0079,-0.0131,0.0073,0.0351,-0.0418,0.005,0.008,-0.0175,0.0082,0.0115,0.0259,-0.0241,0.0164,0.0609,0.0526,-0.0401,-0.0211,0.0551,-0.0337,-0.0326,-0.0014,-0.0082,0.0227,-0.0196,0.0203,0.0353,-0.0124,-0.0518,-0.2357,-0.0038,0.0304,-0.0308,0.0822,-0.0494,0.032,-0.0119,0.0062,0.0446,0.0249,0.0015,-0.0116,-0.0156,-0.0076,0.0612,0.0584,-0.0198,-0.0046,0.0278,-0.0433,0.0231,-0.0038,-0.0996,0.0633,-0.0275,0.1985,0.0354,0.0625,-0.0618,-0.0155,0.0254,-0.023,-0.065,0.0603,0.0365,0.042,-0.0067,-0.0025,-0.0235,-0.0188,0.0126,0.0089,-0.1063,-0.0516,-0.0298,0.0099,0.0022,-0.0171,0.0174,0.0445,-0.0007,0.0366,-0.0195,0.0314,-0.0323,-0.1015,0.0437,-0.0801,0.0716,-0.004,-0.0821,0.0085,-0.0769,0.0791,-0.0133,-0.0198,-0.044,0.0458,-0.0151,-0.0261,0.1091,0.0203,-0.0126,0.0113,0.0144,0.0214,-0.023,-0.0263,-0.0109,0.0335,-0.0252,0.0606,0.0731,0.0109,-0.011,0.0821,-0.0238,0.0168,0.0315,0.0471,0.047,-0.018,0.0052,0.0064,0.0397,-0.3067,0.0406,-0.0059,0.0528,-0.0195,0.0346,0.0453,-0.0108,-0.0624,-0.0303,-0.0316,0.0098,0.0186,0.038,0.0049,0.0331,0.0521,-0.023,0.087,-0.0766,-0.0174,0.1048,0.2538,-0.0543,0.0054,-0.0325,-0.0047,0.0161,0.0405,-0.0244,0.0216,0.0024,0.0692,-0.048,0.0582,0.1111,-0.0743,0.0101,-0.0018,-0.0616,0.0016,0.0316,-0.0734,-0.012,0.1059,-0.0047,-0.0456,0.0004,-0.02,0.0668,-0.0382,0.0051,-0.0068,-0.0386,0.0234,0.006,-0.0398,-0.0536,-0.0154,-0.0443,0.0283,-0.0575,0.0147,0.0037,-0.0048]}
{"key":"[Fast Learning-based Registration of Sparse 3D Clinical Images] We introduce SparseVM, a method that registers clinical-quality 3D MR scans both faster and more accurately than previously possible. Deformable alignment, or registration, of clinical scans is a fundamental task for many clinical neuroscience studies. However, most registration algorithms are designed for high-resolution research-quality scans. In contrast to research-quality scans, clinical scans are often sparse, missing up to 86% of the slices available in research-quality scans. Existing methods for registering these sparse images are either inaccurate or extremely slow. We present a learning-based registration method, SparseVM, that is more accurate and orders of magnitude faster than the most accurate clinical registration methods. To our knowledge, it is the first method to use deep learning specifically tailored to registering clinical images. We demonstrate our method on a clinically-acquired MRI dataset of stroke patients and on a simulated sparse MRI dataset. Our code is available as part of the VoxelMorph package at http://voxelmorph.mit.edu/.","layer":2,"vector":[-0.0012,-0.0066,0.0027,0.0503,-0.0138,0.0542,-0.0104,0.0247,0.0693,-0.0275,0.0298,-0.0806,0.0528,0.0485,-0.0173,0.0219,-0.0525,0.0662,0.0144,0.0158,0.0056,-0.0744,0.0219,-0.0228,0.0037,0.0158,-0.0299,-0.0427,-0.0688,-0.2557,0.0424,-0.0122,0.0605,-0.005,0.0267,-0.016,-0.053,0.097,-0.0431,0.0167,0.017,0.0332,-0.0444,-0.0615,-0.0374,-0.0589,-0.0089,0.001,0.0447,-0.0239,0.0197,-0.0244,0.018,0.0387,0.0413,0.0052,0.0396,0.0197,0.0402,0.0556,0.0218,0.0544,-0.1789,0.0796,0.0648,0.0289,-0.0325,-0.06,0.0117,0.0521,-0.0144,0.0345,0.0339,0.0518,0.0065,-0.0532,0.0135,-0.0414,0.0062,-0.0364,0.0312,-0.0085,-0.0429,-0.0353,-0.0507,-0.0309,0.0238,-0.0775,-0.0152,0.0025,-0.0365,-0.0129,-0.0591,0.0482,-0.0857,-0.0317,0.0493,0.0304,-0.0267,0.2168,-0.0313,0.0393,0.0068,0.0189,0.0684,-0.0424,-0.0646,-0.0192,-0.022,0.0001,-0.0302,-0.0042,0.0367,-0.0305,0.0387,-0.0153,0.0517,0.0156,-0.038,-0.0225,-0.0034,0.0455,0.0518,-0.0089,0.0384,-0.0854,0.0095,0.1215,0.0617,0.0192,0.0717,0.0186,-0.0322,-0.0119,0.0064,0.018,-0.0071,-0.0193,0.0091,0.0043,-0.0187,-0.0104,0.0027,-0.0748,-0.0773,0.1217,-0.0706,0.0184,-0.0971,-0.0515,-0.0111,0.0429,-0.0656,-0.0056,-0.0031,-0.0255,0.0276,0.0526,-0.0606,0.0176,-0.0011,-0.0897,-0.0228,0.108,0.0138,-0.0992,-0.0241,-0.0105,-0.0009,-0.0178,0.0154,0.041,0.0104,0.0215,0.0828,0.0624,-0.0273,-0.0203,0.0089,-0.0068,0.0088,-0.0547,-0.0633,0.0485,0.0575,-0.0188,-0.0159,-0.0355,0.0143,-0.0093,-0.0215,0.0404,-0.0425,-0.0175,-0.0265,0.0059,-0.0008,-0.0132,0.0083,-0.0183,0.0111,-0.0264,-0.0444,0.0047,0.0367,0.0053,-0.0175,0.0108,0.0246,0.0887,-0.0294,-0.0067,0.0801,-0.0609,-0.0393,0.0229,0.0106,0.025,-0.0395,0.0376,0.0334,-0.0709,-0.0435,-0.1925,0.0011,0.0192,0.0142,0.0149,-0.045,0.0521,0.0079,0.0678,0.0483,0.0211,-0.0228,-0.0195,-0.0007,0.0091,0.045,0.0633,0.026,-0.04,-0.0302,-0.0388,0.0193,-0.0275,-0.0475,0.0773,0.0271,0.2201,0.0174,-0.0083,-0.0262,0.0046,0.0076,-0.0272,-0.1362,0.0466,0.0275,0.0672,0.0174,-0.0493,-0.0152,-0.0723,-0.0211,0.0242,-0.1179,-0.0519,-0.0071,-0.0234,0.0318,-0.0372,0.0343,0.0358,-0.0553,0.038,-0.0551,-0.0103,-0.0239,-0.1062,0.0002,-0.0332,-0.0059,0.0069,-0.0601,-0.0009,-0.0665,0.0301,0.0168,0.018,-0.0512,0.0551,-0.0105,0.0135,0.0528,0.0062,0.0402,0.063,0.041,0.0071,0.0068,-0.0114,-0.0168,0.0555,0.0101,0.0283,0.046,0.0334,0.052,0.0582,0.0158,-0.02,-0.0503,0.0,0.0341,-0.068,-0.0388,0.0183,-0.0093,-0.2829,0.0651,0.0146,0.0337,0.0018,0.0439,0.0387,-0.0074,-0.0192,-0.0235,-0.0048,0.0347,0.0215,-0.0293,-0.0155,0.0609,0.0962,-0.0582,0.0607,-0.1125,-0.0024,0.0459,0.1746,-0.0196,-0.0198,0.0703,-0.0312,0.0299,0.0294,-0.0357,0.0196,-0.0339,0.0689,-0.041,0.0766,0.0885,-0.0808,0.0135,0.0262,-0.005,0.0179,0.0195,-0.0303,0.0072,0.0478,-0.0027,0.0018,0.0141,-0.0095,0.0204,0.0006,-0.023,-0.0189,0.0342,0.0296,-0.0086,0.0328,-0.0599,-0.0256,-0.0441,-0.0243,-0.0255,-0.0359,0.0344,-0.0234]}
{"key":"[KFC: A Scalable Approximation Algorithm for $k$-center Fair Clustering] In this paper, we study the problem of fair clustering on the $k-$center objective. In fair clustering, the input is $N$ points, each belonging to at least one of $l$ protected groups, e.g. male, female, Asian, Hispanic. The objective is to cluster the $N$ points into $k$ clusters to minimize a classical clustering objective function. However, there is an additional constraint that each cluster needs to be fair, under some notion of fairness. This ensures that no group is either \"over-represented\" or \"under-represented\" in any cluster. Our work builds on the work of Chierichetti et al. (NIPS 2017), Bera et al. (NeurIPS 2019), Ahmadian et al. (KDD 2019), and Bercea et al. (APPROX 2019). We obtain a randomized $3-$approximation algorithm for the $k-$center objective function, beating the previous state of the art ($4-$approximation). We test our algorithm on real datasets, and show that our algorithm is effective in finding good clusters without over-representation or under-representation, surpassing the current state of the art in runtime speed, clustering cost, while achieving similar fairness violations.","layer":0,"vector":[-0.0473,-0.0304,-0.0184,-0.0201,0.0094,0.0396,0.0229,0.0128,0.0348,0.0112,0.023,-0.0843,0.0272,0.0251,-0.0054,0.0374,0.0311,0.0203,-0.0469,0.0275,-0.0337,-0.0253,-0.0359,-0.0455,0.0185,0.0064,-0.0523,-0.0324,-0.0735,-0.2488,-0.0415,-0.0507,0.0635,-0.0005,-0.0035,-0.0296,-0.0256,0.075,-0.0568,0.0086,0.0516,0.0092,-0.0199,-0.0005,-0.0366,-0.0086,-0.0686,0.0442,-0.0182,-0.0543,0.0324,-0.0535,0.002,0.0651,0.0083,0.0204,0.0647,-0.0141,0.0537,0.0157,0.0344,0.0358,-0.1362,0.0251,0.0322,-0.0016,0.0306,-0.034,0.004,0.0126,0.0011,0.0784,0.0435,0.0145,0.0449,0.0087,-0.0008,-0.0347,-0.0329,0.0319,-0.0192,-0.0171,0.0113,0.0391,-0.0341,-0.0274,0.0278,-0.0784,0.0076,0.0122,-0.0169,-0.0078,0.0062,0.0029,-0.0862,-0.0704,0.0174,0.0384,-0.0149,0.2199,-0.0298,0.069,0.0761,-0.0402,0.0181,-0.0651,0.0128,-0.0684,-0.0258,-0.0114,0.0297,-0.0077,0.0068,-0.0697,0.0079,-0.0051,0.0899,0.0069,-0.0007,-0.0299,0.0353,0.017,0.0303,0.0097,0.0753,-0.0655,-0.0011,0.1405,0.0368,0.0212,0.0344,-0.0122,-0.0628,-0.0053,0.0198,0.0483,-0.0206,-0.0025,-0.0005,0.005,-0.0434,-0.1005,0.0194,-0.0951,-0.0014,0.1208,-0.031,0.0058,-0.0411,-0.0323,0.0192,-0.0242,-0.0516,-0.0191,0.0105,0.0039,0.0719,0.0529,-0.019,0.0657,-0.0015,-0.0383,-0.0241,0.1396,0.0242,-0.0791,0.0146,0.0578,0.0053,-0.0406,0.0443,0.0462,-0.0026,0.0652,0.1087,0.0124,-0.0908,0.0173,0.0146,-0.0214,0.0104,-0.0094,-0.0571,0.0272,0.0491,-0.0655,-0.0154,-0.0117,0.0408,0.0747,-0.0641,-0.0052,-0.0382,-0.0218,-0.0238,-0.0621,0.0193,-0.0337,0.035,-0.0266,-0.0228,0.0185,-0.0482,0.0495,-0.0016,0.0721,0.0021,-0.0584,0.0708,0.0137,-0.0182,0.0317,0.0203,-0.022,-0.017,0.0302,0.0521,0.04,0.0057,0.0376,0.0631,-0.0158,-0.0781,-0.1848,-0.0371,0.0038,0.0212,0.0325,-0.0368,0.0144,-0.0013,0.0528,0.0952,0.0481,0.0062,-0.0329,0.0685,-0.0256,0.0616,0.029,0.0558,-0.0271,0.0256,0.0082,0.0328,-0.0442,-0.045,0.0471,0.0141,0.2265,0.0417,0.0071,-0.0289,0.0504,0.0227,-0.0368,-0.1116,0.0239,0.039,0.0103,-0.0485,-0.0312,-0.0272,-0.0071,0.0181,-0.0085,-0.1162,-0.0258,-0.0084,-0.0092,0.0078,-0.0397,-0.0208,0.0066,-0.057,0.0427,-0.0054,0.0359,-0.0521,-0.0813,0.0206,-0.031,0.0472,0.0495,-0.073,-0.0049,-0.0395,0.0831,-0.018,-0.0547,-0.0083,-0.0217,-0.032,-0.0118,0.0582,-0.034,-0.0009,0.0559,0.0293,0.0225,-0.0232,-0.0023,-0.0122,0.1009,-0.0338,0.0265,0.035,0.0142,0.0209,0.0621,0.0092,0.0235,-0.0294,0.0079,-0.0317,-0.0845,0.0113,-0.0004,-0.015,-0.27,0.0123,-0.0198,0.0058,-0.0276,0.0193,0.0115,-0.0246,-0.0263,-0.0022,0.0247,0.0955,0.0216,-0.0529,-0.0112,0.0261,0.0455,-0.034,0.0234,-0.0653,0.0371,0.0253,0.1927,-0.0487,-0.0034,0.0306,-0.0002,-0.0017,0.0462,-0.0106,-0.054,0.013,0.0819,-0.0537,0.0535,0.0468,0.0203,0.0038,0.0307,-0.043,-0.0385,0.0049,-0.0445,0.0089,0.0938,0.0312,-0.0596,-0.0836,0.0331,0.0052,-0.0542,0.0324,-0.0833,-0.0352,0.0073,0.0707,-0.0674,-0.0094,-0.0509,-0.0483,0.0024,-0.0516,-0.019,-0.003,0.0157]}
{"key":"[Factored Contextual Policy Search with Bayesian Optimization] Scarce data is a major challenge to scaling robot learning to truly complex tasks, as we need to generalize locally learned policies over different task contexts. Contextual policy search offers data-efficient learning and generalization by explicitly conditioning the policy on a parametric context space. In this paper, we further structure the contextual policy representation. We propose to factor contexts into two components: target contexts that describe the task objectives, e.g. target position for throwing a ball; and environment contexts that characterize the environment, e.g. initial position or mass of the ball. Our key observation is that experience can be directly generalized over target contexts. We show that this can be easily exploited in contextual policy search algorithms. In particular, we apply factorization to a Bayesian optimization approach to contextual policy search both in sampling-based and active learning settings. Our simulation results show faster learning and better generalization in various robotic domains. See our supplementary video: https://youtu.be/MNTbBAOufDY.","layer":7,"vector":[-0.0617,0.0067,0.027,-0.053,0.0252,0.0494,0.0703,0.0563,0.008,0.0108,-0.0111,-0.0576,0.0338,0.0713,0.0095,-0.008,-0.0209,0.0621,-0.0274,-0.0198,0.0461,-0.0636,0.0017,-0.0816,-0.0005,0.0518,-0.041,-0.0287,-0.0326,-0.2244,0.0123,-0.0263,0.031,-0.0285,-0.0082,0.0257,0.0027,0.0444,-0.0154,0.0693,0.0345,0.027,-0.0392,-0.0589,-0.0385,-0.0126,-0.0021,-0.0201,-0.038,-0.0433,0.0184,-0.0456,0.0727,0.0483,0.0494,0.0135,0.0588,0.0497,0.0387,0.046,-0.0161,0.0588,-0.16,0.0681,0.0234,0.0338,-0.0488,-0.0121,0.005,0.0543,-0.0344,0.031,0.0348,0.0755,0.0194,-0.0238,-0.0242,-0.0489,-0.0116,-0.0073,-0.0087,-0.0459,-0.0267,0.0031,-0.0236,-0.0607,-0.009,-0.0417,0.0545,0.055,-0.0567,0.012,-0.0207,0.0043,-0.0903,-0.0115,0.0173,0.0526,-0.0591,0.1996,-0.0128,0.0381,0.0294,0.0294,0.028,-0.0235,-0.0261,0.0035,0.0005,-0.0091,0.011,-0.016,-0.0145,-0.0335,-0.0113,0.0023,0.0798,0.0338,0.002,-0.0359,0.0135,0.005,0.0314,-0.0122,0.0113,-0.097,-0.0014,0.1423,0.0049,0.0028,0.0508,-0.0598,-0.0626,-0.0264,0.0619,0.0231,0.0187,0.0203,0.0248,-0.003,-0.0608,-0.0131,0.0237,-0.0909,-0.0585,0.1329,-0.0213,0.0264,-0.0572,-0.0205,0.0063,0.0348,0.0024,-0.0262,0.0081,0.0407,-0.0018,0.0323,-0.0647,-0.0099,-0.0494,-0.0139,-0.021,0.0776,-0.0018,-0.0851,-0.062,-0.0048,-0.0088,0.0429,0.0409,0.0395,-0.0325,0.0347,0.0978,0.011,-0.0734,0.0196,0.007,-0.0003,0.04,-0.0796,0.0012,-0.027,0.0182,0.0017,0.019,-0.046,0.0109,0.0227,-0.0002,0.005,0.0314,-0.0389,-0.0325,-0.0006,0.0163,-0.0247,0.0182,0.0005,-0.0121,0.01,-0.0595,0.0297,0.0055,0.015,-0.0535,-0.0089,0.0513,0.013,-0.0192,0.0065,0.0218,-0.0462,0.0022,-0.0241,0.0043,0.0276,-0.0151,0.0343,0.0102,0.0148,-0.0271,-0.2361,0.0072,-0.0483,-0.0143,0.0764,-0.0747,-0.0049,-0.0077,0.034,0.0635,0.0389,-0.0825,-0.0246,0.0438,-0.0203,0.0593,0.0215,0.006,-0.0362,0.0079,0.0293,-0.0042,-0.0162,-0.0665,0.0382,-0.0524,0.2384,0.0494,0.0212,-0.0186,0.0298,0.0107,-0.0364,-0.0904,0.0551,0.0021,0.0611,-0.0187,-0.0078,-0.0386,-0.0172,0.0127,-0.0545,-0.0836,-0.042,-0.0388,-0.0328,0.0665,-0.0547,-0.0174,0.0537,-0.0193,0.0269,-0.0638,-0.0649,-0.0383,-0.059,0.0041,-0.0452,0.0485,0.0211,-0.0076,0.0297,-0.0325,0.0371,-0.0284,0.0464,-0.0453,0.0123,-0.0003,-0.0132,0.0346,0.0568,-0.0052,0.0703,-0.0022,0.0373,-0.0158,-0.0469,-0.0447,0.106,-0.037,0.0376,0.0357,0.0137,-0.0256,0.0679,-0.0254,0.0534,-0.0078,0.0081,-0.0014,-0.049,0.0455,0.0673,0.0003,-0.3065,0.0575,0.0543,0.0163,-0.0513,0.0281,0.0928,-0.023,-0.0297,0.008,-0.0095,0.0637,0.037,0.0408,-0.0319,0.018,0.0688,-0.0041,0.0411,-0.0647,0.0226,0.05,0.2268,-0.0558,0.0291,0.0152,-0.0344,-0.0376,0.0524,-0.0287,0.0001,0.0177,0.0653,-0.0667,0.0435,0.0681,-0.0012,0.0343,-0.0158,-0.0152,-0.0409,0.0037,-0.0166,-0.0444,0.0743,0.0096,-0.0136,-0.0498,-0.0414,0.0299,-0.0346,0.0242,-0.0137,-0.0404,0.0448,0.0392,-0.012,-0.0704,-0.024,-0.0495,0.0269,-0.0626,0.0262,-0.0429,-0.0036]}
{"key":"[View Invariant Human Body Detection and Pose Estimation from Multiple Depth Sensors] Point cloud based methods have produced promising results in areas such as 3D object detection in autonomous driving. However, most of the recent point cloud work focuses on single depth sensor data, whereas less work has been done on indoor monitoring applications, such as operation room monitoring in hospitals or indoor surveillance. In these scenarios multiple cameras are often used to tackle occlusion problems. We propose an end-to-end multi-person 3D pose estimation network, Point R-CNN, using multiple point cloud sources. We conduct extensive experiments to simulate challenging real world cases, such as individual camera failures, various target appearances, and complex cluttered scenes with the CMU panoptic dataset and the MVOR operation room dataset. Unlike most of the previous methods that attempt to use multiple sensor information by building complex fusion models, which often lead to poor generalization, we take advantage of the efficiency of concatenating point clouds to fuse the information at the input level. In the meantime, we show our end-to-end network greatly outperforms cascaded state-of-the-art models.","layer":0,"vector":[-0.0307,-0.0247,0.0268,-0.0256,0.0494,0.0582,0.0369,0.0085,0.0122,0.0235,0.022,-0.0634,-0.0045,0.0693,0.0247,-0.0078,-0.0022,0.0244,-0.0421,0.0,-0.0129,-0.0442,-0.0112,-0.018,0.0068,0.0247,-0.0273,-0.0561,-0.0593,-0.2153,-0.0392,-0.0312,0.0329,-0.0122,-0.021,-0.0411,0.0023,0.0608,-0.0229,0.0159,0.0119,0.0102,-0.0476,-0.0581,0.0015,-0.0164,-0.0111,0.0389,0.0127,-0.0222,0.0706,-0.0237,0.0416,0.0514,0.0168,0.008,0.0473,0.0236,0.0287,0.0189,0.0521,0.0704,-0.1883,0.0005,0.0444,0.0406,0.0005,-0.0514,0.012,0.017,-0.0089,0.0099,0.0286,0.0487,-0.0345,-0.0081,0.0086,-0.0363,-0.03,-0.0212,0.0212,-0.0142,-0.0138,0.0027,-0.0043,-0.0475,-0.0005,-0.0516,-0.0191,0.0218,-0.0901,0.0008,-0.035,0.0217,-0.0445,-0.016,0.029,0.009,-0.0484,0.1996,-0.0305,0.0073,0.0399,-0.0202,0.0241,-0.0244,-0.0308,-0.0318,0.0113,0.0396,-0.0009,-0.0298,0.0218,-0.019,0.0538,0.0152,0.0922,0.0464,0.004,-0.0372,0.0214,-0.0067,0.1051,-0.0311,0.0088,-0.0723,0.0549,0.172,0.0267,0.0209,0.0766,0.0082,-0.0606,-0.0692,0.0258,0.0465,0.0213,0.028,0.0582,0.0113,-0.0101,-0.0447,0.0487,-0.0836,-0.0077,0.1312,-0.0733,0.0164,-0.0529,-0.0531,-0.004,0.0723,-0.0293,0.0205,0.0089,0.027,0.0479,0.033,-0.0699,0.0349,-0.0102,-0.0554,-0.031,0.0545,-0.006,-0.1302,0.0051,0.0378,-0.0102,-0.0013,0.046,0.038,0.0029,-0.0149,0.0964,0.0551,-0.1302,0.007,0.0184,0.027,-0.0362,-0.0601,-0.0132,-0.0054,0.0358,-0.0238,0.0262,-0.0319,-0.0127,0.0609,-0.0425,0.0372,-0.0345,-0.0047,-0.0029,-0.0292,0.0042,0.0032,0.0025,-0.0201,0.0078,-0.0165,-0.0334,0.0088,-0.025,-0.0276,-0.0096,0.0391,0.0148,0.0418,-0.036,0.0104,0.0388,-0.0399,-0.0115,-0.0007,0.0226,-0.0034,-0.05,-0.002,-0.0121,-0.0442,-0.0281,-0.2159,0.0259,0.0075,-0.0429,0.0163,-0.0557,0.0171,0.0566,0.02,0.0449,0.1073,-0.0384,0.019,0.021,0.0419,0.0524,0.0259,0.0588,-0.059,-0.0246,0.0128,0.0296,-0.0305,-0.0914,0.0817,-0.0062,0.2426,0.0235,0.049,0.0381,0.0484,0.041,-0.0441,-0.0911,0.0575,0.0259,0.074,0.0108,-0.0606,-0.0459,-0.0813,0.0182,0.0171,-0.0964,-0.0182,-0.023,-0.0621,0.0635,-0.0271,-0.0078,0.0181,-0.0626,0.0307,-0.0276,-0.0514,-0.0319,-0.0889,0.0042,-0.044,0.0157,-0.0193,-0.0443,0.0378,-0.0645,0.0732,-0.02,-0.0375,-0.0917,0.0077,-0.0424,-0.0304,0.0539,0.0029,0.0339,0.0584,0.009,0.0523,-0.0294,-0.0193,-0.0627,0.05,-0.0224,-0.0292,0.0413,0.0631,0.0045,0.0568,-0.0079,0.0144,-0.0434,0.0272,0.0256,-0.0266,-0.0358,0.047,0.0135,-0.3149,0.0157,-0.0212,0.0282,-0.0131,0.0291,-0.0024,-0.0056,-0.0125,-0.0049,-0.0055,0.0569,0.0119,0.0088,0.0456,0.0398,0.0624,-0.0172,0.0111,-0.0497,-0.0335,0.0584,0.1675,-0.0263,0.0151,0.0661,-0.0188,-0.0177,0.0235,0.0039,-0.0125,0.0225,0.0221,-0.0728,0.0078,0.0944,0.0071,0.028,0.0318,0.0289,0.0046,-0.0009,0.026,-0.0753,0.0796,0.0306,-0.0514,-0.0107,0.0347,-0.0006,-0.0415,0.0028,-0.0264,0.0113,0.054,0.0157,-0.0744,-0.0668,-0.0548,-0.0075,0.0274,-0.0469,-0.0283,-0.0351,-0.0082]}
{"key":"[Unfolding Latent Tree Structures using 4th Order Tensors] Discovering the latent structure from many observed variables is an important yet challenging learning task. Existing approaches for discovering latent structures often require the unknown number of hidden states as an input. In this paper, we propose a quartet based approach which is \\emph{agnostic} to this number. The key contribution is a novel rank characterization of the tensor associated with the marginal distribution of a quartet. This characterization allows us to design a \\emph{nuclear norm} based test for resolving quartet relations. We then use the quartet test as a subroutine in a divide-and-conquer algorithm for recovering the latent tree structure. Under mild conditions, the algorithm is consistent and its error probability decays exponentially with increasing sample size. We demonstrate that the proposed approach compares favorably to alternatives. In a real world stock dataset, it also discovers meaningful groupings of variables, and produces a model that fits the data better.","layer":2,"vector":[-0.0225,-0.0304,0.0012,0.0073,0.0673,0.0153,0.0283,-0.0136,0.0395,-0.0424,0.0369,-0.0133,0.0428,0.0446,0.0415,0.0164,-0.0216,0.0538,-0.0696,0.0297,0.0132,-0.0679,0.0045,-0.0138,0.0407,0.0336,-0.021,-0.0666,-0.0605,-0.2715,0.0115,-0.0377,0.0642,-0.0402,0.0243,-0.0133,-0.0621,0.0163,-0.039,0.0393,0.024,0.0144,-0.0228,-0.0348,-0.0218,-0.0597,0.0143,-0.0022,-0.0231,-0.0193,-0.0103,-0.0823,0.0221,0.0176,0.0628,0.0419,0.0475,0.0332,0.0064,0.0509,0.0656,0.0369,-0.1523,0.0196,0.0811,0.0478,-0.026,0.0035,-0.0002,0.0678,-0.0234,0.0387,0.0047,-0.01,0.0089,0.0107,0.0523,-0.0478,-0.0086,0.0218,-0.0002,0.0096,-0.0413,-0.028,-0.0045,-0.042,0.0252,-0.0231,0.0448,0.0232,-0.0154,-0.022,-0.0445,0.0471,-0.0453,-0.0603,0.0345,0.0268,-0.0117,0.1983,-0.0614,0.0435,0.0518,-0.0482,-0.0269,-0.0182,-0.0279,-0.0156,0.0003,-0.0003,0.007,-0.0112,0.0613,-0.0847,0.0253,-0.0387,0.0543,0.0464,-0.0239,0.0042,-0.0291,0.0042,0.0539,-0.0335,0.0064,-0.086,-0.0285,0.1206,0.0457,0.0665,0.0383,-0.0259,-0.0296,-0.0428,-0.0018,0.0281,0.0179,0.0099,0.0169,-0.002,-0.0559,-0.0569,0.0567,-0.0722,-0.0625,0.1596,-0.0044,0.0172,-0.052,0.0224,-0.0436,0.0093,-0.0329,-0.0643,0.0355,0.0439,0.0107,0.0206,-0.034,0.0169,-0.0284,-0.0616,-0.0364,0.1027,0.022,-0.1011,-0.026,0.037,0.0362,-0.0125,0.0782,0.0484,-0.0435,0.0556,0.0562,-0.0194,-0.0264,0.001,0.0383,0.0426,0.0273,-0.0708,-0.0316,0.066,0.0665,-0.0072,-0.0072,-0.0365,0.0221,0.0022,-0.0191,0.0226,-0.0559,0.0146,-0.0826,-0.0114,-0.01,-0.0206,-0.0156,0.0085,0.014,-0.0208,-0.0536,0.0255,-0.0482,0.0745,0.008,0.0003,0.0395,0.0606,0.0063,0.0118,0.0505,-0.0079,-0.0144,-0.0101,0.0269,0.0399,-0.0218,0.0631,0.0287,-0.0793,-0.0509,-0.2157,-0.0146,-0.0018,-0.0223,0.0692,-0.0335,0.0241,-0.0218,0.0352,0.0402,0.0074,0.018,-0.0284,0.0034,-0.0258,0.0613,0.0356,0.0066,-0.0116,0.0214,-0.0213,0.0375,-0.0216,-0.0618,0.0453,-0.0088,0.2241,0.0424,-0.0029,-0.0018,0.008,0.0541,-0.0543,-0.0555,0.0765,0.0207,0.0533,-0.017,-0.0511,-0.0145,-0.0457,-0.0106,-0.0003,-0.0529,-0.0383,-0.0465,-0.0368,0.0516,-0.0031,0.0176,0.0651,-0.0749,0.06,-0.0048,-0.016,-0.0737,-0.096,-0.001,-0.024,0.0129,0.0284,-0.0346,-0.0079,-0.0636,0.0336,-0.046,-0.0469,0.0002,0.0043,-0.0494,-0.0244,0.0048,-0.0199,-0.0226,0.0494,0.0146,0.0357,-0.0441,-0.0143,0.0042,0.0865,-0.047,0.0274,-0.0249,0.035,0.0245,0.0878,0.0001,0.0752,-0.0233,-0.0032,0.0254,-0.0375,-0.0124,0.0575,-0.0063,-0.2767,0.0264,0.0053,-0.0009,-0.0145,-0.0251,0.0379,0.0036,-0.0513,-0.0171,0.0284,0.061,0.0368,-0.0626,-0.0216,0.0593,0.0563,-0.0575,0.0376,-0.0337,-0.0068,0.0315,0.2408,-0.0224,0.0265,0.0219,-0.0456,-0.0156,0.0503,-0.0061,0.0112,0.0335,0.0959,-0.0494,0.025,0.0479,-0.0066,0.0353,0.0094,-0.0395,0.0064,-0.0219,-0.0476,-0.0346,0.1175,-0.0378,0.0055,-0.0553,0.0376,0.0296,-0.014,0.0125,-0.0344,0.0047,0.0153,0.0179,-0.0151,-0.049,-0.019,-0.0528,-0.0893,-0.0808,-0.0008,0.0122,0.0009]}
{"key":"[Memory Efficient Invertible Neural Networks for 3D Photoacoustic Imaging] Photoacoustic imaging (PAI) can image high-resolution structures of clinical interest such as vascularity in cancerous tumor monitoring. When imaging human subjects, geometric restrictions force limited-view data retrieval causing imaging artifacts. Iterative physical model based approaches reduce artifacts but require prohibitively time consuming PDE solves. Machine learning (ML) has accelerated PAI by combining physical models and learned networks. However, the depth and overall power of ML methods is limited by memory intensive training. We propose using invertible neural networks (INNs) to alleviate memory pressure. We demonstrate INNs can image 3D photoacoustic volumes in the setting of limited-view, noisy, and subsampled data. The frugal constant memory usage of INNs enables us to train an arbitrary depth of learned layers on a consumer GPU with 16GB RAM.","layer":2,"vector":[-0.0155,-0.0217,0.0264,0.0215,0.0411,0.0371,-0.0028,0.0087,-0.0156,0.0107,0.014,-0.0697,0.0593,0.0448,0.0245,0.0269,0.0138,0.032,-0.0252,0.0241,0.0272,-0.0275,-0.0057,-0.0455,0.0053,-0.0051,-0.0448,-0.0497,-0.0653,-0.2427,0.07,-0.0392,0.0324,-0.0398,0.0215,-0.0365,-0.0249,0.0349,-0.0367,0.0633,0.0409,0.0379,-0.0235,-0.0709,-0.0072,-0.0697,-0.0198,-0.0319,0.0419,-0.0408,0.0183,-0.0583,0.0249,0.0053,0.0327,0.035,0.0338,0.0676,0.0265,0.0159,0.0187,0.0545,-0.1711,0.0844,0.0364,0.0122,0.0059,-0.0396,0.0275,0.0084,-0.0376,0.0321,0.0268,0.052,0.0019,-0.0013,0.009,-0.0356,-0.0238,-0.0114,0.0593,-0.0115,-0.0219,-0.032,-0.0082,-0.0495,0.0042,-0.0707,-0.0307,0.0027,-0.0784,-0.033,-0.039,0.0694,-0.0682,0.0077,0.0447,0.0797,-0.0428,0.2018,-0.0259,0.0111,0.0282,-0.0548,0.0096,-0.0196,-0.0452,0.002,-0.0085,0.0029,0.0072,-0.0236,0.0449,0.0055,0.0228,0.0243,0.0076,0.0217,-0.0091,0.0001,-0.0394,-0.0175,-0.0011,0.0224,0.0477,-0.0889,0.0239,0.0978,0.0563,0.0502,0.0503,-0.0084,-0.0669,-0.0316,-0.0016,-0.0097,0.0351,-0.0081,0.0032,0.027,-0.0256,-0.0876,0.0296,-0.045,-0.0536,0.1446,-0.0472,0.0245,-0.0604,-0.0285,-0.004,0.0413,-0.0223,-0.009,0.0452,0.0234,-0.058,0.0281,-0.0497,0.0514,-0.0238,-0.0501,-0.0539,0.1061,-0.0003,-0.082,-0.038,0.0098,-0.0059,-0.0226,-0.0132,0.0278,-0.0441,0.0397,0.0769,0.0092,-0.0781,-0.0514,0.0044,0.0347,0.0124,-0.0451,-0.0327,-0.0052,0.0488,-0.0587,0.0226,-0.0048,-0.0098,0.0816,-0.029,0.0368,-0.0544,0.006,-0.0638,-0.0164,0.0214,-0.0052,-0.0005,-0.0296,0.0193,-0.0539,-0.0416,0.0647,0.0295,-0.003,0.0332,0.0343,0.0155,0.0274,-0.0402,0.0004,0.0849,-0.0292,-0.0752,0.0018,0.0177,0.0541,-0.0347,0.0771,0.0469,-0.0792,-0.0769,-0.1885,0.0249,0.0135,-0.0088,0.0511,-0.0665,0.0532,-0.0057,0.0859,0.0359,0.0035,0.0182,0.0089,0.0165,-0.0172,0.0208,0.0475,-0.0075,0.003,-0.0404,-0.0136,-0.009,-0.0315,-0.0919,0.1026,0.0085,0.2638,-0.0213,0.0335,-0.0097,0.0199,-0.023,-0.084,-0.1018,0.0544,0.0334,0.0697,0.0113,-0.0714,0.0047,-0.0493,0.0297,-0.0082,-0.0776,-0.0137,-0.022,-0.0205,0.0593,-0.0445,0.0021,0.063,-0.0573,0.0495,0.0131,0.0622,-0.0597,-0.1096,0.0184,-0.0502,-0.0071,0.0165,-0.0536,0.0045,-0.0896,0.0398,-0.027,-0.0026,-0.0617,0.0037,-0.0419,-0.0137,0.0799,-0.0027,-0.0067,0.0669,0.0108,0.066,-0.0436,-0.0526,-0.0384,0.0662,0.0041,0.0333,0.0406,0.0087,0.0528,0.0762,-0.0039,-0.0182,-0.0379,-0.0215,0.0671,-0.0294,-0.0213,-0.0004,-0.0179,-0.2749,0.004,-0.0169,0.0353,-0.0143,0.0219,0.035,0.0021,-0.0316,-0.0087,-0.0328,0.0232,0.0412,0.0273,0.0153,0.042,0.0941,-0.0162,0.0658,-0.0384,-0.0102,0.0515,0.1999,-0.0613,-0.0023,0.0166,-0.0471,0.0042,0.009,-0.0226,0.0081,-0.0376,0.0594,-0.0524,0.0506,0.0972,-0.0295,0.0289,0.0164,0.0205,0.0073,0.0001,-0.0307,-0.0338,0.0436,0.0153,0.0106,-0.0204,-0.0216,0.0123,0.025,0.0174,0.0158,0.0036,0.0384,0.0425,-0.0272,-0.0429,0.014,-0.0276,0.0512,-0.0327,-0.049,0.0319,0.0045]}
{"key":"[Sparsely Activated Networks] Previous literature on unsupervised learning focused on designing structural priors with the aim of learning meaningful features. However, this was done without considering the description length of the learned representations which is a direct and unbiased measure of the model complexity. In this paper, first we introduce the $\\varphi$ metric that evaluates unsupervised models based on their reconstruction accuracy and the degree of compression of their internal representations. We then present and define two activation functions (Identity, ReLU) as base of reference and three sparse activation functions (top-k absolutes, Extrema-Pool indices, Extrema) as candidate structures that minimize the previously defined $\\varphi$. We lastly present Sparsely Activated Networks (SANs) that consist of kernels with shared weights that, during encoding, are convolved with the input and then passed through a sparse activation function. During decoding, the same weights are convolved with the sparse activation map and subsequently the partial reconstructions from each weight are summed to reconstruct the input. We compare SANs using the five previously defined activation functions on a variety of datasets (Physionet, UCI-epilepsy, MNIST, FMNIST) and show that models that are selected using $\\varphi$ have small description representation length and consist of interpretable kernels.","layer":3,"vector":[-0.0138,-0.0288,0.0189,0.0195,0.0133,0.0403,0.044,0.0471,0.0495,-0.0553,0.0039,-0.0669,0.0376,0.0419,0.0231,0.0065,-0.0101,0.058,-0.0601,0.0329,0.0499,-0.0642,-0.0092,0.0017,0.0407,-0.0148,0.0088,-0.0426,-0.0475,-0.266,0.0253,-0.0464,0.1126,-0.0394,0.0119,-0.0182,-0.0259,0.0222,-0.0399,0.0415,0.0716,0.0154,-0.0259,-0.0499,-0.0234,-0.0968,-0.0231,-0.0247,0.0104,-0.0331,0.0461,-0.008,0.0461,0.0264,0.0612,-0.0212,0.0223,0.0233,0.0532,0.0502,-0.0053,0.0686,-0.1419,0.0435,0.0462,0.0535,-0.0753,-0.0348,0.0208,0.0521,-0.0126,0.0633,0.0113,0.0376,-0.0089,-0.006,-0.0255,-0.05,-0.0329,0.0078,0.0456,0.0012,-0.0323,0.0141,0.0059,-0.0306,0.029,-0.0716,0.0259,0.0055,-0.0426,-0.0072,-0.0085,0.0355,-0.0529,-0.0314,0.0071,0.0473,-0.0647,0.2151,-0.0351,0.0467,0.0473,-0.0062,0.0155,-0.0469,-0.0427,0.0002,-0.0207,-0.008,-0.0418,-0.046,0.0058,-0.0729,0.0443,0.0124,0.0545,0.0066,-0.0116,0.022,0.0086,0.0341,0.0242,-0.0564,-0.0036,-0.0657,0.0045,0.1127,0.0698,0.0418,0.0555,-0.0226,-0.0022,-0.0122,0.0385,0.0209,0.0312,0.0023,-0.0012,-0.0122,-0.0129,-0.0155,0.0432,-0.1039,-0.0627,0.1178,-0.0652,0.0087,-0.0346,0.0221,-0.0318,0.0209,-0.0353,-0.042,0.0113,0.0151,-0.0014,0.0122,-0.0661,0.0302,-0.0135,-0.1053,-0.0258,0.0963,0.0243,-0.0585,-0.0307,0.0276,0.01,-0.062,0.0461,0.0133,0.0003,0.0566,0.0715,0.0518,-0.0626,-0.0015,-0.0177,0.0285,-0.0026,-0.0786,-0.0172,0.028,0.0195,-0.0053,-0.0099,-0.0293,0.0185,0.0329,-0.0057,-0.0101,-0.0286,-0.0128,-0.0345,-0.035,0.0309,-0.0037,-0.0077,-0.0246,0.0212,-0.0481,-0.0372,0.0113,-0.008,0.0368,-0.0308,0.0315,0.0476,0.0408,-0.0487,0.0004,0.0497,-0.0464,-0.0181,-0.002,-0.0117,0.0373,-0.007,0.0287,0.009,-0.0895,-0.066,-0.2282,-0.0115,0.012,-0.0213,0.0597,-0.0851,0.0515,-0.0043,0.0458,0.0435,0.0249,0.0079,-0.0241,0.0242,-0.0114,0.069,0.0229,0.0184,-0.0248,-0.0158,0.0031,0.0281,0.0142,-0.0411,0.0739,0.0183,0.2077,0.0171,0.0327,-0.0338,0.0006,0.0538,-0.0368,-0.0707,0.0621,0.0318,0.0665,0.0053,-0.0125,-0.033,-0.0633,-0.0234,-0.0271,-0.0905,-0.0334,0.02,-0.0119,0.0271,-0.0541,0.0157,-0.0018,-0.0068,0.0833,-0.0174,0.0088,-0.0422,-0.1256,-0.0053,-0.0173,0.0266,0.0172,-0.0726,-0.0612,-0.1052,0.0419,0.0005,-0.0411,-0.0138,0.0485,-0.0387,-0.0401,0.0689,-0.0215,0.0062,0.0329,0.021,-0.0134,-0.0387,-0.0339,-0.0085,0.0818,-0.0063,0.0123,0.0153,0.0505,0.0311,0.1216,-0.0155,-0.0031,-0.0325,-0.0016,0.0258,-0.0461,-0.0391,0.031,-0.0273,-0.2952,0.011,0.034,0.0229,-0.0082,0.0147,0.0864,-0.0072,-0.0544,-0.004,0.0216,-0.0005,0.0433,0.0256,-0.0294,0.0429,0.0812,-0.0743,0.0475,-0.0757,-0.0009,0.0508,0.1962,-0.0746,0.028,0.0366,-0.0148,0.0183,-0.0022,-0.0206,0.0441,0.0222,0.0551,-0.053,0.004,0.0638,-0.0486,0.0775,0.054,-0.0318,0.0436,-0.0137,-0.0633,-0.0198,0.0951,-0.0353,0.0087,-0.0272,-0.0264,0.0542,0.0147,0.0162,0.0136,-0.0069,0.0395,0.0347,-0.0209,-0.0299,-0.0221,-0.0299,-0.0193,-0.0322,0.01,0.0139,-0.0355]}
{"key":"[Automated Text Summarization for the Enhancement of Public Services] Natural language processing and machine learning algorithms have been shown to be effective in a variety of applications. In this work, we contribute to the area of AI adoption in the public sector. We present an automated system that was used to process textual information, generate important keywords, and automatically summarize key elements of the Meadville community statements. We also describe the process of collaboration with My Meadville administrators during the development of our system. My Meadville, a community initiative, supported by the city of Meadville conducted a large number of interviews with the residents of Meadville during the community events and transcribed these interviews into textual data files. Their goal was to uncover the issues of importance to the Meadville residents in an attempt to enhance public services. Our AI system cleans and pre-processes the interview data, then using machine learning algorithms it finds important keywords and key excerpts from each interview. It also provides searching functionality to find excerpts from relevant interviews based on specific keywords. Our automated system allowed the city to save over 300 hours of human labor that would have taken to read all interviews and highlight important points. Our findings are being used by My Meadville initiative to locate important information from the collected data set for ongoing community enhancement projects, to highlight relevant community assets, and to assist in identifying the steps to be taken based on the concerns and areas of improvement identified by the community members.","layer":0,"vector":[-0.0423,0.0179,0.0293,-0.0196,0.0637,0.0258,0.0544,0.0455,-0.0144,-0.0367,-0.0302,-0.0381,0.0209,0.0009,0.0459,-0.001,-0.0014,0.0191,-0.0326,0.0295,0.025,0.0023,-0.0402,-0.0526,0.0332,-0.0123,-0.0633,-0.0455,-0.0532,-0.1853,-0.0041,-0.0589,0.1022,-0.0232,-0.0001,0.0165,0.0191,0.0575,0.0035,0.023,0.0601,-0.0349,-0.0183,-0.0372,-0.0153,-0.0605,0.0018,-0.0204,-0.0088,-0.065,0.0296,-0.0391,0.0047,0.0236,0.0316,0.0625,0.0401,0.0481,0.0531,0.0471,0.0113,0.0447,-0.23,0.0996,0.0297,0.0311,-0.034,-0.0251,0.0043,0.0237,-0.0348,0.0571,0.0308,0.0699,-0.0053,0.0371,-0.0169,-0.0392,0.013,0.0135,-0.001,-0.0333,-0.0541,0.0134,-0.0589,-0.0003,0.0175,-0.033,0.0421,-0.0158,-0.0017,-0.0071,0.005,0.0239,-0.0615,-0.0314,0.0213,-0.0184,-0.0044,0.1954,-0.0663,0.0111,0.0288,-0.0907,0.0416,-0.0331,-0.0266,-0.0348,-0.0157,-0.0135,0.0033,-0.0202,0.0104,0.0061,0.0816,0.0175,0.0799,0.0154,-0.0447,0.016,0.0038,0.0309,-0.0009,0.0149,0.008,-0.0459,0.0766,0.1059,0.0437,0.0267,0.0383,-0.003,-0.0538,-0.0229,0.0075,0.0111,0.0028,0.0091,0.0066,0.0116,-0.0272,-0.0402,-0.0466,-0.0851,-0.0644,0.187,-0.0507,-0.0263,-0.0155,-0.0126,-0.0305,0.0165,-0.0497,-0.0127,0.0107,0.0289,0.0812,0.0542,-0.0674,0.03,0.0445,-0.0583,-0.0383,0.1028,0.0278,-0.1073,-0.0591,-0.0266,0.0007,-0.0095,0.0577,0.0603,-0.041,0.0823,0.0432,-0.0148,-0.0283,0.0051,0.0476,0.048,0.0166,-0.0356,-0.047,0.0679,0.032,-0.0433,-0.0413,-0.0936,-0.0019,0.0153,-0.0289,0.0307,-0.0009,-0.0395,-0.0215,-0.0254,0.0273,-0.0276,0.002,-0.0239,-0.0155,0.0283,-0.0549,0.003,0.0254,0.016,-0.03,0.0094,0.0535,-0.0121,-0.0093,0.0005,0.1024,0.0102,-0.0266,0.0276,0.0204,0.0139,0.0084,0.0553,-0.0117,0.0015,-0.0639,-0.2056,0.0133,0.0153,-0.012,-0.0013,-0.0764,0.0388,-0.0492,-0.0258,0.0802,0.0664,-0.0602,-0.0097,0.0541,-0.0134,0.0474,0.0096,0.0274,-0.0029,0.0109,0.024,0.0099,0.0107,-0.1141,0.0265,-0.027,0.2091,0.0135,0.0039,-0.037,0.026,-0.0218,-0.0593,-0.1497,0.0619,0.0101,0.0413,0.0436,-0.0806,0.003,-0.0136,0.0509,-0.0501,-0.0635,0.0068,-0.0487,-0.0056,-0.0168,-0.0367,-0.0055,0.0061,-0.0021,0.0248,0.0214,0.005,-0.0418,-0.1052,0.0576,0.004,-0.0152,-0.0001,-0.0277,0.0239,-0.0458,0.0773,0.021,-0.0168,0.0034,-0.0089,-0.0115,-0.0423,0.0709,-0.022,-0.0559,0.0249,0.0166,0.0114,-0.0438,-0.0328,-0.0552,0.0633,-0.0415,0.0262,0.0077,0.0193,0.0223,0.0491,-0.0018,0.0306,0.0152,0.0225,0.0318,-0.0566,-0.0142,0.0331,0.0225,-0.3068,0.0548,0.0247,-0.0093,-0.0167,0.0184,0.0254,0.0398,0.0135,-0.0025,0.0166,0.0439,0.0155,-0.0581,-0.0122,0.0209,0.0532,0.0019,-0.0132,-0.0631,0.0106,0.0266,0.2084,-0.0517,0.0318,-0.0028,-0.0122,-0.0419,0.0381,-0.0537,-0.0061,0.004,0.1045,0.0049,0.0545,0.0444,-0.0387,0.031,0.0037,0.0374,-0.0339,0.0406,-0.0163,-0.0024,0.0831,0.0079,-0.0264,-0.0713,-0.0048,0.0333,-0.0451,-0.0383,-0.032,0.0355,0.0608,0.0303,-0.0558,-0.003,-0.004,-0.0068,-0.0151,-0.1194,0.0343,0.0114,0.0126]}
{"key":"[System-reliability based multi-ensemble of GAN and one-class joint Gaussian distributions for unsupervised real-time structural health monitoring] Unsupervised health monitoring has gained much attention in the last decade as the most practical real-time structural health monitoring (SHM) approach. Among the proposed unsupervised techniques in the literature, there are still obstacles to robust and real-time health monitoring. These barriers include loss of information from dimensionality reduction in feature extraction steps, case-dependency of those steps, lack of a dynamic clustering, and detection results' sensitivity to user-defined parameters. This study introduces an unsupervised real-time SHM method with a mixture of low- and high-dimensional features without a case-dependent extraction scheme. Both features are used to train multi-ensembles of Generative Adversarial Networks (GAN) and one-class joint Gaussian distribution models (1-CG). A novelty detection system of limit-state functions based on GAN and 1-CG models' detection scores is constructed. The Resistance of those limit-state functions (detection thresholds) is tuned to user-defined parameters with the GAN-generated data objects by employing the Monte Carlo histogram sampling through a reliability-based analysis. The tuning makes the method robust to user-defined parameters, which is crucial as there is no rule for selecting those parameters in a real-time SHM. The proposed novelty detection framework is applied to two standard SHM datasets to illustrate its generalizability: Yellow Frame (twenty damage classes) and Z24 Bridge (fifteen damage classes). All different damage categories are identified with low sensitivity to the initial choice of user-defined parameters with both introduced dynamic and static baseline approaches with few or no false alarms.","layer":0,"vector":[-0.0575,-0.0144,0.0603,-0.0257,0.0469,0.0149,0.0671,0.0008,0.0205,0.0031,-0.0094,-0.0327,0.0143,0.1159,-0.025,0.0016,-0.0055,0.0496,-0.0143,-0.0058,0.0293,-0.0044,-0.0021,-0.0046,0.0151,0.0428,-0.0196,-0.0305,-0.0438,-0.2418,0.031,-0.0606,0.0691,-0.0541,0.0064,-0.0152,-0.0417,0.0633,0.0134,0.0582,0.0001,0.0254,-0.024,-0.0726,0.023,-0.0109,0.0229,-0.0132,0.0011,-0.0376,0.0388,-0.0075,0.0787,0.0168,0.0201,0.0119,0.045,-0.0025,0.0572,0.0769,0.0143,0.0367,-0.1769,0.0349,0.0654,0.0454,-0.0252,-0.0353,0.0304,0.0418,-0.027,0.0438,0.0063,0.0318,-0.0038,0.0031,0.0141,-0.0643,-0.0546,0.0544,0.0592,-0.0494,-0.0212,0.0152,-0.0184,-0.0254,-0.0108,-0.0398,0.0723,-0.0032,-0.036,0.0113,-0.0337,0.0743,-0.0447,-0.0316,0.014,0.0307,-0.0558,0.2106,-0.1065,0.0014,0.0901,0.006,0.0109,-0.0193,-0.0771,-0.0596,-0.0377,-0.0011,0.0353,-0.0153,0.0122,-0.0387,0.004,-0.0281,0.0024,-0.0134,0.0102,-0.037,-0.0514,-0.004,0.0609,-0.0663,0.052,-0.0524,-0.0006,0.1182,0.0362,0.0284,0.0134,0.0346,-0.0508,0.0128,-0.0165,0.0373,0.0296,0.0294,0.0233,0.0049,-0.0119,-0.041,0.0053,-0.0557,-0.024,0.0946,-0.0771,0.0192,-0.049,-0.0468,-0.0404,-0.0259,-0.0396,-0.0424,0.0191,-0.0013,-0.0106,0.02,-0.0443,0.0185,-0.036,-0.0712,-0.0308,0.0777,0.005,-0.1166,0.0165,0.009,-0.0157,-0.0329,0.0077,0.0606,-0.0129,0.0351,0.0972,0.0532,-0.063,-0.0342,-0.0149,0.0103,0.0224,-0.0401,-0.0219,0.0244,0.0602,-0.0522,0.0093,-0.0391,0.013,0.0339,-0.0503,-0.0037,-0.01,-0.0038,-0.0443,0.0074,-0.0108,-0.0008,0.0273,0.002,0.0167,-0.0041,-0.0564,0.0112,-0.0154,0.0346,-0.0319,0.0057,0.0346,0.0401,0.0126,0.0075,0.0852,-0.0231,-0.0065,0.0024,-0.0161,0.0363,-0.0316,0.0283,0.0186,-0.019,-0.0764,-0.263,0.0235,0.0283,-0.0019,0.0271,-0.0436,0.011,-0.0253,0.0627,0.0416,0.0724,0.0446,-0.0362,-0.0334,0.0027,0.0585,0.0249,0.0268,-0.0562,0.0013,0.0053,0.0321,-0.0141,-0.087,0.0618,-0.0348,0.1852,0.0012,0.02,0.0065,0.0468,0.0356,0.0003,-0.0789,0.0731,0.0509,0.0622,-0.0123,-0.0922,-0.0462,-0.0252,0.0369,0.0167,-0.0965,-0.0313,-0.0601,0.0198,0.0407,-0.0866,-0.0089,0.0306,-0.0227,0.0581,-0.0165,0.0263,-0.0393,-0.1207,0.0411,-0.028,-0.0191,0.0026,-0.0765,0.0184,-0.0718,0.0707,-0.0553,-0.0555,-0.0336,-0.0063,-0.0433,-0.028,0.1171,0.0317,0.014,0.0347,0.0225,0.035,-0.0506,-0.0489,-0.0212,0.047,-0.0032,0.0302,0.071,0.0157,-0.007,0.0542,0.0343,0.0189,-0.0073,0.0424,0.0205,0.0129,-0.0206,0.048,-0.0156,-0.2978,0.042,-0.0123,0.0151,-0.0279,-0.0268,0.0288,0.0571,-0.0224,-0.0111,-0.0252,0.0184,0.0556,-0.0673,0.0034,0.0274,0.0371,-0.0506,0.0351,-0.0609,0.032,0.073,0.1882,-0.0475,0.0102,-0.0101,-0.0265,0.0141,-0.0122,-0.0317,-0.0228,0.0346,0.0794,-0.0643,0.0169,0.1094,-0.0208,0.0593,-0.0019,-0.0179,-0.003,0.0092,-0.0299,-0.0142,0.0849,-0.0055,-0.0352,-0.0353,0.0121,0.043,-0.0602,-0.0193,-0.0072,0.0017,0.0349,0.0385,-0.012,-0.0212,0.0038,-0.023,-0.0023,0.0402,-0.0317,-0.0091,-0.0118]}
{"key":"[Tuning Hyperparameters without Grad Students: Scalable and Robust Bayesian Optimisation with Dragonfly] Bayesian Optimisation (BO) refers to a suite of techniques for global optimisation of expensive black box functions, which use introspective Bayesian models of the function to efficiently search for the optimum. While BO has been applied successfully in many applications, modern optimisation tasks usher in new challenges where conventional methods fail spectacularly. In this work, we present Dragonfly, an open source Python library for scalable and robust BO. Dragonfly incorporates multiple recently developed methods that allow BO to be applied in challenging real world settings; these include better methods for handling higher dimensional domains, methods for handling multi-fidelity evaluations when cheap approximations of an expensive function are available, methods for optimising over structured combinatorial spaces, such as the space of neural network architectures, and methods for handling parallel evaluations. Additionally, we develop new methodological improvements in BO for selecting the Bayesian model, selecting the acquisition function, and optimising over complex domains with different variable types and additional constraints. We compare Dragonfly to a suite of other packages and algorithms for global optimisation and demonstrate that when the above methods are integrated, they enable significant improvements in the performance of BO. The Dragonfly library is available at dragonfly.github.io.","layer":0,"vector":[-0.027,-0.0208,0.0212,0.001,0.0456,-0.0104,0.0272,0.0519,0.0415,-0.0098,0.0244,-0.0665,0.0131,0.0749,-0.0003,-0.0097,0.008,0.0072,-0.035,-0.0291,0.0323,-0.0338,-0.0035,-0.0509,0.0444,0.0058,-0.0322,-0.0088,-0.0432,-0.2837,0.0331,-0.054,0.0363,-0.0301,-0.0103,0.0031,-0.0095,0.0411,-0.0013,0.0473,0.02,0.042,-0.0239,-0.0519,-0.0225,-0.0314,-0.0138,-0.0125,-0.0462,-0.031,-0.0015,-0.0382,0.0236,0.0355,0.0544,0.0241,0.0491,0.053,0.0449,0.0514,-0.0116,0.0531,-0.1333,0.0561,0.0295,0.005,-0.0219,-0.0526,0.0167,0.066,-0.0474,0.0349,0.0375,0.0566,0.0271,0.01,-0.0158,-0.0227,0.0011,0.0128,0.0191,-0.0471,-0.0195,-0.0131,0.0054,-0.0108,0.0005,-0.0195,0.0224,0.0108,-0.0267,-0.0031,-0.0534,-0.0197,-0.0772,-0.0024,0.0501,0.0138,-0.0579,0.2298,-0.015,0.0369,0.0102,-0.0079,0.0358,-0.0394,-0.0559,-0.0163,-0.04,-0.0112,0.0091,-0.0372,0.017,-0.0306,0.0018,0.065,-0.0082,-0.005,-0.0211,-0.0077,-0.0237,-0.013,0.0812,0.0212,0.0278,-0.0149,0.024,0.143,-0.0036,0.0579,0.0752,-0.0398,-0.0459,-0.0233,0.0291,0.0133,0.0628,-0.0148,0.0067,0.002,-0.0624,-0.0852,-0.0075,-0.0383,-0.0352,0.1262,-0.0388,0.0461,-0.0802,-0.0459,-0.0016,0.0136,-0.006,-0.0529,0.0159,0.0171,0.0026,0.0491,-0.0449,-0.016,-0.058,-0.0198,-0.0563,0.0888,-0.0213,-0.086,-0.0374,-0.0025,-0.0284,0.0267,0.0476,0.0041,-0.0411,0.0432,0.0906,0.0215,-0.0687,0.0062,0.0342,0.0071,0.032,-0.0076,-0.0369,0.0086,0.0411,-0.0647,-0.0083,-0.0551,0.0128,0.0277,-0.0384,-0.0181,0.0162,-0.0317,-0.0398,-0.0267,-0.0265,-0.0349,0.0514,-0.0433,0.0381,0.0099,-0.0537,0.0422,-0.017,0.0388,0.0189,-0.0004,0.0488,0.0561,-0.0315,-0.0058,0.0805,-0.0539,-0.0029,-0.0126,0.0195,0.0578,0.0147,0.0575,0.029,-0.0742,-0.0345,-0.2378,0.0403,0.0297,-0.0276,0.0986,-0.0253,0.0306,0.0039,0.0367,0.0763,0.0332,-0.0197,-0.0394,0.0389,-0.0247,-0.0008,-0.0045,0.0059,-0.042,-0.0,0.0073,-0.0022,-0.0079,-0.0741,0.0277,-0.0421,0.2136,0.0263,0.0641,-0.0163,0.0257,-0.0018,0.0094,-0.0613,0.037,0.0572,0.0694,-0.006,-0.0594,-0.0425,0.0015,0.0503,-0.0216,-0.1229,-0.0785,-0.0446,-0.0207,0.0186,-0.0563,0.0012,0.0368,0.0,0.0554,-0.039,-0.0231,-0.039,-0.1288,0.0247,-0.0389,0.0465,0.0456,-0.0565,0.0047,-0.0337,0.0338,-0.0667,0.0216,-0.0164,0.0409,-0.016,-0.0244,0.0863,-0.0043,0.035,0.0708,0.023,0.0252,0.0102,-0.0402,-0.0266,0.0506,0.0169,0.0128,0.0322,0.017,-0.0307,0.0884,0.0079,0.0293,-0.0099,-0.013,-0.016,-0.0422,0.0269,0.0232,0.0864,-0.2774,0.0129,0.0236,0.0079,-0.0482,0.0235,0.0806,-0.0037,-0.0343,-0.0342,-0.0305,0.0354,0.0367,0.0103,-0.0103,0.0007,0.0523,-0.0642,0.0601,-0.0746,-0.0074,0.0299,0.2219,-0.0376,0.0315,0.0185,-0.0095,0.0116,0.0238,-0.0703,0.0353,-0.0107,0.0686,-0.0678,0.0223,0.1012,-0.0375,0.0244,0.0234,-0.0123,0.0307,0.0019,-0.0501,-0.0353,0.1098,0.0025,-0.0207,-0.055,0.002,0.0235,-0.0134,0.0474,-0.0376,-0.0082,0.0197,0.0054,-0.0324,-0.0186,-0.0469,-0.0395,0.0266,-0.0652,0.0177,0.0159,-0.0376]}
{"key":"[Agent Modeling as Auxiliary Task for Deep Reinforcement Learning] In this paper we explore how actor-critic methods in deep reinforcement learning, in particular Asynchronous Advantage Actor-Critic (A3C), can be extended with agent modeling. Inspired by recent works on representation learning and multiagent deep reinforcement learning, we propose two architectures to perform agent modeling: the first one based on parameter sharing, and the second one based on agent policy features. Both architectures aim to learn other agents' policies as auxiliary tasks, besides the standard actor (policy) and critic (values). We performed experiments in both cooperative and competitive domains. The former is a problem of coordinated multiagent object transportation and the latter is a two-player mini version of the Pommerman game. Our results show that the proposed architectures stabilize learning and outperform the standard A3C architecture when learning a best response in terms of expected rewards.","layer":9,"vector":[-0.0716,-0.0091,-0.0017,-0.0265,0.0003,0.0213,0.0498,0.0431,0.0416,0.0128,0.069,-0.0505,0.0337,0.0707,0.0185,-0.0162,-0.0422,0.0416,-0.0087,-0.024,0.0294,-0.0597,-0.039,-0.0656,0.0072,-0.0266,-0.0428,-0.0626,-0.0052,-0.2317,0.0216,-0.0216,-0.0217,-0.0341,-0.012,-0.0424,-0.0387,0.0202,-0.0327,0.008,0.0273,-0.0015,-0.0211,-0.0581,-0.0244,-0.0583,0.0019,-0.0396,-0.0187,-0.0335,0.051,-0.0236,0.0231,0.0312,0.0528,0.0217,0.0603,0.1056,0.0459,0.0011,0.0127,0.0113,-0.1848,0.0694,0.0219,0.0812,-0.0422,-0.0024,0.0383,0.0464,-0.0096,0.0237,0.0138,0.014,0.0154,0.0336,-0.0028,-0.0206,0.0057,-0.0196,-0.0046,-0.044,-0.0756,-0.0183,0.0179,-0.1127,0.0004,-0.0248,0.0195,0.0076,-0.0364,-0.0076,-0.0005,0.0167,-0.0234,-0.0153,-0.0079,-0.0065,-0.0839,0.2151,0.0007,0.0226,0.0458,-0.0022,0.0221,-0.0405,0.0034,-0.0223,-0.0116,0.0262,-0.0484,0.0134,0.0175,-0.01,0.0117,0.0488,0.0722,0.0068,-0.0094,-0.018,-0.0195,0.0167,0.0298,-0.018,0.0135,-0.0609,0.0018,0.1473,0.0335,0.0452,0.0473,-0.0241,0.0047,-0.0213,0.0148,0.0156,-0.0069,-0.015,-0.0123,-0.0101,-0.0487,0.0072,-0.0208,-0.1146,-0.0518,0.1032,0.0384,0.0162,-0.0356,-0.0236,-0.0422,-0.0274,0.0217,-0.0285,-0.0066,0.0031,0.027,0.0655,-0.0712,0.01,-0.0187,-0.0303,-0.0447,0.0763,-0.0088,-0.0704,-0.0329,-0.0147,0.0241,-0.016,0.0324,0.0241,-0.053,-0.0131,0.063,0.0169,-0.0594,-0.0089,0.0018,-0.0442,0.0213,-0.0897,-0.0283,-0.0057,0.0217,-0.015,-0.0181,-0.0616,-0.0346,0.0244,-0.0203,0.0314,-0.0265,0.0199,0.0035,-0.0347,0.0162,0.0121,0.0029,-0.0238,-0.0051,0.0132,-0.0626,0.0138,-0.0337,0.01,-0.029,0.0011,0.0322,0.005,-0.0568,-0.0184,0.0567,0.015,-0.0637,0.0241,0.0255,0.0027,-0.004,0.0252,0.0277,0.0223,-0.0045,-0.197,0.006,-0.0312,-0.0559,0.0428,-0.0306,0.0506,-0.0303,0.0171,0.0645,0.0783,-0.0449,0.0094,0.0289,0.002,0.0493,0.039,0.0774,0.013,0.029,-0.0125,-0.0024,-0.0239,-0.0896,0.0496,-0.0128,0.2332,0.0357,0.033,-0.0156,0.0196,0.0649,-0.0609,-0.0822,0.0464,-0.032,0.0973,-0.0167,0.017,-0.0068,0.0131,0.0495,0.0145,-0.0971,0.0015,-0.0114,-0.0529,0.0788,-0.0799,0.0384,0.0359,-0.0498,-0.0008,0.0025,-0.0565,-0.0287,-0.0612,0.0451,-0.0403,0.0397,0.0053,-0.0331,-0.0203,-0.0107,0.0744,0.0086,0.0269,-0.0931,0.0315,0.0045,-0.0076,0.055,-0.0489,0.0157,0.0194,0.0214,0.0288,0.0011,-0.0266,0.0076,0.0446,-0.0269,0.0103,0.0344,0.0364,-0.0047,0.0263,-0.0194,0.0438,-0.0394,0.0281,0.024,-0.059,-0.0034,0.0487,-0.0331,-0.3055,0.0403,0.0283,0.0557,-0.0468,0.0037,0.0146,0.0197,-0.0684,0.0001,0.0578,0.0888,0.0242,0.0587,0.0236,0.007,0.1277,-0.0241,0.0641,-0.0533,0.0193,0.0642,0.2308,-0.037,0.0785,0.0391,0.0064,-0.0103,-0.0087,-0.0274,0.0052,0.0218,0.1234,-0.0624,0.0555,0.0912,-0.0518,0.039,0.01,0.0095,-0.0225,0.0211,0.0231,-0.0134,0.0845,0.027,-0.0528,-0.045,-0.0391,0.0173,-0.0369,0.002,-0.0455,-0.0552,0.0833,0.0023,-0.0388,-0.0421,-0.0461,-0.0142,-0.0227,-0.0538,0.034,-0.0131,0.0091]}
{"key":"[Dyadic Sex Composition and Task Classification Using fNIRS Hyperscanning Data] Hyperscanning with functional near-infrared spectroscopy (fNIRS) is an emerging neuroimaging application that measures the nuanced neural signatures underlying social interactions. Researchers have assessed the effect of sex and task type (e.g., cooperation versus competition) on inter-brain coherence during human-to-human interactions. However, no work has yet used deep learning-based approaches to extract insights into sex and task-based differences in an fNIRS hyperscanning context. This work proposes a convolutional neural network-based approach to dyadic sex composition and task classification for an extensive hyperscanning dataset with $N = 222$ participants. Inter-brain signal similarity computed using dynamic time warping is used as the input data. The proposed approach achieves a maximum classification accuracy of greater than $80$ percent, thereby providing a new avenue for exploring and understanding complex brain behavior.","layer":3,"vector":[-0.0227,0.0376,-0.0045,-0.0491,0.0413,0.0688,0.081,0.0232,0.0844,-0.0319,0.0182,-0.0862,0.0434,0.0426,0.0087,0.0162,-0.0193,0.0098,-0.0594,0.0271,0.0007,-0.0479,-0.0114,-0.0346,-0.0195,-0.0072,-0.0613,-0.0639,-0.0667,-0.2261,0.0306,-0.0339,0.0725,-0.0196,-0.0028,-0.0101,-0.0225,0.0224,-0.0451,0.0273,0.0092,-0.0175,-0.0315,-0.031,-0.0399,-0.0408,-0.0718,0.0049,-0.003,-0.032,0.0218,-0.0791,0.023,0.0702,0.0365,0.0351,0.1346,0.0208,0.0793,0.0211,-0.0096,0.0063,-0.1762,0.0486,0.0363,0.0243,-0.0486,-0.0274,0.031,-0.0204,-0.031,0.065,0.0211,-0.0165,0.0662,0.0228,-0.0265,-0.0816,0.0077,-0.0077,0.0134,0.0317,-0.0163,0.0067,0.0401,-0.039,0.0031,-0.0195,0.0261,0.031,-0.0753,-0.0154,-0.0606,0.0502,-0.0976,0.01,0.0346,0.0056,-0.0464,0.1915,-0.062,0.0193,0.0283,-0.0401,0.024,-0.0002,-0.0243,-0.0536,-0.0598,0.0245,-0.007,-0.0292,0.0135,-0.0316,0.0139,0.0134,0.1024,0.0402,0.0147,-0.0333,-0.0011,0.0075,0.0724,-0.0096,0.0616,-0.078,0.0595,0.1128,0.0532,0.0251,0.0766,0.0033,-0.0219,-0.0036,0.0011,0.0556,0.0039,-0.0181,-0.0014,0.0076,-0.0185,-0.0378,0.0235,-0.058,-0.0242,0.1162,0.0141,-0.0179,-0.0561,0.0384,-0.0286,0.0134,-0.0472,-0.0144,0.0008,0.0322,0.0231,0.0525,-0.0584,0.0225,-0.0088,-0.0842,-0.0748,0.1115,0.0433,-0.0861,-0.0053,-0.0109,-0.0028,-0.0392,0.0145,-0.0062,-0.0534,0.0494,0.1212,0.0675,-0.0416,0.0211,-0.0008,-0.0086,0.027,-0.0644,-0.068,0.0217,0.0567,-0.0439,0.0247,-0.0375,0.0001,0.0562,-0.0146,0.0016,-0.0339,-0.0048,-0.021,-0.0185,-0.0165,-0.0149,-0.0376,-0.0239,0.0003,-0.0011,-0.0316,0.0604,0.0393,0.0014,0.0096,-0.0047,0.0486,0.0283,0.0026,-0.0014,0.0396,-0.0271,-0.054,-0.0246,0.0264,0.0128,-0.0265,0.0829,0.0489,-0.0535,-0.0466,-0.2066,-0.0356,0.0155,-0.002,0.0416,-0.06,0.0655,0.0064,0.0738,0.0841,0.0499,0.0082,-0.0455,0.0198,0.0257,0.0666,0.0338,0.0268,-0.0106,0.0009,0.0154,0.0251,-0.0238,-0.083,0.0377,0.0074,0.1817,0.013,0.0227,-0.0412,0.0358,0.0329,-0.0395,-0.1222,0.063,0.0136,0.0985,-0.0431,-0.0577,-0.0243,-0.082,0.0021,0.0095,-0.0862,-0.0171,-0.0111,-0.0254,-0.0214,-0.0163,0.0153,0.022,-0.0238,0.015,-0.0313,-0.0289,-0.035,-0.1122,0.0156,-0.046,-0.0135,0.0246,-0.0366,-0.0103,-0.0471,0.0767,0.0276,-0.0634,-0.0219,0.0268,0.0064,0.0025,0.0792,-0.0234,0.0391,0.0861,-0.0005,0.0361,-0.0217,-0.0029,0.0002,0.1022,0.0148,0.0342,0.0311,0.0359,-0.0265,0.0534,-0.0067,0.0602,-0.0624,0.0036,0.0357,-0.0644,-0.057,0.0272,0.004,-0.2725,0.0028,-0.0139,0.0372,-0.07,0.0345,0.0035,0.0219,-0.0499,0.0142,-0.0031,0.0341,0.0051,-0.0422,0.0082,0.0483,0.0494,-0.0101,0.0129,-0.0661,-0.0206,0.0546,0.2067,-0.0379,0.0339,0.0183,-0.0199,-0.0156,0.0359,-0.0498,0.0162,0.0335,0.0673,-0.0414,0.014,0.0818,-0.0159,0.0266,0.0559,-0.003,0.0019,0.0056,-0.0209,-0.0582,0.1224,-0.0159,-0.0432,-0.035,-0.0113,0.0108,-0.0332,0.0075,-0.0125,0.0005,0.0266,0.0035,-0.028,-0.0404,-0.0477,-0.0362,-0.0164,0.0131,-0.0262,-0.0178,-0.0239]}
{"key":"[An Intrusion Response System utilizing Deep Q-Networks and System Partitions] Intrusion Response is a relatively new field of research. Recent approaches for the creation of Intrusion Response Systems (IRSs) use Reinforcement Learning (RL) as a primary technique for the optimal or near-optimal selection of the proper countermeasure to take in order to stop or mitigate an ongoing attack. However, most of them do not consider the fact that systems can change over time or, in other words, that systems exhibit a non-stationary behavior. Furthermore, stateful approaches, such as those based on RL, suffer the curse of dimensionality, due to a state space growing exponentially with the size of the protected system. In this paper, we introduce and develop an IRS software prototype, named irs-partition. It leverages the partitioning of the protected system and Deep Q-Networks to address the curse of dimensionality by supporting a multi-agent formulation. Furthermore, it exploits transfer learning to follow the evolution of non-stationary systems.","layer":0,"vector":[-0.0831,0.0049,0.0239,0.0059,0.0111,0.0289,0.0574,0.0271,0.0297,0.0072,0.0261,-0.0208,0.0458,0.0582,0.0175,-0.0102,-0.0187,0.0224,-0.0159,-0.0074,0.0051,-0.0252,-0.0345,-0.0575,-0.0173,-0.0016,-0.0136,-0.0219,-0.0834,-0.2053,0.035,-0.0269,-0.0159,-0.0312,0.0094,0.0019,-0.0264,0.0214,0.0292,0.0397,0.0439,0.0612,0.0134,-0.0649,-0.0274,-0.0889,-0.0449,-0.0303,0.0003,-0.051,-0.0007,0.0056,0.012,0.0564,0.0337,-0.0088,0.0712,0.0537,0.0514,0.005,0.0263,0.0329,-0.1339,0.0417,0.0341,0.0503,-0.0379,0.0094,0.0642,0.0207,-0.0633,0.0532,-0.0112,0.0255,0.05,0.0026,-0.0273,-0.0247,0.0372,-0.01,0.0212,-0.044,-0.0217,0.0024,-0.0648,-0.0542,0.0294,-0.0318,0.0564,0.0186,-0.0385,-0.0042,-0.0068,0.003,0.0042,-0.0268,0.0072,-0.0345,-0.0639,0.211,-0.0222,0.0251,-0.002,-0.0078,0.0534,-0.0081,-0.0199,-0.0607,-0.0333,0.0259,-0.0178,-0.0713,0.0592,-0.0115,0.0179,-0.0173,0.0404,0.0547,-0.0043,-0.0035,0.0093,-0.0114,0.0744,-0.0552,0.0433,-0.065,-0.0061,0.1594,-0.0417,0.0489,-0.0102,-0.028,-0.0225,-0.046,0.0142,0.0426,0.0073,0.004,0.0089,0.0158,-0.0497,-0.0192,0.0399,-0.099,-0.0365,0.0849,-0.0428,0.0038,-0.0356,-0.0128,-0.0174,-0.0034,-0.0369,-0.0869,-0.0042,0.0278,0.0163,0.0634,-0.0404,-0.0206,-0.0451,-0.0543,-0.055,0.136,0.0255,-0.0658,-0.0446,-0.0047,0.0261,-0.034,0.0161,0.0475,-0.0215,0.0266,0.0458,-0.0012,-0.0744,-0.006,-0.0005,-0.0011,0.0185,-0.0544,-0.0424,0.0387,0.0455,0.0053,0.0156,-0.0279,0.029,0.0147,-0.0248,0.055,-0.0465,0.0422,-0.0712,-0.0217,-0.0195,0.0136,-0.0186,-0.0191,0.0071,-0.011,-0.0652,-0.0013,-0.0225,0.0379,-0.0155,-0.0067,0.0082,0.042,-0.022,0.0077,0.0458,-0.0619,-0.0395,0.016,0.0224,0.0296,0.0177,0.0238,0.0313,0.0107,-0.0533,-0.2478,0.0098,-0.0362,-0.0485,0.0249,-0.0653,0.0502,-0.0296,0.063,0.0736,0.0591,-0.0005,0.0186,0.0227,-0.0232,0.0486,0.0589,0.0607,-0.0287,0.0328,-0.0103,-0.0174,-0.0311,-0.0851,0.023,-0.0093,0.2161,0.0421,0.0356,0.0002,0.0122,0.0348,0.004,-0.1327,0.0874,0.02,0.053,-0.0008,-0.0403,-0.0624,-0.0438,0.0217,-0.023,-0.0893,0.0219,-0.0348,-0.0421,0.0315,-0.0769,-0.0166,0.0157,0.0151,0.0512,0.0168,0.0136,-0.0512,-0.0532,0.0741,-0.0263,0.042,0.0077,-0.0293,-0.0089,-0.0616,0.0691,0.0143,0.0007,-0.0353,0.0436,-0.0275,-0.0591,0.0994,0.0121,0.0107,0.0431,0.0217,0.0096,-0.0185,-0.055,-0.0358,0.0545,-0.0712,0.0189,0.014,0.0423,0.0013,0.0825,0.0492,0.0588,-0.0059,0.0334,-0.0048,-0.0368,-0.0154,0.0214,-0.0129,-0.3116,0.0368,0.0026,0.043,-0.0418,0.0096,0.0715,0.0237,-0.0429,0.02,0.0262,0.0476,0.0563,0.0066,-0.0021,-0.0205,0.0878,-0.0325,0.0082,-0.033,0.0093,0.0696,0.2415,-0.0056,0.0483,0.0068,0.0009,0.003,-0.0202,-0.0343,0.0105,0.0198,0.0685,-0.0787,0.0234,0.0563,-0.0101,0.0358,-0.0099,0.0495,-0.0561,-0.0074,-0.0167,0.0275,0.0951,-0.0169,-0.0153,-0.0637,-0.015,0.0495,-0.0085,0.0194,-0.0293,0.0077,0.0507,-0.0109,-0.0907,-0.0717,-0.0532,-0.0157,0.0243,-0.0417,0.0459,-0.0141,-0.0435]}
{"key":"[Utility-aware Privacy-preserving Data Releasing] In the big data era, more and more cloud-based data-driven applications are developed that leverage individual data to provide certain valuable services (the utilities). On the other hand, since the same set of individual data could be utilized to infer the individual's certain sensitive information, it creates new channels to snoop the individual's privacy. Hence it is of great importance to develop techniques that enable the data owners to release privatized data, that can still be utilized for certain premised intended purpose. Existing data releasing approaches, however, are either privacy-emphasized (no consideration on utility) or utility-driven (no guarantees on privacy). In this work, we propose a two-step perturbation-based utility-aware privacy-preserving data releasing framework. First, certain predefined privacy and utility problems are learned from the public domain data (background knowledge). Later, our approach leverages the learned knowledge to precisely perturb the data owners' data into privatized data that can be successfully utilized for certain intended purpose (learning to succeed), without jeopardizing certain predefined privacy (training to fail). Extensive experiments have been conducted on Human Activity Recognition, Census Income and Bank Marketing datasets to demonstrate the effectiveness and practicality of our framework.","layer":5,"vector":[-0.0326,-0.0055,-0.0155,-0.0861,0.0297,0.0164,0.0451,-0.0264,0.0395,0.0132,0.0072,0.0137,0.0219,0.0526,0.0051,0.0531,0.002,0.0723,-0.0659,0.0423,0.0254,-0.0623,-0.0463,-0.0478,0.0091,0.0266,-0.0503,-0.0201,-0.0438,-0.2022,0.0355,-0.0887,0.0533,0.0035,0.0495,-0.0382,-0.0316,0.0547,-0.0822,0.0273,0.0107,0.0066,-0.033,-0.0217,-0.0334,-0.0124,-0.0225,-0.0117,-0.0641,-0.0185,-0.0006,0.0016,0.003,0.062,0.0422,0.0689,0.0519,0.0282,0.0066,0.0506,0.0199,0.0713,-0.1707,0.0647,0.0393,0.0474,-0.0303,-0.0449,-0.0161,-0.029,-0.0351,0.0709,0.0138,0.0487,0.0045,-0.0043,-0.0117,0.0033,-0.034,0.0238,0.0071,-0.0367,0.0046,0.0239,-0.0364,-0.065,0.0196,-0.0711,0.0485,-0.0251,-0.0762,-0.0008,-0.0013,0.0453,-0.0437,-0.0266,-0.0087,0.0386,-0.053,0.187,-0.0498,0.0381,-0.0021,-0.0364,0.0468,-0.0277,0.002,-0.0426,-0.0366,0.0075,0.0239,-0.0283,0.0317,-0.0421,0.031,0.0288,0.0349,0.0519,-0.0222,-0.0202,0.0284,0.0121,0.068,0.0332,0.0448,-0.0888,0.0611,0.1427,-0.0368,0.0285,0.0462,-0.0453,-0.0542,-0.0363,-0.0052,0.0244,0.0083,0.0168,0.0739,-0.0119,-0.0523,-0.0306,0.0078,-0.1036,-0.0317,0.1253,0.0505,0.0416,0.0023,-0.048,-0.0115,0.0378,-0.0282,-0.0396,0.0541,0.0112,0.039,0.0461,-0.0575,0.0076,-0.0236,-0.0334,-0.0323,0.11,-0.0206,-0.1121,0.0213,-0.0009,0.0206,0.0151,0.0029,0.0662,-0.0486,0.0399,0.0846,0.044,-0.0673,0.0178,-0.0205,0.0084,0.0175,-0.094,-0.0311,0.0012,0.0069,-0.0547,-0.019,-0.0032,0.0373,0.0563,-0.075,-0.0052,-0.0578,-0.0457,-0.0445,-0.0771,-0.0547,-0.0114,0.0124,-0.0165,0.0007,0.0167,-0.0314,0.025,-0.0177,0.0443,0.0085,-0.0351,0.0253,0.0109,-0.0045,0.0074,0.0242,-0.0474,-0.0213,0.0068,0.02,0.0298,-0.0101,0.0695,0.0438,-0.0094,-0.0694,-0.2311,-0.0229,-0.0236,-0.0396,0.0247,-0.044,0.0414,-0.0041,0.029,0.0697,0.0594,-0.0475,-0.0419,0.062,-0.0274,0.0333,0.056,0.0245,-0.0101,-0.0227,0.0014,-0.0118,-0.032,-0.0869,0.0682,0.0018,0.2373,0.0149,0.0418,-0.0183,0.0062,0.0156,-0.039,-0.1402,0.0488,-0.0013,-0.015,0.003,-0.0459,-0.0481,-0.0027,0.0007,0.0112,-0.1081,-0.0208,-0.016,-0.0528,0.047,-0.0512,0.0282,0.0509,-0.0066,0.0848,-0.0273,0.0339,-0.0461,-0.0368,0.0477,-0.0253,0.0345,0.0099,-0.0652,0.0097,-0.0484,0.0692,-0.0085,-0.0531,-0.0421,0.0244,-0.0435,-0.0444,0.0661,0.0099,-0.0055,-0.0081,-0.0016,0.0353,-0.0275,-0.0411,-0.0151,0.0798,-0.0019,0.0164,0.0273,0.0549,0.0055,0.1158,0.0064,0.0444,-0.0587,0.0331,0.0191,-0.0493,-0.0504,0.0805,0.0207,-0.283,0.0106,-0.0252,0.0126,-0.0059,0.0254,0.0481,0.0321,-0.0325,0.0199,-0.0356,0.0721,0.0165,-0.0019,0.0189,0.0013,0.0603,0.0067,-0.0012,-0.0401,0.0872,0.0105,0.2025,0.0103,-0.0048,0.0112,0.0068,0.0152,0.046,-0.0057,-0.0219,-0.0073,0.0788,-0.0427,0.0231,-0.0035,-0.0112,0.0345,0.028,0.0139,-0.0128,0.0059,-0.0473,0.0168,0.0973,0.0293,-0.0377,-0.0584,0.0344,0.0158,0.0099,-0.0042,-0.0298,0.0092,0.0694,0.0109,-0.0575,-0.0062,-0.0266,-0.0313,-0.0019,-0.0219,-0.0289,-0.027,-0.0187]}
{"key":"[Evaluating Deep Learning in SystemML using Layer-wise Adaptive Rate Scaling(LARS) Optimizer] Increasing the batch size of a deep learning model is a challenging task. Although it might help in utilizing full available system memory during training phase of a model, it results in significant loss of test accuracy most often. LARS solved this issue by introducing an adaptive learning rate for each layer of a deep learning model. However, there are doubts on how popular distributed machine learning systems such as SystemML or MLlib will perform with this optimizer. In this work, we apply LARS optimizer to a deep learning model implemented using SystemML.We perform experiments with various batch sizes and compare the performance of LARS optimizer with \\textit{Stochastic Gradient Descent}. Our experimental results show that LARS optimizer performs significantly better than Stochastic Gradient Descent for large batch sizes even with the distributed machine learning framework, SystemML.","layer":2,"vector":[-0.0742,-0.0202,-0.0111,0.0039,0.0372,0.0621,-0.0203,0.0081,0.0405,-0.0238,0.0266,-0.0227,0.0465,0.0559,0.0418,-0.0046,0.0338,0.025,-0.0345,-0.0236,0.0286,-0.045,-0.0321,-0.061,0.0417,0.0071,-0.0221,-0.0553,-0.0629,-0.2649,0.0331,-0.036,0.0542,-0.04,0.0128,-0.0392,-0.0136,0.0298,0.0037,0.027,-0.0005,0.0452,-0.0662,-0.0094,-0.0216,-0.0486,-0.0214,-0.0255,-0.0155,0.0031,0.0029,-0.0478,0.0398,0.0399,-0.0147,0.0153,0.0779,0.0529,0.0503,-0.0031,-0.0159,0.0385,-0.1772,0.0684,0.0308,-0.0117,-0.0568,0.0019,0.0054,0.0296,-0.0111,0.0788,0.0118,0.0507,0.0236,0.0533,-0.0126,-0.0302,0.0148,0.0287,0.0097,-0.027,-0.0342,-0.0395,-0.001,-0.052,0.0507,0.0043,0.0222,-0.04,-0.0442,-0.0056,-0.0229,0.0159,-0.0507,0.0204,0.0198,0.0362,-0.0787,0.2226,-0.0615,0.0383,0.0237,-0.0019,0.031,-0.0041,-0.0229,-0.0095,-0.0173,0.0013,-0.0158,-0.0734,-0.0026,-0.0139,0.0393,0.0141,0.0646,0.0297,-0.0048,-0.0054,-0.0159,0.025,0.0478,-0.027,0.0251,-0.0458,0.0357,0.1499,0.0006,0.0047,0.0015,-0.0499,-0.0618,-0.0422,0.0262,0.0375,0.0071,-0.006,0.02,-0.0282,-0.0476,-0.0493,0.0385,-0.0756,-0.0721,0.1247,-0.0248,0.0151,-0.0801,-0.0158,-0.0195,0.0202,-0.0363,-0.0249,0.0319,0.0124,0.0024,0.0335,-0.0321,0.0147,-0.0348,-0.0207,-0.0104,0.1043,0.0198,-0.0758,-0.0788,-0.0339,0.0025,-0.0488,0.0588,0.0323,-0.065,0.0277,0.0333,0.0166,-0.0869,-0.0118,-0.0328,-0.0388,0.0502,0.0178,-0.0238,0.0225,0.0453,-0.0588,0.0139,-0.0699,0.0156,0.0112,-0.0034,0.051,-0.0207,-0.0052,0.004,-0.0317,-0.0323,0.0099,0.0357,0.0036,0.0446,0.0107,-0.0487,0.0259,-0.0066,0.0225,-0.0263,0.0051,0.0318,0.0279,-0.0531,-0.0467,0.0838,-0.0354,-0.0235,0.0109,-0.0044,0.0191,-0.0107,0.0276,0.0505,-0.0387,-0.0585,-0.1859,0.0042,0.017,-0.0444,0.0458,-0.0616,0.0409,-0.0149,0.0297,0.0499,0.0492,-0.0179,0.0115,-0.0184,-0.0195,0.0498,0.0479,0.0057,0.0175,-0.0008,0.0441,0.035,0.0264,-0.1111,0.0378,-0.0211,0.2012,-0.037,0.0442,-0.0001,0.0008,0.0543,-0.0065,-0.1252,0.0615,0.0378,0.0893,-0.0219,-0.0061,-0.011,-0.02,0.047,0.0344,-0.1461,-0.0714,-0.062,0.0041,0.0385,-0.0387,-0.0316,-0.0058,-0.0588,0.0528,-0.0181,0.0052,-0.0139,-0.1202,0.0238,-0.0458,0.0391,-0.0123,-0.0552,-0.018,-0.0467,0.0428,-0.0086,-0.003,-0.0454,0.006,-0.0309,0.0176,0.0872,0.0141,0.0175,0.0537,0.0206,0.0112,-0.0313,-0.054,-0.0403,0.0619,0.0033,0.0767,0.0221,0.0622,0.0453,0.0776,0.0227,0.0327,-0.0162,-0.0314,0.0087,-0.0621,-0.0013,-0.0053,-0.0071,-0.2938,-0.0115,-0.0006,0.0575,-0.0358,0.01,0.0405,-0.0124,-0.0085,0.0049,-0.0027,0.0615,0.046,0.006,-0.0044,0.0374,0.071,-0.0287,0.0691,-0.0529,-0.0015,0.0695,0.2057,-0.0408,0.0106,0.0352,-0.0176,0.0256,0.0609,-0.0061,0.0158,0.0087,0.0665,-0.048,0.0295,0.0782,-0.0413,0.0097,0.0466,-0.0074,0.0143,0.0011,-0.0204,-0.0114,0.0676,-0.0277,0.0025,-0.0886,-0.0176,0.0635,0.01,0.0279,0.0179,0.002,0.0657,0.0423,-0.0659,-0.033,-0.0442,-0.064,0.0633,-0.0799,-0.0175,-0.0065,-0.0088]}
{"key":"[Approximate Leave-One-Out for High-Dimensional Non-Differentiable Learning Problems] Consider the following class of learning schemes: \\begin{equation} \\label{eq:main-problem1} \\hat{\\boldsymbol{\\beta}} := \\underset{\\boldsymbol{\\beta} \\in \\mathcal{C}}{\\arg\\min} \\;\\sum_{j=1}^n \\ell(\\boldsymbol{x}_j^\\top\\boldsymbol{\\beta}; y_j) + \\lambda R(\\boldsymbol{\\beta}), \\qquad \\qquad \\qquad (1) \\end{equation} where $\\boldsymbol{x}_i \\in \\mathbb{R}^p$ and $y_i \\in \\mathbb{R}$ denote the $i^{\\rm th}$ feature and response variable respectively. Let $\\ell$ and $R$ be the convex loss function and regularizer, $\\boldsymbol{\\beta}$ denote the unknown weights, and $\\lambda$ be a regularization parameter. $\\mathcal{C} \\subset \\mathbb{R}^{p}$ is a closed convex set. Finding the optimal choice of $\\lambda$ is a challenging problem in high-dimensional regimes where both $n$ and $p$ are large. We propose three frameworks to obtain a computationally efficient approximation of the leave-one-out cross validation (LOOCV) risk for nonsmooth losses and regularizers. Our three frameworks are based on the primal, dual, and proximal formulations of (1). Each framework shows its strength in certain types of problems. We prove the equivalence of the three approaches under smoothness conditions. This equivalence enables us to justify the accuracy of the three methods under such conditions. We use our approaches to obtain a risk estimate for several standard problems, including generalized LASSO, nuclear norm regularization, and support vector machines. We empirically demonstrate the effectiveness of our results for non-differentiable cases.","layer":2,"vector":[-0.0391,-0.0279,0.0007,-0.0064,0.0018,0.0322,0.0126,0.0378,0.0285,-0.0358,0.0026,-0.0539,0.0396,0.0519,0.0012,0.0415,0.0093,0.0592,-0.0457,0.0439,0.0139,-0.0035,-0.0214,-0.0176,0.0264,-0.0125,-0.0209,-0.0541,-0.0274,-0.2709,0.0505,-0.0401,0.0305,-0.0104,0.0407,0.0215,-0.0274,0.033,-0.051,0.0392,0.0251,0.0342,-0.0254,-0.0544,-0.0525,-0.0567,-0.0362,-0.0574,-0.0295,-0.0338,0.038,-0.0668,0.0438,0.0132,0.0067,0.0359,0.0237,-0.0066,0.0319,0.0822,0.0104,0.0035,-0.1691,0.0311,0.0269,0.0297,-0.021,-0.0288,0.0317,0.121,-0.0225,0.0195,-0.0049,0.0453,0.0524,-0.013,0.057,-0.0703,-0.0345,-0.0015,0.0681,-0.0434,-0.0345,-0.0215,-0.007,-0.0673,0.0412,-0.064,0.0105,-0.0165,-0.0357,0.002,0.0105,0.044,-0.0244,-0.052,0.0901,0.0162,-0.0472,0.1816,-0.0731,0.0355,0.0261,-0.031,0.0378,-0.0297,-0.0308,-0.0379,0.0245,-0.0157,-0.0112,-0.0172,0.037,-0.0407,0.0108,0.0143,0.0553,0.0143,0.006,0.0083,-0.0431,-0.0148,0.0646,-0.027,0.0505,-0.0746,0.0263,0.1199,0.0204,0.0548,0.0405,-0.0147,-0.0206,-0.025,0.0384,0.0083,0.0234,0.03,0.0372,0.0008,-0.0348,-0.0598,0.0411,-0.0804,-0.0065,0.1301,-0.0323,0.009,-0.0512,-0.0608,0.0081,0.0204,-0.0402,0.016,0.019,0.0378,0.0156,0.0175,-0.0392,0.0287,-0.0072,-0.0693,0.0109,0.1028,-0.0534,-0.0477,-0.0032,-0.0183,0.0555,-0.0205,0.0509,0.0161,-0.0306,-0.0095,0.0859,0.0179,-0.0983,-0.0119,0.0368,-0.0052,0.0112,-0.0327,-0.0297,0.0499,0.0485,-0.0287,0.0522,-0.0794,0.0289,0.0292,-0.0377,-0.0304,-0.0386,-0.026,-0.0324,-0.0163,-0.0303,-0.0214,0.0609,-0.0234,0.0091,0.0136,-0.0402,0.0058,-0.0134,0.0429,0.0295,-0.0071,0.0347,0.0572,-0.0126,0.0059,0.0553,-0.0036,-0.0266,0.0041,0.0826,0.0484,0.0045,0.0373,0.0134,-0.037,-0.0541,-0.2364,-0.0466,0.0029,-0.0211,0.0274,-0.1059,0.0802,-0.0365,0.0659,0.0842,0.0625,-0.0117,-0.0466,0.0301,0.0206,0.0556,0.0458,0.0216,-0.0291,0.0016,-0.0578,0.0366,-0.0056,-0.0683,0.041,-0.0168,0.1823,0.0209,0.0331,-0.0431,-0.0274,0.0394,0.0069,-0.0451,0.0631,-0.0026,0.0628,-0.034,-0.0445,-0.0393,0.0285,-0.0154,0.0091,-0.0793,-0.018,-0.0269,-0.0374,0.0201,-0.0811,0.0675,0.0759,-0.0396,0.0444,-0.0602,0.0456,-0.0479,-0.1392,0.0271,-0.0469,0.0222,-0.0101,-0.0541,0.0216,-0.0587,0.0677,-0.0042,-0.0063,0.0072,0.0225,-0.0591,-0.0618,0.0628,-0.0164,0.0087,0.0464,0.0018,0.03,-0.0021,-0.0853,-0.0269,0.0996,-0.0094,0.0294,0.0226,0.0307,0.0258,0.0888,0.0089,0.0072,-0.0305,-0.0306,-0.0329,-0.0656,0.0103,0.0105,0.0255,-0.2655,-0.0025,0.0127,0.0258,-0.0453,-0.0181,0.0223,-0.0275,-0.039,0.0651,-0.0001,0.0318,0.0502,-0.0086,0.0257,0.02,0.0645,-0.0408,0.0514,-0.0651,0.0191,0.0295,0.1841,-0.0794,-0.0071,0.0089,-0.0444,-0.0583,0.0191,-0.0212,0.0348,0.0161,0.1065,-0.0796,0.0487,0.0892,-0.0298,0.0068,-0.0226,-0.0583,0.0427,0.0042,-0.0434,0.007,0.106,-0.0125,0.0064,-0.0025,0.0182,0.0051,-0.0144,0.0281,0.004,0.0144,0.0083,0.0414,-0.0406,-0.0542,-0.0285,-0.0412,0.0264,-0.0254,-0.0492,0.0016,-0.0063]}
{"key":"[Efficient Training Data Generation for Phase-Based DOA Estimation] Deep learning (DL) based direction of arrival (DOA) estimation is an active research topic and currently represents the state-of-the-art. Usually, DL-based DOA estimators are trained with recorded data or computationally expensive generated data. Both data types require significant storage and excessive time to, respectively, record or generate. We propose a low complexity online data generation method to train DL models with a phase-based feature input. The data generation method models the phases of the microphone signals in the frequency domain by employing a deterministic model for the direct path and a statistical model for the late reverberation of the room transfer function. By an evaluation using data from measured room impulse responses, we demonstrate that a model trained with the proposed training data generation method performs comparably to models trained with data generated based on the source-image method.","layer":2,"vector":[-0.0232,-0.0191,0.0661,-0.067,0.0423,0.0407,-0.0062,0.015,0.0193,-0.0118,0.0455,-0.0457,0.0099,0.0279,0.0118,0.0318,0.0235,0.0289,-0.0443,0.0034,0.0606,0.0086,0.0226,-0.0535,0.0459,-0.0227,-0.0502,-0.0179,-0.0744,-0.239,0.0297,-0.0604,0.0673,-0.0386,-0.0101,-0.0622,-0.0271,0.0391,0.0084,0.0883,0.0386,0.0229,-0.0175,-0.0709,-0.0009,-0.0184,-0.031,-0.0093,0.0217,-0.0514,0.0224,-0.0398,-0.001,0.0068,0.0533,0.0408,0.0612,0.0569,0.0396,0.0569,0.0446,0.0214,-0.2046,0.0681,0.0253,0.024,-0.0172,-0.0363,0.0165,0.0076,-0.0319,0.0136,0.0491,-0.0178,0.013,-0.0267,-0.0136,-0.0095,0.0156,-0.0097,0.0183,0.0024,-0.0297,0.0105,-0.0467,-0.0538,-0.0107,-0.0509,-0.0265,-0.0128,-0.0938,0.0041,-0.0691,0.0669,-0.0801,-0.0277,-0.0068,0.0336,-0.0117,0.1971,-0.0146,0.0496,0.0109,-0.0333,-0.0119,-0.0386,-0.0018,-0.0701,-0.0056,0.0254,-0.0063,-0.0166,0.0598,-0.0498,0.0323,0.0077,0.0768,0.0629,0.0194,-0.0442,0.0027,0.0129,0.012,0.0144,0.006,-0.09,0.0355,0.0989,0.0208,0.0617,0.0443,-0.0192,-0.0778,-0.033,0.031,0.035,0.0783,0.0131,0.0049,0.0071,-0.015,-0.0493,0.0266,-0.0396,-0.0344,0.1128,-0.0345,0.013,-0.0333,-0.0592,-0.01,0.0078,0.0092,-0.016,0.0483,-0.0125,-0.0078,0.0496,-0.0262,-0.0036,-0.044,-0.0299,-0.0168,0.1227,-0.0241,-0.0948,-0.0499,0.0188,-0.0013,-0.0003,0.0212,0.0278,-0.0797,0.0184,0.0535,0.0209,-0.0534,0.0127,-0.0215,0.0344,0.0033,-0.0449,-0.0164,0.0125,0.0405,-0.0681,0.0094,-0.0672,-0.0095,0.0239,-0.0166,0.0204,-0.0126,-0.0045,-0.0125,-0.0609,-0.0369,0.0219,0.067,-0.0172,-0.0011,0.0099,-0.0415,0.0315,0.0008,0.0094,0.0217,0.0331,0.0411,0.0438,-0.0333,-0.0145,0.1255,-0.0571,-0.0422,-0.0051,0.0158,0.028,-0.0094,0.0844,-0.0001,-0.0419,-0.0594,-0.2292,0.0291,0.0072,0.004,0.0288,-0.0921,0.0327,0.0448,0.0614,0.0719,0.0499,0.0101,0.0142,0.0493,-0.0173,0.08,0.0263,0.0391,0.0125,0.0019,-0.0242,0.0154,-0.0526,-0.0922,0.0466,-0.0113,0.2088,-0.0098,0.0614,-0.0344,0.0287,0.015,-0.018,-0.0739,0.042,0.0126,0.1134,0.0156,-0.0139,-0.0232,-0.043,0.0057,0.0407,-0.0581,-0.0075,-0.0157,-0.0397,0.0156,-0.0597,-0.0062,0.0112,-0.0374,0.0494,0.0026,0.0213,-0.0421,-0.0723,0.0295,-0.0584,0.006,-0.0019,-0.015,0.0264,-0.0583,-0.0094,0.0045,-0.0339,-0.0677,0.0664,-0.0405,-0.009,0.146,0.0247,0.0494,0.1008,-0.0416,0.0199,-0.0683,-0.0613,-0.0456,0.0204,-0.0419,0.0038,0.0092,0.0408,0.0316,0.0681,0.0062,0.0197,-0.0217,-0.0237,-0.0037,-0.0068,-0.0119,0.0042,-0.043,-0.2808,-0.0026,-0.0094,0.0408,-0.0203,0.0005,0.0289,0.0112,-0.0921,-0.0194,-0.0158,0.0438,-0.0124,-0.0043,0.0448,0.0208,0.0815,-0.0314,0.0352,-0.0707,0.0031,0.0624,0.1886,-0.002,0.0455,-0.0234,-0.0517,0.0205,0.0245,-0.0226,-0.0256,0.0036,0.0965,-0.0199,0.0112,0.077,-0.0238,0.0313,-0.0281,-0.0409,0.008,-0.0039,0.0116,-0.0402,0.0923,-0.0015,-0.0036,-0.0108,0.008,0.0199,-0.0365,0.0145,0.0197,0.0081,0.0286,0.0559,-0.0861,-0.0372,-0.037,-0.0404,0.0451,-0.0798,0.0044,-0.0309,0.0143]}
{"key":"[Perturbation Theory for the Information Bottleneck] Extracting relevant information from data is crucial for all forms of learning. The information bottleneck (IB) method formalizes this, offering a mathematically precise and conceptually appealing framework for understanding learning phenomena. However the nonlinearity of the IB problem makes it computationally expensive and analytically intractable in general. Here we derive a perturbation theory for the IB method and report the first complete characterization of the learning onset, the limit of maximum relevant information per bit extracted from data. We test our results on synthetic probability distributions, finding good agreement with the exact numerical solution near the onset of learning. We explore the difference and subtleties in our derivation and previous attempts at deriving a perturbation theory for the learning onset and attribute the discrepancy to a flawed assumption. Our work also provides a fresh perspective on the intimate relationship between the IB method and the strong data processing inequality.","layer":2,"vector":[-0.0405,-0.0116,0.0196,-0.0705,0.0181,0.031,0.0409,0.0241,0.0562,-0.0114,0.0007,-0.0371,0.0441,0.0518,0.0241,0.058,-0.0183,-0.0042,-0.0741,-0.0097,0.06,-0.0399,-0.0286,-0.0646,0.0231,0.017,-0.0509,-0.0125,-0.0197,-0.2608,0.0309,-0.0216,0.0494,-0.0182,0.0205,-0.0427,-0.0344,0.0667,-0.0181,0.0572,0.0205,0.0067,-0.0633,-0.0337,-0.0369,-0.0003,-0.0273,0.0033,-0.0371,-0.0538,-0.0567,-0.0105,0.0506,-0.0084,0.0388,0.0812,0.0537,0.0475,0.022,0.0809,-0.0199,0.0594,-0.1919,0.0689,0.0674,-0.0158,-0.0361,-0.0294,0.0277,0.0411,-0.0255,0.0431,0.0241,0.0444,0.0319,-0.0106,-0.0281,-0.036,0.0023,0.0216,0.0258,-0.036,-0.0334,-0.024,-0.0459,-0.0561,0.0504,-0.0294,0.0406,0.0007,-0.0153,-0.0109,-0.0439,0.0355,-0.0379,-0.0127,0.0245,0.0549,-0.0107,0.1541,-0.0256,0.0324,0.001,-0.0302,0.0664,-0.0199,-0.0152,-0.0188,-0.0219,-0.0268,0.0029,-0.0298,0.0069,-0.054,0.0397,0.0362,0.04,0.0107,-0.0431,-0.0167,-0.0083,0.0174,0.0424,0.0136,-0.0182,-0.0513,0.0006,0.1256,-0.0091,0.0517,0.0501,-0.0415,-0.0555,0.001,-0.0146,0.0182,0.0144,0.0143,0.0425,-0.0049,-0.0403,-0.0241,0.0141,-0.0955,-0.1182,0.1526,-0.0267,0.048,-0.0428,-0.0524,-0.0213,-0.0093,-0.0065,-0.032,0.0135,0.0414,0.0212,0.0387,-0.0438,0.0114,-0.0272,-0.0647,0.0151,0.0905,0.0183,0.002,0.002,0.0268,0.0322,-0.0494,0.0171,0.0432,-0.0389,0.0325,0.0674,-0.0067,-0.0951,-0.0073,0.0292,0.0208,0.0202,-0.041,-0.0011,0.0408,0.0469,-0.0236,-0.0291,-0.0334,0.0868,0.0172,-0.0441,-0.0319,0.0036,-0.0113,-0.0478,-0.0393,-0.0178,0.0016,-0.0142,-0.0199,-0.0155,0.0021,-0.0693,0.0288,-0.0047,0.032,0.0127,-0.0267,0.0079,-0.0078,-0.0043,-0.0542,0.0357,-0.0349,-0.0287,-0.0347,0.0461,0.0362,0.0135,0.058,0.0236,-0.0432,-0.0736,-0.2474,-0.0275,-0.0098,-0.0222,0.0681,-0.0598,0.0619,0.0038,0.0124,0.0475,0.0019,0.0106,-0.0388,0.045,-0.0078,0.06,0.0352,0.0299,-0.013,0.0177,-0.0525,0.0462,-0.0318,-0.0943,0.022,-0.0434,0.1995,0.0191,0.0254,-0.018,0.0102,0.0428,-0.051,-0.0514,0.057,0.0367,0.0561,-0.0279,-0.0134,0.0025,-0.0554,0.0139,0.0041,-0.0947,-0.0652,-0.0074,-0.0426,0.0216,-0.0777,-0.0125,0.0455,-0.0186,0.0732,0.0123,0.043,-0.0543,-0.0699,0.0657,-0.0666,0.0123,0.0248,-0.0777,-0.0269,-0.0497,0.0099,0.0081,-0.0499,-0.0347,0.0669,-0.0189,-0.0279,0.0636,-0.0005,0.0324,0.048,0.0061,0.0512,-0.0509,-0.0457,0.0002,0.0592,-0.0478,0.0472,0.0281,0.0475,0.0033,0.1035,-0.0556,-0.0037,0.0053,0.0144,0.0589,-0.0641,0.001,0.0392,-0.0209,-0.2809,0.0509,0.0114,0.0241,0.0114,0.0353,0.0651,0.0236,-0.0435,-0.004,-0.0313,0.0694,0.0174,0.0013,0.0045,0.0287,0.0527,-0.0494,0.044,-0.0484,0.039,0.0315,0.2382,-0.0136,0.0217,-0.008,-0.0163,0.0282,0.0303,-0.0586,-0.0058,-0.0168,0.0784,-0.0604,0.0491,0.0477,-0.0367,0.0615,0.0124,-0.0286,0.0312,-0.0047,-0.0334,0.0127,0.1021,0.0137,0.0018,-0.0678,-0.0131,0.0478,-0.0257,0.0301,0.0092,0.0109,0.0626,0.0497,-0.0454,-0.0373,-0.0157,-0.047,0.0136,-0.0441,-0.0228,0.0231,-0.0512]}
{"key":"[Temporal Induced Self-Play for Stochastic Bayesian Games] One practical requirement in solving dynamic games is to ensure that the players play well from any decision point onward. To satisfy this requirement, existing efforts focus on equilibrium refinement, but the scalability and applicability of existing techniques are limited. In this paper, we propose Temporal-Induced Self-Play (TISP), a novel reinforcement learning-based framework to find strategies with decent performances from any decision point onward. TISP uses belief-space representation, backward induction, policy learning, and non-parametric approximation. Building upon TISP, we design a policy-gradient-based algorithm TISP-PG. We prove that TISP-based algorithms can find approximate Perfect Bayesian Equilibrium in zero-sum one-sided stochastic Bayesian games with finite horizon. We test TISP-based algorithms in various games, including finitely repeated security games and a grid-world game. The results show that TISP-PG is more scalable than existing mathematical programming-based methods and significantly outperforms other learning-based methods.","layer":11,"vector":[-0.0545,0.0095,0.0298,-0.0625,-0.0,0.061,0.0328,0.0268,0.0693,-0.0103,0.0335,-0.0311,0.0036,0.0968,0.0185,0.0374,-0.0444,0.0188,-0.0192,-0.0011,0.0286,-0.0663,-0.0327,-0.0499,0.0257,0.0303,-0.0489,-0.0578,-0.0476,-0.2121,0.0364,-0.0827,-0.0544,-0.0202,-0.0511,-0.0413,-0.0381,0.0653,-0.0351,0.0234,0.0528,0.0521,0.0023,-0.0478,-0.034,-0.0639,-0.0011,-0.0289,-0.0151,-0.0109,0.026,0.0162,0.0285,0.0006,0.028,0.0253,0.0352,0.072,0.0513,0.0323,0.017,0.0342,-0.1598,0.0608,0.0643,0.0658,-0.0334,-0.0129,0.0228,0.0363,-0.0402,0.0833,0.0074,0.0539,0.0512,0.0265,0.0049,-0.0404,0.0037,0.0285,-0.0117,-0.0168,-0.0244,0.0015,-0.0271,-0.0747,0.0164,-0.0388,0.0904,0.0251,-0.0243,0.0505,-0.0113,-0.0155,-0.065,0.0247,0.0236,0.0317,-0.0584,0.1945,0.0001,0.0465,-0.0125,0.0062,0.0589,-0.03,-0.0385,-0.0433,-0.0048,-0.0076,-0.0485,-0.0385,0.048,-0.0138,-0.0513,0.019,0.0624,0.0086,0.0027,0.0079,-0.0164,0.013,0.0429,-0.0151,0.0119,-0.0823,0.0309,0.1606,0.0228,0.0067,0.0275,-0.0671,-0.0352,-0.0115,0.0194,0.0185,-0.0152,-0.0275,0.022,-0.0138,-0.0485,-0.0129,0.0272,-0.1255,-0.0583,0.0787,0.0647,0.0635,-0.0161,-0.0213,-0.0025,-0.0079,-0.0072,-0.0488,-0.033,0.0367,0.0603,0.0667,-0.0659,-0.0337,-0.071,-0.057,0.0021,0.0928,0.008,-0.056,-0.0461,-0.0007,-0.0005,-0.0478,0.0034,0.0023,-0.0322,0.0003,0.0823,0.0416,-0.0889,0.0079,0.0186,0.0422,0.0281,-0.0315,0.001,0.0094,0.0254,-0.0337,-0.0114,-0.0247,0.0014,0.0181,-0.0108,-0.005,-0.0062,-0.0281,-0.0317,-0.0666,0.0226,-0.0351,0.0354,-0.0232,-0.021,-0.0554,-0.0585,0.0478,0.0208,0.0143,-0.0415,-0.0077,0.0694,-0.0028,0.0049,0.0204,0.0339,-0.0199,0.001,0.0039,-0.0035,0.0212,-0.0151,0.0483,0.0173,0.0012,-0.0253,-0.1893,0.0063,-0.0656,-0.0368,0.0177,-0.0547,-0.0041,-0.0603,0.0339,0.0762,0.075,-0.0384,-0.015,0.0183,-0.0086,0.0186,0.0132,0.0019,0.0049,0.0083,-0.0104,0.0157,-0.0208,-0.0766,0.0466,-0.0047,0.2186,0.0324,0.0218,-0.0243,0.0173,0.0165,-0.0455,-0.0839,0.0349,0.0385,0.0875,-0.054,-0.0458,-0.0781,-0.0133,0.0434,-0.028,-0.0933,-0.0328,-0.0381,-0.0134,0.0605,-0.0506,-0.0364,0.0681,-0.0223,0.0464,-0.0149,-0.0242,-0.0397,-0.0614,0.0694,0.0079,0.0298,0.0136,0.0034,-0.0121,-0.04,0.0792,-0.0316,0.0047,-0.0684,0.0193,-0.0048,-0.0318,0.0472,-0.0152,0.0175,0.0383,0.0254,0.0571,-0.0192,-0.0545,-0.0232,0.0625,-0.0399,0.0517,0.0891,-0.0014,-0.0094,0.0369,0.0476,0.0485,-0.0098,-0.0098,0.0066,-0.0614,0.0358,0.0193,-0.0214,-0.3175,0.0639,0.0165,0.0376,-0.0561,0.0266,0.0547,0.0122,-0.0638,-0.0095,0.0107,0.068,0.0012,0.0547,-0.0193,-0.0484,0.0674,-0.0389,0.0888,-0.0694,0.0509,0.0391,0.2479,-0.029,0.0299,0.0216,0.0136,0.0414,0.0468,-0.0181,0.0165,-0.018,0.0576,-0.0588,0.0179,0.0711,-0.0035,0.0447,-0.0066,-0.0052,-0.0385,-0.0028,-0.0072,-0.0027,0.0946,-0.0045,-0.0141,-0.0429,-0.0197,0.0259,-0.0456,0.0525,-0.0097,-0.046,0.031,0.0035,-0.0385,-0.0415,-0.0234,-0.0366,-0.0158,-0.0627,0.0281,-0.0175,0.0099]}
{"key":"[Comparison of Neuronal Attention Models] Recent models for image processing are using the Convolutional neural network (CNN) which requires a pixel per pixel analysis of the input image. This method works well. However, it is time-consuming if we have large images. To increase the performance, by improving the training time or the accuracy, we need a size-independent method. As a solution, we can add a Neuronal Attention model (NAM). The power of this new approach is that it can efficiently choose several small regions from the initial image to focus on. The purpose of this paper is to explain and also test each of the NAM's parameters.","layer":1,"vector":[-0.048,-0.0115,0.0286,-0.0428,0.0331,0.0395,0.068,0.0317,0.0184,-0.0026,0.0015,-0.059,0.0297,0.0991,0.0362,0.002,0.0306,0.0098,-0.0415,0.0033,0.0125,-0.0147,-0.014,-0.0251,-0.0233,-0.0093,-0.0056,-0.0165,-0.0692,-0.2321,0.0283,-0.0229,0.0897,-0.0265,-0.026,-0.0136,-0.0214,0.0137,-0.0157,0.009,0.008,0.0167,-0.0509,-0.061,-0.0461,-0.0684,-0.0221,-0.0657,-0.0071,-0.0302,0.0437,-0.0589,0.0331,0.0077,0.0305,0.0249,0.0502,0.0451,0.0616,-0.0061,0.0335,0.0742,-0.1725,0.0455,0.0491,0.0307,-0.0316,-0.0438,-0.0259,0.0461,-0.02,0.048,-0.0169,0.0195,-0.0164,-0.0285,-0.0302,0.0007,0.0202,-0.0121,0.0212,0.0183,-0.0219,-0.0468,0.0174,-0.0121,-0.0534,-0.0117,0.0303,-0.0616,0.003,0.0227,-0.0711,-0.0306,-0.0374,-0.0168,0.0524,0.0041,-0.0771,0.1885,-0.0202,0.0031,0.0612,-0.062,0.0343,-0.0027,-0.0346,-0.0108,-0.0459,0.032,0.0155,-0.0273,0.0253,-0.0195,0.0069,-0.0026,0.0659,-0.0213,0.0032,-0.0338,-0.0499,-0.0017,0.037,-0.043,0.0286,-0.0701,0.002,0.145,0.0677,0.0257,0.0622,-0.0378,-0.0397,-0.0127,0.0098,0.0031,0.0772,0.0111,-0.0042,-0.0554,-0.04,-0.0495,0.0518,-0.0641,-0.0355,0.0908,-0.04,0.0248,-0.0423,-0.0529,-0.0122,0.0406,-0.0334,-0.0367,-0.0045,0.0052,-0.0128,0.0168,-0.0849,0.0348,0.0154,-0.0206,-0.0073,0.0897,0.0109,-0.0522,-0.0364,-0.0302,0.0184,-0.0122,0.0222,0.0232,-0.0464,0.0507,0.0769,0.0381,-0.0592,-0.0252,0.0007,0.0334,0.063,-0.1011,-0.0021,0.0228,0.0067,-0.0795,0.0462,-0.0727,0.0065,0.0554,-0.0721,0.0544,-0.0189,-0.0053,-0.0476,-0.0476,-0.0382,-0.0475,-0.0129,-0.0364,0.0284,-0.0027,-0.0238,0.0117,-0.0085,0.0208,-0.0022,0.0258,0.0358,0.0338,-0.0718,0.0229,0.0621,-0.0417,-0.0349,-0.0022,0.0276,0.0321,0.0334,0.0702,0.0686,-0.0637,-0.0578,-0.2321,-0.0183,0.0269,-0.0298,0.0549,-0.0813,0.0615,0.0232,0.0741,0.0605,0.0638,-0.0503,-0.0245,0.0649,-0.0069,0.052,0.0119,0.0301,-0.0395,-0.0442,0.0277,0.0452,0.0365,-0.0691,0.0391,-0.0301,0.1829,0.0222,0.0443,-0.0305,0.0189,0.0438,-0.0302,-0.0849,0.0227,0.0219,0.0431,0.0136,-0.0222,-0.0025,-0.0229,0.0121,-0.0025,-0.1134,-0.0329,0.0105,-0.0092,0.0341,-0.0326,0.003,0.0384,-0.0759,-0.0034,-0.0109,0.004,-0.0701,-0.0995,-0.016,-0.0478,0.0465,0.0044,-0.036,0.0144,-0.0787,0.0348,0.0368,-0.0641,-0.0046,0.0156,-0.0126,-0.0143,0.0952,0.0497,0.0171,0.0639,-0.0123,0.0678,-0.0058,-0.0259,-0.0203,0.0306,-0.0088,0.051,0.0355,0.0656,0.0077,0.0981,-0.0336,0.0185,-0.0223,0.0147,0.0166,-0.0425,-0.0426,-0.001,-0.0128,-0.2871,0.0507,0.0187,0.0513,0.0052,0.0243,0.02,0.0384,-0.0284,0.0183,-0.0139,0.0334,0.0433,-0.0228,0.0042,0.0439,0.0282,-0.0359,0.0901,-0.0545,-0.0166,0.0193,0.2133,-0.0277,0.0373,0.0019,-0.0272,-0.0058,0.0291,-0.0188,0.039,0.0604,0.0865,-0.0546,0.0457,0.0995,-0.0551,0.0565,0.0077,-0.0041,0.0101,0.0405,-0.0443,-0.0422,0.1003,0.0096,-0.0263,-0.0047,-0.0528,0.0427,-0.0208,-0.0215,0.0272,-0.0017,0.0676,0.0392,-0.0499,-0.0312,-0.0086,-0.0285,0.0402,-0.1013,-0.0004,0.0261,-0.0098]}
{"key":"[Automatic Segmentation of Head and Neck Tumor: How Powerful Transformers Are?] Cancer is one of the leading causes of death worldwide, and head and neck (H&N) cancer is amongst the most prevalent types. Positron emission tomography and computed tomography are used to detect, segment and quantify the tumor region. Clinically, tumor segmentation is extensively time-consuming and prone to error. Machine learning, and deep learning in particular, can assist to automate this process, yielding results as accurate as the results of a clinician. In this paper, we investigate a vision transformer-based method to automatically delineate H&N tumor, and compare its results to leading convolutional neural network (CNN)-based models. We use multi-modal data from CT and PET scans to perform the segmentation task. We show that a solution with a transformer-based model has the potential to achieve comparable results to CNN-based ones. With cross validation, the model achieves a mean dice similarity coefficient (DSC) of 0.736, mean precision of 0.766 and mean recall of 0.766. This is only 0.021 less than the 2020 competition winning model (cross validated in-house) in terms of the DSC score. On the testing set, the model performs similarly, with DSC of 0.736, precision of 0.773, and recall of 0.760, which is only 0.023 lower in DSC than the 2020 competition winning model. This work shows that cancer segmentation via transformer-based models is a promising research area to further explore.","layer":4,"vector":[-0.0501,-0.04,0.0521,-0.036,0.0199,0.0287,0.0596,0.0212,0.0276,0.0063,0.0023,-0.0641,0.0139,0.0363,-0.011,0.0094,-0.0228,0.0306,-0.0147,0.021,0.0109,-0.0345,0.0096,-0.0456,0.0188,0.0239,0.0051,-0.0375,-0.0577,-0.2341,0.0269,-0.06,0.0289,-0.0586,-0.0314,-0.0278,-0.0506,0.0438,-0.0323,0.014,0.01,-0.0143,-0.0266,-0.0134,-0.028,-0.03,-0.0451,-0.036,-0.0051,-0.0414,0.0948,-0.0539,0.0299,0.0588,0.0154,0.006,0.0481,0.0809,0.0664,0.0568,0.023,0.0528,-0.1834,0.098,0.0392,-0.0319,-0.0617,-0.0275,0.0115,0.0106,-0.0086,0.008,0.0066,0.03,0.01,-0.0254,0.024,-0.0253,-0.0221,0.0223,0.0023,0.0186,-0.0119,-0.0518,-0.0321,-0.0517,-0.0055,-0.0797,0.0075,-0.026,-0.0309,0.0069,-0.0221,0.0523,-0.0246,-0.0476,0.0435,0.0488,-0.0278,0.1701,-0.0602,0.0204,0.0219,-0.0429,0.0311,0.0029,-0.0383,0.0042,-0.016,0.0493,-0.0136,0.0019,0.0517,-0.041,0.0009,-0.0274,0.0537,0.0417,-0.0279,-0.0433,-0.0548,-0.0242,0.0545,-0.0098,0.0442,-0.0146,0.0332,0.1147,0.052,0.0432,0.0573,0.0397,-0.036,-0.012,-0.0131,0.0161,-0.0058,-0.023,-0.0347,-0.0092,-0.022,-0.0888,0.0333,-0.074,-0.0078,0.0929,-0.0928,0.0191,-0.0479,-0.0605,-0.002,0.0364,-0.0606,-0.015,0.0595,0.0316,0.0183,0.0257,-0.06,0.0058,0.0165,-0.09,-0.0651,0.117,0.0044,-0.087,-0.0246,0.0093,0.0297,0.0329,0.0243,0.0606,-0.035,0.0038,0.0833,0.0386,-0.0657,-0.0386,0.0072,0.0337,0.0602,-0.0225,-0.0061,0.0048,0.0507,-0.0257,0.012,-0.0462,0.0141,0.0664,-0.0292,0.0439,-0.0333,-0.0288,-0.0092,-0.0725,-0.0488,-0.0156,-0.0149,-0.0206,0.0013,-0.0252,-0.0162,0.0372,0.0245,0.033,-0.0439,0.0041,0.0363,0.0388,-0.0122,-0.0457,0.0802,0.0045,-0.0331,0.0234,0.0426,0.0415,-0.0072,0.0733,0.083,-0.0412,-0.0694,-0.2092,-0.0723,0.0023,-0.0253,0.029,-0.0501,0.036,0.0088,0.044,0.0546,0.0461,-0.0075,0.0174,0.0118,-0.0216,0.054,0.0142,0.0697,-0.0282,-0.0326,-0.0098,0.0334,0.0057,-0.108,0.0752,-0.02,0.2199,0.002,0.0542,0.0466,0.0278,0.0396,-0.0376,-0.0867,0.0723,-0.0071,0.0764,0.0106,-0.0616,-0.0041,-0.0451,-0.0246,-0.0245,-0.0718,0.0038,-0.0352,-0.0157,0.0545,-0.0401,0.0378,0.0114,-0.0846,0.0534,0.0043,-0.0239,-0.0484,-0.1195,0.0213,-0.056,0.0019,0.0185,-0.0341,0.0112,-0.0736,0.0217,0.0393,-0.005,-0.0434,-0.0381,-0.011,-0.0556,0.0696,0.0405,0.0158,0.0256,0.0148,0.0878,-0.0165,-0.0491,-0.0126,0.0718,-0.0351,0.0316,0.002,0.0218,0.0072,0.0898,-0.0112,0.0069,-0.0453,0.0441,0.0123,-0.0515,-0.0319,0.0087,-0.0092,-0.2648,0.033,0.0189,0.0559,-0.0349,-0.0144,0.0217,0.0444,0.0051,-0.0205,-0.019,0.0099,0.0689,-0.0125,-0.0005,0.0076,0.0606,-0.0419,0.0593,-0.0224,-0.0007,0.0243,0.2143,-0.0868,0.0199,-0.0067,-0.008,-0.0038,0.0762,-0.016,0.0115,0.0193,0.1034,-0.0741,0.0209,0.1145,-0.0491,0.0389,-0.0292,-0.049,0.0425,0.073,-0.0127,-0.0289,0.0944,0.0009,-0.0084,-0.0516,0.0356,0.0037,-0.018,0.0274,-0.0122,-0.0183,0.0537,0.0074,0.0021,-0.0555,-0.0309,-0.0291,0.0056,-0.0592,-0.0247,0.042,0.0093]}
{"key":"[Looking Deeper into Tabular LIME] Interpretability of machine learning algorithms is an urgent need. Numerous methods appeared in recent years, but do their explanations make sense? In this paper, we present a thorough theoretical analysis of one of these methods, LIME, in the case of tabular data. We prove that in the large sample limit, the interpretable coefficients provided by Tabular LIME can be computed in an explicit way as a function of the algorithm parameters and some expectation computations related to the black-box model. When the function to explain has some nice algebraic structure (linear, multiplicative, or sparsely depending on a subset of the coordinates), our analysis provides interesting insights into the explanations provided by LIME. These can be applied to a range of machine learning models including Gaussian kernels or CART random forests. As an example, for linear functions we show that LIME has the desirable property to provide explanations that are proportional to the coefficients of the function to explain and to ignore coordinates that are not used by the function to explain. For partition-based regressors, on the other side, we show that LIME produces undesired artifacts that may provide misleading explanations.","layer":0,"vector":[-0.0339,-0.0162,0.0237,-0.0043,0.043,-0.0341,0.0443,0.0162,0.0621,-0.0266,0.0471,-0.0562,0.002,0.0259,0.023,0.0168,0.0013,0.0218,-0.076,-0.0357,0.0453,-0.0151,-0.0387,-0.0505,0.025,0.0182,-0.061,-0.0362,-0.0268,-0.2762,0.0208,-0.0302,0.0391,-0.0441,0.0356,-0.0167,-0.0055,0.0572,-0.0353,0.0265,0.0181,-0.0027,-0.0367,-0.0163,-0.0041,-0.0441,-0.0156,-0.0084,-0.0438,-0.0093,0.0023,-0.012,0.0286,0.0199,0.0219,0.0432,0.0522,0.0949,0.0303,0.0303,0.0811,0.0516,-0.1755,0.0599,0.0571,0.0334,-0.0658,-0.0235,0.0042,0.0812,0.0107,0.0279,-0.0093,0.0404,0.0206,-0.0122,-0.0091,0.0039,-0.0052,-0.0033,0.0304,0.0017,-0.0113,0.0304,-0.0463,-0.0419,0.0244,-0.0522,0.0616,-0.0223,0.0042,0.0311,-0.0718,0.031,-0.0645,-0.0061,0.0477,-0.0154,-0.0288,0.1644,-0.0559,0.0324,0.0262,0.0004,0.0086,-0.0375,-0.0187,-0.045,-0.0055,-0.052,-0.0467,-0.0334,0.0032,-0.0083,-0.0062,-0.0009,0.0505,0.0168,0.0173,-0.0195,-0.0308,0.0042,0.0237,0.0035,0.0512,0.0033,0.0215,0.1813,0.0493,0.0179,0.0517,-0.0505,-0.0477,-0.0157,0.0147,0.0083,0.0366,0.0179,0.0102,0.0039,-0.0524,-0.0686,0.0045,-0.0469,-0.0534,0.1136,-0.0631,0.0339,-0.0728,-0.0006,-0.019,-0.0005,-0.0275,-0.0627,0.0129,0.0373,0.0196,0.025,-0.0668,0.0272,0.0083,-0.0764,-0.0528,0.0782,0.0488,-0.032,-0.0413,-0.0152,0.0268,-0.0155,0.0727,0.0377,-0.0425,0.0372,0.0436,0.0503,-0.0877,-0.0222,-0.0066,-0.0178,0.0532,-0.0445,-0.0497,0.1052,0.0205,-0.0422,-0.0103,-0.0327,0.0192,0.062,-0.0179,-0.0116,-0.0411,0.0033,-0.0404,-0.0452,0.0114,-0.0247,0.0351,-0.0283,-0.0202,0.0507,-0.0455,0.0318,-0.0371,0.02,0.0165,-0.0219,0.0418,0.0589,-0.0271,0.0038,0.0203,-0.0498,-0.0254,0.0075,0.0266,0.0051,-0.0051,0.0222,0.0395,-0.0515,-0.0795,-0.2317,-0.0022,0.0122,-0.002,0.0201,-0.0643,0.0024,-0.0047,0.0187,0.0881,0.0316,-0.0621,-0.025,0.0253,-0.0297,0.0538,0.0065,0.0191,-0.0433,0.0562,0.0183,0.0195,-0.0102,-0.0757,0.0268,0.0349,0.1978,0.0373,0.0368,-0.0074,0.0325,0.0208,0.0062,-0.0948,0.052,0.0099,0.0051,-0.0145,-0.0357,0.0072,-0.0224,0.0238,0.0106,-0.0687,-0.0449,0.0065,-0.0235,0.0154,-0.0648,0.0512,0.0443,-0.0103,0.0855,-0.004,0.0489,-0.038,-0.0684,0.0676,-0.033,0.0281,0.0004,-0.0843,0.0279,-0.0767,0.0215,-0.0326,-0.014,-0.0297,0.0286,-0.014,-0.0175,0.1045,-0.0049,-0.0359,0.0705,0.0202,0.059,-0.0446,-0.05,0.0133,0.0272,-0.0435,0.0521,-0.024,0.0453,-0.0032,0.0732,-0.0243,0.0219,-0.0248,-0.0373,0.0258,-0.0373,-0.0166,0.0874,-0.0192,-0.3026,0.0262,0.0043,0.0064,-0.036,0.0185,0.0276,0.0105,-0.0371,-0.0144,0.0248,0.044,0.0305,-0.0164,0.018,0.0286,0.0973,-0.0646,0.0351,-0.0286,0.0312,0.0248,0.2268,-0.0364,0.0065,0.0377,-0.0328,-0.0645,0.0114,-0.0002,0.0316,-0.003,0.0758,-0.0224,0.0313,0.0849,-0.0331,0.0533,0.0412,-0.0653,0.0317,-0.0198,-0.0937,-0.0511,0.1075,-0.0257,-0.0054,-0.0508,0.0332,0.0122,0.0087,0.0049,-0.0242,0.0029,0.0175,0.0433,-0.0482,-0.0399,-0.0068,-0.05,0.0335,-0.0718,0.0082,0.0015,-0.0234]}
{"key":"[Gram-Gauss-Newton Method: Learning Overparameterized Neural Networks for Regression Problems] First-order methods such as stochastic gradient descent (SGD) are currently the standard algorithm for training deep neural networks. Second-order methods, despite their better convergence rate, are rarely used in practice due to the prohibitive computational cost in calculating the second-order information. In this paper, we propose a novel Gram-Gauss-Newton (GGN) algorithm to train deep neural networks for regression problems with square loss. Our method draws inspiration from the connection between neural network optimization and kernel regression of neural tangent kernel (NTK). Different from typical second-order methods that have heavy computational cost in each iteration, GGN only has minor overhead compared to first-order methods such as SGD. We also give theoretical results to show that for sufficiently wide neural networks, the convergence rate of GGN is \\emph{quadratic}. Furthermore, we provide convergence guarantee for mini-batch GGN algorithm, which is, to our knowledge, the first convergence result for the mini-batch version of a second-order method on overparameterized neural networks. Preliminary experiments on regression tasks demonstrate that for training standard networks, our GGN algorithm converges much faster and achieves better performance than SGD.","layer":2,"vector":[-0.027,0.0092,0.0348,-0.0106,0.0225,0.0228,-0.0012,0.0198,0.0083,-0.028,-0.0076,-0.0439,0.0377,0.0765,0.0236,0.0014,0.0467,0.0301,-0.0469,0.0029,0.0624,-0.0385,0.0013,-0.0812,0.0206,-0.0141,-0.074,-0.043,-0.0547,-0.2398,0.0163,-0.0612,0.0406,-0.0076,0.0231,-0.015,-0.0003,0.055,-0.0247,0.0336,0.0098,0.0297,-0.0579,-0.0478,0.0523,-0.0399,-0.0182,-0.0425,-0.0057,-0.0297,0.0355,-0.0149,0.026,0.0126,0.0246,0.0046,0.0485,0.038,0.0587,0.0066,0.015,0.012,-0.1847,0.0425,0.0004,0.0606,-0.0436,-0.0212,-0.0041,0.0739,-0.0104,0.0396,0.013,0.0348,-0.0082,0.0127,0.039,-0.0084,-0.0006,0.0061,0.0562,-0.0197,0.009,-0.0157,-0.0031,-0.0682,0.0183,-0.035,0.0218,0.0196,-0.0153,-0.0093,-0.0519,0.0218,-0.0862,0.0033,0.003,0.0472,-0.0267,0.1998,-0.031,0.0127,0.0353,-0.0507,0.0139,-0.0051,-0.056,-0.0374,0.0271,-0.0008,-0.0264,-0.0404,0.0217,-0.0024,-0.0025,0.0133,0.0477,0.0251,-0.0034,0.0025,-0.0061,-0.0027,0.0492,-0.0462,0.031,-0.075,-0.036,0.1232,0.0399,-0.0004,0.0379,-0.0499,-0.0612,-0.05,0.0365,0.0362,0.0551,0.0007,0.0051,-0.0165,-0.039,-0.0466,-0.0016,-0.0479,-0.0574,0.1223,-0.054,-0.0101,-0.0245,-0.0267,-0.0018,-0.004,-0.0227,-0.021,0.034,0.0429,0.0564,0.0405,-0.0579,-0.013,-0.0411,-0.028,-0.0717,0.092,0.0394,-0.0617,-0.0273,0.0093,0.0165,-0.0014,0.0709,0.048,-0.0339,-0.0308,0.0676,0.0327,-0.0239,0.0433,-0.0293,0.016,0.0225,-0.0531,-0.0273,0.0722,0.0601,-0.0585,0.0056,-0.0719,-0.0085,0.0202,-0.0383,0.0467,-0.0553,0.021,-0.0225,-0.0461,-0.0191,-0.0265,0.0357,-0.0507,-0.016,-0.0058,-0.0544,0.0014,-0.0054,-0.0111,0.029,-0.0199,0.0225,0.032,-0.0572,0.012,0.0646,-0.0454,-0.0424,0.023,-0.0024,0.0629,-0.0314,0.0551,0.0258,-0.022,-0.0626,-0.2215,-0.0155,0.0217,-0.0354,0.0756,-0.0532,0.0484,-0.0111,0.0523,0.0644,0.0666,0.0118,-0.0256,0.0058,-0.0106,0.0571,0.0665,0.0,0.0149,-0.0042,0.0306,0.0094,0.003,-0.08,0.083,0.0011,0.1806,0.0167,0.0864,-0.0312,0.0395,0.0346,0.0079,-0.0752,0.0773,0.0451,0.1171,-0.001,-0.0399,-0.0439,-0.0431,0.007,0.029,-0.0852,-0.0471,-0.0461,-0.0416,0.0113,-0.0476,-0.0048,0.053,-0.0312,0.1101,-0.0214,0.0176,-0.0278,-0.1039,0.052,-0.0357,0.0205,0.0233,-0.0974,0.0053,-0.0546,0.0571,-0.0037,-0.0271,-0.0371,0.0471,0.0111,0.0148,0.0872,-0.0197,0.0307,0.0599,-0.0083,0.0173,0.026,-0.0879,-0.01,0.0248,-0.0247,0.0535,-0.0227,0.0463,0.025,0.0799,-0.0426,0.0472,-0.0506,-0.0383,-0.0359,-0.0527,0.0153,0.0304,0.0156,-0.2737,0.0663,-0.0104,0.0224,0.0091,-0.0135,0.0399,-0.0019,-0.0579,0.0163,-0.0214,0.0462,0.0547,0.014,0.0349,-0.0149,0.0424,-0.0375,0.0608,-0.053,0.0174,0.0564,0.1984,-0.0776,0.0121,-0.0087,-0.0426,-0.0055,0.073,-0.0182,0.0119,-0.0008,0.0874,-0.05,0.0445,0.1034,-0.0616,0.0414,0.0568,-0.0024,0.0364,-0.0195,-0.0696,-0.059,0.1088,-0.0479,-0.0325,-0.0166,0.0036,0.0448,-0.0428,0.0319,0.0006,0.0052,0.0189,-0.0316,-0.0494,-0.0544,-0.0483,-0.0542,0.0096,-0.0827,-0.0399,-0.038,0.0084]}
{"key":"[GAP++: Learning to generate target-conditioned adversarial examples] Adversarial examples are perturbed inputs which can cause a serious threat for machine learning models. Finding these perturbations is such a hard task that we can only use the iterative methods to traverse. For computational efficiency, recent works use adversarial generative networks to model the distribution of both the universal or image-dependent perturbations directly. However, these methods generate perturbations only rely on input images. In this work, we propose a more general-purpose framework which infers target-conditioned perturbations dependent on both input image and target label. Different from previous single-target attack models, our model can conduct target-conditioned attacks by learning the relations of attack target and the semantics in image. Using extensive experiments on the datasets of MNIST and CIFAR10, we show that our method achieves superior performance with single target attack models and obtains high fooling rates with small perturbation norms.","layer":1,"vector":[-0.0153,-0.0225,-0.0092,-0.0151,0.0473,0.0171,0.051,-0.0156,-0.0009,-0.0084,0.0241,-0.0498,0.0448,0.0716,0.0214,0.024,-0.0056,0.0496,-0.0794,0.0008,0.0322,-0.031,0.0484,-0.0385,0.001,0.0023,-0.011,-0.0006,-0.0195,-0.2189,0.009,-0.0463,0.02,-0.0534,-0.0144,-0.0204,-0.0782,0.0417,-0.0235,0.0237,0.0227,0.0619,-0.0319,-0.0528,-0.024,-0.0079,-0.0366,-0.0489,-0.0146,-0.0506,0.0437,-0.0543,0.0405,0.0019,0.037,-0.0173,0.0833,0.0535,0.0287,0.0827,0.0045,0.0561,-0.1595,0.0298,0.0488,0.0256,-0.0306,0.0099,0.0025,0.0574,-0.0201,0.0415,-0.0101,0.0336,-0.0098,0.0032,-0.0109,-0.028,-0.0453,-0.0075,0.0577,-0.0001,-0.0317,-0.0055,0.016,-0.0343,0.0375,-0.0484,0.0554,0.0178,-0.0011,-0.03,-0.0295,0.0334,-0.0315,-0.0184,0.0458,-0.0054,-0.1007,0.1779,-0.0785,0.0342,0.027,-0.0567,0.0709,-0.0029,-0.0634,-0.0575,-0.0191,-0.0305,0.0022,-0.0309,0.0351,-0.0324,0.0418,-0.0286,0.0699,0.0045,-0.0401,-0.0301,-0.037,-0.0034,0.0251,-0.0172,0.0631,-0.0712,0.0352,0.1382,0.0425,0.0254,0.0213,-0.0386,-0.0152,0.0253,0.0293,0.0118,-0.0213,0.0238,0.0346,0.0237,-0.0608,-0.0366,0.0132,-0.0672,-0.0144,0.0965,-0.0222,0.0511,-0.0771,-0.0342,-0.0002,0.0249,-0.0167,0.0165,0.0019,0.0577,0.0186,0.0507,0.0055,-0.0207,-0.0142,-0.0944,-0.031,0.1085,0.0191,-0.0952,-0.0103,0.018,0.0161,0.0069,0.0051,0.0423,-0.0037,0.0122,0.0338,0.0275,-0.0768,-0.0,-0.0193,0.063,-0.029,-0.0915,-0.0299,0.034,0.0504,-0.0183,0.0043,-0.0692,0.0301,0.0429,-0.0506,0.059,-0.0512,-0.0295,-0.0422,-0.0639,-0.0429,-0.0212,-0.0019,-0.035,-0.006,0.0229,-0.0288,-0.0136,-0.0302,0.0276,0.0007,-0.04,0.012,0.0098,-0.0282,0.0018,0.0525,-0.0202,-0.0161,-0.0127,-0.0069,0.0516,0.0104,0.0656,0.0069,-0.0254,-0.0345,-0.2648,0.0107,-0.015,-0.0821,0.0475,-0.0937,0.0373,-0.0197,0.0182,0.0395,0.0677,-0.026,-0.0181,0.0016,0.0054,0.0661,-0.0319,0.0315,0.0217,0.0126,-0.0444,0.0415,-0.0072,-0.1104,0.0465,-0.0167,0.2164,0.0936,0.0289,-0.0278,0.0343,0.0215,-0.0535,-0.0635,0.0709,-0.009,0.0567,-0.0008,-0.0141,-0.0054,-0.0254,0.0764,0.01,-0.1091,0.0084,-0.0348,-0.0284,0.0353,-0.0316,0.0663,0.0648,0.0079,0.0648,-0.0075,-0.0008,-0.0541,-0.1201,0.0243,-0.0574,0.0486,0.0113,-0.0437,0.017,-0.0977,0.0615,0.0423,-0.0283,-0.0747,0.0522,-0.0067,-0.0188,0.0832,0.0559,-0.0019,0.0314,-0.0066,0.0008,-0.0455,-0.0836,-0.0421,0.0531,-0.0028,0.0215,0.0052,0.0332,-0.0196,0.0934,-0.0219,0.0783,-0.0654,0.0209,0.0017,-0.0524,-0.0008,0.0381,0.0016,-0.2771,0.0422,0.0286,0.1028,-0.0069,0.0355,0.0577,0.0036,-0.0729,-0.0037,-0.0319,0.0259,0.0421,-0.036,-0.0152,0.0267,0.0591,-0.0317,0.0456,-0.0441,0.0226,0.0239,0.1959,-0.0196,0.0149,-0.0159,-0.0004,0.0491,0.0174,-0.011,0.0111,0.002,0.0594,-0.0109,0.007,0.0756,-0.0395,0.0486,-0.0119,-0.0234,-0.0273,0.0002,-0.0539,0.0445,0.0573,0.0172,0.0244,-0.0148,-0.0277,-0.0034,-0.0314,0.0265,0.0321,0.008,0.0796,0.0083,-0.0451,-0.0495,-0.0456,0.0271,0.0115,-0.0122,-0.0322,0.0006,-0.0538]}
{"key":"[Tactile Object Pose Estimation from the First Touch with Geometric Contact Rendering] In this paper, we present an approach to tactile pose estimation from the first touch for known objects. First, we create an object-agnostic map from real tactile observations to contact shapes. Next, for a new object with known geometry, we learn a tailored perception model completely in simulation. To do so, we simulate the contact shapes that a dense set of object poses would produce on the sensor. Then, given a new contact shape obtained from the sensor output, we match it against the pre-computed set using the object-specific embedding learned purely in simulation using contrastive learning. This results in a perception model that can localize objects from a single tactile observation. It also allows reasoning over pose distributions and including additional pose constraints coming from other perception systems or multiple contacts. We provide quantitative results for four objects. Our approach provides high accuracy pose estimations from distinctive tactile observations while regressing pose distributions to account for those contact shapes that could result from different object poses. We further extend and test our approach in multi-contact scenarios where several tactile sensors are simultaneously in contact with the object. Website: http://mcube.mit.edu/research/tactile_loc_first_touch.html","layer":0,"vector":[-0.0091,-0.0107,0.0575,-0.0347,-0.0456,0.0189,0.0477,0.022,0.0308,-0.0029,-0.0124,-0.1031,0.0183,0.0726,-0.0184,-0.029,-0.0057,0.0532,-0.0437,0.0126,0.0399,-0.0552,-0.0146,-0.0695,-0.0026,0.017,-0.0393,0.0122,-0.0263,-0.2346,-0.0217,-0.0139,0.0302,0.0004,-0.0104,-0.0189,-0.0215,0.047,-0.0056,0.0424,0.0092,-0.0184,-0.0345,-0.0837,-0.022,-0.0219,-0.0234,0.0339,0.0066,-0.084,0.0574,-0.0112,0.0345,0.0149,0.05,0.0033,0.0286,0.0277,0.0069,0.0277,0.054,0.0126,-0.145,0.1193,0.0692,0.0525,0.0078,-0.0308,0.0448,0.0063,0.0033,0.0282,0.0091,0.0641,0.0315,-0.0583,0.0055,-0.0263,-0.0276,-0.0166,-0.0253,0.007,-0.0305,0.0312,-0.0143,-0.0421,0.0233,-0.0636,0.0197,-0.0049,-0.0708,0.0112,-0.0678,0.0122,-0.0675,-0.0375,0.0334,0.0149,-0.0091,0.194,-0.0579,0.032,0.0611,-0.0161,0.0148,-0.0424,-0.0118,-0.0536,-0.0565,0.0294,-0.0033,-0.0266,-0.0313,-0.0009,0.028,-0.0006,0.0901,0.0173,0.044,-0.063,0.007,0.0227,0.0233,-0.03,0.0167,-0.0934,0.01,0.1588,0.0304,-0.0028,0.0388,-0.0594,-0.0242,-0.035,0.034,0.0226,0.0129,0.0113,0.0479,0.0338,0.0091,-0.0721,0.0257,-0.0877,-0.032,0.1004,-0.068,0.0404,-0.0463,-0.0038,0.0123,0.0192,-0.0144,0.0277,0.0277,-0.0005,0.0292,0.0321,-0.0623,0.0295,-0.0066,-0.0244,-0.0175,0.0702,-0.0133,-0.0973,-0.0045,0.0086,0.0295,-0.0005,0.0561,0.0681,-0.0356,0.0403,0.0979,0.0224,-0.1241,0.0004,0.0367,0.0285,0.0338,-0.05,-0.0216,-0.0266,0.0137,-0.042,0.0197,-0.0065,0.0078,0.0402,0.0426,0.0345,-0.0162,0.0168,-0.0012,-0.004,-0.0248,-0.0089,-0.0051,-0.0044,0.0149,0.0249,-0.0707,0.0184,-0.0057,-0.0033,0.0055,0.0208,0.0471,0.0118,-0.0386,0.0145,0.0604,-0.0674,-0.018,0.0184,0.0302,0.0257,-0.0166,0.0404,-0.0031,-0.0751,-0.0148,-0.2324,0.0495,0.0058,0.0053,0.0295,-0.0551,0.0001,-0.0473,0.0574,-0.035,0.0499,-0.023,0.008,0.0634,-0.0038,0.03,0.0352,0.0323,-0.0309,-0.0387,-0.0589,0.0086,-0.0648,-0.0402,0.0433,-0.0143,0.2166,0.0393,-0.0155,-0.0525,-0.021,0.0151,-0.0445,-0.0885,0.0445,0.0406,0.0595,-0.0,-0.0111,-0.0399,-0.055,0.0273,-0.0283,-0.0735,0.0111,-0.0519,-0.0665,0.0008,-0.0499,0.0041,0.0225,-0.0222,0.0288,-0.0381,-0.0482,-0.0579,-0.0787,0.0071,-0.0522,0.0616,-0.0013,-0.0162,0.0407,-0.0692,0.06,0.0404,-0.0054,-0.0854,0.0502,-0.0659,-0.0262,0.0652,0.0344,0.0454,0.0245,0.0208,0.0778,-0.0071,-0.0004,-0.0748,0.0476,-0.013,-0.0072,0.0378,0.0008,0.0227,0.111,0.0021,0.0293,-0.0522,0.011,0.0093,-0.0189,-0.0024,0.0792,0.0059,-0.321,0.0133,0.0387,0.0577,-0.0257,0.0426,0.0632,-0.0107,0.0001,-0.0323,-0.0188,0.0338,0.0491,0.0011,0.0318,0.0048,0.0733,-0.0584,0.0316,-0.0816,-0.0213,0.0282,0.1982,-0.0104,0.0382,0.0482,-0.0728,-0.0449,0.0557,-0.0476,-0.0098,0.0357,0.018,-0.0464,0.0114,0.0718,-0.0371,-0.0071,-0.0124,0.0117,0.0184,0.0014,-0.0191,-0.0282,0.1115,0.0084,-0.0236,0.0433,-0.0057,0.0111,-0.0372,0.0244,0.0094,0.0276,0.0727,0.0053,-0.0458,-0.0296,-0.0448,-0.0029,0.035,-0.0486,-0.008,-0.0171,0.0275]}
{"key":"[Few-shot Learning with LSSVM Base Learner and Transductive Modules] The performance of meta-learning approaches for few-shot learning generally depends on three aspects: features suitable for comparison, the classifier ( base learner ) suitable for low-data scenarios, and valuable information from the samples to classify. In this work, we make improvements for the last two aspects: 1) although there are many effective base learners, there is a trade-off between generalization performance and computational overhead, so we introduce multi-class least squares support vector machine as our base learner which obtains better generation than existing ones with less computational overhead; 2) further, in order to utilize the information from the query samples, we propose two simple and effective transductive modules which modify the support set using the query samples, i.e., adjusting the support samples basing on the attention mechanism and adding the prototypes of the query set with pseudo labels to the support set as the pseudo support samples. These two modules significantly improve the few-shot classification accuracy, especially for the difficult 1-shot setting. Our model, denoted as FSLSTM (Few-Shot learning with LSsvm base learner and Transductive Modules), achieves state-of-the-art performance on miniImageNet and CIFAR-FS few-shot learning benchmarks.","layer":1,"vector":[0.0039,-0.014,-0.0041,-0.0244,0.0637,-0.0216,0.0272,0.0536,0.0001,-0.0016,0.0405,-0.0656,0.0183,0.0625,0.0464,-0.0118,0.0612,0.03,-0.0179,-0.0158,-0.0094,-0.0138,0.0094,-0.0548,-0.0123,-0.0275,-0.0036,-0.0477,-0.0534,-0.2399,0.0212,-0.0414,0.0284,-0.0213,-0.0221,-0.0106,-0.024,0.0196,-0.0419,0.0297,0.0417,0.0201,0.0093,-0.066,-0.0453,-0.046,-0.0029,-0.0463,-0.0454,-0.0294,-0.0314,-0.0426,0.0327,0.0241,-0.004,0.0317,0.0148,0.0194,0.0474,0.0372,0.0301,0.0145,-0.1818,0.0718,-0.003,0.031,-0.0212,0.0059,0.0332,0.0645,-0.0292,0.055,-0.0015,0.0119,-0.0066,0.0029,-0.0119,0.0088,0.0023,0.0061,0.0044,-0.0303,-0.0282,-0.0138,-0.0096,-0.006,-0.0113,-0.0553,0.0562,0.0214,-0.0311,-0.0078,-0.0323,0.0243,-0.0427,-0.0303,0.0502,0.007,-0.0479,0.1956,-0.0362,0.0502,0.045,-0.0416,0.055,-0.0436,-0.0655,0.0045,-0.0797,-0.0255,-0.0345,-0.0226,0.0225,-0.0413,0.0082,0.0042,0.075,0.0499,-0.0095,0.026,-0.0294,-0.0196,0.0149,-0.0569,0.0072,-0.065,0.0603,0.1411,0.0264,0.024,0.0336,-0.013,-0.0375,-0.0452,0.0302,0.0632,0.0402,0.0176,0.0114,-0.0198,-0.0391,-0.0569,0.024,-0.0849,-0.0872,0.0823,-0.0283,0.0267,-0.0313,-0.061,0.014,0.0437,-0.0324,-0.0309,0.0366,0.0203,0.0712,0.0651,-0.0226,0.0086,-0.0299,-0.0875,-0.0336,0.0754,-0.0042,-0.0786,-0.032,-0.0214,0.0235,-0.0072,0.0454,0.0478,-0.0351,-0.0027,0.1019,0.0676,-0.0726,-0.0343,-0.032,0.046,0.0486,-0.0659,-0.0193,0.0714,0.058,0.0017,-0.0042,-0.0405,0.028,0.0214,-0.0101,0.0112,-0.0249,-0.0166,-0.0368,0.0079,-0.027,0.0038,0.0039,-0.0728,0.0212,0.0429,-0.0247,0.0918,-0.0014,-0.0096,0.0144,0.0029,0.0197,0.0275,-0.0105,0.0005,0.0739,-0.0439,-0.0466,-0.0274,0.0325,0.0286,-0.0116,0.0672,0.0471,-0.0423,-0.0341,-0.2395,-0.0208,-0.0063,-0.0471,0.0278,-0.0851,-0.0002,0.0199,0.0589,0.0668,0.0963,0.0187,-0.0346,0.0259,-0.012,0.0599,0.0475,-0.0024,-0.0034,-0.0316,-0.0135,0.007,0.0216,-0.091,0.0806,-0.0017,0.2214,0.0337,0.0214,-0.0307,0.0452,0.0197,-0.0435,-0.0677,0.0659,-0.0096,0.0452,-0.034,-0.0376,-0.0064,-0.0269,0.0154,0.0344,-0.1276,-0.0369,-0.0272,-0.0155,0.0132,-0.036,-0.0054,0.0433,-0.0121,0.0366,-0.0385,-0.0182,0.0012,-0.0997,0.0289,0.0065,0.039,0.0078,-0.0489,0.0371,-0.0836,0.0788,-0.0298,-0.0352,-0.0723,0.0555,-0.0389,-0.025,0.0791,0.0007,0.0299,0.016,-0.0031,0.0525,-0.0381,-0.0396,-0.0183,0.0821,0.0095,0.0101,0.0035,0.0352,0.045,0.0682,-0.0044,0.0053,-0.0134,0.0061,0.0361,-0.0136,0.0396,0.0676,-0.0438,-0.3086,-0.0023,0.0348,0.0231,-0.0257,-0.0099,0.034,-0.0345,-0.0214,0.0426,0.011,-0.0095,0.0673,-0.0241,-0.003,-0.007,0.0142,-0.0478,0.0541,-0.0604,0.0137,0.0685,0.2198,-0.0509,0.0451,-0.0078,-0.0376,-0.0004,0.006,0.0031,-0.014,0.0135,0.112,-0.0315,-0.0163,0.0944,-0.0476,0.0253,0.0347,-0.0065,-0.0024,-0.0199,-0.0667,-0.0161,0.0689,-0.0344,0.0152,-0.0321,-0.0279,0.0141,-0.0172,0.0419,-0.0003,-0.025,0.0264,0.0214,-0.0361,-0.0115,-0.0354,0.0024,0.0306,-0.0591,-0.0141,0.0169,0.0507]}
{"key":"[Stochastic Contextual Dueling Bandits under Linear Stochastic Transitivity Models] We consider the regret minimization task in a dueling bandits problem with context information. In every round of the sequential decision problem, the learner makes a context-dependent selection of two choice alternatives (arms) to be compared with each other and receives feedback in the form of noisy preference information. We assume that the feedback process is determined by a linear stochastic transitivity model with contextualized utilities (CoLST), and the learner's task is to include the best arm (with highest latent context-dependent utility) in the duel. We propose a computationally efficient algorithm, $\\texttt{CoLSTIM}$, which makes its choice based on imitating the feedback process using perturbed context-dependent utility estimates of the underlying CoLST model. If each arm is associated with a $d$-dimensional feature vector, we show that $\\texttt{CoLSTIM}$ achieves a regret of order $\\tilde O( \\sqrt{dT})$ after $T$ learning rounds. Additionally, we also establish the optimality of $\\texttt{CoLSTIM}$ by showing a lower bound for the weak regret that refines the existing average regret analysis. Our experiments demonstrate its superiority over state-of-art algorithms for special cases of CoLST models.","layer":6,"vector":[-0.0455,0.017,0.0565,0.0064,0.021,-0.0034,0.0765,0.037,0.0646,-0.0058,-0.0031,-0.0171,0.0141,0.0864,0.0008,0.0349,0.0062,0.0305,-0.033,0.0111,-0.0015,-0.1016,0.0168,-0.0756,0.0433,0.0234,-0.0604,-0.0318,-0.0433,-0.202,0.0226,-0.0162,0.008,-0.0455,-0.0354,-0.0455,-0.0158,0.0473,-0.0123,0.0489,-0.0033,0.0545,-0.0505,-0.1048,-0.0455,-0.0705,0.023,-0.0051,-0.0215,-0.0218,-0.0079,-0.0194,0.0557,-0.0036,0.0451,0.0231,0.041,0.0852,0.0529,0.0242,0.0069,0.053,-0.1495,0.0425,0.0033,0.0074,-0.0527,0.0593,0.0078,0.0413,0.0015,0.0197,-0.0147,0.0622,0.0612,-0.0152,-0.0158,-0.0408,-0.02,0.0429,0.0014,-0.0077,-0.0587,0.0208,-0.0265,-0.0606,-0.0057,-0.051,0.0621,0.0031,-0.0025,0.0014,-0.027,0.0236,-0.0879,-0.037,0.047,0.0118,-0.0636,0.1999,-0.0059,0.0422,-0.0066,-0.0271,0.0695,-0.0076,-0.0506,-0.0223,-0.0199,-0.0095,-0.0029,-0.0305,0.0287,-0.0384,0.0115,0.0061,0.0468,0.0682,-0.0168,0.0175,-0.0594,0.0276,0.0778,-0.0139,0.0136,-0.104,0.0108,0.1501,0.0301,0.0198,0.0494,-0.0703,-0.0347,-0.0388,0.0361,-0.0043,-0.0296,0.0059,0.0318,-0.0522,-0.0273,-0.0366,0.0327,-0.0763,-0.0577,0.095,0.0027,-0.0011,-0.0357,-0.0181,0.0388,-0.0059,-0.0318,-0.0238,0.018,0.0452,0.0195,0.0457,-0.0479,0.0311,-0.0339,-0.0471,0.0403,0.0823,0.0268,-0.0707,-0.0507,0.0364,-0.0038,0.0084,0.0252,0.0327,-0.0786,0.0323,0.0749,0.0573,-0.1007,-0.0181,0.0292,0.0115,0.0526,-0.0218,-0.0159,-0.0021,0.0393,-0.029,-0.0031,-0.0645,0.0179,0.0545,-0.0108,-0.0239,-0.0186,-0.0222,-0.0372,-0.0398,-0.0344,-0.0092,0.0004,0.0178,-0.0081,0.0024,-0.0567,-0.0054,0.0197,0.025,0.0289,-0.0319,0.0034,-0.0007,-0.0355,0.0077,0.0438,-0.0289,-0.007,-0.0138,0.0626,0.049,-0.0337,0.0379,0.041,-0.0023,0.0264,-0.2474,-0.017,-0.0277,0.0019,0.0267,-0.0366,0.0481,-0.0491,0.0571,0.0633,0.0533,-0.0289,-0.006,0.0336,0.0042,0.087,-0.01,0.0037,-0.0029,-0.0088,-0.0245,0.0176,-0.0052,-0.0406,0.0363,-0.0187,0.2419,0.0522,0.0102,0.0112,0.0253,0.0543,-0.0261,-0.0774,0.0237,0.0376,0.0572,-0.0192,-0.0171,-0.0534,-0.0098,0.0186,0.0156,-0.0988,-0.0581,-0.0484,-0.0186,0.0026,-0.0456,0.0363,0.0159,-0.0054,0.0496,-0.0278,-0.0104,-0.0294,-0.1165,0.0343,-0.0148,0.0098,0.0245,-0.0679,0.0099,-0.0949,0.0358,0.0047,0.0617,-0.0566,0.0137,-0.022,0.0075,0.0362,-0.0112,0.0001,0.0022,0.018,0.0558,-0.0639,-0.0562,-0.0043,0.0615,-0.0427,0.0531,0.0483,0.0035,-0.0039,0.0973,0.0061,0.0076,0.0232,-0.0154,0.0121,-0.0496,0.0368,0.0686,-0.0245,-0.3253,0.0493,0.0427,0.0292,-0.0266,0.0187,0.0644,-0.0064,-0.0441,-0.0029,0.0339,0.0755,0.0161,-0.0171,0.024,0.012,0.0803,-0.0281,0.02,-0.0745,0.0239,0.0539,0.1917,-0.052,0.0405,0.0163,-0.029,0.0096,0.0258,-0.0241,-0.001,0.0233,0.0807,-0.0907,0.0341,0.0757,-0.0199,-0.003,0.009,-0.031,-0.0382,-0.0102,-0.0491,-0.0059,0.0937,0.0091,0.0083,-0.0559,-0.0517,0.017,-0.0427,0.0104,-0.0332,-0.0468,0.0136,-0.006,-0.0206,-0.0688,0.0157,-0.0238,0.0132,-0.043,-0.0005,-0.0058,-0.0046]}
{"key":"[Disentangling the Impacts of Language and Channel Variability on Speech Separation Networks] Because the performance of speech separation is excellent for speech in which two speakers completely overlap, research attention has been shifted to dealing with more realistic scenarios. However, domain mismatch between training/test situations due to factors, such as speaker, content, channel, and environment, remains a severe problem for speech separation. Speaker and environment mismatches have been studied in the existing literature. Nevertheless, there are few studies on speech content and channel mismatches. Moreover, the impacts of language and channel in these studies are mostly tangled. In this study, we create several datasets for various experiments. The results show that the impacts of different languages are small enough to be ignored compared to the impacts of different channels. In our experiments, training on data recorded by Android phones leads to the best generalizability. Moreover, we provide a new solution for channel mismatch by evaluating projection, where the channel similarity can be measured and used to effectively select additional training data to improve the performance of in-the-wild test data.","layer":1,"vector":[0.0047,0.027,0.05,-0.0642,-0.009,-0.0193,0.0158,0.0177,0.038,-0.0136,-0.0004,-0.0315,0.0925,0.0506,0.0698,0.005,0.0329,0.0009,-0.0866,0.0423,0.0303,-0.0182,0.0218,-0.03,0.0341,0.0295,-0.0488,-0.0468,0.0081,-0.2691,0.0135,-0.0069,0.0522,-0.0527,-0.0132,-0.0236,-0.0548,0.0403,-0.0162,0.0227,-0.0029,0.0241,0.0248,-0.0701,-0.0467,-0.0552,-0.0832,-0.0015,-0.0316,-0.0613,0.0329,-0.0326,0.0437,0.0249,-0.0114,0.0619,0.0527,0.0921,0.0215,0.0672,0.0175,0.0272,-0.1474,0.0675,0.0496,0.0495,-0.026,0.0207,0.003,0.0488,-0.0381,0.0502,0.0672,0.0309,0.0427,0.011,0.0106,-0.0227,0.0125,0.0149,0.0182,-0.0478,-0.0577,0.0001,0.0194,-0.0257,0.0163,-0.0069,-0.0625,-0.0187,-0.0811,0.0011,-0.0088,0.0597,-0.0372,-0.0396,0.0381,-0.0005,-0.0185,0.2318,-0.0007,0.0339,0.0259,-0.0605,0.0202,-0.0457,-0.0715,-0.004,0.0012,0.0286,0.015,-0.0608,0.0224,-0.0383,0.0388,0.0236,0.0951,-0.0015,0.0014,-0.0111,-0.0345,-0.0308,0.0279,-0.04,0.0387,-0.0521,0.0285,0.1302,0.0082,0.0126,0.071,-0.0374,-0.0368,-0.0352,-0.0144,0.0139,0.0118,-0.0057,0.0257,0.0094,0.0249,-0.114,0.0388,-0.07,-0.0548,0.1174,-0.0838,0.0343,-0.0139,0.01,-0.0361,0.0247,0.0165,-0.0309,0.0374,-0.0088,0.0412,0.0314,-0.0299,0.0033,0.0478,-0.033,-0.021,0.0781,0.0329,-0.0931,-0.0755,0.0216,0.0117,-0.0322,0.0109,0.0602,-0.0295,0.0035,0.0069,-0.0056,-0.0572,0.031,-0.0039,-0.0077,0.0135,-0.035,-0.0301,0.0422,-0.0241,-0.0717,-0.0182,-0.0254,0.0282,0.046,-0.0583,0.0055,-0.0206,-0.0255,-0.04,0.0047,-0.0038,-0.0391,-0.0129,-0.0118,0.0024,0.0439,-0.0476,-0.0079,0.0075,0.0351,0.0028,-0.0393,0.037,0.0556,-0.0078,-0.0399,0.0654,-0.0125,0.0199,-0.0271,0.0269,0.028,0.0242,0.0282,-0.0044,-0.0199,-0.0036,-0.2392,-0.0083,0.0103,-0.0034,0.0675,-0.0586,0.0255,0.0135,0.0829,0.0529,0.0077,-0.0294,-0.0177,0.0268,-0.0259,0.0448,0.0042,0.0407,-0.0106,0.0042,0.0033,-0.021,-0.0346,-0.0759,0.0796,-0.0234,0.174,-0.0131,0.0416,-0.0192,0.0148,0.0528,-0.0373,-0.123,0.0621,0.0264,0.0976,-0.0327,-0.0228,-0.0405,-0.0283,0.025,0.0076,-0.0948,-0.0426,-0.0457,-0.0597,-0.0009,-0.0817,0.0275,0.0595,-0.0222,0.047,0.0077,0.0028,0.0159,-0.0939,0.043,-0.0402,0.0285,0.0014,0.012,0.0132,-0.0594,0.0528,-0.0027,-0.021,-0.0163,0.0394,0.0611,-0.0508,0.0612,-0.0299,-0.0085,0.0433,0.0366,0.0077,-0.0711,-0.035,-0.0402,0.0783,-0.0026,0.0829,-0.004,0.031,0.0159,0.0875,0.0084,0.0544,-0.0561,0.0184,0.0165,-0.0216,-0.0408,0.0406,0.0054,-0.2586,0.02,0.0229,0.0344,-0.035,0.0202,0.0049,0.005,-0.0618,0.0264,0.0219,0.0163,0.0361,-0.0431,0.0403,0.0778,0.1105,-0.0288,0.0432,-0.0114,0.0267,0.0061,0.1778,-0.0455,0.0198,-0.0186,-0.0321,-0.0081,0.0377,-0.0494,0.0082,0.0275,0.1309,-0.0389,-0.0056,0.0661,-0.0179,0.0262,0.0282,-0.0463,-0.0174,-0.0365,-0.0332,-0.014,0.0821,-0.0186,-0.0205,-0.0689,0.0078,0.0213,-0.0579,-0.0224,-0.0375,0.0224,-0.0078,0.0586,-0.035,-0.0502,-0.0877,-0.0564,0.0135,-0.0763,-0.0317,-0.0024,-0.0192]}
{"key":"[Deep Exploration via Randomized Value Functions] We study the use of randomized value functions to guide deep exploration in reinforcement learning. This offers an elegant means for synthesizing statistically and computationally efficient exploration with common practical approaches to value function learning. We present several reinforcement learning algorithms that leverage randomized value functions and demonstrate their efficacy through computational studies. We also prove a regret bound that establishes statistical efficiency with a tabular representation.","layer":0,"vector":[-0.0539,-0.011,0.0424,-0.0481,-0.0034,0.0496,0.0348,0.0464,0.0554,-0.0238,0.0075,-0.0332,0.0703,0.059,0.0295,0.0052,-0.0581,0.0328,-0.0538,-0.0078,0.0471,-0.0593,-0.0039,-0.0897,0.0041,-0.001,-0.0494,-0.0315,-0.0427,-0.2133,0.0253,-0.0353,0.0162,-0.0672,-0.0214,-0.0197,-0.0363,0.053,-0.0172,0.0597,0.0474,0.0028,-0.0251,-0.0505,-0.0338,-0.0562,-0.0043,-0.0531,0.0115,-0.0257,-0.0045,-0.012,0.0036,0.0052,0.0352,0.0382,0.0516,0.0581,0.0658,0.0502,0.0483,0.0262,-0.1547,0.05,0.0329,0.04,-0.0266,-0.049,0.009,0.0268,-0.005,0.0547,0.0061,0.0294,0.053,-0.0054,-0.0121,-0.0521,-0.0025,-0.0292,0.0343,-0.0473,-0.049,0.0272,-0.0304,-0.0847,0.0091,-0.0494,0.0463,0.0283,0.0051,0.0199,-0.0386,0.0394,-0.0458,0.0266,0.0507,-0.0074,-0.0507,0.2047,-0.0255,0.0729,0.015,0.0066,0.0396,0.0044,-0.0645,-0.0034,-0.0421,-0.0232,-0.0173,-0.0474,0.0587,-0.056,-0.0247,0.0235,0.0651,0.0404,0.0173,-0.0509,-0.0426,-0.0106,0.0777,0.0071,0.0162,-0.065,0.0343,0.1676,0.0173,0.025,0.0272,-0.0529,-0.0231,-0.0027,0.0254,0.0464,-0.0017,0.0,0.0349,-0.0011,-0.057,0.0254,0.0037,-0.124,-0.0834,0.0912,-0.0043,0.0054,-0.0017,-0.0388,-0.0004,-0.0121,-0.0106,-0.0216,0.0178,0.0364,0.0334,0.0461,-0.055,0.0043,-0.0366,-0.0327,-0.0016,0.1136,-0.0285,-0.0635,-0.0345,-0.0041,-0.0196,-0.0143,0.0032,0.0242,-0.0758,0.0332,0.1016,0.0283,-0.1018,0.0096,-0.0083,0.0108,0.0148,-0.0255,-0.0228,0.0306,0.0343,-0.0503,0.0063,-0.0447,0.0294,0.0409,-0.0108,0.041,-0.0369,-0.0109,-0.0186,-0.0838,-0.0026,-0.0293,0.0073,0.0275,0.0063,-0.038,-0.0377,-0.0019,0.0113,0.0023,-0.0088,-0.0252,0.0686,0.0082,-0.0328,0.0404,0.0517,-0.0054,-0.0431,-0.0149,0.0316,0.021,-0.0072,-0.0113,0.0486,-0.0237,-0.0298,-0.2443,-0.0057,-0.0278,-0.0066,0.0488,-0.0482,0.0154,-0.0179,0.032,0.0708,-0.0003,-0.0878,-0.0309,0.0325,-0.0005,0.0478,0.0517,-0.005,-0.0282,0.0154,0.0224,0.0143,-0.0077,-0.1056,0.0462,0.0086,0.2393,0.0176,0.0286,-0.0163,0.053,0.048,0.0376,-0.0802,0.0273,0.0129,0.0866,0.0196,-0.0341,-0.0315,0.0054,0.028,-0.0057,-0.1152,-0.0145,-0.028,-0.0278,0.0155,-0.0434,0.0058,0.0214,-0.0299,0.0275,-0.0053,-0.0123,-0.0311,-0.0638,0.0366,-0.0468,0.0073,-0.0031,-0.0687,0.012,-0.0541,0.0591,-0.0116,0.0445,-0.0516,0.074,-0.0278,-0.0362,0.0173,-0.0212,0.021,0.025,0.0241,0.0001,-0.0057,-0.0286,-0.0132,0.0647,-0.0109,-0.0082,-0.0221,-0.024,-0.0177,0.0648,-0.0326,0.0397,-0.0011,-0.0352,0.0028,-0.0497,-0.0121,0.0717,-0.0136,-0.2935,0.0637,0.0153,-0.0012,-0.0266,0.0127,0.0385,-0.0218,-0.0474,0.0174,0.0191,0.0569,0.0733,-0.0147,0.0271,0.025,0.0978,-0.0059,0.0707,-0.0797,0.0399,0.0262,0.2269,-0.0517,0.0275,0.0278,-0.0597,-0.0037,0.0404,-0.0081,0.0299,0.0291,0.0698,-0.0695,0.0451,0.1089,-0.0186,0.0145,0.0249,0.005,-0.0236,0.0121,0.0016,-0.0155,0.1077,0.0072,0.0064,-0.0317,-0.0132,0.0475,-0.0413,0.0111,-0.0129,-0.0462,0.0449,0.0318,-0.0117,-0.0526,-0.0037,-0.0271,0.0071,-0.057,0.0266,-0.0336,-0.0251]}
{"key":"[Predicting Length of Stay in the Intensive Care Unit with Temporal Pointwise Convolutional Networks] The pressure of ever-increasing patient demand and budget restrictions make hospital bed management a daily challenge for clinical staff. Most critical is the efficient allocation of resource-heavy Intensive Care Unit (ICU) beds to the patients who need life support. Central to solving this problem is knowing for how long the current set of ICU patients are likely to stay in the unit. In this work, we propose a new deep learning model based on the combination of temporal convolution and pointwise (1x1) convolution, to solve the length of stay prediction task on the eICU critical care dataset. The model - which we refer to as Temporal Pointwise Convolution (TPC) - is specifically designed to mitigate for common challenges with Electronic Health Records, such as skewness, irregular sampling and missing data. In doing so, we have achieved significant performance benefits of 18-51% (metric dependent) over the commonly used Long-Short Term Memory (LSTM) network, and the multi-head self-attention network known as the Transformer.","layer":6,"vector":[-0.0543,-0.0158,0.027,-0.014,0.0184,0.0432,0.0412,0.0322,0.0361,0.0063,-0.0004,-0.0502,-0.0147,0.0786,0.0084,0.0302,-0.0059,0.035,-0.0292,0.0835,0.05,-0.0302,0.0121,-0.0255,0.038,-0.0091,0.0166,-0.0584,-0.0499,-0.235,0.0182,-0.0358,0.0318,-0.0091,-0.0118,-0.0256,-0.0271,0.0592,-0.0172,0.0792,0.0189,0.0554,0.009,-0.0657,-0.0241,-0.0638,-0.0117,-0.0582,0.0297,0.0238,0.02,-0.0245,0.0348,0.0751,0.021,0.0229,0.081,-0.0122,0.0737,0.0123,0.035,0.0225,-0.1795,0.0322,0.0142,0.0089,-0.0603,-0.0241,-0.0011,0.0209,-0.0113,-0.0198,-0.0192,0.0138,-0.0191,0.0335,0.0622,0.0061,-0.0045,-0.0018,0.0304,-0.0634,-0.0531,-0.0468,-0.0289,-0.0638,-0.0025,-0.0452,0.0057,0.0014,-0.0352,-0.0144,-0.0537,0.0365,-0.0493,-0.0709,0.0317,0.0482,-0.0701,0.2075,-0.0501,0.0316,0.0422,-0.0329,0.035,-0.0416,-0.039,-0.0161,-0.0048,0.0102,-0.0338,-0.0353,0.0322,-0.0568,0.0348,0.0084,0.0883,0.0324,0.0319,-0.0059,-0.0101,0.006,0.0485,-0.0167,0.0128,-0.0683,0.0526,0.1321,-0.0024,-0.0028,0.0618,0.0075,-0.0492,-0.0096,-0.0194,0.0613,0.0432,-0.0137,0.0154,-0.0582,-0.0194,-0.0416,0.0067,-0.095,-0.0336,0.1403,-0.0111,-0.004,-0.0715,-0.041,-0.006,0.0249,0.0048,-0.028,0.0369,0.0162,0.0322,0.0091,-0.0324,0.0151,-0.0099,-0.0545,-0.0288,0.1007,0.0013,-0.0844,-0.0164,-0.01,0.0072,0.0007,0.0509,0.0077,-0.0231,-0.001,0.1289,0.054,-0.0706,0.0074,0.0013,0.0231,-0.0245,-0.0194,-0.0129,0.0394,0.0311,-0.0575,0.0517,-0.0313,0.029,0.0697,-0.0411,0.0468,-0.0348,0.0172,0.0107,-0.0477,-0.0169,0.0116,0.0267,-0.0213,-0.0179,-0.0406,0.0013,0.0298,0.0207,-0.0112,-0.0313,0.0378,0.0411,0.0099,-0.0402,0.0149,0.0676,-0.0389,-0.0275,-0.0067,0.0119,-0.0016,-0.0086,0.0478,0.0479,-0.0394,-0.0328,-0.2084,-0.0093,0.0127,-0.0527,0.0311,-0.0675,0.0108,-0.0157,0.0204,0.0174,0.0937,-0.024,-0.0259,-0.017,-0.0273,0.0435,0.0312,0.0473,-0.0287,-0.0241,0.0228,0.0227,-0.0031,-0.0885,0.0307,0.0196,0.2319,-0.0197,0.05,-0.0033,0.0008,0.0274,-0.0498,-0.1121,0.0743,0.0171,0.0459,-0.0248,-0.0269,-0.0605,-0.0531,0.0469,-0.0164,-0.083,-0.0381,-0.0041,-0.0187,0.0452,-0.0905,0.0088,0.0343,-0.0646,0.0695,0.0073,0.0044,-0.0562,-0.084,0.038,-0.0597,0.0318,0.0142,-0.0582,0.0294,-0.0618,0.037,-0.0244,-0.0513,-0.0329,0.0037,-0.0435,-0.0202,0.0797,-0.0177,0.0327,0.0719,-0.0243,0.0514,-0.0037,-0.0241,0.0011,0.091,-0.0531,-0.0042,0.0366,0.0133,0.0261,0.0459,0.0588,0.0482,-0.0201,0.011,0.0014,-0.044,0.0017,-0.0146,-0.0164,-0.29,0.0737,-0.0047,0.0248,0.007,0.0077,-0.0224,0.0243,-0.0377,0.0218,-0.0027,0.0231,0.0582,-0.039,-0.0257,0.0237,0.104,-0.0772,0.0506,-0.0352,0.0004,0.0019,0.2069,-0.0415,0.0281,0.0186,-0.0195,0.0321,0.0513,-0.0287,0.0135,-0.0045,0.0904,-0.0514,0.02,0.0676,-0.0062,0.0443,0.0173,0.0332,-0.0116,0.028,-0.0077,-0.0531,0.0934,0.0013,-0.0235,-0.0326,-0.0306,-0.0023,-0.0572,0.0167,0.018,0.0313,0.0441,0.0537,-0.052,-0.0643,-0.0286,-0.0297,0.0027,-0.0793,-0.0101,-0.0383,-0.0133]}
{"key":"[Importance measures derived from random forests: characterisation and extension] Nowadays new technologies, and especially artificial intelligence, are more and more established in our society. Big data analysis and machine learning, two sub-fields of artificial intelligence, are at the core of many recent breakthroughs in many application fields (e.g., medicine, communication, finance, ...), including some that are strongly related to our day-to-day life (e.g., social networks, computers, smartphones, ...). In machine learning, significant improvements are usually achieved at the price of an increasing computational complexity and thanks to bigger datasets. Currently, cutting-edge models built by the most advanced machine learning algorithms typically became simultaneously very efficient and profitable but also extremely complex. Their complexity is to such an extent that these models are commonly seen as black-boxes providing a prediction or a decision which can not be interpreted or justified. Nevertheless, whether these models are used autonomously or as a simple decision-making support tool, they are already being used in machine learning applications where health and human life are at stake. Therefore, it appears to be an obvious necessity not to blindly believe everything coming out of those models without a detailed understanding of their predictions or decisions. Accordingly, this thesis aims at improving the interpretability of models built by a specific family of machine learning algorithms, the so-called tree-based methods. Several mechanisms have been proposed to interpret these models and we aim along this thesis to improve their understanding, study their properties, and define their limitations.","layer":0,"vector":[-0.0062,0.0093,0.0032,-0.0089,0.0756,0.019,0.0739,0.0318,0.0704,-0.02,0.0053,-0.0903,0.0257,0.057,0.0202,0.0394,0.0006,0.0133,-0.0648,0.0106,0.0468,0.0121,-0.0217,-0.0386,0.0315,0.0196,-0.0452,-0.0138,-0.0682,-0.2492,0.0185,-0.0801,0.0815,-0.0131,0.0071,-0.0246,-0.0065,0.0445,-0.0243,0.0278,0.0447,-0.0173,-0.0475,-0.0283,-0.0283,-0.0881,0.0073,-0.0128,-0.0591,-0.0306,-0.0131,-0.0335,0.0394,0.0346,0.0495,0.0052,0.0364,0.0363,0.0328,0.0351,0.0569,0.0481,-0.1423,0.0387,0.0477,0.0449,-0.0488,-0.0396,0.0227,0.0875,0.0084,-0.0003,0.0032,0.0219,-0.0008,-0.0038,0.0064,-0.0208,0.0078,0.0437,0.0011,-0.0177,-0.036,0.0019,-0.0281,-0.034,-0.0096,-0.0572,0.06,0.0332,-0.0175,0.0191,-0.0784,0.0354,-0.0396,0.0122,0.0655,-0.0276,-0.0913,0.1794,-0.0506,0.0164,0.0099,-0.0357,0.0072,-0.0252,-0.0282,-0.0427,-0.0059,-0.0111,-0.0081,-0.0366,0.0362,0.0016,-0.0251,0.0117,0.0451,0.0304,0.0358,0.0105,-0.046,0.0062,0.0573,0.0038,0.058,0.003,-0.0112,0.1211,0.0527,0.0206,0.0546,-0.0222,-0.0667,-0.0417,0.0079,-0.0015,0.0182,0.043,0.0041,-0.0033,-0.0371,-0.0562,0.0182,-0.0892,-0.0811,0.1123,-0.0446,0.011,-0.0618,-0.0287,-0.0448,0.0066,-0.0576,-0.0454,0.0235,0.0077,0.0369,-0.0088,-0.0437,0.0101,0.0208,-0.0636,-0.0459,0.098,0.0016,-0.0576,-0.0276,-0.0246,0.0213,-0.0031,0.0834,0.0412,0.0189,0.0709,0.0624,0.0074,-0.0662,-0.0581,-0.0112,-0.0023,0.0324,-0.048,-0.0214,0.0849,0.0219,-0.0238,0.004,-0.0549,0.0477,0.0531,-0.0024,-0.0021,-0.0055,-0.001,-0.0168,-0.0095,-0.0306,-0.0323,0.0048,-0.035,-0.0113,0.0098,-0.0083,0.0127,-0.06,0.0254,-0.0159,0.0242,0.0833,0.0093,-0.0142,0.0374,0.0638,-0.0343,-0.0364,0.0223,0.0443,0.0494,0.0066,0.0543,0.0428,-0.0408,-0.062,-0.1973,-0.0311,0.0282,-0.0246,-0.0044,-0.0537,-0.02,-0.0323,0.0048,0.0851,0.1018,-0.0104,-0.031,0.0263,0.002,0.0437,-0.0035,0.003,-0.0818,0.0261,-0.018,0.0397,0.0234,-0.1048,-0.0384,-0.0119,0.2386,0.0054,0.0136,0.0306,0.0163,0.0282,-0.0595,-0.1106,0.0617,0.0263,0.0385,-0.0148,-0.0157,0.0179,-0.0002,0.0345,-0.0208,-0.0991,-0.061,-0.0036,-0.0332,0.0141,-0.063,0.02,0.0349,0.0084,0.0652,0.0003,0.025,-0.021,-0.0751,0.055,-0.0285,-0.0076,-0.025,-0.042,0.0007,-0.0953,0.0087,-0.043,-0.036,-0.0267,-0.0199,-0.0523,0.0128,0.1289,-0.0191,-0.0863,0.093,0.011,0.0634,-0.0333,-0.0024,-0.0029,0.0585,-0.0366,0.0548,0.0242,0.0342,-0.005,0.1055,-0.0302,0.023,-0.0333,-0.0425,0.0056,-0.0242,-0.0723,0.0579,0.01,-0.2929,0.042,-0.0152,0.0502,-0.0041,-0.0083,0.0043,0.0353,-0.0044,0.0205,0.0278,0.0454,0.0609,-0.0502,-0.0106,0.0289,0.0788,-0.0179,0.0611,-0.0278,0.0601,0.0188,0.2138,-0.0219,0.0064,0.0455,-0.031,-0.0095,0.0241,-0.0138,0.0029,-0.0217,0.0651,-0.0381,0.0383,0.0603,-0.0349,0.0227,0.0159,-0.0295,0.0365,-0.0248,-0.0536,-0.0261,0.0992,0.0004,-0.0377,-0.0486,0.0214,0.051,-0.0337,-0.0165,-0.0323,-0.0124,0.0186,0.0406,-0.0021,-0.0286,-0.0324,-0.0373,0.0022,-0.0598,-0.0355,0.0053,-0.0004]}
{"key":"[Bounding Wasserstein distance with couplings] Markov chain Monte Carlo (MCMC) provides asymptotically consistent estimates of intractable posterior expectations as the number of iterations tends to infinity. However, in large data applications, MCMC can be computationally expensive per iteration. This has catalyzed interest in sampling methods such as approximate MCMC, which trade off asymptotic consistency for improved computational speed. In this article, we propose estimators based on couplings of Markov chains to assess the quality of such asymptotically biased sampling methods. The estimators give empirical upper bounds of the Wassertein distance between the limiting distribution of the asymptotically biased sampling method and the original target distribution of interest. We establish theoretical guarantees for our upper bounds and show that our estimators can remain effective in high dimensions. We apply our quality measures to stochastic gradient MCMC, variational Bayes, and Laplace approximations for tall data and to approximate MCMC for Bayesian logistic regression in 4500 dimensions and Bayesian linear regression in 50000 dimensions.","layer":6,"vector":[-0.0614,0.0156,0.0133,-0.0092,0.0381,0.0505,0.0194,0.0212,0.0573,0.0067,0.0357,-0.0442,0.0032,0.0793,0.0343,0.0289,-0.0222,0.0156,-0.0628,0.0141,0.0037,-0.0575,-0.0055,-0.052,0.0539,0.038,-0.02,-0.0586,-0.0389,-0.2663,0.0204,-0.0858,0.0113,-0.0727,0.0181,-0.0319,-0.0205,0.0274,0.0085,0.0394,0.0064,0.0293,-0.0165,-0.0392,-0.011,-0.0667,-0.0739,-0.0209,-0.0371,-0.0438,-0.0221,0.0001,0.051,0.0336,0.0342,0.0437,0.0496,-0.0156,0.0654,0.0413,-0.0179,0.0567,-0.1939,0.0588,0.0314,0.0062,-0.0411,-0.0137,0.0072,0.0438,-0.0367,0.029,0.0004,0.0743,0.0048,-0.0275,0.0247,-0.0333,-0.0231,0.0036,-0.0037,-0.065,-0.0302,-0.0005,-0.0534,-0.0313,0.0334,-0.0346,0.0438,0.0049,-0.0069,-0.0107,-0.0489,0.0184,-0.0957,-0.0149,0.054,0.0241,0.0038,0.1826,-0.0198,0.0606,0.0208,-0.0063,0.0258,-0.0026,-0.0115,-0.0198,-0.0021,0.0283,-0.0031,-0.0206,0.0286,-0.0538,0.0078,0.0319,0.0439,0.0175,-0.03,-0.0111,-0.0229,0.0101,0.0484,0.0197,0.0141,-0.0695,-0.0437,0.1324,0.0764,0.0262,0.0601,-0.0283,-0.0598,0.0121,0.0352,-0.0144,-0.021,-0.0097,0.0299,0.0236,-0.0327,-0.0917,-0.0084,-0.0965,-0.0464,0.1338,-0.0042,0.0441,-0.0858,-0.0476,-0.0001,0.0017,0.0132,-0.0119,0.0534,0.0247,-0.0178,0.0324,-0.0592,0.0566,-0.0424,-0.051,-0.0094,0.1,0.0482,-0.05,-0.006,0.0245,0.0215,-0.0093,0.0469,0.0271,-0.0002,0.033,0.066,-0.0127,-0.0794,0.0439,0.0144,0.0058,-0.0008,-0.0098,-0.0425,0.0271,0.0375,-0.0488,-0.008,-0.0013,0.0456,0.0928,-0.0011,-0.0543,-0.0011,0.0021,-0.0449,-0.0365,-0.053,-0.0052,0.0189,-0.0128,0.0054,-0.0105,-0.1178,0.0032,0.0075,0.0366,0.0061,0.0001,0.0672,-0.0058,-0.0295,-0.0193,0.045,-0.0085,0.0142,0.0245,0.0171,0.0453,0.0276,0.0073,-0.002,-0.0208,-0.0387,-0.2154,0.0029,0.0402,0.0168,0.0355,-0.0885,0.0089,0.0131,0.0443,0.0768,0.0068,0.0053,-0.0045,0.0117,-0.0256,0.0402,0.0107,0.0184,-0.0342,0.0373,-0.0049,0.0044,-0.0674,-0.003,0.0473,-0.0287,0.1987,0.0236,-0.0055,-0.0296,-0.019,0.0208,-0.0132,-0.057,0.0174,0.0344,0.0571,-0.0213,-0.0254,-0.02,-0.0047,0.0078,-0.0083,-0.1194,-0.057,-0.0397,-0.063,0.0408,-0.0538,-0.0065,0.0213,-0.0384,0.0897,-0.0623,-0.0066,-0.0064,-0.0982,-0.0063,-0.0193,0.048,0.0564,-0.0419,0.0421,-0.0432,0.0572,-0.0332,0.0354,-0.0429,0.0142,-0.0254,-0.0034,0.0562,-0.0507,0.0314,0.0759,0.0156,0.0339,-0.0574,-0.0681,-0.0423,0.0657,-0.0096,-0.004,0.016,0.0127,0.0085,0.0148,0.0064,0.0176,-0.0202,0.0118,-0.0189,-0.0471,0.001,0.0377,0.0027,-0.2874,0.0285,-0.0146,0.0089,-0.0568,0.0091,0.0731,0.0405,-0.0428,-0.0222,0.0459,0.0732,0.0742,-0.0098,-0.0092,-0.0072,0.0518,-0.0329,0.0662,-0.0798,0.0049,0.0068,0.2284,-0.039,0.0343,0.0195,0.0032,0.0276,0.0309,-0.0565,-0.0076,-0.0063,0.0614,-0.045,0.0859,0.0866,-0.0175,0.0485,0.0309,-0.0209,0.0098,0.0392,-0.0228,-0.0304,0.1318,0.0049,-0.0097,-0.0403,0.0455,0.0572,-0.0546,0.0267,0.0266,0.0117,-0.0164,0.013,-0.0438,-0.0334,-0.0806,-0.0539,0.0275,-0.0515,-0.0531,0.012,0.0114]}
{"key":"[Super-resolution of Omnidirectional Images Using Adversarial Learning] An omnidirectional image (ODI) enables viewers to look in every direction from a fixed point through a head-mounted display providing an immersive experience compared to that of a standard image. Designing immersive virtual reality systems with ODIs is challenging as they require high resolution content. In this paper, we study super-resolution for ODIs and propose an improved generative adversarial network based model which is optimized to handle the artifacts obtained in the spherical observational space. Specifically, we propose to use a fast PatchGAN discriminator, as it needs fewer parameters and improves the super-resolution at a fine scale. We also explore the generative models with adversarial learning by introducing a spherical-content specific loss function, called 360-SS. To train and test the performance of our proposed model we prepare a dataset of 4500 ODIs. Our results demonstrate the efficacy of the proposed method and identify new challenges in ODI super-resolution for future investigations.","layer":2,"vector":[0.0005,-0.0263,0.0418,-0.0651,0.0414,0.0564,-0.0008,0.0017,0.0207,0.072,0.0206,-0.0707,0.0569,0.0878,-0.0133,0.0046,0.0221,0.031,-0.031,0.0446,0.0724,-0.0043,0.0165,-0.0297,-0.0164,-0.0018,-0.0452,-0.0799,-0.0364,-0.2611,0.0042,-0.0242,0.0392,0.005,0.001,-0.0325,-0.041,0.0519,0.0035,0.0126,0.0235,0.0289,-0.0117,-0.0482,-0.0139,-0.0012,-0.0259,-0.0032,0.0273,-0.0529,0.0367,-0.0517,-0.0224,0.0285,0.0474,0.0194,0.0202,0.0145,0.0441,0.0639,0.0268,0.0569,-0.1352,0.0695,0.0683,0.0267,-0.0137,-0.0141,0.0227,-0.0158,-0.0279,0.0334,-0.0035,0.0157,-0.0208,-0.0223,-0.0066,-0.0841,-0.0157,-0.0044,0.0567,0.0306,-0.0469,0.0456,0.0069,-0.0216,0.0191,-0.0223,0.0859,0.0152,-0.0355,-0.0147,-0.0646,0.0345,-0.0805,-0.0365,-0.0321,0.0277,-0.0281,0.2301,-0.0593,0.0121,0.075,-0.0193,0.0134,-0.0358,-0.0642,-0.0023,-0.0579,0.0149,-0.0014,-0.038,0.0501,-0.0298,0.0084,-0.0033,0.0332,0.0478,-0.0119,-0.0315,-0.0235,0.0094,0.0359,-0.0207,0.0526,-0.0535,0.0503,0.1249,0.0264,-0.0081,0.0229,-0.0278,-0.0312,-0.0102,0.0152,0.0228,0.0255,-0.0004,0.0113,0.0158,-0.0385,-0.0536,0.0031,-0.0574,-0.0311,0.1011,-0.0318,0.0316,-0.076,-0.0154,0.0065,0.0405,-0.0506,-0.0007,-0.0106,0.0347,0.003,0.0375,-0.0862,0.0628,-0.0323,-0.088,-0.0492,0.0758,0.0214,-0.0953,-0.0145,0.0034,-0.0127,0.0063,-0.01,-0.0012,-0.0209,0.045,0.1087,0.031,-0.107,-0.0031,-0.0266,0.01,-0.0037,-0.0464,-0.018,0.0089,0.015,-0.0907,0.0166,-0.0313,-0.027,0.0313,-0.0137,0.0113,-0.0881,-0.0266,-0.0041,-0.0259,0.0074,-0.0123,0.0314,-0.0156,0.0647,-0.0275,-0.047,0.0183,0.01,0.0335,-0.0002,-0.0401,0.0054,0.0129,-0.0694,-0.0195,0.0478,0.0277,-0.0103,-0.0205,-0.0186,-0.0003,-0.0299,0.0373,0.0345,-0.0678,-0.0523,-0.232,0.0037,-0.0206,-0.0078,0.0382,-0.0602,0.0567,0.0018,0.0886,-0.0204,0.0413,-0.0001,0.0273,0.0323,0.0032,0.0362,-0.0138,0.0439,-0.0226,-0.0343,0.0178,0.0705,-0.0379,-0.057,-0.0033,0.0037,0.2474,0.0165,0.0367,0.0127,0.0475,0.0492,-0.0278,-0.1172,0.0242,0.0509,0.1047,0.0032,-0.0571,-0.0468,-0.0318,0.0772,0.0387,-0.0926,-0.0163,-0.0224,-0.0814,0.041,-0.0381,0.0398,0.041,-0.0192,0.041,-0.0195,-0.0363,-0.0333,-0.0872,0.0059,-0.065,0.0241,-0.0061,-0.0495,-0.0077,-0.0732,0.0956,0.0472,-0.0152,-0.032,0.0471,-0.0542,-0.0003,0.0486,-0.0425,0.0318,0.0556,-0.018,0.0387,-0.0121,-0.0233,-0.0101,0.0609,-0.0035,0.0566,0.0152,0.0508,-0.0124,0.0286,-0.0174,0.0111,-0.0486,-0.0121,0.0384,-0.0593,-0.0151,0.0304,0.004,-0.2886,0.0522,0.0231,0.0328,-0.0089,0.0065,0.0054,0.0371,-0.0486,-0.0189,-0.0183,0.0241,0.0557,0.0138,0.0171,0.0315,0.0841,-0.034,0.074,-0.0487,0.0325,0.046,0.2073,-0.0553,-0.0347,-0.0326,-0.0417,0.0595,-0.0177,-0.0322,0.0131,0.0217,0.0449,0.0129,0.0128,0.0638,-0.0499,0.0113,0.024,0.0061,0.022,-0.0089,-0.0316,0.0092,0.0795,0.0395,-0.0255,0.0426,0.0011,-0.0342,-0.0339,-0.0026,0.0292,0.0121,0.0291,0.0288,-0.0577,-0.0136,-0.0248,0.0027,-0.0174,-0.0248,-0.0252,-0.0081,-0.0112]}
{"key":"[Accelerating Federated Learning with a Global Biased Optimiser] Federated Learning (FL) is a recent development in the field of machine learning that collaboratively trains models without the training data leaving client devices, to preserve data privacy. In realistic FL settings, the training set is distributed over clients in a highly non-Independent and Identically Distributed (non-IID) fashion, which has been shown extensively to harm FL convergence speed and final model performance. To address this challenge, we propose a novel, generalised approach for incorporating adaptive optimisation techniques into FL with the Federated Global Biased Optimiser (FedGBO) algorithm. FedGBO accelerates FL by employing a set of global biased optimiser values during the client-training phase, which helps to reduce `client-drift' from non-IID data, whilst also benefiting from adaptive optimisation. We show that the FedGBO update with a generic optimiser can be reformulated as centralised training using biased gradients and optimiser updates, and apply this theoretical framework to prove the convergence of FedGBO using momentum-Stochastic Gradient Descent (SGDm). We also conduct extensive experiments using 4 realistic FL benchmark datasets (CIFAR100, Sent140, FEMNIST, Shakespeare) and 3 popular adaptive optimisers (RMSProp, SGDm, Adam) to compare the performance of state-of-the-art adaptive-FL algorithms. The results demonstrate that FedGBO has highly competitive performance whilst achieving lower communication and computation costs, and provide practical insights into the trade-offs associated with the different adaptive-FL algorithms and optimisers for real-world FL deployments.","layer":7,"vector":[-0.0224,-0.0444,0.0233,0.0029,0.0533,0.0154,0.0071,0.0305,0.0628,-0.0305,0.0258,-0.0705,0.0124,0.0845,-0.0117,0.0209,0.0168,-0.0112,-0.0315,-0.0356,0.0094,-0.031,-0.0344,-0.0863,-0.0009,0.0428,-0.0475,-0.0086,-0.0886,-0.246,0.0504,-0.0568,0.0047,0.0009,0.0122,0.0226,-0.0019,0.0724,-0.0342,0.0584,-0.0177,0.0364,-0.0675,-0.0208,-0.0141,-0.0187,-0.0042,0.0081,-0.0691,-0.0172,0.0494,-0.0672,0.0074,0.0161,-0.022,0.0571,0.0492,0.0336,0.0526,0.0081,-0.0042,0.0677,-0.1267,0.0765,0.0185,0.0362,-0.0243,-0.0015,-0.014,0.0429,0.0133,0.0528,0.0157,0.0176,0.0115,0.0075,-0.0177,-0.0112,0.008,0.0376,0.0148,-0.0085,-0.0511,-0.0138,-0.0197,-0.0241,0.0108,-0.0432,0.0175,0.0261,-0.0525,-0.009,0.0087,0.0093,-0.039,0.0001,0.0126,0.0174,-0.0762,0.2062,-0.0646,0.0511,0.0207,-0.0153,0.0527,-0.0364,-0.0181,-0.0221,-0.0078,-0.0107,-0.0227,-0.0304,0.0308,-0.0366,-0.0077,0.0385,0.0424,0.0416,-0.0302,0.0219,-0.0239,0.0066,0.0793,-0.0171,0.0319,-0.0246,-0.0087,0.1326,0.0195,0.0139,0.0748,-0.0404,-0.0246,-0.0371,0.0413,0.0611,0.0551,-0.0163,0.0175,0.0406,-0.0279,-0.0761,-0.0144,-0.0408,-0.0264,0.1454,-0.022,0.0677,-0.0983,-0.016,-0.0364,-0.0167,-0.0193,-0.0111,0.0183,0.0166,0.0546,0.0776,-0.0609,0.0006,-0.0567,-0.0392,-0.0179,0.1203,0.0325,-0.1414,-0.0118,0.0076,0.0278,-0.0471,0.0449,0.0248,-0.0267,0.0634,0.0909,0.0083,-0.0188,-0.0039,-0.0084,-0.0167,0.0128,-0.0463,-0.0452,0.0383,-0.0027,-0.0336,0.0387,-0.0652,-0.0142,0.0363,-0.0482,0.0255,-0.0368,-0.005,-0.0167,-0.0348,-0.0025,0.0027,0.0196,-0.0019,-0.0074,0.0251,-0.0276,-0.0009,-0.0037,0.0049,-0.0178,-0.0068,0.0381,0.0074,-0.027,-0.0091,0.036,-0.0353,-0.039,-0.0091,-0.019,0.0267,-0.02,0.0355,0.0416,0.0167,-0.0332,-0.1977,-0.0281,0.032,-0.0507,0.0688,-0.0366,0.0802,0.0064,0.0082,0.0932,0.0695,-0.0226,-0.0489,0.051,-0.0135,0.0266,0.0277,0.0326,-0.008,-0.002,0.025,0.0263,0.04,-0.0624,0.0873,0.0135,0.2005,-0.0053,0.0149,-0.0544,0.0068,0.0292,-0.0069,-0.1569,0.035,-0.0049,0.0579,-0.0341,-0.0406,-0.0112,-0.003,0.0348,0.0508,-0.1494,-0.0369,-0.042,-0.0381,0.0328,-0.0785,0.0219,0.0172,-0.0219,0.0675,0.0361,-0.0231,-0.0451,-0.0809,0.0677,-0.0186,0.0548,0.0253,-0.0606,0.0065,-0.0398,0.0704,-0.0585,0.0252,-0.0129,0.0435,-0.0467,-0.0168,0.0565,-0.0072,-0.0195,0.0363,0.018,0.0307,-0.0391,-0.0742,0.0096,0.1105,-0.0,0.0685,0.0092,-0.0275,0.0124,0.0801,0.0521,0.0244,-0.0302,-0.0177,-0.0071,-0.0814,-0.0236,0.0208,0.0113,-0.2911,-0.0112,-0.0146,0.0284,-0.0466,0.0085,0.0516,0.0197,-0.0214,0.019,0.0094,0.0779,0.026,0.0068,0.0329,0.0441,0.0219,-0.0494,0.0536,-0.0867,0.0253,0.0327,0.2223,-0.0421,0.0283,0.0454,-0.0385,0.0142,0.0343,-0.0088,-0.0451,-0.0085,0.0275,-0.0299,0.0452,0.0683,-0.0179,-0.0158,0.0488,-0.0459,-0.0026,-0.0036,0.0054,-0.0148,0.0664,0.0111,-0.0712,-0.0475,-0.037,0.0141,-0.0136,-0.0008,-0.0194,-0.0056,0.0276,-0.0072,-0.0542,-0.0197,-0.0562,-0.0404,0.0029,-0.0757,-0.0499,-0.0154,-0.0079]}
{"key":"[Label Cleaning Multiple Instance Learning: Refining Coarse Annotations on Single Whole-Slide Images] Annotating cancerous regions in whole-slide images (WSIs) of pathology samples plays a critical role in clinical diagnosis, biomedical research, and machine learning algorithms development. However, generating exhaustive and accurate annotations is labor-intensive, challenging, and costly. Drawing only coarse and approximate annotations is a much easier task, less costly, and it alleviates pathologists' workload. In this paper, we study the problem of refining these approximate annotations in digital pathology to obtain more accurate ones. Some previous works have explored obtaining machine learning models from these inaccurate annotations, but few of them tackle the refinement problem where the mislabeled regions should be explicitly identified and corrected, and all of them require a -- often very large -- number of training samples. We present a method, named Label Cleaning Multiple Instance Learning (LC-MIL), to refine coarse annotations on a single WSI without the need of external training data. Patches cropped from a WSI with inaccurate labels are processed jointly within a multiple instance learning framework, mitigating their impact on the predictive model and refining the segmentation. Our experiments on a heterogeneous WSI set with breast cancer lymph node metastasis, liver cancer, and colorectal cancer samples show that LC-MIL significantly refines the coarse annotations, outperforming state-of-the-art alternatives, even while learning from a single slide. Moreover, we demonstrate how real annotations drawn by pathologists can be efficiently refined and improved by the proposed approach. All these results demonstrate that LC-MIL is a promising, light-weight tool to provide fine-grained annotations from coarsely annotated pathology sets.","layer":3,"vector":[-0.012,-0.0476,0.0504,0.0099,0.0251,0.0025,0.0034,0.0347,0.0047,-0.038,0.0057,-0.0624,0.0208,0.022,-0.02,0.0362,0.0599,0.0548,-0.0421,-0.0187,-0.0039,0.0049,-0.0129,0.0041,0.0482,0.0002,-0.0354,-0.0607,-0.0662,-0.2516,0.0264,-0.023,0.0172,0.0089,0.0142,-0.0363,-0.0436,0.0737,-0.0497,0.0038,-0.0027,0.0128,-0.034,-0.0265,-0.0141,-0.0655,-0.0595,-0.0375,-0.0171,0.0101,0.0256,-0.0626,0.0208,0.0369,0.0144,0.0167,0.0527,0.0563,0.0537,0.0415,0.0386,0.0417,-0.1656,0.0856,0.0732,0.0037,-0.0758,-0.0255,0.0154,0.0885,-0.0499,0.0423,0.0418,0.0548,-0.008,-0.0505,0.0265,-0.0021,-0.0304,0.0197,-0.0174,0.0164,-0.0353,-0.0367,-0.0199,-0.0241,-0.0382,-0.0552,0.0597,-0.0353,0.0122,-0.0419,0.0173,0.0381,-0.0289,-0.0148,0.0169,0.0478,0.0073,0.1805,-0.0554,0.009,0.0355,-0.0292,-0.0093,-0.0218,-0.0501,-0.0288,-0.0135,0.0306,0.0112,-0.0057,0.0066,-0.0395,0.0422,0.0124,0.0934,-0.0113,-0.0171,0.0103,0.0156,-0.0249,0.0277,0.0133,0.0225,-0.0284,0.0323,0.1375,0.0565,-0.0265,0.0367,0.0191,-0.0351,-0.0172,0.0199,0.0378,0.0317,-0.0184,0.0064,0.0243,-0.0256,-0.0769,0.0075,-0.082,-0.0522,0.1402,-0.0668,0.031,-0.0409,-0.0216,-0.0051,0.0114,-0.0337,0.0231,0.0049,0.0184,0.0601,0.0427,-0.0361,0.0114,0.0341,-0.0654,-0.0535,0.0962,0.0117,-0.0568,0.0067,-0.0084,-0.0048,0.0009,0.0446,0.0308,-0.0428,-0.0137,0.0483,0.0295,-0.0587,-0.0232,0.0268,0.0279,0.0132,-0.0497,-0.0763,0.0601,0.0448,-0.0472,0.0089,-0.0802,0.0631,0.0821,-0.0463,0.0351,-0.0001,-0.043,-0.0271,-0.0609,-0.0182,-0.0248,-0.018,-0.0254,0.0126,-0.0271,-0.0256,0.0627,0.0287,-0.0123,-0.0233,0.0085,0.0232,-0.023,-0.0649,0.0227,0.0083,-0.0147,0.011,-0.0187,0.0221,0.0295,0.0157,0.0649,0.0254,0.0051,-0.0964,-0.218,-0.0147,0.0131,-0.0343,0.0085,-0.0485,0.0154,0.031,0.0663,0.0763,0.0476,-0.0057,-0.012,-0.0048,-0.041,0.0366,0.0515,0.0252,-0.0198,-0.0028,0.006,0.0449,-0.0035,-0.0709,0.0877,-0.0158,0.2753,0.0761,0.0514,-0.0238,-0.0039,0.0329,-0.0532,-0.0888,0.0561,0.0095,0.027,-0.0393,-0.0528,-0.0077,-0.0186,-0.0063,-0.0048,-0.1167,-0.0392,-0.05,-0.0649,0.0392,-0.0313,0.0384,0.046,-0.0649,0.0834,-0.0162,0.0095,0.0123,-0.0745,0.0265,-0.0606,0.0084,0.0068,-0.0649,0.036,-0.0904,0.0356,-0.0138,-0.0568,-0.0421,0.0217,-0.0649,-0.0358,0.0683,-0.0037,-0.037,0.0262,0.0193,-0.0073,-0.0128,-0.0508,-0.0459,0.0647,-0.037,0.0233,0.0633,0.005,0.0282,0.0985,-0.0369,0.0431,-0.0414,0.018,0.0289,-0.0621,0.0135,0.0104,0.0066,-0.2744,0.0726,0.0115,0.039,-0.0227,0.0488,0.0467,0.0327,0.02,-0.0029,-0.0154,0.0199,0.0843,-0.0586,0.0042,0.0335,0.0786,-0.0614,0.0687,-0.0637,0.0033,0.0085,0.1744,-0.0426,-0.0061,0.0186,-0.0446,-0.0107,0.0504,0.0205,0.0296,-0.0077,0.0522,-0.0467,0.0314,0.1015,-0.0442,0.0156,-0.0016,-0.0,-0.0012,0.0207,-0.0589,-0.0421,0.0609,0.0007,-0.001,-0.0198,-0.0138,-0.0002,-0.0032,0.0118,-0.0036,0.0403,0.0179,0.0218,-0.0181,-0.0402,-0.0616,-0.0349,0.0612,-0.0463,-0.0359,0.0172,0.0151]}
{"key":"[Clustering Algorithm to Detect Adversaries in Federated Learning] In recent times, federated machine learning has been very useful in building intelligent intrusion detection systems for IoT devices. As IoT devices are equipped with a security architecture vulnerable to various attacks, these security loopholes may bring a risk during federated training of decentralized IoT devices. Adversaries can take control over these IoT devices and inject false gradients to degrade the global model performance. In this paper, we have proposed an approach that detects the adversaries with the help of a clustering algorithm. After clustering, it further rewards the clients for detecting honest and malicious clients. Our proposed gradient filtration approach does not require any processing power from the client-side and does not use excessive bandwidth, making it very much feasible for IoT devices. Further, our approach has been very successful in boosting the global model accuracy, up to 99% even in the presence of 40% adversaries.","layer":6,"vector":[-0.0252,-0.0444,0.0129,-0.0287,0.0314,0.0134,0.0617,0.0225,0.0255,-0.027,0.0374,-0.063,0.0168,0.042,0.0062,-0.0004,0.0276,-0.0051,-0.0349,-0.0119,0.0291,-0.0093,-0.0584,-0.0881,0.0162,0.0573,-0.0033,-0.0382,-0.1051,-0.2074,0.0071,-0.0455,0.0288,-0.0199,0.0047,0.0129,-0.0125,0.0383,-0.0336,0.0461,0.0067,0.0043,-0.0382,-0.0348,-0.0614,-0.0473,-0.0232,0.0052,-0.0216,-0.0536,0.0662,-0.0362,0.0196,0.0337,0.0073,-0.0118,0.0259,0.0465,0.0776,0.0052,0.0231,0.061,-0.1731,0.0676,0.0579,0.0624,-0.0168,-0.0402,0.0389,0.0277,-0.0004,0.0715,0.0055,0.0074,0.0093,0.0215,0.0198,-0.0205,-0.0066,0.039,-0.0103,-0.0182,-0.022,0.0091,-0.042,-0.0311,0.0083,-0.0493,0.0581,-0.0044,-0.0605,0.0433,0.0096,0.0396,-0.0348,-0.0135,0.0044,-0.0011,-0.0723,0.2056,-0.0527,0.0664,0.0344,-0.0404,0.0379,-0.0533,-0.0276,-0.0851,0.0106,0.0065,-0.0465,-0.0236,0.0372,-0.0571,0.0107,0.0357,0.061,0.0703,-0.0062,0.0255,-0.0101,-0.012,0.0841,-0.0224,0.0504,-0.0486,0.0096,0.1403,0.0412,0.0495,0.0034,-0.0103,-0.0266,0.0069,0.0591,0.093,-0.0076,-0.0384,-0.0005,-0.0138,-0.0537,-0.0727,0.0378,-0.0811,-0.0227,0.0993,0.004,0.0832,-0.0624,-0.0476,-0.0164,0.0252,-0.0372,-0.0226,0.0481,0.0502,0.0155,0.0647,-0.0514,0.0037,-0.0481,-0.0313,-0.0339,0.1288,0.0692,-0.0892,-0.0217,0.0125,0.0044,-0.0235,0.0322,0.043,-0.0117,0.0412,0.0423,-0.0088,-0.0333,-0.0377,-0.0212,-0.0337,0.0073,-0.0119,-0.0482,0.0042,0.0384,-0.0499,0.03,-0.0442,0.0115,0.037,-0.0942,0.0164,-0.0255,-0.0268,-0.0292,-0.0632,0.0261,0.0036,-0.0126,-0.0298,-0.0022,0.0157,-0.0472,0.0342,-0.0491,0.0013,-0.0075,-0.0617,0.016,0.0134,-0.0111,-0.0017,0.0374,-0.0576,-0.0058,-0.0315,0.0008,0.0452,0.0017,0.0485,0.0346,0.0296,-0.1077,-0.1914,-0.0296,0.0017,-0.015,0.0325,-0.0522,0.0328,-0.0175,0.0437,0.0301,0.0713,0.0157,-0.0222,0.0351,-0.0363,0.0712,0.0289,0.0596,-0.024,0.0202,-0.0125,0.0273,0.0152,-0.0431,0.0292,0.0802,0.21,0.0108,0.0133,-0.0789,-0.0053,0.0278,-0.0227,-0.1014,0.0571,0.0104,0.0201,0.0044,-0.0267,-0.008,-0.0456,0.0525,0.006,-0.116,-0.0472,-0.0665,-0.0322,0.0618,-0.0561,0.025,0.0089,-0.0026,0.0565,0.0048,0.0176,-0.0539,-0.0777,0.0408,-0.0289,0.0637,0.0049,-0.0392,-0.001,-0.0807,0.1063,-0.0011,-0.0412,-0.0235,0.0109,-0.0229,-0.0181,0.1011,0.0418,-0.0051,0.0323,0.0329,0.0274,-0.0569,-0.0518,-0.0329,0.1044,-0.0263,0.0389,0.0357,-0.031,0.0222,0.0257,0.0446,0.0642,-0.0477,-0.0019,0.0232,-0.0671,0.0004,0.0175,-0.0114,-0.3098,0.008,0.0223,0.0515,-0.0547,0.0007,0.0678,0.0232,-0.036,0.0274,0.0108,0.0264,0.0186,-0.0305,0.024,0.0284,0.0322,-0.0428,0.0268,-0.0514,0.0049,0.0704,0.231,-0.0453,0.0291,0.0321,-0.0063,0.0437,0.0229,-0.0341,0.0042,-0.033,0.0818,-0.0061,0.0278,0.0386,0.0178,-0.0064,0.0561,-0.0507,-0.0123,0.0046,-0.0237,0.0033,0.0489,0.0046,-0.0345,-0.0443,0.009,0.0378,-0.0204,-0.0216,-0.0059,-0.0221,0.0222,0.01,-0.0456,-0.0104,-0.0714,-0.0397,-0.0128,-0.0528,-0.0345,0.0071,-0.0032]}
{"key":"[Self-supervised Graph Neural Networks without explicit negative sampling] Real world data is mostly unlabeled or only few instances are labeled. Manually labeling data is a very expensive and daunting task. This calls for unsupervised learning techniques that are powerful enough to achieve comparable results as semi-supervised/supervised techniques. Contrastive self-supervised learning has emerged as a powerful direction, in some cases outperforming supervised techniques. In this study, we propose, SelfGNN, a novel contrastive self-supervised graph neural network (GNN) without relying on explicit contrastive terms. We leverage Batch Normalization, which introduces implicit contrastive terms, without sacrificing performance. Furthermore, as data augmentation is key in contrastive learning, we introduce four feature augmentation (FA) techniques for graphs. Though graph topological augmentation (TA) is commonly used, our empirical findings show that FA perform as good as TA. Moreover, FA incurs no computational overhead, unlike TA, which often has O(N^3) time complexity, N-number of nodes. Our empirical evaluation on seven publicly available real-world data shows that, SelfGNN is powerful and leads to a performance comparable with SOTA supervised GNNs and always better than SOTA semi-supervised and unsupervised GNNs. The source code is available at https://github.com/zekarias-tilahun/SelfGNN.","layer":4,"vector":[-0.0443,-0.0376,-0.0351,-0.0256,0.0685,0.0258,0.0112,0.0427,-0.0141,-0.0134,0.0296,-0.0588,0.0783,0.0686,0.0006,0.0352,-0.0081,0.0892,-0.0497,-0.0057,0.0001,-0.0401,-0.0243,-0.0542,0.0798,-0.0319,-0.0203,-0.0119,-0.0146,-0.2512,0.0323,-0.0455,0.0459,-0.0145,0.0092,-0.0687,0.001,0.0521,-0.0616,0.0434,0.0488,-0.0076,0.01,-0.0444,-0.0342,-0.0682,0.0155,-0.0314,-0.0165,-0.0311,0.0326,-0.0417,0.0106,0.0123,0.0314,0.0575,0.0399,0.0196,0.0266,0.0104,0.0421,0.0839,-0.1411,0.0402,0.0383,0.0482,-0.0778,-0.0036,0.0392,0.0748,0.032,0.0342,0.0259,0.0013,0.0174,0.0326,0.0047,-0.0143,0.0018,-0.0097,-0.0234,-0.0186,-0.0632,-0.0414,-0.0058,-0.0214,-0.0073,-0.0625,0.0264,0.023,-0.0391,-0.0179,-0.0091,0.0267,-0.0326,-0.0456,0.0409,0.0056,-0.0588,0.1997,-0.0651,0.0477,0.0347,0.0035,0.0233,-0.0538,-0.0075,-0.0377,-0.0743,0.0122,-0.0248,-0.0333,-0.0183,-0.0722,0.0155,0.0054,0.1172,0.03,0.012,0.0095,-0.0003,0.0034,0.0039,-0.0329,0.0097,-0.0698,-0.011,0.0642,-0.0065,0.0021,0.0363,0.004,0.0004,0.0153,-0.0292,0.0024,0.0392,-0.0043,0.0057,-0.0025,-0.0361,-0.0102,0.0061,-0.0786,-0.0939,0.1164,-0.0593,0.012,-0.0306,-0.0476,-0.0568,0.0315,-0.0425,-0.0029,-0.0117,0.0572,0.0269,0.0166,-0.0615,-0.0432,-0.0263,-0.0279,-0.0366,0.0887,0.0416,-0.1207,-0.0084,-0.0075,-0.0092,-0.0052,0.0176,0.0455,-0.0468,0.0588,0.0793,0.0374,-0.09,-0.034,-0.0111,0.0105,0.0276,0.0141,-0.0408,0.0665,0.0232,-0.038,-0.0147,-0.0252,0.0234,0.0603,-0.0287,0.0627,-0.0176,-0.0014,-0.0214,0.0218,-0.0092,-0.0275,0.003,-0.0335,0.0229,-0.0094,-0.0198,0.0214,-0.0138,0.009,-0.0177,-0.007,0.0412,0.0074,-0.0467,0.0021,0.0624,-0.0255,-0.0419,-0.0067,-0.0068,0.041,-0.0037,0.0495,0.025,-0.0191,-0.0329,-0.1968,-0.0246,0.0035,-0.0208,0.0447,-0.1083,0.002,0.0077,0.1089,0.0798,0.0399,0.0048,-0.0449,0.0255,0.0057,0.0643,0.0846,0.0401,0.024,-0.026,-0.0451,0.0435,-0.0079,-0.0664,0.052,-0.0042,0.2477,0.0292,0.0274,-0.0425,-0.013,-0.0252,-0.0636,-0.1014,0.052,0.0486,0.0708,-0.0067,-0.0432,-0.0478,-0.0185,0.0183,-0.0018,-0.0851,-0.0165,-0.0138,-0.0114,-0.038,-0.0674,0.026,0.0548,0.0153,0.0181,0.0312,0.0078,-0.0399,-0.0849,0.0332,-0.0887,0.0136,0.0074,-0.0877,0.0169,-0.0652,0.0481,0.0507,-0.0585,-0.0158,0.0373,0.0079,-0.0338,0.0838,0.0206,-0.0128,0.0294,0.0097,0.034,0.0335,-0.0503,0.0009,0.0383,-0.0234,0.0674,0.0085,0.0123,0.0718,0.0554,0.0202,0.0344,-0.0152,0.0048,0.0186,-0.0217,-0.0685,0.1082,0.0056,-0.2606,0.0513,0.0028,0.0763,-0.0191,0.0462,0.0348,0.068,-0.0313,-0.0299,0.0115,0.0446,0.0405,-0.0451,-0.0115,0.0198,0.0638,-0.0231,0.0711,-0.0559,0.0753,0.0489,0.2117,-0.0564,0.026,0.0623,-0.0642,-0.0154,-0.0063,-0.0328,-0.0161,0.0252,0.0988,-0.0417,0.0188,0.0855,-0.039,0.0196,-0.0088,0.0074,-0.0217,-0.0216,-0.0287,-0.0481,0.0862,-0.0193,-0.0293,-0.0364,0.012,0.0299,-0.0354,0.013,-0.0387,0.0031,0.0272,0.0047,-0.0472,-0.034,-0.0345,-0.0322,0.0304,-0.04,-0.0044,-0.0216,-0.024]}
{"key":"[An empirical study on hyperparameter tuning of decision trees] Machine learning algorithms often contain many hyperparameters whose values affect the predictive performance of the induced models in intricate ways. Due to the high number of possibilities for these hyperparameter configurations, and their complex interactions, it is common to use optimization techniques to find settings that lead to high predictive accuracy. However, we lack insight into how to efficiently explore this vast space of configurations: which are the best optimization techniques, how should we use them, and how significant is their effect on predictive or runtime performance? This paper provides a comprehensive approach for investigating the effects of hyperparameter tuning on three Decision Tree induction algorithms, CART, C4.5 and CTree. These algorithms were selected because they are based on similar principles, have presented a high predictive performance in several previous works and induce interpretable classification models. Additionally, they contain many interacting hyperparameters to be adjusted. Experiments were carried out with different tuning strategies to induce models and evaluate the relevance of hyperparameters using 94 classification datasets from OpenML. Experimental results indicate that hyperparameter tuning provides statistically significant improvements for C4.5 and CTree in only one-third of the datasets, and in most of the datasets for CART. Different tree algorithms may present different tuning scenarios, but in general, the tuning techniques required relatively few iterations to find accurate solutions. Furthermore, the best technique for all the algorithms was the Irace. Finally, we find that tuning a specific small subset of hyperparameters contributes most of the achievable optimal predictive performance.","layer":3,"vector":[0.0003,0.0126,0.0335,-0.0064,0.0409,-0.0188,0.0291,0.0356,0.0797,-0.0066,0.0022,-0.0161,0.0158,0.0202,0.0099,0.0151,0.0242,0.0584,-0.0619,0.0096,0.0296,-0.0122,-0.0384,-0.0629,0.0358,0.0259,-0.028,-0.0496,-0.0667,-0.2793,0.0484,-0.0684,0.0454,-0.0169,-0.0199,-0.0215,0.0072,0.0317,-0.0394,0.042,-0.0033,0.0227,0.0001,-0.0374,0.0014,-0.041,0.0332,-0.0153,-0.0484,0.0081,-0.0021,-0.0337,0.0119,0.0302,0.0352,0.0403,0.0454,0.0311,0.0767,0.0313,0.0192,0.0415,-0.1377,0.0294,0.041,0.0425,-0.0658,-0.0267,0.0528,0.0695,-0.0288,0.0278,0.0055,0.0057,0.0311,0.0162,0.0004,-0.0105,0.0221,0.0206,0.0317,-0.0508,-0.047,-0.0155,0.0192,-0.047,0.0042,-0.0299,0.0727,0.0159,-0.0004,-0.0327,-0.0463,0.0063,-0.0921,0.003,0.0641,0.0154,-0.0907,0.1886,-0.0545,0.0035,-0.0003,-0.0712,0.0058,-0.0168,-0.0505,-0.053,-0.0352,0.0062,0.0201,-0.0561,0.024,0.0018,-0.039,0.0396,0.0675,0.0434,-0.022,0.0014,-0.0066,-0.0225,0.0712,0.0126,0.0467,-0.0207,0.0204,0.1292,-0.0086,0.0094,0.0342,-0.0587,-0.0523,-0.0008,0.0042,0.0217,0.0529,-0.0105,0.0151,0.0128,-0.0513,-0.0448,-0.0167,-0.077,-0.0479,0.1258,-0.0728,0.0053,-0.0761,-0.0248,-0.0389,0.0064,-0.0097,-0.0242,-0.0091,0.0359,0.0246,0.044,0.0148,0.0203,-0.035,-0.0642,-0.045,0.0907,-0.0324,-0.0809,-0.043,0.0013,0.0018,0.0164,0.0596,0.0584,-0.0271,0.0219,0.0635,0.0354,-0.0612,-0.0161,0.0159,-0.0064,0.0796,-0.0244,-0.0229,0.0411,0.067,-0.0225,0.012,-0.0606,0.0599,0.0096,-0.0233,-0.0274,-0.0113,-0.0047,-0.0272,-0.0177,-0.003,0.0181,0.0487,-0.0321,0.0154,0.0354,-0.0165,0.0567,-0.0345,0.0228,-0.0192,-0.0015,0.062,0.0028,-0.0563,0.0461,0.0626,-0.0234,-0.0792,-0.0221,0.0261,0.0649,-0.0014,0.0859,0.0386,-0.0396,-0.0203,-0.2206,-0.0257,0.0302,0.0058,0.0355,-0.041,0.023,0.0129,0.0034,0.035,0.0252,-0.01,-0.0298,0.0196,0.0043,0.0343,0.0072,0.0407,-0.0487,0.0423,0.0102,0.0391,0.0192,-0.1118,0.0093,0.0249,0.1856,-0.0072,0.0177,-0.0541,0.0428,0.0316,-0.023,-0.1075,0.0523,0.0501,0.0399,-0.0085,-0.028,0.0023,-0.0025,0.0061,-0.0526,-0.1534,-0.0621,-0.0111,-0.0339,0.037,-0.0224,0.0111,0.0111,-0.0359,0.0311,-0.03,-0.0177,-0.0601,-0.0779,0.0379,-0.0105,0.0086,0.031,-0.0676,-0.0061,-0.0669,0.0042,-0.0653,-0.0552,-0.0154,-0.0246,-0.0316,-0.0404,0.1054,-0.008,-0.0153,0.0747,0.0423,-0.0111,-0.017,0.0023,0.009,0.032,0.0127,0.0784,0.0469,0.0341,-0.0085,0.0946,0.0089,0.0619,-0.0337,-0.008,-0.0079,-0.0216,-0.0192,0.0646,0.0103,-0.3156,0.0265,0.0143,0.046,0.0001,0.0158,0.0356,-0.0074,-0.0084,-0.0347,0.0362,0.0457,0.0374,-0.0357,-0.0084,-0.0001,0.0222,-0.0415,0.0405,-0.0274,0.0267,0.0513,0.2431,-0.0145,0.052,0.0125,-0.0521,-0.0044,0.0039,-0.0606,0.0057,-0.0134,0.0795,-0.0464,0.0447,0.0841,-0.0354,0.0458,0.043,-0.0293,0.0228,-0.0064,-0.0702,-0.0512,0.0824,0.0042,-0.0286,-0.0545,0.0097,0.0356,-0.0194,0.0101,-0.0428,-0.008,0.0123,-0.0008,-0.0383,-0.0325,-0.0448,-0.0312,0.0073,-0.0539,0.008,-0.0269,0.0381]}
{"key":"[Empirical Analysis of Predictive Algorithms for Collaborative Filtering] Collaborative filtering or recommender systems use a database about user preferences to predict additional topics or products a new user might like. In this paper we describe several algorithms designed for this task, including techniques based on correlation coefficients, vector-based similarity calculations, and statistical Bayesian methods. We compare the predictive accuracy of the various methods in a set of representative problem domains. We use two basic classes of evaluation metrics. The first characterizes accuracy over a set of individual predictions in terms of average absolute deviation. The second estimates the utility of a ranked list of suggested items. This metric uses an estimate of the probability that a user will see a recommendation in an ordered list. Experiments were run for datasets associated with 3 application areas, 4 experimental protocols, and the 2 evaluation metrics for the various algorithms. Results indicate that for a wide range of conditions, Bayesian networks with decision trees at each node and correlation methods outperform Bayesian-clustering and vector-similarity methods. Between correlation and Bayesian networks, the preferred method depends on the nature of the dataset, nature of the application (ranked versus one-by-one presentation), and the availability of votes with which to make predictions. Other considerations include the size of database, speed of predictions, and learning time.","layer":1,"vector":[-0.0634,0.0006,-0.0128,-0.0337,0.0358,0.026,0.0497,0.073,0.058,-0.0381,0.0206,0.0013,0.0184,0.055,0.0679,-0.0146,0.0574,0.0216,-0.0413,-0.0237,0.052,-0.0022,-0.0124,-0.0721,-0.0067,0.0388,-0.0332,-0.0154,-0.0908,-0.1775,-0.0147,-0.0917,0.0886,-0.0074,-0.0497,-0.0458,0.0323,0.0554,-0.0412,0.0171,0.0132,0.0313,-0.0653,-0.0359,-0.0263,-0.0118,-0.0024,-0.0392,-0.031,-0.0031,0.0257,-0.0207,0.0365,0.0216,0.032,0.0463,0.0326,0.0708,-0.0086,0.062,0.0467,0.0308,-0.1645,0.0827,0.0045,0.0176,-0.0513,-0.0331,-0.0123,0.0488,0.0127,0.0482,-0.0022,0.0672,-0.0008,0.0115,-0.007,0.0133,-0.0551,0.0453,-0.0373,-0.0415,-0.0628,-0.0638,0.0278,0.007,0.0174,-0.0113,0.0116,0.0089,-0.0336,0.0055,-0.0383,0.0098,-0.0638,-0.0029,0.048,0.0017,-0.0503,0.189,-0.0416,0.0758,0.0223,-0.0747,0.0162,-0.0406,-0.0012,-0.0362,0.0028,0.0273,-0.0086,-0.0025,0.0175,-0.0691,0.0364,0.028,0.0879,0.0327,0.0224,-0.0369,-0.0665,0.012,0.1056,0.0001,0.0287,-0.0447,0.0065,0.0995,0.0171,0.0189,0.0629,-0.0273,-0.0752,-0.0092,0.0005,0.0834,0.0052,-0.0553,0.0364,0.0321,-0.0091,-0.0563,-0.0117,-0.0567,-0.0584,0.1419,-0.047,-0.011,-0.0395,-0.0523,-0.023,-0.0569,-0.0173,-0.0311,0.0031,0.0037,0.0596,0.0391,-0.064,0.027,-0.0172,-0.0618,-0.0221,0.0715,-0.0184,-0.0791,-0.0381,0.0056,0.0163,-0.0006,0.0365,0.0289,0.007,0.0289,0.0708,0.0337,-0.0562,-0.0063,0.0354,-0.0183,0.021,-0.0016,-0.0386,0.0543,0.0267,-0.0011,0.0053,-0.0525,0.0343,0.0465,-0.022,-0.0603,0.0018,0.019,-0.0039,-0.006,-0.0058,-0.0348,0.0077,-0.0523,0.0049,0.0055,-0.0389,0.042,-0.0368,0.07,-0.0507,-0.0108,0.0868,-0.0244,-0.0564,0.0223,0.0482,-0.0415,-0.0731,0.0025,0.0333,0.0633,0.0356,0.0549,0.0339,-0.0118,-0.0254,-0.2003,0.0016,0.0207,0.0107,0.0576,-0.0585,0.0206,-0.032,0.0083,0.1131,0.0627,0.006,-0.0065,0.0507,-0.0021,0.0852,-0.0148,-0.0025,-0.0308,0.001,-0.024,0.0245,0.0057,-0.0695,-0.0172,0.0425,0.1946,0.027,-0.0126,-0.0198,0.0548,0.0362,-0.0278,-0.0726,0.0393,0.0686,0.0617,-0.034,-0.0211,-0.0394,-0.0206,-0.0015,0.0155,-0.1033,-0.0365,-0.0372,-0.0272,0.033,-0.0686,0.0305,0.0381,-0.0266,0.0585,-0.0383,-0.0049,-0.066,-0.0634,0.0118,0.0031,0.0539,0.0059,-0.0567,0.0113,-0.0617,0.0376,-0.0365,-0.0062,0.0082,0.0185,-0.0149,-0.0519,0.0574,-0.0253,-0.0161,0.0698,-0.0057,0.0194,-0.0333,-0.0295,-0.0355,0.0914,-0.0381,0.0676,0.0065,0.0027,-0.0369,0.0846,0.0045,0.0144,-0.0281,-0.0113,-0.0258,-0.0634,-0.0257,0.0494,-0.0148,-0.3218,0.0334,-0.0012,0.0588,-0.0061,-0.0148,0.0667,0.0428,0.0079,0.0042,0.0518,0.0048,0.0525,-0.0283,-0.0053,0.0142,0.0021,-0.0332,0.0281,-0.0374,0.037,0.003,0.2495,0.0003,0.0323,0.018,0.0359,-0.0356,0.0074,-0.006,0.0058,0.0023,0.1079,-0.062,0.0292,0.0944,-0.0376,0.0057,-0.0081,-0.0361,-0.0117,-0.0191,-0.0531,-0.0516,0.1088,-0.0084,-0.0077,-0.0386,-0.0321,0.0212,-0.056,0.0065,-0.0409,-0.0395,-0.0074,0.049,-0.0379,0.0027,-0.0511,-0.0645,0.0069,-0.0604,0.0179,-0.0023,0.0332]}
{"key":"[Randomized kernels for large scale Earth observation applications] Dealing with land cover classification of the new image sources has also turned to be a complex problem requiring large amount of memory and processing time. In order to cope with these problems, statistical learning has greatly helped in the last years to develop statistical retrieval and classification models that can ingest large amounts of Earth observation data. Kernel methods constitute a family of powerful machine learning algorithms, which have found wide use in remote sensing and geosciences. However, kernel methods are still not widely adopted because of the high computational cost when dealing with large scale problems, such as the inversion of radiative transfer models or the classification of high spatial-spectral-temporal resolution data. This paper introduces an efficient kernel method for fast statistical retrieval of bio-geo-physical parameters and image classification problems. The method allows to approximate a kernel matrix with a set of projections on random bases sampled from the Fourier domain. The method is simple, computationally very efficient in both memory and processing costs, and easily parallelizable. We show that kernel regression and classification is now possible for datasets with millions of examples and high dimensionality. Examples on atmospheric parameter retrieval from hyperspectral infrared sounders like IASI/Metop; large scale emulation and inversion of the familiar PROSAIL radiative transfer model on Sentinel-2 data; and the identification of clouds over landmarks in time series of MSG/Seviri images show the efficiency and effectiveness of the proposed technique.","layer":0,"vector":[-0.0446,-0.0151,0.0682,0.014,0.0449,-0.0324,0.0218,0.0191,0.0184,0.0236,0.0163,-0.0643,0.0235,0.0408,0.0215,0.0227,0.0392,0.062,-0.0189,0.0229,0.0257,0.0145,-0.0336,-0.0469,-0.0052,0.018,-0.0322,-0.0613,-0.0475,-0.2485,0.0158,-0.05,0.052,-0.0121,-0.0085,-0.0344,-0.0287,0.0407,-0.0436,0.0528,0.015,-0.0039,-0.0245,-0.0174,-0.0168,-0.0672,-0.0488,-0.0329,0.0115,-0.003,0.0288,-0.012,0.0097,0.0145,0.0159,0.0557,0.0614,0.0603,0.0794,0.0252,0.0307,0.0509,-0.1666,0.0537,0.025,0.0205,0.0008,-0.0516,0.0384,0.0038,-0.0164,0.0823,0.0245,0.0014,-0.0053,-0.0143,-0.0125,-0.0141,-0.0477,-0.0111,0.0302,0.0053,-0.0136,-0.0277,-0.0046,-0.0222,-0.0089,-0.0211,0.061,0.0481,-0.0275,-0.0465,-0.0077,0.0335,-0.0916,-0.012,0.056,0.0316,0.003,0.1907,-0.0564,0.035,0.04,-0.0262,0.0282,-0.0644,-0.0468,-0.014,0.0126,-0.0474,0.0124,-0.058,0.0204,-0.0162,0.0478,-0.0536,0.0611,0.0444,-0.0214,-0.0434,-0.0123,0.0077,0.0613,0.0202,0.0705,-0.0389,0.0485,0.1429,0.038,0.0581,0.0299,-0.0132,-0.0832,-0.0609,-0.0218,0.0377,0.0179,0.0138,-0.0063,-0.0008,-0.0192,-0.0816,0.0387,-0.0552,-0.0471,0.1491,-0.0662,0.0059,-0.0588,-0.0292,-0.0053,0.0311,-0.0082,-0.0534,0.039,0.0334,0.0259,0.0335,0.0066,0.0107,-0.0289,-0.0296,0.01,0.1066,0.0184,-0.0677,-0.0272,-0.0069,0.0201,-0.0088,0.0082,0.0048,0.0174,0.0008,0.0883,0.0507,-0.0882,0.0234,0.0166,-0.0055,0.0274,-0.0752,-0.0131,0.0234,0.0389,-0.0568,-0.0366,-0.0173,-0.011,0.016,-0.0114,-0.0159,-0.0198,-0.0102,-0.015,-0.0288,-0.0148,-0.0164,0.0686,-0.0366,0.0638,-0.0173,-0.0221,0.0347,-0.0241,0.0186,0.0246,-0.0075,0.0073,0.0205,0.0021,0.0149,0.0221,0.0139,-0.0731,0.0056,-0.0066,0.0243,0.0038,0.0586,0.0714,-0.0885,-0.0996,-0.2317,-0.0193,0.0184,0.0156,0.0487,-0.0678,0.0375,0.0042,0.0777,0.0729,0.0548,-0.0345,-0.0224,0.0344,-0.0477,0.0187,0.0106,0.0492,-0.029,-0.0208,0.0234,0.0146,-0.0113,-0.0527,0.0377,-0.0434,0.174,-0.0023,0.0211,-0.0569,0.0612,-0.0011,-0.0297,-0.0952,0.0452,0.0242,0.0744,0.0382,-0.0806,-0.0006,0.0379,0.0388,0.0323,-0.0552,-0.0455,-0.0318,0.0125,0.0065,-0.0261,0.0214,0.021,0.0145,0.0657,-0.0337,0.0229,0.0083,-0.099,0.0039,0.0101,0.0255,0.0395,-0.1048,-0.0002,-0.0749,0.0597,-0.0198,-0.0327,-0.0217,0.0317,-0.0533,-0.0361,0.1058,-0.0522,0.0239,0.0624,0.0145,0.0312,0.0049,-0.0237,-0.0116,0.1162,-0.0363,0.0198,0.0107,0.0345,0.0175,0.0775,-0.0427,-0.0037,-0.0385,-0.0296,0.03,-0.0783,-0.0143,0.0736,-0.0001,-0.3031,0.0122,-0.0109,0.0179,-0.039,-0.011,-0.0084,0.0056,-0.0514,-0.0035,-0.0065,0.0515,0.0726,-0.0364,0.0338,0.0353,0.057,-0.0087,0.0238,-0.041,-0.0196,-0.0016,0.1957,-0.0515,-0.0006,0.0334,-0.0419,-0.0132,0.0032,-0.0472,0.077,-0.0083,0.0645,-0.0571,0.0298,0.0912,0.0007,0.0174,0.0008,-0.0349,0.0307,-0.0284,-0.0131,-0.0492,0.0795,-0.0093,0.0197,-0.0458,-0.0226,0.0467,-0.0424,-0.0196,-0.0044,0.0026,-0.0027,0.0231,-0.069,-0.038,-0.0651,-0.072,0.0361,-0.0564,-0.0328,-0.0569,0.0344]}
{"key":"[FastGAE: Scalable Graph Autoencoders with Stochastic Subgraph Decoding] Graph autoencoders (AE) and variational autoencoders (VAE) are powerful node embedding methods, but suffer from scalability issues. In this paper, we introduce FastGAE, a general framework to scale graph AE and VAE to large graphs with millions of nodes and edges. Our strategy, based on an effective stochastic subgraph decoding scheme, significantly speeds up the training of graph AE and VAE while preserving or even improving performances. We demonstrate the effectiveness of FastGAE on various real-world graphs, outperforming the few existing approaches to scale graph AE and VAE by a wide margin.","layer":9,"vector":[-0.0006,-0.0342,0.005,-0.017,0.0435,0.0296,-0.026,0.0142,-0.0004,0.0097,0.0283,-0.057,0.0933,0.0875,0.043,0.0244,-0.0111,0.0743,-0.0284,0.0023,0.0192,-0.0465,0.0124,-0.0792,0.0427,0.0175,-0.0154,-0.029,-0.0399,-0.2534,0.0272,-0.0536,0.0869,-0.0401,0.0141,-0.0965,-0.0025,0.0216,-0.0382,0.0622,0.0233,0.0149,-0.0474,-0.0139,-0.011,0.0118,0.0195,-0.0268,-0.0179,-0.0473,0.0599,-0.0344,0.0357,0.0268,0.0298,0.033,0.0365,0.0289,0.0388,0.0784,0.0323,0.065,-0.1158,0.0742,0.0329,0.0182,-0.0678,0.0133,-0.0041,0.0577,0.009,0.0189,0.0184,0.0286,0.0582,0.0316,-0.0007,-0.0295,-0.04,-0.012,0.0113,-0.0656,-0.0569,-0.0005,0.0054,-0.0235,0.0347,-0.0369,0.0449,0.0206,-0.0171,-0.0585,0.0016,0.0126,-0.0677,-0.033,0.0411,0.0232,-0.0689,0.1961,-0.04,0.0325,0.0444,-0.0274,0.047,-0.0749,0.0005,-0.0339,-0.0032,-0.002,0.002,-0.045,0.0235,-0.0968,0.0422,-0.0208,0.0591,0.0489,-0.0166,-0.0068,-0.059,0.0526,0.0092,-0.0291,0.0328,-0.0484,-0.0113,0.1418,0.0397,0.0368,0.0364,0.0512,-0.0133,-0.0057,-0.0134,-0.021,0.0202,-0.0239,0.0204,-0.0089,-0.0078,-0.0052,-0.0068,-0.0722,-0.0494,0.1085,-0.0584,-0.0011,-0.0351,-0.0411,-0.0008,0.021,-0.0398,0.0063,-0.0049,0.0526,0.0458,0.0331,-0.0565,0.0047,-0.0153,-0.0131,-0.0423,0.0684,0.0391,-0.1333,-0.0467,-0.0287,0.0034,-0.0101,0.0298,0.011,-0.0127,0.0233,0.082,0.0588,-0.0789,-0.0309,0.0077,0.0073,0.0057,-0.0103,-0.0232,0.0496,0.0279,-0.0412,0.0128,-0.0484,0.0164,0.0201,-0.0022,0.0438,-0.0314,-0.0052,-0.034,-0.0062,-0.0388,-0.0566,-0.0069,-0.028,0.0613,-0.0156,-0.0219,0.057,-0.0596,0.0154,-0.0313,-0.0027,-0.001,0.0052,-0.0692,-0.0001,0.0435,-0.0008,-0.013,0.0067,0.0093,0.0394,0.0134,0.0317,0.0107,-0.0729,-0.0291,-0.1884,-0.0165,0.0157,-0.0702,0.0534,-0.0325,-0.0007,-0.0291,0.1018,0.0642,0.0485,-0.0103,-0.0221,0.0111,-0.011,0.0949,0.0093,0.0498,0.0049,-0.0072,0.0121,0.0113,-0.0127,-0.0591,0.0231,0.039,0.2579,-0.0042,0.0272,-0.0347,0.0019,0.03,-0.0458,-0.1098,0.087,0.0584,0.0373,0.0134,-0.0462,-0.0055,-0.0538,0.0009,0.0069,-0.1405,-0.0573,-0.036,-0.0423,-0.0077,-0.0342,-0.002,0.0462,0.0068,0.0719,0.0219,-0.044,-0.0491,-0.0716,0.0224,-0.055,0.0093,0.0111,-0.0944,-0.0383,-0.0686,0.0529,0.0506,-0.0476,-0.0332,0.0099,-0.0218,0.0027,0.0616,-0.0165,-0.0151,0.0772,0.029,0.0208,0.0122,-0.0454,-0.0212,0.0321,-0.0442,0.0702,0.0044,0.0451,0.0459,0.0549,-0.0159,0.04,0.0246,0.0056,0.0356,-0.0677,-0.0516,-0.0045,-0.0678,-0.3065,0.0137,0.0658,0.0312,-0.0596,0.0067,0.0255,0.018,-0.041,-0.0037,0.0195,0.0374,0.0412,0.0093,0.0175,0.0402,0.0747,-0.0032,0.0428,-0.0252,0.0333,-0.006,0.2174,-0.0029,0.0315,0.0404,-0.0207,-0.0096,0.0496,-0.0224,-0.0208,0.0039,0.0663,-0.0417,0.044,0.0557,-0.0556,0.0445,0.0162,-0.0299,0.0274,-0.0359,-0.0082,-0.0439,0.056,-0.0165,-0.015,-0.0407,0.0197,0.0384,-0.0259,-0.0199,-0.0361,-0.0042,0.0078,0.0211,0.0021,-0.0188,-0.0502,-0.0621,0.0042,-0.0515,-0.0131,-0.0104,-0.0242]}
{"key":"[Self-Supervised Point Cloud Completion via Inpainting] When navigating in urban environments, many of the objects that need to be tracked and avoided are heavily occluded. Planning and tracking using these partial scans can be challenging. The aim of this work is to learn to complete these partial point clouds, giving us a full understanding of the object's geometry using only partial observations. Previous methods achieve this with the help of complete, ground-truth annotations of the target objects, which are available only for simulated datasets. However, such ground truth is unavailable for real-world LiDAR data. In this work, we present a self-supervised point cloud completion algorithm, PointPnCNet, which is trained only on partial scans without assuming access to complete, ground-truth annotations. Our method achieves this via inpainting. We remove a portion of the input data and train the network to complete the missing region. As it is difficult to determine which regions were occluded in the initial cloud and which were synthetically removed, our network learns to complete the full cloud, including the missing regions in the initial partial cloud. We show that our method outperforms previous unsupervised and weakly-supervised methods on both the synthetic dataset, ShapeNet, and real-world LiDAR dataset, Semantic KITTI.","layer":1,"vector":[-0.008,-0.0456,0.0382,-0.0094,0.0322,0.0508,0.0555,-0.0005,-0.0424,0.0056,0.0226,-0.0746,0.031,0.0585,-0.0142,-0.002,-0.0318,0.0945,-0.0396,0.008,0.0299,-0.0539,-0.0363,-0.0459,0.0379,0.0078,0.0036,-0.0287,-0.0219,-0.2293,-0.0303,-0.0378,0.0147,-0.0144,-0.0134,-0.0263,-0.0078,0.0591,0.0054,0.0192,0.0681,-0.0231,-0.0445,-0.0279,-0.0108,-0.0521,0.0135,-0.0081,-0.0026,-0.0342,0.0299,-0.0552,-0.0018,0.0199,-0.0005,0.079,0.0281,0.0145,0.0543,0.0222,0.0336,0.0305,-0.1716,0.017,0.058,0.0476,-0.0321,-0.0928,-0.0076,0.0651,-0.0148,0.0414,0.0597,0.0642,-0.0031,-0.0016,0.0048,0.0235,-0.0241,-0.0014,0.0403,-0.023,0.0169,0.0203,-0.0214,-0.0371,0.0072,-0.0479,0.0431,0.0366,-0.0171,-0.0293,-0.0265,0.0536,-0.0343,-0.0096,0.0287,0.0222,-0.0222,0.2086,-0.0681,0.0249,0.0274,0.0019,0.0286,-0.056,-0.0226,-0.0776,-0.0443,0.0442,-0.016,0.0133,-0.0039,-0.0129,0.0614,-0.0159,0.1246,0.0741,-0.0348,0.0031,-0.0139,-0.0094,0.0771,0.0166,0.0163,-0.0518,0.0183,0.1209,0.033,0.0233,0.0514,-0.0071,-0.0469,0.004,0.0243,0.0101,0.0054,-0.0206,0.036,-0.0049,-0.0455,-0.0247,0.0306,-0.0712,-0.0066,0.1311,-0.0465,0.0562,-0.0274,-0.0413,-0.0136,0.038,-0.0334,-0.0165,0.0264,0.0325,0.0393,0.037,-0.0462,0.0289,-0.0253,-0.0623,-0.0486,0.0967,-0.0086,-0.1096,-0.049,0.019,-0.0018,0.0077,-0.0251,0.0346,-0.0121,0.0195,0.0838,0.0145,-0.1578,0.0295,-0.0103,0.0224,-0.0094,-0.0595,-0.0091,0.0446,0.0287,-0.061,0.0026,-0.0411,0.0039,0.0358,-0.0346,0.039,0.0156,-0.0222,-0.0048,-0.0418,-0.0277,-0.018,0.0234,-0.0477,0.0189,-0.0121,-0.0174,0.0137,0.0208,0.0163,-0.0097,-0.0102,0.0263,0.0348,-0.0196,0.0306,0.0208,-0.0349,0.0237,-0.0075,0.0003,0.0072,-0.0769,0.0297,-0.0305,-0.06,-0.0718,-0.1815,0.0155,0.0284,-0.0238,0.0363,-0.0925,0.0207,0.0507,0.0184,0.0515,0.0696,-0.0408,0.0349,0.0049,-0.0016,0.0656,0.0245,0.0411,-0.036,-0.0135,0.0219,-0.0006,-0.0872,-0.0784,0.0451,-0.013,0.2638,0.0429,0.0553,-0.0202,0.0148,0.0136,-0.0966,-0.1185,0.0472,0.0206,0.0556,-0.0187,-0.0432,-0.0361,-0.0464,0.0709,0.0231,-0.0951,-0.0237,-0.04,-0.0662,0.0253,-0.0134,0.0061,0.0156,-0.0162,0.0292,-0.0114,-0.019,-0.0175,-0.0597,0.01,-0.0482,0.0163,-0.0119,-0.0496,0.0058,-0.0917,0.0749,0.0022,-0.0684,-0.0389,0.034,-0.0245,-0.0458,0.0622,-0.0285,-0.0338,0.0528,0.0164,0.0632,0.0381,-0.0236,-0.0116,0.0558,-0.0452,0.0401,0.0241,0.0357,0.0607,0.0935,-0.051,-0.0104,-0.0476,0.0337,0.0352,-0.0345,-0.0341,0.0897,0.0123,-0.2959,0.0065,0.0448,0.0476,-0.0257,0.0455,0.0559,0.0703,-0.0095,-0.0201,-0.0349,0.018,0.0096,-0.0222,0.0162,0.0182,0.0506,-0.0164,0.0459,-0.0385,-0.0137,0.0449,0.1851,-0.0516,0.0317,0.0217,-0.0623,-0.0337,0.0895,-0.0035,-0.0027,0.0237,0.0487,-0.0303,0.0242,0.075,-0.0085,0.0582,0.0073,-0.0007,-0.0075,-0.0173,-0.0088,-0.0441,0.017,-0.0171,-0.0385,-0.0119,0.0154,0.0225,-0.0294,-0.006,-0.0094,-0.0101,0.0023,0.0264,-0.0474,-0.0475,-0.0746,-0.0234,0.0118,-0.0306,0.0005,-0.053,0.0309]}
{"key":"[A Hierarchical Approach to Neural Context-Aware Modeling] We present a new recurrent neural network topology to enhance state-of-the-art machine learning systems by incorporating a broader context. Our approach overcomes recent limitations with extended narratives through a multi-layered computational approach to generate an abstract context representation. Therefore, the developed system captures the narrative on word-level, sentence-level, and context-level. Through the hierarchical set-up, our proposed model summarizes the most salient information on each level and creates an abstract representation of the extended context. We subsequently use this representation to enhance neural language processing systems on the task of semantic error detection. To show the potential of the newly introduced topology, we compare the approach against a context-agnostic set-up including a standard neural language model and a supervised binary classification network. The performance measures on the error detection task show the advantage of the hierarchical context-aware topologies, improving the baseline by 12.75% relative for unsupervised models and 20.37% relative for supervised models.","layer":11,"vector":[-0.0475,0.0021,0.0625,-0.0271,0.0064,0.0143,0.049,0.0393,0.0244,-0.0459,-0.0182,-0.0608,0.0196,0.0534,0.038,-0.022,0.0237,0.0492,-0.0164,0.0006,0.0151,-0.0469,0.0298,-0.0241,0.0053,0.0249,-0.0294,-0.0082,-0.0526,-0.2215,-0.0356,-0.0455,0.0255,-0.0039,0.0151,-0.0145,-0.0409,0.0217,-0.0063,0.0538,-0.007,0.0544,0.0004,-0.0839,-0.0082,-0.0729,0.0042,-0.0025,-0.0353,-0.0722,0.0102,-0.0317,0.0417,0.0097,0.004,0.0622,0.0474,0.053,0.0483,0.0345,0.0177,0.0436,-0.1728,0.0436,0.0269,0.0029,-0.0311,0.0195,-0.0222,0.0179,-0.0457,0.0118,-0.01,0.1005,-0.0019,-0.0132,0.011,-0.0595,0.0071,0.0144,0.0197,-0.0053,-0.0498,-0.0147,0.0019,-0.092,0.0077,-0.0234,0.0176,-0.0534,-0.0594,-0.0144,-0.0497,0.0238,-0.0758,-0.0284,0.0155,-0.0222,-0.0768,0.1989,-0.0125,0.0114,0.0401,-0.0505,0.0214,0.0048,0.0115,-0.0124,-0.0545,-0.0105,-0.0293,-0.0381,0.0141,-0.0151,0.0498,-0.0007,0.14,0.0084,-0.0144,-0.038,-0.0551,0.0425,0.0016,-0.0483,0.0114,-0.0597,0.0622,0.1011,0.0547,0.0065,0.0513,-0.0094,-0.0511,0.0122,0.0316,0.0046,0.0132,-0.0134,0.0221,-0.0205,-0.0488,-0.0585,-0.0373,-0.053,-0.099,0.1339,-0.0376,0.0374,-0.0733,-0.0213,-0.056,0.0194,-0.0323,-0.017,0.0002,0.0363,0.0444,0.0087,-0.036,0.0082,0.0104,-0.0263,-0.0542,0.084,0.036,-0.0731,-0.0241,0.0066,0.061,-0.0084,0.0598,-0.0091,-0.022,0.0213,0.0533,0.0461,-0.0666,0.0011,0.0199,0.0026,0.0234,-0.0882,-0.0614,0.0407,0.0462,-0.0672,0.0165,-0.0492,0.0616,0.0306,-0.0422,0.0361,0.0298,-0.0124,-0.0042,-0.0351,-0.0034,-0.0247,-0.0055,-0.0448,-0.0108,-0.0122,-0.0553,0.0065,0.0019,0.0112,-0.0487,0.0107,0.0005,0.0069,-0.034,0.0183,0.0678,-0.0245,-0.0069,-0.0251,0.0041,0.0461,0.0176,0.0627,0.051,-0.0115,-0.0302,-0.2535,0.0007,0.0364,-0.035,0.0036,-0.0529,0.0269,-0.0235,0.0402,0.0616,0.0612,-0.0372,0.0042,0.0091,0.0328,0.0479,0.0384,-0.0017,-0.0174,0.0039,0.0066,0.0184,-0.0161,-0.0662,0.025,-0.0188,0.2252,0.0212,0.0497,-0.0176,0.0519,0.0068,-0.0072,-0.116,0.08,0.0162,0.0447,0.0146,-0.0555,-0.0735,-0.0398,0.0345,-0.0295,-0.0844,-0.0616,-0.0152,-0.0288,-0.0027,-0.0039,0.0188,0.033,-0.05,0.0274,-0.0026,-0.0568,0.0096,-0.074,0.0142,-0.0227,0.0152,0.0454,0.0001,0.0525,-0.0464,0.0594,0.02,-0.003,-0.0638,0.0373,-0.0189,-0.0399,0.1037,0.0216,-0.0337,0.0639,-0.0036,0.0217,-0.0719,-0.0217,-0.0045,0.0666,-0.0823,0.0679,0.0492,0.032,0.0258,0.0674,-0.0092,0.0697,-0.0135,-0.0091,0.0285,-0.0591,-0.0058,0.045,-0.0412,-0.2757,0.0646,0.0213,0.026,-0.0061,0.0234,0.0571,0.0339,-0.0123,-0.0163,-0.0025,0.0527,0.0258,-0.0077,-0.0322,0.0469,0.0632,-0.0114,0.0742,-0.0412,0.0566,0.1037,0.2042,-0.0457,0.0086,0.0311,0.0079,-0.0279,0.0635,0.014,0.01,0.0139,0.1205,-0.028,0.0111,0.019,-0.0039,0.0123,0.0686,-0.0213,0.0038,0.0356,-0.0508,-0.077,0.0849,-0.0285,0.0002,-0.032,-0.017,0.0327,-0.0207,-0.003,0.0079,-0.0056,0.0332,0.0295,-0.0185,-0.0638,-0.0002,-0.0276,0.0121,-0.103,0.024,0.0162,-0.0267]}
{"key":"[DARA: Dynamics-Aware Reward Augmentation in Offline Reinforcement Learning] Offline reinforcement learning algorithms promise to be applicable in settings where a fixed dataset is available and no new experience can be acquired. However, such formulation is inevitably offline-data-hungry and, in practice, collecting a large offline dataset for one specific task over one specific environment is also costly and laborious. In this paper, we thus 1) formulate the offline dynamics adaptation by using (source) offline data collected from another dynamics to relax the requirement for the extensive (target) offline data, 2) characterize the dynamics shift problem in which prior offline methods do not scale well, and 3) derive a simple dynamics-aware reward augmentation (DARA) framework from both model-free and model-based offline settings. Specifically, DARA emphasizes learning from those source transition pairs that are adaptive for the target environment and mitigates the offline dynamics shift by characterizing state-action-next-state pairs instead of the typical state-action distribution sketched by prior offline RL methods. The experimental evaluation demonstrates that DARA, by augmenting rewards in the source offline dataset, can acquire an adaptive policy for the target environment and yet significantly reduce the requirement of target offline data. With only modest amounts of target offline data, our performance consistently outperforms the prior offline RL methods in both simulated and real-world tasks.","layer":1,"vector":[-0.0963,0.0053,0.0357,-0.0179,0.0113,0.0494,0.0132,0.0146,0.0509,0.0082,0.0442,-0.0088,0.0057,0.0444,0.0294,0.0016,-0.0093,0.0564,-0.0151,0.0077,0.0305,-0.0879,0.0206,-0.0017,0.0278,0.0294,-0.066,-0.0761,-0.032,-0.2342,-0.0044,-0.0464,0.0191,-0.0326,0.0005,0.0251,-0.0507,0.055,-0.0289,0.0135,0.0391,0.039,-0.0064,-0.0704,-0.0469,-0.0462,-0.0118,-0.0116,-0.0079,0.0105,0.0439,-0.0315,-0.0406,0.0411,0.0839,0.0432,0.0563,0.0744,0.0356,0.0319,0.0019,0.0427,-0.1535,0.0415,0.0267,0.0109,-0.0681,0.0084,0.0438,0.0421,-0.0391,0.0686,0.0329,0.0239,0.048,0.0093,-0.0521,-0.0201,0.0247,-0.0109,0.0236,-0.0353,-0.0339,-0.0146,-0.0152,-0.0833,0.002,-0.0526,0.0666,0.0244,-0.0543,0.0323,0.0059,0.0059,-0.0775,-0.013,0.0339,0.0273,-0.0764,0.215,-0.0103,0.0372,0.0044,0.0159,0.0308,-0.0639,-0.0325,-0.0241,-0.0356,-0.0152,-0.0258,0.0323,0.0298,-0.0288,0.0158,0.0304,0.0707,-0.0004,0.0225,-0.0263,-0.021,0.014,0.0422,-0.0384,0.0116,-0.0629,0.0224,0.1286,-0.0063,0.0001,0.041,-0.0495,-0.0268,-0.0487,0.0048,-0.0228,0.0523,-0.0142,0.0387,0.0129,-0.0214,-0.0029,-0.0133,-0.1546,-0.0189,0.1108,0.0316,0.0392,-0.0268,-0.0244,0.0041,0.0371,-0.0426,-0.0053,0.0066,0.0458,0.0205,0.047,-0.0517,-0.0091,-0.0526,-0.08,-0.0251,0.0584,-0.0464,-0.0943,-0.0238,-0.0216,-0.0156,0.0206,0.0045,0.0195,-0.0743,0.025,0.0949,-0.0155,-0.0698,-0.0267,-0.0135,0.0231,0.0549,-0.0419,-0.041,0.0441,0.0112,-0.0375,-0.0046,-0.0504,0.0665,-0.0038,-0.0069,-0.0099,-0.0304,0.0059,-0.0257,-0.0276,-0.0256,-0.0263,0.018,-0.0062,0.0209,0.0059,-0.0374,0.0022,-0.0307,0.0129,-0.0085,-0.0048,0.0421,0.0345,-0.029,0.0276,0.04,-0.0331,-0.0385,-0.0034,0.0575,-0.0139,0.0035,0.0147,-0.0025,-0.0152,0.0011,-0.2092,0.0042,-0.0536,-0.0187,0.0419,-0.0523,0.0349,-0.0495,0.0602,0.0418,0.0606,-0.0162,-0.041,0.0373,-0.0156,0.0562,0.0423,0.0307,0.0016,0.0342,-0.0038,-0.0026,-0.052,-0.1277,0.0627,0.0145,0.2103,0.0331,0.0515,-0.0255,-0.0082,0.0673,-0.0002,-0.1193,0.0436,0.0122,0.0881,-0.0354,0.0057,-0.0248,-0.003,0.0166,0.0037,-0.1109,-0.0287,-0.0132,-0.0112,0.0295,-0.0494,-0.028,0.0595,-0.0174,0.033,-0.0335,-0.0157,-0.0384,-0.0515,0.0497,-0.0343,-0.0055,0.0035,-0.0328,0.0187,-0.0545,0.0489,-0.0097,-0.0227,-0.0614,0.0774,-0.0144,0.0025,0.0572,0.0124,0.009,0.0211,0.0151,0.025,-0.0269,-0.0896,-0.0054,0.0484,-0.0249,0.0141,0.0799,0.0065,-0.001,0.0566,-0.024,0.0027,-0.0015,0.0043,0.018,-0.0598,-0.0231,0.0784,0.0197,-0.3018,0.0608,0.024,0.0303,-0.0218,0.0217,0.0653,0.0307,-0.0521,-0.0324,0.0112,0.0731,0.0117,0.0058,0.0373,0.0503,0.1067,-0.0257,0.0394,-0.0643,0.0428,0.0891,0.2139,-0.0887,0.0536,-0.0125,-0.0462,0.0116,0.0349,-0.0256,0.0241,0.0186,0.0608,-0.0553,0.0423,0.0704,-0.0325,0.0367,0.0064,-0.0061,-0.0248,0.042,-0.0032,-0.0319,0.0873,-0.0181,-0.0337,-0.0802,-0.0203,0.0341,-0.0062,-0.0167,0.0043,-0.0225,0.0596,0.0167,-0.0135,-0.0729,-0.0214,-0.0413,-0.0083,-0.0278,0.014,-0.0375,0.0001]}
{"key":"[Wireless Power Control via Counterfactual Optimization of Graph Neural Networks] We consider the problem of downlink power control in wireless networks, consisting of multiple transmitter-receiver pairs communicating with each other over a single shared wireless medium. To mitigate the interference among concurrent transmissions, we leverage the network topology to create a graph neural network architecture, and we then use an unsupervised primal-dual counterfactual optimization approach to learn optimal power allocation decisions. We show how the counterfactual optimization technique allows us to guarantee a minimum rate constraint, which adapts to the network size, hence achieving the right balance between average and $5^{th}$ percentile user rates throughout a range of network configurations.","layer":2,"vector":[-0.0299,-0.0031,0.0329,0.0102,-0.012,0.0355,0.0199,0.013,0.0159,-0.0017,0.0314,-0.0557,0.0298,0.0908,-0.0164,0.0622,-0.0158,0.0378,-0.0368,0.0464,0.0648,-0.057,-0.0312,-0.0553,0.031,-0.016,-0.0451,-0.0436,-0.041,-0.2222,0.02,-0.0104,0.0758,-0.0287,0.0046,-0.0413,0.0024,0.0019,-0.0382,0.0527,0.0273,0.0066,-0.0114,-0.0526,-0.044,-0.0362,-0.0152,0.0157,-0.0162,-0.0299,0.0679,-0.0006,-0.0037,0.0164,0.0608,0.0388,0.0666,0.0817,0.0429,0.0669,0.0097,0.0785,-0.1524,0.0986,0.0657,0.04,-0.0441,-0.0057,0.0014,0.0538,-0.0234,0.046,0.033,-0.0019,0.034,0.0372,-0.0555,-0.0189,-0.0184,0.0175,0.0072,-0.0328,-0.0174,-0.0081,-0.0023,-0.066,0.0135,-0.0187,0.021,0.0062,-0.072,-0.0154,0.0089,0.0185,-0.0585,-0.0178,0.0291,0.025,-0.1294,0.1914,-0.0402,-0.0216,0.0374,-0.0171,-0.0266,-0.0645,-0.0462,-0.026,-0.0399,0.0152,-0.0275,-0.0272,-0.0016,-0.0329,0.0488,0.0564,0.0149,0.0564,0.0089,-0.0139,-0.0639,0.019,0.0742,-0.0334,0.0609,-0.1054,-0.0085,0.1463,-0.0272,0.0172,0.0412,-0.0171,-0.0007,-0.0465,0.0057,-0.0075,0.0171,0.0173,0.0104,0.0089,0.0215,-0.0618,0.035,-0.0996,-0.0294,0.1149,-0.0076,0.0176,-0.0299,-0.0397,-0.0273,0.0067,-0.0249,-0.0418,-0.0404,0.0632,0.0211,0.0282,-0.067,0.027,-0.024,0.0332,-0.0519,0.1114,0.0116,-0.1194,0.0052,0.0032,-0.0182,-0.0153,0.0184,0.0332,-0.0711,-0.002,0.0567,0.0244,-0.1099,-0.0503,-0.0273,-0.0087,-0.0088,0.0076,-0.0248,0.0197,-0.0217,-0.0331,0.0227,-0.0208,0.0171,0.0484,-0.0735,0.0059,-0.0117,0.0181,-0.0395,-0.0093,-0.0232,0.0003,0.0012,-0.0076,-0.0105,-0.019,-0.0702,0.0276,0.0226,0.0161,-0.0305,-0.0021,0.0273,0.0115,-0.025,-0.0103,0.0439,-0.0317,-0.028,0.014,0.0064,0.0289,0.0221,0.047,0.0539,0.0132,-0.0584,-0.2004,0.0111,-0.0146,-0.0122,0.0546,-0.0519,0.0172,-0.0144,0.0201,0.0616,0.0629,0.0156,-0.0931,-0.0132,-0.0137,0.1114,0.023,0.0414,-0.0336,0.0013,0.0148,0.0153,0.0154,-0.0704,0.0684,0.0158,0.2006,-0.0026,0.0772,-0.015,0.0709,0.019,-0.0297,-0.0394,0.0315,0.0613,0.0777,-0.0351,-0.0063,-0.0381,-0.0259,0.0475,-0.0361,-0.0828,-0.0629,-0.0379,-0.0457,-0.0304,-0.0338,-0.0224,0.0522,-0.0284,0.047,0.0211,0.0062,-0.0496,-0.1065,0.0327,-0.0572,0.0482,0.0211,-0.0682,-0.017,-0.0448,0.0387,0.0295,-0.0077,-0.0222,0.0192,0.0643,0.004,0.0477,0.0429,0.013,0.0612,-0.003,0.0505,-0.0256,-0.0486,-0.0001,0.0808,-0.0405,0.0101,-0.0022,0.0022,-0.012,0.0692,0.0103,0.0452,0.0029,-0.0168,0.034,-0.0099,-0.0005,0.0524,-0.0019,-0.2945,0.0482,0.0504,0.0021,-0.029,0.0313,0.0913,0.0533,-0.1173,0.0295,0.0032,0.0261,-0.0114,0.0224,0.0188,0.022,0.0288,-0.0716,0.0268,-0.0538,0.0442,0.0004,0.1874,-0.069,0.042,0.0362,-0.0486,0.0689,0.0336,-0.0231,0.0069,0.0103,0.0985,-0.0648,0.0459,0.0507,-0.0286,0.0278,0.0135,-0.0123,-0.0478,0.001,0.0056,-0.0309,0.0966,-0.024,-0.0574,-0.0301,0.0105,0.0131,-0.0078,0.027,-0.0246,-0.0164,0.0247,-0.0101,-0.0494,-0.0848,-0.0344,-0.0823,0.028,-0.0563,-0.0429,-0.0059,0.0086]}
{"key":"[Provably efficient RL with Rich Observations via Latent State Decoding] We study the exploration problem in episodic MDPs with rich observations generated from a small number of latent states. Under certain identifiability assumptions, we demonstrate how to estimate a mapping from the observations to latent states inductively through a sequence of regression and clustering steps -- where previously decoded latent states provide labels for later regression problems -- and use it to construct good exploration policies. We provide finite-sample guarantees on the quality of the learned state decoding function and exploration policies, and complement our theory with an empirical evaluation on a class of hard exploration problems. Our method exponentially improves over $Q$-learning with na\\\"ive exploration, even when $Q$-learning has cheating access to latent states.","layer":1,"vector":[-0.041,-0.0102,0.0205,-0.0277,-0.0112,0.0589,0.0392,0.0061,0.0019,0.0009,0.0472,-0.0423,0.0281,0.0555,0.0141,0.0313,-0.0094,0.0488,-0.0122,-0.0035,0.0553,-0.0285,0.0003,-0.0306,0.0098,0.052,-0.0187,-0.0871,-0.0453,-0.2488,0.0109,-0.0344,0.0565,-0.0154,0.0245,-0.0284,-0.043,0.0625,-0.0122,0.0171,0.0392,0.0017,-0.0343,-0.0651,-0.0688,-0.047,-0.0062,-0.034,0.0156,-0.0303,-0.0008,-0.001,0.0189,0.0258,0.0681,0.0321,0.0521,0.0646,0.0259,0.0349,0.0321,0.0332,-0.1304,0.021,0.0583,0.0533,-0.0238,-0.0354,0.0224,0.0304,-0.014,0.0509,-0.0146,0.0444,-0.0039,0.0093,0.008,-0.0535,-0.0124,-0.0119,0.0325,-0.0441,-0.0285,0.0254,-0.0352,-0.0823,0.0055,-0.0154,0.0493,0.0376,0.0108,-0.0038,-0.0415,0.0451,-0.0529,-0.0094,0.0161,0.0523,-0.0113,0.2009,-0.015,0.0975,0.0264,-0.0268,0.0117,-0.0462,-0.0214,-0.0104,-0.0169,0.0048,-0.0176,-0.0216,0.0459,-0.0385,-0.0339,-0.0009,0.0755,0.0918,-0.0207,0.0134,-0.031,-0.0007,0.0415,-0.0345,0.0143,-0.116,0.0253,0.1238,0.0399,0.0116,-0.0199,-0.0386,-0.0064,0.0034,0.0114,0.0206,0.0288,-0.0098,0.0391,-0.0103,-0.0877,0.0083,0.0266,-0.1198,-0.0712,0.1154,0.0185,0.031,-0.0485,-0.0327,-0.0184,0.0039,-0.016,-0.0485,0.0177,0.0531,0.0532,0.0025,-0.0716,0.0206,-0.0417,-0.0708,-0.0161,0.1229,-0.0068,-0.0447,-0.0187,-0.0153,0.0117,-0.0197,0.0337,0.0185,-0.063,0.0629,0.0477,-0.0078,-0.0858,0.0307,0.0053,-0.0013,-0.0309,-0.0675,-0.0165,0.0456,0.0261,-0.0752,0.0362,-0.0101,0.0277,0.0106,0.0127,0.0203,-0.0158,0.0265,-0.0828,-0.0557,0.0076,-0.0096,0.0127,-0.032,-0.0065,-0.0416,-0.0528,0.0055,0.0058,0.0317,0.0028,-0.0237,0.0311,0.0161,-0.0457,0.0244,0.0277,-0.0131,0.0023,-0.0071,0.0453,0.0424,0.0218,0.0574,0.0451,-0.0378,-0.0104,-0.2439,-0.0163,0.0241,-0.0327,0.0533,-0.0643,0.0117,-0.0417,0.066,0.0759,-0.0014,-0.048,-0.0748,0.051,0.0033,0.061,0.058,0.026,-0.0077,0.0099,0.0134,-0.0146,-0.0275,-0.116,0.0367,-0.011,0.2622,0.0493,0.0127,0.0008,0.0271,0.0249,-0.0199,-0.1112,0.0483,0.0136,0.0794,0.0185,0.0107,-0.0618,-0.049,-0.0117,-0.0206,-0.0951,-0.0472,-0.0163,-0.0286,0.0038,-0.0134,0.0058,0.0515,-0.0157,0.0496,-0.0522,-0.0153,-0.0516,-0.068,-0.0073,-0.091,0.0571,0.0163,-0.0364,-0.027,-0.0724,0.0572,-0.0008,-0.0145,-0.0449,0.0541,-0.0219,-0.0601,0.04,-0.0454,-0.0129,0.0126,0.0095,0.0063,-0.0397,-0.0269,-0.0276,0.0638,-0.0228,0.0258,-0.0045,0.0263,-0.0121,0.0563,-0.0089,0.0149,-0.0096,-0.0161,0.0084,-0.0616,-0.0233,0.0438,-0.0172,-0.2873,0.0567,0.0089,0.0036,-0.0563,0.029,0.0541,-0.003,-0.0387,-0.0063,0.014,0.0473,0.0553,-0.0195,0.0297,0.0363,0.081,-0.0231,0.0631,-0.0688,0.0412,0.025,0.2035,0.006,0.0434,-0.0068,-0.0337,-0.0249,0.0354,0.0097,0.0086,-0.0034,0.0553,-0.0226,0.0653,0.0968,0.0004,0.023,0.0197,-0.0096,-0.0006,-0.0155,-0.0414,-0.0098,0.1002,0.0141,-0.0072,-0.0376,-0.0398,0.0626,-0.0178,0.0366,-0.047,-0.0094,0.0194,0.0332,-0.0386,-0.0182,0.0268,-0.069,0.025,-0.0426,0.0214,-0.0012,-0.004]}
{"key":"[Automatic Breast Lesion Detection in Ultrafast DCE-MRI Using Deep Learning] Purpose: We propose a deep learning-based computer-aided detection (CADe) method to detect breast lesions in ultrafast DCE-MRI sequences. This method uses both the three-dimensional spatial information and temporal information obtained from the early-phase of the dynamic acquisition. Methods: The proposed CADe method, based on a modified 3D RetinaNet model, operates on ultrafast T1 weighted sequences, which are preprocessed for motion compensation, temporal normalization, and are cropped before passing into the model. The model is optimized to enable the detection of relatively small breast lesions in a screening setting, focusing on detection of lesions that are harder to differentiate from confounding structures inside the breast. Results: The method was developed based on a dataset consisting of 489 ultrafast MRI studies obtained from 462 patients containing a total of 572 lesions (365 malignant, 207 benign) and achieved a detection rate, sensitivity, and detection rate of benign lesions of 0.90 (0.876-0.934), 0.95 (0.934-0.980), and 0.81 (0.751-0.871) at 4 false positives per normal breast with 10-fold cross-testing, respectively. Conclusions: The deep learning architecture used for the proposed CADe application can efficiently detect benign and malignant lesions on ultrafast DCE-MRI. Furthermore, utilizing the less visible hard-to detect-lesions in training improves the learning process and, subsequently, detection of malignant breast lesions.","layer":7,"vector":[-0.0451,0.0163,0.0502,0.0104,0.0635,0.0499,0.0474,0.0005,0.0452,-0.0352,0.0238,-0.0626,0.0118,0.0424,-0.0077,0.0123,0.004,0.0085,-0.0225,0.0191,0.0068,-0.0327,0.004,-0.0294,0.027,-0.029,-0.0035,-0.0251,-0.089,-0.2391,0.0451,-0.0564,0.0248,-0.0261,0.0154,-0.0556,-0.0419,0.017,-0.0259,0.0014,0.0268,0.0172,-0.0163,-0.0782,-0.0219,-0.0355,-0.0262,-0.0475,0.027,-0.0383,0.0404,-0.0311,0.0382,0.059,0.0109,0.0141,0.1037,0.0071,0.0566,0.0456,0.0491,0.0558,-0.1812,0.0457,0.007,0.006,-0.0252,-0.0572,0.0653,0.0027,-0.026,0.0712,0.017,0.0069,0.0257,-0.0344,0.0029,-0.025,-0.0088,0.0304,-0.0069,0.0056,-0.0224,-0.0666,-0.0048,-0.0295,-0.0209,-0.0718,0.0444,-0.0328,-0.0369,-0.0124,-0.0483,0.0359,-0.0985,-0.0094,0.05,0.0584,-0.0448,0.1876,-0.0601,0.0196,0.0799,-0.041,0.0388,-0.0401,0.0228,-0.051,-0.0439,0.034,0.0135,-0.01,0.0353,-0.0143,0.0143,0.019,0.0443,-0.0157,-0.022,0.0164,-0.0349,0.0089,0.0561,-0.03,-0.0248,-0.063,0.031,0.1434,0.0164,0.0386,0.0328,-0.0172,-0.0131,-0.0001,-0.0069,-0.0216,-0.0023,-0.0311,0.0064,-0.0276,-0.0462,-0.0714,0.054,-0.0825,-0.0016,0.0998,-0.0712,0.0588,-0.1275,-0.0184,0.0189,-0.0012,-0.0703,-0.0402,-0.0082,0.0205,0.0498,0.0194,-0.0478,-0.034,-0.0297,-0.0647,-0.031,0.1179,0.0301,-0.078,0.0049,-0.0335,0.0025,-0.0233,0.0297,0.0551,-0.0756,0.0426,0.0726,0.0143,-0.052,-0.0138,0.0168,0.0011,0.0524,-0.0435,-0.0299,0.0499,0.0612,-0.0522,0.0207,-0.0307,0.027,0.0072,-0.0425,0.0268,-0.0493,0.0343,-0.0293,-0.0106,-0.0124,-0.0092,0.0029,-0.0247,0.0471,-0.0343,-0.0042,0.0488,0.0218,0.0118,-0.0183,0.0246,0.0041,0.0281,-0.0396,0.0184,0.0698,-0.0623,-0.0349,-0.0077,0.0048,0.0533,-0.0076,0.0552,0.0558,-0.0536,-0.0649,-0.2272,0.0063,0.019,-0.0183,0.0308,-0.0979,-0.0249,-0.0156,0.0778,0.037,0.0403,0.0182,-0.0205,-0.0003,0.021,0.047,0.0235,0.0154,-0.0391,-0.051,0.0123,0.0455,-0.0482,-0.0715,0.0859,0.0172,0.2038,0.0403,0.0424,-0.019,0.0133,-0.0017,-0.042,-0.1103,0.0189,0.0062,0.0339,0.002,-0.0686,-0.0064,-0.0306,-0.0119,-0.0187,-0.0397,-0.0407,-0.0049,-0.0064,0.0354,0.0058,0.0117,0.0576,-0.0859,0.0236,0.0102,0.0296,-0.0283,-0.1139,0.0549,-0.0697,-0.0162,0.0477,-0.0499,-0.0297,-0.0832,0.0249,0.0093,-0.0403,-0.0577,0.025,-0.0027,-0.0041,0.1126,-0.005,-0.0138,0.0971,-0.0129,0.0312,-0.0434,-0.0237,-0.0343,0.1015,0.0065,0.0267,0.0173,0.0319,0.0477,0.1116,-0.0223,-0.0196,0.0218,0.0001,0.0279,-0.0427,-0.0315,-0.0223,-0.0134,-0.2738,0.0796,0.0171,0.0416,-0.0254,0.0372,-0.0003,0.0019,0.0088,-0.0132,-0.0337,0.0429,0.0368,-0.0117,0.0347,0.0162,0.0839,-0.0315,0.0422,-0.0603,0.0116,0.0394,0.1805,0.0009,-0.0213,0.0502,-0.0153,-0.0025,0.0084,0.0028,-0.0059,0.0729,0.0286,-0.0304,0.0557,0.1288,-0.0273,0.0848,-0.0119,-0.0309,0.0308,0.0158,-0.0226,0.0052,0.0512,-0.036,-0.0114,0.0211,-0.0168,0.0522,-0.0229,-0.0356,-0.0003,-0.0176,0.0515,0.0398,-0.005,-0.0559,-0.0263,-0.0216,0.0227,-0.035,-0.0149,0.0452,-0.0057]}
{"key":"[Throttling Malware Families in 2D] Malicious software are categorized into families based on their static and dynamic characteristics, infection methods, and nature of threat. Visual exploration of malware instances and families in a low dimensional space helps in giving a first overview about dependencies and relationships among these instances, detecting their groups and isolating outliers. Furthermore, visual exploration of different sets of features is useful in assessing the quality of these sets to carry a valid abstract representation, which can be later used in classification and clustering algorithms to achieve a high accuracy. In this paper, we investigate one of the best dimensionality reduction techniques known as t-SNE to reduce the malware representation from a high dimensional space consisting of thousands of features to a low dimensional space. We experiment with different feature sets and depict malware clusters in 2-D. Surprisingly, t-SNE does not only provide nice 2-D drawings, but also dramatically increases the generalization power of SVM classifiers. Moreover, obtained results showed that cross-validation accuracy is much better using the 2-D embedded representation of samples than using the original high-dimensional representation.","layer":1,"vector":[-0.0097,-0.0216,0.0156,-0.0377,0.05,0.0131,0.0188,0.0416,-0.0116,-0.0142,0.0268,-0.0509,0.0665,0.0206,0.0087,-0.0281,0.0249,0.0321,-0.044,0.0059,0.015,-0.0215,0.0051,-0.0567,0.0491,0.0407,-0.0279,0.0065,-0.0779,-0.2481,0.0253,-0.0582,0.0385,0.0044,0.0244,-0.0381,-0.0083,0.0714,-0.042,-0.0126,0.005,0.0626,-0.024,-0.0694,-0.0652,-0.0783,-0.032,-0.0159,0.0203,-0.06,0.0121,-0.0399,0.0051,0.0563,0.0394,0.0113,0.0644,0.0049,0.0419,0.0051,0.0736,0.0362,-0.1239,0.0849,0.0512,0.0453,-0.0671,0.0094,0.0307,0.0485,-0.0372,0.033,-0.0001,0.0744,0.0041,-0.0227,-0.0195,-0.0122,-0.0073,-0.0215,0.009,-0.0405,-0.0011,-0.0053,-0.0087,-0.0211,0.0076,-0.0281,0.1024,-0.0041,-0.0226,-0.0039,-0.0426,0.0001,-0.0561,-0.0381,0.0482,0.022,-0.0369,0.1731,-0.0422,-0.017,0.0557,-0.0217,0.0375,-0.0414,0.0029,-0.0678,-0.0131,-0.0043,0.0072,0.0216,0.011,-0.0276,0.0145,-0.0334,0.0487,0.0237,0.0015,-0.0003,-0.0166,0.0104,0.0649,-0.034,0.0522,-0.0319,0.0506,0.143,0.0306,0.0233,-0.0054,0.0183,-0.0088,0.0014,0.029,0.0334,-0.0282,0.0369,-0.0036,-0.0504,-0.0327,-0.0494,0.0204,-0.0758,-0.0508,0.0941,-0.0751,0.0306,-0.005,-0.0145,0.0107,0.0304,-0.0008,-0.0704,0.01,0.0168,0.049,0.0505,-0.0648,0.0292,-0.0001,-0.0352,-0.0475,0.1036,0.0149,-0.0812,-0.0379,0.0107,0.0363,0.0076,-0.002,0.0307,-0.0632,0.0288,0.0585,-0.0072,-0.0707,-0.0176,-0.0039,-0.0108,0.0595,-0.045,-0.0295,0.0476,0.0762,-0.0533,0.0171,-0.0419,0.014,0.0214,-0.0254,0.0138,-0.0171,-0.0374,-0.0213,0.001,-0.0249,0.0269,-0.0296,-0.0182,0.0693,0.0171,-0.0715,0.0254,-0.0067,0.0337,0.0023,-0.0299,0.0005,0.0493,-0.0454,-0.0215,0.0325,-0.0423,0.0107,-0.0273,-0.0235,0.0411,0.0122,0.0287,0.063,-0.0203,-0.0921,-0.2421,-0.0426,-0.0129,-0.0451,0.0052,-0.0842,0.0072,-0.0519,0.0455,0.0251,0.0552,0.0173,-0.0504,0.0015,-0.0044,0.0849,0.044,0.0347,-0.0379,0.0156,-0.0211,0.0199,0.0287,-0.0583,0.0259,0.0354,0.2022,0.0362,0.0394,-0.0572,0.0243,0.042,-0.0282,-0.0951,0.0835,0.0556,0.0307,-0.0124,-0.0394,-0.0292,-0.0568,0.036,0.0268,-0.1136,-0.0136,-0.0505,0.0118,0.0159,-0.0367,-0.0121,0.068,-0.0043,0.0723,-0.0061,0.023,-0.0436,-0.0951,0.0407,-0.0339,0.0666,-0.0103,-0.0911,0.013,-0.0941,0.066,-0.0045,-0.0636,-0.0425,0.0499,-0.0322,-0.0556,0.0874,-0.0174,-0.0238,0.0688,0.01,0.0355,-0.0101,-0.0881,-0.0014,0.0447,-0.0014,0.0188,0.0086,0.0127,0.0113,0.0503,0.0042,0.0293,-0.04,-0.0054,0.0163,-0.0133,-0.0126,-0.0134,0.0505,-0.2822,0.0283,0.0029,0.0294,-0.0061,-0.004,0.0334,-0.0228,-0.0014,-0.0461,0.0279,0.0301,0.0943,-0.0305,0.0041,0.0117,0.0726,-0.0919,0.055,-0.0517,0.0114,0.0312,0.2509,-0.0397,-0.0136,0.0241,-0.0122,0.0087,-0.0006,-0.0123,0.0143,-0.0014,0.1198,-0.0225,0.002,0.0728,-0.0411,0.001,0.0167,-0.0128,-0.0122,0.0092,-0.0566,-0.035,0.1067,-0.0356,0.0231,-0.0277,0.0335,0.0645,-0.0227,0.0081,-0.0213,0.0033,0.0411,0.0543,-0.0485,0.0259,-0.0497,-0.0502,0.0074,-0.0516,-0.0061,-0.0026,0.0273]}
{"key":"[Transductive Adversarial Networks (TAN)] Transductive Adversarial Networks (TAN) is a novel domain-adaptation machine learning framework that is designed for learning a conditional probability distribution on unlabelled input data in a target domain, while also only having access to: (1) easily obtained labelled data from a related source domain, which may have a different conditional probability distribution than the target domain, and (2) a marginalised prior distribution on the labels for the target domain. TAN leverages a fully adversarial training procedure and a unique generator/encoder architecture which approximates the transductive combination of the available source- and target-domain data. A benefit of TAN is that it allows the distance between the source- and target-domain label-vector marginal probability distributions to be greater than 0 (i.e. different tasks across the source and target domains) whereas other domain-adaptation algorithms require this distance to equal 0 (i.e. a single task across the source and target domains). TAN can, however, still handle the latter case and is a more generalised approach to this case. Another benefit of TAN is that due to being a fully adversarial algorithm, it has the potential to accurately approximate highly complex distributions. Theoretical analysis demonstrates the viability of the TAN framework.","layer":3,"vector":[-0.0228,-0.0243,-0.0293,0.0186,0.0424,0.0185,0.0607,0.006,0.007,0.0139,-0.0122,-0.0506,0.023,0.0839,0.0282,0.0203,0.02,0.0329,-0.0309,0.0186,0.0467,-0.0015,0.0424,-0.0232,0.0403,0.024,-0.0207,-0.0089,-0.0382,-0.2386,-0.0055,-0.0933,-0.0133,-0.0042,0.0106,-0.0501,-0.0308,0.011,0.0161,0.0498,0.037,0.0386,-0.062,-0.108,-0.0265,-0.0053,-0.0365,0.0162,-0.0407,-0.0466,0.0341,-0.0218,0.0226,0.0336,0.0242,-0.0177,0.0584,0.0598,0.0506,0.0481,0.0293,0.0628,-0.1403,0.0882,0.0599,0.031,-0.0348,-0.0487,-0.0152,0.0389,-0.0081,0.031,0.0024,0.0325,0.0056,-0.0094,0.0081,-0.0552,-0.0254,0.0244,0.0755,-0.0084,-0.0118,-0.0055,0.0001,-0.048,-0.0066,-0.0277,0.0294,-0.0111,-0.003,-0.0018,-0.0146,0.0381,-0.012,-0.0186,0.0304,-0.0273,-0.0396,0.2201,-0.0381,-0.0088,0.0241,-0.0298,0.0182,0.014,-0.049,-0.0432,-0.0173,0.0134,-0.0446,-0.0294,-0.0041,-0.0339,0.0131,0.0023,0.0384,0.0372,-0.037,-0.0335,-0.0653,0.0131,0.0296,-0.02,0.0272,-0.0239,0.0076,0.1526,0.0418,0.0366,0.0497,-0.0223,-0.0314,-0.031,0.0359,0.0351,-0.0012,-0.0228,-0.0009,0.0054,-0.0224,-0.0457,0.0277,-0.0122,-0.0362,0.0719,-0.0153,-0.0188,0.0062,-0.0096,0.0264,0.0395,0.0071,-0.0223,-0.0104,0.0469,0.0199,0.0344,0.0147,0.0238,-0.0116,-0.0687,-0.0265,0.1073,0.0309,-0.1116,-0.0356,-0.0067,0.0333,-0.0452,0.0202,0.0206,-0.0362,0.0304,0.0776,0.0393,-0.0942,-0.0456,-0.0247,0.0453,0.0415,-0.0517,-0.043,0.0703,0.019,-0.0079,0.0149,-0.057,0.0126,0.0744,-0.0358,0.0221,-0.0362,-0.0307,-0.0246,-0.038,-0.0034,0.0426,0.0023,-0.0089,0.0013,0.0182,-0.0637,0.0064,-0.0303,0.0178,0.0042,-0.0141,0.0286,0.0344,-0.0139,0.0289,0.0465,-0.025,-0.0689,-0.025,0.0215,0.0365,0.0072,0.0421,0.0176,-0.0243,-0.0091,-0.2372,-0.0153,-0.0162,-0.0362,0.0307,-0.0802,0.0466,0.0054,0.0512,0.0567,0.0642,0.0036,-0.0191,0.018,-0.0339,0.0256,0.0154,-0.0266,0.0319,-0.0176,-0.0362,0.0262,0.0241,-0.1352,0.0438,0.0328,0.2354,0.013,0.0611,-0.0309,-0.0211,0.0272,-0.0278,-0.1118,0.0657,-0.0062,0.0464,0.0017,-0.0337,-0.0017,-0.022,0.0272,0.0277,-0.1636,-0.0263,-0.0497,-0.0051,0.0091,-0.0346,0.0382,0.0533,-0.0198,0.0775,0.008,-0.0238,-0.015,-0.1191,0.0467,-0.0529,0.0202,-0.0372,-0.0362,0.0225,-0.0719,0.0489,0.0124,-0.0239,-0.0686,0.0785,-0.0053,-0.023,0.0495,0.0244,0.0438,0.0266,-0.0199,0.0194,-0.0394,-0.0464,-0.0311,0.0673,-0.0009,0.0338,-0.0139,0.0462,0.0026,0.1142,-0.0085,0.0119,-0.019,-0.0174,0.0059,-0.0502,0.0149,0.0519,-0.0103,-0.3258,0.0194,0.0152,0.0389,-0.0108,0.0119,0.0418,0.0502,-0.0826,-0.0369,-0.0276,0.023,0.0134,-0.0194,-0.0112,0.0202,0.0697,-0.061,0.0227,-0.0498,0.0347,0.0214,0.2149,-0.0359,0.0298,0.0123,-0.0139,0.0178,0.0112,-0.0533,0.0023,0.0285,0.0816,-0.0287,0.0135,0.1032,-0.034,0.0247,0.0038,-0.0196,-0.0316,0.0019,-0.0206,0.001,0.0903,0.0093,-0.0277,-0.0342,-0.0525,0.0144,-0.0187,0.023,-0.0045,0.0168,0.0578,0.0183,-0.0048,-0.0516,-0.0514,-0.0488,0.0095,-0.0515,-0.0186,-0.0036,-0.0384]}
{"key":"[Joint cross-domain classification and subspace learning for unsupervised adaptation] Domain adaptation aims at adapting the knowledge acquired on a source domain to a new different but related target domain. Several approaches have beenproposed for classification tasks in the unsupervised scenario, where no labeled target data are available. Most of the attention has been dedicated to searching a new domain-invariant representation, leaving the definition of the prediction function to a second stage. Here we propose to learn both jointly. Specifically we learn the source subspace that best matches the target subspace while at the same time minimizing a regularized misclassification loss. We provide an alternating optimization technique based on stochastic sub-gradient descent to solve the learning problem and we demonstrate its performance on several domain adaptation tasks.","layer":9,"vector":[-0.0297,-0.0623,0.0196,0.0007,0.0485,0.0191,0.0098,0.0249,0.0277,0.0231,0.041,-0.0507,-0.0191,0.0554,0.0444,0.0087,0.0036,0.1011,-0.0484,0.0023,0.0261,0.0217,0.0069,-0.0346,0.007,-0.0048,-0.0436,-0.0441,-0.0514,-0.2423,0.0192,-0.0328,0.0193,-0.0042,0.0266,-0.0672,-0.0321,0.0588,-0.0467,0.0374,-0.0065,0.0165,-0.015,-0.0342,-0.036,-0.0696,-0.0059,-0.0234,-0.0055,-0.0104,0.047,-0.0099,0.0237,0.0221,0.0163,0.0422,0.0425,0.027,0.0334,0.0241,0.0054,0.0407,-0.157,0.0792,0.0239,0.0178,-0.0353,-0.0257,0.0129,0.0609,-0.0169,0.0546,0.0303,0.0524,0.0087,0.0101,-0.0407,-0.0199,0.0156,0.0319,0.0552,-0.006,-0.0509,-0.0154,-0.0098,-0.0597,0.0195,-0.0329,0.0615,0.0323,-0.0885,-0.031,0.0036,0.0303,-0.0762,-0.0212,0.0589,0.0289,-0.0443,0.2077,-0.0605,-0.0054,0.035,-0.0284,-0.0042,-0.0234,-0.0352,0.0114,-0.0085,-0.021,-0.0157,0.008,0.0095,-0.0368,0.0388,-0.0118,0.0515,0.0181,-0.032,-0.0276,-0.0408,-0.0401,0.0913,-0.04,0.0385,-0.0377,0.0382,0.1453,0.0277,0.0195,0.05,-0.0415,-0.0314,-0.0467,0.0273,0.0371,0.0373,0.0039,0.0206,0.0021,-0.0204,-0.1046,0.033,-0.0307,-0.0441,0.111,-0.0445,-0.0004,-0.0098,0.012,0.0047,0.0114,-0.0007,-0.0244,-0.038,0.0139,0.0462,-0.0013,-0.0127,0.0156,-0.0178,-0.0552,0.0216,0.0954,0.0176,-0.0895,-0.0362,-0.0287,0.0168,-0.0252,0.0397,0.0437,-0.0346,0.0341,0.0856,0.0528,-0.0863,-0.0153,0.0309,-0.008,0.0421,-0.0887,-0.0736,0.0542,0.055,-0.0155,0.0031,-0.037,0.0058,0.0283,-0.0152,-0.0092,-0.0419,0.0113,-0.0337,-0.0078,-0.0172,-0.0189,0.0348,-0.0442,0.033,0.0444,-0.0455,0.0096,-0.0427,-0.0099,0.0163,-0.0065,0.0447,0.0239,-0.0162,0.0135,0.0454,-0.0317,-0.0096,-0.0121,0.0059,0.0467,0.0144,0.0521,0.0428,-0.0066,-0.0357,-0.2291,-0.0067,-0.005,-0.0203,0.0239,-0.0588,0.0309,0.0025,0.0429,0.056,0.0614,0.0045,-0.0371,0.0066,0.0136,0.0463,0.0825,-0.0044,0.0017,-0.0203,-0.0114,0.0542,0.0305,-0.0868,0.0777,-0.0025,0.1924,0.0129,0.071,-0.0354,0.0154,0.0292,0.0078,-0.1223,0.0308,0.0382,0.0702,-0.0484,-0.0576,-0.0051,-0.0203,-0.0,0.043,-0.0931,-0.0247,-0.0555,-0.0462,-0.0093,-0.0572,0.037,0.0432,-0.0413,0.066,-0.0164,-0.0435,0.0057,-0.0804,0.0178,-0.003,0.0055,0.0314,-0.0568,0.0508,-0.1,0.0223,0.004,-0.047,-0.0256,0.0396,-0.0285,-0.0418,0.0233,0.0049,0.0076,0.021,-0.0297,0.0201,-0.0299,-0.0948,0.0308,0.0691,-0.0503,0.0478,-0.0011,0.0767,0.0393,0.1135,-0.044,0.0015,-0.0101,-0.0283,-0.0037,-0.072,-0.016,0.0201,-0.0219,-0.2866,-0.0149,0.0593,-0.0088,-0.0201,-0.0201,0.042,0.0129,-0.047,0.009,-0.0121,0.0272,0.0246,0.0293,0.0183,0.0628,0.0914,-0.0328,0.056,-0.0654,0.0192,0.0209,0.2389,-0.0771,0.0204,-0.0205,-0.0409,-0.0253,-0.001,-0.0087,0.0009,0.0125,0.0921,-0.0449,0.0491,0.1147,-0.0292,0.0164,0.0253,-0.0076,0.0007,-0.0103,-0.0209,-0.0227,0.0898,0.0064,0.0204,-0.0189,-0.0408,0.0379,-0.0046,-0.004,0.0049,-0.0121,0.0416,-0.0073,-0.0638,-0.0826,-0.0146,-0.0469,0.0286,-0.0669,-0.0748,-0.0028,-0.0127]}
{"key":"[Multi-objective optimization and explanation for stroke risk assessment in Shanxi province] Stroke is the top leading causes of death in China (Zhou et al. The Lancet 2019). A dataset from Shanxi Province is used to identify the risk of each patient's at four states low/medium/high/attack and provide the state transition tendency through a SHAP DeepExplainer. To improve the accuracy on an imbalance sample set, the Quadratic Interactive Deep Neural Network (QIDNN) model is first proposed by flexible selecting and appending of quadratic interactive features. The experimental results showed that the QIDNN model with 7 interactive features achieve the state-of-art accuracy $83.25\\%$. Blood pressure, physical inactivity, smoking, weight and total cholesterol are the top five important features. Then, for the sake of high recall on the most urgent state, attack state, the stroke occurrence prediction is taken as an auxiliary objective to benefit from multi-objective optimization. The prediction accuracy was promoted, meanwhile the recall of the attack state was improved by $24.9\\%$ (to $84.83\\%$) compared to QIDNN (from $67.93\\%$) with same features. The prediction model and analysis tool in this paper not only gave the theoretical optimized prediction method, but also provided the attribution explanation of risk states and transition direction of each patient, which provided a favorable tool for doctors to analyze and diagnose the disease.","layer":4,"vector":[-0.0462,-0.0133,-0.0118,-0.0164,0.0224,0.0484,0.0802,0.0221,0.0335,-0.0107,-0.0079,-0.0326,0.0093,0.0472,0.0003,0.0019,0.0324,-0.0049,-0.0189,0.0332,0.031,-0.0244,0.0198,-0.0426,0.0202,0.0528,-0.0189,-0.0627,-0.0243,-0.2215,0.0396,-0.0187,0.0548,-0.0595,-0.0173,-0.0486,-0.0275,0.0771,-0.0246,0.045,0.0503,0.0651,0.002,-0.0042,0.0168,-0.0617,-0.0039,-0.023,0.0399,0.0143,0.0301,-0.0465,0.0471,0.0293,0.0429,-0.0182,0.058,0.0168,0.0155,0.0894,0.0417,0.0176,-0.1825,0.0346,0.0521,0.0095,-0.0405,-0.0255,0.0143,0.0007,-0.0294,0.0587,0.0171,0.0394,-0.0376,0.0338,0.0233,-0.0397,-0.0174,-0.0028,0.0673,-0.001,-0.0525,-0.0015,-0.0138,-0.0532,0.0347,-0.0589,0.0031,0.015,-0.0488,-0.0008,-0.0397,0.0107,-0.0486,-0.0035,0.0304,0.0153,-0.0993,0.1961,-0.0452,0.0235,0.0069,-0.028,0.0439,-0.0295,-0.0522,-0.0574,-0.0389,0.0023,0.026,-0.0167,0.0472,-0.0016,0.0258,0.0137,0.0517,0.0672,-0.0102,-0.012,-0.0157,-0.0038,0.0577,-0.0091,0.0286,-0.0637,0.0333,0.124,0.0084,-0.016,0.0514,-0.0387,-0.0389,0.0028,0.0222,0.0014,0.0192,0.0082,0.0488,-0.0263,-0.0233,-0.032,-0.0015,-0.122,-0.0655,0.0898,-0.032,-0.0023,-0.0414,-0.0584,0.0125,0.002,-0.0517,0.0043,0.0565,0.0044,0.0462,0.0612,-0.0208,-0.0309,-0.0503,-0.0923,-0.0544,0.0958,0.0493,-0.0733,-0.0344,-0.0026,0.0057,-0.0789,0.0572,0.0229,0.0042,0.0283,0.1174,0.0397,-0.0464,-0.0638,-0.0134,-0.0268,0.06,-0.0042,-0.0589,0.0368,0.0301,-0.0379,0.0028,-0.0523,-0.0245,0.0085,-0.0536,0.0041,-0.0173,0.0434,-0.0427,0.0078,-0.0261,-0.0345,0.0413,-0.0661,0.0169,-0.0171,-0.0505,0.004,-0.0064,0.0162,-0.0153,-0.0145,-0.0059,0.0615,-0.0129,0.0329,0.0684,-0.032,-0.0245,0.0133,0.0115,0.0425,-0.0227,0.0384,0.0782,-0.0204,-0.0444,-0.2246,-0.0307,0.0268,-0.0251,0.0357,-0.0667,0.0254,-0.0144,0.0464,0.0957,0.0486,0.0115,-0.0291,0.0073,-0.0099,0.0498,0.0348,-0.0131,-0.0512,-0.0113,0.0201,-0.001,-0.0132,-0.0894,0.0423,-0.0025,0.1948,-0.0003,0.0074,0.0233,-0.001,0.0123,-0.0018,-0.118,0.103,0.0172,0.0453,-0.0228,-0.0656,-0.0517,-0.018,0.0155,0.0144,-0.073,-0.0541,-0.0586,-0.0372,0.0097,-0.0499,0.0309,0.0577,-0.0464,0.0329,0.0282,0.0391,-0.0577,-0.1565,0.0583,-0.0665,-0.0258,-0.0233,-0.007,0.011,-0.0267,0.0435,0.0111,-0.0024,-0.0421,0.0432,-0.0051,-0.0465,0.1037,0.039,0.014,0.0481,0.0347,0.0369,-0.0428,0.0075,-0.0392,0.0733,-0.0408,0.0184,0.0338,0.0152,-0.0005,0.0507,0.014,0.026,-0.0384,0.0225,-0.0495,-0.0335,-0.0344,0.0031,-0.0131,-0.3001,0.0206,-0.0442,0.0536,-0.0118,-0.0171,0.0468,0.0026,-0.0036,0.0239,0.0081,0.0379,0.0694,-0.0288,-0.0431,0.0053,0.0621,-0.0431,0.0328,-0.0419,-0.0303,0.0401,0.2312,-0.0253,0.0389,0.0014,-0.0343,-0.0041,0.0161,-0.0179,0.0082,-0.0158,0.1054,-0.0489,0.0425,0.1069,-0.0231,0.0173,0.0093,0.0001,0.0141,-0.0045,-0.0489,0.0101,0.0914,0.015,-0.0507,-0.026,-0.0107,0.0562,-0.0266,-0.005,-0.0187,0.0017,0.068,0.0172,-0.0,-0.0364,-0.0049,-0.0177,0.0488,-0.029,-0.0145,0.0042,-0.0381]}
{"key":"[Variational Leakage: The Role of Information Complexity in Privacy Leakage] We study the role of information complexity in privacy leakage about an attribute of an adversary's interest, which is not known a priori to the system designer. Considering the supervised representation learning setup and using neural networks to parameterize the variational bounds of information quantities, we study the impact of the following factors on the amount of information leakage: information complexity regularizer weight, latent space dimension, the cardinalities of the known utility and unknown sensitive attribute sets, the correlation between utility and sensitive attributes, and a potential bias in a sensitive attribute of adversary's interest. We conduct extensive experiments on Colored-MNIST and CelebA datasets to evaluate the effect of information complexity on the amount of intrinsic leakage.","layer":1,"vector":[-0.0013,-0.0354,0.0165,-0.0174,0.0098,0.0132,0.0949,0.0126,0.0571,-0.0004,-0.0313,-0.0253,0.0666,0.0457,0.0413,0.0115,0.0071,0.0466,-0.0671,0.034,0.0268,-0.0563,-0.0047,-0.0596,-0.006,0.0117,-0.0176,-0.0404,-0.0365,-0.2494,0.0264,-0.0468,0.0306,-0.0135,0.0316,-0.0497,-0.0461,0.0381,-0.0366,0.0324,0.0171,0.0609,0.0179,-0.0486,-0.0461,-0.0704,-0.0002,0.0289,-0.0236,-0.0571,0.0252,-0.0301,0.0281,0.0786,0.0522,0.0344,0.0757,0.0213,0.0133,0.0254,0.0343,0.0657,-0.1675,0.0682,0.0617,0.0332,-0.0665,-0.0541,0.0372,0.004,0.0169,0.0455,0.0151,-0.0002,0.0147,0.022,-0.0146,-0.0028,-0.0235,0.0125,0.0188,-0.0218,-0.0315,0.0149,-0.0257,-0.072,0.0196,-0.028,0.0557,-0.0212,-0.0364,-0.0225,-0.0016,0.0149,-0.0302,-0.0136,0.0226,0.0312,-0.0977,0.1904,-0.0333,0.0326,0.0401,-0.0307,0.0759,-0.0329,0.0079,-0.0316,-0.0408,-0.0095,0.0212,-0.0031,0.016,-0.0386,0.0193,0.0431,0.0213,0.0214,-0.0131,-0.0268,-0.043,0.04,0.0413,0.082,0.0212,-0.0533,-0.0051,0.129,-0.0051,0.0241,-0.0067,-0.0404,-0.0119,-0.0375,0.0521,0.0409,-0.0187,0.0442,0.021,-0.0449,-0.0597,-0.0184,0.0342,-0.0828,-0.0681,0.1632,-0.0306,0.0316,-0.0011,-0.0047,-0.0168,0.0298,-0.013,-0.0602,0.0457,0.0064,0.0126,0.0307,-0.0828,0.024,0.0302,-0.0318,0.0303,0.1256,-0.0137,-0.0815,-0.0101,0.0124,0.0249,-0.0448,0.0346,0.0439,-0.0406,0.0229,0.0299,-0.0125,-0.0979,0.0115,-0.0162,-0.0072,-0.0023,-0.0225,-0.0111,0.0396,0.0157,-0.0017,0.0175,0.0003,0.037,0.054,-0.0308,0.014,-0.0572,-0.0148,-0.0386,-0.0185,-0.0019,-0.0144,-0.0252,-0.0227,-0.0078,-0.0229,-0.0729,0.0413,-0.0076,0.0393,0.0156,-0.0568,0.0412,0.0421,-0.0519,-0.0133,-0.0155,-0.0256,-0.0092,0.0333,0.024,0.043,0.0136,0.0354,-0.0119,-0.0651,-0.0657,-0.2463,-0.0225,-0.0219,-0.0405,0.0561,-0.032,0.0582,-0.0205,0.0502,0.0812,0.0111,-0.023,-0.0209,0.0238,-0.0188,0.0639,0.0338,0.0413,-0.0238,0.0355,-0.0386,0.0302,-0.0334,-0.0851,0.0217,0.0145,0.2045,-0.0122,-0.0113,-0.0349,0.0174,0.0477,-0.0604,-0.0935,0.085,-0.0011,0.0189,-0.0026,-0.0235,-0.0187,-0.002,-0.0126,0.0027,-0.082,-0.0106,-0.0336,-0.0524,0.0438,-0.1222,0.0386,0.0625,-0.0002,0.0519,0.0282,0.0527,-0.0583,-0.0527,0.0057,-0.0595,0.0544,0.0091,-0.0614,0.0117,-0.0725,0.0428,-0.0026,-0.0314,-0.0582,0.0253,-0.0543,-0.0576,0.0876,0.019,-0.0357,0.043,-0.0243,0.0503,-0.0174,-0.0413,-0.005,0.0633,0.0036,0.0351,0.0044,-0.0098,-0.0047,0.0583,-0.0019,0.0437,-0.0315,0.0033,-0.0227,-0.0423,-0.0226,0.0379,-0.0344,-0.2954,0.0161,-0.0059,0.0244,-0.0055,0.0016,0.058,0.0126,-0.0282,-0.0044,0.0192,0.0705,-0.032,-0.0314,0.0056,0.0377,0.0747,-0.0432,0.017,-0.005,0.0164,0.0492,0.2317,-0.0323,0.0186,-0.0088,0.0093,0.0319,0.0029,-0.0152,0.0229,-0.0056,0.0846,-0.0417,0.0582,0.0557,-0.0188,-0.0059,-0.0055,0.002,-0.0041,-0.0188,-0.0405,0.0078,0.1288,-0.0361,-0.043,-0.0204,0.0148,0.0182,-0.0446,0.0282,-0.0263,-0.0065,0.0483,0.0664,-0.0677,-0.0324,0.003,-0.0339,-0.0146,-0.0309,-0.0232,0.034,-0.0108]}
{"key":"[Local Class-Specific and Global Image-Level Generative Adversarial Networks for Semantic-Guided Scene Generation] In this paper, we address the task of semantic-guided scene generation. One open challenge in scene generation is the difficulty of the generation of small objects and detailed local texture, which has been widely observed in global image-level generation methods. To tackle this issue, in this work we consider learning the scene generation in a local context, and correspondingly design a local class-specific generative network with semantic maps as a guidance, which separately constructs and learns sub-generators concentrating on the generation of different classes, and is able to provide more scene details. To learn more discriminative class-specific feature representations for the local generation, a novel classification module is also proposed. To combine the advantage of both the global image-level and the local class-specific generation, a joint generation network is designed with an attention fusion module and a dual-discriminator structure embedded. Extensive experiments on two scene image generation tasks show superior generation performance of the proposed model. The state-of-the-art results are established by large margins on both tasks and on challenging public benchmarks. The source code and trained models are available at https://github.com/Ha0Tang/LGGAN.","layer":3,"vector":[0.0129,-0.0236,0.0418,-0.0455,0.0449,0.0299,-0.0048,-0.0255,-0.0288,0.0196,-0.0122,-0.0423,0.0472,0.1285,0.0186,0.0159,0.0273,0.0288,-0.064,-0.0317,0.0575,-0.0309,0.0362,-0.0542,-0.0091,-0.0058,0.0294,-0.0317,-0.0473,-0.2469,-0.0057,-0.0459,0.0202,0.0251,-0.0258,-0.0668,-0.0349,0.0361,-0.0041,0.0254,0.0098,0.0192,-0.0342,-0.0661,-0.0088,-0.0143,-0.0156,-0.0374,-0.0152,-0.0401,0.0378,-0.0669,0.0237,0.0303,-0.0107,0.0687,0.0241,0.0472,0.0376,0.0296,0.014,0.0754,-0.1403,0.0228,0.0539,0.0446,-0.0259,0.0227,-0.0005,0.0226,-0.0071,0.0313,-0.009,0.0499,0.0094,-0.0415,-0.0101,-0.0406,-0.0362,-0.0396,0.0271,0.0112,-0.0563,-0.0284,0.0429,-0.0052,0.0033,-0.0204,0.0368,-0.0143,-0.0476,-0.0379,-0.0593,0.0382,-0.0499,0.0045,0.0271,-0.0249,-0.0696,0.1861,-0.0751,0.0106,0.0639,-0.0478,0.0041,-0.027,-0.0592,0.002,-0.0603,-0.0024,-0.0028,-0.0161,-0.0184,-0.0019,0.0222,-0.0329,0.0437,0.0572,-0.049,-0.009,-0.0259,0.0011,0.0002,-0.068,0.0634,-0.0671,0.045,0.1308,0.0745,0.0331,0.0288,0.0258,-0.0548,-0.0208,0.0049,-0.0235,0.0134,-0.0145,0.026,-0.0189,0.0063,-0.0253,-0.0084,-0.0309,0.0095,0.0879,-0.0357,0.0672,-0.0506,-0.029,-0.0306,0.0103,-0.0262,-0.0083,0.0046,0.0652,0.0475,0.0543,-0.0327,-0.0158,-0.0002,-0.0646,-0.0471,0.0761,-0.0064,-0.104,-0.0336,0.0361,0.0098,0.0079,0.0122,0.0536,-0.0307,0.0509,0.0991,0.0431,-0.0861,0.0148,-0.0081,0.0161,0.008,-0.0713,-0.032,-0.0077,0.0278,-0.0553,0.0464,-0.0619,0.0032,0.051,0.0005,0.0217,0.0003,-0.0308,0.0124,-0.0332,-0.0003,-0.0186,-0.0002,-0.0252,0.0189,0.0331,-0.0415,0.0162,-0.0104,0.0269,0.0077,0.009,0.0368,0.0221,-0.0509,0.0169,0.036,0.0165,-0.0448,-0.0091,-0.0163,0.0345,-0.035,0.0236,0.0258,-0.0596,-0.0111,-0.2331,0.0489,0.0198,-0.0442,0.0042,-0.0833,0.0081,-0.0058,0.0196,0.0222,0.0905,-0.0179,0.0262,0.026,0.0111,0.0163,0.0052,0.0383,-0.0137,-0.0154,-0.0149,0.0222,-0.0012,-0.1091,0.0436,-0.0496,0.2282,0.0681,0.0563,-0.0102,0.0607,0.0514,-0.0422,-0.089,0.0156,0.0187,0.086,0.0048,-0.0263,-0.0121,-0.0199,0.0294,-0.0356,-0.1309,-0.0036,-0.0657,-0.0229,0.0455,-0.001,0.0142,0.0319,-0.0535,0.037,-0.0104,-0.07,-0.044,-0.0947,0.0412,-0.0483,0.0371,-0.0313,-0.0375,0.0475,-0.0829,0.057,0.0297,-0.0759,-0.0781,0.0421,-0.0421,-0.0015,0.0737,0.0131,-0.0081,0.0694,-0.0054,0.0445,0.0172,-0.0433,-0.0559,0.0754,-0.0125,0.0228,0.0119,0.0493,-0.0217,0.0761,-0.015,-0.0042,-0.0152,0.0143,0.0385,-0.0568,-0.0168,0.0555,-0.039,-0.3226,0.0298,0.0018,0.0632,-0.0049,0.0402,0.0586,0.0388,-0.0265,0.0307,0.0339,-0.0082,0.0521,0.006,-0.0177,0.0271,0.0588,-0.0703,0.0698,-0.013,0.0155,0.0141,0.2023,-0.046,0.033,-0.0419,-0.0053,0.0092,0.0332,0.008,0.0177,0.0675,0.0616,-0.0372,-0.0012,0.0915,-0.059,0.0153,0.0204,-0.029,-0.01,0.0431,-0.0581,0.0098,0.0396,0.0205,0.0058,-0.0343,-0.0031,0.0174,-0.014,0.0479,0.0119,0.0219,0.0656,0.0187,-0.029,-0.0442,-0.0251,0.0411,0.0547,-0.0454,-0.0258,-0.0053,-0.0438]}
{"key":"[Smart Meter Data Anomaly Detection using Variational Recurrent Autoencoders with Attention] In the digitization of energy systems, sensors and smart meters are increasingly being used to monitor production, operation and demand. Detection of anomalies based on smart meter data is crucial to identify potential risks and unusual events at an early stage, which can serve as a reference for timely initiation of appropriate actions and improving management. However, smart meter data from energy systems often lack labels and contain noise and various patterns without distinctively cyclical. Meanwhile, the vague definition of anomalies in different energy scenarios and highly complex temporal correlations pose a great challenge for anomaly detection. Many traditional unsupervised anomaly detection algorithms such as cluster-based or distance-based models are not robust to noise and not fully exploit the temporal dependency in a time series as well as other dependencies amongst multiple variables (sensors). This paper proposes an unsupervised anomaly detection method based on a Variational Recurrent Autoencoder with attention mechanism. with \"dirty\" data from smart meters, our method pre-detects missing values and global anomalies to shrink their contribution while training. This paper makes a quantitative comparison with the VAE-based baseline approach and four other unsupervised learning methods, demonstrating its effectiveness and superiority. This paper further validates the proposed method by a real case study of detecting the anomalies of water supply temperature from an industrial heating plant.","layer":0,"vector":[-0.0324,-0.0313,0.0332,-0.006,0.0605,0.0026,0.0295,-0.0007,0.043,-0.0393,0.0121,-0.0654,0.0271,0.0629,-0.0037,0.014,-0.0155,0.0351,-0.0174,0.0171,0.0408,-0.0257,-0.0067,-0.0587,0.021,-0.0009,-0.0109,-0.0286,-0.0675,-0.2419,0.0411,-0.0553,0.0621,-0.0167,0.0474,-0.0206,-0.0601,0.0413,-0.0161,0.0268,-0.0186,0.0049,0.0145,-0.0925,-0.0487,-0.0787,0.0394,-0.0226,-0.0087,-0.0271,0.0531,-0.0172,0.0091,0.0342,0.0401,0.0548,0.0575,0.0054,0.0824,0.0469,0.0315,0.0486,-0.1743,0.0441,0.0716,-0.006,-0.012,-0.0005,0.021,0.0072,0.0143,0.0221,-0.01,0.0262,0.0068,0.0481,-0.0175,-0.0133,-0.062,0.0231,0.0207,-0.0352,-0.0225,-0.0374,-0.0131,-0.0652,0.0053,-0.0606,0.0932,-0.0164,-0.0417,-0.0049,0.0091,0.0399,-0.0528,-0.0116,0.0198,0.0482,-0.049,0.1816,-0.0668,0.0495,0.0268,0.0042,0.0342,-0.0252,-0.0338,-0.0471,-0.0233,-0.0174,0.0038,-0.0256,0.0376,-0.1062,0.0468,-0.0223,0.0683,0.0163,0.0227,0.0359,-0.0101,0.0007,0.0703,-0.0297,0.0302,-0.0469,0.0523,0.138,-0.013,0.0335,0.0302,0.0063,-0.0613,-0.0264,0.0323,0.0243,0.0321,-0.0133,0.0647,0.0124,-0.0369,-0.0356,0.0433,-0.1269,-0.0504,0.087,-0.0449,0.0113,-0.0558,-0.0259,-0.0087,0.0014,-0.0198,-0.0295,0.0839,0.0537,0.0465,-0.0057,-0.0599,0.0593,-0.0274,-0.0222,-0.0456,0.0848,0.0092,-0.1049,-0.0119,-0.044,-0.0026,0.0013,0.049,0.0578,-0.0418,0.0206,0.0771,0.0367,-0.0374,0.0083,-0.0366,-0.0071,0.0047,-0.0291,-0.0136,0.0218,0.0687,-0.0417,0.0103,-0.0397,0.012,0.0397,-0.0462,-0.0052,-0.0256,-0.0201,0.0018,-0.0162,-0.029,-0.039,0.0177,-0.0855,0.0022,-0.0018,-0.0071,0.0507,-0.0079,0.0495,-0.0238,-0.0035,0.0434,0.0302,0.0083,0.01,0.0633,-0.0521,-0.0614,0.0103,0.0512,0.032,-0.0137,0.0047,0.0435,0.0132,-0.0746,-0.2529,-0.0215,0.0507,-0.0626,0.0349,-0.0429,0.0033,-0.0494,0.057,0.0506,0.0412,-0.028,-0.0261,0.004,-0.0098,0.085,0.061,0.0415,-0.0679,0.0229,-0.0293,0.0053,0.0233,-0.0955,0.0263,0.0227,0.1972,-0.0058,0.0518,-0.0542,0.0205,-0.0265,-0.0095,-0.0615,0.0693,0.0247,0.0468,0.0534,-0.0616,-0.0448,-0.0379,0.0364,-0.0098,-0.0118,-0.0361,-0.0678,-0.0276,0.0136,-0.0515,0.042,0.0568,-0.0197,0.0498,-0.0074,0.0179,-0.0397,-0.0847,0.0492,-0.0011,-0.0226,0.0435,-0.0311,0.0177,-0.0537,0.045,-0.006,-0.0538,-0.0453,0.0092,0.0221,-0.0479,0.1647,0.0155,-0.03,0.0364,0.0231,0.0095,-0.0278,-0.0228,-0.0094,0.0426,-0.0219,0.0383,0.0232,0.0291,-0.0102,0.0237,0.0068,0.0276,-0.0125,0.0078,0.0006,-0.0119,-0.0432,0.0184,-0.0107,-0.2808,0.0154,-0.0159,0.0104,-0.01,-0.0281,-0.0116,0.0309,0.005,-0.015,-0.0413,-0.0063,0.0345,-0.0055,0.0007,0.0306,0.0447,-0.0729,0.0578,-0.0431,0.0334,0.0738,0.2259,-0.0268,0.0278,0.0082,-0.0124,0.0025,0.0516,0.0068,-0.0122,0.0061,0.0965,-0.0389,0.0187,0.0699,-0.0078,0.0497,0.0218,-0.0461,-0.0172,0.0042,-0.0371,-0.0509,0.0793,-0.0148,0.0012,-0.069,-0.0,0.0906,-0.0066,-0.0276,-0.0328,0.0299,0.0051,0.0634,-0.059,-0.0546,-0.0453,-0.0401,0.0081,-0.0412,-0.0152,-0.0377,-0.028]}
{"key":"[CheckSel: Efficient and Accurate Data-valuation Through Online Checkpoint Selection] Data valuation and subset selection have emerged as valuable tools for application-specific selection of important training data. However, the efficiency-accuracy tradeoffs of state-of-the-art methods hinder their widespread application to many AI workflows. In this paper, we propose a novel 2-phase solution to this problem. Phase 1 selects representative checkpoints from an SGD-like training algorithm, which are used in phase-2 to estimate the approximate training data values, e.g. decrease in validation loss due to each training point. A key contribution of this paper is CheckSel, an Orthogonal Matching Pursuit-inspired online sparse approximation algorithm for checkpoint selection in the online setting, where the features are revealed one at a time. Another key contribution is the study of data valuation in the domain adaptation setting, where a data value estimator obtained using checkpoints from training trajectory in the source domain training dataset is used for data valuation in a target domain training dataset. Experimental results on benchmark datasets show the proposed algorithm outperforms recent baseline methods by up to 30% in terms of test accuracy while incurring a similar computational burden, for both standalone and domain adaptation settings.","layer":5,"vector":[-0.0852,0.0096,0.0383,-0.0312,0.0577,0.033,0.019,0.0026,0.0454,-0.0696,0.0047,-0.0346,0.0537,0.0603,-0.0126,0.0451,0.0253,0.0321,-0.014,0.0084,0.0059,-0.0423,0.0045,-0.0578,0.0038,0.0357,-0.0148,-0.0278,-0.0509,-0.2629,-0.0061,-0.0357,0.0218,0.0124,0.0264,0.0111,-0.0444,0.064,0.0047,0.0282,-0.0242,0.0084,-0.0449,-0.0082,-0.0053,-0.0668,-0.0156,-0.0147,-0.0282,-0.0089,0.0443,-0.048,0.0191,0.013,0.0019,0.0436,0.0363,0.0809,0.0249,0.0275,0.0015,0.0229,-0.1524,0.0266,0.0458,0.0518,-0.0172,-0.0391,0.0016,0.0663,-0.0014,0.053,0.0085,0.0423,0.0158,-0.018,0.0273,-0.0331,0.0144,0.007,0.038,-0.0276,-0.0556,0.0326,-0.0637,-0.0525,0.0344,-0.0271,0.0473,0.0113,-0.0323,-0.0278,-0.0337,0.0297,-0.0654,-0.0289,0.0219,0.0332,-0.0095,0.2191,-0.0123,0.0372,-0.0396,0.0071,0.0328,-0.0605,-0.0343,-0.0539,-0.023,-0.0416,-0.0001,-0.0173,0.0316,-0.0253,0.0436,0.0408,0.0707,-0.0074,-0.0085,-0.019,-0.0008,-0.0032,0.1006,-0.0068,0.0178,-0.058,0.0418,0.1477,0.0311,0.0245,0.0421,-0.0614,-0.1039,-0.0929,0.0208,0.0072,0.0305,0.0195,-0.0232,0.0024,-0.0442,-0.0509,0.0476,-0.0834,-0.0372,0.1382,-0.0518,0.0257,-0.0377,-0.0679,-0.0231,0.013,-0.0275,-0.0243,0.0218,0.0313,0.0531,0.0537,-0.0448,-0.0083,0.0101,-0.0288,-0.0469,0.1185,-0.0039,-0.0948,-0.0715,-0.0245,0.0078,-0.0079,0.0167,0.0342,-0.0487,0.0062,0.0765,0.0256,-0.0555,0.0236,0.0198,0.0135,0.0223,-0.0555,-0.0405,0.0522,0.0078,-0.0541,-0.0082,-0.0607,-0.0105,0.0007,-0.034,-0.0084,-0.0324,-0.0265,-0.0098,-0.0452,0.0248,0.0019,-0.0122,-0.0332,0.0285,-0.0132,-0.0184,-0.0258,-0.0124,0.0309,-0.022,-0.0095,0.0411,0.0542,-0.0303,0.0233,0.0407,-0.0125,-0.0102,0.0174,0.0131,0.0281,-0.0143,0.0524,0.0367,-0.0004,-0.0343,-0.2044,-0.0123,-0.0221,0.0121,0.0501,-0.0846,0.061,0.0151,0.0444,0.0692,0.0206,-0.0549,-0.0176,0.0242,-0.0167,0.0228,0.0554,0.0218,-0.0307,0.0036,-0.0003,0.0213,-0.0319,-0.0964,0.0723,-0.0158,0.2225,0.0185,0.0436,-0.0228,0.0257,0.0341,0.0108,-0.1003,0.0555,0.0177,0.0702,-0.0035,-0.0431,0.0081,0.02,0.0167,0.0109,-0.1018,-0.0361,-0.0485,-0.0308,0.0241,-0.0265,0.0189,0.0426,-0.0086,0.0418,-0.0282,0.0221,-0.0311,-0.0536,0.0576,-0.0077,-0.0019,0.0392,-0.0737,0.0055,-0.0114,0.0876,-0.0287,0.0024,-0.0454,0.0465,-0.0369,-0.0252,0.056,-0.0267,-0.0003,0.0547,-0.0019,-0.0257,-0.0267,-0.0398,0.0084,0.0556,-0.0183,0.005,0.0133,0.0433,0.0343,0.1162,0.0028,0.0418,-0.0002,0.0204,-0.0282,-0.0821,-0.0353,0.0496,0.0205,-0.2964,0.0335,0.0263,0.0334,-0.0206,0.0028,0.037,0.0064,-0.0486,-0.0262,0.0153,0.0235,-0.0088,-0.0198,0.0181,0.0709,0.0654,-0.0417,0.0237,-0.1078,0.0007,0.0434,0.2283,-0.0834,0.0258,0.0216,-0.0543,-0.0113,0.0091,-0.0178,-0.0082,-0.0016,0.0643,-0.017,0.0355,0.123,-0.0616,0.0236,0.0371,-0.0463,0.0275,0.021,-0.0444,0.0001,0.0798,-0.0194,-0.0149,-0.036,-0.0115,0.0147,-0.0363,-0.0237,-0.0088,0.0011,0.0141,0.0625,-0.0188,-0.0217,-0.0573,-0.0736,0.0093,0.0065,-0.0071,0.0049,0.0139]}
{"key":"[Comprehensive Privacy Analysis of Deep Learning: Passive and Active White-box Inference Attacks against Centralized and Federated Learning] Deep neural networks are susceptible to various inference attacks as they remember information about their training data. We design white-box inference attacks to perform a comprehensive privacy analysis of deep learning models. We measure the privacy leakage through parameters of fully trained models as well as the parameter updates of models during training. We design inference algorithms for both centralized and federated learning, with respect to passive and active inference attackers, and assuming different adversary prior knowledge. We evaluate our novel white-box membership inference attacks against deep learning algorithms to trace their training data records. We show that a straightforward extension of the known black-box attacks to the white-box setting (through analyzing the outputs of activation functions) is ineffective. We therefore design new algorithms tailored to the white-box setting by exploiting the privacy vulnerabilities of the stochastic gradient descent algorithm, which is the algorithm used to train deep neural networks. We investigate the reasons why deep learning models may leak information about their training data. We then show that even well-generalized models are significantly susceptible to white-box membership inference attacks, by analyzing state-of-the-art pre-trained and publicly available models for the CIFAR dataset. We also show how adversarial participants, in the federated learning setting, can successfully run active membership inference attacks against other participants, even when the global model achieves high prediction accuracies.","layer":2,"vector":[-0.0119,-0.0506,-0.0088,0.0121,0.0417,0.0355,0.0874,0.025,0.0663,-0.0095,0.0302,-0.0149,0.0567,0.0592,-0.0067,0.0164,-0.0354,0.0076,-0.0385,-0.0051,0.0436,-0.0572,0.0394,-0.044,-0.0104,0.0354,-0.062,-0.0065,-0.0833,-0.2273,0.0394,-0.0596,0.0411,-0.0255,0.0525,-0.0619,-0.0094,0.0617,-0.0369,0.0458,0.0223,0.0288,-0.0076,-0.0244,-0.0162,-0.0214,-0.0256,-0.016,-0.0354,-0.0256,0.0248,-0.0322,0.0129,0.0625,0.0205,0.0193,0.0546,0.0232,0.0311,0.0474,0.0036,0.0607,-0.1664,0.042,0.0225,0.0702,-0.0066,-0.0158,0.012,0.0228,0.0,0.0486,0.0277,0.0018,0.0078,0.0151,-0.0252,-0.0135,-0.0218,0.0234,0.0147,0.0031,-0.0214,0.0147,-0.0143,-0.0414,0.0228,-0.0348,0.0392,-0.029,-0.0411,-0.019,0.0279,0.0207,-0.0309,-0.0041,0.0253,0.017,-0.0769,0.2008,-0.0531,0.0415,0.0085,-0.0197,0.0246,-0.0061,-0.0434,-0.0379,-0.0187,0.0087,-0.0296,-0.0572,0.0148,-0.0214,0.0374,0.0313,0.0783,0.0323,-0.0677,-0.0189,0.0,-0.0262,0.0721,0.0345,-0.0108,-0.0874,0.0151,0.1597,0.024,0.0247,0.0247,-0.0584,-0.0239,-0.0289,0.0362,0.0653,0.0496,0.0323,0.025,0.0028,-0.0592,-0.0147,0.0436,-0.053,-0.0254,0.1189,-0.0056,0.0377,-0.0442,-0.0204,-0.0116,0.0326,0.0125,-0.0486,0.0424,0.0067,0.0098,0.0858,-0.059,-0.0219,-0.0276,-0.0324,0.0014,0.1251,0.0438,-0.0825,-0.0345,0.0022,0.0184,-0.0732,0.0607,0.0199,-0.0177,0.02,0.0146,-0.0043,-0.0736,0.0026,-0.0298,0.0111,-0.0026,-0.054,-0.0432,-0.0084,0.0095,-0.0197,0.017,-0.0509,-0.001,0.0489,-0.0571,0.0345,-0.102,0.0,-0.0403,-0.0431,0.0169,-0.0074,-0.0133,-0.046,-0.0356,0.0005,-0.0418,-0.0068,-0.0191,0.0311,-0.0159,-0.0173,0.0481,0.0063,-0.0684,0.003,0.0205,-0.0796,0.0071,0.0098,-0.0165,0.008,-0.014,0.0131,0.0477,-0.0453,-0.0381,-0.1835,-0.0343,0.0141,-0.0403,0.0561,-0.0571,0.0869,-0.0084,0.0589,0.0801,0.0629,-0.016,-0.0455,0.0191,0.012,0.0756,0.0311,0.0351,-0.0255,0.0208,-0.0175,0.0268,0.0102,-0.1138,0.0468,0.0696,0.2308,0.0234,0.0383,-0.0416,0.0046,0.0129,-0.0146,-0.134,0.0439,0.021,0.032,-0.0206,-0.0052,-0.0095,-0.0458,0.0475,0.0184,-0.1136,-0.0163,-0.0211,-0.0403,0.0497,-0.0861,0.0296,0.0505,-0.007,0.0505,0.0519,0.0262,-0.0545,-0.0547,0.0299,-0.0456,0.0846,0.0017,-0.0214,0.009,-0.0978,0.0656,-0.0345,-0.0298,-0.0707,0.0708,-0.0476,0.0173,0.0828,0.0287,0.0141,0.0288,0.0313,0.0379,-0.0496,-0.0882,-0.0289,0.0666,0.0192,0.0134,0.0139,-0.0048,-0.0071,0.1049,0.047,0.0496,-0.0248,-0.0213,0.0198,-0.1029,-0.0499,0.0482,0.0033,-0.2966,-0.0012,-0.0328,0.0523,-0.0377,0.0183,0.0743,-0.0109,-0.0686,-0.0323,-0.0058,0.0647,0.0329,-0.0155,-0.016,0.0207,0.0563,-0.0507,0.0421,-0.0033,0.0083,0.0203,0.1878,-0.0124,0.0207,0.0302,0.0212,0.0448,0.0186,-0.0359,-0.0138,-0.0184,0.0689,-0.0387,0.0236,0.0666,-0.0367,0.0028,0.0424,-0.0278,-0.004,-0.0441,-0.0542,-0.0029,0.0534,-0.0141,-0.0186,-0.0279,0.0206,-0.0107,0.0253,-0.0155,-0.0053,-0.0068,0.0352,0.0182,-0.0531,-0.0654,-0.0198,-0.0025,-0.0192,-0.0621,-0.0489,0.0007,-0.0186]}
{"key":"[Beyond Individual and Group Fairness] We present a new data-driven model of fairness that, unlike existing static definitions of individual or group fairness is guided by the unfairness complaints received by the system. Our model supports multiple fairness criteria and takes into account their potential incompatibilities. We consider both a stochastic and an adversarial setting of our model. In the stochastic setting, we show that our framework can be naturally cast as a Markov Decision Process with stochastic losses, for which we give efficient vanishing regret algorithmic solutions. In the adversarial setting, we design efficient algorithms with competitive ratio guarantees. We also report the results of experiments with our algorithms and the stochastic framework on artificial datasets, to demonstrate their effectiveness empirically.","layer":12,"vector":[-0.0597,-0.0209,-0.0295,-0.0201,0.009,0.0427,0.0605,0.0123,0.0634,0.0147,-0.0148,-0.008,0.0176,0.0895,-0.0054,0.0295,-0.008,0.0259,-0.065,0.0027,0.0142,-0.048,-0.029,-0.0368,-0.0238,0.0236,-0.032,-0.0816,-0.067,-0.237,0.013,-0.0936,0.0573,-0.0482,-0.0264,-0.0478,-0.0329,0.0344,-0.031,0.0701,-0.0021,0.0325,-0.0235,-0.0211,-0.0092,-0.008,-0.0208,0.0034,-0.0402,-0.0452,0.0355,-0.0373,0.0015,0.0432,0.0127,0.0218,0.068,0.042,0.0222,0.0224,0.0155,0.0417,-0.1249,0.0802,0.0396,0.0478,0.0002,-0.0094,-0.0272,0.0431,-0.0161,0.0384,0.0404,0.0254,0.0095,0.0143,0.0239,-0.0425,-0.0219,0.0432,-0.0064,0.0163,-0.0478,0.0233,0.003,-0.0558,0.0076,-0.047,0.0478,-0.0042,-0.0167,0.0311,0.008,0.0345,-0.0362,0.0077,0.0407,0.0339,-0.0299,0.2207,-0.0166,0.015,0.0315,-0.0392,0.033,-0.0286,-0.02,-0.0697,-0.0303,0.0074,0.0047,-0.0126,0.0388,-0.0307,0.022,0.0288,0.0097,0.0261,-0.0332,-0.0131,-0.0207,0.0176,0.0613,0.0049,0.0343,-0.0584,0.013,0.1874,0.0039,0.0056,0.0221,-0.0899,-0.0682,-0.0028,0.0371,0.0468,-0.0384,-0.0104,0.013,0.0037,-0.0605,-0.0757,0.0038,-0.0796,-0.0212,0.1113,-0.0196,0.0277,-0.0407,-0.0249,-0.0239,-0.0055,-0.0218,-0.0258,0.0083,0.0473,0.0254,0.0473,-0.0425,0.0182,0.009,-0.0466,-0.0405,0.1395,-0.0175,-0.0659,-0.035,0.0354,0.0085,-0.0025,0.0094,0.0203,-0.0291,0.0363,0.0596,0.0228,-0.0743,-0.0066,-0.0268,-0.0136,0.0217,-0.012,-0.0169,0.0002,0.0564,-0.007,-0.0127,-0.0324,0.0289,0.0505,-0.0567,-0.0039,-0.0568,-0.0291,-0.0341,-0.0496,0.0024,-0.0404,0.0266,-0.007,-0.0362,-0.0177,-0.0541,0.042,0.0333,0.0433,-0.0128,-0.034,0.0455,0.0044,-0.0393,0.0035,0.0343,-0.0235,0.0215,0.0312,0.0514,0.0588,0.0285,0.0093,0.0137,-0.0061,-0.0324,-0.2033,-0.0174,-0.0353,-0.002,0.0514,-0.0242,0.0183,-0.0398,0.0255,0.1074,0.0955,-0.0195,-0.0206,0.0663,0.0302,0.049,-0.0153,0.0713,-0.0231,0.0079,-0.0133,0.0486,0.0013,-0.0762,0.0473,0.0282,0.2428,0.015,-0.0281,-0.0235,0.0401,0.0316,-0.0188,-0.0808,0.0217,0.0221,0.023,-0.0344,-0.0595,-0.0504,0.0117,0.0267,0.0292,-0.1339,-0.0087,-0.0304,-0.0204,0.0428,-0.07,0.0179,-0.0059,-0.05,0.0393,0.0025,0.0325,-0.0514,-0.1227,0.0362,-0.0287,0.0294,0.0471,-0.0529,-0.0067,-0.0518,0.0905,0.0132,-0.0341,-0.0694,0.0036,-0.0118,0.0195,0.0338,0.0026,-0.0143,0.0025,0.0445,-0.0096,-0.0462,-0.0464,-0.0329,0.077,-0.0054,0.0107,0.0523,0.0527,-0.0265,0.0565,0.0296,0.0438,-0.0343,-0.0101,-0.0098,-0.0534,0.0271,0.0305,-0.004,-0.2877,0.0413,-0.0147,0.0501,-0.0508,0.0354,0.0245,0.0049,-0.0593,-0.0135,0.0206,0.085,0.0256,-0.002,0.0075,0.0332,0.0871,-0.0537,0.0311,-0.037,0.0394,0.0291,0.226,-0.0713,0.0111,0.0247,0.0162,0.0195,0.0285,-0.042,-0.017,0.0129,0.0847,-0.0531,0.0526,0.0527,-0.0505,-0.0039,0.0006,-0.0142,-0.0742,-0.0008,-0.0024,0.062,0.1019,0.0305,-0.0439,-0.0544,0.0147,0.0281,-0.0537,0.0436,-0.0598,-0.0217,0.0094,0.0461,-0.0599,-0.0443,0.0044,-0.0658,-0.0136,-0.0315,-0.0258,0.0003,-0.0213]}
{"key":"[Procrustes registration of two-dimensional statistical shape models without correspondences] Statistical shape models are a useful tool in image processing and computer vision. A Procrustres registration of the contours of the same shape is typically perform to align the training samples to learn the statistical shape model. A Procrustes registration between two contours with known correspondences is straightforward. However, these correspondences are not generally available. Manually placed landmarks are often used for correspondence in the design of statistical shape models. However, determining manual landmarks on the contours is time-consuming and often error-prone. One solution to simultaneously find correspondence and registration is the Iterative Closest Point (ICP) algorithm. However, ICP requires an initial position of the contours that is close to registration, and it is not robust against outliers. We propose a new strategy, based on Dynamic Time Warping, that efficiently solves the Procrustes registration problem without correspondences. We study the registration performance in a collection of different shape data sets and show that our technique outperforms competing techniques based on the ICP approach. Our strategy is applied to an ensemble of contours of the same shape as an extension of the generalized Procrustes analysis accounting for a lack of correspondence.","layer":1,"vector":[-0.037,-0.0061,0.0567,-0.0165,0.0139,0.0476,0.0339,0.0047,-0.0269,-0.047,0.0641,-0.1004,0.0138,0.065,-0.0689,0.0282,-0.0597,0.0971,0.003,0.0208,0.0136,-0.0405,-0.0243,-0.0503,-0.0078,0.0181,-0.034,-0.0192,-0.0176,-0.2626,0.0227,-0.0511,0.045,0.0068,-0.0324,-0.0123,-0.0558,0.0547,0.0228,0.0008,0.0151,0.0288,-0.0381,-0.071,0.0256,-0.0518,0.0074,0.0002,0.019,-0.0461,0.0312,-0.0546,0.0486,0.0481,-0.0043,0.0662,0.0831,0.0645,0.0684,0.0586,0.0132,0.014,-0.1827,0.0618,0.0654,0.0132,-0.0257,-0.069,0.0066,0.0438,-0.0293,0.0499,0.0254,0.0383,-0.0233,-0.034,0.0094,-0.0335,-0.0288,-0.0269,0.007,-0.024,-0.0085,0.0069,-0.0111,-0.0338,0.0042,-0.06,0.0398,-0.0025,-0.0602,-0.012,-0.0193,0.0024,-0.0869,-0.0604,0.062,0.0427,0.0149,0.1865,-0.0665,0.0321,0.0329,0.0092,0.0681,-0.0566,-0.0694,-0.0245,-0.0161,0.0195,-0.0011,-0.0339,0.0166,-0.0444,0.0251,0.0064,0.054,0.0392,0.0047,-0.0446,-0.0069,0.0018,0.0522,-0.0268,0.0203,-0.0512,0.0403,0.115,0.0451,0.0331,0.0728,0.0081,-0.0115,-0.0312,-0.0107,0.0187,0.0102,0.0239,0.0642,-0.0317,-0.0244,-0.0608,0.0182,-0.0621,-0.003,0.1597,-0.0719,0.05,-0.0601,0.0047,0.0139,0.0083,-0.0513,-0.0145,0.0175,0.0168,0.0485,0.0345,-0.0393,0.0223,-0.0646,-0.0626,0.0045,0.0814,0.0332,-0.1041,-0.046,-0.0195,0.0502,0.0274,0.0005,0.0297,0.0012,0.0412,0.1013,0.0799,-0.0847,-0.0033,0.0548,0.0212,0.0537,-0.0731,-0.0224,-0.0087,0.0138,-0.0369,-0.0275,-0.0244,0.0162,0.0482,-0.0373,0.017,-0.0233,-0.0319,-0.0362,-0.0246,0.0116,-0.0145,0.0319,-0.0486,0.0273,-0.0322,-0.0328,-0.0116,0.0074,0.0025,-0.0341,0.0032,0.0366,0.0613,-0.0003,-0.0518,0.0518,-0.0313,-0.0552,0.0227,0.0447,0.0458,-0.0218,0.0385,0.0112,-0.0536,-0.0395,-0.2075,0.0198,0.0245,0.0239,0.0244,-0.0504,0.0088,0.0249,0.0605,0.0446,0.0353,-0.0091,0.0226,0.0278,-0.0273,0.0497,0.0228,0.0442,-0.0306,-0.0543,-0.0141,0.0118,-0.0784,-0.0726,0.0525,0.0409,0.21,0.0169,0.0018,-0.0125,0.0027,0.0034,-0.0643,-0.0618,0.0372,0.0051,0.0617,-0.0374,-0.0141,-0.0109,-0.0213,0.0174,0.058,-0.0806,-0.0475,-0.0297,-0.0065,0.0243,-0.0257,0.0131,-0.0001,0.0057,0.0599,-0.0293,0.0036,-0.032,-0.0652,0.0019,-0.0414,0.0477,-0.0311,-0.0541,0.0192,-0.0782,0.0313,0.014,-0.0403,-0.0304,-0.0009,-0.0331,0.0019,0.0778,-0.0102,0.0075,0.0519,0.0199,0.0518,-0.0313,-0.0453,-0.0305,0.0402,-0.037,0.0465,0.0546,0.0516,0.0215,0.0476,-0.0223,-0.0025,-0.0717,0.0449,-0.0157,-0.024,0.0693,-0.0139,0.0077,-0.3049,0.0164,-0.0097,0.04,-0.0435,0.0058,-0.0192,0.0068,0.0114,-0.0295,0.0094,0.0354,0.0599,-0.0094,-0.0104,0.0461,0.0654,-0.1004,0.0718,-0.0796,-0.0129,0.0293,0.2109,-0.0422,-0.0062,0.0119,-0.0133,0.004,0.0581,-0.0065,0.0383,0.0031,0.0595,-0.0379,0.0291,0.0899,0.0014,0.0073,-0.0213,-0.035,-0.0075,0.0028,0.0127,-0.0279,0.0718,0.0242,-0.0072,0.0058,0.0139,0.0173,-0.0293,0.0018,-0.0299,0.0307,0.0237,0.0473,-0.0028,-0.0538,-0.0535,-0.0295,-0.0192,-0.0559,-0.0559,-0.0236,0.0137]}
{"key":"[Hybrid Micro/Macro Level Convolution for Heterogeneous Graph Learning] Heterogeneous graphs are pervasive in practical scenarios, where each graph consists of multiple types of nodes and edges. Representation learning on heterogeneous graphs aims to obtain low-dimensional node representations that could preserve both node attributes and relation information. However, most of the existing graph convolution approaches were designed for homogeneous graphs, and therefore cannot handle heterogeneous graphs. Some recent methods designed for heterogeneous graphs are also faced with several issues, including the insufficient utilization of heterogeneous properties, structural information loss, and lack of interpretability. In this paper, we propose HGConv, a novel Heterogeneous Graph Convolution approach, to learn comprehensive node representations on heterogeneous graphs with a hybrid micro/macro level convolutional operation. Different from existing methods, HGConv could perform convolutions on the intrinsic structure of heterogeneous graphs directly at both micro and macro levels: A micro-level convolution to learn the importance of nodes within the same relation, and a macro-level convolution to distinguish the subtle difference across different relations. The hybrid strategy enables HGConv to fully leverage heterogeneous information with proper interpretability. Moreover, a weighted residual connection is designed to aggregate both inherent attributes and neighbor information of the focal node adaptively. Extensive experiments on various tasks demonstrate not only the superiority of HGConv over existing methods, but also the intuitive interpretability of our approach for graph analysis.","layer":5,"vector":[0.01,-0.0158,0.003,-0.0541,0.0549,0.0421,0.0085,0.0398,0.009,0.0182,0.0241,-0.0669,0.0586,0.0767,0.0131,0.0375,0.0093,0.052,-0.0527,-0.0025,-0.0028,-0.0606,0.0033,-0.0188,0.0581,-0.0153,0.0042,0.0039,-0.0664,-0.2675,-0.0113,-0.0373,0.0736,-0.0168,0.002,-0.0431,0.0294,0.0232,-0.0407,0.0542,0.0202,-0.0287,-0.021,-0.0373,-0.0416,-0.0386,-0.0138,-0.0301,-0.0006,-0.0365,0.0317,-0.0187,0.0564,0.0117,0.0227,0.0768,0.0544,0.0148,0.0627,0.0458,0.0358,0.0575,-0.1461,0.0494,0.0187,0.0152,-0.0519,0.0446,0.0209,0.0867,0.0313,0.0303,-0.0016,-0.0086,0.0028,-0.0193,0.0207,-0.0179,-0.029,-0.016,-0.0272,-0.022,-0.0479,-0.0024,0.0129,-0.0353,-0.0013,-0.0529,0.051,0.0263,-0.0509,-0.033,-0.0409,0.0311,-0.0454,-0.0113,0.0783,0.0057,-0.031,0.2044,-0.0651,0.0125,0.0512,-0.009,0.0257,-0.0254,0.0136,-0.026,-0.0338,-0.0005,-0.017,-0.0472,-0.0056,-0.0303,0.0035,-0.0002,0.0525,0.0457,-0.0254,0.0001,-0.0055,0.039,0.0105,-0.0653,0.0676,-0.0758,-0.0152,0.1156,0.046,0.0099,0.0417,0.023,0.0068,-0.0222,-0.0302,0.0104,0.0036,-0.0096,-0.0023,0.0185,-0.0222,-0.0301,-0.02,-0.0602,-0.0773,0.138,-0.0564,-0.0163,0.0078,-0.0085,-0.0283,0.0236,-0.0454,-0.0058,0.0049,0.0222,0.0289,0.0221,-0.0459,0.0307,-0.0297,-0.0323,-0.0913,0.1182,0.0357,-0.1343,-0.0099,0.0097,-0.0059,-0.0204,0.034,0.0848,-0.0199,0.0052,0.0805,0.0496,-0.1193,-0.0288,0.0028,-0.0428,0.0306,-0.0398,-0.0705,0.0494,0.0088,-0.0402,0.0214,-0.0101,-0.0046,0.0479,-0.0524,0.0375,-0.0049,-0.0106,-0.0052,-0.0171,-0.0215,-0.0348,-0.0103,-0.0306,0.02,-0.021,-0.05,0.0317,-0.0436,-0.0051,-0.0049,0.015,0.0148,-0.0239,-0.0435,-0.0166,0.0117,0.0108,0.0432,0.0116,0.0222,0.0468,0.0153,0.0331,-0.007,-0.0969,-0.0653,-0.2131,-0.0417,0.0184,-0.0301,0.0416,-0.0405,0.0529,-0.0102,0.0952,0.0787,0.0534,0.004,-0.0328,-0.0212,-0.0065,0.0889,0.0607,0.042,-0.0083,-0.0141,-0.0028,0.0102,0.0042,-0.0896,0.0464,0.0243,0.2226,0.0335,0.0251,-0.0034,-0.0204,0.0391,-0.0536,-0.0894,0.0401,0.0057,0.0457,0.023,-0.0553,-0.0546,-0.0544,0.0061,0.0174,-0.1251,-0.0153,-0.0059,-0.0219,0.0601,-0.087,0.01,0.0229,-0.0401,0.0499,-0.0039,-0.0003,-0.0034,-0.0694,0.0581,-0.0511,0.0043,0.0147,-0.0617,-0.0119,-0.0813,0.0688,0.0174,-0.0156,-0.0064,0.0057,-0.0294,-0.0013,0.0715,0.0467,0.0088,0.0561,-0.0124,0.05,0.0144,-0.0351,-0.0333,0.0451,-0.0729,0.0421,0.0277,0.0102,0.0189,0.059,-0.0112,0.0523,0.002,0.0193,0.0017,-0.0543,-0.0263,0.0499,-0.04,-0.2939,0.0088,0.0634,0.0206,-0.0625,-0.0204,0.038,0.0311,-0.0527,-0.0007,0.0193,0.0329,0.0353,-0.0331,-0.0301,0.0545,0.067,-0.007,0.0501,-0.0362,0.0159,0.0223,0.2169,-0.0324,0.0463,0.0394,-0.0399,-0.0161,0.0149,-0.0034,-0.0173,0.0217,0.0677,-0.0442,0.0719,0.0463,-0.0137,0.0495,0.0156,0.0088,0.0124,-0.0137,-0.0479,-0.0739,0.0905,-0.0012,-0.0469,-0.0375,0.0482,0.0351,0.0038,-0.0227,0.0038,-0.0045,0.0361,0.0089,-0.0087,-0.0113,-0.038,-0.0096,-0.0225,-0.0098,-0.0497,-0.0283,-0.0364]}
{"key":"[Learning non-Gaussian Time Series using the Box-Cox Gaussian Process] Gaussian processes (GPs) are Bayesian nonparametric generative models that provide interpretability of hyperparameters, admit closed-form expressions for training and inference, and are able to accurately represent uncertainty. To model general non-Gaussian data with complex correlation structure, GPs can be paired with an expressive covariance kernel and then fed into a nonlinear transformation (or warping). However, overparametrising the kernel and the warping is known to, respectively, hinder gradient-based training and make the predictions computationally expensive. We remedy this issue by (i) training the model using derivative-free global-optimisation techniques so as to find meaningful maxima of the model likelihood, and (ii) proposing a warping function based on the celebrated Box-Cox transformation that requires minimal numerical approximations---unlike existing warped GP models. We validate the proposed approach by first showing that predictions can be computed analytically, and then on a learning, reconstruction and forecasting experiment using real-world datasets.","layer":0,"vector":[-0.0378,-0.0262,0.021,-0.007,0.063,-0.0286,-0.0028,0.0083,0.0365,-0.0268,0.0346,-0.0484,0.0208,0.051,-0.0005,-0.0018,-0.0144,0.0359,-0.0715,0.0119,0.0471,-0.0198,-0.0065,-0.0149,0.0269,0.0045,-0.0185,-0.0158,-0.0373,-0.2346,-0.0028,-0.0429,0.0259,0.0127,-0.0252,-0.0057,-0.0355,0.0487,-0.0371,0.0663,0.0471,0.0207,-0.075,-0.0113,-0.005,-0.0846,-0.0483,-0.0108,-0.0501,-0.009,0.0236,-0.004,0.0201,0.0395,0.0474,0.0546,0.0612,0.0103,0.0663,0.0314,-0.01,0.0843,-0.2059,0.0732,0.0307,0.0274,-0.0201,-0.0195,0.013,0.0348,-0.0423,0.0415,-0.0065,0.0453,0.0374,0.0198,-0.0241,0.0028,-0.0348,0.0155,0.0191,-0.0061,-0.0202,-0.0406,-0.0452,-0.0374,0.0213,-0.0653,0.0322,0.026,-0.0524,-0.007,-0.0468,0.0208,-0.078,0.0047,0.0292,0.0472,-0.016,0.191,-0.0794,0.0425,0.0477,-0.0234,0.0739,-0.0523,-0.036,-0.0289,-0.0249,0.0106,0.0004,-0.0544,0.0151,-0.0527,0.023,0.0058,0.0339,0.0102,-0.0027,-0.0021,-0.0255,-0.0033,0.0639,-0.0135,-0.004,-0.0424,0.0466,0.1549,0.0557,0.0189,0.0727,-0.0294,-0.0656,0.0256,0.0421,-0.0187,0.0649,0.0389,0.0158,0.0093,-0.0403,-0.0296,0.0033,-0.0611,-0.0435,0.1165,-0.0563,0.0009,-0.1019,0.0122,-0.0369,-0.0256,0.0161,-0.0587,0.0289,0.0075,0.0179,0.0433,-0.068,0.0221,-0.057,-0.0411,-0.0418,0.1,0.0224,-0.0709,-0.0438,0.041,0.0457,0.0301,0.0556,0.0356,-0.0357,0.0129,0.1257,0.0415,-0.0479,0.0318,0.0196,0.0172,0.0356,-0.0498,-0.024,0.0438,0.0621,-0.0394,0.0058,-0.0409,-0.0,-0.0234,-0.0059,-0.0085,-0.0375,-0.0096,-0.0165,-0.0171,-0.0382,-0.0076,0.0344,-0.0549,0.0068,-0.0303,-0.0514,-0.0075,-0.0293,-0.0021,-0.0115,0.0413,0.0508,0.0329,-0.0058,-0.0193,0.0658,-0.021,-0.0633,0.0312,-0.0128,0.0595,-0.027,0.0211,0.0107,-0.0512,-0.0547,-0.2215,0.012,0.0408,-0.0147,0.0426,-0.0184,0.011,-0.0018,0.0636,0.093,0.0461,0.0049,-0.0493,0.0124,-0.0232,0.0394,0.0165,0.0514,-0.0011,0.0136,-0.0125,-0.0108,-0.0243,-0.1523,0.044,-0.0347,0.1962,-0.0042,0.0575,-0.064,0.0148,-0.0035,-0.0285,-0.0682,0.0589,0.0715,0.0872,0.0019,-0.0555,-0.0237,-0.0459,-0.0192,0.0433,-0.0636,-0.0641,-0.0346,-0.0412,0.0406,-0.0632,0.02,0.0332,-0.0201,0.0987,-0.0284,-0.0225,-0.0366,-0.04,0.0348,-0.0443,0.0385,0.019,-0.0462,0.0471,-0.0451,0.0306,-0.0435,-0.0014,-0.0411,-0.0197,-0.0313,0.0138,0.1053,-0.0346,0.0152,0.0559,0.0194,0.025,-0.0102,-0.0816,-0.033,0.06,-0.0309,0.0429,0.0006,0.0312,-0.0166,0.0596,-0.0368,0.0194,-0.0468,0.0024,-0.0175,-0.0436,0.0158,0.0206,0.0055,-0.2787,0.0102,-0.0198,0.0313,-0.0101,-0.0416,-0.0015,0.0422,-0.045,0.0406,-0.0318,0.0172,0.0345,-0.0193,0.0122,0.0297,0.0816,-0.0699,0.0628,-0.0513,-0.0043,0.0556,0.2045,-0.0138,0.0479,0.0223,-0.0084,-0.0001,0.0318,-0.0477,0.0426,0.0084,0.0501,-0.0474,0.0344,0.0415,-0.0064,0.0921,0.0394,-0.0313,0.0262,0.0052,0.0005,-0.0369,0.1057,-0.0089,-0.0432,-0.0286,-0.0131,0.0575,-0.0117,0.0494,0.0123,0.0149,-0.0111,0.0609,-0.0372,-0.0413,-0.0195,-0.021,-0.0038,-0.0871,-0.0227,-0.0075,-0.0237]}
{"key":"[On a scalable entropic breaching of the overfitting barrier in machine learning] Overfitting and treatment of \"small data\" are among the most challenging problems in the machine learning (ML), when a relatively small data statistics size $T$ is not enough to provide a robust ML fit for a relatively large data feature dimension $D$. Deploying a massively-parallel ML analysis of generic classification problems for different $D$ and $T$, existence of statistically-significant linear overfitting barriers for common ML methods is demonstrated. For example, these results reveal that for a robust classification of bioinformatics-motivated generic problems with the Long Short-Term Memory deep learning classifier (LSTM) one needs in a best case a statistics $T$ that is at least 13.8 times larger then the feature dimension $D$. It is shown that this overfitting barrier can be breached at a $10^{-12}$ fraction of the computational cost by means of the entropy-optimal Scalable Probabilistic Approximations algorithm (eSPA), performing a joint solution of the entropy-optimal Bayesian network inference and feature space segmentation problems. Application of eSPA to experimental single cell RNA sequencing data exhibits a 30-fold classification performance boost when compared to standard bioinformatics tools - and a 7-fold boost when compared to the deep learning LSTM classifier.","layer":1,"vector":[-0.0097,-0.0145,0.0516,-0.0075,0.0127,0.0531,0.0418,0.0141,0.0227,-0.022,0.0164,-0.0341,0.0457,0.0468,0.0324,0.0449,0.0251,0.0324,-0.0423,-0.006,0.041,-0.0516,-0.0165,-0.0497,0.0196,0.0335,-0.0121,-0.0701,-0.0584,-0.2769,0.0143,-0.0382,0.0293,-0.0324,0.0492,-0.0378,-0.0363,0.0323,-0.0592,0.0232,0.0308,0.0361,-0.048,-0.0181,-0.0155,-0.062,-0.0371,-0.044,-0.0233,-0.0372,0.0013,-0.0241,0.023,0.0147,0.0138,0.0452,0.0533,0.0331,0.03,0.05,0.0234,0.0588,-0.1201,0.05,0.0349,0.0009,-0.0181,-0.0236,0.0174,0.0381,-0.0324,0.0549,0.0396,0.0844,0.013,0.0228,-0.0068,-0.0178,0.0351,0.0257,0.0103,-0.0196,0.0044,-0.0217,0.0089,-0.0643,0.0437,-0.0594,0.04,-0.0331,-0.0214,0.0125,-0.0005,0.0459,-0.0558,-0.0114,0.0326,0.0028,-0.0503,0.1823,-0.0499,0.0509,0.0019,-0.0181,0.0286,-0.0507,-0.0358,-0.0297,-0.0243,-0.0277,-0.0059,-0.0173,0.0087,-0.0215,0.0072,0.0211,0.0744,0.0032,-0.0267,0.0395,-0.0461,-0.0214,0.0258,-0.0238,0.0272,-0.0286,0.0127,0.1787,0.0472,0.0092,0.0375,0.0128,-0.0604,-0.0384,0.0353,0.0212,0.0386,0.0231,0.0568,-0.029,-0.0332,-0.0383,0.0007,-0.0783,-0.0441,0.1457,-0.0871,0.0122,-0.0391,-0.0788,-0.0311,0.0167,-0.0203,-0.0293,0.0481,0.0226,0.0031,0.0222,-0.0453,0.0007,0.0225,-0.0119,0.0029,0.1124,0.0406,-0.0679,-0.0307,0.0027,-0.0041,-0.0025,0.0242,0.0067,-0.0652,0.0299,0.0176,0.0095,-0.1067,-0.0393,0.0191,0.0116,0.0018,-0.0668,-0.0211,0.0023,0.0404,-0.0579,0.0074,-0.0135,0.0222,0.0346,-0.0339,0.01,-0.0245,-0.0238,-0.0214,-0.0506,0.0074,0.0214,0.0125,-0.0636,-0.0104,0.0473,-0.0535,0.0678,-0.0118,0.0474,-0.0163,-0.0019,0.0502,0.0094,-0.0214,0.0203,0.0181,-0.0156,-0.0194,0.0152,0.0059,0.0179,0.0195,0.0119,0.065,-0.0393,-0.0612,-0.2247,-0.0181,0.0049,-0.0356,0.0194,-0.0814,0.051,-0.0004,0.0213,0.0709,0.0426,0.0199,-0.0441,-0.0112,-0.031,0.0705,0.0725,0.0338,-0.0529,0.0399,0.0464,0.066,-0.0244,-0.0606,0.0428,-0.039,0.2265,-0.0095,0.0595,-0.0085,0.0344,0.0239,-0.0024,-0.0824,0.0742,0.0045,0.0596,-0.0176,-0.0573,-0.0335,-0.021,0.0338,-0.0058,-0.1632,-0.0648,-0.0505,-0.0041,-0.0237,-0.0689,-0.017,0.0611,-0.0052,0.0561,0.0138,0.0312,-0.0198,-0.0828,0.0327,-0.0218,0.0096,0.0047,-0.0742,0.0509,-0.0205,0.0162,-0.0182,-0.0126,-0.0064,0.0118,-0.0433,-0.0445,0.0699,-0.0231,0.0021,0.0883,-0.0023,0.0704,-0.0262,-0.0454,-0.039,0.0759,-0.0147,-0.0057,0.0138,0.0182,0.037,0.1159,-0.0225,0.0123,-0.011,0.0336,-0.0128,-0.0402,-0.0062,0.0119,0.0135,-0.2782,0.0638,-0.0146,0.0253,-0.0374,-0.008,0.0158,-0.0069,-0.0069,0.0196,0.0279,0.0386,0.0794,-0.019,0.0024,0.0309,0.0603,-0.025,0.0429,-0.0528,0.0236,0.0064,0.207,-0.0565,0.0223,0.0412,0.0004,0.0192,0.0193,-0.076,0.0222,0.0138,0.0594,-0.0465,0.0435,0.0884,-0.0084,0.0094,0.0283,-0.0554,0.0585,-0.0032,-0.0414,-0.028,0.0979,-0.0515,-0.0206,-0.0657,-0.0381,0.0174,-0.0487,0.0085,-0.0159,0.0422,0.0241,0.0285,-0.0528,-0.0629,-0.0119,-0.0695,0.0177,-0.0766,-0.0464,0.0312,-0.0127]}
{"key":"[Deep Learning for Channel Coding via Neural Mutual Information Estimation] End-to-end deep learning for communication systems, i.e., systems whose encoder and decoder are learned, has attracted significant interest recently, due to its performance which comes close to well-developed classical encoder-decoder designs. However, one of the drawbacks of current learning approaches is that a differentiable channel model is needed for the training of the underlying neural networks. In real-world scenarios, such a channel model is hardly available and often the channel density is not even known at all. Some works, therefore, focus on a generative approach, i.e., generating the channel from samples, or rely on reinforcement learning to circumvent this problem. We present a novel approach which utilizes a recently proposed neural estimator of mutual information. We use this estimator to optimize the encoder for a maximized mutual information, only relying on channel samples. Moreover, we show that our approach achieves the same performance as state-of-the-art end-to-end learning with perfect channel model knowledge.","layer":3,"vector":[-0.0377,-0.0039,0.0335,-0.0584,0.0166,0.0302,0.0256,0.0341,0.0429,0.0019,0.0049,-0.0715,0.037,0.0666,0.0197,0.0358,-0.023,0.041,-0.0395,0.0043,0.0538,-0.0511,-0.0143,-0.0373,-0.0018,-0.0488,-0.0236,-0.0599,-0.0156,-0.2211,0.0429,-0.0179,0.0441,-0.039,-0.0022,-0.0483,-0.0682,0.013,-0.0713,0.0627,0.0072,0.0338,-0.027,-0.0448,-0.0028,-0.0515,-0.041,-0.0189,-0.0167,-0.0634,0.0374,0.002,0.0104,0.0157,0.012,0.0474,0.0437,0.0648,0.0099,0.0727,0.0336,0.0579,-0.1778,0.0588,0.0265,0.0417,-0.0296,0.0487,0.0267,0.0342,-0.0229,0.0481,0.0476,0.046,0.0251,0.0287,0.0038,-0.0375,0.0141,0.0209,-0.012,-0.0405,-0.0377,-0.02,-0.0069,-0.0463,0.0179,-0.0096,0.0117,0.0161,-0.0809,0.0248,-0.0423,0.0389,-0.0844,0.0208,-0.0266,0.002,-0.0593,0.2055,-0.0357,0.027,0.0571,-0.0565,0.0473,-0.0406,-0.0395,0.0134,-0.0392,0.0118,-0.0342,-0.0349,-0.0038,-0.0347,0.0558,0.0198,0.0816,0.0298,-0.0031,-0.0022,-0.0358,0.0073,0.0333,0.0029,0.038,-0.0725,0.0044,0.1473,0.0173,0.0382,0.0416,-0.001,-0.0203,-0.0449,0.0186,0.0333,0.0275,0.0041,-0.0031,-0.0049,-0.0018,-0.0373,0.0171,-0.0382,-0.0721,0.0931,-0.0242,-0.0007,-0.0688,-0.035,-0.0295,0.0223,-0.0164,-0.0351,0.0629,0.0237,0.0414,0.0428,-0.0661,0.0093,-0.0352,-0.0422,-0.0175,0.1315,0.0108,-0.0775,-0.0378,0.017,0.0473,-0.0083,0.0253,0.0322,-0.0435,0.0191,0.0508,-0.009,-0.0921,0.0184,-0.0104,0.0127,-0.0372,-0.0665,0.0047,0.0308,0.0049,-0.0472,0.0443,-0.0578,0.0322,0.0207,0.0052,0.0227,-0.0183,0.0044,-0.0064,-0.0413,-0.0091,-0.0189,-0.0337,-0.0263,-0.0375,0.0016,-0.084,-0.0066,-0.0247,0.0229,0.0037,0.0023,0.0506,-0.0197,0.0008,-0.0117,0.1182,-0.0368,-0.0112,-0.0387,-0.0076,0.0316,-0.0089,0.0402,0.0174,-0.0617,-0.0162,-0.2172,0.0073,0.0288,-0.0462,0.0628,-0.0816,0.0503,-0.0005,0.027,0.0701,0.0513,0.011,-0.0114,0.0242,0.0234,0.0657,0.0526,0.072,0.0112,0.0099,-0.0158,0.0186,-0.0084,-0.0819,0.0291,-0.0175,0.1961,0.012,0.0405,0.0121,0.0086,0.0867,0.0013,-0.0827,0.0482,0.0174,0.1105,0.0021,-0.0086,-0.0297,-0.0516,0.0188,-0.0045,-0.1052,-0.0343,-0.0483,-0.0469,0.0132,-0.0766,-0.0195,0.0376,-0.0303,0.0609,-0.0289,0.0006,-0.0307,-0.1057,0.0163,-0.0458,0.0524,0.0111,-0.0392,0.0021,-0.0627,0.0213,0.0069,0.002,-0.0655,0.0489,0.0125,-0.0041,0.0693,0.0173,0.0228,0.0604,0.0073,0.0253,-0.0618,-0.0705,-0.0269,0.0651,-0.0442,0.0685,0.0159,0.0315,0.0095,0.0385,0.0211,0.0399,-0.0221,0.006,0.0119,-0.0064,-0.0206,0.0436,-0.0571,-0.2891,0.0149,-0.0248,0.0092,-0.042,0.0242,0.057,0.0305,-0.103,0.0106,0.0008,0.0128,-0.0029,-0.0316,0.0388,0.0457,0.0613,-0.0563,0.0175,-0.0509,0.0221,0.0198,0.1949,0.0048,0.0389,-0.0632,-0.0681,0.0399,0.0527,-0.0139,0.0099,0.0337,0.1082,-0.0478,0.0278,0.0914,-0.0189,0.0372,-0.0114,0.0043,0.0256,0.0115,-0.0309,0.0173,0.0952,0.0324,-0.0502,-0.0058,-0.0296,0.0221,-0.0594,0.0381,-0.0423,0.0137,0.0494,0.0341,-0.1087,-0.0625,-0.0055,0.004,0.0382,-0.0757,-0.0547,-0.006,-0.0099]}
{"key":"[FS-NCSR: Increasing Diversity of the Super-Resolution Space via Frequency Separation and Noise-Conditioned Normalizing Flow] Super-resolution suffers from an innate ill-posed problem that a single low-resolution (LR) image can be from multiple high-resolution (HR) images. Recent studies on the flow-based algorithm solve this ill-posedness by learning the super-resolution space and predicting diverse HR outputs. Unfortunately, the diversity of the super-resolution outputs is still unsatisfactory, and the outputs from the flow-based model usually suffer from undesired artifacts which causes low-quality outputs. In this paper, we propose FS-NCSR which produces diverse and high-quality super-resolution outputs using frequency separation and noise conditioning compared to the existing flow-based approaches. As the sharpness and high-quality detail of the image rely on its high-frequency information, FS-NCSR only estimates the high-frequency information of the high-resolution outputs without redundant low-frequency components. Through this, FS-NCSR significantly improves the diversity score without significant image quality degradation compared to the NCSR, the winner of the previous NTIRE 2021 challenge.","layer":0,"vector":[0.0044,0.0011,-0.0105,-0.041,0.0348,0.0181,-0.0107,0.0157,0.0507,0.0344,0.0241,-0.0602,0.0693,0.0384,-0.0011,0.0015,0.0232,0.0467,-0.0194,0.0303,0.0256,-0.0058,0.0238,-0.0352,0.0009,-0.0107,-0.0261,-0.0949,-0.0513,-0.2881,0.0235,-0.0093,0.0475,-0.0097,0.0422,-0.0827,-0.0411,0.0687,-0.0313,0.0541,-0.0153,0.0456,0.0344,-0.0462,-0.0485,-0.0321,-0.05,-0.0292,0.0,-0.0298,0.034,-0.0391,-0.0008,0.0331,0.0123,0.0449,0.0398,0.0025,0.0562,0.0457,0.024,0.0432,-0.1956,0.044,0.0656,-0.0239,-0.0083,0.0293,0.0284,-0.0314,-0.0345,0.0549,-0.0104,0.0397,0.0226,-0.0418,0.0165,-0.0289,-0.0046,0.0023,0.0567,0.0034,-0.0091,-0.0089,-0.0275,-0.0169,0.0271,-0.0399,0.0492,-0.0206,-0.0581,-0.0393,-0.0249,0.0511,-0.0745,-0.0173,-0.0416,-0.0077,0.0272,0.1809,-0.0519,0.0464,0.0744,-0.0488,0.0436,-0.0441,-0.026,0.0396,-0.0592,-0.0229,-0.0164,-0.0595,0.0767,-0.0254,-0.0032,0.0046,0.0506,0.0337,0.0165,-0.0105,-0.0316,-0.0071,0.0359,-0.0063,0.0353,-0.0604,0.0513,0.1248,0.0305,-0.0084,0.0314,-0.0269,-0.0209,-0.0253,-0.004,0.0254,0.0098,0.0248,-0.0077,0.01,-0.0498,-0.0549,-0.0111,-0.1099,-0.044,0.0855,-0.0727,0.0807,-0.0819,-0.0345,-0.0182,0.0381,-0.0158,0.0291,0.0228,0.0326,0.0063,0.0411,-0.0555,0.0233,-0.0136,-0.0927,-0.0531,0.0676,0.0281,-0.0735,-0.0058,-0.0151,0.0046,-0.0135,-0.0427,-0.0066,-0.0064,0.0183,0.1132,0.036,-0.0352,0.0099,0.0093,0.0051,0.0098,-0.0701,-0.0352,0.0093,0.0446,-0.0809,-0.0029,-0.0064,-0.0014,0.0017,-0.0594,0.0152,-0.002,-0.0152,-0.0311,-0.0108,-0.0238,-0.0242,-0.0069,-0.0307,0.0514,-0.0051,-0.0325,0.0213,0.0032,0.0391,0.0158,-0.0059,-0.0125,0.0595,-0.0312,-0.0289,0.0889,0.0267,-0.0089,0.0059,0.0306,-0.0077,0.0173,0.0468,0.0615,-0.0501,-0.1239,-0.2265,-0.0143,0.0131,0.0142,0.1078,-0.0059,0.0382,-0.0071,0.0808,0.0594,0.065,-0.0142,0.0081,0.0451,0.0064,0.0589,0.0211,0.0341,-0.0305,-0.0269,0.0029,0.0714,-0.0091,-0.0705,0.0574,-0.0479,0.2089,0.0264,0.0535,-0.0258,0.0348,0.002,-0.0318,-0.058,0.0391,0.027,0.0745,-0.0087,-0.032,-0.0461,-0.032,0.0109,-0.0048,-0.0966,-0.0433,-0.022,-0.0627,0.0158,-0.0478,-0.0109,0.036,0.0125,0.0059,-0.0441,0.0019,0.006,-0.0916,0.0335,-0.0302,0.0266,0.018,-0.0525,0.0026,-0.0593,0.0993,0.0254,-0.0145,-0.0132,0.0125,-0.0341,-0.0188,0.0514,0.009,0.0067,0.0564,0.0321,0.064,-0.0476,-0.0336,-0.0519,0.0866,-0.0307,0.0533,0.0486,0.0245,0.0533,0.0878,-0.0346,-0.0163,-0.0516,0.0255,-0.0276,-0.0312,-0.0125,0.0088,0.032,-0.2909,-0.0411,0.0169,0.0081,-0.0346,0.0161,0.0311,0.0103,-0.049,0.0392,-0.0384,0.0406,0.049,0.0204,0.0324,0.0291,0.0661,-0.01,0.0691,-0.0688,-0.0049,0.0542,0.1895,-0.0112,-0.0284,-0.0112,-0.0322,0.0058,-0.0028,-0.0591,0.037,0.0292,0.0523,-0.0443,-0.006,0.0891,-0.0371,0.0217,0.0155,0.0011,-0.0219,-0.0091,-0.0066,-0.0278,0.0996,-0.0395,-0.018,0.0136,-0.0161,-0.03,-0.0414,0.0081,0.0208,-0.0151,0.058,0.0324,-0.0456,-0.0463,-0.0208,0.0217,-0.0242,-0.0307,-0.0459,0.0387,-0.0383]}
{"key":"[\"Garbage In, Garbage Out\" Revisited: What Do Machine Learning Application Papers Report About Human-Labeled Training Data?] Supervised machine learning, in which models are automatically derived from labeled training data, is only as good as the quality of that data. This study builds on prior work that investigated to what extent 'best practices' around labeling training data were followed in applied ML publications within a single domain (social media platforms). In this paper, we expand by studying publications that apply supervised ML in a far broader spectrum of disciplines, focusing on human-labeled data. We report to what extent a random sample of ML application papers across disciplines give specific details about whether best practices were followed, while acknowledging that a greater range of application fields necessarily produces greater diversity of labeling and annotation methods. Because much of machine learning research and education only focuses on what is done once a \"ground truth\" or \"gold standard\" of training data is available, it is especially relevant to discuss issues around the equally-important aspect of whether such data is reliable in the first place. This determination becomes increasingly complex when applied to a variety of specialized fields, as labeling can range from a task requiring little-to-no background knowledge to one that must be performed by someone with career expertise.","layer":2,"vector":[-0.0148,-0.018,-0.0048,-0.009,0.0524,-0.0027,0.0384,0.0565,0.0113,-0.0212,0.0227,-0.0413,0.0121,0.0499,0.0119,0.0553,0.004,0.0199,-0.0375,-0.0188,0.0256,-0.0121,-0.0267,-0.0458,0.0454,0.0232,-0.0758,-0.0653,-0.043,-0.2469,0.0148,-0.0308,0.0687,0.0034,-0.0105,-0.0094,-0.0035,0.0617,0.0126,0.0332,0.0153,-0.0579,-0.0187,-0.073,-0.0218,-0.0048,-0.051,-0.051,-0.0655,-0.0002,0.0298,-0.0215,-0.0207,0.0444,0.0207,0.0564,0.0931,0.0171,0.036,0.0565,0.0259,0.0727,-0.2034,0.0627,0.038,0.0261,-0.0806,0.0034,-0.0141,0.0307,0.0356,0.0135,0.0183,0.0697,0.0158,0.0189,0.0195,-0.018,-0.0138,-0.0026,0.0037,-0.0243,-0.029,-0.0191,-0.0268,-0.0421,0.0162,-0.0124,0.0627,0.0032,0.0272,0.0034,-0.0039,0.0571,-0.0595,-0.0069,0.0059,0.0167,-0.0728,0.1931,-0.0438,0.0504,0.0342,-0.0371,0.0211,-0.0417,-0.0399,-0.0305,-0.0316,-0.0249,-0.0355,-0.0101,0.0003,-0.039,0.0615,0.0188,0.0891,0.0471,0.0014,0.0129,-0.0344,-0.0045,0.0346,0.0065,0.0065,-0.0393,0.0353,0.0985,0.042,-0.0017,0.0337,-0.0094,-0.0925,-0.0297,0.0298,0.0311,0.0243,0.0182,0.0498,0.0036,-0.0094,-0.023,0.0003,-0.0517,-0.0558,0.1597,-0.0497,0.0245,-0.0336,0.0124,-0.0119,0.0427,-0.0516,0.0262,0.043,0.023,0.0545,0.0242,-0.0713,0.0194,0.012,-0.0585,-0.0549,0.0895,-0.0223,-0.0556,-0.0337,-0.0019,0.0174,-0.0058,0.0638,0.0313,-0.0223,0.0665,0.0189,0.0191,-0.0647,-0.0148,0.0135,0.0375,0.0134,-0.0158,-0.0695,0.058,0.0394,-0.0471,0.0076,-0.0619,0.0437,0.0612,-0.018,0.0104,-0.0094,-0.0636,-0.023,-0.0268,-0.0181,-0.0076,0.014,-0.0441,-0.0681,0.0379,-0.0213,0.0232,0.0252,0.0055,-0.0288,-0.0225,0.0822,0.0102,-0.0392,-0.0087,0.0185,-0.0387,-0.0356,-0.0181,0.0504,0.055,-0.0096,0.0441,0.0088,-0.0053,-0.0375,-0.2361,0.0019,0.015,-0.0094,0.0471,-0.0655,0.0379,0.0024,0.0265,0.0855,0.074,-0.0649,-0.0691,-0.012,-0.0087,0.0399,0.0382,0.0136,-0.0265,0.0137,-0.018,-0.0372,-0.0165,-0.0862,0.0588,-0.0123,0.2329,0.0606,0.0204,-0.0245,0.0115,-0.0054,-0.0384,-0.1353,0.0635,0.0032,0.0466,-0.0164,-0.0455,-0.0126,-0.0358,0.0205,0.0451,-0.1254,-0.0518,-0.0271,-0.0563,-0.0355,-0.0372,0.0213,0.0185,-0.0146,0.0492,-0.0272,-0.0258,-0.0238,-0.0814,0.038,-0.0641,0.0318,0.0034,-0.0754,0.0564,-0.0541,0.0396,-0.0292,-0.0307,-0.0292,0.0242,-0.0081,-0.041,0.1057,0.0135,-0.0514,0.0608,-0.0152,-0.0333,-0.097,-0.0583,0.0044,0.0595,-0.013,0.0656,0.0287,0.0392,0.0326,0.0282,-0.051,0.0535,-0.0079,-0.0145,0.0464,-0.0447,-0.0193,0.0548,-0.0182,-0.26,0.0453,0.0211,0.0906,-0.0096,0.0096,0.0461,0.0386,0.0044,0.0146,-0.0051,0.0099,0.0286,-0.0238,0.0186,0.0674,0.034,-0.0432,0.0062,-0.0514,0.0336,0.0266,0.179,-0.0609,0.0146,0.021,-0.0406,0.011,0.0772,-0.0215,0.021,-0.0226,0.0766,0.0195,0.0458,0.045,-0.0327,-0.0093,0.0038,0.0073,-0.0479,0.0098,-0.0697,-0.0302,0.0896,0.0023,0.0094,-0.0802,-0.0144,-0.0085,-0.0052,-0.0185,-0.0469,0.0005,0.0373,0.0586,-0.0126,-0.0545,-0.0555,-0.0286,0.0475,-0.0107,-0.0103,0.0333,0.0183]}
{"key":"[NetKet 3: Machine Learning Toolbox for Many-Body Quantum Systems] We introduce version 3 of NetKet, the machine learning toolbox for many-body quantum physics. NetKet is built around neural-network quantum states and provides efficient algorithms for their evaluation and optimization. This new version is built on top of JAX, a differentiable programming and accelerated linear algebra framework for the Python programming language. The most significant new feature is the possibility to define arbitrary neural network ans\\\"atze in pure Python code using the concise notation of machine-learning frameworks, which allows for just-in-time compilation as well as the implicit generation of gradients thanks to automatic differentiation. NetKet 3 also comes with support for GPU and TPU accelerators, advanced support for discrete symmetry groups, chunking to scale up to thousands of degrees of freedom, drivers for quantum dynamics applications, and improved modularity, allowing users to use only parts of the toolbox as a foundation for their own code.","layer":0,"vector":[-0.0893,0.0055,0.017,0.017,-0.0112,0.0282,0.0316,0.0111,0.0176,-0.0113,0.0203,-0.0686,0.0251,0.0127,0.0383,-0.02,-0.0157,0.0138,-0.0593,-0.0131,-0.0078,0.0051,-0.0045,-0.0845,0.022,0.0309,0.0113,-0.0195,-0.0481,-0.2101,0.022,-0.0213,0.0229,-0.0324,0.0187,-0.0335,-0.0124,0.0213,-0.0304,0.0323,0.0372,0.0287,-0.0065,-0.0279,0.0159,-0.0548,-0.0375,-0.049,-0.0348,-0.0553,0.0051,-0.0388,0.0455,0.0514,0.0598,0.0288,0.0589,0.0396,0.0263,0.0016,0.006,0.0382,-0.1497,0.0505,0.1043,0.0227,-0.0323,-0.0253,0.0588,0.0482,-0.0454,0.0643,0.0371,0.0308,-0.0256,-0.0014,0.0345,-0.0111,-0.0153,-0.019,-0.0115,-0.0387,-0.0632,-0.0258,-0.0268,-0.0361,0.0169,0.0044,-0.0249,0.0056,-0.059,-0.0219,-0.0194,-0.0075,-0.0365,0.0201,0.047,0.044,-0.0535,0.226,-0.0329,0.0125,-0.024,-0.0006,0.0494,-0.0564,-0.0268,-0.0196,-0.0254,-0.0138,0.0094,-0.0072,0.0213,-0.0604,0.0079,0.0132,0.0588,0.0117,-0.037,-0.023,-0.0384,0.0098,0.0432,0.0008,-0.0068,-0.0129,-0.0379,0.1243,0.0184,0.0727,0.0488,0.0017,-0.0191,-0.0535,0.0131,0.0285,0.037,0.0031,0.0245,0.0236,-0.0038,-0.0694,0.0164,-0.1018,-0.0622,0.0765,-0.0473,0.0122,-0.0184,0.006,-0.0048,0.0704,-0.0657,-0.0277,0.0638,0.0779,0.0058,0.0505,-0.0343,0.0304,-0.0607,-0.0502,-0.046,0.0866,0.0238,-0.0877,0.002,-0.0363,-0.0219,-0.0577,0.0426,0.0597,-0.0629,0.0539,0.091,-0.0009,-0.0633,0.0284,0.0118,0.0338,-0.0019,-0.0304,-0.0499,0.0436,0.0508,-0.0255,0.0029,-0.0329,0.0001,-0.0315,0.0101,0.0646,-0.0468,-0.0019,-0.0314,-0.0206,-0.0574,-0.0125,-0.0057,-0.0252,0.0726,-0.0203,-0.0237,0.0288,-0.0552,-0.015,0.023,-0.0255,0.0286,0.0127,-0.0113,-0.0247,0.0284,-0.0198,-0.0722,-0.03,0.0073,0.0285,-0.0069,0.0515,0.0292,-0.0532,-0.1338,-0.2332,0.0103,0.0228,-0.0445,0.1196,-0.0726,0.0317,-0.0071,0.0273,0.0682,0.0461,0.0425,-0.0204,-0.0236,-0.011,0.0439,0.0326,0.03,-0.0516,0.0545,-0.0106,0.0098,-0.0638,-0.1159,0.0258,-0.0282,0.2094,0.0478,0.0423,0.0005,0.0434,0.0385,-0.0492,-0.0629,0.09,-0.0125,0.0671,0.0331,-0.0436,-0.0269,-0.0238,0.0423,0.0317,-0.0798,0.0041,-0.0287,-0.003,0.0296,-0.0102,-0.0124,0.0675,-0.0283,0.0126,0.0078,-0.0412,-0.027,-0.0637,0.031,-0.0182,0.0453,0.0098,-0.0605,-0.0021,-0.0204,0.0284,-0.0276,-0.0078,-0.0406,0.0782,-0.0675,-0.0133,0.0491,-0.0421,-0.0185,0.0642,0.0103,0.0254,0.0254,-0.0296,0.0008,0.0424,0.0123,0.0383,0.0021,-0.0093,-0.0004,0.0828,0.0176,0.0384,-0.0117,-0.0142,0.0152,-0.0358,0.0773,0.0339,0.0265,-0.3107,0.0432,-0.0077,0.0266,-0.0186,0.0176,0.0462,0.0384,-0.0895,-0.0291,0.0064,0.0649,0.0315,-0.0166,0.0294,0.0255,0.0792,-0.0501,-0.0021,-0.0318,0.0095,0.0606,0.2524,-0.0325,0.0459,0.0498,-0.0264,0.0136,0.0303,-0.0382,-0.0158,-0.0538,0.0588,-0.0813,0.036,0.0499,-0.0387,-0.0024,0.0202,-0.0073,0.0036,0.0275,-0.0082,-0.0391,0.095,-0.0185,-0.0352,-0.0421,0.0261,0.028,0.0028,0.0297,-0.0102,-0.0249,0.0464,0.019,-0.022,-0.0791,-0.0173,0.002,0.0101,-0.0369,0.0039,-0.0099,-0.0034]}
{"key":"[iNNvestigate neural networks!] In recent years, deep neural networks have revolutionized many application domains of machine learning and are key components of many critical decision or predictive processes. Therefore, it is crucial that domain specialists can understand and analyze actions and pre- dictions, even of the most complex neural network architectures. Despite these arguments neural networks are often treated as black boxes. In the attempt to alleviate this short- coming many analysis methods were proposed, yet the lack of reference implementations often makes a systematic comparison between the methods a major effort. The presented library iNNvestigate addresses this by providing a common interface and out-of-the- box implementation for many analysis methods, including the reference implementation for PatternNet and PatternAttribution as well as for LRP-methods. To demonstrate the versatility of iNNvestigate, we provide an analysis of image classifications for variety of state-of-the-art neural network architectures.","layer":8,"vector":[-0.0619,-0.0489,-0.0322,0.0024,0.0388,0.0212,0.0711,-0.0017,0.0363,-0.0402,-0.0153,-0.0365,0.0078,0.0306,-0.0091,0.0009,-0.0115,0.0258,-0.0362,-0.0041,0.0217,-0.0257,0.0094,-0.0807,0.0189,-0.0004,-0.0224,-0.0635,-0.0196,-0.2409,0.0657,-0.0482,0.011,-0.0418,0.0082,-0.024,-0.0855,0.0548,-0.0158,-0.0127,0.0122,-0.0066,-0.0307,-0.0577,0.0018,-0.0509,-0.0263,-0.0531,-0.0109,-0.0589,0.0105,-0.0429,0.0391,0.0379,0.0521,0.0045,0.0555,0.0344,0.0613,0.013,0.043,0.0375,-0.1996,0.0496,0.0134,0.036,0.0123,0.0059,0.0166,0.0236,-0.0329,0.0597,0.0253,0.0041,0.0174,-0.0022,0.0231,-0.038,-0.0096,-0.0119,0.0515,0.0613,-0.0329,-0.0061,0.0261,-0.0136,0.0202,-0.0502,0.0334,-0.0183,-0.0229,-0.0179,-0.0647,0.0213,-0.0349,0.0052,0.0265,0.0276,-0.068,0.209,-0.0473,-0.0082,0.0392,-0.0134,0.0283,0.0179,-0.0172,-0.032,-0.0613,0.0034,-0.0168,-0.0369,0.0194,-0.0299,-0.0001,0.0034,0.0352,0.0375,0.0154,0.0115,-0.0111,0.0281,0.0474,-0.0193,0.016,-0.075,-0.0037,0.1287,0.0261,0.0184,0.0333,0.0001,-0.0474,0.0062,0.0273,0.0324,-0.0003,0.0139,0.0072,-0.0219,-0.0345,-0.0194,0.004,-0.0642,-0.0655,0.1384,-0.0551,-0.0092,-0.0141,0.0129,-0.038,0.0212,-0.0734,-0.0387,0.0181,0.0236,-0.0257,0.0703,-0.0327,0.0125,-0.0145,-0.0679,-0.0343,0.1092,0.0123,-0.1211,0.005,-0.0263,-0.0029,-0.0109,0.015,0.0213,-0.0349,-0.0102,0.076,0.0578,-0.0704,-0.0568,-0.0133,-0.0199,0.0259,-0.0457,-0.04,0.0815,0.0691,-0.012,0.0139,-0.0407,0.0122,0.0259,-0.0499,-0.0032,-0.0377,-0.0066,-0.0033,-0.0177,-0.0289,0.0102,0.0254,-0.0248,0.0205,0.0269,-0.0136,0.042,-0.016,0.0081,-0.0512,-0.0123,0.0202,0.0244,-0.0069,0.0283,0.0728,-0.024,-0.0052,0.0312,0.0448,0.0278,0.023,0.0796,0.0588,-0.0678,-0.068,-0.2636,0.0067,-0.0045,-0.042,0.0649,-0.0686,0.0557,-0.0132,0.0273,0.0476,0.0506,-0.0388,0.0001,0.0111,-0.0057,0.0393,0.056,0.0191,-0.0582,0.0175,-0.0239,0.0159,0.0145,-0.0966,0.0373,-0.0052,0.1827,0.0469,0.0553,0.0109,-0.0162,0.0429,-0.0242,-0.0916,0.072,0.0187,0.0892,0.004,-0.0043,-0.0645,-0.0664,0.0273,0.0209,-0.1036,-0.0093,-0.0518,0.0215,0.0064,-0.0717,-0.0364,0.0633,-0.018,0.0084,0.0126,0.0187,-0.0315,-0.0765,0.0266,0.0408,-0.0066,-0.0202,-0.0639,0.0092,-0.076,0.049,0.0154,-0.0133,-0.0161,0.0212,-0.0214,0.005,0.1168,0.028,-0.0306,0.0701,0.0104,0.0277,-0.0079,-0.0389,0.0024,0.0536,-0.0411,-0.0067,0.0326,0.0334,0.0453,0.0857,-0.0459,0.047,0.0045,0.0143,0.0319,-0.0434,0.0195,0.0505,0.0226,-0.301,0.0645,-0.0005,0.0394,-0.0296,-0.0274,0.0334,-0.0165,-0.0103,-0.0036,-0.0076,-0.0146,0.0477,-0.0003,-0.0322,0.0214,0.0456,-0.0745,0.0934,-0.0447,0.0501,0.0352,0.2088,-0.0558,0.0234,0.0287,-0.0328,-0.0294,0.0141,-0.0079,0.0538,0.0082,0.1,-0.0612,0.0182,0.0658,-0.0075,0.0579,0.0394,-0.0067,0.0092,0.0014,-0.0314,0.0055,0.0591,-0.0042,0.0006,-0.0066,0.0146,0.0378,-0.0244,-0.001,-0.0401,-0.0332,0.0346,0.0462,-0.0467,-0.0482,-0.0479,-0.0271,0.0665,-0.079,-0.0085,0.0179,-0.0311]}
{"key":"[LIMO: Latent Inceptionism for Targeted Molecule Generation] Generation of drug-like molecules with high binding affinity to target proteins remains a difficult and resource-intensive task in drug discovery. Existing approaches primarily employ reinforcement learning, Markov sampling, or deep generative models guided by Gaussian processes, which can be prohibitively slow when generating molecules with high binding affinity calculated by computationally-expensive physics-based methods. We present Latent Inceptionism on Molecules (LIMO), which significantly accelerates molecule generation with an inceptionism-like technique. LIMO employs a variational autoencoder-generated latent space and property prediction by two neural networks in sequence to enable faster gradient-based reverse-optimization of molecular properties. Comprehensive experiments show that LIMO performs competitively on benchmark tasks and markedly outperforms state-of-the-art techniques on the novel task of generating drug-like compounds with high binding affinity, reaching nanomolar range against two protein targets. We corroborate these docking-based results with more accurate molecular dynamics-based calculations of absolute binding free energy and show that one of our generated drug-like compounds has a predicted $K_D$ (a measure of binding affinity) of $6 \\cdot 10^{-14}$ M against the human estrogen receptor, well beyond the affinities of typical early-stage drug candidates and most FDA-approved drugs to their respective targets. Code is available at https://github.com/Rose-STL-Lab/LIMO.","layer":2,"vector":[-0.0634,0.0142,0.0151,-0.0038,0.0062,0.0362,0.0441,0.0271,0.0074,0.0117,-0.0085,-0.0278,0.0379,0.0821,0.0159,0.0207,-0.0069,0.0337,-0.0466,-0.0132,0.0289,-0.0299,-0.02,-0.0506,-0.0115,0.0272,-0.02,-0.0355,-0.0369,-0.236,0.0256,-0.0347,0.0577,-0.0372,0.0087,-0.0134,-0.0455,0.0564,-0.0469,-0.0084,0.0478,0.0365,-0.0286,-0.0414,-0.0132,-0.0631,-0.0523,0.0172,-0.0034,-0.0192,0.0322,-0.0624,0.0095,0.0529,0.0514,0.0337,0.0195,0.0496,0.0162,0.013,0.0217,0.0647,-0.1289,0.0291,0.0405,0.0005,-0.032,-0.0332,0.0627,0.0771,-0.0141,0.0307,0.0194,0.0188,0.0634,0.0184,-0.006,-0.021,0.0286,-0.0146,0.0016,-0.0215,-0.0638,-0.0613,-0.0379,0.0138,-0.0219,-0.0063,0.0621,0.0275,-0.0211,-0.0498,-0.0307,0.018,-0.0783,0.0001,0.0351,-0.0213,-0.0595,0.1914,-0.0445,0.0468,-0.0583,-0.0352,0.0225,-0.0774,-0.0428,-0.0275,0.0182,-0.0143,-0.0002,-0.0132,0.0793,-0.0849,0.0116,0.0037,0.0533,0.0401,-0.0157,-0.0125,-0.0491,-0.0002,0.0555,0.0214,0.0089,-0.0372,-0.0442,0.152,0.0455,0.0555,0.0436,0.0293,-0.05,0.0058,0.0363,-0.0116,-0.0426,-0.0212,-0.0206,-0.0077,-0.0359,0.0121,-0.0333,-0.1007,-0.0371,0.1097,-0.0094,0.0471,-0.0233,0.0304,0.0292,0.0308,-0.0416,-0.0136,0.0256,0.0567,0.0035,0.0287,-0.0586,0.0261,-0.068,-0.0031,-0.0661,0.0976,-0.0429,-0.0593,-0.0551,0.0193,-0.0123,0.0271,0.0437,0.0351,-0.0571,0.0192,0.0608,0.0194,-0.0589,0.0242,0.021,0.0152,0.0154,-0.0603,-0.0159,0.0488,0.0274,-0.0809,-0.0069,-0.0246,-0.0112,0.0634,-0.0069,0.0755,-0.0313,0.0328,-0.0175,-0.0257,-0.0144,0.0116,-0.0063,-0.0326,0.0618,-0.0116,-0.0408,0.0561,0.007,0.0428,0.0114,0.0132,0.057,0.0304,-0.0441,0.0307,0.0347,-0.0396,-0.0775,-0.0074,-0.0222,0.0001,-0.0055,0.0408,0.0522,-0.0242,-0.026,-0.2383,0.0603,-0.0093,-0.0402,0.0545,-0.0427,0.0022,-0.0255,-0.0197,0.0278,0.0321,0.0021,-0.0069,0.0059,-0.0595,0.0483,0.0035,0.027,0.0349,-0.0212,0.0103,-0.0123,-0.0368,-0.0669,0.0282,-0.0063,0.2109,0.0815,0.0178,-0.0159,0.0446,0.0314,-0.0282,-0.1085,0.0465,-0.0124,0.1203,-0.0389,-0.0214,-0.0144,-0.0589,0.002,0.0053,-0.0726,-0.0433,-0.0144,-0.0373,0.0277,-0.0202,0.0585,0.0713,-0.0118,0.0496,-0.0393,-0.0024,-0.0612,-0.0945,0.0594,-0.0629,0.0468,0.0322,-0.0264,0.0071,-0.0302,0.0195,-0.0312,-0.0255,-0.0355,0.0285,-0.0443,-0.008,0.082,-0.0265,0.0395,0.105,-0.0058,0.0181,-0.0253,-0.0819,0.0235,0.0126,-0.0211,0.0234,-0.0193,0.0028,-0.0342,0.0391,0.0056,0.0355,0.0009,-0.0302,-0.0011,-0.0505,-0.0026,0.0379,-0.0073,-0.2963,0.068,-0.0101,0.0549,-0.0058,-0.0206,0.0676,-0.0022,-0.0506,0.0076,-0.0344,0.0384,0.0441,0.0022,-0.0246,-0.0081,0.0996,-0.0576,0.0546,-0.0502,0.0385,0.0355,0.25,-0.0481,0.0328,0.0363,0.0152,0.0406,0.0282,0.0032,-0.0327,0.0372,0.0633,-0.0629,0.0129,0.1062,-0.0564,0.0177,0.0411,-0.0337,-0.0085,0.0334,-0.0186,-0.0038,0.077,-0.0427,-0.0243,-0.0079,-0.0134,0.0487,-0.0414,-0.0012,-0.0419,0.0024,0.0348,0.0183,-0.0384,-0.0626,-0.0263,-0.0325,-0.0193,-0.0467,-0.0387,0.0633,0.0038]}
{"key":"[Object-centric Process Predictive Analytics] Object-centric processes (a.k.a. Artifact-centric processes) are implementations of a paradigm where an instance of one process is not executed in isolation but interacts with other instances of the same or other processes. Interactions take place through bridging events where instances exchange data. Object-centric processes are recently gaining popularity in academia and industry, because their nature is observed in many application scenarios. This poses significant challenges in predictive analytics due to the complex intricacy of the process instances that relate to each other via many-to-many associations. Existing research is unable to directly exploit the benefits of these interactions, thus limiting the prediction quality. This paper proposes an approach to incorporate the information about the object interactions into the predictive models. The approach is assessed on real-life object-centric process event data, using different KPIs. The results are compared with a naive approach that overlooks the object interactions, thus illustrating the benefits of their use on the prediction quality.","layer":3,"vector":[-0.017,0.0015,0.0302,-0.0479,0.0428,-0.0353,0.0754,0.0294,0.0304,0.0231,0.0042,-0.032,0.0146,0.008,0.006,-0.0044,-0.0128,0.0836,-0.0253,-0.0057,0.0235,-0.009,-0.0313,-0.0583,0.0081,0.0809,-0.0106,-0.0232,-0.0591,-0.2296,-0.0111,-0.0208,0.0778,-0.004,0.0042,-0.0202,0.0032,0.0765,-0.0265,0.0301,0.0456,-0.0039,-0.049,-0.0444,-0.0307,-0.0369,0.0047,-0.0284,-0.0356,0.0031,0.0454,-0.0411,-0.0059,0.0161,0.0526,0.0531,0.0693,0.0247,0.0487,-0.0185,0.021,-0.0271,-0.1314,0.0611,0.0366,0.0277,-0.0375,-0.027,-0.0125,0.0096,-0.0315,0.037,0.026,0.0607,0.0214,-0.0345,-0.0389,-0.034,-0.0514,0.0357,0.0057,0.0,-0.0439,-0.0197,-0.0523,-0.05,0.0365,0.0035,0.0817,0.0127,-0.0697,-0.0511,0.0104,-0.0054,-0.0262,0.0126,0.0262,0.037,-0.0133,0.2016,-0.0568,0.0199,0.0387,0.0088,0.0281,-0.0222,-0.036,-0.0363,0.0028,0.0362,-0.018,0.0084,0.0111,-0.0811,0.0519,-0.0048,0.0773,0.0363,-0.0176,0.0299,0.0217,-0.0068,0.0726,0.0174,-0.0044,-0.0447,0.0155,0.1437,0.0104,-0.047,0.0309,-0.0171,-0.0918,-0.0424,0.0053,0.0081,0.0428,-0.0088,0.0,0.0214,-0.0785,0.0108,0.0054,-0.081,-0.0425,0.1628,-0.0251,0.0231,-0.0256,-0.022,-0.0534,0.0532,-0.0277,-0.0335,0.0104,-0.0148,-0.0064,0.0422,-0.0409,0.0741,-0.0337,-0.0266,-0.0461,0.0775,0.0025,-0.0954,-0.0315,0.0162,0.0399,-0.022,0.0414,0.0299,-0.0279,-0.0097,0.0798,0.0356,-0.0632,-0.012,-0.0346,-0.0065,0.0858,-0.0341,-0.0664,0.0955,0.0412,-0.0595,0.0201,-0.043,0.0124,0.0345,-0.0299,0.0262,-0.0321,0.0418,0.0066,0.0036,0.0024,-0.0295,0.0137,-0.047,-0.0116,0.0388,-0.0773,0.0415,-0.0064,0.0385,-0.0477,0.0139,0.0217,0.023,-0.0471,-0.0505,0.0542,-0.0232,-0.0434,0.0117,0.0169,0.0521,0.023,0.0623,0.0435,-0.0031,-0.043,-0.2338,0.0217,0.0103,0.0146,0.0131,-0.043,0.0135,-0.0085,-0.0102,0.0556,0.0583,-0.0267,-0.0243,0.0191,-0.0091,0.0498,-0.0137,0.0049,-0.0421,0.0244,-0.0347,-0.017,-0.0159,-0.1119,0.0236,0.0234,0.1974,0.0362,0.0007,-0.0684,0.0222,0.0245,-0.0461,-0.0938,0.0615,0.0761,0.0741,-0.012,0.0003,-0.0156,-0.0753,0.0437,0.0015,-0.1001,-0.0318,-0.0228,-0.009,0.0065,-0.0605,0.0302,0.023,-0.0395,0.0737,0.0186,-0.0037,-0.0464,-0.0506,0.0702,-0.0315,0.0259,0.0306,-0.0649,0.024,-0.0418,0.0202,-0.0039,0.0127,-0.0771,-0.0113,-0.0196,-0.0454,0.1299,-0.0407,-0.061,0.0375,0.0191,0.003,-0.0479,-0.024,-0.0139,0.0671,-0.0767,0.0419,0.0448,0.0253,-0.0005,0.0772,-0.0225,0.0127,-0.0709,-0.011,-0.0555,-0.0208,-0.0178,0.0355,-0.0146,-0.3054,0.0197,-0.0009,0.0553,0.0101,0.0154,0.0329,0.0231,-0.0176,0.0305,0.0372,0.0015,0.0314,-0.0146,-0.0004,0.0548,0.0588,-0.0506,0.0033,-0.0548,0.05,0.0722,0.2247,0.0115,0.0014,-0.0117,-0.0327,-0.0031,0.0021,-0.0205,0.006,0.0254,0.0701,-0.0439,0.0428,0.0462,-0.0361,0.059,0.0361,-0.0242,0.0328,-0.0099,-0.0417,-0.0393,0.1066,0.0055,-0.0395,-0.0702,-0.0154,0.0235,-0.0417,-0.0134,-0.0346,0.0027,0.019,0.0256,-0.0512,0.0132,-0.0531,-0.0219,0.0543,-0.0155,0.007,0.0327,-0.0197]}
{"key":"[Fast Generating A Large Number of Gumbel-Max Variables] The well-known Gumbel-Max Trick for sampling elements from a categorical distribution (or more generally a nonnegative vector) and its variants have been widely used in areas such as machine learning and information retrieval. To sample a random element $i$ (or a Gumbel-Max variable $i$) in proportion to its positive weight $v_i$, the Gumbel-Max Trick first computes a Gumbel random variable $g_i$ for each positive weight element $i$, and then samples the element $i$ with the largest value of $g_i+\\ln v_i$. Recently, applications including similarity estimation and graph embedding require to generate $k$ independent Gumbel-Max variables from high dimensional vectors. However, it is computationally expensive for a large $k$ (e.g., hundreds or even thousands) when using the traditional Gumbel-Max Trick. To solve this problem, we propose a novel algorithm, \\emph{FastGM}, that reduces the time complexity from $O(kn^+)$ to $O(k \\ln k + n^+)$, where $n^+$ is the number of positive elements in the vector of interest. Instead of computing $k$ independent Gumbel random variables directly, we find that there exists a technique to generate these variables in descending order. Using this technique, our method FastGM computes variables $g_i+\\ln v_i$ for all positive elements $i$ in descending order. As a result, FastGM significantly reduces the computation time because we can stop the procedure of Gumbel random variables computing for many elements especially for those with small weights. Experiments on a variety of real-world datasets show that FastGM is orders of magnitude faster than state-of-the-art methods without sacrificing accuracy and incurring additional expenses.","layer":5,"vector":[-0.051,-0.0245,-0.0158,0.0082,0.0441,-0.0056,0.0307,0.0415,0.0598,-0.0146,0.0211,-0.0497,0.0569,0.0518,0.0385,0.0306,0.0179,0.0321,-0.0691,0.001,0.029,-0.0662,-0.0411,-0.0807,0.0645,0.0048,-0.0516,-0.0122,-0.0124,-0.2687,-0.0049,-0.0435,0.0974,-0.0417,-0.0075,-0.0357,-0.0612,0.0484,-0.0693,0.0559,0.0515,0.0349,-0.0407,-0.0478,-0.0179,-0.0285,-0.0362,-0.0132,0.0111,-0.0091,0.0008,-0.0323,0.03,0.0356,0.0679,0.0293,0.0228,0.0015,0.0051,0.0274,0.0279,0.0595,-0.1649,0.0351,0.008,-0.0131,-0.0533,-0.0288,0.0207,0.0524,-0.0083,0.0665,0.0083,0.0721,-0.0019,-0.0087,0.0339,-0.0229,-0.0469,-0.0101,-0.0107,-0.0273,-0.0415,0.0117,0.0005,-0.0219,0.0244,-0.041,0.0249,-0.0005,-0.0011,-0.0228,0.0007,0.0203,-0.0416,-0.0291,0.0456,0.0085,-0.01,0.24,-0.0475,0.0276,0.0615,-0.0277,0.0533,-0.0346,-0.0256,-0.0293,-0.0188,-0.02,-0.0433,-0.053,0.0008,-0.047,-0.0016,-0.005,0.0898,0.0286,-0.0152,-0.0077,-0.0315,0.0003,0.0154,-0.0065,0.0402,-0.0503,-0.008,0.1109,0.0095,0.0579,0.048,0.0241,-0.0693,-0.0184,-0.0302,0.009,0.0402,0.0113,-0.021,-0.0359,-0.0061,-0.0227,0.0064,-0.0722,-0.0483,0.1028,-0.0628,0.0176,-0.0679,-0.0298,0.0038,-0.0112,-0.0368,-0.0358,0.0216,0.0498,0.0826,0.0758,-0.0168,-0.0054,0.003,-0.07,0.015,0.1092,0.0361,-0.0857,-0.0035,0.0026,-0.0071,-0.0078,0.0268,0.0349,-0.066,0.0481,0.0989,-0.0279,-0.0522,0.0074,0.0781,0.0403,-0.0109,-0.0094,-0.0439,0.0645,0.0072,-0.0413,0.041,-0.01,0.0165,-0.0054,-0.0244,0.0044,-0.013,-0.0065,-0.0128,-0.025,-0.0117,-0.0039,0.0525,-0.0053,-0.0154,-0.0204,-0.0485,0.0411,-0.0047,0.0218,0.0114,-0.0378,0.0251,0.0347,-0.0193,-0.0403,0.0571,-0.0174,-0.0127,0.0038,0.0167,0.0451,0.0103,0.0396,0.0256,-0.1152,-0.0481,-0.2245,-0.0615,0.021,-0.0045,0.0563,-0.0597,0.0412,-0.0553,0.0709,0.0546,0.0638,-0.0228,-0.0735,0.076,-0.0081,0.0616,-0.0119,0.0228,-0.0041,0.0127,0.0009,0.021,-0.0175,-0.0427,0.0255,-0.0379,0.2583,0.0509,-0.0125,-0.0189,0.0042,0.0126,0.0012,-0.0662,0.0643,0.0162,-0.0057,0.007,-0.0176,0.0091,-0.0394,0.0224,0.0209,-0.0973,-0.0279,-0.0532,-0.0089,0.0226,-0.025,0.0398,0.0406,-0.0272,0.0943,-0.0229,0.0129,-0.1008,-0.0715,0.0297,-0.0763,-0.0006,0.026,-0.1094,0.0058,-0.0523,0.02,-0.0434,-0.0243,-0.0173,0.0275,0.011,-0.0057,0.0638,-0.0123,-0.0106,0.0403,0.0102,0.0294,-0.0246,-0.0405,-0.039,0.0433,-0.0044,0.0418,-0.0077,0.0221,0.0181,0.0754,0.0218,0.0394,0.0194,0.0119,-0.0181,-0.0332,-0.0353,0.0264,0.006,-0.269,0.0316,0.0103,0.0195,-0.0122,0.0023,0.0523,-0.0074,-0.0009,0.0126,0.0316,0.0476,0.0423,-0.0138,-0.0032,0.0539,0.075,-0.0296,0.0575,-0.0357,0.0144,-0.0074,0.2176,-0.0237,0.0084,0.0174,-0.0268,-0.0198,0.0341,-0.0429,0.0031,-0.0196,0.1013,-0.0753,0.0061,0.0886,-0.0309,0.034,0.0234,-0.0547,-0.0002,-0.0063,-0.0746,-0.0158,0.1151,-0.0243,-0.0105,-0.0436,0.0312,0.0335,-0.0267,-0.0089,-0.0437,0.0164,0.0234,0.0346,-0.0035,-0.0071,-0.0218,-0.0434,-0.0303,-0.0461,-0.0149,-0.0084,0.0066]}
{"key":"[IRIS: Implicit Reinforcement without Interaction at Scale for Learning Control from Offline Robot Manipulation Data] Learning from offline task demonstrations is a problem of great interest in robotics. For simple short-horizon manipulation tasks with modest variation in task instances, offline learning from a small set of demonstrations can produce controllers that successfully solve the task. However, leveraging a fixed batch of data can be problematic for larger datasets and longer-horizon tasks with greater variations. The data can exhibit substantial diversity and consist of suboptimal solution approaches. In this paper, we propose Implicit Reinforcement without Interaction at Scale (IRIS), a novel framework for learning from large-scale demonstration datasets. IRIS factorizes the control problem into a goal-conditioned low-level controller that imitates short demonstration sequences and a high-level goal selection mechanism that sets goals for the low-level and selectively combines parts of suboptimal solutions leading to more successful task completions. We evaluate IRIS across three datasets, including the RoboTurk Cans dataset collected by humans via crowdsourcing, and show that performant policies can be learned from purely offline learning. Additional results at https://sites.google.com/stanford.edu/iris/ .","layer":0,"vector":[-0.0485,-0.0267,0.0392,-0.0173,-0.0092,0.0105,0.0261,0.0586,-0.0135,0.0268,0.0534,-0.0072,0.0406,0.0862,-0.0169,0.0324,-0.02,0.0578,-0.0647,0.0567,0.0275,-0.0769,-0.0222,-0.0445,0.0147,0.0253,-0.0518,-0.0862,-0.0166,-0.2283,0.005,-0.0567,0.0556,-0.0104,0.0106,0.0073,-0.0281,0.0452,-0.0282,-0.0131,0.0248,0.0109,-0.0105,-0.0405,-0.0434,-0.0221,0.0056,-0.0529,-0.0057,0.0014,0.0006,-0.0326,0.0086,0.017,0.0806,0.0304,0.046,0.0505,0.0761,0.0241,0.0334,0.0224,-0.12,0.0711,0.0194,0.0638,-0.0296,-0.0286,0.0423,0.0218,-0.0302,0.0477,0.0374,0.0636,0.0245,-0.0436,-0.0365,-0.0293,-0.0119,-0.0198,0.0152,-0.0411,-0.0354,-0.0173,-0.053,-0.0496,0.0207,-0.0491,0.0498,0.0195,-0.031,0.0058,-0.0325,0.0282,-0.0563,-0.0586,0.0444,0.0038,-0.061,0.2175,-0.0438,0.0413,-0.012,0.003,0.0069,-0.065,-0.0214,-0.0229,-0.0102,-0.0124,-0.0401,0.0177,0.0159,-0.041,0.0437,0.0137,0.0742,0.0306,-0.0031,-0.0017,0.014,0.0065,0.0481,-0.018,0.0277,-0.055,-0.0035,0.1398,0.0294,-0.0095,0.0535,-0.0726,-0.045,-0.0227,0.0173,0.0104,0.042,0.0017,0.0386,0.0094,-0.0504,-0.0183,0.0187,-0.1033,-0.0199,0.1058,0.0074,0.0441,-0.0238,-0.022,-0.0295,0.0254,-0.0031,-0.0151,0.0095,0.0307,0.0402,0.0487,-0.0565,0.0365,-0.0632,-0.0255,-0.0082,0.0585,-0.0375,-0.1218,-0.0484,-0.0004,0.0148,0.0132,-0.0205,0.0037,-0.0537,0.0233,0.1086,0.0249,-0.1044,0.0028,0.0035,0.0264,0.0186,-0.0966,0.0075,0.0266,0.0092,-0.0124,0.0064,-0.0246,0.0369,0.0348,-0.0475,0.0599,-0.0297,-0.0094,-0.0183,-0.0344,0.0156,-0.0167,0.0064,0.0059,-0.0416,-0.0075,-0.0124,-0.0152,-0.0359,0.052,-0.0163,0.0049,0.0631,0.0342,-0.0334,0.0313,0.0219,-0.0302,-0.0688,0.0101,0.0287,0.0059,0.0023,0.0185,0.0253,0.0066,-0.0187,-0.2198,0.0001,0.0087,-0.0418,0.0769,-0.0691,0.0314,-0.0476,0.0316,0.047,0.0724,-0.0612,-0.0357,0.0431,-0.0468,0.0733,0.0091,-0.0173,-0.0029,-0.0301,-0.0059,0.0041,-0.0027,-0.0842,0.0342,-0.012,0.2204,0.0261,0.0262,-0.0258,-0.0006,0.0461,-0.0538,-0.0961,0.0693,0.0199,0.0921,-0.0632,0.0171,0.0045,-0.0262,0.0239,-0.0174,-0.1103,-0.0239,-0.03,-0.0399,0.0352,-0.0603,-0.0057,0.046,-0.0235,0.0421,-0.0397,-0.075,-0.0087,-0.082,0.065,-0.0289,0.0431,0.0478,-0.0275,-0.0023,-0.0701,0.0405,0.0488,0.014,-0.0341,0.0539,-0.0273,-0.0308,0.0535,0.0408,0.0491,0.0228,0.0144,0.0181,-0.0165,-0.0532,-0.0282,0.0632,-0.0073,0.0044,0.0467,0.0222,-0.0318,0.0913,-0.0255,0.0611,-0.0168,0.0462,0.0185,-0.0637,0.0132,0.084,-0.0036,-0.299,0.0458,0.0441,0.0671,-0.0298,0.0253,0.0634,-0.0366,-0.0584,-0.0117,-0.0279,0.0662,0.0269,0.0285,0.0271,0.0383,0.0781,-0.0006,0.0329,-0.0759,0.019,0.0687,0.2014,-0.0326,0.028,-0.009,-0.0482,-0.0375,0.0719,-0.0402,-0.0164,0.0074,0.0404,-0.0867,0.0262,0.0666,-0.0449,0.0263,0.0222,0.0062,-0.08,0.0175,0.0232,-0.0305,0.0834,0.0125,-0.0273,-0.0529,-0.0304,0.0533,-0.0088,-0.0012,-0.0091,-0.0233,0.0278,0.0383,-0.0185,-0.0477,-0.0294,-0.0548,-0.0079,-0.0349,0.008,-0.0282,-0.0139]}
{"key":"[Deep Multi-instance Networks with Sparse Label Assignment for Whole Mammogram Classification] Mammogram classification is directly related to computer-aided diagnosis of breast cancer. Traditional methods rely on regions of interest (ROIs) which require great efforts to annotate. Inspired by the success of using deep convolutional features for natural image analysis and multi-instance learning (MIL) for labeling a set of instances/patches, we propose end-to-end trained deep multi-instance networks for mass classification based on whole mammogram without the aforementioned ROIs. We explore three different schemes to construct deep multi-instance networks for whole mammogram classification. Experimental results on the INbreast dataset demonstrate the robustness of proposed networks compared to previous work using segmentation and detection annotations.","layer":1,"vector":[-0.0038,-0.0295,-0.0045,0.0086,0.0263,0.0133,0.0582,0.0171,0.0118,-0.0376,0.0076,-0.0873,0.0298,0.0223,0.0337,0.0102,0.0413,0.0439,-0.0399,0.0213,0.0144,-0.0188,-0.0203,-0.0324,0.0479,-0.0077,0.0015,-0.032,-0.0635,-0.2444,0.0494,-0.0148,0.0419,-0.0105,0.0286,-0.0428,-0.0061,0.0189,-0.0547,0.0429,0.0354,-0.0043,-0.0298,-0.0564,0.0132,-0.05,-0.0255,-0.044,0.0197,-0.005,0.0526,-0.0464,0.0145,0.0692,0.0099,0.0184,0.0515,0.0386,0.0445,0.034,0.0743,0.0422,-0.1593,0.0393,0.0437,0.0496,-0.0528,-0.0429,0.0271,0.0492,-0.0002,0.0527,0.0451,0.0374,0.0048,-0.0146,0.0249,0.0197,-0.0271,0.0348,-0.0149,0.0149,-0.0078,-0.0685,0.0356,-0.0441,0.0178,-0.0691,0.0348,-0.018,-0.0369,-0.0387,-0.0188,0.0336,-0.0402,-0.0213,0.0269,0.0516,-0.0414,0.1817,-0.0876,0.0016,0.0434,-0.0342,0.0258,0.0164,-0.0333,-0.0225,-0.0118,0.0262,0.0078,-0.0174,0.0052,-0.0429,0.0022,-0.0134,0.0717,0.0126,0.0137,-0.0079,-0.0246,-0.0088,0.063,-0.0205,0.0156,-0.0298,0.0238,0.1557,0.0548,-0.0103,0.0273,0.0003,-0.0527,-0.0053,-0.0034,-0.0295,0.053,-0.0089,0.0327,0.0173,-0.0614,-0.088,0.0426,-0.06,-0.0398,0.0922,-0.0495,0.0554,-0.0354,-0.0488,-0.0246,0.0073,-0.068,-0.0123,0.0473,0.0073,0.0298,0.0388,-0.0743,-0.0042,0.0093,-0.0643,-0.0224,0.1071,0.0425,-0.1014,-0.0306,-0.0342,-0.0099,-0.0277,0.0674,0.0276,-0.0205,0.0182,0.0659,0.0155,-0.0816,-0.0211,-0.0094,0.0265,-0.0076,-0.0384,-0.0429,0.001,0.0381,-0.0521,-0.0195,-0.0864,0.0252,0.0243,-0.0332,0.0749,-0.0147,0.0324,-0.0361,-0.0722,0.0174,0.0043,-0.0094,-0.0207,-0.0099,-0.0161,-0.0001,0.033,0.0266,0.0282,-0.0339,0.0359,0.0287,-0.0242,-0.0453,0.0234,0.0384,-0.0279,-0.0018,-0.0376,0.0066,0.0324,-0.0231,0.0573,0.0514,-0.0375,-0.0782,-0.2137,-0.0138,0.0498,-0.0335,-0.0159,-0.0924,0.0472,0.0248,0.0391,0.0597,0.0728,-0.0182,-0.0299,0.0089,0.0029,0.0234,0.0295,0.0401,0.002,-0.0346,0.0407,0.084,-0.0037,-0.0715,0.0781,0.025,0.2408,0.0362,0.0295,0.0149,0.0253,0.0606,-0.0598,-0.1091,0.0101,-0.0253,0.0541,0.0118,-0.056,0.0129,-0.0117,0.0014,0.007,-0.1009,-0.0429,-0.0339,-0.0325,0.0387,-0.0508,-0.0029,0.0228,-0.0656,0.0322,0.0149,0.0211,-0.0139,-0.1242,0.0257,-0.0248,0.0203,0.0103,-0.0747,-0.0028,-0.0955,0.0527,0.0047,-0.0656,-0.0393,-0.0063,-0.0472,-0.034,0.0827,-0.0118,-0.0444,0.0455,0.0237,0.0271,0.0082,-0.064,-0.0111,0.0958,-0.0188,0.0238,0.0135,0.0133,0.0518,0.1226,0.0142,0.0473,-0.0271,0.0426,0.0408,-0.0229,-0.0092,-0.007,-0.0128,-0.2778,0.0797,-0.0057,0.0601,-0.0427,0.0359,0.0484,0.0512,0.0153,-0.0076,0.0016,-0.0056,0.0649,-0.0354,0.0005,0.0379,0.0413,-0.0554,0.0676,-0.0819,0.0432,0.0135,0.1696,-0.0548,0.003,0.0085,-0.06,-0.0067,-0.0015,0.0265,0.0405,0.0301,0.0692,-0.0534,0.0417,0.1096,-0.0257,0.0266,0.0056,-0.0198,0.0055,0.005,-0.0575,-0.0467,0.0949,-0.0214,-0.0299,-0.0122,-0.0259,0.009,-0.005,-0.0027,-0.0285,-0.0031,-0.0046,0.0017,-0.0238,-0.0514,-0.0694,-0.0296,0.0516,-0.0718,-0.0253,0.0197,-0.0258]}
{"key":"[Surrogate Regret Bounds for Bipartite Ranking via Strongly Proper Losses] The problem of bipartite ranking, where instances are labeled positive or negative and the goal is to learn a scoring function that minimizes the probability of mis-ranking a pair of positive and negative instances (or equivalently, that maximizes the area under the ROC curve), has been widely studied in recent years. A dominant theoretical and algorithmic framework for the problem has been to reduce bipartite ranking to pairwise classification; in particular, it is well known that the bipartite ranking regret can be formulated as a pairwise classification regret, which in turn can be upper bounded using usual regret bounds for classification problems. Recently, Kotlowski et al. (2011) showed regret bounds for bipartite ranking in terms of the regret associated with balanced versions of the standard (non-pairwise) logistic and exponential losses. In this paper, we show that such (non-pairwise) surrogate regret bounds for bipartite ranking can be obtained in terms of a broad class of proper (composite) losses that we term as strongly proper. Our proof technique is much simpler than that of Kotlowski et al. (2011), and relies on properties of proper (composite) losses as elucidated recently by Reid and Williamson (2010, 2011) and others. Our result yields explicit surrogate bounds (with no hidden balancing terms) in terms of a variety of strongly proper losses, including for example logistic, exponential, squared and squared hinge losses as special cases. We also obtain tighter surrogate bounds under certain low-noise conditions via a recent result of Clemencon and Robbiano (2011).","layer":0,"vector":[-0.0316,-0.0074,0.0208,-0.0225,-0.0087,-0.0091,0.0568,0.0425,0.0418,0.0079,0.0102,-0.0451,0.0309,0.0701,0.011,0.0291,0.0172,0.063,-0.0549,0.037,0.0112,-0.0354,-0.0167,-0.0825,0.0198,0.0062,-0.0531,-0.0544,-0.0422,-0.2611,0.0286,-0.0441,0.0394,-0.0545,-0.0201,-0.0262,-0.0364,0.0735,-0.0529,0.0344,0.0434,0.0533,0.0026,-0.0295,-0.0311,-0.0265,0.0195,-0.0235,-0.0299,-0.047,0.0418,-0.0544,-0.0063,0.0356,0.0448,0.0264,0.0224,0.0789,-0.0061,0.059,0.0167,0.0451,-0.1422,-0.009,0.0418,0.0215,-0.0707,-0.028,-0.0135,0.0771,0.0022,0.0112,0.0171,0.0184,-0.013,-0.0035,0.0014,-0.0373,-0.0455,0.0176,0.0177,0.0053,-0.0382,0.0187,0.0164,-0.0844,0.0253,-0.0219,0.0507,-0.0193,0.0099,-0.0085,-0.0355,0.0172,-0.0741,-0.0005,0.0261,0.0728,-0.0722,0.2022,-0.0729,0.0446,-0.0078,-0.0427,0.0007,-0.0656,-0.0342,-0.0502,-0.0335,-0.0514,0.0031,0.0185,0.0608,-0.059,0.0244,0.0502,0.0741,0.0928,0.0026,-0.0274,-0.0334,0.0259,0.1017,-0.009,0.0213,-0.0469,0.0064,0.122,0.0283,0.0404,0.0248,-0.0606,-0.0512,-0.0036,0.0022,0.0409,-0.0247,0.0259,0.0412,-0.0151,-0.0129,-0.0868,0.0466,-0.0572,-0.0368,0.1626,-0.0342,0.0406,-0.0333,-0.0515,-0.006,-0.0189,-0.0201,-0.0085,0.0013,0.0394,0.0236,0.0578,-0.0534,0.0357,0.0067,-0.0421,0.0092,0.1019,0.045,-0.0641,-0.0195,-0.0228,-0.0277,0.0095,0.03,0.0087,-0.0247,0.0328,0.0432,0.0063,-0.0598,-0.0333,0.0301,-0.0002,0.0014,-0.0565,-0.0227,0.023,0.0529,0.0038,-0.0399,-0.0336,0.0482,0.0325,-0.0432,-0.0137,-0.0277,-0.0053,0.0225,-0.0228,0.0044,-0.0064,0.0285,0.014,-0.0001,0.0033,-0.047,0.016,0.0126,0.0338,-0.0029,0.0172,0.0466,0.0147,-0.0088,-0.0276,0.0516,-0.0138,-0.0137,-0.0387,0.0387,0.0566,0.0039,0.0341,0.0345,-0.0189,-0.0176,-0.2129,-0.0038,0.0207,0.0142,0.0381,-0.0716,0.063,-0.0388,0.0254,0.091,0.0794,-0.0227,-0.0433,0.045,-0.0124,0.0608,0.0384,-0.0145,-0.0362,0.0206,-0.0458,0.05,-0.013,-0.0699,0.0654,0.016,0.2181,0.0327,0.0239,-0.0124,0.0037,0.0297,-0.0066,-0.0595,0.0496,0.0189,0.0277,-0.0377,-0.0886,-0.0185,-0.0216,-0.0245,0.0199,-0.0986,-0.03,-0.0261,-0.0424,0.043,-0.0578,0.03,0.048,-0.0431,0.058,-0.0418,0.0208,-0.0356,-0.0873,-0.0079,-0.0515,0.0145,0.0249,-0.0814,0.0256,-0.1043,0.0508,-0.0172,-0.0153,0.0075,0.009,-0.0525,-0.0492,0.0672,-0.0083,-0.0175,0.0356,0.0205,0.0497,-0.0343,0.0065,-0.0424,0.0715,-0.0283,0.0251,-0.0215,0.0067,0.0112,0.1102,-0.0106,0.0258,0.0116,0.0005,0.0256,-0.0656,0.0097,0.0352,0.0213,-0.3027,0.0447,0.005,0.0084,-0.0161,-0.0317,0.0282,0.0033,-0.0678,0.0093,0.0275,0.0543,0.062,-0.0277,0.0134,0.0432,-0.001,-0.0523,0.0433,-0.0306,0.0164,0.0283,0.1988,-0.0076,0.0524,0.0048,-0.041,0.0044,-0.0038,-0.0458,0.0451,0.0034,0.07,-0.0727,0.0475,0.0966,-0.0114,0.0483,0.0064,-0.029,-0.0059,-0.0032,-0.0675,-0.0386,0.1469,-0.0295,-0.0534,-0.0464,-0.0115,-0.0122,-0.0332,0.0154,0.0119,-0.0277,-0.0014,-0.0036,-0.0233,-0.0475,-0.0328,-0.0342,0.031,-0.0003,-0.0212,0.0184,-0.02]}
{"key":"[A Similarity Measure for Material Appearance] We present a model to measure the similarity in appearance between different materials, which correlates with human similarity judgments. We first create a database of 9,000 rendered images depicting objects with varying materials, shape and illumination. We then gather data on perceived similarity from crowdsourced experiments; our analysis of over 114,840 answers suggests that indeed a shared perception of appearance similarity exists. We feed this data to a deep learning architecture with a novel loss function, which learns a feature space for materials that correlates with such perceived appearance similarity. Our evaluation shows that our model outperforms existing metrics. Last, we demonstrate several applications enabled by our metric, including appearance-based search for material suggestions, database visualization, clustering and summarization, and gamut mapping.","layer":1,"vector":[-0.0381,-0.0039,-0.0032,0.0333,0.046,0.025,0.0527,0.0091,0.0108,-0.0431,0.02,-0.0606,0.0347,0.0829,0.0455,-0.0287,0.025,0.031,-0.0648,0.0258,0.0424,-0.0305,-0.0212,-0.062,0.0017,0.0138,-0.0479,-0.0086,-0.0455,-0.2286,0.0147,-0.0532,0.0777,-0.0156,0.0222,-0.03,0.0115,0.0791,-0.0795,-0.017,0.0039,0.0056,-0.0563,-0.0319,-0.0495,-0.0378,-0.0292,-0.0186,0.0242,-0.0321,0.0498,-0.0512,-0.0103,0.011,0.0295,0.0607,0.0999,0.0511,0.0497,0.0261,0.051,0.035,-0.1379,0.0697,0.0548,0.0278,-0.0287,-0.0304,0.0074,0.0141,0.0082,0.0373,0.002,0.0655,0.014,0.0008,-0.0092,-0.0461,-0.0568,-0.0312,0.004,0.0152,-0.0381,-0.0368,-0.0111,-0.0052,-0.0193,0.0014,0.0273,-0.0012,-0.0648,0.0154,-0.0591,0.011,-0.0765,-0.0303,0.0235,0.008,-0.0212,0.2284,-0.0534,0.0733,0.0631,-0.0656,0.0238,-0.0083,0.0083,-0.0052,-0.022,0.0194,-0.0295,-0.0309,-0.0135,-0.0248,0.0168,-0.0091,0.0668,0.0394,-0.0057,-0.0515,-0.0002,0.0082,0.0463,-0.0295,0.0074,-0.0787,0.0308,0.1053,0.0142,0.01,0.0308,-0.007,-0.0558,-0.0394,0.0088,0.0514,0.0212,-0.0219,0.0016,0.0141,-0.0084,-0.0305,0.0066,-0.045,-0.0683,0.1202,-0.0331,0.0169,-0.066,-0.0236,-0.0262,0.0011,-0.0353,-0.0126,0.0273,0.0173,0.0554,0.0447,-0.0337,0.02,0.0172,-0.0504,0.0053,0.0607,-0.0162,-0.0945,-0.0533,0.0302,0.0229,-0.0214,0.0206,0.049,-0.0194,0.0566,0.1085,0.0348,-0.0482,-0.023,0.045,0.0455,0.0465,-0.0554,-0.0532,0.0493,0.039,-0.0484,-0.0211,-0.0351,0.0423,0.0708,0.0115,0.0192,-0.0059,0.034,0.0067,-0.0616,-0.0518,-0.0464,-0.0283,-0.0218,0.0144,0.0047,-0.0343,0.0125,-0.0199,0.0385,-0.0029,-0.0053,0.0339,0.0056,-0.0835,-0.0129,0.0015,0.0087,-0.0323,-0.0327,0.0205,0.0335,0.0198,0.0276,0.0503,-0.0613,-0.0818,-0.1998,0.0193,0.0187,0.0059,0.0652,-0.0437,0.0331,-0.0251,0.0219,0.029,0.0281,-0.0049,0.0087,0.0183,-0.03,0.0464,0.0229,0.0566,-0.0192,-0.0289,-0.0446,0.0372,-0.0213,-0.0933,0.0354,0.0042,0.2317,0.036,-0.0369,-0.0194,0.0061,0.0328,-0.0692,-0.1071,0.0176,0.0345,0.0996,-0.0132,-0.072,-0.024,-0.0712,0.0172,0.0205,-0.0748,0.0024,-0.0345,-0.0411,0.0235,-0.0858,0.0488,0.027,-0.034,0.0448,0.0285,-0.0291,-0.0525,-0.081,-0.001,-0.0233,0.0399,-0.0012,-0.0583,0.0031,-0.0617,0.0579,-0.0149,-0.0382,-0.0331,0.0227,-0.0336,-0.0133,0.0699,-0.022,-0.0271,0.0293,0.0446,0.0531,0.0284,-0.0147,-0.0294,0.0721,0.0409,-0.0071,0.0051,0.0622,0.0489,0.0898,-0.0568,0.0703,-0.0324,0.0324,-0.0075,-0.0672,-0.04,0.0298,-0.0106,-0.2901,0.0453,0.0032,0.0912,-0.0522,-0.0017,0.067,0.057,0.0277,-0.0666,0.0065,0.0454,0.0358,-0.0272,-0.006,0.0193,0.0812,-0.0706,0.0376,-0.0473,0.0056,0.0011,0.2337,-0.0475,0.0225,0.0088,0.0069,-0.0358,0.033,0.0057,0.017,0.0117,0.0739,-0.0453,-0.0076,0.1047,-0.0379,0.0388,-0.0033,-0.0263,0.0105,0.0068,-0.0621,0.0155,0.0838,0.0176,0.0032,-0.0304,-0.0102,-0.024,-0.0469,0.0356,-0.0173,-0.0202,0.031,0.0211,-0.0836,0.0072,0.0002,-0.0137,0.0134,-0.0348,-0.0402,0.024,0.0176]}
{"key":"[Concentration Inequalities for Statistical Inference] This paper gives a review of concentration inequalities which are widely employed in non-asymptotical analyses of mathematical statistics in a wide range of settings, from distribution-free to distribution-dependent, from sub-Gaussian to sub-exponential, sub-Gamma, and sub-Weibull random variables, and from the mean to the maximum concentration. This review provides results in these settings with some fresh new results. Given the increasing popularity of high-dimensional data and inference, results in the context of high-dimensional linear and Poisson regressions are also provided. We aim to illustrate the concentration inequalities with known constants and to improve existing bounds with sharper constants.","layer":1,"vector":[0.0094,-0.0268,0.0477,-0.0445,0.0291,0.041,0.0645,0.0645,0.0334,-0.0244,0.0179,-0.0623,0.0451,0.0784,0.0398,0.0851,-0.0145,-0.0119,-0.0887,0.0547,0.0334,-0.09,-0.0304,-0.037,0.0402,-0.0079,-0.0564,-0.046,-0.0363,-0.2723,0.0019,-0.0346,0.0609,-0.0513,-0.0377,-0.0421,-0.0357,-0.0049,-0.0049,0.0581,0.0168,0.0401,-0.0358,-0.042,-0.0462,-0.0556,-0.0492,0.0065,-0.0296,-0.0508,0.013,0.0245,0.0401,0.0623,0.0288,-0.0177,0.0057,0.0176,0.05,0.0533,-0.0081,0.0045,-0.1757,0.0143,0.0474,0.0157,-0.0794,-0.0306,-0.0046,0.0587,-0.0041,0.053,-0.029,0.0403,0.0103,0.0118,0.0355,-0.0368,-0.0284,0.0255,0.0327,-0.0514,-0.0247,0.0127,-0.0149,-0.0727,0.0196,-0.0583,0.0384,0.0111,0.0074,-0.0066,-0.0459,0.0209,-0.0444,-0.0108,0.0416,0.0467,0.0144,0.1793,-0.0554,0.0136,0.0156,-0.013,0.0143,-0.013,-0.0212,0.0041,0.0241,0.0245,-0.0087,-0.054,0.0509,-0.0181,0.0071,0.0132,0.1001,-0.0018,-0.0075,-0.0155,-0.0353,0.0142,0.028,-0.021,0.0246,-0.0241,-0.0022,0.1471,0.0215,0.0049,0.0198,-0.0207,-0.0638,0.0024,-0.0366,-0.0128,0.0266,0.0102,0.0086,0.0259,-0.0034,-0.0913,0.0307,-0.0779,0.0012,0.0883,-0.063,0.0511,-0.0564,-0.0875,-0.0168,0.0555,-0.0146,-0.0354,0.044,0.0096,0.0076,0.0477,-0.0601,0.0265,-0.0528,-0.0564,0.0293,0.1113,-0.0419,-0.0093,-0.0311,0.0307,0.0036,0.0267,0.0492,0.0183,-0.0502,0.0511,0.0903,-0.0124,-0.0588,0.0257,0.0116,0.0121,0.0161,-0.0286,-0.0219,0.0284,0.0314,-0.0317,0.0252,-0.0029,0.0368,0.1014,-0.0533,0.0057,-0.0321,-0.0116,-0.0113,-0.0186,0.0135,-0.0028,0.0318,-0.0254,0.0519,-0.0011,-0.0825,0.0278,0.0217,0.0733,0.011,0.0229,0.0655,0.0331,-0.0142,-0.0248,0.0372,-0.0093,-0.0209,0.0683,0.0245,0.0198,0.0325,0.0715,0.0271,-0.0494,-0.0849,-0.2419,-0.0102,0.0208,0.0026,0.0568,-0.0228,0.0518,0.012,0.0519,0.109,0.0358,-0.0046,-0.0502,0.0372,0.0024,0.0166,-0.0131,0.0248,-0.0331,0.0154,-0.0146,0.0149,-0.0323,-0.0313,0.0851,-0.027,0.1781,-0.0107,-0.0132,-0.0372,-0.0008,-0.0034,-0.0148,-0.0641,0.0526,0.0525,-0.0013,-0.0215,-0.0444,0.0053,-0.019,0.0322,0.0117,-0.072,-0.074,-0.054,-0.0208,0.0611,-0.0558,-0.0009,0.0363,-0.0565,0.0682,-0.0284,0.0356,-0.0331,-0.111,0.0415,-0.0497,0.0091,-0.0374,-0.0751,0.0407,-0.0605,0.0241,0.0167,-0.0233,-0.0634,0.0135,0.0072,-0.0254,0.0823,-0.0358,-0.0022,0.0365,0.0172,0.0398,-0.0385,-0.0584,-0.0412,0.0666,-0.066,0.047,0.0161,0.0478,-0.0111,0.0357,-0.0006,0.0389,-0.0169,-0.0259,0.0172,-0.0851,-0.0191,-0.0166,0.0479,-0.2758,0.0172,0.01,0.0208,-0.0509,-0.013,0.0425,0.0078,-0.0573,-0.0182,0.0654,0.0346,0.0451,-0.0295,0.0185,0.0078,0.0212,-0.0825,0.0593,-0.0129,0.0306,0.0218,0.1927,-0.032,0.041,0.0145,0.0106,0.0296,-0.011,-0.0637,0.0165,0.0064,0.0945,-0.0311,0.0317,0.0615,-0.0768,0.0575,0.0041,-0.0238,0.0299,-0.0197,-0.0467,0.0066,0.1108,-0.0392,-0.014,-0.0611,0.0421,0.0371,-0.0482,0.0277,0.0339,0.03,0.0216,0.0077,-0.0418,-0.0431,-0.0347,-0.0477,-0.013,-0.0416,-0.0336,-0.0162,0.009]}
{"key":"[Rate-Constrained Remote Contextual Bandits] We consider a rate-constrained contextual multi-armed bandit (RC-CMAB) problem, in which a group of agents are solving the same contextual multi-armed bandit (CMAB) problem. However, the contexts are observed by a remotely connected entity, i.e., the decision-maker, that updates the policy to maximize the returned rewards, and communicates the arms to be sampled by the agents to a controller over a rate-limited communications channel. This framework can be applied to personalized ad placement, whenever the content owner observes the website visitors, and hence has the context, but needs to transmit the ads to be shown to a controller that is in charge of placing the marketing content. Consequently, the rate-constrained CMAB (RC-CMAB) problem requires the study of lossy compression schemes for the policy to be employed whenever the constraint on the channel rate does not allow the uncompressed transmission of the decision-maker's intentions. We characterize the fundamental information theoretic limits of this problem by letting the number of agents go to infinity, and study the regret that can be achieved, identifying the two distinct rate regions leading to linear and sub-linear regrets respectively. We then analyze the optimal compression scheme achievable in the limit with infinite agents, when using the forward and reverse KL divergence as distortion metric. Based on this, we also propose a practical coding scheme, and provide numerical results.","layer":0,"vector":[-0.0551,-0.0043,0.0155,-0.0395,0.0024,0.0116,0.0536,0.0515,0.0199,-0.0059,0.0126,-0.0014,0.0549,0.0621,0.0542,0.0177,-0.01,0.0235,-0.0237,0.0204,0.0736,-0.0669,0.0302,-0.0712,0.0485,-0.0269,-0.0331,-0.0556,-0.0218,-0.1946,0.0009,-0.03,0.0404,-0.016,0.0206,-0.0133,-0.011,0.0092,-0.0059,0.1047,0.0214,0.0497,-0.0456,-0.0642,-0.0434,-0.0488,0.0215,0.0242,-0.0317,0.0108,0.001,-0.0059,0.0453,0.0342,0.0368,0.0042,0.0332,0.096,0.0365,0.0416,0.0054,0.053,-0.155,0.0311,0.0203,0.0535,-0.0465,0.0597,0.0096,0.0443,-0.0073,0.0145,-0.0023,0.028,0.0286,-0.0009,-0.0315,-0.0208,-0.0171,-0.0144,0.0082,-0.0631,-0.0399,-0.009,-0.0109,-0.074,0.0341,-0.0354,0.0481,0.0117,-0.0189,-0.0075,-0.0042,-0.0218,-0.0857,-0.0332,-0.0055,0.023,-0.0671,0.1995,-0.0106,0.0477,0.0206,-0.0389,0.0609,-0.0807,-0.0041,0.0052,0.0031,-0.001,-0.0161,0.0037,0.0469,-0.0053,0.0376,0.0178,0.0386,0.0563,0.0304,-0.0213,-0.0518,0.0267,0.0827,-0.0182,0.0538,-0.0592,-0.0027,0.137,0.0117,0.0249,0.038,-0.0231,-0.0128,-0.0294,0.0033,-0.0062,-0.0166,0.0494,0.0444,-0.0257,-0.056,-0.0132,0.0255,-0.0675,-0.0385,0.1247,0.0148,0.0345,-0.0813,-0.0172,-0.0019,-0.0165,-0.0057,-0.0304,0.0007,0.0294,0.0367,0.0593,-0.0499,0.0414,-0.018,-0.0404,0.0248,0.0928,-0.0037,-0.0905,-0.022,0.0259,0.0265,0.0055,0.005,0.0104,-0.0612,0.0224,0.0785,-0.0033,-0.1139,-0.012,-0.0014,-0.0131,0.0007,-0.0193,0.0179,0.0221,-0.0309,-0.0308,0.0288,-0.0446,0.0536,0.0177,-0.032,-0.014,-0.0208,-0.0108,-0.0339,-0.0092,0.0342,-0.0065,-0.0037,0.0013,0.0357,-0.0262,-0.0845,0.0046,0.0232,-0.006,-0.0023,-0.0331,0.0125,0.002,-0.035,0.0137,0.0522,-0.0394,-0.0376,-0.0194,0.0691,0.0636,0.0121,0.0347,0.0192,0.0102,-0.0137,-0.2447,0.0126,-0.0336,-0.0211,0.0317,-0.0323,0.0516,-0.0176,0.0422,0.0535,0.0883,-0.0884,-0.0541,0.0317,-0.0178,0.0304,0.0298,0.0149,-0.0204,0.0165,0.0091,0.0135,-0.0004,-0.0334,0.0435,-0.0122,0.2449,0.0495,0.0195,-0.0169,0.0437,0.0445,-0.0224,-0.11,0.0174,0.0331,0.0459,-0.0208,0.0099,-0.0454,-0.0093,0.0149,0.0171,-0.0722,-0.0377,-0.058,-0.0701,0.0631,-0.0582,0.032,0.0201,-0.0281,0.0313,-0.0147,-0.0164,-0.0257,-0.0617,0.0046,-0.0342,0.0447,0.0442,-0.0679,0.0324,-0.0663,0.0775,-0.0248,0.0232,-0.0243,0.0393,-0.0114,-0.0199,0.0583,-0.0116,0.003,0.0038,-0.0009,0.0235,-0.0817,-0.0415,-0.0321,0.0373,-0.057,0.0277,0.0168,0.0125,0.0039,0.0681,-0.0181,-0.0064,-0.0193,-0.0124,0.0168,-0.0601,0.0311,0.0451,-0.0698,-0.3236,0.0488,0.0003,0.0189,-0.0163,0.0612,0.0549,0.0088,-0.0693,0.0065,0.0162,0.0318,-0.0175,0.0045,0.0596,0.0093,0.047,-0.0614,0.0357,-0.075,0.0496,0.0063,0.2127,-0.0371,0.0295,0.0195,-0.0178,-0.0136,0.0318,-0.0372,0.0042,0.0151,0.0846,-0.1057,0.064,0.0635,-0.0695,0.0491,0.0406,-0.01,-0.0486,0.0141,-0.0453,-0.0222,0.102,0.0352,-0.0803,-0.053,-0.0474,0.0102,-0.0097,0.0111,-0.0049,-0.046,0.052,0.0069,-0.0511,-0.0873,-0.0018,-0.0245,0.016,-0.049,-0.0341,-0.0402,-0.0064]}
{"key":"[A Simple Fine-tuning Is All You Need: Towards Robust Deep Learning Via Adversarial Fine-tuning] Adversarial Training (AT) with Projected Gradient Descent (PGD) is an effective approach for improving the robustness of the deep neural networks. However, PGD AT has been shown to suffer from two main limitations: i) high computational cost, and ii) extreme overfitting during training that leads to reduction in model generalization. While the effect of factors such as model capacity and scale of training data on adversarial robustness have been extensively studied, little attention has been paid to the effect of a very important parameter in every network optimization on adversarial robustness: the learning rate. In particular, we hypothesize that effective learning rate scheduling during adversarial training can significantly reduce the overfitting issue, to a degree where one does not even need to adversarially train a model from scratch but can instead simply adversarially fine-tune a pre-trained model. Motivated by this hypothesis, we propose a simple yet very effective adversarial fine-tuning approach based on a $\\textit{slow start, fast decay}$ learning rate scheduling strategy which not only significantly decreases computational cost required, but also greatly improves the accuracy and robustness of a deep neural network. Experimental results show that the proposed adversarial fine-tuning approach outperforms the state-of-the-art methods on CIFAR-10, CIFAR-100 and ImageNet datasets in both test accuracy and the robustness, while reducing the computational cost by 8-10$\\times$. Furthermore, a very important benefit of the proposed adversarial fine-tuning approach is that it enables the ability to improve the robustness of any pre-trained deep neural network without needing to train the model from scratch, which to the best of the authors' knowledge has not been previously demonstrated in research literature.","layer":2,"vector":[-0.0222,-0.0348,-0.0005,-0.0263,-0.0046,0.0229,0.0055,-0.0208,0.0008,0.0129,-0.0099,-0.0165,0.0839,0.1086,0.022,0.0004,0.0436,0.0311,-0.0479,0.0234,0.0243,-0.0268,0.0164,-0.0398,0.0109,-0.0074,-0.0145,-0.0444,-0.0505,-0.3038,-0.0124,-0.0398,0.023,-0.0274,-0.0032,-0.0322,-0.0508,0.0367,-0.0223,0.0461,0.012,0.0377,-0.0496,-0.08,-0.0039,-0.0106,-0.0338,-0.0012,-0.0167,-0.0227,0.0396,-0.0565,0.006,0.0184,0.0127,-0.01,0.0566,0.0413,0.0628,0.0357,-0.0079,0.062,-0.147,0.0265,0.0246,0.0226,-0.0281,-0.0019,-0.0041,0.0572,-0.0169,0.0364,0.0074,0.0043,0.0242,0.0253,-0.0081,-0.0561,0.0374,0.0131,0.0911,-0.0103,-0.0322,0.0162,0.0399,-0.0422,0.0279,-0.0287,0.0476,-0.0146,0.0092,0.0037,-0.0401,0.0675,-0.0256,0.0235,0.0786,0.0225,-0.1032,0.212,-0.0309,0.0129,0.0388,-0.03,0.0078,-0.0215,-0.0395,-0.0235,-0.0166,-0.0011,-0.0317,-0.0119,0.0638,-0.015,0.0453,0.0469,0.0483,0.0268,-0.0089,0.0239,-0.0073,0.0013,0.0213,-0.0445,0.042,-0.048,-0.0039,0.1442,0.0255,0.0425,0.0107,-0.068,-0.047,-0.032,0.032,0.0318,0.002,0.0284,0.0224,-0.0232,-0.0615,-0.057,0.0022,-0.0733,-0.0188,0.1093,-0.0211,0.0099,-0.0039,-0.0614,-0.0404,-0.0061,-0.0346,-0.0257,0.0011,0.0027,0.004,0.0788,-0.0384,-0.014,-0.0307,-0.0364,-0.0317,0.0649,-0.0093,-0.0643,-0.0428,0.0031,-0.0116,-0.0197,0.0005,-0.0105,-0.0197,0.043,0.0753,0.0197,-0.0854,-0.0126,-0.0372,0.0105,0.0112,-0.0541,-0.0148,0.0379,0.0217,-0.0452,0.0201,-0.0437,0.0087,-0.0073,-0.0712,0.0033,-0.0303,-0.0123,-0.0423,-0.0118,-0.0192,-0.0044,-0.0078,-0.0094,-0.0197,0.0039,-0.054,0.04,0.0197,0.0397,0.0151,-0.0034,0.0268,0.054,-0.0687,-0.0034,0.0437,-0.0309,-0.0468,-0.0014,-0.0205,0.0659,-0.0342,0.055,0.0354,-0.0207,-0.0436,-0.2295,0.0052,-0.0437,-0.0277,0.0664,-0.0832,0.0157,-0.0189,0.0352,0.0395,0.0449,0.0009,-0.0157,-0.003,-0.0093,0.0755,0.0311,0.0028,-0.0217,0.0256,-0.0079,0.0706,0.0146,-0.094,0.0608,0.0381,0.1983,-0.0213,0.0459,-0.0574,0.0584,0.0203,-0.0221,-0.0943,0.0604,-0.0125,0.0954,-0.0098,-0.0359,-0.005,0.0038,0.0441,-0.0026,-0.1517,-0.0751,-0.0142,-0.0229,0.0325,-0.0913,0.0248,0.0352,-0.0509,0.0574,0.0048,0.0258,-0.0376,-0.1131,0.0748,-0.075,0.0416,0.0074,-0.0756,-0.0011,-0.0638,0.0458,0.0238,-0.0177,-0.0573,0.0473,-0.0039,0.0188,0.0481,0.0204,0.0232,0.0532,-0.0298,-0.0077,-0.0081,-0.0656,-0.0313,0.0588,0.033,0.0192,-0.0016,0.0305,-0.0325,0.0851,0.0045,0.0414,0.03,-0.015,0.0097,-0.0522,-0.0364,0.0574,0.0232,-0.3024,0.0448,0.0351,0.0406,-0.0227,0.0145,0.0545,0.0117,-0.0113,0.0033,-0.0164,0.0295,0.0268,0.0158,0.0394,0.0181,0.0585,-0.0469,0.055,-0.0192,0.0159,0.0175,0.204,-0.0436,0.0024,0.0334,0.0113,0.0681,0.0446,-0.059,-0.0001,-0.0074,0.1,-0.0065,0.0221,0.0874,-0.0441,0.0301,-0.0105,-0.0373,-0.0187,-0.0004,-0.0056,0.0272,0.0666,-0.0112,-0.0144,-0.0022,-0.0187,-0.0093,-0.0317,-0.0142,0.0209,0.0125,0.0541,0.0433,-0.0437,-0.0289,-0.0323,-0.0299,0.0159,-0.0308,-0.0574,0.0379,-0.0433]}
{"key":"[VCNet and Functional Targeted Regularization For Learning Causal Effects of Continuous Treatments] Motivated by the rising abundance of observational data with continuous treatments, we investigate the problem of estimating the average dose-response curve (ADRF). Available parametric methods are limited in their model space, and previous attempts in leveraging neural network to enhance model expressiveness relied on partitioning continuous treatment into blocks and using separate heads for each block; this however produces in practice discontinuous ADRFs. Therefore, the question of how to adapt the structure and training of neural network to estimate ADRFs remains open. This paper makes two important contributions. First, we propose a novel varying coefficient neural network (VCNet) that improves model expressiveness while preserving continuity of the estimated ADRF. Second, to improve finite sample performance, we generalize targeted regularization to obtain a doubly robust estimator of the whole ADRF curve.","layer":7,"vector":[-0.036,0.0143,0.054,-0.004,0.0602,0.0031,0.0183,0.0541,0.0055,-0.0231,0.0581,-0.0324,0.0209,0.063,-0.0136,0.0324,-0.0003,0.0506,-0.0293,0.0317,-0.0292,-0.0435,0.0026,-0.0212,0.0063,0.0038,-0.0463,-0.0244,-0.0276,-0.2551,0.051,-0.0103,0.0439,-0.0571,-0.0108,0.0023,-0.0368,0.0596,-0.0231,0.0583,-0.012,0.0342,-0.0117,-0.0525,-0.0008,-0.0388,-0.0368,-0.0107,-0.0051,-0.0004,0.0644,-0.0723,0.0459,0.0484,0.0568,-0.0109,0.0188,0.0342,0.0294,0.064,0.0008,0.0444,-0.1797,0.0393,0.0506,0.0118,-0.0501,0.0064,0.022,0.0944,-0.0363,0.0349,-0.0025,0.0354,0.018,-0.0275,0.0172,-0.0176,-0.0079,0.0046,0.0715,-0.0095,-0.0061,-0.0387,-0.0001,-0.0349,0.0318,-0.0565,0.0344,-0.0051,-0.0335,-0.0115,-0.0149,-0.0157,-0.0797,-0.0386,0.0684,0.0168,-0.0428,0.1847,-0.0567,0.0218,-0.0227,-0.0016,0.0348,-0.0341,-0.049,-0.023,0.027,0.0088,-0.0004,-0.0082,0.0345,-0.0933,0.0183,0.0221,0.0421,0.0322,0.0069,-0.0476,-0.0372,0.0007,0.0246,-0.0064,0.0408,-0.0261,0.0314,0.1477,0.0474,0.0404,0.0832,-0.0378,-0.0704,-0.0138,0.003,0.0168,0.0143,-0.0182,0.0048,0.0532,-0.0541,-0.0496,0.0186,-0.108,-0.1084,0.1472,0.0013,0.0177,-0.0454,-0.0222,0.005,0.0315,-0.0455,0.0141,0.03,0.0507,-0.0031,0.0365,-0.0444,-0.0381,-0.0322,-0.0703,-0.0582,0.1074,0.0086,-0.0254,-0.0363,0.0242,0.0491,-0.0129,0.0684,0.047,0.0096,0.0113,0.0709,-0.0156,-0.035,0.0068,-0.0199,0.0149,0.0111,-0.0717,-0.0126,0.0509,0.0241,-0.0174,0.013,-0.0029,0.0073,0.0241,-0.0452,0.0159,-0.0546,-0.002,-0.0117,-0.0492,-0.0445,-0.0076,0.0209,-0.0284,0.0335,0.0125,-0.0266,0.009,-0.0122,0.0242,-0.0227,0.0041,0.0078,0.0098,-0.0341,0.0206,0.1185,-0.0094,-0.037,0.0645,-0.0034,-0.0229,0.021,0.0539,0.0324,-0.0329,-0.0165,-0.2415,-0.0552,0.0358,-0.0175,0.0777,-0.0932,0.0422,-0.0173,0.0647,0.0848,0.0255,0.0384,-0.0135,0.0092,-0.0408,0.0124,0.0175,0.0459,-0.0281,-0.004,-0.0438,-0.0046,0.0062,-0.0601,0.0186,0.0143,0.1976,0.0272,0.0364,-0.0214,0.0249,0.0385,0.017,-0.1113,0.0783,-0.0039,0.0545,0.0125,-0.0721,-0.0408,-0.0423,0.0242,-0.0229,-0.0905,-0.0672,-0.0237,-0.0099,0.0208,-0.0929,-0.0062,0.0133,-0.0059,0.0472,0.0082,0.0316,-0.0408,-0.0993,0.0496,-0.0551,-0.0269,0.0186,-0.0207,-0.0057,-0.0597,0.0196,0.0204,0.003,-0.0718,0.0214,-0.0309,-0.023,0.0833,-0.0101,-0.0009,0.0339,0.0261,-0.0078,-0.0082,-0.0948,0.0079,0.0658,-0.036,0.0106,0.0156,-0.0158,0.0175,0.0418,-0.0273,0.0587,-0.0236,0.0002,-0.0166,-0.0503,0.0162,0.0111,0.002,-0.2816,0.0354,0.008,0.0093,-0.051,0.0528,0.0324,0.0007,-0.0401,-0.0251,-0.0224,0.0691,0.0797,0.0092,0.0193,0.0157,0.0495,-0.0632,0.0666,-0.0824,0.0282,0.0332,0.192,-0.0647,0.0345,0.023,-0.0516,0.0242,0.0291,0.0191,0.0297,0.0771,0.0553,-0.0546,0.063,0.0702,-0.0586,0.0271,-0.0058,-0.0278,0.0078,-0.0023,-0.0275,-0.0472,0.0977,-0.0028,-0.0244,-0.0865,0.0127,0.0028,-0.0369,0.0114,0.0014,0.0503,0.0374,0.0005,-0.0455,-0.0423,-0.0645,-0.0268,0.0029,-0.0275,-0.0154,0.0072,-0.0276]}
{"key":"[Regularized Gradient Descent Ascent for Two-Player Zero-Sum Markov Games] We study the problem of finding the Nash equilibrium in a two-player zero-sum Markov game. Due to its formulation as a minimax optimization program, a natural approach to solve the problem is to perform gradient descent/ascent with respect to each player in an alternating fashion. However, due to the non-convexity/non-concavity of the underlying objective function, theoretical understandings of this method are limited. In our paper, we consider solving an entropy-regularized variant of the Markov game. The regularization introduces structure into the optimization landscape that make the solutions more identifiable and allow the problem to be solved more efficiently. Our main contribution is to show that under proper choices of the regularization parameter, the gradient descent ascent algorithm converges to the Nash equilibrium of the original unregularized problem. We explicitly characterize the finite-time performance of the last iterate of our algorithm, which vastly improves over the existing convergence bound of the gradient descent ascent algorithm without regularization. Finally, we complement the analysis with numerical simulations that illustrate the accelerated convergence of the algorithm.","layer":3,"vector":[-0.0755,0.0026,0.0487,-0.0124,-0.0081,0.0795,0.0332,0.0296,0.0687,0.0012,0.0174,-0.0214,0.021,0.0585,0.0218,0.0434,-0.0104,0.0285,-0.0338,-0.0088,0.0215,-0.0685,-0.0072,-0.1085,0.0586,0.0276,-0.0579,-0.0281,-0.0419,-0.2423,0.0635,-0.0701,0.0175,-0.016,0.0383,-0.0269,-0.0138,0.05,-0.0278,-0.0096,-0.0188,0.0246,-0.0104,-0.0843,-0.0177,-0.061,-0.0327,-0.0109,-0.0346,-0.0285,0.0022,0.0073,0.0143,0.0046,0.0227,0.0239,0.0375,0.0588,0.0161,0.0476,0.0002,0.0025,-0.1725,0.0679,0.0919,0.036,-0.0515,-0.0197,0.0276,0.0722,0.0096,0.0472,0.0602,0.0389,0.0179,0.0005,-0.0026,-0.039,0.0277,0.0043,-0.0025,-0.0714,-0.0409,-0.0383,0.0152,-0.0478,0.0106,-0.0198,0.044,0.0085,-0.0184,0.0279,0.024,0.0128,-0.0459,0.0325,0.0183,0.0333,-0.054,0.1972,-0.0321,0.0406,-0.009,-0.0429,0.0583,-0.0008,-0.0111,-0.0279,0.0331,-0.0127,-0.0396,0.0047,0.096,-0.0269,0.0194,0.0645,0.0368,0.0936,-0.033,0.0245,-0.023,-0.002,0.0233,0.0101,0.0243,-0.075,-0.0363,0.139,0.0599,0.0108,0.0634,-0.062,-0.0187,-0.0313,0.0256,-0.0095,-0.0177,0.0025,0.0322,0.0258,-0.0384,-0.0675,-0.0094,-0.1332,-0.0535,0.1035,0.0183,0.0471,-0.0287,-0.0307,0.0003,-0.0387,-0.0195,-0.0228,0.003,0.0342,0.0464,0.0247,-0.0652,0.008,-0.0509,-0.0196,-0.0218,0.0877,-0.0061,-0.0721,0.0116,-0.0401,-0.0153,-0.054,0.0477,0.0077,-0.0333,0.0017,0.0629,0.0274,-0.093,-0.021,0.0104,0.0098,0.0239,0.0003,-0.0468,-0.0025,0.0005,-0.0286,0.0127,-0.0182,0.0086,0.0048,-0.051,-0.0188,-0.029,0.0044,-0.016,-0.0254,-0.0173,-0.0214,0.04,-0.0444,0.0139,0.003,-0.0487,0.0632,0.0384,0.0111,-0.017,-0.0645,0.0331,-0.0185,-0.0439,-0.0286,-0.0014,0.002,-0.0414,0.0275,0.0164,0.0023,-0.0449,0.054,0.0411,0.0128,-0.0324,-0.191,-0.0099,-0.068,-0.0225,0.0229,-0.0274,0.0406,-0.033,0.0513,0.0509,0.0714,-0.0556,-0.0575,0.0592,-0.0072,0.0529,0.0028,0.047,0.0214,0.0414,0.0384,0.0205,0.0143,-0.0667,0.0585,-0.0376,0.2307,0.0404,0.0434,-0.0159,0.0134,0.044,-0.0442,-0.0508,0.0227,0.0039,0.0887,-0.0501,-0.0549,-0.0467,-0.0136,0.0596,-0.0024,-0.0686,-0.0244,-0.0166,-0.0552,0.0358,-0.0431,-0.0052,0.0529,0.0008,0.0326,-0.0301,-0.012,-0.0463,-0.0716,0.0189,-0.0015,0.0531,0.0083,-0.0607,-0.0101,-0.0098,0.0443,0.0015,0.018,-0.0332,0.0303,-0.018,-0.0088,0.0104,0.0109,-0.0082,0.0081,0.0512,0.0395,-0.0373,-0.0315,-0.0344,0.0462,-0.0618,0.0526,0.0422,0.0236,-0.0273,0.075,-0.0052,0.003,-0.0366,-0.0226,0.0249,-0.0793,0.0399,0.025,-0.0094,-0.3235,0.0191,0.0206,0.0033,-0.0383,0.0233,0.0849,0.0433,-0.0826,0.0141,0.0132,0.0509,0.0069,0.0244,0.0249,0.0133,0.0756,-0.0289,0.0746,-0.075,0.0058,0.0311,0.2298,-0.0567,0.0406,0.0005,-0.0175,0.0164,-0.0046,-0.0233,-0.0204,0.0191,0.0899,-0.0591,0.0625,0.089,0.0049,0.0169,0.0202,0.0264,-0.0448,-0.0006,0.0126,0.0139,0.0507,0.0257,-0.0102,-0.0391,-0.0096,0.0134,-0.0356,0.0218,-0.0006,0.0005,0.0552,-0.005,-0.0778,-0.0811,-0.0268,-0.0395,-0.0107,-0.0747,-0.0356,-0.0208,-0.0097]}
{"key":"[Visual Explanation using Attention Mechanism in Actor-Critic-based Deep Reinforcement Learning] Deep reinforcement learning (DRL) has great potential for acquiring the optimal action in complex environments such as games and robot control. However, it is difficult to analyze the decision-making of the agent, i.e., the reasons it selects the action acquired by learning. In this work, we propose Mask-Attention A3C (Mask A3C), which introduces an attention mechanism into Asynchronous Advantage Actor-Critic (A3C), which is an actor-critic-based DRL method, and can analyze the decision-making of an agent in DRL. A3C consists of a feature extractor that extracts features from an image, a policy branch that outputs the policy, and a value branch that outputs the state value. In this method, we focus on the policy and value branches and introduce an attention mechanism into them. The attention mechanism applies a mask processing to the feature maps of each branch using mask-attention that expresses the judgment reason for the policy and state value with a heat map. We visualized mask-attention maps for games on the Atari 2600 and found we could easily analyze the reasons behind an agent's decision-making in various game tasks. Furthermore, experimental results showed that the agent could achieve a higher performance by introducing the attention mechanism.","layer":0,"vector":[-0.0734,0.0169,0.0242,-0.03,0.0191,0.0189,0.06,0.0288,0.0309,0.0107,0.029,-0.0622,0.0405,0.0798,0.0284,0.014,-0.0427,0.0213,-0.0067,-0.0229,0.0346,-0.0405,-0.0185,-0.0798,0.0109,-0.0005,-0.054,-0.0473,-0.0228,-0.2181,0.0444,-0.0489,0.0294,-0.0415,-0.0328,-0.0771,-0.0531,0.0304,-0.0376,-0.0176,0.0079,0.0052,-0.0566,-0.0697,-0.0554,-0.0563,0.0142,-0.058,0.0037,-0.044,0.047,-0.0263,0.0247,0.0193,0.026,0.0345,0.0611,0.114,0.0422,-0.0012,0.0374,0.038,-0.1755,0.0489,0.0506,0.0618,-0.0215,-0.0036,0.038,0.049,-0.0414,0.014,0.0099,0.0051,0.0199,-0.01,0.0006,-0.0311,0.0328,-0.025,0.0325,-0.0248,-0.066,-0.0113,0.0104,-0.069,-0.0039,-0.0148,0.0252,-0.0397,-0.0603,-0.0069,-0.0124,-0.0105,-0.0415,0.0191,0.0166,0.0194,-0.0591,0.2041,-0.0406,0.0156,-0.0056,-0.0219,0.0489,-0.022,-0.0126,-0.0289,-0.0198,0.0189,-0.0342,-0.0042,0.0237,0.0166,0.0442,0.0379,0.0824,0.0246,0.002,-0.0154,0.0253,-0.0037,0.0303,-0.025,0.0225,-0.0719,-0.0032,0.1678,0.0558,0.0376,0.0497,-0.0588,-0.0443,-0.0332,0.0216,0.0131,0.0276,0.0122,0.0072,-0.0394,-0.0543,0.0422,0.0073,-0.0971,-0.0393,0.0723,0.0061,0.038,-0.0399,-0.0026,-0.006,-0.0188,0.0029,-0.0169,-0.0042,0.0213,0.0034,0.0636,-0.0667,0.0076,-0.0052,-0.0458,-0.013,0.0586,0.0155,-0.0778,-0.0444,-0.042,-0.0291,-0.0224,0.0052,0.0368,-0.0704,0.0044,0.0664,0.0263,-0.0559,-0.0016,-0.0101,-0.023,0.0444,-0.0789,-0.0198,-0.0112,0.0563,-0.0373,0.0106,-0.0213,-0.0114,0.027,-0.0393,0.0563,-0.0489,-0.0016,-0.0343,-0.0281,0.0216,-0.006,-0.0398,-0.05,0.0196,-0.0116,-0.043,0.0128,-0.0381,0.026,-0.0679,-0.0242,0.0822,-0.0106,-0.0966,0.0259,0.0166,-0.0018,-0.0412,0.0047,0.0215,-0.0083,-0.0274,0.019,0.0603,-0.002,-0.0229,-0.2347,-0.0078,-0.044,-0.0463,0.0147,-0.0565,0.0619,-0.0179,0.0702,0.0465,0.0791,-0.057,-0.0411,0.0354,-0.0145,0.0378,0.0475,0.0341,-0.0235,0.0091,0.0277,0.0114,0.0081,-0.0888,0.0217,0.0115,0.2428,0.0373,0.0186,-0.0079,-0.0155,0.0536,-0.0496,-0.0896,0.0616,-0.0072,0.0846,0.0025,-0.0065,-0.0294,-0.0037,0.0406,0.0063,-0.0654,0.0053,-0.0028,-0.028,0.0812,-0.0598,0.0138,0.0728,-0.0329,0.0045,0.0192,-0.0293,-0.0265,-0.0468,0.0048,-0.0232,0.0531,0.011,-0.032,-0.0109,-0.0065,0.0673,0.0312,0.0344,-0.0572,0.043,0.0045,-0.023,0.0724,-0.0094,-0.0323,0.0492,0.0305,0.062,-0.0148,-0.0258,-0.0062,0.0386,-0.0139,0.0226,0.0007,0.0287,-0.0012,0.0224,-0.0192,0.0358,-0.0116,-0.0144,0.0204,-0.0227,-0.0155,0.0376,-0.0005,-0.3256,0.0557,0.0235,0.0181,-0.0054,0.0083,0.0513,0.0077,-0.0182,-0.0054,0.0044,0.0662,0.0308,-0.0066,-0.0213,0.0028,0.0656,-0.0477,0.0992,-0.0375,0.0367,0.0262,0.228,-0.03,0.0418,-0.0117,-0.0289,-0.0093,0.0203,-0.0099,0.0105,0.0128,0.1163,-0.0462,0.0491,0.1054,-0.0428,0.0478,0.0161,0.0211,-0.0026,0.0374,-0.007,0.0063,0.1013,0.0246,-0.0534,-0.0402,-0.027,0.0195,-0.0252,0.0053,-0.0434,-0.0403,0.0611,0.0577,-0.0591,-0.0154,-0.0402,-0.0066,0.001,-0.0713,0.0177,0.0027,0.0123]}
{"key":"[Distinguishing rule- and exemplar-based generalization in learning systems] Despite the increasing scale of datasets in machine learning, generalization to unseen regions of the data distribution remains crucial. Such extrapolation is by definition underdetermined and is dictated by a learner's inductive biases. Machine learning systems often do not share the same inductive biases as humans and, as a result, extrapolate in ways that are inconsistent with our expectations. We investigate two distinct such inductive biases: feature-level bias (differences in which features are more readily learned) and exemplar-vs-rule bias (differences in how these learned features are used for generalization). Exemplar- vs. rule-based generalization has been studied extensively in cognitive psychology, and, in this work, we present a protocol inspired by these experimental approaches for directly probing this trade-off in learning systems. The measures we propose characterize changes in extrapolation behavior when feature coverage is manipulated in a combinatorial setting. We present empirical results across a range of models and across both expository and real-world image and language domains. We demonstrate that measuring the exemplar-rule trade-off while controlling for feature-level bias provides a more complete picture of extrapolation behavior than existing formalisms. We find that most standard neural network models have a propensity towards exemplar-based extrapolation and discuss the implications of these findings for research on data augmentation, fairness, and systematic generalization.","layer":0,"vector":[-0.0224,-0.0135,0.0505,-0.0186,0.0408,-0.0192,0.0466,0.0369,0.0388,0.0307,0.0062,-0.0218,0.0555,0.017,0.0191,0.0505,0.0002,0.0006,-0.0903,-0.0113,0.0042,-0.0563,-0.0105,-0.0293,-0.0037,0.0256,-0.0555,-0.0638,-0.0455,-0.2804,0.0201,-0.0494,0.0752,-0.0196,-0.0163,-0.0178,-0.0301,0.0695,-0.066,0.0393,-0.0086,0.0145,0.0021,-0.0236,-0.045,-0.0199,-0.0452,0.0012,-0.026,-0.0423,0.0255,-0.0406,0.0196,0.0555,0.0341,0.0495,0.083,0.0054,0.0123,0.0489,0.03,0.0546,-0.1566,0.0529,0.0297,0.0228,-0.0462,-0.0263,0.0029,0.0644,0.0408,0.0379,0.0113,0.0714,0.0219,0.0068,-0.0214,-0.0393,0.0022,0.029,0.0279,-0.0288,0.0132,-0.0046,0.0259,-0.0262,-0.0029,-0.0703,0.0247,-0.0026,-0.0278,-0.0133,-0.0374,0.0317,-0.0304,0.0019,0.0387,0.0135,-0.0479,0.1758,-0.0268,0.0013,0.0234,0.0006,0.0464,-0.0616,-0.0311,-0.0244,-0.0507,-0.0107,-0.0036,0.0165,0.0178,-0.0456,-0.0158,-0.0017,0.0471,0.0062,0.0133,-0.0211,-0.0551,-0.0149,0.0132,-0.0382,0.028,-0.068,0.0083,0.1091,0.0173,-0.0363,0.0439,-0.0706,-0.0661,-0.0131,0.0412,0.0705,0.039,0.0325,0.008,0.0292,-0.0567,-0.0337,0.006,-0.067,-0.0635,0.1431,-0.0498,0.0109,-0.0139,0.0202,-0.0281,0.028,-0.0446,-0.0532,-0.0081,0.0704,0.0109,0.0267,-0.0401,0.0363,0.0256,-0.0471,-0.0242,0.0884,0.0057,-0.0498,-0.013,-0.0106,0.0425,-0.0149,0.0333,-0.0165,-0.0527,0.0144,0.0773,0.0103,-0.0704,-0.0013,-0.0314,0.0258,0.0736,-0.0522,-0.0349,0.0452,0.051,-0.0222,0.0277,-0.0351,0.0292,0.0497,-0.0187,0.0256,-0.0474,0.0035,-0.0471,-0.0353,-0.0141,0.0237,0.0089,-0.0272,-0.0132,0.0497,-0.029,0.0182,-0.0159,0.0249,-0.0096,-0.007,0.0946,-0.0292,-0.0347,-0.003,0.0424,-0.0549,-0.0171,0.0107,0.0114,0.026,0.0198,0.0078,-0.0117,-0.0507,-0.0467,-0.2249,-0.0275,0.0002,-0.0071,0.0598,-0.1145,0.0572,0.0387,0.0296,0.0883,0.0332,-0.0453,-0.0289,0.0277,-0.01,0.0575,-0.0129,0.0239,-0.0138,0.0596,-0.034,0.0209,0.0247,-0.0881,0.0605,-0.0038,0.2115,0.0434,0.0137,-0.039,0.0116,0.0001,-0.0361,-0.0882,0.0538,0.0118,0.05,-0.0305,-0.0152,-0.0157,0.0049,0.0271,-0.0093,-0.119,-0.0123,-0.0066,-0.0138,0.0182,-0.0658,0.0198,0.0318,-0.0504,0.0423,-0.0465,-0.0003,-0.0142,-0.1446,0.0201,-0.0338,0.0181,0.0307,-0.0635,0.0462,-0.061,0.0294,0.0347,-0.0025,-0.0246,0.0593,-0.0176,-0.0217,0.1117,0.0215,-0.0568,0.011,0.026,0.057,-0.0113,-0.0632,-0.0142,0.0612,-0.037,-0.0172,0.0071,-0.0064,-0.0063,0.015,-0.0065,0.0799,-0.002,0.0042,0.0165,-0.0581,-0.0235,0.0452,0.0083,-0.2817,0.06,-0.001,0.0218,-0.0047,0.0304,0.0313,0.0297,-0.0068,-0.0243,0.0081,0.0241,0.0522,-0.0216,0.0031,0.0183,0.0762,-0.0503,0.0929,-0.0073,0.0488,0.0821,0.2437,-0.0473,0.0156,0.0166,-0.0003,-0.0599,0.0089,-0.0046,0.0468,0.01,0.1162,-0.0595,0.0418,0.0806,-0.0303,-0.0069,0.0052,-0.0193,-0.0192,-0.0103,-0.078,0.0021,0.0759,0.016,-0.0094,-0.0543,0.0205,0.0199,-0.0242,-0.0071,-0.0792,-0.016,0.0632,0.0116,-0.0387,-0.0331,-0.0196,-0.0204,0.0214,-0.0481,-0.0291,0.0011,-0.0016]}
{"key":"[Binarized Neural Networks] We introduce a method to train Binarized Neural Networks (BNNs) - neural networks with binary weights and activations at run-time and when computing the parameters' gradient at train-time. We conduct two sets of experiments, each based on a different framework, namely Torch7 and Theano, where we train BNNs on MNIST, CIFAR-10 and SVHN, and achieve nearly state-of-the-art results. During the forward pass, BNNs drastically reduce memory size and accesses, and replace most arithmetic operations with bit-wise operations, which might lead to a great increase in power-efficiency. Last but not least, we wrote a binary matrix multiplication GPU kernel with which it is possible to run our MNIST BNN 7 times faster than with an unoptimized GPU kernel, without suffering any loss in classification accuracy. The code for training and running our BNNs is available.","layer":3,"vector":[-0.0288,0.0387,0.0193,-0.0062,0.0264,0.0403,0.0075,0.0439,0.0228,-0.028,-0.0054,-0.0892,0.013,0.0182,0.0318,-0.0073,0.0006,0.025,-0.0263,0.0057,-0.0038,-0.0517,-0.0079,-0.0569,0.0192,-0.0147,-0.0451,-0.0702,-0.0669,-0.245,0.0293,0.0006,0.0745,-0.0713,-0.0025,-0.0063,-0.0148,0.0487,-0.0226,-0.0048,0.0265,0.0631,0.0128,-0.0232,-0.0223,-0.0557,-0.0306,-0.0471,0.0015,-0.0216,0.0366,-0.0513,0.0492,0.0309,0.0326,0.0039,0.0273,0.0371,0.0422,-0.0046,-0.0066,0.0673,-0.1532,0.0434,0.0524,-0.005,-0.0073,-0.0681,0.0109,0.0454,-0.0134,0.0388,0.0611,0.0115,-0.0119,0.0219,-0.0444,-0.0519,-0.0065,-0.0296,0.0048,-0.0235,-0.0255,-0.0285,0.0307,0.0015,-0.0,0.001,-0.0132,0.0053,-0.0674,-0.0183,-0.0354,0.0188,-0.0581,-0.0048,0.0377,0.0466,-0.089,0.2168,-0.0254,0.0109,0.0298,-0.035,0.0134,-0.0125,-0.0467,-0.0257,-0.0457,-0.0477,0.0118,-0.0496,0.0397,-0.0232,0.033,0.0259,0.0217,0.0424,-0.036,0.012,-0.0262,-0.0106,0.0148,-0.0137,0.0181,-0.0511,-0.0227,0.1189,-0.0033,0.0672,0.0319,-0.0035,-0.0499,-0.0198,0.0584,0.058,-0.013,-0.0125,-0.0109,0.0173,-0.053,-0.0183,0.0162,-0.0913,-0.0235,0.0731,-0.0247,0.045,0.0042,-0.0245,-0.0487,0.0194,-0.0372,-0.0441,0.0024,0.026,0.0598,0.0369,-0.0758,-0.0075,-0.0174,-0.0195,-0.0314,0.1173,0.0664,-0.056,-0.0245,-0.0064,0.018,-0.056,0.041,0.0157,-0.0344,0.0094,0.075,0.0592,-0.0426,-0.023,-0.0038,0.0336,0.0354,-0.0433,-0.0339,0.04,0.0371,-0.0318,0.0102,-0.0649,0.0119,0.0046,-0.0523,0.0202,-0.065,-0.0042,-0.0303,-0.0269,-0.0176,-0.0305,0.0114,-0.0186,0.0284,0.0046,-0.0285,0.0422,0.0026,0.0129,-0.01,-0.0242,0.0306,0.0346,-0.0004,-0.0238,0.0558,-0.0186,-0.0307,-0.0243,-0.0123,0.0629,-0.0016,0.0241,0.0407,-0.0667,-0.0762,-0.2137,0.0061,0.0039,-0.0355,0.108,-0.106,0.0639,0.011,0.0261,0.0689,0.0603,-0.0151,0.0091,0.0213,-0.0494,0.0535,0.0696,0.0337,0.0181,0.0112,-0.0062,0.0017,-0.0042,-0.0877,0.0219,0.0134,0.217,0.0138,0.1016,-0.0185,0.0089,0.0184,-0.0763,-0.0843,0.1269,0.007,0.0715,0.0086,-0.037,-0.0249,-0.0307,0.0487,-0.0023,-0.1375,-0.0138,0.0028,-0.0116,0.014,-0.033,0.0203,0.0012,-0.0479,0.0422,0.0,0.0292,-0.0854,-0.0931,0.0298,-0.0644,0.0337,0.0044,-0.1158,0.0167,-0.0423,0.0406,-0.0017,-0.0116,-0.0369,0.0242,0.0036,-0.003,0.0845,0.0341,-0.0332,0.0854,-0.0132,0.0272,-0.0199,-0.0027,-0.0115,0.0573,-0.0021,0.0686,0.0472,0.0214,0.0221,0.0664,-0.0106,0.0021,0.0044,-0.0567,0.0233,-0.0654,0.0271,0.0524,0.0043,-0.2985,0.08,0.0258,0.0145,-0.0165,-0.0211,0.0506,-0.0043,-0.0306,-0.0036,-0.0293,0.0456,0.0732,0.0066,0.0243,0.0418,0.0613,-0.0479,0.0187,-0.009,0.0295,0.0202,0.1982,-0.004,0.038,0.0231,0.0052,-0.0004,0.0631,0.0032,-0.0028,0.0047,0.068,-0.0599,0.0119,0.09,-0.0291,0.0532,0.0506,-0.027,-0.0138,0.0108,-0.0712,-0.0316,0.1075,-0.0202,-0.0083,-0.0311,-0.0094,0.0162,-0.0173,0.0394,0.0446,0.0152,0.0139,-0.0092,-0.0469,-0.0303,-0.0739,-0.0678,0.0258,-0.0898,0.0341,0.0087,-0.0431]}
{"key":"[Multi-Agent Distributed Lifelong Learning for Collective Knowledge Acquisition] Lifelong machine learning methods acquire knowledge over a series of consecutive tasks, continually building upon their experience. Current lifelong learning algorithms rely upon a single learning agent that has centralized access to all data. In this paper, we extend the idea of lifelong learning from a single agent to a network of multiple agents that collectively learn a series of tasks. Each agent faces some (potentially unique) set of tasks; the key idea is that knowledge learned from these tasks may benefit other agents trying to learn different (but related) tasks. Our Collective Lifelong Learning Algorithm (CoLLA) provides an efficient way for a network of agents to share their learned knowledge in a distributed and decentralized manner, while preserving the privacy of the locally observed data. Note that a decentralized scheme is a subclass of distributed algorithms where a central server does not exist and in addition to data, computations are also distributed among the agents. We provide theoretical guarantees for robust performance of the algorithm and empirically demonstrate that CoLLA outperforms existing approaches for distributed multi-task learning on a variety of data sets.","layer":7,"vector":[-0.0283,-0.0063,0.0216,-0.0131,0.0005,0.0378,0.015,0.0126,0.0343,-0.0447,0.0226,-0.0516,0.0895,0.0587,-0.0036,0.0397,-0.0422,0.0507,-0.0092,-0.0378,-0.0055,-0.0495,-0.0051,-0.0459,-0.0012,0.0293,-0.0659,-0.0779,-0.0585,-0.204,0.051,-0.0427,0.013,-0.0089,-0.0076,-0.0183,-0.0416,0.0311,-0.0334,0.0526,0.0525,0.0125,-0.0086,-0.0688,-0.0263,-0.0243,-0.0118,-0.0238,-0.0009,-0.016,0.0108,-0.0251,0.0305,0.0416,0.0141,0.0662,-0.0004,0.0639,0.0409,0.0203,0.0468,0.0494,-0.1682,0.0685,0.0268,0.0781,-0.0234,0.0072,0.0497,-0.0023,0.012,0.059,0.0447,0.0454,0.045,0.022,0.0013,-0.03,-0.0084,0.0192,-0.0302,-0.009,-0.015,-0.0384,-0.0514,-0.0093,0.0163,-0.0786,0.0457,0.0231,-0.033,0.0062,0.0068,0.0114,-0.0462,-0.063,0.0263,0.0529,-0.0472,0.2016,-0.0513,0.0164,0.0376,-0.0528,0.0034,-0.005,-0.0244,-0.0303,-0.0035,0.0012,-0.0397,0.0021,0.0124,-0.0441,0.017,0.0186,0.0724,0.0544,-0.0303,-0.0132,-0.0115,0.0148,0.0585,0.0074,0.0487,-0.0666,0.003,0.156,-0.0084,0.0271,0.0382,-0.0311,-0.0329,-0.0449,0.0327,0.0404,0.0252,-0.0098,0.0269,-0.0224,-0.0275,-0.0426,0.0181,-0.0567,-0.0823,0.1139,0.035,0.0514,-0.0157,-0.021,-0.0368,-0.0052,0.0246,0.0048,0.0055,0.0234,0.0777,0.0255,-0.0613,-0.0177,-0.038,-0.0479,0.0056,0.1305,0.0122,-0.1003,-0.0355,-0.0024,0.0232,-0.0051,0.0369,0.0506,-0.0249,0.0531,0.0376,0.0389,-0.0586,0.0063,0.0229,-0.0429,-0.0005,-0.0491,-0.0101,0.0508,0.0256,-0.0262,-0.0191,-0.0404,0.008,0.045,-0.0148,0.059,-0.0165,0.0166,-0.0291,-0.0005,0.0198,-0.0018,0.0193,-0.0383,-0.0139,0.002,-0.0435,-0.0159,-0.0085,0.0285,0.0282,0.0149,0.0613,-0.0035,-0.0306,-0.0438,0.0597,-0.0323,-0.0642,0.0144,0.0242,0.0262,0.0037,-0.0078,0.0629,0.0131,-0.0729,-0.2203,-0.0136,-0.0002,-0.0391,0.0534,-0.0137,0.0798,-0.0362,0.0183,0.0402,0.0819,-0.0214,-0.0333,0.0089,-0.0292,0.0446,0.0593,0.0518,-0.0317,0.0295,0.0133,-0.0268,0.0067,-0.0967,0.0613,0.0138,0.2469,0.0087,0.0067,-0.0346,0.0037,0.0259,-0.056,-0.1261,0.0314,0.0007,0.0003,-0.0032,-0.0096,-0.0179,0.0051,0.0422,-0.026,-0.1009,-0.0671,-0.0488,-0.0356,0.0274,-0.0341,-0.0099,0.0409,-0.0213,0.0129,-0.0512,-0.0383,-0.0661,-0.0615,0.0331,-0.0575,0.0257,-0.0024,-0.0074,0.0068,-0.0841,0.0968,-0.0296,-0.009,-0.0435,0.0654,-0.0456,-0.0121,0.0735,-0.0135,-0.035,-0.0076,-0.0046,0.0237,-0.0477,-0.0425,0.0054,0.08,-0.0863,0.0297,0.0371,0.0541,0.0046,0.0664,0.0073,0.0228,-0.0205,0.0634,0.021,-0.0727,-0.0141,0.0359,-0.0081,-0.3072,0.0268,-0.0299,0.0168,-0.0448,0.0254,0.048,0.003,-0.0688,0.0146,0.0139,0.0993,0.0137,0.0023,0.0365,0.0429,0.0542,-0.0159,-0.0179,-0.0426,0.0383,0.0351,0.2281,-0.0418,0.048,0.0342,-0.0179,-0.0172,-0.0089,-0.0214,-0.0227,-0.0048,0.0833,-0.0712,0.0366,0.0754,-0.0442,0.0392,0.0375,-0.0029,-0.0396,-0.025,-0.002,-0.0104,0.0985,0.0203,-0.0168,-0.0729,-0.0652,0.0221,0.0108,-0.0213,-0.0167,-0.0296,0.0413,-0.0043,-0.0061,-0.0404,-0.0196,-0.0111,-0.0331,-0.0488,0.0088,-0.0057,-0.0112]}
{"key":"[Expanding boundaries of Gap Safe screening] Sparse optimization problems are ubiquitous in many fields such as statistics, signal/image processing and machine learning. This has led to the birth of many iterative algorithms to solve them. A powerful strategy to boost the performance of these algorithms is known as safe screening: it allows the early identification of zero coordinates in the solution, which can then be eliminated to reduce the problem's size and accelerate convergence. In this work, we extend the existing Gap Safe screening framework by relaxing the global strong-concavity assumption on the dual cost function. Instead, we exploit local regularity properties, that is, strong concavity on well-chosen subsets of the domain. The non-negativity constraint is also integrated to the existing framework. Besides making safe screening possible to a broader class of functions that includes beta-divergences (e.g., the Kullback-Leibler divergence), the proposed approach also improves upon the existing Gap Safe screening rules on previously applicable cases (e.g., logistic regression). The proposed general framework is exemplified by some notable particular cases: logistic function, beta = 1.5 and Kullback-Leibler divergences. Finally, we showcase the effectiveness of the proposed screening rules with different solvers (coordinate descent, multiplicative-update and proximal gradient algorithms) and different data sets (binary classification, hyperspectral and count data).","layer":3,"vector":[-0.0193,-0.0139,0.0147,-0.0343,0.0558,0.0308,-0.0167,0.0139,0.0228,-0.0325,0.0017,-0.0549,0.0072,0.0638,0.0071,0.0493,0.0342,0.0447,-0.0465,0.0058,0.0446,-0.0311,-0.0111,-0.0831,0.0588,0.0136,-0.019,-0.0592,-0.0304,-0.2938,0.002,-0.0041,0.0305,-0.0432,0.0253,0.0108,-0.0208,0.054,-0.0435,0.0253,-0.0014,0.0067,-0.012,-0.0556,-0.0101,-0.0616,-0.009,-0.0408,-0.0473,-0.0365,0.0264,-0.0468,0.012,0.0326,0.0016,0.0058,0.0505,0.0173,0.037,0.0517,0.0339,0.0273,-0.1509,0.0141,0.0759,0.0191,-0.0261,-0.0475,0.0356,0.0807,-0.0138,0.0544,0.0433,0.0547,-0.0045,0.006,0.0109,-0.0111,-0.0312,0.0238,0.0702,-0.0106,-0.0528,0.0001,0.0058,-0.0328,0.0282,-0.0505,0.0208,0.0016,-0.0005,-0.0283,-0.0313,0.0414,-0.085,-0.035,0.0633,0.0339,-0.0639,0.1959,-0.0556,0.0646,0.0059,-0.0093,0.0227,-0.0344,-0.0453,-0.0046,-0.0058,-0.0086,0.0153,-0.0034,0.0504,-0.0225,-0.0307,0.0208,0.0365,0.0022,0.0335,-0.0075,-0.025,0.024,0.0755,0.0047,0.0884,-0.05,0.002,0.1309,0.0521,0.0516,0.0084,-0.0399,-0.0192,-0.0171,-0.0139,-0.0109,-0.0403,0.0429,0.0254,-0.0112,-0.0562,-0.0688,0.0312,-0.0898,0.0042,0.1059,-0.0682,0.0468,-0.071,-0.0747,-0.0013,0.0125,-0.0046,-0.0215,0.0301,0.0006,0.0139,0.0162,-0.0374,0.04,-0.0034,-0.0899,-0.0167,0.0962,-0.0159,-0.0739,-0.0297,-0.015,0.0061,0.0023,0.0099,0.0304,0.005,0.0394,0.0692,0.0143,-0.0781,0.0184,0.004,-0.0065,-0.0349,-0.0432,-0.0577,0.0295,0.0446,-0.0578,0.0172,-0.0253,0.0455,0.0398,-0.0768,-0.0393,-0.0234,-0.0207,0.0079,-0.0156,-0.0209,-0.0001,0.0355,0.0055,0.0175,0.0206,-0.0494,0.051,0.0331,0.0462,0.0098,-0.0179,0.0047,0.0605,-0.0689,-0.0227,0.0484,-0.0067,-0.011,-0.0157,0.0414,0.026,0.0128,0.0537,0.042,-0.0277,-0.0812,-0.2603,-0.0389,-0.01,0.0168,0.0069,-0.0691,0.0591,-0.0105,0.0445,0.0872,0.0559,-0.0179,-0.074,0.063,-0.027,0.0312,0.0381,0.0125,-0.026,0.0069,-0.0266,0.0135,-0.0101,-0.0669,0.0514,0.0121,0.2061,0.0452,0.0218,-0.0175,-0.0146,-0.021,-0.0013,-0.0585,0.0638,0.0132,0.0442,-0.0274,-0.0727,-0.0195,-0.0041,0.045,-0.024,-0.0621,-0.0656,-0.0294,-0.0439,0.0442,-0.0308,0.0144,0.0567,-0.0003,0.0313,-0.0389,0.0296,-0.0103,-0.0555,0.0313,-0.0333,0.0292,0.019,-0.03,-0.0039,-0.041,0.0527,0.0213,0.0039,-0.032,0.0334,-0.0181,-0.0097,0.0739,-0.0034,-0.0237,0.0525,0.0377,0.0573,0.0056,-0.0321,-0.0304,0.0716,-0.012,0.0396,-0.0261,0.0417,0.0164,0.1186,-0.0065,-0.0031,-0.0503,-0.0097,0.004,-0.0742,0.0076,0.0601,0.0192,-0.2918,0.0038,-0.0199,-0.0108,-0.0249,-0.008,0.0725,0.0128,-0.0552,0.0013,-0.0109,0.0387,0.0109,-0.0232,0.0031,0.011,0.0719,-0.0375,0.036,-0.0874,-0.0131,0.0276,0.2234,-0.043,0.0248,0.0647,-0.01,-0.0202,-0.0036,-0.0412,0.0216,0.0141,0.046,-0.0942,0.0235,0.104,-0.0068,0.0394,0.0291,-0.0705,0.0348,0.0156,-0.051,-0.0064,0.0921,-0.0387,-0.026,-0.0337,0.0105,0.0171,-0.0512,0.0292,0.0187,0.0191,-0.0077,0.0173,-0.0484,-0.0175,-0.0429,-0.0387,0.0078,-0.0311,-0.0487,0.0187,0.0051]}
{"key":"[Classical and Quantum Algorithms for Tensor Principal Component Analysis] We present classical and quantum algorithms based on spectral methods for a problem in tensor principal component analysis. The quantum algorithm achieves a quartic speedup while using exponentially smaller space than the fastest classical spectral algorithm, and a super-polynomial speedup over classical algorithms that use only polynomial space. The classical algorithms that we present are related to, but slightly different from those presented recently in Ref. 1. In particular, we have an improved threshold for recovery and the algorithms we present work for both even and odd order tensors. These results suggest that large-scale inference problems are a promising future application for quantum computers.","layer":1,"vector":[-0.0989,0.0219,0.0067,0.0065,-0.0058,0.0507,0.0362,-0.0049,0.0528,0.0034,0.044,-0.0837,0.0145,0.0374,0.0408,0.0188,-0.0004,0.0679,-0.0424,0.0048,0.0317,-0.0496,-0.0309,-0.047,0.002,-0.0215,-0.017,-0.0266,-0.0375,-0.2025,0.0056,-0.0309,0.0756,-0.0369,0.0136,-0.0287,-0.0482,0.0553,-0.0517,0.0185,0.0194,0.03,-0.0217,0.0204,0.0019,-0.0325,-0.0375,0.0131,-0.0399,-0.0289,-0.0016,0.0248,0.0176,0.0349,0.0677,0.006,0.0394,0.0131,0.0461,0.0318,0.0138,0.0333,-0.1597,0.0566,0.074,0.0225,0.0106,-0.0385,0.0077,0.0458,-0.0527,0.0874,-0.0254,-0.0116,0.0242,-0.038,0.0382,-0.0047,-0.0047,0.0246,0.0059,-0.0386,-0.0449,-0.0019,-0.0517,0.0002,-0.009,-0.0219,0.0291,-0.0018,-0.0183,-0.0747,-0.0065,0.0101,-0.0167,-0.0254,0.0276,0.0421,0.002,0.2043,-0.0779,0.0056,0.0105,-0.005,0.0071,-0.0815,-0.0252,-0.027,-0.0287,-0.0041,0.022,-0.0193,0.0356,-0.0652,0.0078,-0.035,0.0665,0.0293,-0.0293,0.0149,-0.0459,-0.0181,0.0361,-0.025,-0.0127,-0.0764,-0.0125,0.1458,0.0282,0.0906,0.0405,-0.0051,-0.0189,-0.0603,0.0103,0.0181,0.0567,0.0109,0.0451,0.0342,-0.0577,-0.082,0.008,-0.0645,-0.0162,0.133,-0.0405,-0.0057,-0.0369,0.0258,0.0139,0.0403,-0.0124,-0.0228,0.0115,0.0371,0.0082,0.0244,-0.0561,0.0194,-0.0449,-0.082,-0.002,0.104,0.0299,-0.0736,-0.0366,-0.0091,0.0305,-0.0004,0.0791,0.069,-0.0346,0.024,0.0828,-0.0329,-0.0636,0.0159,0.0158,0.0498,0.0068,-0.0483,-0.045,0.0417,0.0432,-0.051,0.0042,-0.0079,-0.0128,-0.0335,-0.0403,0.0014,-0.077,-0.0463,-0.058,-0.0119,-0.0137,-0.0041,-0.027,-0.0364,0.0407,0.014,-0.0397,0.0222,-0.0141,0.0236,-0.0096,0.0086,0.0128,0.0382,0.0054,-0.0236,0.0192,-0.0222,-0.031,-0.0157,0.0517,0.0238,-0.0071,0.0672,0.0767,-0.0961,-0.1069,-0.2259,-0.0209,-0.0059,-0.0036,0.0542,-0.0878,0.0566,0.0032,0.0737,0.086,0.0298,0.0575,-0.0135,0.0137,-0.0218,0.0704,0.0511,0.0107,0.0017,-0.0029,-0.028,0.0552,-0.0342,-0.0448,0.0284,-0.0107,0.2002,0.0592,0.0331,0.0006,0.011,0.0538,-0.0428,-0.0828,0.0949,0.0106,0.0618,0.0363,-0.0224,-0.009,-0.0177,0.0332,0.0028,-0.0499,-0.0262,-0.0137,-0.0099,0.0003,-0.0147,0.021,0.0496,-0.0586,0.0133,-0.0489,-0.032,-0.0692,-0.0557,0.0067,-0.0304,0.0112,0.0139,-0.0678,0.0209,-0.0487,0.0542,-0.0183,-0.0244,-0.0204,0.065,-0.0242,-0.0527,0.0697,-0.0304,-0.0085,0.1016,-0.0271,0.0779,-0.0212,0.0032,0.0286,0.0749,-0.0508,0.0281,0.002,0.0114,-0.0062,0.0871,-0.0092,-0.0072,-0.0292,-0.0103,0.0337,-0.0491,0.0299,0.012,0.0039,-0.3035,0.0322,0.0269,0.0143,-0.0385,-0.0017,0.0231,0.0224,-0.0571,-0.0444,-0.0063,0.0431,0.0701,-0.0315,-0.0118,0.0498,0.0686,-0.0336,0.0382,-0.0484,0.0047,0.029,0.2568,-0.0105,0.0278,0.0126,0.0124,0.0108,0.0281,-0.0555,-0.0193,0.006,0.0868,-0.0702,0.0707,0.0347,-0.0446,0.0401,0.02,-0.026,0.0171,-0.0132,-0.0381,-0.0467,0.0968,-0.0251,-0.0372,-0.0406,0.022,0.004,0.0183,0.023,-0.0042,-0.0075,-0.0129,0.0257,-0.0132,-0.0564,-0.0201,-0.0219,-0.0226,-0.0735,-0.0355,-0.0189,-0.0209]}
{"key":"[Distributed Cooperative Multi-Agent Reinforcement Learning with Directed Coordination Graph] Existing distributed cooperative multi-agent reinforcement learning (MARL) frameworks usually assume undirected coordination graphs and communication graphs while estimating a global reward via consensus algorithms for policy evaluation. Such a framework may induce expensive communication costs and exhibit poor scalability due to requirement of global consensus. In this work, we study MARLs with directed coordination graphs, and propose a distributed RL algorithm where the local policy evaluations are based on local value functions. The local value function of each agent is obtained by local communication with its neighbors through a directed learning-induced communication graph, without using any consensus algorithm. A zeroth-order optimization (ZOO) approach based on parameter perturbation is employed to achieve gradient estimation. By comparing with existing ZOO-based RL algorithms, we show that our proposed distributed RL algorithm guarantees high scalability. A distributed resource allocation example is shown to illustrate the effectiveness of our algorithm.","layer":1,"vector":[-0.0393,-0.0151,0.0173,-0.0267,-0.0179,0.0601,-0.0087,0.068,0.0488,-0.0117,0.0812,-0.0442,0.0333,0.0753,0.0235,-0.0168,-0.0507,0.0364,-0.0152,-0.0261,0.0096,-0.0366,-0.0332,-0.0899,0.0366,0.0264,-0.0812,-0.053,-0.0497,-0.2277,0.0572,-0.0303,0.0116,0.004,-0.0155,0.0026,-0.0081,0.0121,-0.0458,0.038,0.0147,0.021,-0.0107,-0.0566,-0.0173,-0.0354,-0.0169,-0.0475,-0.0193,-0.0385,0.0603,-0.0344,0.003,0.0005,0.0184,0.0597,0.0441,0.0601,0.0352,0.0388,0.0054,0.04,-0.1888,0.0636,0.067,0.0466,-0.0016,-0.0024,0.0311,0.0291,-0.0018,0.0097,0.0656,-0.0084,0.0362,-0.0211,0.0153,-0.0383,0.0315,-0.0391,0.0057,-0.0461,-0.0507,-0.0253,-0.005,-0.0434,0.0599,-0.038,0.0282,0.0182,-0.0159,0.034,0.0047,-0.0103,-0.085,0.0028,-0.0217,0.0002,-0.0703,0.1907,-0.018,0.0585,0.0264,-0.026,0.039,-0.03,0.0099,-0.0166,-0.0033,0.0134,-0.0457,-0.0213,0.0312,-0.0276,-0.0028,0.0345,0.0591,0.0339,0.0109,-0.0074,-0.0117,-0.0134,0.0581,0.0163,0.0589,-0.085,0.0108,0.15,-0.0086,0.0384,0.0151,-0.0134,0.0208,-0.0092,0.0063,0.015,0.0262,-0.0141,0.0279,0.0045,-0.0188,-0.0162,-0.0216,-0.1123,-0.0702,0.1055,0.0122,0.0445,-0.0671,-0.014,-0.034,-0.0216,0.0203,0.0403,-0.0205,-0.0065,0.054,0.0601,-0.0446,0.0064,-0.0374,-0.0391,0.0016,0.1151,-0.0018,-0.1086,-0.0561,-0.0474,0.0425,-0.0227,0.0065,0.0604,-0.0535,0.0179,0.0811,0.0201,-0.0701,-0.0096,0.0232,-0.0242,0.0374,-0.0222,-0.0107,0.0274,0.0348,-0.0018,-0.0161,-0.0636,0.0207,-0.0103,-0.0588,0.0023,0.0079,0.0003,-0.0438,-0.0384,0.0308,-0.0271,-0.0005,0.0033,0.046,-0.021,-0.0598,0.0066,-0.028,0.0031,-0.0043,0.0093,-0.004,0.014,-0.0454,-0.0168,0.0509,-0.0187,-0.0786,0.0525,0.0349,0.0167,0.0104,0.0216,0.0492,0.0158,-0.0424,-0.2061,-0.0103,0.0016,-0.041,0.0612,-0.0568,0.0785,-0.0196,0.0387,0.075,0.1004,0.014,-0.0177,0.0385,0.0119,0.0777,0.0433,0.0212,0.0224,-0.0135,0.0234,0.0005,-0.0025,-0.0915,0.069,-0.0125,0.2031,-0.0192,0.0147,-0.0193,0.0436,0.05,-0.0414,-0.0875,0.0013,0.03,0.0121,-0.0423,-0.0357,-0.0222,0.0057,0.0399,-0.0061,-0.1151,-0.0346,-0.0222,-0.0393,0.0688,-0.072,-0.0395,0.0543,0.0143,-0.006,-0.0238,-0.0273,-0.0473,-0.0535,0.0283,-0.0382,0.0162,0.0047,0.0038,0.0097,-0.0435,0.1011,-0.0213,0.0105,-0.0561,0.0404,0.0088,-0.0125,0.0487,0.0331,0.0218,-0.0063,0.0174,0.0192,-0.0285,-0.0328,-0.0105,0.0455,-0.0446,0.0243,0.0322,0.0462,-0.0258,0.0434,0.0156,0.031,-0.0081,0.0162,-0.0074,-0.0733,-0.0056,0.0591,-0.0298,-0.2933,0.0464,0.0186,0.0224,-0.0522,0.0067,0.0667,-0.0072,-0.0924,0.0058,0.0498,0.077,0.0093,0.0081,0.0296,0.0296,0.0476,-0.0336,0.0208,-0.0919,0.0215,0.0538,0.2469,-0.0239,0.0548,0.016,-0.0286,-0.0297,-0.0088,-0.0324,-0.0174,0.0139,0.0577,-0.0715,0.0509,0.0545,-0.0387,0.0146,0.033,-0.0055,-0.0319,0.033,0.0528,-0.0192,0.1041,0.0376,-0.0413,-0.083,-0.0398,0.0463,-0.0335,0.0088,0.0052,-0.0441,0.0269,0.0208,-0.0031,-0.1094,-0.0614,-0.0226,-0.0104,-0.0536,0.0028,-0.0307,-0.0081]}
{"key":"[Causal Analysis and Classification of Traffic Crash Injury Severity Using Machine Learning Algorithms] Causal analysis and classification of injury severity applying non-parametric methods for traffic crashes has received limited attention. This study presents a methodological framework for causal inference, using Granger causality analysis, and injury severity classification of traffic crashes, occurring on interstates, with different machine learning techniques including decision trees (DT), random forest (RF), extreme gradient boosting (XGBoost), and deep neural network (DNN). The data used in this study were obtained for traffic crashes on all interstates across the state of Texas from a period of six years between 2014 and 2019. The output of the proposed severity classification approach includes three classes for fatal and severe injury (KA) crashes, non-severe and possible injury (BC) crashes, and property damage only (PDO) crashes. While Granger Causality helped identify the most influential factors affecting crash severity, the learning-based models predicted the severity classes with varying performance. The results of Granger causality analysis identified the speed limit, surface and weather conditions, traffic volume, presence of workzones, workers in workzones, and high occupancy vehicle (HOV) lanes, among others, as the most important factors affecting crash severity. The prediction performance of the classifiers yielded varying results across the different classes. Specifically, while decision tree and random forest classifiers provided the greatest performance for PDO and BC severities, respectively, for the KA class, the rarest class in the data, deep neural net classifier performed superior than all other algorithms, most likely due to its capability of approximating nonlinear models. This study contributes to the limited body of knowledge pertaining to causal analysis and classification prediction of traffic crash injury severity using non-parametric approaches.","layer":2,"vector":[-0.0382,-0.0112,0.0201,-0.0249,0.0543,0.0376,0.0608,0.0449,0.0598,-0.0266,0.0017,-0.0136,0.0135,0.066,-0.0333,0.0142,0.022,0.0465,-0.058,-0.0125,0.0009,-0.0133,-0.0065,-0.0353,0.0041,0.0436,-0.0272,0.0376,-0.0639,-0.2194,-0.0012,-0.0764,0.0671,-0.0134,-0.0231,-0.0221,-0.0375,0.0724,0.0502,0.0349,0.0179,0.0282,-0.0444,-0.044,0.0086,-0.0595,0.0218,0.0093,0.0013,-0.0465,0.0211,-0.0416,0.0557,0.0364,0.0297,0.0095,0.0182,0.0223,0.0385,0.0376,0.0068,-0.0082,-0.2426,0.0169,0.0576,0.0339,-0.0693,0.0086,0.0326,0.0777,-0.023,0.0291,0.0228,0.0141,0.0032,0.019,0.0319,-0.0222,-0.0062,0.017,0.0458,-0.0315,-0.035,-0.0002,-0.0199,-0.0568,-0.0075,-0.0642,0.043,0.0079,-0.0319,-0.006,-0.0329,0.0219,-0.0375,-0.0192,0.0515,-0.0079,-0.0443,0.1751,-0.0701,0.0178,0.0204,-0.0271,-0.0076,-0.0024,-0.0237,-0.062,-0.066,0.0141,0.0461,-0.0048,0.0328,0.0126,-0.0155,-0.0042,0.0625,0.073,0.0066,0.0211,-0.0137,0.005,0.013,-0.0245,-0.0086,-0.087,0.0044,0.1438,0.0521,-0.0058,0.0224,-0.0349,-0.0512,-0.0001,0.0124,-0.0085,0.0531,0.0194,-0.0203,-0.0193,-0.0077,-0.0655,0.0446,-0.0807,-0.0678,0.1025,-0.0654,-0.0222,-0.0562,-0.0224,-0.0236,0.0177,-0.0775,-0.0213,0.0332,0.0433,0.0441,0.0226,-0.0277,0.0445,-0.0102,-0.0645,-0.0269,0.1052,-0.0093,-0.0854,-0.0252,0.0433,-0.0022,-0.0185,0.0177,0.0532,-0.0148,0.0144,0.0943,0.0398,-0.0232,0.0104,-0.0303,0.0171,0.0512,-0.0315,-0.0675,0.0548,0.0621,0.0247,-0.0353,-0.0385,0.0162,0.0334,-0.0505,-0.0375,-0.0284,-0.0223,-0.0217,0.0159,0.0061,0.0068,0.0421,-0.003,0.0124,-0.0152,-0.006,0.0223,-0.027,0.0139,-0.0375,0.0178,0.0383,0.0163,-0.0085,0.0077,0.0649,-0.0141,-0.0636,0.0202,-0.0017,0.0588,-0.0079,0.0608,0.0489,-0.0386,-0.0557,-0.2018,-0.06,0.0153,-0.0011,0.0271,-0.047,0.0089,0.0171,0.0202,0.0816,0.0621,0.0023,-0.06,-0.0319,-0.0044,0.0463,-0.0047,0.0324,-0.0669,-0.0034,-0.0187,0.0228,-0.0008,-0.0943,0.0426,0.0004,0.1711,-0.0141,0.0531,-0.0194,0.0314,0.014,0.0034,-0.1285,0.0855,0.0011,0.0571,0.0065,-0.0958,-0.0373,-0.0283,0.0135,-0.0198,-0.1104,-0.0322,-0.0325,-0.0409,0.0489,-0.0176,0.0324,0.0097,-0.005,0.0449,0.0291,0.0334,-0.0429,-0.0772,0.0545,-0.049,-0.0502,0.0173,-0.0712,0.034,-0.0805,0.0459,0.0177,-0.0389,-0.0528,-0.0304,-0.0082,-0.0388,0.1394,0.008,-0.0336,0.0503,0.0287,0.0213,-0.0037,-0.0268,0.0119,0.0951,-0.0191,0.0521,0.0119,0.0229,0.0097,0.0803,-0.0025,0.0217,-0.0197,-0.0064,0.0008,-0.0546,-0.017,0.0319,0.0157,-0.2664,0.0517,-0.0169,-0.0372,-0.0216,-0.0408,0.0412,0.0233,-0.004,-0.0232,0.0292,0.0495,0.0609,-0.0274,-0.0149,0.0204,0.0726,-0.0072,0.0169,-0.0396,0.0059,0.0675,0.2372,-0.0351,0.0564,0.0621,-0.0532,-0.0158,0.0079,-0.0766,0.0296,0.0201,0.105,-0.1097,0.0343,0.0515,-0.0322,0.0429,0.0492,-0.0298,0.0232,0.0082,-0.0273,-0.0419,0.1036,0.0228,-0.0435,-0.0797,0.0221,0.0485,-0.0115,-0.0163,-0.0569,-0.0123,0.042,0.0285,-0.0329,-0.0073,-0.051,-0.0518,-0.0113,-0.0289,-0.0147,0.0029,0.004]}
{"key":"[A DIRT-T Approach to Unsupervised Domain Adaptation] Domain adaptation refers to the problem of leveraging labeled data in a source domain to learn an accurate model in a target domain where labels are scarce or unavailable. A recent approach for finding a common representation of the two domains is via domain adversarial training (Ganin & Lempitsky, 2015), which attempts to induce a feature extractor that matches the source and target feature distributions in some feature space. However, domain adversarial training faces two critical limitations: 1) if the feature extraction function has high-capacity, then feature distribution matching is a weak constraint, 2) in non-conservative domain adaptation (where no single classifier can perform well in both the source and target domains), training the model to do well on the source domain hurts performance on the target domain. In this paper, we address these issues through the lens of the cluster assumption, i.e., decision boundaries should not cross high-density data regions. We propose two novel and related models: 1) the Virtual Adversarial Domain Adaptation (VADA) model, which combines domain adversarial training with a penalty term that punishes the violation the cluster assumption; 2) the Decision-boundary Iterative Refinement Training with a Teacher (DIRT-T) model, which takes the VADA model as initialization and employs natural gradient steps to further minimize the cluster assumption violation. Extensive empirical results demonstrate that the combination of these two models significantly improve the state-of-the-art performance on the digit, traffic sign, and Wi-Fi recognition domain adaptation benchmarks.","layer":3,"vector":[-0.0565,-0.0576,0.0293,-0.0123,0.0362,0.0365,0.0264,-0.0038,-0.0101,-0.0049,0.005,-0.0184,-0.0063,0.022,0.0202,0.0416,0.0114,0.0687,-0.0389,0.0202,0.025,0.0096,0.0083,-0.0063,0.0111,0.0221,-0.0034,-0.0022,-0.0492,-0.255,0.0462,-0.01,0.0189,0.0108,0.0267,-0.0297,-0.0502,0.0557,0.0133,0.0304,-0.0259,0.0042,-0.0423,-0.0889,-0.0334,-0.043,-0.0286,-0.0063,-0.0167,-0.0732,0.0688,-0.0377,0.0036,-0.0002,0.0186,0.0096,0.06,0.013,0.0619,0.0465,0.0223,0.0858,-0.1555,0.0707,0.0522,0.0134,-0.032,0.0034,-0.0052,0.042,0.0274,0.0898,-0.0142,0.018,-0.011,0.0468,-0.0034,-0.0284,-0.0139,0.0092,0.0497,0.0021,-0.0674,0.0262,0.0028,-0.0517,-0.0137,-0.0515,0.0467,0.0057,-0.103,-0.0359,-0.0307,0.0125,-0.0697,-0.0346,0.0235,0.0115,-0.0543,0.1895,-0.0148,0.0151,0.001,-0.0475,0.0272,0.0163,-0.0193,-0.0168,0.004,0.0124,-0.0184,0.0006,-0.0257,0.0015,0.0232,-0.0099,0.0808,0.0198,-0.0249,-0.0287,-0.0443,-0.0207,0.0561,-0.0303,0.0278,-0.0224,0.035,0.1434,0.0065,0.0352,0.0304,-0.0401,-0.058,-0.0418,0.011,0.0465,0.0191,0.0315,-0.0183,0.0051,-0.0415,-0.087,0.0458,-0.0056,-0.0201,0.1029,-0.0492,0.0153,-0.0595,-0.0198,0.0025,0.0233,-0.0525,-0.0292,-0.0094,0.0265,0.0411,0.0312,-0.0191,0.0181,0.0074,-0.0388,-0.0323,0.0939,0.0013,-0.0939,-0.0658,-0.0111,-0.0043,-0.0595,0.022,0.0241,-0.0194,0.0299,0.0809,0.0038,-0.1093,-0.0162,-0.022,0.0238,-0.0168,-0.0496,-0.0675,0.0355,0.067,-0.0379,-0.0211,-0.0445,0.0192,0.0608,-0.0465,0.0036,-0.0426,-0.0199,-0.0067,-0.0458,-0.0431,0.0159,0.0066,-0.0131,-0.0023,0.0505,-0.0404,-0.0077,-0.0412,0.0081,-0.0019,-0.0608,0.0537,0.0348,-0.0113,0.0398,0.0394,-0.0311,-0.001,-0.009,0.0187,0.0701,0.0141,0.046,0.0365,0.0185,-0.0454,-0.2426,-0.0047,-0.0199,-0.0227,0.0524,-0.059,0.0343,0.0404,0.0762,0.0493,0.054,-0.037,-0.0513,0.0415,-0.0336,0.0447,0.0475,0.0254,0.0028,-0.0002,-0.0164,0.0347,-0.0017,-0.087,0.0408,0.0433,0.2039,-0.006,0.0401,-0.0187,0.0522,0.0392,0.01,-0.129,0.0332,0.0183,0.0911,0.0011,-0.0423,0.0046,-0.0012,0.0246,0.046,-0.1282,-0.039,-0.0257,-0.0178,0.0245,-0.0567,0.0403,0.0769,-0.0224,0.0645,-0.0166,-0.0183,0.0153,-0.0855,0.0391,-0.0171,0.0067,-0.0303,-0.0583,0.0387,-0.0713,0.0659,0.0224,-0.0463,-0.0849,0.0366,-0.0243,-0.0197,0.0496,0.0512,0.0108,0.0215,-0.0237,0.0254,-0.0219,-0.0757,-0.0156,0.0603,-0.0211,0.0367,0.0368,0.0293,0.0266,0.1147,-0.0252,-0.004,-0.012,-0.0247,0.0196,-0.0313,-0.0424,0.0359,0.0153,-0.2929,0.0073,0.0622,0.0487,-0.0149,0.0402,0.0428,0.05,-0.0545,0.0021,0.0033,0.008,0.0399,-0.024,0.0147,0.077,0.067,-0.0664,0.0029,-0.0804,0.0153,0.0392,0.1967,-0.0531,0.01,-0.0141,-0.0484,0.0073,0.0477,-0.0534,-0.0247,0.011,0.1059,-0.0101,0.0118,0.0766,-0.0437,-0.0103,0.002,0.0022,0.0146,0.0354,-0.0394,0.0213,0.0571,0.0258,0.005,-0.0515,-0.0178,0.0515,-0.0063,-0.0317,-0.0141,0.0119,0.0561,0.0223,-0.021,-0.069,-0.0583,-0.054,0.0364,-0.0316,-0.0387,0.0106,-0.0591]}
{"key":"[A Bayesian Network View on Acoustic Model-Based Techniques for Robust Speech Recognition] This article provides a unifying Bayesian network view on various approaches for acoustic model adaptation, missing feature, and uncertainty decoding that are well-known in the literature of robust automatic speech recognition. The representatives of these classes can often be deduced from a Bayesian network that extends the conventional hidden Markov models used in speech recognition. These extensions, in turn, can in many cases be motivated from an underlying observation model that relates clean and distorted feature vectors. By converting the observation models into a Bayesian network representation, we formulate the corresponding compensation rules leading to a unified view on known derivations as well as to new formulations for certain approaches. The generic Bayesian perspective provided in this contribution thus highlights structural differences and similarities between the analyzed approaches.","layer":1,"vector":[-0.0496,-0.0031,0.0046,-0.0118,0.0357,0.0402,0.0492,0.0115,0.0161,-0.021,0.0194,-0.067,0.0367,0.0338,0.0532,0.0097,0.0664,0.0829,0.0099,-0.0238,0.0268,-0.0267,0.0175,0.0119,0.0383,0.0405,-0.0245,0.0044,-0.0365,-0.2474,-0.0051,-0.0685,0.0696,-0.0341,-0.0164,-0.0127,-0.0614,0.0657,-0.0245,0.0737,0.0342,0.033,-0.0115,-0.083,-0.0359,-0.0541,-0.0002,-0.0337,-0.0257,-0.0654,0.0085,-0.0169,0.0149,0.0052,0.0346,0.0269,0.0724,0.0839,0.0201,0.0825,-0.0023,0.0503,-0.1982,0.0968,0.0247,0.0452,-0.0232,-0.0207,0.0108,0.0241,-0.0296,0.0172,-0.0152,0.0467,0.0173,-0.0616,0.0348,-0.0126,-0.0174,0.0095,0.031,-0.0238,-0.0162,-0.0135,-0.0337,-0.0322,0.0118,-0.0644,-0.0075,0.0115,-0.0836,0.0095,-0.0383,0.0296,-0.0417,-0.0078,0.0884,0.0314,-0.0261,0.1871,0.0318,0.0254,0.0197,-0.045,0.0603,-0.0422,-0.0414,-0.0359,-0.0206,0.0343,0.0183,0.0043,0.0057,-0.0357,0.0162,0.0182,0.0561,0.0091,-0.0119,-0.0394,-0.036,0.0195,0.0496,-0.0472,0.0381,-0.0477,0.0618,0.114,0.0495,0.0653,0.1085,-0.0447,-0.0019,-0.0443,0.0546,0.026,0.0369,0.0069,-0.0054,0.006,-0.0316,-0.1373,-0.0139,-0.0604,-0.0819,0.115,-0.0911,-0.0268,-0.0333,-0.03,0.0105,0.0244,0.0189,-0.0215,0.0373,-0.0033,0.0474,0.0408,-0.0562,-0.0025,0.0048,-0.0431,-0.0228,0.0701,-0.0039,-0.0776,-0.0922,0.0035,0.0123,0.0009,0.0674,0.0058,-0.0105,0.021,0.0598,0.04,-0.0694,0.0048,0.0323,0.0141,-0.0314,-0.0756,-0.023,0.0144,0.0302,-0.028,0.0043,-0.0576,0.0315,0.047,-0.0219,-0.0094,0.0064,0.0181,-0.0608,-0.0359,-0.0196,-0.0282,0.0154,-0.0514,-0.0072,-0.0015,-0.0445,0.0076,-0.0386,0.0548,-0.0297,0.0316,0.0311,0.0332,-0.005,0.0216,0.0687,-0.0548,-0.0274,-0.0118,0.0297,0.0193,0.0069,0.0075,0.0194,-0.0292,-0.0689,-0.211,0.0155,0.0253,0.0066,0.0611,-0.0583,-0.0126,-0.0139,0.0706,0.0453,0.0285,-0.01,-0.0269,0.0338,-0.0137,0.029,-0.0025,0.0053,-0.0202,0.0373,-0.0485,0.0039,-0.0464,-0.0698,0.0394,0.0026,0.1874,-0.0177,0.0778,-0.0008,0.0017,0.0389,-0.0426,-0.0549,0.0792,0.0746,0.0641,0.0116,-0.0309,-0.0407,-0.0445,-0.0371,-0.0174,-0.0767,-0.0656,-0.0132,-0.0567,-0.0073,-0.0703,0.0173,0.0439,0.0205,0.0589,-0.0155,0.012,-0.026,-0.0691,0.0211,-0.0423,0.027,0.0374,-0.036,-0.0222,-0.0803,0.0259,-0.0241,-0.0272,-0.0413,-0.0062,-0.0386,-0.0543,0.0986,0.0127,0.0171,0.0484,0.0193,0.0025,-0.0907,-0.075,-0.0615,0.0683,-0.0157,0.0681,0.0269,0.0143,0.0087,0.0899,0.005,-0.0073,-0.0045,-0.0003,-0.0288,0.0609,-0.015,0.0559,0.0259,-0.2765,0.0052,0.035,0.0344,-0.0482,0.0106,0.0297,0.0223,-0.0588,-0.0091,0.0054,0.0442,0.0313,-0.0328,-0.028,0.0188,0.078,-0.0182,0.0964,-0.0567,0.0306,0.0572,0.1889,-0.0097,0.0644,-0.0166,0.0266,0.0163,0.0168,-0.0659,0.0132,0.0017,0.0954,-0.0468,0.0185,0.0813,-0.0497,0.0293,-0.0155,-0.0022,-0.0184,-0.0119,-0.0526,-0.0473,0.1109,0.0029,-0.0018,-0.033,-0.0127,0.0135,0.0013,-0.0048,-0.0094,-0.0096,0.0231,0.0317,-0.0266,-0.0323,-0.0131,-0.0224,0.0586,-0.0611,-0.0167,-0.0139,-0.0633]}
{"key":"[Predicting Physics in Mesh-reduced Space with Temporal Attention] Graph-based next-step prediction models have recently been very successful in modeling complex high-dimensional physical systems on irregular meshes. However, due to their short temporal attention span, these models suffer from error accumulation and drift. In this paper, we propose a new method that captures long-term dependencies through a transformer-style temporal attention model. We introduce an encoder-decoder structure to summarize features and create a compact mesh representation of the system state, to allow the temporal model to operate on a low-dimensional mesh representations in a memory efficient manner. Our method outperforms a competitive GNN baseline on several complex fluid dynamics prediction tasks, from sonic shocks to vascular flow. We demonstrate stable rollouts without the need for training noise and show perfectly phase-stable predictions even for very long sequences. More broadly, we believe our approach paves the way to bringing the benefits of attention-based sequence models to solving high-dimensional complex physics tasks.","layer":1,"vector":[-0.0559,0.0047,0.0487,-0.0032,-0.0002,0.017,-0.0183,0.006,0.0223,-0.0079,-0.0441,-0.0417,0.0308,0.0806,-0.0103,0.0319,0.0082,0.0582,-0.0105,0.0083,0.0172,-0.0047,0.0048,-0.0343,-0.0343,-0.0124,-0.0411,-0.031,-0.0479,-0.2397,0.0192,-0.0608,0.0011,-0.025,0.0147,-0.0595,-0.0158,0.042,-0.0306,0.0578,0.0078,0.0309,-0.0278,-0.0279,-0.0013,-0.0217,0.0017,-0.041,0.0137,-0.0465,0.0324,-0.0942,0.0009,0.0445,0.0513,0.0431,0.0556,0.0153,0.0383,-0.0021,0.0373,0.0234,-0.1447,0.0824,0.0665,0.0277,-0.0443,-0.03,0.0298,0.0262,0.0043,0.0075,0.0088,0.0208,0.0174,0.0088,0.0177,-0.02,-0.0001,-0.0018,0.0542,-0.0389,-0.0872,-0.0549,-0.016,-0.0241,0.0253,-0.0234,0.0112,-0.0291,-0.1091,-0.0521,-0.0365,0.0688,-0.0638,0.01,0.0732,0.0417,-0.0493,0.2142,-0.0417,0.0287,0.0336,-0.0384,0.059,-0.0703,-0.0171,-0.0203,-0.0223,0.0162,-0.0297,-0.015,0.0584,-0.0386,0.0206,0.0047,0.0548,0.0379,-0.0295,-0.0078,-0.0203,0.0119,0.0142,-0.028,0.0108,-0.0713,0.0203,0.1208,0.0152,0.0228,0.0521,0.0308,-0.0515,-0.017,0.0231,0.0347,0.0417,-0.0106,-0.022,0.0046,-0.0375,0.0006,-0.0257,-0.1102,-0.042,0.1325,-0.0534,-0.0022,-0.0296,-0.0214,-0.0236,0.0382,-0.0245,-0.0508,0.0264,0.0228,0.0071,0.0167,-0.0578,0.0414,-0.0502,-0.0208,-0.0791,0.0767,-0.026,-0.0677,-0.0065,0.0021,0.0352,-0.0374,0.0079,-0.0038,-0.0109,0.0252,0.133,0.0616,-0.0533,0.009,0.0173,0.0292,0.0344,-0.0407,-0.0347,0.0487,0.0358,-0.0508,0.0216,-0.0401,-0.0209,0.0422,-0.0337,0.0242,-0.0045,0.033,-0.0268,-0.0158,0.0005,0.004,0.0039,-0.0498,0.0069,-0.0277,-0.0284,0.0344,-0.0241,0.0218,-0.0289,0.0054,0.0161,0.0156,-0.0455,-0.0203,0.0491,-0.0334,-0.0194,0.0053,0.0193,0.022,-0.0181,0.0389,0.0534,-0.0523,-0.0713,-0.2013,0.0061,0.0246,-0.0293,0.0795,-0.0571,0.0513,-0.0348,0.0911,0.055,0.0655,-0.0032,-0.0348,-0.024,-0.0092,0.0238,0.004,-0.0182,-0.0326,-0.0008,-0.0015,0.0107,-0.0143,-0.0882,0.02,-0.0068,0.245,0.0358,0.0638,-0.0355,0.0232,0.0131,-0.038,-0.0471,0.0492,0.0255,0.076,-0.0134,-0.0055,-0.035,-0.0727,-0.0058,0.0024,-0.0811,-0.051,-0.0309,0.0041,0.0448,-0.074,0.0379,0.0922,-0.0729,0.0688,0.009,0.0032,-0.0559,-0.0683,0.0135,-0.0323,0.0262,-0.0229,-0.067,0.0024,-0.062,0.0222,0.0157,-0.0256,-0.0567,-0.0009,-0.0477,-0.0187,0.0761,0.0376,0.0112,0.0762,0.0036,0.0448,-0.0013,-0.0422,0.0107,0.0595,-0.0225,0.0461,0.0347,0.068,0.0195,0.0477,-0.004,-0.0025,-0.0402,-0.011,0.0424,-0.0298,0.0048,0.0016,-0.0199,-0.3268,0.0347,0.0107,0.0288,-0.0015,-0.005,0.0442,-0.0055,-0.0054,0.027,-0.0167,0.0329,0.0492,-0.0053,-0.0195,0.0406,0.1075,-0.0259,-0.0072,-0.0438,0.0107,0.0533,0.2311,-0.0028,0.0664,-0.0021,-0.0419,-0.0066,0.0445,-0.0062,0.0215,0.0165,0.0677,-0.0732,0.0129,0.0631,-0.0002,0.0509,0.0473,0.0237,0.0385,0.0275,-0.004,-0.0622,0.0693,0.0229,-0.0197,-0.0599,-0.0445,0.0271,-0.0309,0.0218,0.0154,0.0309,0.033,0.0531,-0.0048,-0.0391,0.011,-0.0304,-0.0081,-0.0961,-0.0094,-0.0328,-0.0212]}
{"key":"[Detection of Iterative Adversarial Attacks via Counter Attack] Deep neural networks (DNNs) have proven to be powerful tools for processing unstructured data. However for high-dimensional data, like images, they are inherently vulnerable to adversarial attacks. Small almost invisible perturbations added to the input can be used to fool DNNs. Various attacks, hardening methods and detection methods have been introduced in recent years. Notoriously, Carlini-Wagner (CW) type attacks computed by iterative minimization belong to those that are most difficult to detect. In this work we outline a mathematical proof that the CW attack can be used as a detector itself. That is, under certain assumptions and in the limit of attack iterations this detector provides asymptotically optimal separation of original and attacked images. In numerical experiments, we experimentally validate this statement and furthermore obtain AUROC values up to 99.73% on CIFAR10 and ImageNet. This is in the upper part of the spectrum of current state-of-the-art detection rates for CW attacks.","layer":0,"vector":[-0.0423,0.0024,0.0033,0.043,0.0036,0.0757,0.0532,0.0083,-0.0031,-0.0358,0.024,-0.0127,0.0464,0.0839,-0.0205,0.0463,0.0107,0.0276,-0.0383,0.0179,0.0085,-0.0507,-0.0119,-0.0446,0.0115,0.0269,-0.0249,-0.0664,-0.0489,-0.2653,0.006,-0.0676,0.0522,-0.0616,0.0205,-0.0459,-0.0412,0.0452,-0.0302,0.0337,-0.013,0.0422,-0.0029,-0.0532,-0.0556,-0.0547,-0.0362,-0.0147,-0.0088,-0.0133,0.0436,-0.0161,0.0571,-0.0034,0.0227,-0.0226,0.0557,0.0423,0.0381,0.0438,0.0469,0.0449,-0.1662,0.0447,0.0199,0.0481,-0.0288,-0.0406,0.0059,0.0565,-0.0004,0.0471,-0.0015,0.0027,0.0541,0.0019,-0.0339,-0.0271,0.0013,-0.0129,0.0105,-0.0077,-0.0314,0.0033,-0.0194,-0.0699,0.0012,-0.0134,0.0502,-0.006,-0.0315,-0.012,-0.0169,0.0038,-0.023,-0.0308,-0.0038,0.013,-0.089,0.2037,-0.0149,-0.0039,0.0458,-0.0209,0.0451,0.0104,-0.0858,-0.0728,-0.0104,-0.0171,0.0223,-0.0385,0.0216,-0.0408,0.0435,-0.021,0.0368,0.01,-0.0549,-0.0004,0.0059,-0.0138,0.0481,-0.012,0.0466,-0.0314,0.0032,0.1314,0.0191,0.0529,-0.0093,-0.0167,-0.0069,-0.0126,0.0197,0.0633,0.0096,0.0356,0.022,-0.0293,-0.0416,-0.0697,0.0449,-0.0635,-0.0261,0.0749,-0.0428,0.0875,-0.0202,-0.0317,0.0309,0.0537,-0.0456,-0.0029,0.018,0.0373,-0.0071,0.0441,-0.0587,-0.0136,-0.0262,-0.0488,-0.0006,0.1448,0.0528,-0.0684,0.0352,-0.0143,0.0081,-0.0408,0.0031,0.0621,-0.0274,0.0205,0.0342,0.0329,-0.0697,-0.0632,-0.0266,0.0331,0.0214,-0.0181,-0.0678,0.0652,0.0481,-0.0054,0.0218,-0.0417,0.0351,0.0366,-0.0804,0.0253,-0.0584,-0.0333,-0.0549,-0.0077,-0.0204,0.0002,-0.0313,-0.0149,0.0504,-0.0316,-0.0492,-0.0107,0.0014,0.0669,-0.0414,-0.0387,0.0015,0.0184,0.0053,-0.0173,0.0505,-0.0626,-0.0006,0.0011,0.0179,0.0432,-0.0276,0.0531,0.0497,-0.03,-0.0537,-0.2265,-0.0367,-0.0136,-0.0338,0.0579,-0.1147,0.0305,-0.0234,0.0535,0.0223,0.024,0.039,0.0015,0.0055,0.0006,0.0553,0.0201,0.0478,-0.0261,0.0082,-0.0104,0.0434,-0.0132,-0.0499,0.0314,-0.0054,0.2211,0.056,0.0581,0.0055,0.0446,0.0018,-0.025,-0.0585,0.0067,-0.03,0.0666,-0.0137,-0.0371,-0.035,-0.0201,0.0022,0.0103,-0.0958,-0.0089,-0.0493,-0.0387,0.0123,-0.0372,0.032,0.0491,0.0013,0.0369,0.0216,0.044,-0.0385,-0.1116,0.0244,-0.0807,0.0677,0.0166,-0.054,0.0351,-0.0352,0.0667,0.013,-0.041,-0.0694,0.0512,-0.0093,-0.0122,0.0635,0.0464,0.0095,0.0379,0.0065,0.0257,-0.0353,-0.0429,0.0101,0.0516,-0.0173,0.0245,0.0142,-0.0016,-0.0092,0.0698,-0.0122,0.0145,-0.0397,0.0267,0.0144,-0.0613,0.0002,-0.0133,0.0124,-0.3085,0.0113,0.0427,0.0682,-0.0229,0.0189,0.0732,-0.0105,-0.0604,0.0034,-0.0531,0.0515,0.0327,-0.0114,0.0143,0.0421,0.045,-0.0559,0.0582,-0.0099,0.0074,0.0347,0.2476,-0.0528,-0.0442,0.023,0.0103,0.0235,-0.0048,-0.012,0.0063,0.0202,0.0682,-0.0208,0.0372,0.0773,0.012,0.0594,-0.0003,-0.0508,-0.0297,0.0072,-0.0326,-0.0249,0.0687,-0.0156,-0.0151,-0.0108,0.0237,0.0376,-0.0423,-0.0713,0.0169,-0.0395,0.0254,0.0459,-0.0574,-0.0165,-0.0595,-0.0074,0.04,-0.022,-0.0243,0.0495,-0.0246]}
{"key":"[Trade-offs and Guarantees of Adversarial Representation Learning for Information Obfuscation] Crowdsourced data used in machine learning services might carry sensitive information about attributes that users do not want to share. Various methods have been proposed to minimize the potential information leakage of sensitive attributes while maximizing the task accuracy. However, little is known about the theory behind these methods. In light of this gap, we develop a novel theoretical framework for attribute obfuscation. Under our framework, we propose a minimax optimization formulation to protect the given attribute and analyze its inference guarantees against worst-case adversaries. Meanwhile, it is clear that in general there is a tension between minimizing information leakage and maximizing task accuracy. To understand this, we prove an information-theoretic lower bound to precisely characterize the fundamental trade-off between accuracy and information leakage. We conduct experiments on two real-world datasets to corroborate the inference guarantees and validate this trade-off. Our results indicate that, among several alternatives, the adversarial learning approach achieves the best trade-off in terms of attribute obfuscation and accuracy maximization.","layer":1,"vector":[0.0043,-0.041,-0.0068,-0.0317,0.0362,0.0367,0.0694,-0.027,0.0045,0.0146,-0.0063,-0.0282,0.0421,0.0723,0.0259,0.0162,0.0107,0.0364,-0.0417,0.0077,0.0381,-0.0789,-0.013,-0.0353,0.0249,0.0371,-0.0425,-0.031,-0.0425,-0.2445,0.029,-0.0849,0.016,-0.0085,0.0113,-0.0237,-0.0131,0.0389,-0.0279,0.0674,0.0022,0.0115,-0.0183,-0.0584,-0.0277,0.0067,-0.0197,0.0194,-0.0243,-0.0129,0.0531,-0.0072,0.0108,0.0372,0.0617,-0.002,0.0507,0.0288,0.0154,0.0794,0.0065,0.0699,-0.1248,0.0583,0.0331,0.0379,-0.0347,-0.0408,-0.0213,0.0074,0.0237,0.052,0.0091,0.048,0.0137,0.0337,-0.0098,-0.0245,-0.0239,0.01,0.0278,-0.0294,-0.0018,0.0542,-0.0142,-0.0681,0.042,-0.041,0.0511,-0.0228,-0.0447,-0.0267,-0.0156,0.0182,-0.0487,-0.0102,0.0617,0.0411,-0.1124,0.2096,-0.0666,0.036,0.0296,-0.0491,0.0364,-0.0499,-0.0319,-0.0266,-0.0057,0.0279,0.0053,0.0001,0.022,-0.0098,0.0518,0.0437,0.0616,0.0311,-0.0752,0.0076,-0.0545,0.0005,0.0768,0.0412,-0.0057,-0.0382,0.034,0.1158,0.0224,0.0502,-0.0036,-0.02,-0.0377,-0.0069,0.0179,0.0592,-0.0138,0.0481,0.0244,-0.0327,-0.0411,-0.0146,0.0074,-0.0515,-0.0507,0.1452,-0.022,0.0213,0.0057,-0.0553,0.0011,0.0384,-0.0317,-0.0434,0.0188,0.0106,0.0305,0.0607,-0.0602,0.0099,-0.0087,-0.0488,-0.0026,0.1043,-0.0282,-0.0908,0.0012,0.0144,0.0092,-0.0321,0.0167,0.0121,-0.0527,0.0206,0.0699,0.0057,-0.099,0.0088,-0.0161,-0.0009,-0.0196,-0.0376,-0.0202,0.0328,0.0257,-0.0289,0.0114,-0.0358,0.037,0.0624,-0.0348,0.0115,-0.0257,-0.0113,-0.0331,-0.0407,0.0215,0.0107,-0.0272,-0.0116,-0.0299,0.0131,-0.1226,0.0365,0.0013,0.0357,0.0547,-0.0504,0.0357,0.0444,-0.0141,-0.0146,-0.0136,-0.0314,-0.0154,0.0287,-0.0058,0.0389,0.0143,0.0251,-0.0041,-0.0573,-0.0631,-0.2374,-0.0379,-0.0133,-0.0253,0.0494,-0.0644,0.0225,-0.0107,0.0441,0.0507,0.0721,-0.0518,-0.0097,0.0432,-0.0163,0.0439,0.0319,0.0371,-0.0126,0.0176,-0.0405,0.0394,-0.0214,-0.0782,0.0129,0.0051,0.2247,0.0413,-0.0239,-0.0331,0.0073,0.0524,-0.0345,-0.118,0.0456,0.0044,0.0119,0.0096,-0.0381,0.0088,0.0013,0.0155,0.0258,-0.1385,-0.0211,-0.063,-0.0491,0.0354,-0.1072,0.0477,0.0568,-0.0073,0.0987,-0.003,0.0184,-0.0592,-0.0628,0.0569,-0.0434,0.0488,0.0102,-0.0655,0.0059,-0.0902,0.043,0.0117,-0.0264,-0.0653,0.0304,-0.0177,-0.0174,0.0822,0.0078,0.0185,0.0315,0.0081,0.0529,-0.0421,-0.0543,0.0047,0.0791,0.0147,0.0411,0.0013,0.033,-0.0038,0.0597,0.0005,0.0117,-0.0543,-0.0165,-0.0167,-0.0338,-0.0272,0.0581,-0.017,-0.3106,0.0094,0.0157,0.0393,-0.064,-0.0129,0.0585,-0.0039,-0.0752,-0.0198,0.0108,0.0492,0.0044,-0.0302,-0.0079,0.0549,0.0734,-0.053,-0.0091,0.002,0.0251,0.0731,0.2028,-0.0206,0.0258,-0.0027,-0.0402,0.0738,0.0131,-0.0364,0.026,0.0029,0.0866,-0.0048,-0.0167,0.0363,-0.0602,-0.0072,0.04,-0.0285,-0.01,-0.0012,-0.0118,0.0129,0.0864,0.0089,-0.0123,-0.0184,0.0171,0.0215,-0.0392,0.0011,-0.0421,-0.0101,0.0115,0.0361,-0.0589,-0.014,0.0223,-0.0161,0.001,-0.0468,-0.0331,0.0256,-0.0331]}
{"key":"[Collaborative Intelligence Orchestration: Inconsistency-Based Fusion of Semi-Supervised Learning and Active Learning] While annotating decent amounts of data to satisfy sophisticated learning models can be cost-prohibitive for many real-world applications. Active learning (AL) and semi-supervised learning (SSL) are two effective, but often isolated, means to alleviate the data-hungry problem. Some recent studies explored the potential of combining AL and SSL to better probe the unlabeled data. However, almost all these contemporary SSL-AL works use a simple combination strategy, ignoring SSL and AL's inherent relation. Further, other methods suffer from high computational costs when dealing with large-scale, high-dimensional datasets. Motivated by the industry practice of labeling data, we propose an innovative Inconsistency-based virtual aDvErsarial Active Learning (IDEAL) algorithm to further investigate SSL-AL's potential superiority and achieve mutual enhancement of AL and SSL, i.e., SSL propagates label information to unlabeled samples and provides smoothed embeddings for AL, while AL excludes samples with inconsistent predictions and considerable uncertainty for SSL. We estimate unlabeled samples' inconsistency by augmentation strategies of different granularities, including fine-grained continuous perturbation exploration and coarse-grained data transformations. Extensive experiments, in both text and image domains, validate the effectiveness of the proposed algorithm, comparing it against state-of-the-art baselines. Two real-world case studies visualize the practical industrial value of applying and deploying the proposed data sampling algorithm.","layer":0,"vector":[0.0029,-0.0266,0.0024,-0.0016,0.0512,0.0378,0.0285,0.0478,-0.0107,-0.0145,0.0119,-0.0418,0.0507,0.0331,0.0095,0.0315,0.0104,0.0357,-0.0422,0.0341,0.0028,-0.0106,-0.0018,-0.0332,0.0551,0.0684,-0.0269,-0.0467,-0.0566,-0.2506,0.0159,-0.04,0.0662,-0.0252,-0.0008,-0.0208,-0.0385,0.0836,0.0014,0.0589,0.0197,0.021,-0.0443,-0.0739,-0.03,-0.0012,-0.0453,-0.0094,-0.0303,0.0157,0.0138,-0.0525,0.01,0.0018,0.0351,0.0466,0.0362,-0.009,0.0274,0.0598,0.0212,0.0439,-0.1454,0.0584,0.0239,-0.0039,-0.0249,-0.0229,-0.031,0.0312,0.0342,0.0406,0.0259,0.0405,-0.0067,0.0146,-0.021,-0.037,-0.0151,0.0049,0.0505,-0.0184,-0.0892,-0.0027,-0.0273,-0.0604,0.0042,-0.0299,0.0922,0.0094,0.002,0.0133,-0.0269,0.0382,-0.0298,-0.0067,0.0267,0.0308,-0.0843,0.2168,-0.0949,0.0725,0.0209,-0.0216,0.0075,-0.0255,-0.0703,-0.0103,-0.0224,0.0208,-0.0548,-0.0033,0.0154,-0.0664,0.0632,0.0232,0.0645,0.0419,-0.0132,-0.0259,0.021,0.0094,0.0294,0.0204,0.0197,-0.0516,0.0328,0.1378,0.0233,0.0052,0.023,-0.0185,0.0073,-0.0269,-0.0113,0.0405,0.0085,0.0045,0.0479,0.0551,-0.0533,0.0043,0.0419,-0.0831,-0.0252,0.1473,-0.0337,0.0143,-0.0503,-0.0032,-0.0094,0.0069,-0.0215,0.0081,-0.0077,0.053,0.0595,0.0435,-0.0319,0.0031,-0.0269,-0.0691,-0.0303,0.1282,0.0058,-0.089,-0.0221,-0.0112,-0.0064,-0.0189,0.0361,0.0434,-0.0397,0.0041,0.081,0.0624,-0.0816,-0.0293,-0.0474,0.0122,0.015,-0.0404,-0.0314,0.0362,0.0293,-0.0812,0.0277,-0.0634,0.0133,0.0484,-0.0353,0.0202,-0.0186,-0.0125,-0.0252,-0.039,0.016,-0.002,0.0128,-0.0158,-0.0019,-0.0445,-0.0401,-0.0006,0.0115,0.0029,0.0097,0.0104,0.0406,0.0081,0.0005,0.0178,-0.0045,-0.003,-0.0463,-0.022,0.0234,0.069,-0.0139,0.061,0.0012,-0.0079,-0.0241,-0.2619,-0.024,-0.0017,-0.012,0.0264,-0.0414,0.0468,0.002,0.0371,0.0878,0.0583,0.0122,-0.0314,0.0115,0.0064,0.0808,0.0539,0.0327,-0.0353,0.005,-0.0394,0.0448,0.0025,-0.0671,0.0672,-0.0302,0.2281,0.0407,0.0703,-0.0338,0.0231,0.0311,-0.0271,-0.0996,0.053,0.0119,0.0593,-0.0233,-0.0525,-0.0162,-0.0432,0.0058,0.0113,-0.1547,-0.0146,-0.0197,-0.0499,0.0327,-0.0663,-0.0017,0.0182,-0.0306,0.0734,-0.0353,-0.0585,0.001,-0.0844,0.0431,-0.0509,0.027,0.0032,-0.045,-0.0204,-0.0712,0.0134,0.0007,-0.0523,-0.048,0.0862,-0.021,-0.031,0.0842,0.0064,-0.0198,0.0303,0.0054,0.0023,-0.0307,-0.0179,0.0073,0.0672,-0.0185,0.0173,-0.0006,0.0431,0.0029,0.0745,-0.0255,0.0601,-0.0277,0.01,0.0546,-0.0599,0.0005,0.0593,0.0047,-0.2676,0.009,0.0285,0.047,-0.099,0.0083,0.0488,0.0113,-0.0186,-0.0077,-0.0121,-0.0122,0.0258,-0.0264,-0.0528,0.0411,0.0676,-0.0919,0.0339,-0.0589,0.0024,0.0506,0.2039,-0.0538,0.0263,0.0127,-0.0199,0.0211,0.0281,-0.0364,-0.0372,0.0263,0.0696,-0.0253,-0.0036,0.0354,-0.0309,0.0195,0.0085,-0.0085,-0.0175,0.019,-0.0719,-0.0495,0.0858,0.0376,0.0108,-0.0288,-0.0228,0.0028,-0.0089,-0.0236,-0.0059,-0.0044,0.0237,0.0318,-0.0278,-0.0471,-0.0391,0.0128,0.0022,-0.06,-0.024,-0.0249,-0.0039]}
{"key":"[Graph-GAN: A spatial-temporal neural network for short-term passenger flow prediction in urban rail transit systems] Short-term passenger flow prediction plays an important role in better managing the urban rail transit (URT) systems. Emerging deep learning models provide good insights to improve short-term prediction accuracy. However, a large number of existing prediction models combine diverse neural network layers to improve accuracy, making their model structures extremely complex and difficult to be applied to the real world. Therefore, it is necessary to trade off between the model complexity and prediction performance from the perspective of real-world applications. To this end, we propose a deep learning-based Graph-GAN model with a simple structure and high prediction accuracy to predict short-term passenger flows of the URT network. The Graph-GAN consists of two major parts: (1) a simplified and static version of the graph convolution network (GCN) used to extract network topological information; (2) a generative adversarial network (GAN) used to predict passenger flows, with generators and discriminators in GAN just composed of simple fully connected neural networks. The Graph-GAN is tested on two large-scale real-world datasets from Beijing Subway. A comparison of the prediction performance of Graph-GAN with those of several state-of-the-art models illustrates its superiority and robustness. This study can provide critical experience in conducting short-term passenger flow predictions, especially from the perspective of real-world applications.","layer":2,"vector":[-0.0043,-0.037,0.0675,-0.0259,0.0271,0.0294,0.0414,-0.0075,0.0104,-0.0344,-0.0195,-0.029,0.0275,0.0691,0.0095,-0.012,0.0315,0.0501,-0.014,-0.0162,0.0174,-0.0451,-0.0428,-0.0511,0.0411,0.001,0.0132,-0.0486,-0.0677,-0.2432,0.0071,-0.0296,0.0031,-0.0328,-0.0036,-0.0606,-0.0143,0.0597,0.0246,0.0379,-0.0029,0.0292,-0.0303,-0.0377,0.0095,-0.0175,-0.0344,0.0056,-0.0313,-0.0362,0.0523,-0.0466,0.0238,0.0473,0.0116,0.0142,0.0422,0.0129,0.0476,0.0516,-0.0014,-0.0047,-0.1747,0.0563,0.034,0.0071,-0.0428,0.0174,-0.0181,0.0379,0.0115,0.0283,-0.0119,0.0423,-0.0027,-0.0143,0.0135,0.0093,-0.0304,-0.0179,0.0547,-0.0012,-0.0451,-0.0075,0.0264,-0.0325,-0.0171,-0.0158,0.0015,0.0133,-0.0287,-0.0254,-0.0246,0.0576,-0.0592,0.0092,0.0294,-0.0223,-0.0512,0.1856,-0.0671,0.0753,0.0273,0.0262,0.011,-0.0285,-0.0308,-0.0347,-0.0576,-0.0054,-0.0032,-0.0328,0.0504,-0.0108,0.0325,0.0055,0.0733,0.0478,-0.0327,0.0069,-0.0456,0.0274,0.0148,-0.0212,-0.0048,-0.0408,0.0475,0.1235,0.0172,0.0384,0.0695,0.0163,-0.0793,0.0082,-0.0395,-0.0019,0.0681,0.0106,-0.0487,-0.0001,-0.0307,-0.0266,-0.0044,-0.081,-0.0698,0.1018,-0.0126,-0.0305,-0.0245,-0.0276,-0.0276,0.01,-0.0369,-0.0559,0.0043,0.0393,0.0224,0.0687,-0.0501,0.046,-0.025,-0.0108,-0.1209,0.0709,0.0083,-0.1061,0.0072,-0.0206,0.0117,-0.0534,-0.0037,0.0372,-0.0395,0.0336,0.0686,0.0156,-0.063,0.0098,-0.0358,0.0071,0.0307,-0.0556,-0.0269,0.0315,0.0585,-0.0223,-0.0225,-0.0648,-0.044,0.0785,-0.0351,0.046,-0.0004,-0.0,-0.0301,-0.0242,-0.0251,-0.0612,-0.0291,-0.0325,0.0122,-0.008,-0.0363,-0.0218,-0.0224,0.0166,-0.0238,-0.0139,0.0166,0.0055,-0.0191,0.0101,0.0987,-0.0082,0.0201,0.0236,0.0137,0.0394,0.0001,0.0559,0.0344,0.0041,-0.0763,-0.2197,0.0156,0.0053,-0.01,0.0672,-0.075,0.0096,-0.0196,0.0786,0.0493,0.1078,0.0031,-0.0005,0.0291,0.0223,0.0562,-0.0054,0.0666,0.0034,-0.0166,0.0126,0.0329,-0.0015,-0.1228,0.014,0.0216,0.2215,0.0014,0.0634,-0.0491,0.0413,0.0113,-0.0136,-0.0744,0.0818,0.0151,0.087,0.0055,-0.0405,-0.0243,-0.0487,0.0404,0.0068,-0.068,-0.0491,-0.0165,-0.0201,0.0157,-0.0645,0.0068,0.0187,-0.023,0.0586,0.0067,0.0359,-0.0427,-0.1092,0.0684,-0.0461,0.012,-0.0237,-0.0592,-0.0069,-0.0315,0.0257,0.0366,-0.0484,-0.0533,-0.0019,0.011,-0.0189,0.1199,0.0205,-0.0125,0.0973,-0.0497,0.0192,0.0255,-0.0171,-0.0199,0.0428,-0.045,0.0486,0.0538,0.0211,0.0291,0.0994,-0.0201,0.0375,-0.0142,0.0272,0.0031,0.0064,-0.024,0.0264,-0.0146,-0.2986,0.0535,0.0334,0.0441,-0.0282,-0.0022,0.0347,0.0611,-0.0154,-0.0305,0.0314,0.0219,0.0826,-0.0606,0.0211,0.0063,0.1204,-0.0497,-0.0073,-0.0318,0.0025,0.0237,0.2072,-0.0434,0.06,0.0295,-0.0627,0.0134,0.0189,-0.0303,-0.0264,0.0042,0.0974,-0.0485,0.0231,0.0627,-0.0393,0.0852,0.0375,-0.001,-0.0198,0.035,-0.0285,-0.0251,0.0532,-0.0012,-0.0559,-0.0232,0.0213,-0.0048,-0.0462,-0.0138,-0.0286,0.0093,0.0322,0.0436,-0.0718,-0.0752,-0.0298,0.0063,0.0011,-0.0636,0.0031,-0.0347,-0.002]}
{"key":"[Restructuring, Pruning, and Adjustment of Deep Models for Parallel Distributed Inference] Using multiple nodes and parallel computing algorithms has become a principal tool to improve training and execution times of deep neural networks as well as effective collective intelligence in sensor networks. In this paper, we consider the parallel implementation of an already-trained deep model on multiple processing nodes (a.k.a. workers) where the deep model is divided into several parallel sub-models, each of which is executed by a worker. Since latency due to synchronization and data transfer among workers negatively impacts the performance of the parallel implementation, it is desirable to have minimum interdependency among parallel sub-models. To achieve this goal, we propose to rearrange the neurons in the neural network and partition them (without changing the general topology of the neural network), such that the interdependency among sub-models is minimized under the computations and communications constraints of the workers. We propose RePurpose, a layer-wise model restructuring and pruning technique that guarantees the performance of the overall parallelized model. To efficiently apply RePurpose, we propose an approach based on $\\ell_0$ optimization and the Munkres assignment algorithm. We show that, compared to the existing methods, RePurpose significantly improves the efficiency of the distributed inference via parallel implementation, both in terms of communication and computational complexity.","layer":1,"vector":[-0.0331,-0.006,0.0429,0.007,0.0176,0.021,-0.0006,-0.0345,0.024,-0.0326,0.0371,-0.0742,0.0254,0.0491,0.0616,-0.0017,-0.0301,0.0513,0.0043,-0.0396,0.0081,-0.0411,-0.0665,-0.0144,0.0705,-0.0065,-0.0491,-0.0292,-0.0511,-0.255,0.0418,-0.0291,0.0461,0.023,0.0444,-0.0265,0.0006,-0.0033,-0.0392,0.0694,0.0142,0.0228,-0.0316,-0.0564,0.0087,-0.0184,-0.032,-0.0351,-0.0222,-0.0233,0.0573,-0.0388,-0.0323,0.0415,0.0607,0.0273,0.028,0.0349,0.054,0.0508,0.0339,0.0483,-0.1591,0.0481,0.0719,0.0127,-0.0336,-0.0641,0.011,0.0496,-0.0238,0.0422,0.0571,0.0005,0.0108,0.0163,0.0168,-0.0485,-0.005,0.007,-0.0075,-0.0306,-0.0131,-0.013,-0.0031,-0.0388,0.0422,-0.0417,0.0147,-0.0246,-0.058,0.0121,-0.0207,0.0203,-0.0551,-0.0179,0.0238,0.029,-0.0359,0.204,-0.0561,0.0256,0.0443,-0.0103,0.0104,-0.0157,-0.0508,-0.0319,-0.0597,0.0518,-0.0228,-0.0311,0.0076,0.0025,0.0405,0.0138,0.0839,0.0807,-0.0234,0.012,-0.0358,0.0156,0.0303,0.0053,0.036,-0.0612,0.0257,0.1563,0.017,0.0648,0.0497,-0.0016,0.0114,-0.0518,0.005,0.0345,0.0229,-0.0212,0.0084,0.0124,-0.0155,-0.0316,0.0307,-0.0604,-0.0288,0.1266,-0.0394,-0.005,-0.0442,-0.0546,-0.0756,0.0103,-0.0376,-0.0107,0.0462,0.0265,-0.0121,0.024,-0.0591,0.0319,-0.0238,-0.0068,-0.0168,0.0928,0.046,-0.0655,-0.0303,-0.0127,0.0339,-0.0458,0.0332,-0.0198,0.0079,0.0174,0.0471,0.047,-0.0689,-0.0248,-0.0213,0.0254,0.0265,-0.0533,-0.0392,0.0022,0.0037,-0.0335,0.0005,-0.0397,-0.0515,0.0428,-0.066,0.0356,-0.016,0.0043,-0.0028,-0.0291,0.0019,0.016,0.0235,-0.0281,0.0005,0.0085,-0.0378,0.0008,-0.0365,0.0241,0.0038,0.0308,-0.0064,0.0475,0.0052,-0.0069,0.0861,-0.0451,-0.0438,-0.0089,0.012,0.0309,0.0367,0.0317,0.0309,-0.0634,-0.05,-0.1949,0.0121,0.0187,-0.0431,0.0651,-0.0584,0.0357,-0.0256,0.0325,0.0575,0.0612,-0.0412,-0.0234,0.0349,0.0115,0.0983,0.0148,0.0613,-0.0296,-0.0168,-0.0055,0.0026,-0.0263,-0.0957,0.0599,0.0228,0.2216,0.0097,0.0321,-0.0255,0.0061,0.0227,0.0034,-0.1133,0.0395,0.0532,0.0616,0.0311,-0.0392,-0.0084,-0.0304,0.0445,-0.0024,-0.1289,-0.0496,-0.0459,-0.0573,0.0195,-0.0496,-0.0207,0.0227,-0.0271,0.013,0.0038,-0.0042,-0.0147,-0.1038,0.0304,-0.0598,0.0169,0.0229,-0.0348,-0.0724,-0.0669,0.0981,-0.0172,-0.0013,-0.0341,0.0204,-0.0203,-0.0145,0.0598,0.0095,0.0484,0.011,0.0419,0.0049,-0.0412,-0.0406,-0.0113,0.107,-0.0692,-0.013,0.0621,0.0567,0.0163,0.0984,0.0058,0.0131,-0.0074,-0.0144,-0.0012,-0.0541,0.0272,0.0537,-0.0258,-0.2896,0.0311,-0.0139,0.0309,-0.0352,0.0333,0.0183,0.0437,-0.0447,0.0403,0.012,0.0964,0.0296,0.0404,-0.0257,0.0476,0.0628,-0.0474,0.0233,-0.0892,0.0165,0.0336,0.1946,-0.0427,0.0449,0.081,-0.0222,-0.0146,0.0336,-0.0066,-0.0001,-0.0065,0.0646,-0.0698,0.0202,0.0922,-0.0235,0.0023,0.0762,0.0009,-0.0144,0.0005,0.0227,-0.0309,0.0766,-0.0028,-0.0409,-0.0514,-0.0421,0.0289,-0.0305,-0.0268,-0.0161,-0.0062,0.0224,0.0077,-0.0352,-0.0607,-0.0894,-0.0424,0.0405,-0.0768,-0.0271,-0.0293,-0.0444]}
{"key":"[IPO: Interior-point Policy Optimization under Constraints] In this paper, we study reinforcement learning (RL) algorithms to solve real-world decision problems with the objective of maximizing the long-term reward as well as satisfying cumulative constraints. We propose a novel first-order policy optimization method, Interior-point Policy Optimization (IPO), which augments the objective with logarithmic barrier functions, inspired by the interior-point method. Our proposed method is easy to implement with performance guarantees and can handle general types of cumulative multiconstraint settings. We conduct extensive evaluations to compare our approach with state-of-the-art baselines. Our algorithm outperforms the baseline algorithms, in terms of reward maximization and constraint satisfaction.","layer":1,"vector":[-0.0445,0.0197,0.0453,-0.0706,0.0068,0.0463,0.0393,0.0222,0.0771,0.0136,0.035,-0.02,0.0039,0.1023,0.0112,-0.0072,-0.0104,0.0554,-0.0182,0.041,0.0263,-0.0812,-0.0219,-0.129,0.0068,0.0146,-0.0386,-0.0402,-0.0304,-0.2277,0.0408,-0.0304,-0.0042,-0.0368,-0.0186,0.0407,-0.0173,0.0852,-0.0564,0.0512,0.0601,0.0496,-0.0102,-0.0609,-0.0352,-0.0781,-0.0086,-0.0203,0.0032,0.0091,-0.0068,-0.0313,-0.0046,0.0184,0.036,0.0103,0.054,0.0627,0.0231,0.0333,0.0127,0.0225,-0.1601,0.0424,0.0388,0.0515,-0.0435,0.0222,0.0116,0.0479,-0.045,0.0061,0.0295,0.0016,0.0065,0.0318,0.0242,-0.0164,-0.0128,-0.0086,0.0433,-0.0549,-0.0371,0.0054,-0.0296,-0.0543,0.0268,-0.0846,0.0593,0.0504,-0.0068,-0.0101,-0.0181,-0.0004,-0.053,-0.0102,0.045,0.0144,-0.0977,0.1856,-0.0233,0.0645,-0.0127,-0.0277,0.0272,-0.041,-0.0344,-0.0017,-0.0363,-0.0011,-0.0422,0.0186,0.0594,0.0092,0.0073,0.0201,0.0775,0.0019,0.0297,-0.0101,0.0211,0.0253,0.0567,0.0092,0.0257,-0.0826,0.0319,0.1372,-0.0056,0.0207,0.0112,-0.0435,-0.0348,-0.0244,0.021,0.0448,-0.0057,0.0157,0.0305,-0.0045,-0.0183,-0.0094,0.0222,-0.1339,-0.0288,0.1258,0.0,0.024,-0.0509,-0.0449,0.0099,-0.0073,0.0175,-0.0159,-0.0199,-0.0025,-0.0125,0.0608,-0.0472,-0.0295,-0.0157,-0.0603,-0.0325,0.0835,-0.0106,-0.078,-0.0266,-0.0202,-0.0142,-0.0282,0.0323,0.0848,-0.0732,0.0068,0.1057,0.0512,-0.0908,0.0072,-0.0134,-0.0165,0.0183,-0.0911,-0.0306,0.0002,0.0584,-0.0327,0.0164,-0.0231,0.0155,0.0209,-0.0202,0.0202,-0.0333,0.0155,-0.0075,-0.0533,0.0079,-0.0024,0.0186,-0.021,-0.003,-0.0053,-0.0462,0.0193,-0.0136,0.0072,-0.0143,-0.0357,0.0723,-0.0131,-0.0586,0.0561,0.0749,-0.0154,-0.0117,-0.0108,0.0258,0.008,0.0004,0.0506,0.0205,0.0297,-0.012,-0.2376,-0.0036,-0.028,-0.0046,0.0417,-0.0468,0.0263,-0.0202,0.0232,0.0413,0.0753,-0.0198,-0.0344,0.0524,-0.0119,0.0254,0.0511,0.0137,-0.0133,0.0086,0.0201,-0.0019,-0.0335,-0.0653,0.0384,-0.003,0.2058,-0.0037,0.0305,-0.0223,0.0264,0.03,-0.017,-0.102,0.0412,0.0089,0.0523,-0.0272,-0.045,-0.0792,0.0294,0.0634,-0.0708,-0.069,-0.0026,-0.0054,-0.039,0.0695,-0.073,-0.0361,0.0777,0.0034,0.0189,-0.0173,-0.0403,-0.04,-0.0714,0.0414,-0.0238,0.0444,-0.013,-0.0435,0.0084,-0.0356,0.0448,-0.0075,0.026,-0.0213,0.0605,-0.0044,-0.015,0.0578,0.0039,-0.0117,0.0413,0.0165,0.025,0.0179,-0.0437,-0.0472,0.0698,-0.0587,0.0749,0.0324,-0.0454,-0.0135,0.0652,-0.0204,0.0239,-0.0043,0.0048,0.0103,-0.0354,-0.0036,0.0548,0.0148,-0.3238,0.0056,0.0173,0.003,-0.0351,0.0404,0.031,0.013,-0.0567,-0.0024,0.0249,0.0739,0.0004,0.0443,0.0301,-0.0041,0.0531,-0.0212,0.0412,-0.0825,0.0074,0.0335,0.2168,-0.0397,0.0449,0.0182,-0.0375,-0.0031,0.0196,0.0041,-0.0221,0.0132,0.0267,-0.0765,0.0677,0.0904,-0.0364,0.0472,0.0399,-0.0022,-0.0511,0.0107,-0.0119,-0.0228,0.0372,0.0104,-0.0352,-0.037,-0.0096,0.0242,-0.0759,0.02,0.0279,-0.0013,0.048,0.0247,-0.0619,-0.0942,-0.0373,-0.015,0.0242,-0.066,0.0196,-0.0223,-0.0105]}
{"key":"[Reliable Causal Discovery with Improved Exact Search and Weaker Assumptions] Many of the causal discovery methods rely on the faithfulness assumption to guarantee asymptotic correctness. However, the assumption can be approximately violated in many ways, leading to sub-optimal solutions. Although there is a line of research in Bayesian network structure learning that focuses on weakening the assumption, such as exact search methods with well-defined score functions, they do not scale well to large graphs. In this work, we introduce several strategies to improve the scalability of exact score-based methods in the linear Gaussian setting. In particular, we develop a super-structure estimation method based on the support of inverse covariance matrix which requires assumptions that are strictly weaker than faithfulness, and apply it to restrict the search space of exact search. We also propose a local search strategy that performs exact search on the local clusters formed by each variable and its neighbors within two hops in the super-structure. Numerical experiments validate the efficacy of the proposed procedure, and demonstrate that it scales up to hundreds of nodes with a high accuracy.","layer":2,"vector":[-0.0468,0.0072,0.0034,-0.0025,0.0741,0.0248,0.0292,0.0173,0.0338,-0.0145,0.0565,-0.0267,0.0123,0.0716,0.0255,0.0417,0.0053,0.0605,-0.0351,0.0184,0.0148,-0.0463,0.0181,-0.0296,0.0623,0.0565,-0.0027,-0.0086,-0.0357,-0.2673,-0.0079,-0.066,0.0654,0.0085,-0.0068,-0.0362,-0.0097,0.0439,0.0132,0.0438,0.0472,0.0523,0.0035,-0.0384,-0.0235,-0.0053,-0.004,0.0117,-0.0279,-0.0627,0.0263,-0.0172,0.0599,0.0715,0.0416,0.0604,0.0227,0.0472,0.0491,0.0495,0.0143,0.042,-0.176,0.0159,0.1089,0.0214,-0.0462,-0.0072,0.0301,0.0543,-0.0006,0.0633,0.011,0.0689,0.0295,0.0207,0.0104,-0.0126,-0.0396,-0.0186,0.0228,-0.0041,-0.0008,0.0098,-0.0227,-0.0788,0.034,-0.0331,0.0463,0.0141,-0.0342,0.0062,0.0136,0.0341,-0.1195,-0.0161,-0.0065,0.0268,-0.0031,0.1894,-0.0706,0.0598,-0.0173,-0.0078,0.0509,-0.0151,0.0016,-0.0255,0.0228,-0.0238,0.0287,-0.0377,0.0126,-0.063,0.0043,-0.0003,0.0603,0.0533,-0.0318,-0.0199,-0.0178,-0.002,0.0758,-0.0404,0.0177,-0.0719,-0.0334,0.1131,0.0281,-0.0056,0.056,-0.0195,-0.0268,-0.0115,0.0376,-0.0316,0.0316,-0.023,0.0004,0.004,-0.014,-0.0476,0.0371,-0.066,-0.1152,0.1225,-0.0242,0.0042,-0.054,-0.0559,-0.0125,0.0225,-0.0227,-0.0347,-0.0146,0.0659,-0.0081,0.0617,-0.0312,0.0228,-0.0537,-0.047,-0.0565,0.0924,0.0232,-0.0823,-0.0143,0.0442,-0.0143,-0.0179,-0.0069,0.0377,-0.0154,0.0344,0.0659,0.017,-0.0908,0.0235,0.0109,0.0065,0.0232,-0.031,-0.0149,0.0556,0.0298,0.0078,0.0117,-0.0361,-0.0009,-0.0153,-0.0494,-0.0007,-0.0399,-0.021,-0.0348,-0.0338,0.031,-0.0227,0.0187,0.0266,0.013,-0.0321,-0.0835,-0.0049,-0.0601,0.0399,-0.0086,-0.0174,0.0203,0.0116,-0.0203,0.0399,0.0425,-0.0282,-0.0431,-0.0051,0.0044,0.017,-0.0168,0.0603,0.029,-0.0557,-0.0652,-0.2159,-0.038,-0.0209,-0.007,0.0322,-0.0476,0.0275,-0.0213,0.0562,0.1045,0.0398,0.007,-0.0361,0.0181,-0.0175,0.0524,0.0144,0.0231,0.0075,-0.007,-0.0187,0.0038,-0.0263,-0.0679,0.0845,0.017,0.2303,0.0416,0.0194,-0.0051,0.0431,0.0039,-0.0379,-0.0851,0.0546,0.042,0.0476,0.0088,-0.0438,-0.0218,-0.0651,0.0186,-0.0367,-0.0742,-0.0854,-0.0018,0.0043,0.0111,-0.0442,0.0104,0.0461,0.0035,0.0834,-0.0218,-0.0037,-0.0538,-0.0715,0.0014,-0.0427,-0.0048,0.0325,-0.0266,-0.0321,-0.069,0.0524,-0.0416,-0.0247,-0.0277,0.0026,-0.0384,0.0058,0.0445,-0.0075,-0.0034,0.0505,-0.014,0.0419,-0.059,-0.0616,-0.0302,0.0878,-0.0467,0.03,-0.0024,0.0113,0.0012,0.0884,0.0023,0.0393,-0.0085,-0.006,-0.0146,-0.0336,0.0056,0.0313,-0.0157,-0.2895,0.0617,-0.0127,-0.0101,-0.0402,-0.0055,0.0547,-0.0027,-0.0385,-0.0228,0.0025,0.0405,0.0504,0.0262,-0.0436,0.0454,0.0305,-0.0465,0.0499,-0.0727,-0.0003,0.0353,0.2501,-0.0467,0.0191,0.0305,-0.0109,-0.0294,0.0007,-0.0119,0.0196,0.029,0.0274,-0.0523,0.0628,0.0591,-0.0286,0.0088,0.0252,-0.0474,-0.0114,0.0063,-0.0432,-0.057,0.131,-0.0014,-0.0019,-0.0803,-0.0067,0.0356,-0.0633,0.014,0.0075,-0.0168,-0.0058,0.0152,-0.0264,-0.0374,-0.0405,-0.0556,-0.0309,0.0033,-0.0054,0.0109,-0.0231]}
{"key":"[On Robust Classification using Contractive Hamiltonian Neural ODEs] Deep neural networks can be fragile and sensitive to small input perturbations that might cause a significant change in the output. In this paper, we employ contraction theory to improve the robustness of neural ODEs (NODEs). A dynamical system is contractive if all solutions with different initial conditions converge to each other asymptotically. As a consequence, perturbations in initial conditions become less and less relevant over time. Since in NODEs, the input data corresponds to the initial condition of dynamical systems, we show contractivity can mitigate the effect of input perturbations. More precisely, inspired by NODEs with Hamiltonian dynamics, we propose a class of contractive Hamiltonian NODEs (CH-NODEs). By properly tuning a scalar parameter, CH-NODEs ensure contractivity by design and can be trained using standard backpropagation and gradient descent algorithms. Moreover, CH-NODEs enjoy built-in guarantees of non-exploding gradients, which ensures a well-posed training process. Finally, we demonstrate the robustness of CH-NODEs on the MNIST image classification problem with noisy test datasets.","layer":3,"vector":[-0.0233,-0.0453,-0.0039,-0.0107,-0.0181,0.0155,0.0323,-0.0075,0.0332,0.0087,-0.0178,-0.0208,0.0275,0.0681,0.0019,-0.035,0.0009,0.0708,-0.0584,0.0191,-0.0023,0.0071,-0.0145,-0.0116,0.0434,-0.0126,0.0068,0.0133,-0.0306,-0.2642,-0.0032,-0.07,0.0079,-0.043,0.071,-0.0134,-0.0536,0.0206,-0.0296,0.006,0.0227,0.0122,-0.0216,-0.047,0.0299,-0.0308,0.0183,-0.0575,-0.0409,-0.0685,0.0097,-0.0301,0.0426,0.021,0.0009,0.0366,0.043,0.0435,0.0606,0.0671,0.0236,0.062,-0.1334,0.0315,0.0438,0.0227,-0.0607,0.0002,0.0214,0.0478,0.0095,0.041,-0.0004,0.0143,0.0005,-0.0164,0.0156,-0.0385,0.0051,0.0032,0.0652,-0.0438,-0.0284,-0.043,0.0096,-0.0481,0.0151,-0.047,0.0119,-0.0056,-0.0525,-0.0366,-0.0357,0.0215,-0.0528,0.0017,0.0449,0.0593,-0.0405,0.1947,-0.0394,0.0408,0.0468,-0.0393,0.0079,-0.0092,-0.0628,-0.0282,-0.0254,0.0091,-0.0033,-0.0144,0.0067,-0.0331,-0.0136,-0.0059,0.0328,0.0535,-0.0005,-0.0021,-0.0393,0.0324,0.0398,-0.0121,0.0453,-0.0414,0.0183,0.1586,0.0565,-0.0014,0.0243,0.0018,-0.0328,-0.0325,0.0392,0.0211,0.0193,-0.0111,0.0054,0.0069,-0.0558,-0.0757,-0.0123,-0.0861,-0.1089,0.0753,-0.042,0.072,-0.0254,-0.0364,-0.0342,0.0177,-0.0645,-0.0229,0.0233,0.0488,0.0256,0.0229,-0.0911,0.0346,-0.0308,-0.0921,-0.047,0.1092,0.0274,-0.0526,-0.0017,-0.0006,-0.0268,-0.047,0.018,0.0358,-0.0218,0.024,0.0608,0.0111,-0.0546,-0.0031,0.008,0.0115,-0.0132,-0.0482,0.0034,0.0279,0.0839,-0.0132,-0.0197,-0.0539,0.0424,0.0538,-0.0186,-0.0067,-0.0237,0.0025,-0.0413,-0.0288,-0.039,-0.0024,-0.0038,-0.0309,0.0037,-0.008,0.0046,0.0508,0.0344,0.0019,-0.0248,0.0375,0.0132,0.0304,-0.0438,-0.0229,0.0607,-0.0454,-0.0543,-0.0005,-0.0034,0.0608,-0.0209,0.0331,0.079,-0.0224,-0.0313,-0.2305,-0.0154,-0.0113,-0.0402,0.0997,-0.1154,0.0289,-0.0417,0.062,0.0505,0.0287,0.0352,0.0171,-0.0152,0.0214,0.0527,0.0191,0.0301,-0.0166,-0.0034,-0.0066,0.0348,-0.0241,-0.1071,0.0636,0.0361,0.2209,0.0222,0.0613,-0.0189,0.0203,0.0167,-0.0359,-0.0631,0.0504,-0.0146,0.0759,-0.0372,-0.0507,-0.0518,-0.0139,-0.0034,0.0031,-0.0894,-0.0036,-0.0368,-0.0503,0.0302,-0.0868,0.0213,0.039,-0.0035,0.0158,-0.0078,0.008,0.0005,-0.0894,0.027,-0.0371,0.0294,-0.026,-0.063,0.0091,-0.0155,0.0732,0.0319,-0.0123,-0.0689,0.0365,-0.048,0.0247,0.0524,-0.0016,-0.0011,0.0751,0.0046,0.0194,0.0138,-0.0291,0.0176,0.0431,-0.0159,0.0572,0.0331,0.017,0.0116,0.0786,-0.0183,0.0242,-0.0098,0.0414,0.0487,-0.0777,0.0195,0.0525,0.0149,-0.297,0.0094,-0.0109,0.0224,-0.0571,0.0379,0.0435,-0.0027,-0.0397,-0.0253,-0.0226,0.0591,0.0749,0.017,0.0058,-0.0011,0.0554,-0.0607,0.0914,-0.075,0.0454,0.0562,0.2622,-0.0384,0.0346,0.034,-0.0066,0.0156,0.0282,-0.0569,-0.0032,0.0262,0.0628,-0.0507,0.0374,0.0893,-0.0426,0.0166,0.015,0.0008,0.0137,0.0349,-0.0091,-0.0165,0.0479,0.0038,-0.0137,-0.0349,-0.0002,0.0112,-0.0401,0.0144,-0.0476,0.0062,0.0122,0.008,-0.0422,-0.0312,-0.0432,-0.0409,0.021,-0.0829,0.0358,0.0049,-0.0328]}
{"key":"[Deep Face Forgery Detection] Rapid progress in deep learning is continuously making it easier and cheaper to generate video forgeries. Hence, it becomes very important to have a reliable way of detecting these forgeries. This paper describes such an approach for various tampering scenarios. The problem is modelled as a per-frame binary classification task. We propose to use transfer learning from face recognition task to improve tampering detection on many different facial manipulation scenarios. Furthermore, in low resolution settings, where single frame detection performs poorly, we try to make use of neighboring frames for middle frame classification. We evaluate both approaches on the public FaceForensics benchmark, achieving state of the art accuracy.","layer":0,"vector":[-0.0519,-0.0093,0.0026,-0.0406,0.0304,0.0387,0.0818,-0.032,-0.0023,0.022,0.0459,-0.0704,-0.034,0.0409,0.0347,-0.0017,-0.0022,0.0631,-0.0236,0.0135,0.0055,-0.0158,0.0172,-0.0799,0.0125,0.0026,-0.0328,-0.0188,-0.0331,-0.233,0.029,-0.0342,0.0037,-0.0096,0.0354,-0.0434,-0.017,0.0488,-0.0268,0.0252,0.0001,0.0292,-0.0248,-0.0664,-0.0311,-0.0481,-0.0158,0.0189,0.0097,-0.0302,0.0117,-0.0432,0.0513,0.067,0.008,-0.0015,0.0824,0.0338,0.0623,0.0221,-0.0112,0.0457,-0.1351,0.0245,0.0168,0.0511,-0.0295,-0.0264,0.0277,0.0158,-0.0629,0.0123,-0.0104,0.0155,-0.022,-0.0002,-0.0058,-0.0542,-0.0381,-0.0166,-0.0133,-0.0141,-0.0222,-0.0571,-0.0666,-0.0364,0.0342,-0.0335,0.0343,0.0147,-0.0524,0.0193,-0.0109,-0.0068,-0.0393,-0.0176,0.0427,0.0532,-0.0481,0.2161,-0.046,0.0124,0.0181,-0.0473,0.0955,-0.0105,-0.0524,-0.0216,-0.051,-0.0049,-0.0125,-0.0079,0.0156,-0.0449,0.0474,0.014,0.0214,0.0812,-0.044,-0.0372,0.0309,0.0145,0.0549,-0.0351,-0.0107,-0.0506,0.0295,0.1316,0.045,0.0093,0.0113,-0.0372,-0.0163,0.013,-0.001,0.0271,0.008,0.0149,0.0393,-0.0486,-0.0466,-0.0486,0.0278,-0.0673,-0.0127,0.0735,-0.0584,0.0375,-0.0354,-0.0178,-0.0243,0.0728,-0.059,0.0369,0.0392,-0.0358,0.0677,0.0347,-0.0553,0.0266,0.0485,-0.0164,-0.0165,0.0889,0.0599,-0.1282,-0.035,-0.0074,-0.008,-0.0175,0.0152,0.0431,-0.0236,0.0306,0.0435,0.0797,-0.0894,-0.0002,0.0063,0.0092,0.0223,-0.0624,-0.0748,0.0304,0.0636,-0.0198,0.0514,-0.0338,0.0471,0.0583,-0.0519,0.0327,-0.0448,-0.0208,-0.0144,0.0028,-0.0515,-0.0276,0.01,0.0225,0.0132,0.0302,-0.019,0.0232,-0.013,0.0381,-0.0298,-0.0176,0.0444,0.0634,-0.029,0.0198,0.0722,-0.0349,-0.0227,-0.0476,0.04,0.0457,-0.0048,-0.0103,-0.0002,-0.0571,-0.0301,-0.2542,-0.0057,-0.0178,0.0225,0.063,-0.0863,0.0603,0.0151,0.0729,0.042,0.0715,-0.0498,-0.0404,0.0267,0.0024,0.0373,0.0071,0.0383,-0.0165,-0.0115,-0.0603,0.0065,-0.0262,-0.0684,0.0253,0.0081,0.2272,0.0421,0.0075,-0.0727,-0.0093,0.0522,-0.0508,-0.101,0.0766,-0.0274,0.03,0.0057,0.0013,-0.0137,-0.0231,-0.0171,0.0379,-0.1308,-0.0157,0.0004,-0.0364,-0.0149,-0.0402,0.0517,0.0684,-0.0053,0.0172,0.018,-0.0297,-0.071,-0.0599,0.0187,-0.0389,0.05,-0.027,-0.0533,0.0123,-0.0948,0.0927,0.015,-0.0419,-0.0489,0.0324,-0.0185,0.0059,0.1001,0.0321,-0.0206,0.0474,0.0352,0.03,-0.027,-0.071,-0.052,0.0585,0.0262,-0.0213,0.0196,0.061,0.0534,0.0584,0.0173,0.0298,-0.0223,0.0194,-0.0157,-0.0617,0.0091,0.0196,0.026,-0.2865,-0.0344,0.0124,0.0273,-0.0258,-0.0102,0.0754,-0.0088,0.0107,-0.0013,-0.0477,0.0194,0.0657,0.0161,0.0162,0.0092,0.0452,-0.0559,0.031,-0.0475,0.005,0.0067,0.2171,-0.0217,0.02,-0.0258,0.0027,0.0025,0.0567,-0.038,0.0244,0.0189,0.0754,-0.0217,-0.0149,0.0561,-0.0549,0.0086,0.0118,-0.0251,0.0082,-0.0028,-0.0328,-0.0089,0.1279,-0.0395,-0.0063,-0.0276,0.0137,0.0631,-0.009,0.0286,0.0298,-0.0389,0.0441,0.036,-0.0346,0.0017,-0.0472,0.0081,0.0316,-0.0374,-0.0141,0.0005,-0.021]}
{"key":"[A Human-Centric Take on Model Monitoring] Predictive models are increasingly used to make various consequential decisions in high-stakes domains such as healthcare, finance, and policy. It becomes critical to ensure that these models make accurate predictions, are robust to shifts in the data, do not rely on spurious features, and do not unduly discriminate against minority groups. To this end, several approaches spanning various areas such as explainability, fairness, and robustness have been proposed in recent literature. Such approaches need to be human-centered as they cater to the understanding of the models to their users. However, there is a research gap in understanding the human-centric needs and challenges of monitoring machine learning (ML) models once they are deployed. To fill this gap, we conducted an interview study with 13 practitioners who have experience at the intersection of deploying ML models and engaging with customers spanning domains such as financial services, healthcare, hiring, online retail, computational advertising, and conversational assistants. We identified various human-centric challenges and requirements for model monitoring in real-world applications. Specifically, we found the need and the challenge for the model monitoring systems to clarify the impact of the monitoring observations on outcomes. Further, such insights must be actionable, robust, customizable for domain-specific use cases, and cognitively considerate to avoid information overload.","layer":0,"vector":[-0.027,-0.0216,0.0032,-0.0071,0.0309,-0.0026,0.0474,0.058,0.0201,0.0022,0.0138,-0.0267,0.0,0.0236,0.015,0.0106,-0.0309,0.0501,-0.0354,0.0362,-0.014,-0.0076,-0.0484,-0.0445,0.0118,0.0166,-0.0341,-0.0367,-0.0679,-0.2312,0.0308,-0.0573,0.0783,-0.0187,0.0241,-0.0223,-0.0216,0.0498,0.0172,0.0402,-0.0004,-0.0272,-0.0324,-0.0472,-0.0244,-0.0265,0.0084,0.0086,-0.1051,-0.0283,0.0516,-0.0609,-0.0072,0.0197,0.0538,0.0128,0.0964,0.0301,0.0365,0.0344,0.0388,0.0247,-0.177,0.0601,0.0056,0.0637,-0.0185,-0.039,-0.0094,0.0229,-0.0128,0.0213,0.0155,0.0695,-0.0175,-0.01,0.0217,-0.0352,0.0053,0.0301,0.046,-0.0252,-0.0112,-0.0228,-0.0184,-0.0626,0.0416,0.0021,0.0453,-0.0196,-0.0495,-0.0175,-0.0206,0.021,-0.0535,0.0081,0.0463,0.001,-0.0916,0.2258,0.0123,0.0447,0.0242,-0.0356,0.0438,-0.0491,-0.0461,-0.0363,0.0034,-0.0228,-0.0294,0.0256,0.0525,-0.0249,0.0182,0.0075,0.0561,0.0369,0.0292,0.0018,-0.0019,0.0255,0.0617,-0.0281,0.0395,-0.0418,0.0472,0.1602,0.0088,0.0079,0.0641,-0.0305,-0.0766,-0.0332,0.0107,0.0156,0.0034,0.0208,0.008,0.0266,-0.0467,-0.0152,-0.0287,-0.1166,-0.0649,0.1594,-0.0218,-0.0085,-0.0016,-0.0301,-0.0153,0.0612,-0.0464,-0.0287,0.0265,0.0226,0.0259,0.0487,-0.0534,0.0231,0.0178,-0.0757,-0.0731,0.0624,-0.0184,-0.0977,-0.0026,0.0261,0.0401,0.0034,0.0574,-0.0027,-0.0395,0.0156,0.0376,0.0153,-0.0789,-0.0253,-0.0141,-0.0006,0.0288,-0.0461,-0.0455,0.0464,0.0611,-0.0498,0.0205,-0.0564,0.0463,0.0391,0.0289,-0.0074,-0.032,0.0362,-0.0444,0.0024,-0.0258,-0.0196,0.0147,-0.0137,-0.0411,0.0411,-0.0399,0.0434,0.0115,0.0552,-0.0127,0.0093,0.0747,0.02,-0.0168,0.0396,0.058,-0.0281,-0.0572,0.0073,0.0475,0.0219,0.0164,0.0621,0.0384,0.0476,-0.0305,-0.2246,-0.0135,-0.0216,0.0121,0.0202,-0.0426,0.0079,-0.0131,0.0171,0.0609,0.0698,-0.009,-0.0631,-0.0059,0.0069,0.023,-0.0339,0.0435,-0.0893,0.003,-0.0365,0.0007,-0.0483,-0.1247,0.0371,0.0296,0.2149,0.0295,-0.0029,-0.0153,0.0048,-0.0005,-0.0657,-0.1136,0.0782,0.0118,0.0258,-0.0184,-0.0493,-0.0021,-0.0206,0.0363,-0.0017,-0.1038,-0.0501,-0.0301,-0.0251,0.0038,-0.0854,0.0215,0.0315,-0.0563,0.0857,-0.0192,-0.0209,-0.0465,-0.0773,0.0368,-0.0191,0.0666,0.0307,-0.0428,0.0261,-0.0432,0.0276,-0.0183,-0.0171,-0.0837,-0.0009,-0.0492,-0.0239,0.0999,-0.012,-0.0449,0.0284,0.0488,-0.0018,-0.0921,-0.056,0.0306,0.0916,-0.0005,0.0407,0.0505,0.0573,-0.0044,0.0467,-0.0061,0.0628,-0.0502,-0.0399,-0.0187,-0.0303,-0.0335,0.0377,-0.0499,-0.2473,0.0254,-0.0063,0.0501,-0.0315,0.0243,0.0149,0.0482,-0.0357,0.0217,0.0206,0.0147,0.0482,-0.024,0.0166,0.0326,0.0747,-0.0464,0.0443,-0.0468,0.0475,0.0549,0.2011,-0.0264,0.0407,0.0235,0.0003,-0.0151,0.0629,-0.0051,0.0004,-0.0038,0.0737,-0.0299,0.0393,0.0519,-0.0328,0.0237,0.0507,-0.0081,0.0162,0.0193,-0.0223,-0.0259,0.0844,0.0292,-0.0199,-0.0461,0.0063,0.0169,-0.004,-0.0405,-0.0353,0.0236,0.0202,0.0287,-0.0599,-0.0367,-0.057,-0.0396,0.0067,-0.0505,0.0143,-0.0161,-0.0361]}
{"key":"[Variance-Reduced Off-Policy Memory-Efficient Policy Search] Off-policy policy optimization is a challenging problem in reinforcement learning (RL). The algorithms designed for this problem often suffer from high variance in their estimators, which results in poor sample efficiency, and have issues with convergence. A few variance-reduced on-policy policy gradient algorithms have been recently proposed that use methods from stochastic optimization to reduce the variance of the gradient estimate in the REINFORCE algorithm. However, these algorithms are not designed for the off-policy setting and are memory-inefficient, since they need to collect and store a large ``reference'' batch of samples from time to time. To achieve variance-reduced off-policy-stable policy optimization, we propose an algorithm family that is memory-efficient, stochastically variance-reduced, and capable of learning from off-policy samples. Empirical studies validate the effectiveness of the proposed approaches.","layer":0,"vector":[-0.0636,0.0207,0.0461,-0.0193,0.0032,0.0261,0.0144,0.0591,0.0428,0.0243,0.0406,0.0105,0.0309,0.0778,-0.0251,-0.0052,-0.005,0.0454,0.0057,0.0033,0.0328,-0.0966,-0.0287,-0.048,0.0038,-0.0098,-0.0771,-0.107,-0.038,-0.2323,0.0405,-0.0394,0.0209,-0.0216,0.0033,0.0183,-0.0596,0.0491,-0.0542,0.0122,0.0517,0.0507,-0.0569,-0.0733,-0.0224,-0.0268,-0.0072,-0.0263,-0.006,-0.0405,0.025,-0.0225,0.0163,0.0108,0.0663,0.0302,0.0383,0.0493,0.0341,0.0348,-0.0183,0.0357,-0.1653,0.0309,0.0046,0.0711,-0.0366,-0.0096,0.0528,0.0445,-0.065,0.061,0.0376,0.0462,0.0269,0.0145,-0.0273,-0.0314,0.0037,0.0142,0.0267,-0.0387,-0.0411,0.003,-0.0293,-0.0768,0.0091,-0.0511,0.0682,0.0234,-0.005,0.0128,0.0078,0.0197,-0.087,0.0309,0.0049,0.054,-0.0831,0.1908,-0.0124,0.0731,0.0031,0.0173,0.0259,-0.0598,-0.0512,0.0138,-0.0224,-0.025,-0.0063,-0.0147,0.0348,-0.0246,-0.0031,-0.0059,0.0506,0.0199,0.0052,-0.0028,0.0091,0.0099,0.0306,-0.0089,0.0003,-0.0815,0.0056,0.1428,-0.0141,0.0133,0.0287,-0.063,-0.0098,-0.0602,0.0414,0.0407,0.0236,-0.0165,0.0484,-0.0124,-0.0221,-0.001,0.0311,-0.1399,-0.031,0.1269,0.0154,0.0447,-0.0148,-0.0268,0.0161,0.0225,0.021,-0.0223,0.0093,0.0309,-0.003,0.0864,-0.0492,-0.0002,-0.0298,-0.0713,-0.0244,0.0799,-0.0523,-0.068,-0.075,-0.0067,-0.0337,-0.0155,0.045,0.0322,-0.073,0.0096,0.0788,-0.0032,-0.0712,-0.0121,0.0081,0.0127,-0.0001,-0.0829,-0.0617,0.0032,0.0265,-0.0061,0.0214,-0.0536,-0.0098,0.0008,-0.02,-0.0203,0.0244,-0.0206,-0.0628,-0.0296,0.0226,-0.0292,0.0412,-0.0088,0.0066,-0.0184,-0.0615,0.0479,-0.0028,0.0335,-0.0369,-0.0433,0.0784,0.0064,-0.0064,0.0216,0.0493,-0.0063,-0.0471,0.0017,0.0315,0.0338,0.0075,0.0709,0.0461,-0.0185,-0.0317,-0.2083,-0.008,-0.0446,0.0101,0.0703,-0.066,0.0391,-0.0334,0.0547,0.064,0.0349,-0.0251,-0.0248,0.0579,-0.0193,0.0652,0.0681,-0.0002,0.01,-0.0023,0.0397,0.0149,-0.0291,-0.11,0.0822,0.003,0.2011,-0.0114,0.0272,-0.0197,0.0182,0.0317,0.0015,-0.0973,0.03,0.0419,0.0437,-0.0217,0.0294,-0.044,-0.0152,0.0609,-0.0352,-0.1048,-0.0303,-0.0488,-0.0273,0.0603,-0.0674,0.0091,0.0365,-0.0101,0.0226,-0.0245,0.0097,-0.0161,-0.0984,0.0205,-0.0249,0.021,0.0127,-0.0491,0.014,-0.0577,0.0386,-0.0026,0.0278,-0.0411,0.0049,0.0144,-0.0219,0.0442,0.0048,-0.0072,0.0091,0.0125,-0.0082,-0.004,-0.0863,-0.0322,0.0767,-0.0468,0.0305,0.0517,0.0117,-0.0078,0.0639,-0.0179,0.0305,0.0156,-0.0112,0.0228,-0.0393,-0.0111,0.0381,-0.0073,-0.2947,0.0646,0.0236,0.0199,-0.0062,0.0131,0.0588,-0.0059,-0.0424,0.0083,-0.0244,0.0606,0.0412,0.0224,0.022,0.0295,0.1,-0.026,0.0839,-0.0831,0.0419,0.0417,0.1991,-0.0362,0.0295,-0.0156,-0.0464,0.0093,0.0288,-0.0232,-0.0194,0.0006,0.0587,-0.0462,0.0708,0.0869,-0.0503,0.0316,0.0121,-0.0155,-0.0427,-0.0061,-0.008,-0.0161,0.125,-0.0086,-0.04,-0.0295,-0.0112,0.0477,-0.0466,0.0074,-0.0234,-0.0134,0.0443,0.0387,-0.0534,-0.0862,-0.022,-0.0211,0.0146,-0.0586,0.0087,-0.008,-0.005]}
{"key":"[Analog readout for optical reservoir computers] Reservoir computing is a new, powerful and flexible machine learning technique that is easily implemented in hardware. Recently, by using a time-multiplexed architecture, hardware reservoir computers have reached performance comparable to digital implementations. Operating speeds allowing for real time information operation have been reached using optoelectronic systems. At present the main performance bottleneck is the readout layer which uses slow, digital postprocessing. We have designed an analog readout suitable for time-multiplexed optoelectronic reservoir computers, capable of working in real time. The readout has been built and tested experimentally on a standard benchmark task. Its performance is better than non-reservoir methods, with ample room for further improvement. The present work thereby overcomes one of the major limitations for the future development of hardware reservoir computers.","layer":2,"vector":[-0.0566,-0.0103,0.0442,-0.0102,-0.0014,0.0022,0.0417,0.0063,0.0156,-0.0331,0.0491,-0.0444,0.0461,0.0079,0.021,-0.0348,-0.0107,0.006,0.0134,0.0067,0.057,-0.0276,-0.0915,-0.0955,-0.035,-0.0009,-0.0319,0.0016,-0.0763,-0.2241,0.0214,-0.0202,0.0473,-0.0322,0.025,-0.0508,-0.0315,0.0274,-0.0286,0.04,0.0193,0.0095,-0.0384,-0.0553,-0.0014,-0.0388,0.0049,-0.0546,-0.0035,-0.0332,-0.0052,0.0112,0.0543,0.0073,0.0317,0.0236,0.0341,0.0358,0.0337,-0.0086,0.0304,0.0774,-0.2036,0.0633,0.0113,0.0141,0.0245,-0.052,0.0361,0.0264,-0.032,0.0492,0.0371,0.0158,0.0026,-0.0052,0.0087,-0.0182,-0.0329,0.0214,0.036,-0.0429,-0.0104,-0.044,-0.0258,0.018,0.0423,-0.0122,0.0246,0.0092,-0.0351,0.0013,-0.0293,-0.0043,-0.0281,0.0066,0.0108,-0.0037,-0.0146,0.2221,-0.0858,0.0417,0.0155,-0.0407,0.0526,-0.0592,-0.0259,-0.0236,-0.0421,-0.0134,-0.0026,-0.0502,0.0441,-0.0281,0.0176,0.0343,0.0224,-0.0001,-0.0027,0.0163,-0.0185,0.0063,-0.0026,0.0021,0.0346,-0.0749,0.05,0.1153,-0.0065,0.0584,0.0117,-0.065,-0.0199,-0.0314,0.0576,0.0367,0.004,-0.0231,0.0176,-0.0348,-0.0646,-0.0339,0.0491,-0.0391,-0.0358,0.1582,-0.0294,0.0498,-0.045,-0.0223,0.012,0.0511,0.0022,-0.0422,-0.0143,0.0332,0.0204,0.0011,-0.0706,0.0197,-0.0294,-0.0519,-0.0283,0.102,0.0461,-0.101,-0.0286,0.0055,-0.0107,-0.0231,0.0699,0.0277,-0.0384,0.0196,0.0505,-0.0052,-0.0501,-0.0288,-0.0122,-0.0049,0.0248,-0.0463,-0.0149,0.0342,0.0561,-0.0166,0.0004,-0.0495,-0.0237,0.0682,-0.0343,-0.0109,-0.0275,0.0089,-0.0489,-0.0377,-0.0195,-0.0113,0.0219,-0.0602,0.0414,-0.0098,-0.0499,0.0252,0.0301,-0.0146,0.0015,0.0151,0.0331,0.0615,-0.029,-0.0423,0.0375,-0.0348,-0.0519,-0.0234,0.0305,0.0131,0.0342,0.0396,0.0253,-0.0568,-0.1272,-0.2387,0.046,-0.0055,-0.0303,0.0582,-0.0829,0.0305,-0.0112,0.0335,0.0582,0.0397,-0.0003,0.0316,0.0224,-0.035,0.0569,0.0711,0.0144,-0.0499,-0.0339,-0.0131,0.0571,-0.0137,-0.0732,0.0712,0.0132,0.2136,0.0044,0.0335,-0.0474,0.0338,0.013,-0.026,-0.0712,0.0257,0.0423,0.0744,0.0215,-0.0178,0.01,-0.0146,0.0438,-0.0158,-0.1,-0.0163,-0.0197,0.0166,0.0454,-0.0436,0.0023,0.0225,-0.1054,0.0309,-0.0009,0.033,-0.0392,-0.078,0.0498,-0.0217,0.0172,0.0286,-0.0325,0.0099,-0.0173,0.0442,-0.0186,-0.0443,-0.0391,0.0472,-0.0267,0.0075,0.0875,0.0091,0.0143,0.0819,-0.0235,0.0256,-0.0672,0.0028,0.0067,0.0775,-0.0339,0.044,0.0715,0.0471,0.0145,0.0499,0.003,-0.0083,0.036,-0.0268,-0.0051,-0.027,-0.0156,0.0361,-0.0002,-0.2869,0.0225,0.0205,0.015,-0.0142,0.0054,0.0535,0.0352,0.0232,0.0112,-0.0646,0.0454,-0.0042,-0.0024,0.0029,0.0846,0.0568,-0.0537,0.0198,-0.0433,0.0389,0.0399,0.2261,-0.0634,0.0097,0.0514,-0.0158,0.0208,0.0102,-0.0401,0.028,0.0448,0.0819,-0.0735,0.015,0.0919,-0.0553,0.0174,0.0535,-0.0118,0.0232,0.0245,-0.0603,-0.0521,0.0775,0.0063,-0.028,-0.0604,-0.0131,0.0397,-0.0176,0.0477,0.0158,0.0129,0.0039,0.0411,-0.0437,-0.0646,-0.0039,-0.0247,0.0208,-0.0979,0.011,-0.0393,-0.0079]}
{"key":"[Effect of Various Regularizers on Model Complexities of Neural Networks in Presence of Input Noise] Deep neural networks are over-parameterized, which implies that the number of parameters are much larger than the number of samples used to train the network. Even in such a regime deep architectures do not overfit. This phenomenon is an active area of research and many theories have been proposed trying to understand this peculiar observation. These include the Vapnik Chervonenkis (VC) dimension bounds and Rademacher complexity bounds which show that the capacity of the network is characterized by the norm of weights rather than the number of parameters. However, the effect of input noise on these measures for shallow and deep architectures has not been studied. In this paper, we analyze the effects of various regularization schemes on the complexity of a neural network which we characterize with the loss, $L_2$ norm of the weights, Rademacher complexities (Directly Approximately Regularizing Complexity-DARC1), VC dimension based Low Complexity Neural Network (LCNN) when subject to varying degrees of Gaussian input noise. We show that $L_2$ regularization leads to a simpler hypothesis class and better generalization followed by DARC1 regularizer, both for shallow as well as deeper architectures. Jacobian regularizer works well for shallow architectures with high level of input noises. Spectral normalization attains highest test set accuracies both for shallow and deeper architectures. We also show that Dropout alone does not perform well in presence of input noise. Finally, we show that deeper architectures are robust to input noise as opposed to their shallow counterparts.","layer":2,"vector":[-0.0142,-0.0396,0.0112,0.0199,-0.0123,0.0254,0.0264,-0.0024,0.0669,-0.0604,-0.0084,-0.0171,0.0628,0.0662,0.0537,-0.0102,0.0333,-0.0039,-0.0675,0.0326,0.0237,-0.0303,0.0054,-0.0595,-0.0233,-0.0075,-0.0137,-0.0577,-0.0268,-0.3023,0.0259,-0.0251,0.0796,-0.028,0.0348,-0.0602,-0.0266,0.0269,-0.062,0.0458,0.0515,0.0544,0.0097,-0.0836,-0.0124,-0.0555,-0.0302,-0.0191,0.0001,-0.0431,0.0269,-0.0302,0.014,0.0234,0.0655,0.015,0.0555,0.0253,0.0599,0.0705,0.0017,0.0627,-0.1845,0.0314,0.0132,-0.0029,-0.0691,-0.0546,0.0116,0.0604,0.0106,0.0102,0.0234,0.031,0.055,-0.0114,0.0097,-0.0424,0.001,0.0408,0.0441,-0.0413,-0.0193,-0.025,0.0101,-0.0752,0.015,-0.0218,0.0124,-0.0135,-0.0429,-0.0025,-0.0256,0.0199,-0.0203,-0.0196,0.0528,0.0209,-0.0587,0.1692,-0.0546,0.0126,0.0563,-0.0155,0.033,0.0164,-0.0574,-0.0337,-0.0181,-0.0066,-0.0246,-0.044,0.0222,0.005,-0.0001,0.0117,0.0549,0.0582,-0.017,-0.0326,-0.0563,-0.0097,0.0304,0.0059,0.0347,-0.0437,-0.0193,0.1105,0.0419,0.0404,0.0363,-0.0371,-0.06,-0.0353,0.0312,0.0115,0.0087,0.0033,0.0019,-0.0092,-0.0681,-0.0426,0.0165,-0.0534,-0.0373,0.0806,-0.0553,0.0211,0.0043,-0.0176,-0.0277,0.0216,-0.0157,-0.0382,0.0598,0.0319,-0.0059,0.0314,-0.0911,-0.0046,-0.0168,-0.0395,-0.0058,0.1152,-0.0275,-0.0666,-0.0585,-0.0259,0.0188,0.0012,0.0619,0.023,-0.0216,0.0008,0.0623,0.0241,-0.1122,0.0212,-0.0314,0.0091,0.0074,-0.0383,0.0128,0.0396,0.0757,-0.0183,0.0143,-0.0576,0.0271,0.0324,-0.0383,0.0322,-0.0708,0.0025,-0.0237,0.0032,-0.0237,0.006,0.0032,-0.0385,0.005,0.0117,-0.0176,0.0308,-0.0131,0.0375,0.0005,0.0048,0.0208,0.0459,-0.0352,-0.0121,0.0538,-0.046,-0.0199,0.0212,0.0231,0.0374,-0.0028,0.0343,0.0315,-0.0528,-0.09,-0.2288,-0.0022,0.0072,-0.0151,0.0952,-0.0999,0.0721,-0.0164,0.0471,0.0505,-0.0004,0.0017,-0.0217,0.0158,0.0027,0.0884,0.0358,0.0326,-0.0495,0.0068,-0.0305,0.0533,-0.0151,-0.0525,0.0455,-0.0229,0.1979,-0.0513,0.0832,-0.0581,0.014,0.0182,-0.0351,-0.0385,0.0648,0.0347,0.0913,0.0217,-0.0222,-0.0201,-0.0293,-0.0005,-0.0112,-0.1057,-0.0554,0.0187,0.0036,-0.0121,-0.0945,-0.0088,0.0344,-0.0254,0.0449,0.0374,0.0534,-0.0202,-0.14,0.0036,-0.0541,0.0371,0.031,-0.0826,0.0079,-0.0453,0.0361,0.0176,0.0179,-0.0346,0.0501,-0.0229,-0.0208,0.0855,0.022,0.0052,0.056,-0.0236,0.0246,-0.0094,0.0257,-0.0195,0.069,-0.0271,0.0044,0.0062,0.0171,0.0265,0.0953,-0.0367,0.0113,-0.0319,-0.0048,-0.0032,-0.0582,0.0069,0.022,-0.0256,-0.2772,0.0184,0.0071,0.0204,0.0041,0.0028,0.0219,0.0278,-0.0389,0.0072,0.0163,0.0578,0.0345,-0.0255,0.0146,0.0672,0.0544,-0.0428,0.0506,-0.0435,-0.0299,0.0624,0.2069,-0.0928,0.0012,0.005,-0.0195,0.002,0.0152,-0.0327,0.0632,0.0335,0.1114,-0.0531,0.0314,0.0993,-0.004,-0.0023,0.023,0.0151,0.0301,-0.0341,-0.0471,-0.0055,0.0883,-0.0596,-0.0058,-0.0073,0.0284,0.0609,-0.0312,-0.0236,0.0037,-0.0095,0.0273,0.0202,-0.0352,-0.0605,-0.0072,-0.027,0.0051,-0.0401,-0.0016,0.0055,-0.0167]}
{"key":"[Yes, IoU loss is submodular - as a function of the mispredictions] This note is a response to [7] in which it is claimed that [13, Proposition 11] is false. We demonstrate here that this assertion in [7] is false, and is based on a misreading of the notion of set membership in [13, Proposition 11]. We maintain that [13, Proposition 11] is true. ([7] = arXiv:1809.00593, [13] = arXiv:1512.07797)","layer":2,"vector":[-0.0427,-0.0403,0.0359,0.0193,-0.0217,0.0274,0.0766,0.0176,0.026,-0.0156,0.0579,-0.1343,0.052,0.0179,-0.0009,-0.0385,-0.0129,0.0162,-0.0459,0.0471,0.0999,-0.0582,-0.0331,-0.0231,0.0465,0.0367,-0.046,-0.0592,-0.0295,-0.2393,0.0071,-0.0258,0.0265,-0.0062,0.0006,0.0065,-0.0478,0.02,-0.0428,0.0643,0.0634,0.0298,-0.0019,-0.0251,-0.058,0.0022,-0.0025,-0.0242,-0.0182,-0.0368,0.0513,0.0034,0.0156,0.0729,0.0195,0.0398,0.0384,0.0614,0.0483,0.021,0.0218,0.0327,-0.2128,0.0486,0.0294,0.0428,-0.0008,-0.0543,0.0301,0.0375,-0.0248,0.0036,0.0266,0.0294,0.0258,-0.0263,0.0235,-0.0068,-0.0485,0.0306,0.0177,-0.0321,-0.0386,0.0017,-0.0165,-0.0851,0.0055,-0.1136,0.007,-0.0046,-0.0057,-0.0074,-0.0128,0.0301,-0.0282,0.0174,0.035,0.0448,-0.0618,0.1598,-0.0567,0.0285,0.0195,0.0107,-0.0143,-0.0688,0.0115,-0.0471,-0.0022,-0.0109,-0.0288,-0.0493,0.0722,-0.0418,0.009,-0.0042,0.0602,0.0564,-0.0097,0.003,-0.0409,0.0374,0.0391,-0.0044,0.0132,-0.0726,0.0203,0.0918,0.016,0.0192,0.0188,-0.043,-0.0529,-0.034,0.0108,0.0568,0.0071,-0.0121,0.0432,0.0083,0.0098,-0.0964,0.0257,-0.0517,0.003,0.1249,-0.0052,0.0281,0.0129,0.0408,-0.0211,0.0244,-0.0207,-0.0783,-0.0201,0.0015,0.0004,0.0527,-0.0361,-0.0029,0.0019,-0.033,-0.0297,0.0859,-0.0003,-0.0248,-0.0293,0.0067,0.0333,-0.0202,0.0247,0.0673,-0.0377,-0.0121,0.0832,0.0318,-0.0763,-0.0406,-0.0052,0.013,0.0429,-0.0379,-0.0451,-0.004,0.0437,-0.0088,-0.0206,-0.0425,0.0146,0.0578,-0.0291,0.0496,-0.1223,-0.0112,-0.0361,-0.0265,-0.0077,-0.0216,0.0088,-0.0162,0.036,0.0136,-0.0333,0.0464,-0.0452,0.0143,-0.0078,-0.0526,0.0236,0.0291,-0.0337,-0.0021,0.0229,-0.0028,-0.0228,-0.0138,0.0437,0.0247,0.0218,0.0343,0.0546,-0.0808,-0.0329,-0.2473,-0.0466,-0.0122,-0.0446,0.0267,-0.0553,0.0121,0.0005,0.0056,0.0368,0.0328,0.0177,-0.0477,-0.0083,-0.0006,0.0704,0.016,0.0034,-0.0345,0.004,-0.0794,0.0528,-0.0347,-0.0519,0.0327,-0.0058,0.213,0.0518,0.0423,-0.0307,0.0204,0.0449,-0.0515,-0.0622,0.0412,0.055,-0.005,-0.027,-0.0158,-0.0258,-0.0579,0.0432,0.0064,-0.0384,-0.0296,-0.0201,-0.0344,0.0476,-0.0608,0.0637,0.0515,-0.0213,0.033,0.0298,0.0521,-0.0157,-0.1161,-0.049,0.0,0.0508,0.0053,0.0033,0.0214,-0.067,0.0841,0.0065,-0.0014,-0.0317,0.0032,-0.0033,-0.0279,0.0606,-0.0266,-0.0076,0.0152,0.0068,0.0165,-0.0435,-0.0631,-0.0014,0.0809,-0.0727,0.044,0.0234,0.0276,0.0306,0.0623,0.0517,-0.0407,-0.0157,0.0196,0.0406,-0.0493,0.0094,0.0349,-0.0209,-0.2797,0.039,0.0317,-0.0141,-0.0547,0.0268,0.0247,0.0177,-0.0877,0.0106,0.0479,0.0553,0.0251,-0.0252,0.0334,0.0507,0.0374,-0.0515,0.0706,0.0074,0.013,0.039,0.2132,-0.032,0.0137,0.0146,-0.0081,0.0271,0.0335,0.0092,0.0222,-0.0066,0.1039,-0.0556,0.0298,0.0018,-0.0559,0.0542,0.0386,-0.0354,-0.0378,-0.0134,-0.0562,0.0073,0.1692,0.0174,-0.029,-0.0531,0.0314,0.0452,-0.0588,-0.0178,0.0055,-0.0195,0.0225,0.01,-0.0558,-0.0593,-0.0247,-0.0172,-0.0068,-0.0472,-0.0412,0.0606,0.0195]}
{"key":"[Contextual Multi-armed Bandit Algorithm for Semiparametric Reward Model] Contextual multi-armed bandit (MAB) algorithms have been shown promising for maximizing cumulative rewards in sequential decision tasks such as news article recommendation systems, web page ad placement algorithms, and mobile health. However, most of the proposed contextual MAB algorithms assume linear relationships between the reward and the context of the action. This paper proposes a new contextual MAB algorithm for a relaxed, semiparametric reward model that supports nonstationarity. The proposed method is less restrictive, easier to implement and faster than two alternative algorithms that consider the same model, while achieving a tight regret upper bound. We prove that the high-probability upper bound of the regret incurred by the proposed algorithm has the same order as the Thompson sampling algorithm for linear reward models. The proposed and existing algorithms are evaluated via simulation and also applied to Yahoo! news article recommendation log data.","layer":0,"vector":[-0.0829,0.0446,0.0417,-0.0401,0.0241,0.0129,0.0738,0.0477,0.0329,0.0133,-0.0022,-0.0027,0.0136,0.0713,-0.0057,0.0026,0.0005,0.0168,-0.032,0.0091,0.0355,-0.0664,0.031,-0.061,0.0797,0.0216,-0.0311,-0.0397,-0.0565,-0.187,-0.0092,-0.0098,0.0522,-0.0356,-0.022,-0.0164,-0.024,0.0267,0.0114,0.1029,-0.0004,0.0492,-0.0508,-0.0895,-0.043,-0.069,-0.0076,-0.001,-0.0237,-0.0287,0.0243,-0.0148,0.0394,0.0271,0.0424,0.0139,0.0111,0.0718,0.0161,0.0593,0.0193,0.058,-0.1531,0.0224,0.0002,0.0222,-0.0485,0.0072,0.0001,0.0461,-0.0107,0.0435,0.0357,0.0493,0.0557,-0.0409,-0.0045,-0.0258,-0.0114,0.0167,-0.0158,-0.0365,-0.0414,-0.0019,-0.0259,-0.0411,-0.0189,-0.085,0.0586,0.0294,0.0012,0.0134,-0.0167,0.0093,-0.0761,-0.0785,0.0001,-0.0053,-0.0521,0.1979,-0.0122,0.0377,0.0381,-0.0208,0.0327,-0.0266,-0.0131,-0.041,-0.0231,0.0047,0.0182,-0.0142,0.0533,-0.0297,0.0052,0.0275,0.0803,0.0608,0.0428,-0.0385,-0.0525,0.0229,0.0592,-0.0394,0.0079,-0.0467,0.007,0.1349,0.0326,0.0199,0.0195,-0.0353,-0.0318,-0.0241,0.0335,-0.0001,-0.0388,-0.0055,0.0366,-0.0214,-0.0384,-0.0406,0.0246,-0.1126,-0.0418,0.1381,-0.0099,0.0265,-0.0923,-0.0276,0.0143,-0.0037,0.0118,-0.0155,0.0323,0.01,0.0254,0.0627,-0.0242,0.0127,-0.0374,-0.064,-0.0047,0.0682,0.0175,-0.0679,-0.0096,0.0347,-0.043,0.0326,0.0284,0.0238,-0.0835,-0.0058,0.1301,0.0256,-0.0769,-0.031,0.0204,-0.0216,0.0365,-0.0136,-0.0253,0.0282,-0.013,-0.0186,0.024,-0.0722,0.0577,0.0479,-0.0199,-0.0034,-0.0023,-0.012,-0.0111,-0.0451,0.0022,-0.0088,0.0328,-0.0052,0.0318,-0.0298,-0.0505,-0.0247,0.0121,0.0088,-0.0282,-0.036,0.0325,0.0007,-0.0402,0.0175,0.0268,-0.0094,-0.0456,0.0139,0.0417,0.034,0.0048,0.085,0.0318,0.0213,0.0062,-0.2424,-0.0076,-0.0183,0.0251,0.0489,-0.0382,0.0307,-0.0354,0.0595,0.0744,0.0844,-0.054,-0.0285,0.0406,-0.0001,0.0837,0.0333,0.0233,-0.0004,-0.0037,-0.0186,0.0125,-0.0137,-0.0689,0.0362,0.0023,0.1893,0.0651,0.0363,-0.0399,0.0365,0.0211,-0.0099,-0.1007,0.0384,0.0526,0.0302,-0.0225,-0.022,-0.0519,-0.0177,0.0251,-0.0165,-0.0722,-0.0868,-0.0532,-0.0464,0.0269,-0.0213,0.0249,0.0622,-0.0246,0.0462,-0.0157,-0.03,-0.0305,-0.0599,0.0322,-0.0401,0.0316,0.0273,-0.067,0.0331,-0.0428,0.0703,-0.021,0.004,0.0109,0.0299,-0.0164,-0.0136,0.0703,-0.0116,0.0276,0.0262,-0.0073,0.0432,-0.048,-0.0226,-0.0409,0.0477,-0.0504,0.048,0.043,-0.0245,-0.0145,0.0722,-0.0214,0.0025,0.0221,-0.047,-0.004,-0.0771,-0.0005,0.0751,-0.0051,-0.3338,0.062,0.0084,0.0301,-0.0155,0.0028,0.0315,-0.0096,-0.0435,0.0375,-0.0032,0.088,0.0281,-0.0277,0.0026,-0.0017,0.0314,-0.05,0.0074,-0.044,0.0181,0.0373,0.2162,-0.0342,0.0367,0.0286,-0.0069,0.0146,0.0347,0.0043,-0.0188,-0.0182,0.0974,-0.0781,0.0375,0.0819,-0.0569,0.0646,0.0326,-0.0426,-0.0473,0.0327,-0.0542,-0.0631,0.0954,0.0237,-0.038,-0.0596,-0.0275,0.0256,-0.0279,0.0001,-0.0259,-0.0503,0.0436,0.0036,-0.0139,-0.0962,-0.0088,-0.0165,-0.0239,-0.047,0.0014,-0.0186,-0.0029]}
{"key":"[Beyond the Hype: A Real-World Evaluation of the Impact and Cost of Machine Learning-Based Malware Detection] In this paper, we present a scientific evaluation of four market-leading malware detection tools to assist an organization with two primary questions: To what extent do ML-based tools accurately classify previously- and never-before-seen files? Is it worth purchasing a network-level malware detector? To identify weaknesses, we tested each tool against 3,536 total files (2,554 or 72\\% malicious, 982 or 28\\% benign) of a variety of file types, including hundreds of malicious zero-days, polyglots, and APT-style files, delivered on multiple protocols. We present statistical results on detection time and accuracy, consider complementary analysis (using multiple tools together), and provide two novel applications of the recent cost-benefit evaluation procedure of Iannacone \\& Bridges. While the ML-based tools are more effective at detecting zero-day files and executables, the signature-based tool may still be an overall better option. Both network-based tools provide substantial (simulated) savings when paired with either host tool, yet both show poor detection rates on protocols other than HTTP or SMTP. Our results show that all four tools have near-perfect precision but alarmingly low recall, especially on file types other than executables and office files -- 37% of malware tested, including all polyglot files, were undetected. Priorities for researchers and takeaways for end users are given.","layer":0,"vector":[-0.0852,-0.0312,0.0289,0.0046,0.1,0.0045,0.0405,0.0413,0.0138,-0.0364,0.0281,0.0272,0.0209,0.0115,0.0186,-0.036,0.0417,-0.0383,-0.0184,0.0312,0.0252,-0.0205,0.0205,-0.0387,0.0146,0.0573,-0.0327,0.0311,-0.1001,-0.212,-0.0002,-0.0982,0.0368,-0.0118,0.0026,-0.0128,0.0002,0.02,0.0052,0.0178,0.0241,0.0624,-0.0376,-0.0594,-0.0742,-0.1024,-0.0385,-0.0296,-0.0202,-0.0142,-0.001,-0.034,0.0099,0.0655,0.0003,-0.0009,0.0572,0.0655,0.0323,0.0541,0.024,0.0545,-0.1487,0.0512,0.0083,0.0112,-0.0782,-0.0355,0.0204,0.0513,-0.0538,-0.0011,-0.0174,0.0568,-0.0136,0.0063,0.0039,-0.0082,-0.0086,-0.0133,-0.0356,-0.028,-0.0167,-0.0,-0.0008,-0.0068,0.0027,0.0047,0.0769,-0.0312,-0.0286,0.0126,-0.012,-0.0235,-0.0425,0.0122,0.043,0.0133,-0.0758,0.2028,-0.0855,-0.0014,0.0116,-0.0604,0.0477,-0.0154,0.0172,-0.0387,-0.0033,-0.0194,0.0078,0.0135,0.0418,-0.0576,0.0385,-0.0006,0.0219,0.0051,-0.0035,-0.001,-0.0211,-0.0051,0.0997,-0.0045,0.0207,-0.0369,0.0414,0.1317,0.0487,0.0325,-0.0053,-0.0288,-0.008,-0.0081,0.0347,0.082,-0.0158,0.0224,0.0391,-0.0408,-0.0469,-0.0343,0.0191,-0.0552,-0.0597,0.1051,-0.0408,0.0343,-0.0006,-0.0519,-0.0185,0.0597,-0.051,-0.0676,0.018,-0.0022,0.0692,0.0512,-0.076,0.0178,-0.0302,-0.0303,-0.046,0.0891,0.0248,-0.0855,-0.0155,-0.0268,-0.0249,-0.0226,0.0504,0.0441,0.0041,0.0267,-0.0042,-0.0377,-0.0637,-0.0344,-0.0227,0.0308,0.0256,-0.0094,-0.0315,0.0482,0.0626,-0.0203,-0.0247,-0.0187,0.039,0.0628,-0.0679,0.0004,0.0554,-0.0296,0.0034,-0.0029,-0.0151,0.0045,0.0091,-0.0406,0.034,0.0192,-0.002,0.0101,-0.0325,0.048,-0.0096,-0.0088,0.0531,0.0082,-0.0149,0.0121,0.0215,-0.0132,-0.0513,-0.0085,0.0313,0.093,0.0182,0.0571,-0.0057,-0.0332,-0.0629,-0.2168,-0.0071,-0.0198,-0.0077,0.0605,-0.096,0.0319,-0.0431,0.0456,0.0202,0.0843,-0.0274,-0.0226,0.0303,-0.0005,0.0837,0.0167,0.0431,-0.0734,0.0542,-0.035,-0.0193,0.0113,-0.0724,0.0256,0.0062,0.1809,0.072,0.0382,-0.0571,0.0413,0.02,-0.0189,-0.1236,0.034,0.0558,0.061,0.0036,-0.044,-0.0103,-0.0637,0.0547,-0.0047,-0.1217,-0.0071,-0.0314,-0.0146,0.0189,-0.0351,0.0124,0.0153,0.0033,0.081,0.0093,0.0109,-0.0984,-0.0404,0.0245,-0.0204,0.0637,0.0414,-0.0518,0.0359,-0.0819,0.0765,-0.0479,-0.0187,-0.01,0.0526,-0.019,-0.0434,0.1022,0.0045,-0.0365,0.0382,0.0071,0.0386,-0.0912,-0.0912,-0.0395,0.044,0.0304,0.0346,0.0005,-0.0122,0.0316,0.0691,0.0106,0.0493,-0.0315,0.0215,0.003,-0.0355,-0.0237,0.03,0.0347,-0.2986,0.0456,0.0133,0.0375,-0.0479,-0.0085,0.0304,0.0091,-0.0107,-0.0,0.022,0.0089,0.013,-0.0692,0.0005,0.0251,0.0468,-0.032,0.0363,-0.0476,-0.0052,0.0143,0.2435,0.0064,-0.0348,0.0212,0.0202,0.0239,0.0131,-0.062,0.0529,-0.0018,0.0553,-0.0677,0.0223,0.0605,-0.0363,0.0219,0.0034,-0.0156,0.0112,0.0223,-0.062,-0.0085,0.0569,-0.029,0.0007,-0.0609,0.0491,0.056,-0.0537,-0.0233,-0.0268,0.0318,0.0472,0.012,-0.0342,-0.0322,-0.0589,-0.0348,0.0438,-0.0114,0.002,0.0477,-0.0366]}
{"key":"[On the Double Descent of Random Features Models Trained with SGD] We study generalization properties of random features (RF) regression in high dimensions optimized by stochastic gradient descent (SGD). In this regime, we derive precise non-asymptotics error bounds of RF regression under both constant and polynomial-decay step-size SGD setting, and observe the double descent phenomenon both theoretically and empirically. Our analysis shows how to cope with multiple randomness sources of initialization, label noise, and data sampling (as well as stochastic gradients) with no closed-form solution, and also goes beyond the commonly-used Gaussian/spherical data assumption. Our theoretical results demonstrate that, with SGD training, RF regression still generalizes well for interpolation learning, and is able to characterize the double descent behavior by the unimodality of variance and monotonic decrease of bias. Besides, we also prove that the constant step-size SGD setting incurs no loss in convergence rate when compared to the exact minimum-norm interpolator, as a theoretical justification of using SGD in practice.","layer":2,"vector":[-0.0146,-0.0171,0.0418,-0.0104,0.0284,0.0157,0.0044,0.0276,0.0154,-0.0244,0.0254,-0.0281,0.0427,0.0853,0.0027,0.0283,0.014,0.0284,-0.0915,0.0411,0.0449,0.0058,0.0237,-0.0676,0.0065,-0.0266,-0.0518,-0.0336,-0.0177,-0.3011,0.0359,-0.0127,0.0231,-0.0264,0.0149,-0.0303,-0.0119,0.0633,-0.0377,0.042,-0.0181,0.0242,-0.0543,-0.0408,-0.0186,-0.0308,-0.0247,-0.0218,-0.0592,-0.0017,0.0119,-0.0111,-0.0058,0.0303,0.0046,0.0262,0.0401,0.0191,0.0459,0.0704,-0.0129,0.039,-0.1648,0.0201,0.044,0.0217,-0.0702,-0.012,0.0034,0.0451,-0.0007,0.0241,-0.0007,0.0629,-0.0065,0.0005,0.0064,-0.0266,-0.022,0.0474,0.0653,-0.0067,-0.0481,0.0065,-0.0541,-0.0531,0.0398,-0.0284,0.0617,0.0194,-0.0208,-0.0216,-0.0497,0.0556,-0.0696,0.0146,0.0573,0.0146,-0.0244,0.2024,-0.0573,0.0536,0.0268,0.0019,0.0227,-0.0233,-0.0451,-0.0332,0.0155,0.0067,-0.012,-0.0077,0.0131,-0.0584,-0.0245,-0.0148,0.0748,0.0379,-0.0095,0.0007,-0.0319,-0.0149,0.0591,-0.0139,0.0412,-0.0858,-0.0072,0.1184,0.0354,0.0147,0.0599,-0.0385,-0.0325,-0.0394,0.021,0.0239,0.0197,0.0316,0.0313,-0.0287,-0.069,-0.0009,0.0263,-0.0713,-0.0414,0.1155,-0.1045,0.0257,-0.0224,-0.0135,0.0047,0.0348,0.0275,-0.0565,0.0208,0.047,0.0329,0.0025,-0.0604,-0.0012,-0.0312,-0.0378,-0.0168,0.0855,0.0009,-0.0805,-0.0298,0.0151,-0.0241,0.0188,0.0325,0.0554,-0.0326,-0.0087,0.0913,0.042,-0.0903,0.0135,-0.0113,0.0101,0.0199,-0.0219,-0.0625,0.0347,0.0311,-0.0041,0.0235,-0.0559,0.0031,0.0426,0.0029,0.0243,-0.0446,-0.0467,-0.0165,-0.0323,-0.0201,-0.0055,0.0651,-0.024,0.0206,0.0012,-0.0426,0.0551,0.0273,0.0082,0.0042,-0.0142,0.0011,0.028,-0.0463,-0.0269,0.0196,-0.0196,-0.0008,0.0171,0.0174,0.0381,-0.0272,0.0418,0.0533,-0.0191,-0.0962,-0.2381,-0.0323,0.036,-0.0063,0.0249,-0.0937,0.0294,0.0085,0.0956,0.0773,0.0399,-0.0123,-0.0299,-0.0028,0.0016,0.0545,0.0105,-0.0069,-0.0238,0.0226,-0.0052,0.0334,0.011,-0.0501,0.0604,-0.0168,0.186,0.0124,0.0461,-0.042,0.0089,0.0475,-0.0182,-0.0766,0.0968,0.0199,0.0749,-0.0208,-0.0617,-0.0191,-0.0326,0.0465,0.0914,-0.083,-0.0469,-0.0396,-0.0712,0.0179,-0.0622,0.0276,0.0342,-0.0064,0.0851,-0.0235,0.0255,0.0072,-0.1063,0.0113,0.001,0.0079,0.0091,-0.0877,0.0428,-0.0934,0.0256,0.0127,-0.0392,-0.0319,0.001,-0.02,-0.0258,0.0678,-0.0175,0.0046,0.051,-0.0113,0.0371,-0.0034,-0.0628,-0.0195,0.0541,-0.05,0.018,0.041,0.0256,0.0062,0.0858,-0.0383,0.021,-0.0362,-0.0455,-0.0086,-0.0444,0.0043,0.0241,0.0017,-0.2696,0.0095,-0.0187,0.0188,-0.0256,0.012,0.0583,0.007,-0.0395,-0.0015,-0.0116,0.036,0.0861,-0.0214,0.0295,0.0479,0.0734,-0.0535,0.0598,-0.0764,0.0311,0.0545,0.1906,-0.0302,-0.0034,-0.0103,-0.0279,0.005,0.0168,-0.0245,0.0609,-0.0032,0.0616,-0.0423,0.0277,0.1235,-0.0727,0.0273,-0.0045,-0.008,0.0143,-0.0272,-0.0534,-0.0363,0.1157,-0.0246,-0.0006,0.0055,-0.0138,0.0463,-0.027,0.0149,-0.0158,-0.0047,0.0401,0.0319,-0.0583,-0.0243,-0.0267,-0.0334,0.0328,-0.0446,-0.0791,-0.0381,-0.0121]}
{"key":"[Neural Control Variates] We propose neural control variates (NCV) for unbiased variance reduction in parametric Monte Carlo integration. So far, the core challenge of applying the method of control variates has been finding a good approximation of the integrand that is cheap to integrate. We show that a set of neural networks can face that challenge: a normalizing flow that approximates the shape of the integrand and another neural network that infers the solution of the integral equation. We also propose to leverage a neural importance sampler to estimate the difference between the original integrand and the learned control variate. To optimize the resulting parametric estimator, we derive a theoretically optimal, variance-minimizing loss function, and propose an alternative, composite loss for stable online training in practice. When applied to light transport simulation, neural control variates are capable of matching the state-of-the-art performance of other unbiased approaches, while providing means to develop more performant, practical solutions. Specifically, we show that the learned light-field approximation is of sufficient quality for high-order bounces, allowing us to omit the error correction and thereby dramatically reduce the noise at the cost of negligible visible bias.","layer":0,"vector":[-0.0227,-0.0121,0.0479,0.0429,0.0631,0.0476,0.034,0.0249,0.0322,-0.0159,0.0351,-0.0308,0.0261,0.0711,0.0081,0.0035,-0.0242,0.0269,-0.057,0.0099,0.0206,-0.0282,-0.0184,-0.041,0.006,-0.0192,-0.0567,-0.0679,-0.0585,-0.2797,0.0026,-0.0562,0.0212,-0.0526,-0.0056,-0.0052,-0.064,0.0211,-0.0278,0.0231,0.0472,-0.0157,-0.0174,-0.0725,-0.0003,-0.0715,-0.0163,0.0084,0.0026,-0.0386,0.026,-0.0502,0.0177,-0.0005,0.0418,0.0246,0.0662,0.0487,0.0687,0.0627,0.007,0.0821,-0.1919,0.0634,0.0556,0.0069,-0.026,-0.0329,0.019,0.0378,-0.0536,0.0401,0.0125,0.0579,0.0232,-0.0292,0.0366,-0.0678,-0.0416,0.0142,0.0118,0.0006,-0.0492,0.003,0.0064,0.0101,0.0241,-0.0196,0.0451,0.0081,-0.0007,-0.0051,-0.0687,0.0014,-0.0229,-0.0029,0.0381,0.0181,-0.0228,0.1964,-0.0015,0.0204,0.0247,-0.0019,0.0439,-0.0435,-0.0571,-0.0253,-0.0432,0.0036,-0.0015,-0.0437,0.0331,-0.0558,-0.0121,0.0399,0.0168,0.0033,-0.0319,-0.0166,-0.0197,0.0164,0.052,0.0241,0.0065,-0.098,-0.0214,0.123,0.0352,0.0149,0.0579,0.0203,-0.0276,-0.0674,0.0385,0.0254,-0.0131,0.0013,-0.0112,0.0307,-0.0627,-0.0538,0.0361,-0.0967,-0.0465,0.1315,-0.0098,0.0203,-0.0123,-0.0542,-0.036,0.0121,-0.0331,-0.033,0.0102,0.0284,0.0177,0.0739,-0.0645,0.0399,-0.0276,-0.037,0.0049,0.083,-0.0145,-0.0724,-0.0057,0.0295,0.0285,-0.0295,0.011,0.0142,-0.0284,0.0193,0.109,-0.0248,-0.1006,0.0234,-0.0002,0.0171,0.0157,-0.0577,-0.0053,0.0375,0.0205,-0.009,0.0179,-0.0225,-0.0453,0.0675,-0.0454,0.0027,-0.0209,0.0154,-0.0334,0.0053,-0.0277,-0.0309,0.0225,-0.0334,-0.0105,-0.0532,-0.0441,-0.0173,0.0102,0.0214,-0.0142,0.013,0.0449,0.0637,-0.053,-0.0509,0.0754,-0.0224,-0.0299,0.04,-0.0007,0.0254,0.0194,0.0511,0.0286,-0.0511,-0.0561,-0.2446,0.015,0.0169,-0.0067,0.0982,-0.0721,0.0345,0.0201,0.0353,0.0783,0.0485,-0.0059,0.0122,-0.0077,-0.0272,0.0041,0.0105,-0.0066,-0.0515,-0.0102,-0.0039,0.0176,-0.0482,-0.0397,0.0498,-0.0051,0.1868,0.0346,0.0785,-0.0597,-0.0121,0.0056,0.055,-0.0343,0.0697,0.018,0.0835,-0.0073,-0.0283,-0.0571,-0.015,0.0448,-0.0121,-0.1258,-0.049,-0.0244,-0.015,0.0672,-0.0425,-0.0019,0.0406,-0.0241,0.0634,-0.0123,0.0145,-0.0317,-0.0639,0.0327,-0.0073,0.0352,0.0199,-0.0586,0.0376,-0.0788,0.0543,0.0025,0.0254,-0.0259,0.0417,-0.0173,-0.0144,0.0909,-0.0197,0.0674,0.0496,0.0077,-0.0004,-0.0091,-0.0525,-0.0115,0.072,0.0173,0.0137,0.0634,0.0562,0.0168,0.0488,-0.0685,0.0216,-0.0101,-0.0273,0.0336,-0.0747,-0.0121,0.0153,-0.0067,-0.2832,0.0336,0.0282,0.0132,-0.0562,0.0057,0.0729,-0.0186,-0.024,-0.0312,-0.045,0.0228,0.0375,0.0363,-0.0193,0.0183,0.0279,-0.0583,0.0361,-0.041,0.0304,0.0394,0.2054,-0.0438,-0.0084,0.0178,-0.0108,0.0228,0.0571,-0.0116,0.0235,0.0314,0.0668,-0.0359,0.0675,0.0745,-0.0316,0.0282,0.0258,-0.0409,0.0156,0.0145,0.0095,-0.0375,0.1049,-0.0137,-0.04,-0.0044,-0.0078,0.0066,-0.044,0.0578,-0.0052,-0.0482,0.0208,0.0266,-0.0809,-0.0578,-0.0319,-0.0348,0.0001,-0.0644,-0.0082,-0.0226,0.0101]}
{"key":"[Provably Efficient Offline Multi-agent Reinforcement Learning via Strategy-wise Bonus] This paper considers offline multi-agent reinforcement learning. We propose the strategy-wise concentration principle which directly builds a confidence interval for the joint strategy, in contrast to the point-wise concentration principle that builds a confidence interval for each point in the joint action space. For two-player zero-sum Markov games, by exploiting the convexity of the strategy-wise bonus, we propose a computationally efficient algorithm whose sample complexity enjoys a better dependency on the number of actions than the prior methods based on the point-wise bonus. Furthermore, for offline multi-agent general-sum Markov games, based on the strategy-wise bonus and a novel surrogate function, we give the first algorithm whose sample complexity only scales $\\sum_{i=1}^mA_i$ where $A_i$ is the action size of the $i$-th player and $m$ is the number of players. In sharp contrast, the sample complexity of methods based on the point-wise bonus would scale with the size of the joint action space $\\Pi_{i=1}^m A_i$ due to the curse of multiagents. Lastly, all of our algorithms can naturally take a pre-specified strategy class $\\Pi$ as input and output a strategy that is close to the best strategy in $\\Pi$. In this setting, the sample complexity only scales with $\\log |\\Pi|$ instead of $\\sum_{i=1}^mA_i$.","layer":2,"vector":[-0.0772,0.0059,0.0386,-0.0408,-0.0299,0.0661,0.0387,0.0465,0.0549,-0.0016,0.028,-0.0325,0.0204,0.0979,0.0291,-0.0037,-0.0229,0.0397,-0.0226,-0.0194,0.0381,-0.075,-0.0001,-0.0377,0.0417,0.0012,-0.044,-0.0948,-0.0226,-0.2198,0.0289,-0.0088,0.0044,-0.0238,0.0137,-0.0016,0.0004,0.0164,-0.0368,0.0002,0.0452,0.0392,0.0181,-0.0704,-0.0499,-0.0373,0.016,0.0079,-0.0026,-0.0406,0.0242,-0.0356,-0.0074,0.0309,0.053,0.0188,0.0141,0.0806,0.0174,0.0401,0.0064,0.0327,-0.1706,0.0346,0.0188,0.0427,-0.0609,-0.0123,0.0464,0.0637,-0.0076,0.0525,0.0076,0.0214,0.0313,0.0043,-0.0018,-0.0194,0.0084,-0.0002,-0.0172,-0.0736,-0.0563,-0.0149,-0.0336,-0.0784,0.0127,-0.0176,0.0525,-0.0143,-0.0022,0.037,0.0029,0.0,-0.0424,-0.016,0.012,0.0164,-0.0407,0.2013,-0.0003,0.0201,-0.0045,-0.0137,0.0479,-0.0456,-0.0141,-0.0442,-0.014,-0.0037,-0.0247,0.0051,0.0595,-0.0126,0.0082,0.0519,0.0694,0.0027,-0.0001,-0.0231,-0.0203,0.008,0.06,-0.0019,0.0212,-0.056,-0.0152,0.124,0.0337,0.0308,0.017,-0.0457,-0.0094,-0.0584,0.0531,-0.0008,0.0135,0.0281,0.0255,0.0024,-0.0444,-0.0026,0.0074,-0.1447,-0.0269,0.1235,0.0263,0.0533,-0.0491,-0.0224,-0.0363,0.022,-0.0003,-0.0147,-0.0171,0.0014,0.0493,0.0308,-0.0738,0.0211,-0.0681,-0.0489,0.0123,0.1104,-0.0287,-0.077,-0.0371,-0.0297,-0.0007,-0.004,0.0189,-0.0298,-0.0512,0.0238,0.0701,-0.0303,-0.0793,-0.0301,0.0246,-0.0233,0.0142,-0.0325,-0.0039,0.0392,0.0171,-0.0427,-0.0064,-0.0321,0.0365,0.0071,-0.0445,0.0078,-0.0039,0.0087,-0.048,-0.0263,-0.0051,-0.0027,0.032,-0.0133,-0.0019,-0.0695,-0.061,-0.0015,-0.0081,0.015,0.0183,0.0448,0.05,0.0224,-0.0508,0.0173,0.0304,0.0044,-0.0661,0.022,0.074,0.0128,-0.0422,0.0187,0.0168,-0.0347,-0.0438,-0.198,-0.0113,-0.0263,-0.0259,0.0184,-0.0393,0.0488,-0.0548,0.0563,0.0573,0.1167,-0.042,-0.0424,0.0375,-0.0277,0.0807,0.0114,0.0172,-0.0106,0.0406,0.0271,0.0107,-0.0255,-0.08,0.0764,-0.021,0.2203,0.0208,0.0222,0.009,0.0178,0.0699,-0.0509,-0.0825,0.056,0.0133,0.0853,-0.0551,-0.0087,-0.0609,-0.0191,0.0126,-0.0137,-0.122,-0.03,-0.0208,-0.0586,0.0544,-0.0467,-0.0432,0.0656,-0.0092,0.0482,-0.0448,-0.0237,-0.0235,-0.0678,0.0803,-0.0424,0.0216,0.0066,-0.0207,0.005,-0.0308,0.037,-0.0096,0.0189,-0.0588,0.0711,0.0091,-0.0154,0.0508,-0.005,0.0328,-0.008,0.0052,0.043,-0.0091,-0.0533,-0.0188,0.0762,-0.0655,0.0105,0.0449,0.0271,-0.04,0.0522,-0.0126,0.0421,-0.0004,0.0246,0.0054,-0.0527,-0.0004,0.0232,-0.0187,-0.3001,0.0478,-0.0039,0.0247,-0.0393,0.004,0.0507,0.0054,-0.0712,-0.0141,0.0397,0.0854,-0.0216,0.0298,0.0135,0.0128,0.0839,-0.0382,0.0783,-0.0513,0.0606,0.036,0.2268,-0.067,0.0715,0.0238,0.003,0.0073,0.0108,-0.0521,0.0076,-0.0003,0.0722,-0.0951,0.0437,0.0545,-0.0203,0.0302,-0.0002,0.0014,-0.0512,0.0233,0.0379,-0.0048,0.1229,-0.0172,-0.0324,-0.0448,-0.0145,0.0434,-0.044,-0.0207,0.009,-0.0322,0.0643,0.0156,-0.0072,-0.0858,-0.0036,-0.0076,0.0006,-0.0669,0.0228,0.0015,0.0001]}
{"key":"[Information-theoretic Feature Selection via Tensor Decomposition and Submodularity] Feature selection by maximizing high-order mutual information between the selected feature vector and a target variable is the gold standard in terms of selecting the best subset of relevant features that maximizes the performance of prediction models. However, such an approach typically requires knowledge of the multivariate probability distribution of all features and the target, and involves a challenging combinatorial optimization problem. Recent work has shown that any joint Probability Mass Function (PMF) can be represented as a naive Bayes model, via Canonical Polyadic (tensor rank) Decomposition. In this paper, we introduce a low-rank tensor model of the joint PMF of all variables and indirect targeting as a way of mitigating complexity and maximizing the classification performance for a given number of features. Through low-rank modeling of the joint PMF, it is possible to circumvent the curse of dimensionality by learning principal components of the joint distribution. By indirectly aiming to predict the latent variable of the naive Bayes model instead of the original target variable, it is possible to formulate the feature selection problem as maximization of a monotone submodular function subject to a cardinality constraint - which can be tackled using a greedy algorithm that comes with performance guarantees. Numerical experiments with several standard datasets suggest that the proposed approach compares favorably to the state-of-art for this important problem.","layer":8,"vector":[-0.0316,-0.0006,0.0337,0.0211,0.0366,0.0188,0.0622,0.0143,0.0255,-0.0238,0.0015,-0.059,0.0015,0.039,0.0462,0.0332,0.0114,0.0572,-0.0445,0.0148,0.0613,-0.0522,-0.0492,-0.035,0.051,-0.0019,-0.0309,0.0073,-0.0586,-0.2214,0.0391,-0.0209,0.0566,-0.0479,0.0105,-0.0211,-0.0433,0.0364,-0.086,0.0201,-0.0027,0.0028,-0.0108,-0.0073,-0.0289,-0.0589,0.0208,-0.008,-0.0482,0.0117,0.022,-0.0368,-0.0231,0.0294,0.0341,0.0251,0.0279,0.0023,0.0294,0.0659,0.0236,0.0473,-0.1542,0.0223,0.0508,0.0186,-0.0231,-0.0113,0.0219,0.0443,-0.0248,0.0883,0.0122,0.0396,-0.0221,-0.0112,0.0299,-0.0012,-0.0265,0.0193,0.0174,-0.0043,-0.058,-0.0222,-0.0328,-0.0285,0.0502,-0.0596,0.0306,-0.0107,-0.0183,-0.0369,-0.04,0.0351,-0.0551,-0.0148,0.0734,0.0198,-0.0441,0.1741,-0.0701,0.0444,0.0294,-0.0355,0.0241,-0.0582,-0.0305,-0.0077,-0.0468,0.0083,0.0119,-0.0165,-0.005,-0.0435,0.0166,0.003,0.0548,0.0348,-0.0074,0.0164,-0.0414,0.0106,0.0993,-0.0312,-0.0247,-0.0475,-0.0061,0.1278,0.0074,0.0595,0.0529,-0.0057,-0.0775,-0.048,0.054,0.0254,0.0363,0.0181,-0.0159,0.018,-0.0761,-0.0473,0.0484,-0.0735,-0.0395,0.1644,-0.0652,-0.0002,-0.0022,-0.0256,0.0073,0.0267,-0.0095,-0.0234,0.0205,0.0372,-0.0256,0.0283,-0.0801,0.0259,0.0015,-0.0439,-0.004,0.0849,-0.0117,-0.1032,-0.0579,0.0159,0.0058,0.0005,0.0627,0.0408,-0.0282,0.0328,0.0904,0.0331,-0.0677,0.0185,0.0475,0.0044,0.0295,-0.0654,-0.0356,0.0387,0.0426,-0.0457,0.0052,-0.0098,0.0468,-0.0104,-0.0267,0.0279,0.0017,-0.0387,-0.0709,0.0014,-0.0163,0.0061,0.0164,-0.0551,-0.0034,0.018,-0.0596,0.0444,-0.013,0.0267,-0.0203,-0.0197,-0.0026,0.0268,-0.0193,-0.0063,0.0447,-0.0141,-0.0252,-0.0152,0.0265,0.0571,0.0129,0.0487,0.0482,-0.0562,-0.0452,-0.2264,0.0119,0.0119,-0.008,0.0107,-0.1029,0.0374,0.0175,0.0291,0.0777,0.0662,0.045,-0.0747,0.0253,-0.0255,0.0672,0.0412,-0.0245,-0.006,-0.0224,0.0076,0.0366,-0.0202,-0.0606,0.0616,0.0213,0.2049,0.053,0.0218,-0.0437,0.0296,0.0492,-0.0347,-0.0793,0.0631,0.0335,0.0647,-0.0127,-0.0985,0.0087,-0.0301,0.0624,0.0445,-0.0672,-0.0552,-0.0293,-0.0157,0.0361,-0.0648,0.0198,0.0843,0.0019,0.0599,-0.0197,-0.0232,-0.0686,-0.0529,-0.0127,-0.0209,-0.0101,0.0312,-0.066,0.0089,-0.0561,0.0292,0.0033,-0.0339,-0.0143,0.0175,0.0054,-0.0535,0.0549,0.0263,0.0034,0.0749,-0.0317,0.051,-0.031,-0.0374,-0.0083,0.0833,-0.0526,0.0083,0.0198,-0.0118,-0.0249,0.0778,-0.0131,-0.0182,-0.0502,-0.0176,-0.0179,-0.0238,0.0062,0.0361,-0.0068,-0.3095,0.023,0.0568,0.0073,-0.0189,-0.0334,0.0609,0.0039,-0.0127,0.0101,0.0328,0.0245,0.0295,-0.0363,-0.0454,0.0537,0.0819,-0.0497,0.0251,-0.0612,-0.0138,0.0369,0.216,-0.0164,0.0402,0.0089,-0.0259,0.0151,0.0074,-0.0061,0.0346,0.0299,0.1195,-0.1015,-0.0042,0.0973,-0.0295,-0.0076,0.0076,-0.0417,0.0204,0.011,-0.0396,-0.0661,0.1295,-0.0395,0.0016,-0.0377,-0.0065,0.0355,0.0121,0.0218,-0.054,-0.0039,0.0247,0.0146,-0.0366,-0.0415,-0.0011,-0.0143,-0.0046,-0.0646,-0.0376,0.008,-0.0478]}
{"key":"[Learning-based Model Predictive Control for Safe Exploration and Reinforcement Learning] Reinforcement learning has been successfully used to solve difficult tasks in complex unknown environments. However, these methods typically do not provide any safety guarantees during the learning process. This is particularly problematic, since reinforcement learning agent actively explore their environment. This prevents their use in safety-critical, real-world applications. In this paper, we present a learning-based model predictive control scheme that provides high-probability safety guarantees throughout the learning process. Based on a reliable statistical model, we construct provably accurate confidence intervals on predicted trajectories. Unlike previous approaches, we allow for input-dependent uncertainties. Based on these reliable predictions, we guarantee that trajectories satisfy safety constraints. Moreover, we use a terminal set constraint to recursively guarantee the existence of safe control actions at every iteration. We evaluate the resulting algorithm to safely explore the dynamics of an inverted pendulum and to solve a reinforcement learning task on a cart-pole system with safety constraints.","layer":0,"vector":[-0.0519,-0.0466,0.042,-0.0283,-0.04,0.0101,0.0374,0.0152,0.0098,0.0273,0.0073,-0.0462,0.0159,0.0766,-0.009,0.01,-0.0444,0.0808,0.008,0.05,0.0516,0.0,-0.0046,-0.0622,-0.0179,0.037,-0.0426,-0.0418,-0.0243,-0.2324,-0.0179,-0.0725,-0.0058,-0.0272,-0.0195,-0.0211,-0.0459,0.0651,-0.0086,0.0147,0.038,0.0316,-0.0053,-0.078,0.0009,-0.0222,0.0206,-0.0303,0.0074,-0.037,0.0191,-0.0444,0.0134,0.0068,0.0611,0.0304,0.0542,0.0513,0.0353,0.0688,0.0044,0.009,-0.1736,0.0687,0.019,0.0598,-0.0535,-0.0145,0.0539,0.0302,-0.0299,0.0519,0.0229,0.073,0.0332,-0.02,-0.0002,-0.0849,-0.0004,0.024,0.0592,-0.0652,-0.0354,-0.0044,-0.063,-0.1119,0.008,-0.0209,0.0367,0.0085,-0.0272,-0.0076,-0.0312,0.0121,-0.0561,0.003,0.0272,0.014,-0.0775,0.1834,-0.0121,0.0753,0.0233,-0.0009,0.0115,-0.0351,-0.0095,-0.0058,-0.0393,-0.0515,-0.0247,-0.034,0.0374,-0.0148,-0.0371,0.045,0.0861,0.0403,0.0218,-0.0119,-0.0126,0.0174,0.079,-0.0239,0.0183,-0.0781,0.0254,0.1641,0.0062,0.0205,0.0193,-0.0552,-0.0202,-0.0136,-0.0107,0.015,0.0221,-0.003,0.0443,-0.0204,-0.0522,-0.0205,0.0138,-0.1213,-0.0617,0.0791,0.0064,0.004,-0.0139,-0.02,-0.029,0.0258,0.0137,0.0134,0.021,0.0152,0.0247,0.0381,-0.0297,0.0209,-0.0653,-0.0601,-0.0207,0.0786,-0.0126,-0.0427,-0.0226,0.0349,0.033,-0.0223,0.0094,0.0233,-0.0538,0.0022,0.0703,0.0434,-0.0601,0.0025,0.011,0.0065,0.0218,-0.0644,-0.0117,0.0122,0.0799,-0.018,0.0037,-0.0315,0.0223,0.0327,-0.0213,0.0025,-0.0315,0.0194,-0.054,-0.0224,-0.0068,-0.0409,0.0371,-0.0419,0.0149,-0.0043,-0.0321,0.0435,-0.0198,0.0064,-0.0082,-0.0092,0.0127,0.0155,-0.0821,0.0204,0.0598,-0.0607,-0.0655,-0.0095,0.0269,0.0511,-0.0094,0.0621,0.073,0.0429,-0.0302,-0.2432,0.0063,-0.0162,0.0329,0.0492,-0.0658,0.0252,-0.0688,0.0025,0.0047,0.0495,-0.0434,-0.0743,0.033,-0.0278,0.0436,0.0135,-0.0139,-0.018,0.0063,-0.0404,-0.0202,-0.0528,-0.0947,0.0366,0.0127,0.2117,0.0364,0.0691,-0.0308,0.0554,-0.0053,-0.0463,-0.0445,0.0864,0.0145,0.0246,-0.0207,-0.0087,-0.0452,0.0374,0.0331,-0.0031,-0.1075,-0.0452,-0.0094,-0.0154,0.0743,-0.0359,0.0028,0.0438,-0.0046,0.0245,-0.0293,-0.0444,-0.0312,-0.0291,0.0564,-0.0237,0.034,0.006,-0.0436,0.0046,-0.0582,0.0052,-0.007,0.041,-0.0748,0.0799,-0.0154,-0.0248,0.101,0.0555,0.0184,0.0142,0.0107,-0.0176,-0.0175,-0.0417,-0.0026,0.0225,-0.0136,0.0612,0.0655,0.0113,-0.0291,0.0366,-0.0279,0.0421,-0.01,-0.0028,-0.0126,-0.0414,-0.0017,0.0594,0.0088,-0.2873,0.0411,-0.0034,0.005,-0.0391,-0.0198,0.0843,0.0319,-0.0447,0.0017,-0.018,0.0811,0.0764,0.0521,0.0105,0.0117,0.0809,-0.0362,0.0651,-0.0872,0.0427,0.0419,0.2127,-0.021,0.0572,0.0092,-0.0469,-0.0127,0.0429,-0.0116,0.0127,0.0069,0.0381,-0.0585,0.0704,0.1044,-0.0222,0.0367,0.0055,0.0096,-0.045,0.0273,0.0245,-0.031,0.0556,0.0016,-0.0525,-0.0521,0.0005,0.0367,-0.0043,-0.0149,-0.0239,-0.0176,0.0155,0.0299,-0.0141,-0.052,-0.0255,-0.0898,0.0294,-0.0511,0.0609,-0.031,-0.0292]}
{"key":"[Direct Evolutionary Optimization of Variational Autoencoders With Binary Latents] Discrete latent variables are considered important for real world data, which has motivated research on Variational Autoencoders (VAEs) with discrete latents. However, standard VAE-training is not possible in this case, which has motivated different strategies to manipulate discrete distributions in order to train discrete VAEs similarly to conventional ones. Here we ask if it is also possible to keep the discrete nature of the latents fully intact by applying a direct discrete optimization for the encoding model. The approach is consequently strongly diverting from standard VAE-training by sidestepping sampling approximation, reparameterization trick and amortization. Discrete optimization is realized in a variational setting using truncated posteriors in conjunction with evolutionary algorithms. For VAEs with binary latents, we (A) show how such a discrete variational method ties into gradient ascent for network weights, and (B) how the decoder is used to select latent states for training. Conventional amortized training is more efficient and applicable to large neural networks. However, using smaller networks, we here find direct discrete optimization to be efficiently scalable to hundreds of latents. More importantly, we find the effectiveness of direct optimization to be highly competitive in `zero-shot' learning. In contrast to large supervised networks, the here investigated VAEs can, e.g., denoise a single image without previous training on clean data and/or training on large image datasets. More generally, the studied approach shows that training of VAEs is indeed possible without sampling-based approximation and reparameterization, which may be interesting for the analysis of VAE-training in general. For `zero-shot' settings a direct optimization, furthermore, makes VAEs competitive where they have previously been outperformed by non-generative approaches.","layer":4,"vector":[-0.0182,0.0296,0.0351,-0.0288,0.0538,0.0303,-0.01,0.0254,-0.0054,-0.0222,0.037,-0.0712,0.0475,0.0579,0.0241,0.0088,0.0163,0.0282,-0.0368,0.0241,0.0027,-0.016,0.0302,-0.0725,0.033,-0.0635,0.0311,-0.0455,-0.0284,-0.262,0.0421,-0.0267,0.0667,-0.0422,-0.0017,-0.0488,-0.0798,0.0338,-0.0441,0.0563,0.0263,0.0102,0.0102,-0.048,-0.0254,-0.0471,0.0122,-0.0381,-0.036,-0.0214,0.0612,-0.0677,0.0201,0.0431,0.0334,0.01,0.0579,0.0421,0.0363,0.0487,0.0242,0.0589,-0.1496,0.0512,0.0498,0.0554,-0.0347,-0.0077,0.0245,0.0629,0.0036,0.0433,0.012,0.0201,0.0132,0.0078,0.0203,-0.0178,-0.0395,-0.0094,0.0365,-0.0397,-0.007,-0.003,-0.0061,-0.0322,0.018,-0.0466,0.0442,0.014,-0.0111,-0.0225,-0.0115,-0.0058,-0.0637,-0.0054,0.024,0.0565,-0.0547,0.188,-0.0124,0.0347,0.0467,-0.0514,0.022,-0.0621,-0.0426,-0.0248,-0.0141,0.0099,0.0085,0.0053,0.0177,-0.0762,0.0306,-0.0017,0.0322,0.025,0.0228,0.0174,-0.0468,0.003,0.006,0.0046,0.0164,-0.0649,-0.0261,0.1657,0.0076,0.0555,0.0238,0.0097,-0.0501,-0.0566,0.0503,0.0141,0.0252,-0.0381,-0.0005,-0.0211,-0.0839,-0.0391,0.0295,-0.0708,-0.0406,0.0659,-0.0429,0.0394,-0.0492,-0.0505,0.0028,0.0344,-0.0508,-0.0093,0.0563,0.0858,0.017,-0.0081,-0.0701,0.0091,-0.0044,-0.0205,-0.0244,0.1041,0.0117,-0.0747,-0.075,-0.0361,0.0171,-0.0197,0.0261,-0.0143,-0.0538,0.0347,0.0704,0.0401,-0.102,0.0084,0.0311,0.0539,0.0101,-0.0405,-0.0224,0.0547,0.0364,-0.0308,0.012,-0.0136,0.0276,0.0441,0.0166,0.0176,-0.0258,-0.0193,0.0104,-0.0198,-0.0077,0.0009,-0.0006,-0.0156,0.0209,0.0269,-0.0282,0.0468,0.0264,-0.0257,-0.0044,0.003,0.0666,0.0478,-0.0401,-0.0241,0.0839,-0.0128,-0.0344,0.0024,-0.0103,0.0199,-0.0065,0.062,0.016,-0.049,-0.0285,-0.2405,-0.0041,-0.0383,-0.0767,0.0684,-0.0678,0.0582,0.0072,0.0556,0.0812,0.0026,-0.0545,-0.0366,0.0505,-0.0024,0.0635,0.0274,0.0382,0.0266,0.0083,-0.0041,-0.004,0.0022,-0.0699,0.0632,0.0201,0.2412,-0.0155,0.0659,-0.0119,0.0216,0.0613,-0.0329,-0.072,0.0644,-0.0148,0.0759,-0.0315,-0.0595,0.0198,-0.0083,0.0118,-0.0097,-0.1273,-0.0379,-0.0593,-0.0654,0.0271,-0.0807,-0.0094,0.071,-0.0259,0.0443,-0.0388,-0.0196,-0.0725,-0.1015,0.0084,-0.051,0.0327,0.0262,-0.0601,-0.0418,-0.0498,0.0186,0.0353,-0.0238,-0.0636,0.0138,-0.0044,-0.0358,0.0695,-0.0253,-0.0006,0.0701,0.0224,-0.0103,-0.0166,-0.0427,-0.0408,0.0579,-0.0356,0.0231,0.0415,0.0396,0.0219,0.0823,0.0019,0.0186,0.0072,-0.0336,0.0358,-0.0955,0.0045,0.036,-0.0115,-0.2735,0.0103,0.0491,0.0183,-0.025,0.0131,0.0517,0.0233,-0.0204,0.0032,-0.0436,0.0126,0.018,0.0025,0.0439,0.0047,0.084,-0.0313,0.0606,-0.062,-0.0015,-0.0001,0.2133,-0.0213,0.0274,0.0088,-0.063,0.0098,0.0114,-0.048,-0.0161,0.0198,0.0828,-0.0521,0.0195,0.0744,-0.0374,0.0428,0.0011,-0.0207,-0.0116,-0.0109,-0.0111,-0.0243,0.0655,0.0246,0.0125,-0.0032,-0.0444,0.008,-0.0108,0.0441,-0.0007,-0.0122,0.0222,0.0386,-0.0438,-0.0545,-0.0415,-0.0314,0.0429,-0.0411,-0.0199,-0.0262,-0.0188]}
{"key":"[Joint Manifold Learning and Density Estimation Using Normalizing Flows] Based on the manifold hypothesis, real-world data often lie on a low-dimensional manifold, while normalizing flows as a likelihood-based generative model are incapable of finding this manifold due to their structural constraints. So, one interesting question arises: $\\textit{\"Can we find sub-manifold(s) of data in normalizing flows and estimate the density of the data on the sub-manifold(s)?\"}$. In this paper, we introduce two approaches, namely per-pixel penalized log-likelihood and hierarchical training, to answer the mentioned question. We propose a single-step method for joint manifold learning and density estimation by disentangling the transformed space obtained by normalizing flows to manifold and off-manifold parts. This is done by a per-pixel penalized likelihood function for learning a sub-manifold of the data. Normalizing flows assume the transformed data is Gaussianizationed, but this imposed assumption is not necessarily true, especially in high dimensions. To tackle this problem, a hierarchical training approach is employed to improve the density estimation on the sub-manifold. The results validate the superiority of the proposed methods in simultaneous manifold learning and density estimation using normalizing flows in terms of generated image quality and likelihood.","layer":2,"vector":[-0.0123,-0.0367,0.0338,-0.0156,0.0407,0.0306,0.0039,0.0269,0.0378,-0.0049,-0.009,-0.0708,-0.0181,0.0494,0.0253,0.0149,0.0167,0.0509,-0.0599,0.0201,0.0259,-0.0184,-0.014,-0.0377,0.0179,0.0157,-0.0273,-0.0818,-0.0328,-0.2523,0.0458,-0.0474,0.0444,-0.0109,0.0193,-0.0788,-0.0326,0.0597,-0.0275,0.0501,-0.0038,0.0496,-0.0532,-0.0309,-0.0412,-0.0371,-0.0306,-0.0029,-0.0088,-0.0146,0.025,-0.0478,0.0283,0.0458,0.0501,0.0237,0.0612,0.01,0.0691,0.0796,0.0005,0.0138,-0.1521,0.0676,0.0602,-0.0294,-0.0578,-0.021,0.0243,0.0106,-0.0054,0.0194,-0.0053,0.0153,0.0499,-0.0459,0.0122,-0.0385,-0.0314,-0.0019,0.0305,0.0215,-0.0325,-0.0161,-0.0216,-0.0143,0.0559,-0.045,0.0297,-0.0597,-0.0653,-0.0157,-0.1001,0.0586,-0.0731,-0.0212,0.0134,0.0128,0.0206,0.1912,-0.0664,0.0478,0.0516,-0.0158,0.0141,-0.0375,-0.0363,0.0254,0.0142,-0.0082,0.0164,-0.0396,0.0152,-0.0215,0.0174,-0.0365,0.0617,0.0588,-0.0338,-0.0276,-0.0417,-0.0074,0.0402,-0.0323,0.0535,-0.0804,0.0253,0.1195,0.075,0.0486,0.052,-0.0224,-0.0491,-0.0284,-0.0005,-0.002,0.0446,0.0303,0.0069,0.0293,-0.0568,-0.0751,-0.0044,-0.071,-0.0321,0.1364,-0.0666,0.0537,-0.0347,-0.0241,-0.0071,0.0196,-0.0157,-0.0096,0.0204,0.0097,0.0206,0.0309,-0.0378,0.0348,-0.0557,-0.0762,-0.0562,0.1011,-0.0042,-0.0602,-0.0084,-0.0141,0.0332,0.0428,0.0057,0.0511,-0.0291,0.0395,0.0932,0.0506,-0.07,0.0172,-0.0031,0.0184,0.0194,-0.0472,-0.0244,0.0089,0.0193,-0.0469,0.0135,-0.0374,0.0453,0.046,-0.0039,-0.0026,0.0145,-0.0237,0.0193,-0.0122,0.0,-0.0264,0.0175,-0.0476,0.0399,0.011,-0.0354,-0.0315,-0.0695,0.0308,0.0156,0.0123,0.0199,0.0248,0.0291,-0.0684,0.0314,-0.011,-0.0162,-0.0049,0.0056,0.0301,-0.0286,0.0478,0.0312,-0.0618,-0.0632,-0.2373,0.0092,-0.0122,0.0079,0.048,-0.071,0.0408,-0.0163,0.0277,0.0623,0.0572,0.0272,-0.0229,0.0419,0.0139,0.0463,0.0081,0.0542,-0.031,-0.029,-0.0209,0.043,-0.0342,-0.111,0.094,-0.0153,0.2229,0.0058,0.0306,-0.0392,0.0348,0.0283,-0.0132,-0.0858,0.0452,0.0216,0.0422,-0.0063,-0.0473,-0.0069,-0.0546,-0.03,0.0325,-0.0831,0.0046,-0.0251,-0.0544,0.0228,-0.0512,0.0035,0.0238,-0.0454,0.0582,-0.0596,-0.0382,-0.041,-0.0931,0.0026,-0.0581,0.0575,0.0061,-0.0639,0.0414,-0.0824,0.0605,-0.0347,-0.0279,-0.0667,0.0367,0.0008,-0.0454,0.0519,0.0271,0.0015,0.104,0.0427,0.0616,0.0048,-0.027,-0.0608,0.0606,-0.0307,0.0233,0.0418,0.0493,0.0165,0.0677,-0.0437,-0.0205,-0.0366,0.0199,-0.0028,-0.0346,-0.0003,0.0424,0.0305,-0.2797,0.0158,-0.001,0.0218,0.003,0.0008,0.0403,0.0459,-0.0259,-0.0464,0.0018,0.0279,0.0502,0.0248,0.0047,0.0574,0.0372,-0.03,0.0552,-0.0376,-0.0086,0.028,0.2095,-0.0081,-0.0351,-0.009,-0.0586,0.0281,0.0286,-0.0336,0.0101,0.0482,0.1005,-0.0425,0.0459,0.0665,-0.0112,0.0309,0.0282,-0.0289,0.0223,0.0226,-0.0138,-0.0305,0.0967,-0.0116,0.0071,-0.0001,-0.018,0.0299,-0.0338,-0.0116,0.0228,0.0305,0.025,0.0392,-0.0459,-0.0599,0.0006,-0.0098,-0.0137,-0.0921,-0.0448,0.0035,-0.0377]}
{"key":"[Faults in Deep Reinforcement Learning Programs: A Taxonomy and A Detection Approach] A growing demand is witnessed in both industry and academia for employing Deep Learning (DL) in various domains to solve real-world problems. Deep Reinforcement Learning (DRL) is the application of DL in the domain of Reinforcement Learning (RL). Like any software systems, DRL applications can fail because of faults in their programs. In this paper, we present the first attempt to categorize faults occurring in DRL programs. We manually analyzed 761 artifacts of DRL programs (from Stack Overflow posts and GitHub issues) developed using well-known DRL frameworks (OpenAI Gym, Dopamine, Keras-rl, Tensorforce) and identified faults reported by developers/users. We labeled and taxonomized the identified faults through several rounds of discussions. The resulting taxonomy is validated using an online survey with 19 developers/researchers. To allow for the automatic detection of faults in DRL programs, we have defined a meta-model of DRL programs and developed DRLinter, a model-based fault detection approach that leverages static analysis and graph transformations. The execution flow of DRLinter consists in parsing a DRL program to generate a model conforming to our meta-model and applying detection rules on the model to identify faults occurrences. The effectiveness of DRLinter is evaluated using 15 synthetic DRLprograms in which we injected faults observed in the analyzed artifacts of the taxonomy. The results show that DRLinter can successfully detect faults in all synthetic faulty programs.","layer":2,"vector":[-0.0669,-0.0353,0.0197,-0.0167,-0.005,0.0397,0.0319,0.0202,0.0516,-0.0346,0.0143,-0.0282,0.0368,0.0638,0.0223,-0.0106,-0.0188,0.0355,-0.0217,-0.026,0.0346,-0.0639,-0.0405,-0.0344,-0.0189,0.051,-0.0381,-0.0572,-0.0599,-0.2467,-0.0082,-0.0493,0.0242,-0.0276,0.0159,0.0117,-0.0519,0.0367,-0.0279,0.0341,0.0136,0.0019,-0.0096,-0.0582,-0.0021,-0.0418,-0.0296,-0.0223,0.0161,-0.029,0.0175,-0.0455,0.0229,0.0141,0.051,0.0253,0.1101,0.0676,0.0406,0.0327,0.0181,0.0316,-0.1888,0.027,0.0298,0.0612,-0.0213,-0.0295,0.0339,0.0409,-0.0188,0.0473,-0.0056,0.0078,0.0097,0.0067,0.0262,-0.0367,0.0379,0.0013,0.0442,-0.0534,-0.015,-0.012,-0.0122,-0.0603,0.0318,-0.0108,0.0572,0.0034,-0.0125,0.0477,0.0346,0.0289,-0.0625,0.0259,0.0386,-0.0021,-0.1105,0.2132,-0.0371,0.0537,0.0146,0.0185,0.0409,-0.0476,-0.036,-0.0334,-0.0315,-0.0392,-0.0095,0.0019,0.0594,-0.0155,0.0057,0.0005,0.067,0.0378,-0.0084,0.0196,-0.0144,-0.0054,0.0554,-0.0343,0.0049,-0.0679,-0.0119,0.1482,0.008,-0.0029,0.0173,-0.0379,-0.0308,0.0009,0.0202,0.0158,0.033,-0.0114,0.0187,0.0176,-0.0727,0.0279,-0.0002,-0.0688,-0.0724,0.0856,-0.0263,-0.021,-0.0441,-0.0219,-0.0144,0.0224,-0.0443,-0.0064,0.0504,0.0336,0.0257,0.067,-0.0319,0.0,0.0088,-0.0562,-0.0502,0.1214,0.0,-0.0556,-0.0553,-0.0088,-0.0336,-0.0269,0.0293,0.0463,-0.0208,-0.0064,0.0153,0.0122,-0.0581,0.0028,-0.0001,0.0152,0.0527,-0.0188,-0.0461,0.0106,0.0725,-0.0336,0.0088,-0.0555,0.0276,0.0136,-0.052,0.0306,-0.0384,-0.0085,0.0008,-0.024,0.029,-0.0091,-0.0042,-0.0308,-0.02,0.029,-0.0211,0.0047,-0.065,0.0385,-0.0697,0.0006,0.0466,0.0048,-0.0364,0.0265,0.029,-0.0143,-0.0373,-0.0061,0.0025,0.0325,-0.0281,0.0473,0.0581,-0.0093,-0.009,-0.2306,-0.0149,-0.0063,-0.0384,0.0332,-0.0292,0.0196,-0.0228,0.0368,0.0428,0.0476,-0.0343,-0.0237,-0.0277,0.0184,0.0436,0.0398,0.0358,-0.0574,-0.0204,-0.0137,0.0114,0.0334,-0.1063,0.0338,0.0209,0.2295,0.0508,0.0494,-0.0282,0.0541,0.0483,-0.011,-0.1329,0.1097,0.0012,0.0717,-0.0109,0.0058,-0.0488,-0.0215,0.0226,-0.0273,-0.0781,0.0151,0.0087,-0.0456,0.007,-0.0635,0.0212,0.0391,-0.0606,0.0342,0.0426,-0.0275,-0.0351,-0.1128,0.0206,-0.0192,-0.0031,0.0249,-0.0489,0.072,-0.095,0.0706,-0.0074,0.0217,-0.0894,0.0358,-0.0047,0.0132,0.1007,0.0011,0.0013,0.0503,0.0078,-0.005,-0.0295,-0.0335,-0.0326,0.0478,-0.0254,0.0042,0.0424,0.0156,-0.0242,0.0372,0.0108,0.0781,-0.0357,0.0356,0.0255,-0.0457,-0.0227,0.0672,0.0551,-0.2722,0.0672,0.0324,0.012,-0.0331,0.0074,0.0596,0.0118,-0.0257,-0.0375,0.0381,0.0155,0.0398,-0.046,0.0005,0.0469,0.0734,-0.0258,0.0652,-0.0677,-0.0051,0.075,0.2188,-0.0399,-0.0027,0.0042,-0.0109,0.0239,0.0298,-0.0089,-0.0329,0.0057,0.0651,-0.0274,0.0325,0.112,-0.0487,-0.0073,0.0457,0.0279,-0.002,0.0119,-0.0019,-0.0053,0.053,-0.0119,-0.0297,-0.0317,-0.0193,0.0527,-0.0289,-0.0506,-0.0422,-0.0296,0.0093,0.024,-0.0381,-0.0577,-0.0943,-0.05,0.017,-0.0626,0.0529,-0.0137,-0.0046]}
{"key":"[Kernel absolute summability is only sufficient for RKHS stability] Regularized approaches have been successfully applied to linear system identification in recent years. Many of them model unknown impulse responses exploiting the so called Reproducing Kernel Hilbert spaces (RKHSs) that enjoy the notable property of being in one-to-one correspondence with the class of positive semidefinite kernels. The necessary and sufficient condition for a RKHS to be stable, i.e. to contain only BIBO stable linear dynamic systems, has been known in the literature at least since 2006. However, an open question still persists and concerns the equivalence of such condition with the absolute summability of the kernel. This paper provides a definite answer to this matter by proving that such correspondence does not hold. A counterexample is introduced that illustrates the existence of stable RKHSs that are induced by non-absolutely summable kernels.","layer":0,"vector":[-0.0771,-0.0601,0.0471,-0.0155,-0.0319,0.0093,0.0426,0.0085,0.0201,-0.0132,0.0128,-0.0384,0.0033,0.0186,-0.0069,0.0522,0.0807,0.0459,-0.0144,0.028,0.0299,0.0132,-0.0166,-0.0151,0.0442,0.0138,0.0128,-0.0269,-0.0358,-0.2539,0.0151,-0.0525,0.0709,-0.0517,0.0449,-0.0132,-0.0037,0.0444,-0.0056,0.0485,-0.0027,-0.0007,0.0282,-0.0419,-0.0439,-0.055,-0.0271,-0.0407,-0.0177,-0.0388,0.0272,-0.0054,0.0522,0.0033,0.0142,0.0321,0.0569,0.0355,0.0774,0.0107,0.0187,0.0317,-0.1588,0.0008,0.0682,0.0305,0.0027,-0.0545,0.0696,0.0394,-0.0262,0.0403,0.001,-0.0241,-0.0091,-0.059,0.0079,-0.0437,-0.0272,0.0366,0.0168,-0.0487,-0.038,-0.0386,-0.0661,-0.0428,0.0326,-0.0699,0.0414,0.0445,-0.07,-0.0227,0.0308,0.0331,-0.0681,-0.0271,-0.0102,0.0364,-0.0368,0.1809,-0.0704,0.0301,0.0784,-0.0297,0.0087,-0.0321,-0.0466,-0.0324,-0.0157,-0.0154,0.0027,-0.0475,0.0426,0.0031,0.0651,-0.0261,0.0361,0.0319,0.0275,-0.0242,0.0072,0.0659,0.0515,-0.0292,0.051,-0.0573,0.0336,0.146,0.0364,0.1112,0.0223,-0.0584,-0.0244,-0.0421,-0.0478,0.0459,0.0508,0.0576,0.0424,0.0234,-0.024,-0.1007,0.013,-0.0531,-0.0594,0.1046,-0.0428,0.0012,-0.0389,0.0057,0.0233,-0.0003,-0.0588,-0.0241,-0.0161,0.0609,0.0062,0.0221,-0.0456,0.0168,-0.0402,-0.0529,-0.0271,0.116,-0.0374,-0.0192,-0.0114,0.0062,0.0523,-0.0188,0.0493,0.0107,-0.0182,0.0263,0.0356,0.0042,-0.0654,0.0151,0.0221,0.0044,0.0425,-0.0709,-0.0189,0.0441,0.021,-0.0234,-0.0259,-0.0628,0.0054,0.0414,-0.0472,-0.0259,-0.0078,0.0061,-0.0346,-0.0559,-0.0078,-0.0599,0.0519,-0.0511,0.0143,0.039,-0.0226,-0.0056,0.022,0.0328,-0.0164,0.0199,0.0016,0.0131,-0.0217,-0.0008,0.0764,-0.0462,-0.0714,0.0225,-0.0136,0.0316,0.0043,0.0396,0.0373,0.0168,-0.0584,-0.2446,-0.0,-0.0142,-0.0099,0.071,-0.0763,0.08,-0.0645,0.0786,0.0464,0.0166,0.0297,-0.0314,0.0371,-0.0215,0.0528,-0.01,-0.0147,0.0072,-0.0233,-0.0412,0.0098,-0.0221,-0.049,0.0694,-0.048,0.175,0.0078,0.0605,-0.0238,0.0216,0.0289,-0.0121,-0.0425,0.0687,0.0446,0.0338,-0.0372,0.004,-0.0076,0.0161,-0.0351,0.0148,-0.0552,-0.0126,-0.0345,-0.0441,-0.0017,-0.0347,0.0076,0.0333,-0.0193,0.0475,-0.0349,0.0436,-0.0104,-0.0908,0.0039,-0.0125,0.0641,-0.0084,-0.0675,0.0033,-0.0042,0.0837,0.0395,-0.0097,-0.0377,0.0353,-0.0405,-0.016,0.0847,0.0545,-0.0046,0.0712,-0.0059,0.0114,-0.0179,-0.0592,-0.0181,0.0397,-0.0548,0.0817,0.0155,0.0572,0.0107,0.0546,-0.0279,-0.0071,-0.0389,-0.0681,0.0489,-0.0665,-0.011,0.0552,0.0041,-0.2943,0.0144,0.0035,-0.0049,-0.0549,-0.0126,0.0015,-0.0256,-0.097,0.0202,-0.0319,0.0851,0.0493,0.0039,0.0267,0.0457,0.0614,-0.0926,0.0515,-0.0882,0.0221,0.0823,0.2031,-0.0278,0.0255,-0.0184,-0.0082,-0.0178,-0.025,-0.0188,0.0149,0.017,0.0708,-0.0576,0.0418,0.038,-0.0249,0.0543,-0.0137,0.0188,0.0073,0.0488,-0.0401,-0.0058,0.0871,-0.0121,-0.0158,-0.0291,-0.0068,0.0125,-0.0078,0.0019,0.0253,0.0157,0.0804,0.0116,-0.0826,-0.046,-0.0525,-0.0273,0.0027,-0.0469,-0.0209,-0.0084,-0.014]}
{"key":"[Unsupervised Mismatch Localization in Cross-Modal Sequential Data] Content mismatch usually occurs when data from one modality is translated to another, e.g. language learners producing mispronunciations (errors in speech) when reading a sentence (target text) aloud. However, most existing alignment algorithms assume the content involved in the two modalities is perfectly matched and thus leading to difficulty in locating such mismatch between speech and text. In this work, we develop an unsupervised learning algorithm that can infer the relationship between content-mismatched cross-modal sequential data, especially for speech-text sequences. More specifically, we propose a hierarchical Bayesian deep learning model, named mismatch localization variational autoencoder (ML-VAE), that decomposes the generative process of the speech into hierarchically structured latent variables, indicating the relationship between the two modalities. Training such a model is very challenging due to the discrete latent variables with complex dependencies involved. We propose a novel and effective training procedure which estimates the hard assignments of the discrete latent variables over a specifically designed lattice and updates the parameters of neural networks alternatively. Our experimental results show that ML-VAE successfully locates the mismatch between text and speech, without the need for human annotations for model training.","layer":0,"vector":[-0.043,-0.0196,0.0389,-0.0368,0.0024,0.0299,0.035,-0.0209,0.0308,-0.024,0.0098,-0.0636,0.0573,0.052,0.066,-0.018,-0.0138,0.0635,-0.0519,-0.0061,0.0306,-0.0291,0.0039,-0.0338,0.0379,0.0118,-0.0511,-0.0476,-0.0214,-0.2824,0.0284,-0.0237,0.0579,-0.0178,-0.0063,-0.0371,-0.0779,0.0497,-0.0113,0.0267,0.0585,0.016,0.0356,-0.0426,-0.0188,-0.0785,-0.0057,-0.0142,-0.011,-0.0631,0.0271,-0.0433,0.0536,0.0281,0.0225,0.0516,0.0865,0.0608,0.0397,0.0794,-0.0028,0.0327,-0.1831,0.0932,0.0445,0.0211,-0.0127,-0.0016,-0.0204,0.0232,-0.0247,0.0129,0.0197,0.0769,0.0305,0.0131,0.0297,-0.0085,-0.0169,-0.0164,0.0361,-0.0143,-0.0055,-0.0106,-0.0336,-0.0696,-0.0012,-0.0524,0.0093,-0.0295,-0.0724,-0.0375,-0.0123,0.0413,-0.0506,-0.0257,0.0258,0.0437,-0.0173,0.1942,0.0129,0.0325,0.0135,-0.0337,0.0264,-0.0294,-0.0708,-0.0083,-0.0073,0.0229,-0.0175,-0.023,0.0305,-0.0715,0.0349,0.0175,0.0875,0.0156,-0.0108,-0.0211,-0.0017,0.0066,0.0063,0.0041,0.0316,-0.0644,0.0388,0.0837,0.035,0.004,0.0935,-0.0105,-0.0235,-0.0348,0.0395,0.0297,0.0152,-0.0328,0.0299,-0.0138,0.0064,-0.1182,0.0036,-0.0372,-0.054,0.1114,-0.072,-0.0008,-0.0585,-0.007,0.001,0.0048,0.0016,-0.0312,0.0407,0.0215,0.0418,0.0066,-0.0363,-0.0152,0.0403,-0.034,-0.029,0.0878,0.0498,-0.0773,-0.0813,-0.0162,0.0264,-0.0129,0.0427,0.0422,-0.0045,0.0176,0.0349,0.0337,-0.0654,0.0339,0.0182,0.0472,-0.0056,-0.0433,-0.05,0.0753,0.0631,-0.0474,0.0036,-0.0371,0.0312,0.0393,0.0131,0.0168,0.0165,-0.0238,-0.0163,-0.046,-0.0305,-0.0515,0.0194,-0.0293,0.018,0.0256,-0.0727,0.0023,-0.0465,0.0175,0.0044,0.0124,0.0736,0.0486,-0.0003,-0.0189,0.0691,-0.0185,-0.0271,-0.0488,0.0159,0.0417,0.0088,0.0354,-0.0191,-0.0235,-0.0112,-0.2252,-0.0098,0.0181,-0.0221,0.0343,-0.0196,0.0118,0.0211,0.0702,0.0558,0.0057,-0.0345,-0.0322,0.0359,0.0127,0.0553,0.0387,0.0495,0.0213,0.0238,-0.0048,-0.0147,-0.0869,-0.0813,0.0718,-0.0202,0.1977,0.0296,0.0572,-0.0279,0.0391,0.0362,-0.0142,-0.0675,0.0542,0.0049,0.0717,-0.0111,-0.0586,-0.0205,-0.0501,-0.007,0.0233,-0.1038,-0.0278,-0.0405,-0.0809,-0.0244,-0.0488,0.0527,0.0564,-0.0259,0.0332,0.0096,-0.009,-0.0424,-0.1064,-0.0135,-0.0532,-0.0265,0.0329,0.0025,0.0306,-0.0534,-0.0002,0.0118,-0.0133,-0.0497,0.0423,0.041,-0.0488,0.0729,-0.028,0.0057,0.0289,0.0096,0.0107,-0.0623,-0.0786,-0.0236,0.0701,0.0018,0.0687,0.0086,0.0184,0.0051,0.0506,0.0044,0.0724,-0.0118,-0.0054,0.0313,-0.0423,-0.0243,0.0064,-0.0227,-0.2816,0.0248,0.0341,0.0192,-0.0224,-0.0248,0.0319,-0.0051,-0.0154,-0.0226,-0.0221,0.0414,0.019,-0.0322,-0.0326,0.0396,0.1058,0.0023,0.0547,-0.0395,0.005,0.0382,0.2258,-0.043,0.0011,-0.0331,-0.0268,0.0132,0.0442,-0.0429,-0.0111,0.0056,0.1189,0.01,-0.0133,0.0696,-0.0438,0.0413,0.0421,-0.018,-0.0163,0.0083,-0.0552,-0.0646,0.0864,-0.0024,0.0213,-0.0007,-0.0187,0.0189,0.0127,0.0044,-0.0164,0.0285,0.0171,0.0408,-0.0292,-0.0629,-0.045,-0.0755,0.0376,-0.0599,-0.0388,-0.0007,-0.0562]}
{"key":"[Uncertainty-Aware Deep Classifiers using Generative Models] Deep neural networks are often ignorant about what they do not know and overconfident when they make uninformed predictions. Some recent approaches quantify classification uncertainty directly by training the model to output high uncertainty for the data samples close to class boundaries or from the outside of the training distribution. These approaches use an auxiliary data set during training to represent out-of-distribution samples. However, selection or creation of such an auxiliary data set is non-trivial, especially for high dimensional data such as images. In this work we develop a novel neural network model that is able to express both aleatoric and epistemic uncertainty to distinguish decision boundary and out-of-distribution regions of the feature space. To this end, variational autoencoders and generative adversarial networks are incorporated to automatically generate out-of-distribution exemplars for training. Through extensive analysis, we demonstrate that the proposed approach provides better estimates of uncertainty for in- and out-of-distribution samples, and adversarial examples on well-known data sets against state-of-the-art approaches including recent Bayesian approaches for neural networks and anomaly detection methods.","layer":3,"vector":[-0.0063,-0.0023,0.0353,-0.0431,0.0728,0.0381,0.0517,-0.0112,0.0201,-0.0224,-0.0212,-0.0562,-0.0042,0.0787,0.0227,-0.0016,-0.0073,0.0518,-0.0491,-0.0116,0.0519,-0.0524,0.0327,-0.0603,0.0339,0.0035,-0.0063,-0.0694,-0.0644,-0.2321,0.0345,-0.0515,0.0476,-0.0491,0.0389,-0.0496,-0.0612,0.0329,0.0066,0.025,0.0027,0.026,-0.0427,-0.0709,-0.0022,-0.0449,0.0177,-0.0231,-0.0377,-0.0589,0.0174,-0.0489,0.042,0.0354,0.0392,0.0177,0.0539,0.0257,0.0572,0.1177,0.0285,0.0496,-0.1263,0.0567,0.0273,0.019,-0.0517,-0.0457,0.0015,0.0057,0.0274,0.049,-0.0013,0.0954,0.0282,0.0085,0.0252,-0.042,-0.0637,0.0328,0.0276,0.0033,-0.0302,0.013,0.0144,-0.0506,0.0267,-0.0273,0.0421,-0.0134,-0.0458,0.0088,-0.0528,0.0136,-0.0297,0.0211,0.0182,0.0351,-0.0576,0.1772,-0.0415,0.0044,0.0282,-0.0083,0.0283,-0.0247,-0.0771,-0.0218,-0.0613,-0.0204,0.0139,-0.0058,0.0268,-0.0367,-0.0357,-0.0173,0.0621,0.0072,-0.0465,-0.0146,-0.0742,0.002,0.0305,-0.0057,0.0344,-0.0471,0.0287,0.1484,0.0186,0.0145,0.0232,-0.0112,-0.0314,-0.0485,0.064,0.0094,0.0198,0.0403,0.038,-0.0181,-0.0046,-0.0343,0.0076,-0.0432,-0.0666,0.0795,-0.073,0.0317,-0.0138,-0.0541,-0.0094,0.0175,-0.0363,-0.0336,0.034,0.0481,0.0022,0.0195,-0.0675,-0.0076,0.0084,-0.0715,-0.0139,0.1457,-0.0199,-0.0794,-0.0578,0.0166,0.0322,0.0246,0.045,0.0207,-0.0027,0.0423,0.0698,0.0193,-0.0484,-0.0003,-0.0152,0.0022,-0.0182,-0.0202,-0.0022,0.0316,0.0351,-0.0484,-0.0086,-0.0578,0.0241,0.0454,0.0119,-0.0148,-0.021,-0.0463,-0.018,-0.0448,-0.0314,0.0153,0.0229,-0.0347,-0.0062,0.0235,-0.0466,0.0323,-0.0343,0.0547,0.0267,0.0164,0.0585,0.047,-0.0324,0.0032,0.0492,-0.0367,0.0062,0.0142,0.0326,0.0584,-0.0109,0.0094,0.0437,-0.035,-0.0053,-0.2497,0.032,0.0035,-0.018,0.0023,-0.0757,0.0337,-0.0074,0.0177,0.0618,0.0361,-0.0068,-0.0085,-0.0005,-0.0355,0.0504,0.0094,0.032,-0.0455,0.0459,-0.038,0.0427,-0.0286,-0.1161,0.0338,0.0427,0.2103,0.018,0.0596,-0.0226,0.0111,0.0539,-0.0129,-0.0642,0.0785,0.0207,0.0535,0.0092,-0.0409,-0.0196,0.0068,0.0308,-0.0074,-0.1146,-0.0151,-0.0667,-0.0281,0.0468,-0.0796,0.0174,0.0516,-0.0382,0.0918,-0.0309,0.0124,-0.037,-0.1189,0.0541,-0.0056,-0.0224,0.0139,-0.0407,0.0314,-0.0553,0.0329,0.0192,-0.04,-0.0631,0.0502,-0.0521,-0.0049,0.1226,-0.0043,0.0244,0.0602,-0.0214,0.0407,-0.0242,-0.0676,-0.0479,0.0444,-0.0334,0.0117,0.0155,0.0293,-0.0037,0.0502,-0.0085,0.04,-0.0105,0.004,0.0542,-0.04,-0.01,0.0356,-0.0098,-0.2923,0.0309,0.009,0.0285,-0.0362,-0.0116,0.0565,0.0388,-0.0238,-0.0392,-0.0211,0.0094,0.0417,-0.0388,-0.0443,0.0323,0.0575,-0.0771,0.0669,-0.0144,0.0344,0.0295,0.235,-0.02,0.0146,-0.0131,0.0064,0.0187,-0.0046,-0.0265,0.0149,0.0109,0.062,-0.0645,0.0372,0.1097,-0.0356,0.0503,0.0094,-0.0388,-0.0011,-0.0153,-0.029,-0.0418,0.0797,-0.0104,-0.0059,0.0081,0.0026,0.0087,-0.037,-0.002,-0.06,0.0137,0.0262,0.0594,-0.0275,-0.0496,-0.0214,-0.06,0.0619,-0.0471,-0.0167,-0.0167,-0.0387]}
{"key":"[MixText: Linguistically-Informed Interpolation of Hidden Space for Semi-Supervised Text Classification] This paper presents MixText, a semi-supervised learning method for text classification, which uses our newly designed data augmentation method called TMix. TMix creates a large amount of augmented training samples by interpolating text in hidden space. Moreover, we leverage recent advances in data augmentation to guess low-entropy labels for unlabeled data, hence making them as easy to use as labeled data.By mixing labeled, unlabeled and augmented data, MixText significantly outperformed current pre-trained and fined-tuned models and other state-of-the-art semi-supervised learning methods on several text classification benchmarks. The improvement is especially prominent when supervision is extremely limited. We have publicly released our code at https://github.com/GT-SALT/MixText.","layer":0,"vector":[0.0127,0.0255,0.009,-0.0172,0.0274,0.0109,0.0396,0.0222,0.0096,0.0033,0.0166,-0.0491,0.0259,0.0592,0.0526,0.0315,0.0464,0.0583,-0.0592,-0.0046,0.0335,0.0081,0.029,-0.0334,0.0897,0.0445,-0.0346,-0.0337,-0.0422,-0.2196,0.0312,-0.025,0.04,0.0005,0.0409,-0.0068,-0.0374,0.0292,-0.0314,0.0421,0.0119,-0.067,0.0003,-0.0281,-0.027,-0.0737,-0.0487,-0.0453,-0.041,-0.0191,-0.001,-0.0235,-0.0003,0.0145,0.0298,0.0631,0.0556,-0.0114,-0.0004,0.071,0.0531,0.0599,-0.1496,0.0638,0.0241,0.0082,-0.082,0.0218,-0.0003,0.0474,-0.0087,0.0747,0.0176,0.0597,-0.0164,0.0121,-0.0229,-0.0323,0.0057,0.0219,0.0203,-0.0278,-0.0619,-0.0147,-0.007,-0.0681,0.0239,-0.035,0.0313,0.0354,-0.0564,-0.0302,-0.0069,0.0083,-0.0397,-0.0246,0.022,0.0357,-0.0465,0.2063,-0.0616,0.0301,0.0178,-0.0069,0.0359,-0.0413,-0.0028,-0.0371,-0.0633,-0.0085,0.0047,-0.008,0.0063,-0.0263,0.0453,0.0062,0.1099,-0.0084,-0.0117,0.0185,-0.0209,-0.0176,0.0186,0.0067,0.0342,-0.0295,0.0454,0.1446,0.0123,-0.0124,0.0452,-0.0039,-0.0501,-0.0438,-0.0126,0.0317,-0.0003,0.018,0.0723,-0.0419,-0.0116,-0.0768,-0.0162,-0.1059,-0.0803,0.1323,-0.0434,0.0231,-0.0912,-0.0299,-0.0167,0.0321,-0.0279,-0.0256,0.0255,-0.0091,0.0821,0.0633,-0.0252,0.0023,0.0316,-0.0227,-0.0437,0.0818,0.0311,-0.0936,-0.0177,0.0127,-0.0278,-0.0285,0.043,0.0356,-0.0559,0.0133,0.0454,0.0456,-0.0775,0.0005,0.0007,-0.0269,0.0158,-0.0475,-0.0798,0.1068,0.0063,-0.0523,0.0115,-0.0622,0.0212,0.0299,-0.0217,0.0432,0.0049,0.0026,-0.0397,-0.0115,-0.0112,0.0045,-0.0048,-0.0744,0.0224,0.0548,-0.0407,0.0093,-0.0059,0.0035,0.0102,0.0169,0.0703,0.0153,-0.0187,0.0103,0.0254,-0.0291,-0.0559,-0.0243,-0.0098,0.0278,-0.0046,-0.003,0.0211,-0.0207,-0.0335,-0.2172,-0.0026,0.0165,-0.0144,0.0393,-0.0901,0.0559,0.0093,0.0663,0.0973,0.0437,-0.0185,-0.0409,0.0123,-0.022,0.0299,0.0782,0.0141,0.0013,0.0088,0.0101,0.0371,-0.0382,-0.0591,0.0778,-0.0054,0.208,0.0568,0.0877,-0.0416,0.052,-0.0045,-0.0591,-0.1087,0.0804,0.0139,0.0461,-0.0427,-0.0696,-0.039,-0.0086,0.0367,-0.0159,-0.1406,0.0042,-0.0623,-0.0329,-0.0813,-0.0652,0.0387,0.055,-0.0019,0.0613,0.0263,-0.0449,0.0046,-0.0693,0.0079,-0.022,-0.0062,-0.0077,-0.0469,0.0576,-0.0798,0.0005,0.0255,-0.0629,-0.0118,0.0267,-0.0259,-0.0402,0.0802,-0.0384,-0.0157,0.0525,0.0209,0.0091,0.0181,-0.0546,0.0062,0.0809,0.0418,0.0709,0.0168,0.0064,0.0147,0.0773,-0.0192,0.0472,-0.0054,0.0217,0.0469,-0.0451,-0.0206,0.0641,-0.0018,-0.2818,0.0052,0.021,-0.0034,-0.0241,-0.0265,0.0552,-0.0046,-0.009,0.0243,0.0277,0.0124,0.0191,-0.0677,-0.0065,0.0181,0.0582,-0.0548,0.0273,-0.0614,0.0273,0.0179,0.1996,-0.0068,0.0158,0.0077,-0.0075,0.0089,0.0429,-0.0125,-0.0073,0.0346,0.0758,-0.0083,0.0307,0.0415,-0.0095,0.0104,-0.0091,-0.0082,0.0107,0.0121,-0.0902,-0.0415,0.0404,0.0038,-0.0187,-0.0759,-0.0452,0.0133,-0.0261,0.0458,-0.0243,0.0148,0.0644,0.0272,-0.0562,-0.0669,-0.0187,-0.0452,0.0091,-0.0884,-0.0048,0.0016,-0.014]}
{"key":"[Semi-Orthogonal Multilinear PCA with Relaxed Start] Principal component analysis (PCA) is an unsupervised method for learning low-dimensional features with orthogonal projections. Multilinear PCA methods extend PCA to deal with multidimensional data (tensors) directly via tensor-to-tensor projection or tensor-to-vector projection (TVP). However, under the TVP setting, it is difficult to develop an effective multilinear PCA method with the orthogonality constraint. This paper tackles this problem by proposing a novel Semi-Orthogonal Multilinear PCA (SO-MPCA) approach. SO-MPCA learns low-dimensional features directly from tensors via TVP by imposing the orthogonality constraint in only one mode. This formulation results in more captured variance and more learned features than full orthogonality. For better generalization, we further introduce a relaxed start (RS) strategy to get SO-MPCA-RS by fixing the starting projection vectors, which increases the bias and reduces the variance of the learning model. Experiments on both face (2D) and gait (3D) data demonstrate that SO-MPCA-RS outperforms other competing algorithms on the whole, and the relaxed start strategy is also effective for other TVP-based PCA methods.","layer":2,"vector":[-0.0581,-0.0124,0.0462,-0.018,-0.0088,0.0556,0.0342,-0.0046,0.0026,0.0316,0.0431,-0.0847,-0.0042,0.0713,0.0494,0.0131,0.0425,0.0855,-0.0261,0.0582,0.0204,-0.0101,0.0056,-0.0278,0.0318,-0.0149,-0.0142,-0.0177,-0.0307,-0.236,-0.0188,0.0115,0.0565,-0.0404,-0.0026,-0.0013,-0.0718,0.0643,-0.0037,0.0419,-0.0227,0.0118,-0.0096,-0.0312,-0.0182,-0.0371,-0.0211,0.0231,0.0122,-0.0134,0.037,-0.0151,-0.0184,-0.0043,0.0435,0.0118,0.0178,0.0133,0.0293,0.0269,0.0261,0.0337,-0.1646,0.0635,0.0424,0.0103,-0.0265,-0.0361,0.0244,0.0229,-0.0337,0.0775,0.0304,-0.013,-0.0195,-0.0238,0.0068,-0.006,0.0099,0.0282,0.0392,0.0091,-0.0392,0.0001,-0.0552,-0.047,-0.0111,-0.0464,0.0283,0.0201,-0.023,-0.0424,-0.0211,0.0015,-0.0264,-0.0558,0.0605,0.0431,-0.0478,0.2039,-0.0715,0.0662,0.0528,0.014,0.0304,-0.033,0.0019,-0.0072,-0.0228,0.0202,0.0402,0.0032,0.0194,-0.06,0.043,0.0018,0.0553,0.0468,0.0165,-0.0119,0.0043,-0.0398,0.0134,-0.0205,-0.0277,-0.0865,0.0608,0.1192,0.0611,0.0436,0.0455,-0.0157,-0.0546,-0.0411,0.0082,-0.0032,0.0163,0.0271,-0.0037,-0.0228,-0.0512,-0.0526,0.0382,-0.0959,-0.0065,0.1396,-0.0286,0.0057,-0.0351,0.0144,0.0231,0.0218,-0.0165,0.0079,0.0059,0.0024,0.0286,0.0367,-0.0294,0.0235,-0.0334,-0.0411,-0.0053,0.0579,0.0231,-0.1113,-0.0519,-0.0368,0.0323,0.0103,0.0451,0.0743,-0.0438,-0.0159,0.0954,0.0214,-0.0982,0.0261,-0.0301,0.0161,0.0542,-0.0665,-0.0479,-0.0003,0.008,-0.032,0.0085,-0.0047,-0.0076,-0.0125,-0.0401,-0.0029,-0.0604,-0.0231,-0.0283,-0.0295,-0.0135,-0.0395,0.0105,-0.0438,0.0292,0.0138,-0.0187,0.045,-0.0148,0.045,-0.0075,-0.0141,0.0495,0.0387,-0.0079,-0.0095,0.0538,-0.0436,-0.0143,0.0049,0.0274,0.0367,-0.0225,0.0404,0.0497,-0.044,-0.0868,-0.2562,0.016,0.0124,0.0086,0.0416,-0.0631,0.0434,-0.0011,0.0759,0.0788,0.108,0.0318,-0.0502,0.0361,-0.0456,0.0837,0.0595,0.0182,0.0013,-0.0143,-0.0489,0.0377,-0.0145,-0.0459,0.0487,0.0186,0.181,0.0326,0.019,-0.0543,0.0193,0.0549,-0.0232,-0.12,0.0995,0.0031,0.0237,-0.0494,-0.0367,-0.0391,0.0026,0.029,0.0401,-0.0646,-0.0578,-0.0811,0.0144,0.0178,-0.0327,-0.0092,0.045,-0.0292,0.0398,-0.0205,0.0026,-0.06,-0.0729,0.0198,-0.0412,-0.0006,-0.0333,-0.0774,0.0392,-0.0698,0.0475,0.0128,-0.0526,-0.0032,0.0708,0.0051,-0.0413,0.1042,-0.0113,0.0009,0.0864,-0.0212,0.0629,0.0156,-0.0558,0.0057,0.0699,-0.0481,-0.0273,0.0149,0.0052,-0.0024,0.0932,-0.0375,-0.0055,-0.0583,0.0288,0.0123,-0.0696,-0.0194,0.0471,0.0262,-0.2971,-0.0138,0.0419,0.0231,-0.0275,-0.0016,0.0031,0.0272,-0.0433,-0.0312,-0.0045,0.0497,0.0613,-0.0231,0.0035,0.0217,0.0738,-0.064,0.0543,-0.0547,-0.0094,0.0269,0.1934,-0.0448,0.0131,-0.0308,-0.0107,0.013,0.0306,-0.0319,-0.0183,-0.0264,0.0855,-0.0577,0.0647,0.0859,-0.0556,0.0391,-0.0031,-0.0269,0.0077,0.0227,-0.0487,-0.0519,0.087,0.0096,-0.0322,-0.0312,0.0146,-0.0086,0.0377,0.0158,-0.0121,-0.0168,0.0037,0.0054,-0.0091,-0.0361,-0.0599,-0.0269,0.0067,-0.0406,-0.0776,-0.0026,-0.0009]}
{"key":"[$\\mathsf{G^2Retro}$: Two-Step Graph Generative Models for Retrosynthesis Prediction] Retrosynthesis is a procedure where a molecule is transformed into potential reactants and thus the synthesis routes are identified. We propose a novel generative framework, denoted as $\\mathsf{G^2Retro}$, for one-step retrosynthesis prediction. $\\mathsf{G^2Retro}$ imitates the reversed logic of synthetic reactions, that is, first predicting the reaction centers to convert the target molecule into fragments named synthons, and then transforming synthons into reactants, following previous semi-template-based methods. In predicting reaction centers, $\\mathsf{G^2Retro}$ defines a comprehensive set of reaction center types, and enables diversity in the predicted reactions by considering multiple reaction center candidates. In completing synthons, $\\mathsf{G^2Retro}$ deploys a sequence of substructure attachments to transform synthons into reactants, which utilize a holistic view of the most updated structures of the synthons to be completed, as well as all the involved synthon and product structures. Here we show that $\\mathsf{G^2Retro}$ is able to better prioritize the most possible reactants in the benchmark dataset than the state-of-the-art methods, and discover novel and highly likely reactions that are not included in the benchmark dataset.","layer":0,"vector":[-0.0587,0.0009,0.01,-0.0192,0.0208,0.0211,-0.0112,0.026,-0.0526,0.0124,-0.0097,-0.0483,0.029,-0.0055,0.038,-0.0007,-0.0328,0.0765,-0.0516,-0.0116,0.0558,-0.0188,-0.0094,-0.019,0.0488,0.0727,-0.0123,0.0297,-0.0286,-0.2319,0.0327,0.0028,0.0114,-0.029,-0.0148,-0.0184,-0.0168,0.0322,-0.0633,0.0503,0.0223,0.0011,-0.0191,-0.0419,-0.0149,-0.0185,-0.0716,0.0142,-0.0194,-0.0406,0.0487,-0.0286,0.0204,0.0332,0.0647,0.0502,0.0495,0.0382,0.0132,0.0143,0.0276,0.0675,-0.136,0.0046,0.0354,0.0322,-0.0453,-0.0117,0.0343,0.1132,-0.0415,0.0206,-0.0179,0.005,0.0178,0.0384,-0.0216,-0.0238,-0.0025,0.0252,-0.0109,0.0004,-0.0671,0.0097,-0.0152,-0.0399,0.0226,0.008,0.0756,0.0388,-0.0357,-0.0526,0.0239,0.0417,-0.0793,0.0059,0.0157,0.0334,0.0093,0.215,-0.052,0.0372,0.043,-0.0264,0.0288,-0.0851,0.0035,-0.0378,0.0126,-0.0294,0.0213,-0.008,0.0473,-0.0416,-0.004,0.0056,0.0737,0.0663,-0.0322,-0.0046,0.0007,-0.0033,0.0113,0.0009,0.0229,-0.0491,-0.0154,0.1236,0.0714,0.0399,0.0338,0.0311,-0.0133,-0.0068,0.0019,0.0038,0.0528,0.0178,0.0261,-0.0309,-0.0063,-0.0086,-0.0284,-0.0567,-0.0455,0.1344,-0.0394,0.0203,-0.0572,0.0332,0.0035,0.027,-0.0349,-0.0152,0.0373,0.0148,-0.0012,0.0174,-0.0597,0.0418,-0.0291,0.0089,-0.0867,0.0849,-0.0035,-0.0674,-0.0235,0.013,0.0006,0.0091,0.0204,0.0326,-0.0035,0.0234,0.0695,0.0495,-0.0142,-0.0336,-0.0034,0.0149,0.0677,-0.0424,-0.0464,0.0512,-0.0022,-0.0741,0.0109,-0.0245,-0.0081,0.0665,-0.0269,0.0446,-0.0701,0.0062,-0.0258,-0.0435,-0.016,0.0027,0.0204,-0.0436,0.051,-0.0067,-0.0511,0.0182,-0.0239,-0.0387,0.0131,0.0154,0.0307,0.0311,-0.0096,-0.0016,0.0466,-0.0516,-0.051,-0.0205,-0.0313,0.0373,0.0207,0.0055,0.0487,-0.0546,-0.0702,-0.2064,-0.0138,0.0229,-0.0445,0.0715,-0.0567,0.0029,-0.0394,0.0467,0.0409,0.0557,0.0142,-0.0446,0.0137,-0.0473,0.0142,0.0167,0.0112,0.0064,-0.0105,-0.0049,-0.0112,0.0046,-0.0963,0.0108,-0.0148,0.2042,0.0771,0.0188,-0.0122,0.0294,0.0402,-0.0356,-0.0943,0.0568,0.0605,0.0621,-0.0491,-0.0224,-0.0313,-0.0086,0.0016,-0.052,-0.116,-0.0421,0.0066,-0.0517,0.0005,-0.0406,0.0268,0.0599,0.0141,0.068,0.0034,-0.0301,-0.0305,-0.0713,0.02,-0.0268,0.0072,-0.0021,-0.0267,-0.0201,-0.0045,0.0474,-0.0154,-0.0032,-0.072,0.044,-0.0277,0.0416,0.0968,0.0029,0.0052,0.0654,0.02,-0.0384,-0.0419,-0.1025,-0.0293,0.0176,-0.0559,0.034,-0.0178,0.0269,0.0046,0.0614,0.0005,0.0265,-0.0406,-0.0074,0.0198,-0.042,-0.0107,0.0038,0.0156,-0.3061,-0.0096,0.0129,0.0415,-0.0067,-0.023,0.105,0.0002,-0.0714,0.0106,0.034,0.0484,0.0278,0.0343,0.01,0.0106,0.1027,-0.0651,0.0176,-0.0716,0.028,0.0276,0.2637,-0.0241,0.0169,0.0325,-0.0202,0.0046,0.0526,0.0231,0.0007,-0.0045,0.0628,-0.0945,0.0598,0.0639,-0.103,0.0574,0.0127,-0.0084,-0.0474,-0.0103,-0.0423,-0.007,0.1076,-0.0442,-0.0497,-0.0359,0.0038,0.017,-0.0626,0.0465,-0.0294,-0.0015,0.0063,0.0006,-0.0796,-0.0443,-0.0458,-0.0468,-0.0135,-0.0235,0.0315,0.066,-0.0428]}
{"key":"[Data-Driven Optimization of Public Transit Schedule] Bus transit systems are the backbone of public transportation in the United States. An important indicator of the quality of service in such infrastructures is on-time performance at stops, with published transit schedules playing an integral role governing the level of success of the service. However there are relatively few optimization architectures leveraging stochastic search that focus on optimizing bus timetables with the objective of maximizing probability of bus arrivals at timepoints with delays within desired on-time ranges. In addition to this, there is a lack of substantial research considering monthly and seasonal variations of delay patterns integrated with such optimization strategies. To address these,this paper makes the following contributions to the corpus of studies on transit on-time performance optimization: (a) an unsupervised clustering mechanism is presented which groups months with similar seasonal delay patterns, (b) the problem is formulated as a single-objective optimization task and a greedy algorithm, a genetic algorithm (GA) as well as a particle swarm optimization (PSO) algorithm are employed to solve it, (c) a detailed discussion on empirical results comparing the algorithms are provided and sensitivity analysis on hyper-parameters of the heuristics are presented along with execution times, which will help practitioners looking at similar problems. The analyses conducted are insightful in the local context of improving public transit scheduling in the Nashville metro region as well as informative from a global perspective as an elaborate case study which builds upon the growing corpus of empirical studies using nature-inspired approaches to transit schedule optimization.","layer":1,"vector":[-0.0181,-0.014,0.0678,0.0121,0.0255,0.0115,0.041,0.0321,0.0183,0.0011,0.0091,-0.0056,0.0242,0.0331,-0.008,-0.0084,0.0188,0.0348,-0.0079,-0.0405,0.0371,-0.0362,-0.0818,-0.0536,0.0675,0.0319,-0.0211,-0.0081,-0.0659,-0.2521,-0.0297,-0.0502,0.0261,-0.0505,0.0253,0.0052,-0.0141,0.0929,-0.0098,0.018,0.0265,0.0317,-0.0395,-0.0399,-0.0341,-0.038,-0.0306,0.0082,-0.0669,-0.0298,0.0141,-0.0117,0.0097,0.0529,-0.0121,0.0626,0.0353,0.0267,0.0231,0.027,-0.0042,0.0094,-0.1897,0.0819,0.0767,0.0081,-0.0051,-0.0128,0.0052,0.0272,-0.0093,0.0642,0.0157,0.0821,0.0305,0.0251,-0.0164,-0.0374,-0.0016,0.0502,0.0121,-0.0586,0.0009,0.0191,0.0149,-0.0464,0.0112,-0.0089,0.0635,0.0229,0.0016,0.007,-0.0235,0.0208,-0.0784,-0.0011,0.0225,0.0115,-0.0063,0.1836,-0.0521,0.0538,0.0194,-0.0262,0.0045,-0.0385,-0.0132,-0.0787,-0.0723,-0.0056,0.0262,-0.0332,0.0255,-0.0311,0.0304,0.0322,0.0306,0.0215,-0.0086,0.0374,-0.0307,0.0118,0.0426,0.0093,0.0075,-0.0567,0.0358,0.1181,-0.007,0.0134,0.0719,0.0033,-0.066,-0.0232,0.0066,-0.0018,0.0284,-0.0363,-0.0128,-0.0269,-0.0241,-0.061,0.0239,-0.1082,-0.0413,0.1069,-0.0018,0.0418,-0.0393,-0.0088,-0.0377,-0.0475,-0.0385,-0.0838,-0.0442,0.0411,0.0293,0.0608,-0.0398,0.0311,-0.0201,-0.0059,-0.0197,0.0906,-0.0335,-0.0702,-0.0191,-0.0146,0.0208,-0.0542,0.0154,0.0636,-0.0274,0.0772,0.1005,-0.0075,-0.0484,0.0321,0.0015,-0.0089,0.044,-0.0226,-0.0145,-0.0038,0.074,-0.0441,-0.009,-0.0243,-0.0483,0.0516,-0.0121,-0.0683,0.0181,-0.0139,-0.0502,-0.0761,0.0134,-0.0067,0.0225,-0.0201,0.0146,0.0292,-0.0466,0.0419,0.0259,0.0255,-0.0242,-0.042,0.0582,0.0318,-0.0361,0.0122,0.0843,0.0127,-0.0375,0.0046,0.0134,0.0393,0.0432,0.0594,0.0097,0.0295,-0.0759,-0.1913,-0.0421,-0.0226,-0.004,0.0602,-0.0019,-0.0009,-0.0253,0.0371,0.0516,0.0789,-0.0217,-0.0435,0.0427,-0.0093,0.0276,-0.0085,0.0317,-0.0076,-0.0131,0.0459,0.0363,-0.0095,-0.1011,0.019,0.0248,0.1865,0.0032,0.0559,-0.0503,0.0573,-0.0645,-0.0347,-0.0488,0.0448,0.037,0.0828,0.0103,-0.0264,-0.0385,-0.0417,0.0619,-0.0292,-0.0329,-0.0702,-0.0455,0.0264,0.0174,-0.0478,-0.032,0.0054,-0.0158,0.0185,-0.0148,0.0233,-0.0383,-0.0899,0.0173,-0.0377,0.0143,0.0039,-0.0586,-0.0129,-0.0264,0.0362,-0.0093,-0.0239,-0.0025,-0.0259,-0.0267,-0.0203,0.1296,-0.0118,-0.0057,0.0534,-0.0116,0.0182,-0.0009,-0.0077,-0.0127,0.0495,-0.0385,0.0507,0.0038,0.0138,-0.0084,0.0841,-0.0125,0.0249,0.0041,0.0227,-0.026,-0.0214,0.0071,0.0475,0.0031,-0.2983,0.0331,0.0397,-0.0255,-0.0192,0.0105,-0.0046,0.0427,-0.0179,-0.0392,0.0306,0.0702,0.0606,-0.0009,0.0221,0.0296,0.0601,-0.0418,0.0159,-0.1056,0.0109,-0.0063,0.2553,-0.0639,0.04,0.0569,-0.0585,0.0425,-0.0005,-0.0788,-0.034,-0.001,0.0981,-0.043,0.0171,0.0622,-0.0578,0.0747,0.017,0.0171,-0.0404,0.0284,-0.0138,-0.0099,0.1266,-0.0076,-0.0578,-0.0692,0.0489,0.0118,-0.0565,-0.0215,-0.0877,-0.018,0.0101,0.0517,-0.0646,-0.0509,-0.0166,0.0013,0.0141,-0.052,-0.0221,0.0144,0.0327]}
{"key":"[Causal Feature Selection via Orthogonal Search] The problem of inferring the direct causal parents of a response variable among a large set of explanatory variables is of high practical importance in many disciplines. However, established approaches often scale at least exponentially with the number of explanatory variables, are difficult to extend to nonlinear relationships, and are difficult to extend to cyclic data. Inspired by {\\em Debiased} machine learning methods, we study a one-vs.-the-rest feature selection approach to discover the direct causal parent of the response. We propose an algorithm that works for purely observational data while also offering theoretical guarantees, including the case of partially nonlinear relationships possibly under the presence of cycles. As it requires only one estimation for each variable, our approach is applicable even to large graphs. We demonstrate significant improvements compared to established approaches.","layer":0,"vector":[-0.0488,0.0359,0.0275,-0.0338,0.0608,-0.0124,0.0355,0.0665,0.0215,-0.0531,0.0229,-0.056,0.0242,0.0618,0.0178,0.0027,0.0233,0.0677,-0.0585,0.0216,0.0157,-0.051,-0.0091,-0.0405,0.0249,0.0129,-0.0084,-0.0044,-0.0371,-0.249,-0.0074,-0.0362,0.0552,-0.0084,0.0153,-0.019,-0.0205,0.0633,-0.0304,0.0489,0.0035,0.0275,-0.0138,-0.0495,-0.0411,-0.053,0.0117,0.0178,-0.037,-0.0198,0.0367,-0.0491,0.029,0.0277,0.0483,0.0455,0.0287,0.029,0.032,0.0443,0.0129,0.0275,-0.1598,0.0291,0.0689,0.0535,-0.0375,-0.019,0.0065,0.0846,-0.0004,0.0643,-0.0047,0.0193,-0.0183,-0.0054,0.0268,-0.0113,-0.0131,0.0049,0.0321,0.0234,-0.0399,0.0081,-0.0187,-0.057,0.0082,-0.0695,0.0745,0.0295,-0.0689,-0.0029,0.0039,-0.012,-0.0704,-0.0507,0.0607,-0.0018,0.0178,0.1683,-0.0972,0.0468,-0.0443,-0.0156,0.0262,-0.0769,-0.0269,-0.0567,-0.0319,0.0029,0.0245,-0.0213,-0.0024,-0.0561,0.0191,-0.0144,0.042,0.0324,-0.0236,-0.0137,-0.0089,-0.0178,0.0559,-0.0591,0.002,-0.0836,0.0153,0.145,0.0513,-0.0189,0.0614,-0.0184,-0.0542,-0.003,0.0058,0.0078,0.0484,0.0312,-0.0069,0.0046,-0.064,-0.061,0.0539,-0.0689,-0.0906,0.158,-0.0262,-0.0034,-0.055,-0.0396,-0.0336,0.042,-0.029,-0.0717,0.033,0.0894,-0.0089,0.0327,-0.0203,0.0357,-0.0272,-0.0523,-0.0269,0.0803,0.0162,-0.1032,-0.036,0.0083,0.0035,-0.014,0.0507,0.0319,-0.0558,0.0314,0.0935,0.0268,-0.0691,0.0326,0.0233,0.0296,0.042,-0.0528,-0.0352,0.0154,0.0678,-0.0033,0.0109,-0.0276,0.0193,0.0062,-0.0073,-0.028,0.001,0.0098,-0.0397,-0.0127,-0.0065,-0.0194,-0.013,-0.007,0.0154,-0.008,-0.0351,0.0149,-0.0492,0.0238,-0.0342,0.0027,0.0443,0.0011,-0.0093,0.0481,0.0424,-0.0434,-0.0322,0.016,-0.0239,0.0402,0.0086,0.052,0.0451,-0.0556,-0.0067,-0.2651,-0.0583,-0.0122,-0.0014,0.0146,-0.0863,0.0301,-0.0007,0.0835,0.0906,0.02,0.0217,-0.0917,0.0293,0.0007,0.0518,0.0311,0.0369,-0.0195,0.0005,-0.0338,0.0033,0.0071,-0.0436,0.0366,0.0042,0.2051,0.0535,0.0265,-0.0391,0.0275,-0.007,0.0046,-0.082,0.0412,0.0418,0.0199,-0.0167,-0.0477,-0.0498,-0.003,0.0573,-0.0363,-0.0342,-0.0305,-0.0401,0.0114,0.0088,-0.0574,0.0507,0.0382,-0.0086,0.0804,-0.0019,0.0057,-0.0527,-0.1249,-0.0027,-0.057,0.0106,-0.0029,-0.0414,-0.0071,-0.0576,0.0379,-0.0012,-0.0185,-0.0365,0.0178,-0.0204,-0.0044,0.0896,-0.0191,-0.005,0.0438,0.0329,0.0545,-0.0504,-0.0379,-0.0147,0.076,-0.0689,-0.0142,0.0187,-0.0128,-0.0054,0.0704,-0.0094,0.0431,-0.0223,0.0172,0.0142,-0.0496,-0.0383,0.0295,0.031,-0.2932,0.0386,0.0141,0.0142,0.0034,0.0304,0.0347,0.0368,-0.0391,-0.0168,0.0245,0.018,0.0432,-0.0283,-0.0193,0.0546,0.0658,-0.0543,0.0337,-0.0702,0.0265,0.0515,0.2088,-0.0076,0.042,0.0294,-0.051,0.012,-0.0193,-0.0127,0.0572,0.0199,0.0443,-0.0417,0.0525,0.0596,-0.0405,-0.0081,0.0237,-0.0312,0.002,0.015,-0.0551,-0.0395,0.1456,-0.0067,-0.049,-0.0331,0.0043,0.015,-0.0105,0.0014,-0.0444,0.0292,0.0226,0.0245,-0.0401,0.0034,-0.0025,-0.054,0.0128,0.0004,0.0068,-0.0036,-0.0032]}
{"key":"[Disentangled and Side-aware Unsupervised Domain Adaptation for Cross-dataset Subjective Tinnitus Diagnosis] EEG-based tinnitus classification is a valuable tool for tinnitus diagnosis, research, and treatments. Most current works are limited to a single dataset where data patterns are similar. But EEG signals are highly non-stationary, resulting in model's poor generalization to new users, sessions or datasets. Thus, designing a model that can generalize to new datasets is beneficial and indispensable. To mitigate distribution discrepancy across datasets, we propose to achieve Disentangled and Side-aware Unsupervised Domain Adaptation (DSUDA) for cross-dataset tinnitus diagnosis. A disentangled auto-encoder is developed to decouple class-irrelevant information from the EEG signals to improve the classifying ability. The side-aware unsupervised domain adaptation module adapts the class-irrelevant information as domain variance to a new dataset and excludes the variance to obtain the class-distill features for the new dataset classification. It also align signals of left and right ears to overcome inherent EEG pattern difference. We compare DSUDA with state-of-the-art methods, and our model achieves significant improvements over competitors regarding comprehensive evaluation criteria. The results demonstrate our model can successfully generalize to a new dataset and effectively diagnose tinnitus.","layer":0,"vector":[-0.0291,-0.0554,0.0048,-0.02,0.0184,0.0324,0.0622,0.0087,-0.0066,0.0162,0.0487,-0.0281,-0.0028,0.0405,0.0488,0.028,-0.0111,0.0403,-0.0264,-0.0039,0.034,0.0169,0.0189,-0.0128,0.0169,0.0014,0.0017,-0.03,-0.0648,-0.2377,0.0286,-0.0004,-0.001,-0.0228,0.0376,-0.0561,-0.0349,0.0634,-0.022,0.0134,0.0269,0.0011,0.0371,-0.0484,-0.0381,-0.0803,-0.0526,-0.0173,0.0001,-0.0461,0.0282,-0.0422,-0.0004,0.0072,0.012,0.0401,0.0595,0.0431,0.0392,0.0512,-0.0102,0.051,-0.1323,0.0769,0.0476,0.0346,-0.0387,-0.0705,0.0172,-0.0056,-0.0078,0.0561,0.0148,0.0108,-0.0102,-0.0095,-0.0086,-0.0089,-0.0308,0.0326,-0.0102,0.0242,-0.0329,-0.0438,0.0257,-0.0196,0.0189,-0.0807,-0.0007,0.0105,-0.0869,-0.0335,-0.0079,0.0124,-0.018,-0.0088,0.0086,0.0404,-0.0883,0.2077,-0.0067,-0.0097,0.0111,-0.0328,0.0133,-0.0272,-0.0449,-0.0448,-0.0188,0.0002,0.0135,0.0028,0.0492,-0.0556,0.0448,0.0093,0.0524,0.0439,0.0553,0.0256,-0.0446,0.0013,0.0476,-0.0386,0.0223,-0.0504,0.0394,0.1525,0.0082,0.0207,0.0433,-0.0366,-0.0556,-0.0015,0.013,-0.0041,0.0217,0.0161,-0.0002,0.0075,-0.0247,-0.09,0.0533,-0.0825,-0.0783,0.0974,-0.0543,0.0036,-0.0373,0.0062,-0.0259,0.0095,-0.061,-0.01,0.0094,0.0612,0.0115,0.0043,0.0008,0.0108,-0.0163,-0.0355,-0.0055,0.1123,0.0064,-0.0839,-0.064,-0.036,0.0129,-0.0475,0.0502,0.019,-0.0368,0.0628,0.1029,0.0495,-0.0271,-0.0247,-0.0007,0.0063,0.0432,-0.0346,-0.029,0.0409,0.0033,-0.061,0.0162,-0.0312,0.0199,0.0138,-0.0341,-0.0281,-0.0261,0.0321,-0.028,-0.0165,-0.0128,-0.0201,0.0092,0.0049,0.031,0.0263,-0.0153,0.0286,0.0072,0.0552,-0.0119,0.0083,0.087,0.0527,0.0026,0.0096,0.0986,-0.0153,-0.0342,0.0219,0.0075,0.0688,0.0067,0.0648,0.0209,-0.0208,-0.0874,-0.2337,-0.0021,0.0245,-0.0378,0.0213,-0.0636,0.0575,0.0126,0.0707,0.0778,0.0618,0.0178,-0.0543,0.0069,-0.006,0.0636,0.0569,0.0369,0.0166,-0.0162,0.0272,0.0185,0.01,-0.1106,0.0422,-0.0153,0.2019,-0.0285,0.0581,-0.035,-0.0246,0.0339,-0.0102,-0.1384,0.0391,0.0364,0.0669,0.008,-0.0847,-0.0425,-0.0666,0.034,0.002,-0.089,-0.0538,-0.0618,-0.0104,-0.0217,-0.0377,0.0316,0.0534,-0.0331,0.0936,0.011,0.0109,-0.0447,-0.0925,-0.0023,-0.0337,0.0189,0.0166,-0.0114,0.0117,-0.0785,0.0151,0.0252,-0.0576,-0.0337,0.0372,-0.0464,-0.0184,0.0942,-0.0094,-0.0177,-0.0112,-0.0157,0.0481,-0.0253,-0.0609,-0.0002,0.0512,-0.0239,0.0185,-0.005,0.055,0.0369,0.0912,0.0041,0.0021,-0.0389,-0.0058,0.0219,-0.0411,-0.0178,0.0502,-0.0023,-0.292,-0.003,0.0146,0.0038,-0.0496,0.0042,0.0043,0.0339,-0.0742,-0.0281,-0.0584,-0.0158,0.0135,-0.018,0.0195,0.0337,0.0729,-0.0913,0.0618,-0.0245,0.003,0.071,0.2141,-0.0095,0.0451,-0.0083,-0.0341,-0.0229,0.041,-0.0133,-0.019,0.0093,0.0644,-0.0562,0.0687,0.0595,-0.0555,0.0139,0.0213,0.0167,0.0344,-0.0015,-0.0198,-0.0155,0.1074,0.032,-0.0098,0.0,-0.0077,0.0349,-0.0287,-0.0059,0.0162,0.0232,0.0106,0.0054,-0.029,-0.0251,0.0246,-0.0908,0.0165,-0.0841,-0.0681,0.0391,-0.0091]}
{"key":"[Knowledge Discovery In Nanophotonics Using Geometric Deep Learning] We present here a new approach for using the intelligence aspects of artificial intelligence for knowledge discovery rather than device optimization in electromagnetic (EM) nanostructures. This approach uses training data obtained through full-wave EM simulations of a series of nanostructures to train geometric deep learning algorithms to assess the range of feasible responses as well as the feasibility of a desired response from a class of EM nanostructures. To facilitate the knowledge discovery and reduce the computation complexity, our approach combines the dimensionality reduction technique (using an autoencoder) with convex-hull and one-class support-vector-machine (SVM) algorithms to find the range of the feasible responses in the latent (or the reduced) response space of the EM nanostructure. We show that by using a small set of training instances (compared to all possible structures), our approach can provide better than 95% accuracy in assessing the feasibility of a given response. More importantly, the one-class SVM algorithm can be trained to provide the degree of feasibility (or unfeasibility) of a response from a given nanostructure. This important information can be used to modify the initial structure to an alternative one that can enable an initially unfeasible response. To show the applicability of our approach, we apply it to two important classes of binary metasurfaces (MSs), formed by array of plasmonic nanostructures, and periodic MSs formed by an array of dielectric nanopillars. In addition to theoretical results, we show the experimental results obtained by fabricating several MSs of the second class. Our theoretical and experimental results confirm the unique features of this approach for knowledge discovery in EM nanostructures.","layer":3,"vector":[-0.0345,0.0198,0.0621,0.006,-0.0046,-0.0029,0.047,0.0549,-0.0113,-0.0279,0.007,-0.0359,0.0357,0.0427,0.0566,0.0203,0.0183,0.0378,-0.0571,0.0257,0.0762,-0.0505,-0.0409,-0.071,0.0286,0.0221,-0.0058,0.0179,-0.0818,-0.2356,0.0429,-0.0226,0.0119,0.0151,0.0123,-0.0362,-0.0453,0.0353,-0.0484,0.0447,0.0153,-0.0179,0.0228,-0.0311,-0.008,-0.0452,-0.0128,-0.0246,-0.0273,-0.0974,0.0003,-0.0518,-0.0058,0.0226,0.0151,0.067,0.0252,0.0511,0.0584,0.0807,0.0363,0.0467,-0.1586,0.0903,0.01,0.0415,-0.0372,-0.0403,0.027,0.0774,0.0166,0.0167,0.0166,0.0164,0.0111,0.0151,-0.0007,-0.0648,-0.0129,0.0228,0.0064,-0.0215,-0.0366,0.0268,0.0073,-0.011,0.0033,-0.0639,0.0262,0.0224,-0.0264,-0.0262,-0.0406,-0.0378,-0.0742,-0.0394,0.0263,0.0069,-0.0506,0.1736,-0.0465,0.0053,0.0067,-0.0884,-0.0227,-0.0176,-0.0381,-0.0293,-0.0414,-0.024,0.0017,-0.0302,-0.0111,-0.0422,0.0124,0.0094,0.0557,0.0448,-0.0222,-0.0191,-0.0489,0.0037,0.0226,-0.0278,0.003,-0.0664,-0.0283,0.143,0.0098,0.0616,0.037,-0.0121,-0.0397,-0.0378,0.0386,0.0131,0.0139,-0.0367,0.0118,0.0362,-0.0402,-0.0359,0.0272,-0.0821,-0.057,0.1193,-0.0619,0.0293,-0.0658,-0.0292,0.0087,0.0364,-0.0496,-0.0419,0.0127,0.0391,-0.0131,0.0284,-0.0353,0.0187,-0.017,-0.0093,-0.0126,0.1592,-0.0221,-0.0951,-0.062,-0.0027,0.0223,-0.0341,0.0371,0.0923,-0.032,0.0239,0.0611,0.0209,-0.0576,-0.0118,0.0018,0.0318,-0.02,-0.0469,0.0223,0.0578,0.0593,-0.0066,0.0211,-0.0418,-0.0073,0.0115,-0.0287,0.0119,-0.0152,-0.0195,-0.0348,-0.0053,-0.0491,0.0085,0.0181,0.001,0.0457,-0.0009,-0.0483,0.0998,0.0144,0.019,0.0736,0.0326,0.076,0.014,0.0058,-0.0526,0.0289,-0.0696,-0.0057,0.0105,-0.0032,0.0115,-0.021,0.0539,0.0191,-0.0764,-0.1516,-0.2052,-0.0291,-0.0104,-0.0481,0.0362,-0.0356,0.0377,-0.0115,0.0385,0.0431,0.0454,0.064,-0.0315,-0.0392,-0.0193,0.0304,0.0421,0.0571,-0.0385,0.043,-0.0281,0.0225,-0.004,-0.0816,0.0344,0.0214,0.2279,0.0815,-0.0191,-0.0046,0.0168,0.0242,-0.0143,-0.1009,0.0172,0.0225,0.056,-0.0015,-0.0468,-0.0046,-0.0303,0.047,-0.0299,-0.0581,-0.058,-0.0183,0.0115,-0.0037,-0.0434,0.0509,0.0387,-0.058,0.028,-0.0027,-0.0166,-0.0467,-0.0829,0.0172,-0.0091,-0.0015,0.0222,-0.0322,0.0042,-0.038,0.0381,0.0288,-0.0421,-0.0222,0.039,-0.061,-0.0095,0.1381,0.0191,0.0286,0.0492,-0.0178,0.0712,-0.0059,-0.0263,-0.0006,0.085,0.0386,0.0484,0.0127,0.0338,0.0456,0.0642,-0.0199,0.0216,-0.0236,-0.0008,0.0271,-0.0289,0.0139,-0.0329,0.0045,-0.2758,0.0426,0.0462,0.0487,-0.0377,0.0077,0.0847,0.0272,-0.0191,-0.012,-0.0834,0.0322,0.0359,-0.0082,0.0275,0.0281,0.0416,-0.0825,-0.018,-0.0471,0.0503,0.0269,0.2357,-0.0488,-0.0033,0.0014,0.0101,-0.0033,-0.0088,-0.0323,0.0323,-0.0068,0.0303,-0.0514,0.0477,0.0861,-0.0497,-0.0042,0.0307,0.0111,0.0073,-0.0041,-0.0404,-0.0116,0.0961,-0.0401,-0.0046,-0.0387,0.0183,0.0043,-0.0213,0.0325,-0.0299,-0.0121,0.0309,0.0398,-0.0275,-0.0324,-0.0385,-0.0223,0.0212,-0.045,-0.045,-0.0196,0.0444]}
{"key":"[Interactive Knowledge Distillation] Knowledge distillation is a standard teacher-student learning framework to train a light-weight student network under the guidance of a well-trained large teacher network. As an effective teaching strategy, interactive teaching has been widely employed at school to motivate students, in which teachers not only provide knowledge but also give constructive feedback to students upon their responses, to improve their learning performance. In this work, we propose an InterActive Knowledge Distillation (IAKD) scheme to leverage the interactive teaching strategy for efficient knowledge distillation. In the distillation process, the interaction between teacher and student networks is implemented by a swapping-in operation: randomly replacing the blocks in the student network with the corresponding blocks in the teacher network. In the way, we directly involve the teacher's powerful feature transformation ability to largely boost the student's performance. Experiments with typical settings of teacher-student networks demonstrate that the student networks trained by our IAKD achieve better performance than those trained by conventional knowledge distillation methods on diverse image classification datasets.","layer":5,"vector":[-0.0189,-0.0256,0.0488,-0.0003,0.0299,0.0179,0.0368,-0.0088,-0.0282,0.0018,0.015,-0.0481,0.0218,0.0588,0.0048,0.0075,0.0017,0.0265,-0.0375,0.0322,0.0231,-0.03,-0.0158,-0.0582,0.0219,-0.0026,-0.0076,-0.0825,-0.0532,-0.249,0.0476,-0.0295,0.0191,0.0244,-0.0331,-0.0369,-0.0271,0.0352,-0.0229,0.01,0.0597,0.0001,-0.0229,-0.0389,-0.0237,-0.0598,-0.0327,-0.0496,-0.0138,-0.0581,0.0271,-0.0723,0.0029,0.048,0.0098,0.0852,0.0305,0.0661,0.0744,0.0784,0.0187,0.0587,-0.1477,0.0775,0.0213,0.0094,-0.0616,0.0157,-0.003,0.0379,0.019,0.04,0.0483,0.0666,0.0275,-0.0087,-0.0512,-0.0115,-0.018,-0.0267,0.0179,-0.0458,-0.0212,-0.0448,0.038,-0.0352,0.0332,-0.0723,0.0589,0.008,-0.0517,-0.0134,-0.0323,0.0376,-0.0282,-0.0358,0.0185,0.0127,-0.0119,0.1803,-0.0575,-0.0139,0.0439,-0.0492,0.0131,-0.0221,-0.0238,0.003,-0.0614,0.0027,-0.0103,-0.0187,0.0319,-0.0356,0.0343,0.0152,0.0606,0.048,-0.016,-0.0105,-0.0051,0.0179,0.0406,-0.0164,0.0201,-0.064,-0.0153,0.115,-0.0008,0.0431,0.0436,-0.0107,-0.0668,0.005,-0.0094,0.005,0.0518,-0.0226,0.0345,-0.0284,-0.0197,-0.0157,0.0371,-0.1116,-0.0236,0.102,-0.0295,0.0433,0.002,-0.0075,-0.0212,0.01,-0.0814,-0.0235,0.0181,0.0061,0.0719,0.0493,-0.0466,-0.0386,-0.0337,-0.0726,-0.0328,0.0602,0.0733,-0.0569,-0.0264,0.0066,-0.0189,-0.0314,0.0379,0.0554,-0.0231,0.0231,0.1109,0.0101,-0.1087,-0.0333,0.0334,-0.0244,0.063,-0.0379,-0.0141,0.0445,0.0345,0.0064,0.0184,-0.0667,0.0347,0.0475,-0.0407,0.0395,-0.0375,-0.0266,-0.0582,0.0176,0.0037,-0.0207,0.0095,-0.0644,-0.0045,0.0162,-0.0287,0.0009,-0.0256,0.0217,0.0061,0.0239,0.0544,0.0123,-0.017,-0.0068,0.0274,-0.0388,-0.0243,0.0034,0.0557,0.0592,0.0149,0.0366,0.0828,-0.0518,-0.0293,-0.2642,-0.02,0.0419,0.0241,0.087,-0.0494,0.0369,-0.0078,0.0361,0.0447,0.0653,0.0197,0.0136,0.0062,-0.0036,0.0113,0.0372,0.0326,-0.0152,-0.0594,-0.0123,0.0174,0.0333,-0.1191,0.0779,0.0365,0.209,0.0626,0.0546,0.0118,0.0186,0.0632,-0.0313,-0.1198,0.0537,0.0059,0.0696,-0.0125,-0.0443,-0.0482,-0.0085,0.0196,-0.0205,-0.1091,-0.054,-0.0206,-0.043,-0.0007,-0.065,0.045,0.0353,-0.0038,0.0087,-0.0015,-0.035,-0.0298,-0.0751,0.0709,-0.0419,0.0065,0.0169,-0.0357,-0.038,-0.0698,0.0417,-0.031,-0.0567,-0.0087,0.0412,-0.0177,-0.0413,0.0855,0.0289,0.0219,0.0576,0.0525,0.0353,-0.021,-0.0548,-0.0271,0.061,-0.0644,-0.0085,-0.0082,0.048,-0.0017,0.0651,0.0019,0.0194,-0.0142,0.025,0.039,-0.0188,-0.0052,0.0058,-0.0242,-0.2807,0.049,-0.0279,0.0501,-0.0098,0.0373,0.0861,-0.0051,-0.0266,-0.0372,-0.0136,-0.0133,0.0409,-0.0212,0.0126,0.0729,0.0393,-0.0691,0.0592,-0.0474,-0.014,0.053,0.2141,-0.008,0.0263,-0.0214,-0.0299,-0.0317,0.0441,-0.0306,0.0407,0.0003,0.0824,-0.0307,0.0353,0.0601,-0.0347,0.0323,0.0234,-0.0176,-0.024,-0.0275,-0.051,-0.0283,0.0686,-0.0004,0.0018,-0.0848,0.0049,-0.0321,0.0082,0.0062,-0.0083,0.0382,0.0085,0.0461,-0.0538,-0.0328,-0.0545,-0.0297,0.0241,-0.0448,-0.0005,-0.0033,-0.0308]}
{"key":"[Finding Strength in Weakness: Learning to Separate Sounds with Weak Supervision] While there has been much recent progress using deep learning techniques to separate speech and music audio signals, these systems typically require large collections of isolated sources during the training process. When extending audio source separation algorithms to more general domains such as environmental monitoring, it may not be possible to obtain isolated signals for training. Here, we propose objective functions and network architectures that enable training a source separation system with weak labels. In this scenario, weak labels are defined in contrast with strong time-frequency (TF) labels such as those obtained from isolated sources, and refer either to frame-level weak labels where one only has access to the time periods when different sources are active in an audio mixture, or to clip-level weak labels that only indicate the presence or absence of sounds in an entire audio clip. We train a separator that estimates a TF mask for each type of sound event, using a sound event classifier as an assessor of the separator's performance to bridge the gap between the TF-level separation and the ground truth weak labels only available at the frame or clip level. Our objective function requires the classifier applied to a separated source to assign high probability to the class corresponding to that source and low probability to all other classes. The objective function also enforces that the separated sources sum up to the mixture. We benchmark the performance of our algorithm using synthetic mixtures of overlapping events created from a database of sounds recorded in urban environments. Compared to training a network using isolated sources, our model achieves somewhat lower but still significant SI-SDR improvement, even in scenarios with significant sound event overlap.","layer":0,"vector":[0.0118,-0.0117,0.0062,-0.0162,0.0253,0.0439,0.006,-0.0253,0.046,-0.0183,-0.0117,-0.0233,0.0619,0.0525,0.0756,-0.0094,0.0476,0.0762,-0.0286,-0.0169,0.0018,-0.0019,-0.0256,-0.0033,0.0268,0.0322,-0.0402,-0.0669,-0.0554,-0.2018,0.0293,-0.0651,0.0845,-0.0451,0.0423,-0.0379,-0.0243,0.0568,-0.0486,0.0466,0.0428,-0.0078,0.012,-0.081,-0.0354,-0.0462,-0.0454,-0.0586,-0.0144,-0.0496,0.0207,-0.0247,0.0075,0.0014,-0.0209,0.0349,0.0669,0.0685,0.0521,0.0341,0.0041,0.0158,-0.162,0.0284,0.0614,0.0421,-0.0217,0.0089,0.0054,0.0492,-0.0421,0.0743,0.0447,0.0157,0.0058,0.0,0.0379,-0.0261,-0.0215,-0.009,0.0369,-0.0408,-0.0233,0.0002,-0.0179,-0.0009,0.0135,-0.0345,-0.0195,0.0085,-0.0793,0.0161,-0.0022,0.0546,-0.0251,-0.0403,0.0409,0.0362,-0.0204,0.222,-0.0356,0.0476,0.0185,-0.0371,0.0376,-0.0582,-0.0348,-0.0473,-0.0226,0.0058,0.0125,-0.0331,0.0067,-0.0264,0.0423,0.0166,0.0884,0.0164,-0.0293,0.0082,0.0011,-0.0441,0.0703,-0.0058,0.0617,-0.0554,0.0265,0.1314,0.0358,0.0309,0.0576,-0.0399,-0.0644,-0.0279,0.0322,-0.0082,0.0025,0.0171,0.0398,0.0125,-0.0267,-0.0441,-0.0005,-0.0655,-0.0458,0.1346,-0.0627,0.0097,-0.0603,-0.0514,-0.0186,-0.0107,-0.0068,-0.041,0.0824,0.0172,0.0337,0.033,-0.0579,0.0523,0.0237,-0.0596,0.0206,0.1073,-0.0248,-0.0837,-0.0646,0.0194,0.0032,-0.0353,0.005,0.0058,0.0005,0.0161,0.0599,0.0036,-0.0948,0.0149,0.0082,-0.0085,-0.0147,-0.0769,-0.074,0.0441,0.0132,-0.0556,0.0024,-0.0893,0.0001,0.0857,-0.0344,0.0001,0.0102,0.0244,-0.0252,-0.0048,-0.0092,-0.0061,-0.0169,-0.0032,-0.0128,-0.0389,-0.0329,0.0261,-0.0022,0.0378,-0.0144,-0.0179,0.0219,0.0309,0.0153,-0.0082,0.0849,-0.0329,0.0028,-0.039,0.0193,0.0297,-0.0011,-0.0009,-0.007,-0.0719,-0.0728,-0.2573,0.0225,0.0283,0.0041,0.0601,-0.0566,0.0192,-0.0129,0.0591,0.0524,0.0676,-0.0141,-0.0044,0.0368,-0.0304,0.0622,0.0449,0.0162,-0.011,0.0274,0.0065,0.0052,-0.0296,-0.0721,0.0787,-0.0013,0.2158,0.0107,0.0678,-0.0213,-0.0071,0.0096,-0.0351,-0.0933,0.0197,0.0043,0.0582,-0.0206,-0.0396,-0.0092,-0.0632,0.0194,0.0018,-0.1043,-0.0362,-0.0338,-0.0138,-0.0075,-0.0546,-0.0069,0.0929,-0.0132,0.0174,0.0035,0.0055,-0.0148,-0.089,0.0351,-0.0454,0.0099,-0.0098,-0.0118,0.053,-0.0809,0.0378,0.0347,-0.0318,-0.0365,0.0146,0.0294,-0.0162,0.0872,-0.016,-0.0109,0.0532,-0.0314,-0.0032,-0.0948,-0.0647,-0.0339,0.0815,-0.002,0.0388,0.0195,0.0142,0.0305,0.1021,0.01,0.0329,-0.0401,0.0385,0.0358,-0.0407,-0.0001,0.0437,-0.0152,-0.2881,0.0325,0.0453,0.0303,-0.0227,0.0123,0.0296,0.0218,-0.0679,-0.0132,-0.0021,0.0093,-0.0107,-0.0418,0.0199,0.0662,0.0895,-0.001,0.0356,-0.022,-0.0159,0.0825,0.168,-0.0009,0.0109,-0.0054,-0.0424,-0.0049,0.0424,-0.0785,0.0146,0.019,0.1202,-0.0777,-0.0127,0.0652,-0.0083,0.0386,0.0098,-0.0238,0.0034,-0.0412,-0.0218,-0.0169,0.0693,-0.0679,-0.03,-0.052,-0.021,0.0571,-0.026,0.0118,0.0431,0.0091,-0.0038,0.0329,-0.0285,-0.023,-0.0022,-0.0306,0.0278,-0.0672,-0.008,0.0175,-0.0301]}
{"key":"[An Analysis of Constant Step Size SGD in the Non-convex Regime: Asymptotic Normality and Bias] Structured non-convex learning problems, for which critical points have favorable statistical properties, arise frequently in statistical machine learning. Algorithmic convergence and statistical estimation rates are well-understood for such problems. However, quantifying the uncertainty associated with the underlying training algorithm is not well-studied in the non-convex setting. In order to address this shortcoming, in this work, we establish an asymptotic normality result for the constant step size stochastic gradient descent (SGD) algorithm--a widely used algorithm in practice. Specifically, based on the relationship between SGD and Markov Chains [DDB19], we show that the average of SGD iterates is asymptotically normally distributed around the expected value of their unique invariant distribution, as long as the non-convex and non-smooth objective function satisfies a dissipativity property. We also characterize the bias between this expected value and the critical points of the objective function under various local regularity conditions. Together, the above two results could be leveraged to construct confidence intervals for non-convex problems that are trained using the SGD algorithm.","layer":3,"vector":[-0.0509,-0.0053,0.0172,-0.032,0.027,0.0659,0.013,0.012,0.0575,0.0072,0.034,-0.0025,0.0509,0.0829,-0.0164,0.0449,0.0019,0.023,-0.036,0.04,0.0171,-0.0145,0.0369,-0.0524,0.0145,0.0028,-0.0171,-0.0619,-0.0239,-0.2786,-0.0091,-0.0554,0.0366,-0.0376,0.0295,-0.025,-0.0321,0.0606,-0.0162,0.036,0.0014,0.0383,-0.067,-0.0774,0.0312,-0.0256,-0.0307,0.0016,-0.0607,-0.0231,-0.011,-0.0279,0.0378,0.0161,-0.0072,0.0433,0.0328,0.0324,0.0712,0.0551,-0.0243,0.02,-0.1852,0.003,0.0595,-0.0138,-0.0421,-0.0352,-0.0117,0.0671,0.0058,0.0081,0.0366,0.0727,0.0066,-0.0119,0.0275,0.0139,-0.0113,0.0282,0.0201,-0.0259,-0.0541,-0.0053,-0.0338,-0.048,0.0518,-0.015,0.0445,0.0019,-0.0214,-0.0111,-0.0503,0.0182,-0.0515,-0.0138,0.032,0.0247,-0.0305,0.1745,-0.0654,0.0849,-0.0008,0.0191,0.0378,0.0142,-0.0213,-0.0341,0.0217,-0.0292,-0.0458,-0.0416,0.0529,-0.0443,0.0037,0.0229,0.0633,0.0075,-0.0245,0.0033,-0.0522,0.0175,0.06,-0.0167,-0.006,-0.0386,-0.0248,0.1335,0.0301,0.0349,0.0596,-0.0693,-0.0645,-0.0267,0.0203,0.0182,-0.0104,-0.0115,0.024,-0.0026,-0.0314,-0.0457,0.0296,-0.1147,-0.0207,0.1491,-0.0525,0.0219,-0.0669,-0.0453,0.0207,0.0219,0.0028,-0.0388,0.0143,0.0097,0.0005,0.0172,-0.0817,-0.0209,-0.0491,-0.0647,-0.0398,0.1251,-0.0082,-0.0671,-0.036,-0.0255,0.0231,0.0017,0.0587,0.08,-0.0373,0.0412,0.0575,0.021,-0.093,-0.0036,0.0094,0.0028,0.0078,-0.0288,-0.0176,0.0286,0.0588,-0.036,0.0359,-0.0294,0.0134,0.0455,-0.0376,-0.019,-0.0297,-0.0442,-0.0593,-0.0291,-0.0042,-0.0049,0.049,0.0167,-0.0034,-0.0144,-0.0226,0.0574,0.0151,0.0258,-0.0483,0.0133,0.0044,0.0342,-0.0373,-0.0335,0.0258,-0.0079,-0.0353,0.0287,0.0403,0.0445,-0.0069,0.0424,0.0564,-0.0069,-0.0758,-0.2015,-0.0092,0.0372,0.0104,0.0665,-0.0639,0.01,-0.0014,0.0703,0.0425,0.0787,-0.023,-0.0303,-0.0132,0.015,0.0672,0.0562,0.0216,-0.037,0.0321,0.0117,0.0297,-0.0366,-0.0915,0.0668,-0.0183,0.1627,-0.0089,0.0652,-0.0502,0.0348,0.0338,-0.0135,-0.0771,0.0538,-0.0192,0.0778,-0.0456,-0.0444,-0.011,-0.0271,0.0257,0.0309,-0.0611,-0.0581,-0.0288,-0.0443,0.0442,-0.0421,0.0029,0.0089,-0.0169,0.0941,-0.0322,0.018,-0.0092,-0.0741,0.02,-0.0262,0.0058,0.0037,-0.0412,0.0337,-0.0838,0.0377,-0.0158,-0.0281,-0.0079,0.0404,-0.0304,-0.0172,0.0873,0.0148,-0.0038,0.0604,-0.0103,0.0557,0.0064,-0.0477,-0.0474,0.0555,-0.0465,0.0308,0.0293,0.0305,-0.0119,0.0592,-0.0258,0.027,-0.0085,0.0377,-0.0183,-0.0289,0.021,0.024,0.0002,-0.283,0.0211,-0.0083,-0.0089,-0.0137,-0.0066,0.0837,0.0083,-0.0559,0.0133,-0.015,0.074,0.0457,-0.0113,0.0202,0.0091,0.0531,-0.0415,0.0813,-0.1015,0.0011,0.0483,0.2347,-0.0453,0.0022,-0.0076,-0.0372,0.0238,0.0193,-0.0419,0.0178,-0.0344,0.0615,-0.0306,0.0823,0.0734,-0.0367,0.0198,0.0008,0.0029,0.0091,-0.0013,-0.0028,0.0089,0.1398,-0.0165,-0.013,-0.0461,0.0066,0.0218,-0.0525,0.0007,0.0246,-0.0058,0.0377,0.0247,-0.0697,-0.0669,-0.0446,-0.041,0.0144,-0.0708,-0.0721,-0.0186,-0.0071]}
{"key":"[Learning Utilities and Equilibria in Non-Truthful Auctions] In non-truthful auctions, agents' utility for a strategy depends on the strategies of the opponents and also the prior distribution over their private types; the set of Bayes Nash equilibria generally has an intricate dependence on the prior. Using the First Price Auction as our main demonstrating example, we show that $\\tilde O(n / \\epsilon^2)$ samples from the prior with $n$ agents suffice for an algorithm to learn the interim utilities for all monotone bidding strategies. As a consequence, this number of samples suffice for learning all approximate equilibria. We give almost matching (up to polylog factors) lower bound on the sample complexity for learning utilities. We also consider a setting where agents must pay a search cost to discover their own types. Drawing on a connection between this setting and the first price auction, discovered recently by Kleinberg et al. (2016), we show that $\\tilde O(n / \\epsilon^2)$ samples suffice for utilities and equilibria to be estimated in a near welfare-optimal descending auction in this setting. En route, we improve the sample complexity bound, recently obtained by Guo et al. (2020), for the Pandora's Box problem, which is a classical model for sequential consumer search.","layer":3,"vector":[-0.0583,0.0029,0.0066,-0.001,0.003,0.0217,0.0491,0.0275,0.0535,0.0325,0.0218,-0.0188,0.0043,0.056,0.0023,0.0385,-0.0161,0.0352,-0.0569,-0.0127,0.0466,-0.0597,-0.0213,-0.0542,0.0305,-0.024,-0.0042,-0.0406,-0.0234,-0.2299,0.0216,-0.0366,0.0266,-0.0348,0.0233,-0.0373,-0.0541,0.0263,-0.034,0.0342,0.0214,0.0303,-0.0426,-0.0556,-0.0439,-0.0523,0.0036,-0.001,-0.0031,-0.078,0.0001,-0.014,0.0151,0.0102,0.0509,0.0308,0.0364,0.0547,0.0481,0.0712,0.0146,0.0196,-0.1456,0.0697,0.0664,0.033,-0.0326,0.0006,-0.0132,0.0568,0.059,0.0675,0.0109,0.0222,0.0263,0.0001,-0.0068,-0.0326,-0.0337,-0.0162,-0.0612,-0.028,-0.0248,-0.0089,-0.0471,-0.0811,0.0166,-0.0267,0.0443,0.0146,0.0083,0.0119,-0.0577,0.0122,-0.0303,-0.0147,0.0342,0.0,-0.0405,0.1966,0.0106,0.083,-0.01,-0.0413,0.0393,-0.0336,-0.0451,-0.0475,-0.0481,-0.0053,-0.0453,-0.0022,0.0863,-0.0509,-0.0104,0.0369,0.0252,0.0156,0.0237,-0.0209,-0.0127,0.0073,0.0662,-0.0001,-0.0174,-0.0624,-0.0099,0.1086,0.019,0.039,0.0047,-0.0943,-0.0328,-0.0213,0.0451,0.0593,-0.0004,0.0429,-0.0069,0.0172,-0.0634,-0.0507,0.0188,-0.1473,-0.0554,0.0954,0.0389,0.0211,-0.0673,-0.0409,0.0137,0.0039,-0.0087,-0.0453,0.0465,0.0144,0.0832,0.1072,-0.0952,0.0003,-0.037,-0.031,0.0008,0.1143,-0.0227,-0.0809,-0.0242,-0.0319,-0.0136,-0.0028,0.0416,0.07,-0.0593,0.0266,0.0866,-0.0125,-0.0939,0.0168,0.0055,0.0054,-0.0057,-0.0229,-0.0704,0.0273,0.0026,-0.016,-0.0138,-0.0361,0.0208,0.0468,-0.0138,0.0118,-0.0436,0.0368,-0.0346,-0.0337,-0.0215,-0.0068,0.0234,-0.0139,-0.0229,-0.0026,-0.0685,0.0525,0.0288,0.0437,0.0039,-0.0076,0.0633,-0.0069,-0.0352,0.0024,0.0067,0.0281,-0.0289,0.0208,0.05,0.0232,-0.0004,0.0714,-0.0264,-0.0444,-0.0113,-0.1848,-0.0047,-0.0226,-0.0145,0.0092,-0.0205,0.049,-0.0159,0.0322,0.0777,0.0422,-0.0238,-0.0461,0.0553,-0.0005,0.0299,0.0471,0.0084,0.0186,0.0322,0.0103,0.0293,0.0134,-0.0833,0.0835,0.0136,0.2474,0.0228,-0.0004,-0.0683,0.0329,-0.004,-0.0792,-0.0542,0.0074,0.029,0.0379,-0.0077,0.0105,-0.0134,-0.0161,0.0004,-0.0266,-0.0702,-0.055,0.033,-0.0657,0.0299,-0.0777,0.0395,0.0639,0.0081,0.0387,-0.0264,0.0117,-0.0608,-0.078,-0.0007,-0.0483,0.0681,0.0387,-0.03,-0.0064,-0.0378,0.0746,-0.0101,0.0097,-0.0539,0.0106,-0.031,-0.0185,0.043,-0.0201,0.0174,-0.0066,0.0199,0.0416,-0.0523,-0.0235,-0.0076,0.0252,-0.0637,-0.0053,0.0161,0.0157,0.0115,0.0662,-0.0282,0.0234,-0.0144,-0.011,0.0326,-0.0822,0.0051,0.0146,-0.0135,-0.2976,0.0619,0.03,-0.0078,-0.0312,0.0608,0.043,0.0196,-0.0476,-0.0311,0.0538,0.1041,0.0052,-0.0079,-0.0017,-0.0039,0.0995,-0.0574,0.0789,-0.0413,0.012,0.0151,0.2262,-0.0395,-0.0238,0.0044,-0.0195,0.0406,-0.0123,-0.0312,0.0653,-0.0114,0.0814,-0.0743,0.0673,0.0607,0.0083,0.0272,0.0,-0.0084,-0.0837,-0.001,-0.0387,-0.0203,0.1068,0.0181,0.0096,-0.0601,-0.0182,0.0381,-0.0221,0.0463,-0.0084,-0.0172,0.0284,0.0176,-0.0823,-0.0192,-0.0106,-0.0425,0.0413,0.0076,-0.016,-0.0104,0.0185]}
{"key":"[Deep Learning for IoT] Deep learning and other machine learning approaches are deployed to many systems related to Internet of Things or IoT. However, it faces challenges that adversaries can take loopholes to hack these systems through tampering history data. This paper first presents overall points of adversarial machine learning. Then, we illustrate traditional methods, such as Petri Net cannot solve this new question efficiently. To help IoT data analysis more efficient, we propose a retrieval method based on deep learning (recurrent neural network). Besides, this paper presents a research on data retrieval solution to avoid hacking by adversaries in the fields of adversary machine leaning. It further directs the new approaches in terms of how to implementing this framework in IoT settings based on adversarial deep learning.","layer":4,"vector":[-0.047,-0.0148,0.0198,-0.0432,0.0282,0.0335,0.072,0.0111,0.0076,-0.0155,0.0093,-0.0169,0.0308,0.0354,0.0227,-0.0128,-0.0007,0.0531,-0.0529,0.0014,0.0608,-0.0376,-0.0305,-0.0524,0.0097,-0.0154,-0.0179,-0.028,-0.0776,-0.1998,-0.0046,-0.0654,0.0232,-0.004,0.0311,-0.0273,0.0028,0.0431,0.0046,0.0725,0.0268,-0.0085,-0.0102,-0.0232,-0.008,-0.0124,0.003,-0.0202,0.0071,-0.0527,0.0476,-0.0179,0.0101,0.0276,0.0531,0.0201,0.0669,0.0639,0.0427,0.0119,0.043,0.059,-0.1474,0.0654,0.0762,0.0506,-0.0516,-0.0407,0.0102,-0.0146,0.0214,0.0139,0.0087,0.0191,-0.0469,0.056,0.0042,-0.029,-0.0408,-0.0088,0.0183,0.0359,-0.0215,0.0008,-0.0528,-0.059,-0.014,-0.0063,0.061,-0.0338,-0.062,0.0134,0.0023,0.0312,-0.0298,-0.0344,0.0145,0.0329,-0.0802,0.2187,-0.0934,0.048,0.0122,-0.0112,0.0149,0.0046,-0.0038,-0.0662,-0.0327,0.0313,-0.037,0.0134,0.061,-0.0231,0.0766,0.0021,0.0223,0.0463,-0.007,0.0231,-0.0446,0.0171,0.0483,-0.0112,0.0227,-0.0696,0.0398,0.1207,0.0103,0.0255,-0.0125,-0.0367,-0.0499,-0.0014,0.0417,0.0796,-0.0151,-0.0435,0.0252,-0.0206,-0.0487,-0.0529,0.0597,-0.0681,-0.033,0.0984,-0.0023,0.0391,-0.009,-0.064,-0.0351,0.0232,0.0107,-0.0018,0.0332,0.0809,0.0088,0.0673,-0.0254,-0.0183,-0.0236,0.0052,-0.0374,0.1112,0.0115,-0.1166,-0.0145,-0.017,-0.0118,-0.0231,0.0232,0.0226,-0.0311,0.0255,0.0674,0.0338,-0.0698,-0.0246,-0.036,0.0231,-0.0601,-0.0819,-0.0159,-0.0059,0.0701,-0.0458,0.01,-0.0348,-0.0191,0.0352,-0.0744,0.0303,-0.0514,0.0248,-0.0142,-0.0412,-0.0123,0.0057,0.0154,-0.0063,-0.0171,-0.0296,-0.0249,0.0043,-0.0066,0.0252,-0.036,-0.0474,0.0074,0.0252,-0.0079,-0.0145,0.0214,-0.0744,0.0024,-0.0339,-0.0022,0.0701,0.0008,0.0453,0.0136,-0.0578,-0.0608,-0.2317,-0.0334,-0.0213,-0.0284,0.0381,-0.0944,0.0107,-0.022,0.0438,0.0087,0.0577,-0.0043,-0.0103,0.0232,-0.0158,0.0603,0.0487,0.0601,-0.0287,0.0168,-0.0077,-0.0046,0.0172,-0.0759,0.0019,0.0233,0.2325,0.0238,0.0281,-0.0506,0.0418,0.0238,-0.0298,-0.1155,0.0684,0.0012,0.0453,0.0491,-0.0457,-0.0205,-0.0547,0.0367,0.0094,-0.1086,-0.0364,-0.0604,-0.0408,0.0104,-0.0428,0.0768,-0.0007,0.0304,0.0399,0.0312,0.0124,-0.0218,-0.0786,0.0255,-0.0201,0.0342,-0.0159,-0.0286,-0.0213,-0.0776,0.09,0.0189,-0.0244,-0.0545,0.0158,0.0095,-0.0376,0.0867,0.0293,0.0278,0.0433,0.0027,0.0296,-0.0452,-0.0282,-0.0221,0.0539,0.0166,0.0407,0.0118,0.0348,0.0236,0.0721,-0.0054,0.042,-0.0258,-0.0016,0.0104,-0.0732,-0.0419,0.0291,0.0181,-0.2892,0.0467,0.0112,0.0628,-0.0608,0.0026,0.0397,0.0156,-0.0237,0.0477,-0.01,0.0473,0.0489,-0.0432,0.0037,-0.0029,0.0705,-0.0599,0.0209,-0.0283,0.0409,0.0814,0.2583,-0.0354,0.0161,0.0021,-0.0191,0.062,0.0299,-0.0453,0.0042,-0.0477,0.0852,-0.0178,0.0169,0.0375,-0.0225,0.0038,0.0356,-0.0174,-0.0065,0.0127,-0.0429,0.0189,0.0994,-0.0039,-0.0209,-0.0604,0.0076,0.0439,-0.0609,-0.0488,-0.0172,0.0211,0.0359,0.0229,-0.0784,-0.0294,-0.0203,-0.0291,0.0388,-0.049,-0.0154,-0.0277,0.0013]}
{"key":"[Population-Guided Parallel Policy Search for Reinforcement Learning] In this paper, a new population-guided parallel learning scheme is proposed to enhance the performance of off-policy reinforcement learning (RL). In the proposed scheme, multiple identical learners with their own value-functions and policies share a common experience replay buffer, and search a good policy in collaboration with the guidance of the best policy information. The key point is that the information of the best policy is fused in a soft manner by constructing an augmented loss function for policy update to enlarge the overall search region by the multiple learners. The guidance by the previous best policy and the enlarged range enable faster and better policy search. Monotone improvement of the expected cumulative return by the proposed scheme is proved theoretically. Working algorithms are constructed by applying the proposed scheme to the twin delayed deep deterministic (TD3) policy gradient algorithm. Numerical results show that the constructed algorithm outperforms most of the current state-of-the-art RL algorithms, and the gain is significant in the case of sparse reward environment.","layer":6,"vector":[-0.063,0.0201,0.021,-0.0278,-0.0049,0.067,0.0103,0.0185,0.0638,-0.0064,0.0434,-0.0311,0.0319,0.0553,0.0135,0.0011,-0.0107,0.0532,0.0124,-0.0157,0.037,-0.0671,-0.0169,-0.0477,0.0235,0.0092,-0.0406,-0.0952,-0.0393,-0.2309,0.032,-0.0171,0.0035,-0.0367,0.0029,-0.0008,-0.0402,0.0458,-0.029,0.0268,0.0194,0.0088,-0.0173,-0.0776,-0.0346,0.0001,-0.0384,-0.0523,0.024,-0.0591,0.0191,0.0073,-0.0044,0.0154,0.0457,0.0205,0.0096,0.0658,0.0023,0.0485,0.0306,-0.0139,-0.1942,0.0222,0.045,0.05,-0.0236,-0.04,0.037,0.0722,-0.0619,0.0512,0.0612,0.0142,0.0152,0.0053,-0.0043,-0.0199,0.0059,-0.0103,0.0226,-0.0473,-0.0142,-0.0335,0.0097,-0.092,0.0359,-0.0368,0.0323,0.0267,-0.0019,0.0216,-0.0062,0.0342,-0.0871,0.0131,0.0274,0.0422,-0.0787,0.1847,-0.0179,0.0623,0.0346,-0.0005,0.0305,-0.0382,-0.0378,0.0072,-0.0458,-0.0024,-0.0378,-0.0166,0.0191,0.0002,0.0005,0.023,0.0423,-0.0089,0.0141,0.0045,0.013,-0.0073,0.0715,-0.0282,0.0268,-0.0606,0.0069,0.1629,-0.0379,0.0085,0.0692,-0.0419,-0.0363,-0.0732,0.0099,0.0204,0.0159,-0.0253,0.0644,-0.0166,-0.0326,0.0067,0.0156,-0.127,-0.0264,0.1088,0.0276,0.0261,-0.0336,-0.0226,-0.0504,0.0128,0.0235,-0.0212,-0.0126,0.0359,0.059,0.1079,-0.0415,0.0123,-0.0165,-0.044,-0.0414,0.0865,-0.0043,-0.1022,-0.0242,-0.0144,0.0007,-0.026,0.0074,-0.0083,-0.0425,0.0047,0.0901,-0.0038,-0.0796,-0.0039,-0.0086,-0.0234,0.0208,-0.0596,-0.0153,0.008,0.0502,-0.0225,0.023,-0.0634,-0.0032,0.0158,-0.0207,0.0061,-0.0114,-0.0118,-0.0226,-0.0594,-0.001,-0.0541,0.066,-0.0145,0.0062,0.0074,-0.0333,0.0294,-0.0419,0.0442,0.0109,-0.0242,0.0469,0.0197,-0.0052,0.0364,0.0573,-0.0109,0.0033,0.0077,0.0345,0.016,-0.0012,0.0717,0.0433,-0.0168,-0.0471,-0.1904,-0.0181,-0.0807,-0.0157,0.0327,-0.0452,0.0461,-0.0051,0.0349,0.0447,0.0605,-0.0196,-0.0606,0.0528,-0.004,0.0777,0.033,0.0327,-0.0134,0.0208,0.0272,0.0281,-0.0198,-0.1198,0.0664,-0.0048,0.1848,0.0209,0.0088,-0.0299,0.0225,0.0444,0.0059,-0.1104,0.0176,0.0318,0.0937,0.0036,-0.0191,-0.0554,0.0125,0.0374,-0.048,-0.1043,0.01,-0.0463,-0.0312,0.0568,-0.0862,0.018,0.0524,-0.0184,0.0106,-0.0162,-0.0475,-0.0139,-0.1139,0.053,-0.0175,0.0168,-0.0017,-0.0366,0.0126,-0.0345,0.0538,0.0185,0.03,-0.0323,0.0227,-0.0089,-0.0431,0.0452,-0.0016,0.0206,-0.0,0.0265,0.0182,-0.0072,-0.0652,-0.0437,0.1074,-0.0475,0.0155,0.0059,-0.0064,-0.0136,0.0799,0.0106,0.0054,-0.0177,-0.0083,-0.0163,-0.0632,0.0014,0.0427,-0.0072,-0.2956,0.0747,-0.0064,0.029,0.009,0.0222,0.0518,-0.0158,-0.0334,0.0189,-0.0092,0.0515,0.0192,0.0298,0.026,0.0363,0.0797,-0.0294,0.0539,-0.0873,0.0507,0.0579,0.2343,-0.0383,0.0518,-0.009,-0.0595,-0.0303,0.0275,-0.0324,-0.0198,0.0245,0.0249,-0.0931,0.0403,0.1021,-0.0429,0.0262,0.0359,-0.0053,-0.0052,0.0166,-0.012,-0.0048,0.0895,0.0279,-0.0175,-0.0713,-0.0468,0.0437,-0.051,0.0146,-0.0042,-0.0346,0.0435,0.0401,-0.0071,-0.0922,-0.0476,-0.0298,0.0186,-0.034,0.0094,0.0163,-0.0184]}
{"key":"[Bias Discovery in Machine Learning Models for Mental Health] Fairness and bias are crucial concepts in artificial intelligence, yet they are relatively ignored in machine learning applications in clinical psychiatry. We computed fairness metrics and present bias mitigation strategies using a model trained on clinical mental health data. We collected structured data related to the admission, diagnosis, and treatment of patients in the psychiatry department of the University Medical Center Utrecht. We trained a machine learning model to predict future administrations of benzodiazepines on the basis of past data. We found that gender plays an unexpected role in the predictions-this constitutes bias. Using the AI Fairness 360 package, we implemented reweighing and discrimination-aware regularization as bias mitigation strategies, and we explored their implications for model performance. This is the first application of bias exploration and mitigation in a machine learning model trained on real clinical psychiatry data.","layer":1,"vector":[-0.0461,0.0001,0.0413,-0.0036,0.0412,0.0564,0.0486,0.0317,0.0519,-0.0523,0.0044,-0.0413,-0.008,0.0256,0.0148,0.0514,0.0109,0.003,-0.0703,0.0385,-0.0283,-0.023,-0.0035,-0.0205,0.011,0.0047,-0.0565,-0.0464,-0.0583,-0.2071,0.0356,-0.0706,0.056,-0.0578,0.0116,-0.0309,-0.0389,0.0735,-0.0507,0.0683,0.0314,-0.0106,0.0012,-0.0508,-0.0244,-0.0374,-0.0223,0.0187,-0.0302,-0.0286,0.0288,-0.0435,0.0183,0.0376,0.012,0.0389,0.0521,0.0188,0.0398,0.0739,0.0084,0.0295,-0.1637,0.066,0.0431,0.0158,-0.0413,-0.0741,0.0099,0.0506,-0.0066,0.0186,0.0501,0.0404,-0.0138,-0.0031,0.0355,-0.03,0.0322,0.0048,0.0109,-0.029,-0.0133,-0.0153,0.0105,-0.0544,0.0403,-0.0571,0.0328,0.0231,-0.0052,-0.0093,0.0165,0.0294,-0.032,0.0143,0.0378,0.0132,-0.0558,0.2032,-0.0439,0.0041,-0.0027,-0.0044,0.0561,-0.0098,-0.0051,-0.0616,-0.032,-0.0355,0.0199,-0.0095,0.0718,-0.0416,-0.0071,0.0236,0.0591,0.0334,0.011,0.0038,-0.0563,0.0135,0.0413,-0.0064,0.0228,-0.0301,0.0027,0.1525,0.009,-0.0242,0.0715,-0.0681,-0.0547,0.0113,0.0332,0.0296,0.0232,-0.0049,0.0317,0.0163,-0.0516,-0.038,-0.0254,-0.0878,-0.0817,0.1517,-0.0547,0.011,-0.0149,-0.0223,-0.0164,0.0066,-0.068,-0.0356,0.0125,0.0359,0.0205,0.0725,-0.0566,0.0831,0.017,-0.0707,-0.06,0.115,-0.0193,-0.0333,0.0021,0.0133,0.02,0.0074,0.0541,0.0111,-0.0257,0.0411,0.0474,-0.0081,-0.0318,0.0046,-0.034,-0.0309,0.0124,-0.0357,-0.0328,0.0496,0.0452,-0.0525,0.0334,-0.0417,0.0518,0.0545,-0.0453,0.0213,-0.0156,-0.0079,-0.0302,-0.0511,-0.0491,-0.0133,0.0,0.0268,0.0035,0.0136,-0.0121,0.0198,0.0182,0.0312,-0.0119,-0.027,0.0742,0.0088,-0.0261,-0.0039,0.0743,-0.0274,-0.0446,0.0099,0.0171,0.0102,0.044,0.0473,0.0258,0.0216,-0.0629,-0.2072,-0.0194,0.0273,0.0178,0.0284,-0.0618,0.0298,-0.0427,0.0231,0.1117,0.0464,-0.0071,-0.033,0.0228,-0.0005,0.0329,0.0108,0.0541,-0.0622,-0.0026,-0.0279,0.008,0.0121,-0.0921,0.0403,0.0087,0.2365,-0.0026,-0.0069,-0.0338,0.0138,-0.0158,-0.0473,-0.114,0.0688,0.0145,0.0055,-0.0452,-0.0603,-0.0047,-0.0325,0.0613,0.0002,-0.1124,-0.0787,-0.0017,-0.0131,0.0332,-0.0431,0.0268,-0.0323,-0.0397,0.0328,-0.0115,0.0204,-0.0278,-0.1303,0.039,-0.063,0.0148,0.0632,-0.0938,0.0223,-0.0646,0.0316,0.0047,-0.0234,-0.0495,0.0479,-0.0251,0.0063,0.0956,-0.0145,-0.0341,0.0512,0.0043,0.005,-0.0312,-0.0417,0.0208,0.0464,-0.0527,0.0232,0.022,0.0386,-0.0258,0.0489,-0.0109,0.0344,-0.0304,-0.0475,-0.0042,-0.0804,-0.0183,0.0167,0.0189,-0.2881,0.0258,-0.0206,0.0417,-0.0092,0.0149,0.0378,0.0238,-0.029,-0.0293,0.0044,0.0695,0.028,-0.0137,-0.0265,0.0193,0.1034,-0.0642,0.0783,-0.0212,0.0084,0.0429,0.1961,-0.0423,0.0499,0.0362,-0.0143,-0.0389,0.0093,0.0045,-0.0246,0.0259,0.0957,-0.0502,0.0608,0.0701,-0.0306,-0.0203,0.0616,-0.0054,-0.0247,0.0299,-0.0168,0.0155,0.107,-0.0132,-0.017,-0.0377,0.0156,0.047,-0.0339,0.0415,-0.0417,0.0154,0.0617,0.0362,-0.0527,-0.0216,-0.0308,-0.0577,-0.0182,-0.0554,-0.0188,0.0008,-0.0335]}
{"key":"[An Outer-approximation Guided Optimization Approach for Constrained Neural Network Inverse Problems] This paper discusses an outer-approximation guided optimization method for constrained neural network inverse problems with rectified linear units. The constrained neural network inverse problems refer to an optimization problem to find the best set of input values of a given trained neural network in order to produce a predefined desired output in presence of constraints on input values. This paper analyzes the characteristics of optimal solutions of neural network inverse problems with rectified activation units and proposes an outer-approximation algorithm by exploiting their characteristics. The proposed outer-approximation guided optimization comprises primal and dual phases. The primal phase incorporates neighbor curvatures with neighbor outer-approximations to expedite the process. The dual phase identifies and utilizes the structure of local convex regions to improve the convergence to a local optimal solution. At last, computation experiments demonstrate the superiority of the proposed algorithm compared to a projected gradient method.","layer":0,"vector":[-0.0184,-0.025,0.0541,-0.0307,0.0188,0.0486,-0.0004,0.0627,0.0157,-0.0127,0.0206,-0.1041,0.0604,0.0303,-0.0006,-0.0045,0.042,0.0794,-0.0111,0.0245,0.0256,-0.047,-0.0259,-0.0611,0.0207,-0.0306,-0.0306,-0.0182,-0.0495,-0.2317,-0.0075,-0.0489,0.0762,-0.049,0.0099,0.0235,-0.0316,0.0454,-0.0459,0.0519,0.0165,0.0433,0.0193,-0.079,-0.0123,-0.0386,-0.0238,0.0084,0.02,-0.0226,0.0486,0.009,-0.0006,0.0208,0.026,0.0246,0.0266,0.0375,-0.0028,0.0559,-0.0131,0.0411,-0.1634,0.0038,0.0633,0.0351,0.0119,-0.0507,0.0139,0.066,-0.0336,0.0623,0.0217,0.024,-0.0112,0.0477,0.0142,-0.0244,-0.0,0.0029,0.0297,0.0037,-0.0078,-0.0067,-0.0169,-0.0212,0.0279,-0.041,0.0295,-0.0037,-0.0221,-0.0707,-0.048,-0.0291,-0.0868,-0.0374,0.0228,0.058,-0.071,0.213,-0.0278,0.0625,0.0814,-0.0365,0.039,-0.012,-0.0402,-0.0021,-0.0245,0.0221,-0.0309,-0.0103,-0.0046,0.0179,0.0115,0.0528,0.0127,0.0295,-0.0124,-0.0363,-0.0223,-0.0221,0.0202,-0.0241,0.0225,-0.0773,0.0253,0.0947,0.0477,0.0626,0.0268,-0.0393,-0.0562,-0.0536,0.0296,0.0063,0.0384,-0.0199,0.0008,0.0449,-0.0432,-0.0948,0.0533,-0.0745,-0.0883,0.1024,-0.0532,0.0045,-0.0304,-0.0681,-0.0325,0.0113,-0.0529,-0.0002,0.0358,0.0363,-0.0126,0.0218,-0.0684,0.0495,-0.0113,-0.0631,-0.08,0.1233,-0.009,-0.0834,-0.0371,-0.0081,0.023,-0.0127,0.0306,0.045,-0.0473,0.0417,0.1069,0.0635,-0.0665,-0.0132,-0.0365,0.0292,0.0098,-0.0474,-0.0364,0.0257,0.0389,-0.04,0.0229,-0.0521,-0.0253,0.0487,-0.0843,-0.001,-0.0623,0.0049,-0.047,-0.0564,0.0175,-0.0143,0.031,-0.0183,0.0283,-0.0346,-0.0409,0.0614,0.0119,0.0098,0.0409,-0.0457,0.0136,0.0571,-0.0375,-0.0051,0.0872,-0.0677,-0.0543,0.0161,0.0101,0.005,0.0091,0.0868,0.0378,-0.0365,-0.0609,-0.2282,-0.0002,0.0072,0.0105,0.0457,-0.0667,0.037,-0.0281,0.0488,0.0267,0.0325,0.0134,-0.0107,0.0362,-0.0361,0.0305,0.0605,-0.0006,-0.0244,-0.0102,0.0122,0.0328,0.0149,-0.0594,0.0476,-0.0364,0.2162,-0.0003,0.0809,-0.0166,0.0166,0.0227,-0.0085,-0.0808,0.0575,0.0275,0.0784,-0.0122,-0.0232,-0.0241,-0.0127,0.0203,0.0145,-0.0465,0.001,-0.0117,-0.0339,0.0302,-0.0604,0.0247,0.0103,-0.0137,0.0226,-0.0295,-0.005,-0.0401,-0.0734,0.021,-0.0068,0.0021,0.0166,-0.0358,-0.0276,-0.056,0.0606,0.0474,-0.0064,-0.0222,0.0347,0.007,-0.0179,0.093,0.042,0.0377,0.0242,0.0039,0.0607,0.0049,-0.0489,-0.0171,0.0528,-0.0661,0.0486,-0.0053,0.0291,0.0313,0.0914,-0.0633,0.002,-0.0055,-0.0134,0.0009,-0.0318,-0.0046,0.0596,0.0345,-0.2818,0.0106,0.0085,0.0337,-0.0176,0.0151,0.033,0.0431,-0.0306,-0.016,-0.0374,0.0173,0.0166,0.0179,0.0198,-0.0121,0.0446,-0.066,0.0507,-0.099,0.016,0.0773,0.1832,-0.0806,0.0217,0.039,-0.0124,-0.0557,0.0207,-0.0607,0.0579,-0.0073,0.0446,-0.0738,0.056,0.1043,-0.0299,0.0476,0.0211,-0.014,-0.0181,-0.0227,-0.0434,-0.0363,0.0776,0.0155,-0.0227,-0.0149,0.0108,0.0083,-0.0447,0.0189,0.0357,0.0002,0.0319,0.0453,-0.0643,-0.0676,-0.0261,-0.0216,0.0598,-0.0868,-0.0104,-0.0323,0.0051]}
{"key":"[Computer-Generated Music for Tabletop Role-Playing Games] In this paper we present Bardo Composer, a system to generate background music for tabletop role-playing games. Bardo Composer uses a speech recognition system to translate player speech into text, which is classified according to a model of emotion. Bardo Composer then uses Stochastic Bi-Objective Beam Search, a variant of Stochastic Beam Search that we introduce in this paper, with a neural model to generate musical pieces conveying the desired emotion. We performed a user study with 116 participants to evaluate whether people are able to correctly identify the emotion conveyed in the pieces generated by the system. In our study we used pieces generated for Call of the Wild, a Dungeons and Dragons campaign available on YouTube. Our results show that human subjects could correctly identify the emotion of the generated music pieces as accurately as they were able to identify the emotion of pieces written by humans.","layer":0,"vector":[-0.0408,-0.0192,0.0811,-0.0708,-0.0078,-0.0002,0.0339,-0.0163,0.0331,-0.0646,-0.0012,0.0178,0.0341,0.0058,0.0279,0.0216,0.0442,0.0465,-0.0568,0.0566,0.0317,0.0093,-0.0035,-0.0779,-0.0151,0.0124,-0.0612,-0.0796,-0.0549,-0.2063,0.0123,-0.0397,0.056,-0.0523,-0.0188,-0.0299,-0.0223,0.08,-0.0491,0.0312,0.0123,0.0073,-0.0189,-0.0486,0.0042,-0.0289,-0.024,-0.0304,-0.0085,-0.0113,-0.0113,-0.0351,0.0108,0.0215,0.0277,0.043,0.0623,0.0743,0.0598,0.0434,-0.0231,0.0283,-0.1567,0.1019,0.0211,0.0309,-0.0347,-0.012,-0.0029,0.0225,-0.052,0.0549,0.039,0.0459,0.0377,-0.0199,-0.0035,-0.0851,0.0206,0.0081,0.015,-0.0087,-0.0384,0.0343,-0.0217,-0.0286,0.0409,0.0101,0.0007,0.0254,-0.0522,0.0092,-0.0224,0.0339,-0.0995,-0.0317,0.03,0.0144,-0.0353,0.2326,-0.0297,-0.0034,0.0355,-0.0629,0.0372,0.0171,-0.0386,-0.009,-0.0461,0.0007,-0.0018,-0.0615,0.0307,-0.019,0.0287,0.026,-0.0046,0.021,0.0049,-0.0473,-0.0135,0.0316,0.0295,-0.0189,0.0792,-0.0599,0.0494,0.1338,0.0512,0.003,0.0545,-0.0357,-0.0513,-0.0314,0.012,0.017,0.0045,-0.0289,0.0129,-0.0132,-0.0066,-0.052,0.0156,-0.0718,-0.0662,0.1014,0.0506,0.0185,-0.078,0.0537,-0.035,0.0172,-0.0108,0.0058,0.0062,0.019,0.0226,0.0647,-0.098,0.0177,0.0009,-0.0601,0.0186,0.071,-0.0176,-0.0539,-0.0306,0.0056,0.0185,-0.0502,0.0088,0.0209,-0.0989,0.0475,0.0634,0.0275,-0.0529,-0.0035,-0.0103,0.0299,0.0552,-0.0292,-0.0353,0.0155,0.0146,-0.0691,0.0064,-0.0751,-0.009,0.0312,-0.0043,0.027,0.0054,-0.0176,-0.018,-0.0243,0.0151,-0.0178,0.0004,-0.0247,-0.0004,0.0159,-0.0481,0.0492,0.0473,0.0062,-0.0181,-0.0253,0.0444,0.0433,-0.0157,-0.0105,0.0699,0.0115,-0.0207,-0.0012,0.0137,0.0114,-0.0418,0.054,-0.0425,-0.0677,-0.076,-0.2544,0.0225,0.0119,-0.0066,0.0496,-0.0306,0.0296,-0.033,0.0299,0.066,0.0571,-0.0162,-0.0092,0.0104,-0.0168,0.0041,-0.0063,-0.0183,0.012,0.027,0.0467,-0.0069,-0.0243,-0.0863,0.021,-0.0195,0.2235,0.0631,0.0161,-0.0431,-0.0097,0.0364,-0.06,-0.1064,0.0603,0.0371,0.0895,0.0072,-0.0578,-0.0092,-0.0429,0.0275,0.0133,-0.0665,-0.0388,-0.0188,-0.0114,-0.0042,-0.0305,0.0376,0.055,-0.0136,0.079,0.0398,-0.0436,-0.0744,-0.0976,0.0273,-0.0011,0.0249,0.0265,-0.0288,0.0218,-0.0504,0.0068,-0.0066,-0.0227,-0.021,0.0527,-0.017,0.0083,0.0326,-0.006,-0.0054,0.0851,-0.0365,0.0394,-0.0471,-0.0404,-0.0164,0.0376,-0.0098,0.0296,0.0067,0.0312,0.005,0.0577,-0.0036,0.0553,-0.0568,-0.0009,0.0331,-0.0256,0.0252,-0.0012,0.0533,-0.3223,0.0374,0.0056,0.0202,-0.0394,0.0017,0.0202,0.0196,-0.0734,0.0047,-0.0114,0.0467,0.0147,-0.0284,0.002,0.0458,0.0982,-0.0186,0.0469,-0.0537,0.0223,0.0527,0.2492,-0.0163,0.0313,-0.0314,0.0043,-0.0001,-0.0071,-0.0388,0.019,0.0022,0.123,-0.0355,0.0147,0.0328,-0.0496,0.0117,-0.0167,-0.0186,-0.011,-0.0021,-0.0373,0.0035,0.0695,0.0115,0.0101,-0.038,-0.0064,0.0207,-0.0016,0.0471,-0.0098,-0.0084,-0.0151,0.0644,-0.0424,-0.0219,0.0056,-0.0387,0.0288,-0.044,0.0305,0.0284,-0.0143]}
{"key":"[Towards Maximizing the Representation Gap between In-Domain & Out-of-Distribution Examples] Among existing uncertainty estimation approaches, Dirichlet Prior Network (DPN) distinctly models different predictive uncertainty types. However, for in-domain examples with high data uncertainties among multiple classes, even a DPN model often produces indistinguishable representations from the out-of-distribution (OOD) examples, compromising their OOD detection performance. We address this shortcoming by proposing a novel loss function for DPN to maximize the \\textit{representation gap} between in-domain and OOD examples. Experimental results demonstrate that our proposed approach consistently improves OOD detection performance.","layer":4,"vector":[-0.0315,-0.0252,0.0385,-0.0619,0.073,0.0423,0.043,0.0194,0.0474,-0.0639,0.0044,-0.0672,-0.0115,0.0584,0.0162,0.0105,0.0375,0.079,-0.0239,0.0051,0.0473,-0.0491,0.0268,-0.0339,0.0289,0.0177,-0.0292,-0.0062,-0.0714,-0.266,0.0194,-0.0859,0.0209,-0.0644,0.0446,-0.0337,-0.0415,0.0578,-0.0093,0.0434,-0.013,0.0303,-0.0164,-0.079,-0.0087,-0.0584,0.0267,-0.0334,-0.0392,-0.054,0.0173,-0.0325,0.0586,0.0211,0.0459,0.0176,0.0342,0.0244,0.0419,0.0866,0.0131,0.0572,-0.146,0.0418,0.0563,0.0183,-0.06,-0.0183,0.0179,0.0265,-0.0207,0.0181,0.0135,0.0997,0.0115,0.0125,0.0083,-0.0543,-0.0393,-0.0206,0.0385,0.0052,-0.012,0.0037,-0.0061,-0.069,0.0519,-0.0509,0.0214,-0.0146,-0.0892,-0.035,-0.0305,0.0334,-0.0727,0.0016,0.0443,0.0128,-0.0373,0.1839,-0.0249,-0.0055,0.0382,-0.0469,0.05,-0.0062,-0.0426,-0.001,-0.0057,0.0059,0.0064,-0.0188,0.0404,-0.0107,0.0473,-0.0001,0.0995,-0.0013,-0.0179,-0.0214,-0.0636,-0.0047,0.0478,-0.0161,0.026,-0.0583,0.0232,0.1214,0.0327,0.0309,0.049,-0.0602,-0.0338,-0.0184,0.0473,0.0034,0.0029,-0.0019,0.021,-0.0089,-0.0301,-0.0423,0.0085,-0.0754,-0.1024,0.124,-0.0511,0.0304,-0.0167,-0.0361,0.022,0.0463,-0.0003,0.0069,0.0464,0.0123,0.0257,0.0642,-0.0675,-0.0064,-0.0033,-0.0419,-0.0521,0.1185,-0.0008,-0.0679,-0.047,-0.0037,0.0304,-0.0301,0.063,0.0325,0.0163,0.0447,0.0846,0.0004,-0.0664,-0.0169,0.0194,0.0054,0.0309,-0.0089,-0.0044,0.0396,0.0559,-0.0546,-0.0139,-0.0153,0.0259,0.0461,-0.035,-0.0253,-0.0169,0.0416,-0.0555,-0.0186,-0.0298,-0.0267,0.0209,-0.0484,0.0073,-0.019,-0.0408,0.0069,-0.0756,0.0232,0.03,-0.0019,0.0137,0.0202,-0.0487,0.0056,0.0681,0.0058,0.022,0.0128,0.0443,0.0643,0.0044,0.0331,0.0194,-0.0379,-0.0369,-0.2534,0.0047,0.0191,-0.0281,0.0262,-0.0625,-0.004,0.0046,0.0637,0.1062,0.0322,0.0164,-0.0164,-0.0069,-0.0186,0.0054,0.0315,0.0144,-0.0479,0.0453,-0.0186,0.0052,-0.044,-0.0904,0.078,-0.0313,0.1911,0.0018,0.0489,-0.0201,-0.0126,0.035,-0.0177,-0.0384,0.0627,0.0102,0.0479,-0.0052,-0.0317,-0.0066,-0.0055,0.0173,0.0157,-0.1019,-0.0333,-0.049,-0.0646,0.0193,-0.0838,0.051,0.0482,-0.0416,0.0778,-0.0397,-0.0073,-0.0687,-0.0738,-0.0209,-0.025,-0.0087,0.0376,-0.0311,0.014,-0.0615,0.0605,-0.0267,-0.0494,-0.0472,0.0417,-0.044,-0.0321,0.086,-0.0073,0.0174,0.0677,0.0033,0.0253,-0.0334,-0.0488,-0.0667,0.092,-0.0412,0.0214,0.0227,0.0417,0.0274,0.069,0.0119,0.0414,-0.0155,-0.0139,0.0005,-0.0091,-0.0213,0.0358,0.0094,-0.2754,0.0221,0.0373,0.0174,-0.0444,0.0465,0.0561,0.0234,-0.0388,-0.008,-0.0434,0.0487,-0.0025,-0.0239,-0.0472,0.0472,0.0172,-0.007,0.0301,-0.0483,0.0255,0.0427,0.2129,-0.0302,0.0144,-0.0111,-0.0072,-0.0188,-0.0355,0.0116,0.0132,0.0069,0.0688,-0.0663,0.0042,0.0949,-0.0612,0.0436,0.0338,-0.0255,0.038,-0.0179,-0.0248,-0.0488,0.1016,-0.0047,0.0072,-0.0316,-0.0056,0.0458,-0.0125,0.0147,-0.0328,0.0172,0.0113,0.0388,0.0056,-0.0289,-0.0619,-0.057,0.0422,-0.045,-0.007,-0.0043,0.0004]}
{"key":"[Evaluating Protein Transfer Learning with TAPE] Protein modeling is an increasingly popular area of machine learning research. Semi-supervised learning has emerged as an important paradigm in protein modeling due to the high cost of acquiring supervised protein labels, but the current literature is fragmented when it comes to datasets and standardized evaluation techniques. To facilitate progress in this field, we introduce the Tasks Assessing Protein Embeddings (TAPE), a set of five biologically relevant semi-supervised learning tasks spread across different domains of protein biology. We curate tasks into specific training, validation, and test splits to ensure that each task tests biologically relevant generalization that transfers to real-life scenarios. We benchmark a range of approaches to semi-supervised protein representation learning, which span recent work as well as canonical sequence learning techniques. We find that self-supervised pretraining is helpful for almost all models on all tasks, more than doubling performance in some cases. Despite this increase, in several cases features learned by self-supervised pretraining still lag behind features extracted by state-of-the-art non-neural techniques. This gap in performance suggests a huge opportunity for innovative architecture design and improved modeling paradigms that better capture the signal in biological sequences. TAPE will help the machine learning community focus effort on scientifically relevant problems. Toward this end, all data and code used to run these experiments are available at https://github.com/songlab-cal/tape.","layer":2,"vector":[-0.0758,-0.0484,0.012,-0.0109,0.0498,0.0185,0.037,0.0475,-0.002,0.0023,-0.0007,-0.0918,0.0238,0.087,0.0162,-0.004,0.0049,0.1013,-0.0713,-0.0129,0.0058,-0.0058,-0.0112,-0.0474,0.0381,0.0318,-0.039,0.0267,-0.0543,-0.2466,-0.0041,-0.0929,0.0039,-0.024,0.0257,0.0001,-0.0155,0.0547,-0.0632,0.0372,0.0691,-0.0006,-0.0372,-0.0684,0.0024,-0.0585,-0.0663,-0.0565,0.0129,-0.043,0.0224,-0.0209,-0.0216,0.0366,0.0142,0.0398,0.0693,0.0589,-0.0076,0.0368,0.0219,0.045,-0.1204,0.0555,0.0654,0.0415,-0.0584,0.0232,-0.0155,0.0809,-0.0311,0.0365,-0.0055,0.0475,0.0009,-0.0216,0.0098,0.019,0.0455,-0.012,0.0024,-0.0314,0.0002,-0.0582,-0.0088,-0.0027,-0.0024,-0.0548,0.0225,0.0436,-0.0117,-0.0197,-0.0137,0.0162,-0.0705,0.0099,0.0871,-0.0248,-0.0689,0.2006,-0.0753,0.0413,0.0358,-0.0093,0.0364,-0.0422,-0.0157,-0.0285,-0.017,0.0169,-0.0103,0.0031,0.015,-0.048,0.046,-0.0011,0.0529,0.0565,-0.0368,0.0072,-0.0148,0.0357,0.0484,-0.0145,0.0355,-0.0296,0.0154,0.1233,0.051,0.0344,0.0819,0.003,-0.0397,0.0122,-0.0114,0.0737,-0.0179,-0.043,-0.0084,-0.0145,-0.0105,-0.0291,0.0006,-0.079,-0.0916,0.1146,-0.046,0.0233,-0.0519,-0.0023,0.0192,0.0249,-0.0128,0.0011,0.0156,0.0765,0.042,0.0539,-0.0374,0.0087,-0.0487,-0.0078,-0.0145,0.0537,0.0053,-0.0645,-0.0495,-0.0181,0.0256,-0.0208,0.0477,0.051,-0.0314,0.0137,0.0372,0.0256,-0.0751,-0.026,-0.011,-0.0003,0.0026,-0.0423,-0.0543,0.0535,0.0295,-0.0293,0.0082,-0.0559,0.0152,0.0772,-0.0243,0.0499,-0.064,0.0242,-0.0488,-0.0482,-0.0176,-0.001,-0.0097,-0.0341,0.0359,0.023,0.0142,0.0231,-0.0491,-0.0081,-0.0154,0.0411,0.0604,0.0319,-0.049,0.0221,-0.0465,-0.0036,-0.0643,0.0218,-0.0044,0.0968,0.0721,0.0564,-0.0116,-0.0302,-0.0393,-0.203,0.001,0.0681,-0.0189,0.0578,-0.0719,0.0364,0.0055,0.0768,0.077,0.0742,-0.0112,-0.0234,0.0015,-0.0365,0.032,0.04,0.0038,-0.0473,0.0247,-0.0452,0.0189,0.0021,-0.0364,0.0311,-0.0461,0.1987,0.0543,0.034,-0.0147,0.021,0.0139,-0.0527,-0.1374,0.0388,-0.0012,0.0196,-0.0043,-0.003,-0.0089,-0.0535,0.0304,0.0096,-0.1226,-0.0399,-0.0197,-0.036,-0.0153,-0.0561,0.032,0.0708,-0.0554,0.0746,-0.0172,-0.029,-0.0142,-0.0443,0.0422,-0.048,0.0416,-0.0004,-0.0739,0.0018,-0.064,-0.0094,-0.0396,-0.0447,-0.0246,0.0255,-0.061,-0.0189,0.0679,-0.0081,0.0384,0.06,-0.0131,0.0295,-0.0681,-0.0524,-0.0011,0.06,-0.0201,0.0156,-0.0148,0.023,0.0333,0.1053,-0.0053,0.014,-0.0305,-0.0106,-0.0257,-0.0645,-0.0267,0.0491,-0.0071,-0.294,0.0417,0.038,0.0621,-0.0141,0.0114,0.0385,-0.009,-0.0475,0.0327,0.0324,0.0118,0.0753,0.0213,-0.009,0.0134,0.074,-0.0835,0.0244,-0.0696,0.0143,0.036,0.2073,-0.0232,0.0238,0.0212,-0.0287,0.0143,0.0616,-0.022,0.0046,0.0166,0.0782,-0.0399,0.0235,0.0719,-0.0441,-0.0136,-0.0101,-0.0097,0.05,-0.0099,-0.0705,-0.0227,0.0571,0.0097,-0.0183,-0.0142,-0.0135,0.0314,-0.0273,0.0142,-0.0002,-0.0397,0.0319,0.033,-0.0262,-0.0209,-0.0593,0.0064,0.06,-0.073,-0.0519,0.0097,-0.013]}
{"key":"[A Longitudinal Framework for Predicting Nonresponse in Panel Surveys] Nonresponse in panel studies can lead to a substantial loss in data quality due to its potential to introduce bias and distort survey estimates. Recent work investigates the usage of machine learning to predict nonresponse in advance, such that predicted nonresponse propensities can be used to inform the data collection process. However, predicting nonresponse in panel studies requires accounting for the longitudinal data structure in terms of model building, tuning, and evaluation. This study proposes a longitudinal framework for predicting nonresponse with machine learning and multiple panel waves and illustrates its application. With respect to model building, this approach utilizes information from multiple waves by introducing features that aggregate previous (non)response patterns. Concerning model tuning and evaluation, temporal cross-validation is employed by iterating through pairs of panel waves such that the training and test sets move in time. Implementing this approach with data from a German probability-based mixed-mode panel shows that aggregating information over multiple panel waves can be used to build prediction models with competitive and robust performance over all test waves.","layer":1,"vector":[-0.0216,0.0308,0.023,-0.0139,0.0452,0.0151,-0.0018,0.0293,0.0046,-0.0295,-0.0087,-0.0449,0.0006,0.0425,-0.0151,0.0319,-0.0109,0.0379,-0.0272,0.0446,-0.0046,0.0077,0.0207,-0.0523,0.0481,0.0188,-0.0364,-0.0631,-0.0599,-0.2471,0.0321,-0.0441,0.0087,-0.024,0.0394,0.005,-0.0281,0.0223,-0.0012,0.0436,0.0202,0.0035,-0.0101,-0.0478,-0.0213,-0.0692,0.0086,-0.0062,-0.0278,-0.0354,0.0153,-0.0482,0.0511,0.0351,0.0478,0.0315,0.0077,0.0117,0.0458,0.0608,0.0559,0.0124,-0.2226,0.0642,0.043,0.072,-0.0258,0.0058,-0.0138,0.0372,-0.0287,0.0221,0.0134,0.0554,0.0442,0.0125,0.0185,-0.031,0.005,0.0027,0.0264,-0.0227,-0.0424,-0.0392,-0.0126,-0.0499,-0.0091,-0.0348,0.0503,0.0046,-0.0246,-0.012,-0.0608,0.0145,-0.0936,-0.0456,0.071,0.0236,-0.0308,0.2152,-0.0513,0.0579,0.0196,-0.0393,0.0412,-0.0576,-0.0177,-0.0138,-0.0281,0.0062,-0.019,-0.034,0.0272,-0.0544,0.016,-0.0053,0.0187,0.0082,0.0339,-0.0079,-0.0336,0.0048,0.0523,-0.0341,0.0341,-0.0356,0.0401,0.1467,0.0244,-0.0117,0.0224,-0.0676,-0.0753,0.0054,-0.0256,-0.0075,0.0438,0.0469,0.0017,0.0221,-0.0142,-0.0525,0.0085,-0.1141,-0.0593,0.1085,-0.0206,0.0186,-0.0431,-0.0103,-0.036,0.0826,-0.0197,-0.0526,0.0265,0.0089,-0.021,0.0433,-0.0184,0.046,0.0027,-0.0651,-0.0693,0.1083,-0.0089,-0.0625,-0.0251,0.014,-0.0075,-0.0177,0.0412,0.0296,-0.0709,0.0263,0.068,0.009,-0.0461,0.0243,0.0185,0.0547,0.0179,-0.0184,-0.0611,0.075,0.0523,0.003,-0.018,-0.0522,0.018,0.0027,-0.0564,-0.0013,-0.0177,0.0578,-0.023,-0.0429,-0.027,-0.043,0.0263,-0.0305,0.0018,0.0085,-0.0126,0.0468,-0.0174,0.0273,0.0192,0.0105,0.1091,-0.001,-0.0253,-0.0149,0.0831,-0.0271,-0.0161,-0.0112,0.0246,0.047,0.0083,0.0679,0.0382,-0.0497,-0.0424,-0.1887,-0.0137,0.006,0.0371,0.042,-0.0317,0.0328,-0.0095,0.0526,0.1245,0.0439,0.0422,-0.0366,0.0342,-0.0233,0.0159,0.0371,-0.0006,-0.0537,-0.0223,-0.0325,-0.0148,-0.0141,-0.0426,0.0581,-0.0369,0.1881,-0.0062,-0.0322,-0.0272,0.0294,0.026,-0.0404,-0.0938,0.0899,0.0135,0.0358,-0.0238,-0.0948,-0.0066,-0.0157,-0.003,-0.0172,-0.0646,-0.0689,-0.0034,-0.0147,0.0055,-0.054,-0.0071,0.0063,-0.0804,0.0727,0.0046,-0.0107,-0.0298,-0.1066,0.0553,-0.013,0.0196,0.039,0.0031,0.0268,-0.0503,0.0151,0.015,-0.0699,-0.0237,-0.0078,-0.0027,-0.0203,0.1026,-0.0038,-0.0186,0.0263,-0.0063,-0.0028,-0.0711,-0.0523,-0.0032,0.061,-0.0359,0.0388,0.0577,0.0479,0.0272,0.0383,0.0219,0.0443,-0.043,0.0149,0.0089,-0.0662,-0.0257,0.0108,0.048,-0.2882,0.0519,-0.0065,0.0308,-0.0394,-0.0008,0.01,0.0054,-0.0185,0.0008,0.0262,0.078,0.0437,-0.041,0.0034,0.0194,0.0827,-0.0495,0.0333,-0.0517,0.0541,0.07,0.2289,-0.0134,0.0575,0.0141,0.0113,-0.0135,0.0165,-0.0429,0.0178,-0.0157,0.0862,-0.0723,0.0232,0.0312,-0.0494,0.0174,-0.0142,-0.0357,-0.0352,0.0327,0.002,0.0164,0.1264,-0.0053,-0.0223,-0.077,0.0038,0.0672,-0.0338,-0.0131,-0.0403,0.0527,0.0334,0.0105,-0.0635,-0.015,-0.0271,0.0145,-0.0117,-0.0047,-0.0227,-0.0573,-0.0027]}
{"key":"[Thermodynamics of Restricted Boltzmann Machines and related learning dynamics] We investigate the thermodynamic properties of a Restricted Boltzmann Machine (RBM), a simple energy-based generative model used in the context of unsupervised learning. Assuming the information content of this model to be mainly reflected by the spectral properties of its weight matrix $W$, we try to make a realistic analysis by averaging over an appropriate statistical ensemble of RBMs. First, a phase diagram is derived. Otherwise similar to that of the Sherrington- Kirkpatrick (SK) model with ferromagnetic couplings, the RBM's phase diagram presents a ferromagnetic phase which may or may not be of compositional type depending on the kurtosis of the distribution of the components of the singular vectors of $W$. Subsequently, the learning dynamics of the RBM is studied in the thermodynamic limit. A \"typical\" learning trajectory is shown to solve an effective dynamical equation, based on the aforementioned ensemble average and explicitly involving order parameters obtained from the thermodynamic analysis. In particular, this let us show how the evolution of the dominant singular values of $W$, and thus of the unstable modes, is driven by the input data. At the beginning of the training, in which the RBM is found to operate in the linear regime, the unstable modes reflect the dominant covariance modes of the data. In the non-linear regime, instead, the selected modes interact and eventually impose a matching of the order parameters to their empirical counterparts estimated from the data. Finally, we illustrate our considerations by performing experiments on both artificial and real data, showing in particular how the RBM operates in the ferromagnetic compositional phase.","layer":2,"vector":[-0.0706,-0.0088,0.0484,-0.015,-0.0015,0.036,0.0312,-0.0018,0.0261,-0.0492,0.0144,-0.049,0.0393,0.0443,0.0663,0.0122,-0.0101,0.022,-0.0634,-0.0024,0.0098,-0.0414,-0.011,-0.0463,-0.0002,0.027,-0.028,-0.0287,-0.0584,-0.2524,0.0504,-0.0533,0.084,-0.0311,0.0335,0.0085,-0.0553,0.0358,-0.0201,0.0392,0.0246,0.0054,-0.0135,-0.0507,-0.0198,-0.0153,0.0087,0.0269,-0.0408,-0.0546,0.0088,-0.0122,-0.0014,0.0402,0.0309,0.055,0.0667,0.0491,0.0297,0.0401,0.0002,0.0539,-0.1633,0.0523,0.0814,0.0271,-0.0566,-0.0034,0.0002,0.028,0.0324,0.0585,0.0116,0.014,-0.0137,-0.0187,-0.0305,-0.0059,0.0182,0.0082,-0.0352,-0.0295,-0.0157,-0.0385,-0.0044,-0.0256,0.0454,-0.0468,0.0091,0.0177,-0.0552,0.0005,-0.0558,0.0317,-0.0548,-0.0242,0.0654,0.0382,-0.0117,0.1742,-0.0348,0.0288,0.0537,-0.0251,-0.0067,-0.06,-0.0183,-0.0194,-0.0303,-0.0373,-0.0106,-0.0253,-0.0083,-0.0655,0.0451,-0.0071,0.0683,0.0094,-0.0162,-0.0053,-0.0618,0.0117,0.0227,-0.0277,0.0149,-0.0489,-0.0249,0.159,0.0356,0.03,0.0186,-0.0039,-0.0747,-0.0442,0.0183,0.0129,0.0168,-0.0043,0.0404,-0.0117,-0.022,0.0077,-0.0346,-0.1274,-0.0387,0.0889,-0.041,0.059,-0.0587,0.0057,-0.0034,-0.0191,-0.0538,-0.0439,0.0274,0.081,-0.008,0.0555,-0.0768,0.0273,-0.0505,-0.0769,-0.0024,0.0792,-0.0088,-0.0264,-0.0178,0.0224,0.0375,-0.0461,0.057,0.0652,-0.0287,0.0198,0.0453,0.0085,-0.06,-0.014,0.0142,-0.0205,0.0115,-0.0449,-0.0211,0.0614,0.0479,-0.0195,0.0127,-0.079,0.0075,0.0673,0.0053,0.0289,0.0113,-0.0209,-0.0235,0.0198,-0.0147,-0.0023,0.0116,-0.0307,0.0212,0.0015,-0.0538,0.0608,-0.005,0.0181,-0.0017,0.0251,0.0182,0.0148,-0.0264,-0.0203,0.0262,-0.0301,-0.0451,0.0137,-0.0056,-0.0063,-0.0098,0.0235,0.0813,-0.0548,-0.0746,-0.2347,0.029,-0.0083,-0.0043,0.1102,-0.0491,0.0415,-0.0137,0.0352,0.0252,0.0423,0.0274,-0.0571,-0.0452,-0.0119,0.0106,0.0507,0.0475,-0.0018,0.0572,-0.0108,0.0119,-0.0033,-0.0888,0.026,0.0009,0.159,0.0347,0.0512,-0.031,0.0027,0.0555,-0.0412,-0.036,0.0637,0.0405,0.0754,0.005,-0.0192,-0.0178,0.0015,0.0071,-0.001,-0.0665,-0.0632,-0.0234,-0.0275,-0.0004,-0.0755,0.0079,0.0687,0.0098,0.0558,-0.0473,-0.0159,-0.0637,-0.1031,0.0521,-0.0601,0.0213,0.0222,-0.0877,0.0174,-0.0504,0.0123,-0.0145,-0.0184,-0.033,0.0507,-0.0173,-0.0414,0.1152,0.061,0.0216,0.0495,0.0136,0.0392,-0.0262,-0.064,0.026,0.0396,-0.0186,0.0596,0.0314,0.0518,0.0023,0.0695,-0.0318,-0.0322,-0.0226,-0.0182,0.0426,-0.0616,0.0451,0.05,-0.0088,-0.2895,0.0742,0.001,0.1044,0.0073,-0.0081,0.0297,0.0211,-0.0762,0.0171,0.0098,0.0531,0.04,0.0167,0.0079,0.018,0.076,-0.0868,0.047,-0.061,0.0107,0.0705,0.2252,-0.0508,0.0322,0.0008,-0.0301,0.0361,-0.0036,-0.0326,0.0135,-0.0384,0.0795,-0.0397,0.0816,0.0759,-0.025,0.0453,0.0281,-0.0361,0.0195,0.0156,-0.0477,-0.0606,0.0647,-0.025,-0.0033,-0.0605,-0.0481,-0.005,-0.0042,0.0187,-0.0225,0.0045,0.0464,0.0494,-0.0426,-0.0634,0.0288,-0.0503,0.0272,-0.0275,-0.0535,-0.0296,-0.057]}
{"key":"[Anderson Acceleration as a Krylov Method with Application to Asymptotic Convergence Analysis] Anderson acceleration is widely used for accelerating the convergence of fixed-point methods $x_{k+1}=q(x_{k})$, $x_k \\in \\mathbb{R}^n$. We consider the case of linear fixed-point methods $x_{k+1}=M x_{k}+b$ and obtain polynomial residual update formulas for AA($m$), i.e., Anderson acceleration with window size $m$. We find that the standard AA($m$) method with initial iterates $x_k$, $k=0, \\ldots, m$ defined recursively using AA($k$), is a Krylov space method. This immediately implies that $k$ iterations of AA($m$) cannot produce a smaller residual than $k$ iterations of GMRES without restart (but without implying anything about the relative convergence speed of (windowed) AA($m$) versus restarted GMRES($m$)). We introduce the notion of multi-Krylov method and show that AA($m$) with general initial iterates $\\{x_0, \\ldots, x_m\\}$ is a multi-Krylov method. We find that the AA($m$) residual polynomials observe a periodic memory effect where increasing powers of the error iteration matrix $M$ act on the initial residual as the iteration number increases. We derive several further results based on these polynomial residual update formulas, including orthogonality relations, a lower bound on the AA(1) acceleration coefficient $\\beta_k$, and explicit nonlinear recursions for the AA(1) residuals and residual polynomials that do not include the acceleration coefficient $\\beta_k$. We apply these results to study the influence of the initial guess on the asymptotic convergence factor of AA(1).","layer":7,"vector":[-0.1225,0.0024,0.0355,0.0198,0.0125,0.0117,-0.0308,0.0317,0.0743,-0.0475,0.0508,-0.0415,0.0262,0.0359,-0.0143,0.0351,0.0038,0.048,-0.0418,0.007,0.0328,-0.0462,0.0076,-0.0685,0.0641,-0.0256,-0.0348,-0.0399,-0.0219,-0.2547,0.0251,-0.0098,0.0681,0.0092,0.0018,0.0155,-0.0256,0.0429,-0.0478,0.0373,0.0381,0.0238,-0.0514,-0.0486,-0.0295,-0.0438,-0.0502,0.0022,-0.0195,-0.0274,0.0275,0.0015,0.0392,0.015,0.0149,0.0042,0.0526,0.011,0.007,0.0151,0.064,0.0119,-0.2177,0.0508,0.0299,0.0143,0.0183,-0.0579,0.0283,0.0886,-0.0128,0.0556,0.0247,0.0316,0.0589,0.0077,0.0023,-0.0419,0.0078,0.0327,-0.0244,-0.0849,-0.0642,-0.0642,-0.0232,-0.0198,-0.0078,-0.0753,0.0625,0.0157,-0.0057,-0.0243,-0.0066,-0.0022,-0.06,-0.0258,0.0249,0.017,-0.014,0.1761,-0.0107,0.0074,0.0083,-0.0059,-0.0044,-0.0579,-0.0213,-0.0382,-0.0116,-0.003,-0.0092,-0.0105,0.0321,-0.0168,0.004,0.0193,0.0286,0.0215,-0.0021,0.0015,-0.0081,0.0245,0.0295,-0.0222,0.0164,-0.0745,-0.0017,0.1309,0.0421,0.0647,0.016,-0.0152,-0.0447,-0.0244,0.0013,0.0191,0.0067,-0.0272,0.0746,-0.0205,-0.0204,-0.0785,-0.0033,-0.0735,-0.0211,0.1506,-0.0123,0.0288,-0.0221,-0.0333,0.0011,0.0494,-0.0295,-0.0316,0.0427,0.0064,0.0288,0.0619,-0.0549,0.0392,-0.0885,-0.0324,-0.0318,0.0985,0.0073,-0.0354,0.0177,0.0175,0.0278,-0.0258,0.0189,0.0474,-0.0162,-0.0114,0.0688,-0.0291,-0.0296,-0.0125,0.0261,0.0031,0.0238,-0.0108,-0.0738,0.07,0.0217,-0.0395,-0.0187,-0.0077,0.0584,-0.0107,-0.0679,-0.0056,-0.0188,-0.0034,-0.0109,-0.0183,-0.0102,-0.0343,0.0382,-0.0594,0.0164,-0.0148,-0.0375,0.0305,-0.0148,0.0064,-0.0316,0.0235,0.0176,0.043,-0.0683,-0.0461,0.071,-0.0568,-0.0442,-0.0007,0.0156,-0.0284,-0.0091,0.0325,0.0586,-0.0238,-0.0628,-0.2336,-0.0301,-0.0233,0.0103,0.037,-0.0961,0.0693,-0.0266,0.077,0.0375,0.0039,-0.0004,0.0349,0.01,0.012,0.0395,0.0031,0.0278,0.0163,0.0189,-0.0513,0.0071,0.0142,-0.0432,0.0661,-0.0216,0.1713,0.0739,0.0544,-0.0029,0.035,0.0123,0.0249,-0.0382,0.0573,0.0365,0.0909,-0.0237,0.0257,-0.0268,-0.0096,0.0327,0.0073,-0.0336,-0.0586,0.0119,-0.0603,-0.029,-0.0111,-0.0097,0.0719,-0.0393,0.0338,-0.0321,0.0259,-0.0375,-0.0894,0.0027,0.0034,0.0313,-0.0318,-0.0849,0.0378,-0.0058,0.1009,0.0022,-0.0124,-0.0438,0.0083,-0.0349,-0.0162,0.0613,-0.0101,0.0156,0.0602,0.0459,0.0392,0.0001,-0.0742,-0.0084,0.0448,-0.0619,0.062,-0.0312,0.0288,-0.0094,0.0612,-0.0146,-0.0373,-0.0276,0.0231,0.0123,-0.0621,0.0656,0.0287,-0.0067,-0.2937,0.0313,0.0104,0.0122,-0.0513,0.0074,0.017,-0.0194,-0.0839,0.0205,-0.0489,0.1006,0.0493,-0.0294,0.0577,0.0299,0.0591,-0.0117,0.0376,-0.0703,0.0206,0.0305,0.213,-0.0243,-0.0116,0.0283,0.0116,0.0392,0.0475,-0.0256,0.0261,0.0052,0.0448,-0.0322,0.0737,0.0521,-0.0614,0.075,0.0509,-0.0214,0.0274,0.0132,-0.0772,-0.053,0.0683,-0.0003,-0.0479,-0.0979,0.0463,-0.0098,-0.0399,0.0084,0.037,-0.0155,0.0252,0.0461,-0.1112,-0.0385,-0.0559,-0.0081,-0.0104,-0.0277,-0.0292,0.0127,0.0205]}
{"key":"[Entropy from Machine Learning] We translate the problem of calculating the entropy of a set of binary configurations/signals into a sequence of supervised classification tasks. Subsequently, one can use virtually any machine learning classification algorithm for computing entropy. This procedure can be used to compute entropy, and consequently the free energy directly from a set of Monte Carlo configurations at a given temperature. As a test of the proposed method, using an off-the-shelf machine learning classifier we reproduce the entropy and free energy of the 2D Ising model from Monte Carlo configurations at various temperatures throughout its phase diagram. Other potential applications include computing the entropy of spiking neurons or any other multidimensional binary signals.","layer":0,"vector":[-0.0627,0.0086,0.0714,-0.0095,0.0295,0.0266,0.0738,0.0004,0.0481,-0.0276,0.0411,-0.0552,0.0252,0.0201,0.0145,0.0379,0.0092,0.0052,-0.0316,-0.0279,0.063,-0.0635,-0.0025,-0.1014,0.0218,0.0529,-0.0244,-0.0151,-0.0483,-0.2054,0.0173,-0.0608,0.0453,-0.0475,0.0316,-0.0268,-0.0178,0.0534,-0.0691,0.0159,0.028,-0.0068,-0.0096,-0.057,0.0223,-0.041,0.0045,-0.0008,0.0011,-0.0336,0.0146,-0.0205,-0.017,0.0168,0.0651,0.0605,0.0473,0.0034,0.0616,0.0376,0.0267,0.0891,-0.1883,0.0599,0.0577,0.0234,-0.0277,-0.0134,0.038,0.0391,-0.0135,0.0639,0.0688,0.0146,-0.0123,-0.0147,-0.0414,-0.0428,-0.0104,0.0183,-0.0201,-0.0594,-0.035,0.0004,-0.0135,-0.0181,0.0495,-0.0363,0.019,-0.0035,-0.0553,0.0141,-0.0646,-0.0014,-0.075,0.0079,0.0402,0.0275,-0.0076,0.1583,-0.0276,0.0159,0.0498,-0.0394,0.0482,-0.0551,-0.0109,-0.0564,-0.0479,-0.016,-0.0123,-0.0022,0.0427,-0.0503,0.0411,-0.0096,0.0456,0.0662,-0.0097,-0.0083,-0.0249,-0.0136,0.0301,0.0064,0.0329,-0.0348,-0.0374,0.1478,0.0184,0.01,0.0283,0.042,-0.0443,-0.0322,0.0329,-0.0159,0.0198,0.0314,0.0162,0.0166,-0.0106,-0.0257,0.0036,-0.123,-0.0491,0.0917,-0.0651,0.0414,-0.0618,0.0071,-0.0178,-0.0214,-0.0268,-0.0792,0.0525,0.0352,-0.0203,0.0244,-0.0647,0.0505,-0.0369,-0.0509,0.0188,0.0894,0.0301,-0.0561,0.0029,0.0012,0.016,-0.0587,0.0375,0.0537,-0.0789,0.0063,0.0666,0.002,-0.0765,-0.0418,0.02,-0.0016,0.0142,-0.0288,-0.0311,0.0068,0.0417,-0.0272,-0.0037,-0.0489,-0.0034,0.0671,-0.0006,-0.0067,-0.013,-0.0028,-0.0297,-0.0152,-0.0262,0.0073,-0.0096,-0.0461,0.0416,0.0189,-0.0759,0.0441,0.0,0.0571,0.0114,-0.0007,0.0511,0.0,0.0051,-0.0098,0.025,-0.0468,-0.027,0.0136,0.0098,0.0396,0.0128,0.0343,0.0826,-0.0867,-0.0973,-0.2211,-0.0184,-0.0101,-0.0208,0.0693,-0.0729,0.0698,-0.0141,0.0377,0.0234,0.0267,0.0085,-0.0209,0.0032,-0.049,-0.0006,0.0769,-0.0007,-0.055,0.0356,0.0286,0.0494,0.0,-0.0814,0.0049,0.0049,0.2161,0.0382,0.0369,-0.0167,0.0066,-0.0012,-0.0359,-0.0331,0.0695,0.0065,0.0441,0.0042,-0.0452,-0.0428,-0.025,0.0351,-0.0162,-0.0686,-0.0627,-0.0404,-0.0244,-0.004,-0.0628,0.0269,0.0462,0.0092,0.0543,-0.0051,0.0067,-0.0305,-0.0768,-0.0122,-0.02,0.0183,-0.0138,-0.0792,0.0185,-0.0498,-0.0009,-0.0159,0.004,-0.0451,0.0404,-0.0362,-0.0046,0.0998,0.0485,-0.0543,0.0713,-0.0367,0.0457,-0.0255,-0.0667,0.0057,0.0318,-0.0041,0.0418,0.0463,0.0263,0.0269,0.0667,-0.0077,0.0058,0.0054,-0.0159,0.0419,-0.0391,0.0307,0.0469,0.0138,-0.2969,0.0408,-0.0107,0.0651,-0.024,-0.035,0.0536,0.0209,-0.0719,0.0159,-0.0225,0.0016,0.03,-0.0165,-0.01,-0.0007,0.0853,-0.0617,0.0196,-0.0499,0.0269,0.0306,0.259,-0.0014,0.0777,0.013,-0.0016,0.0464,0.0335,-0.0339,0.0422,-0.0232,0.0464,-0.0437,0.0745,0.0871,-0.0532,0.0152,0.0446,-0.0195,0.0262,0.038,-0.0498,-0.0279,0.1137,-0.0214,-0.0288,-0.012,-0.0331,0.0252,-0.0395,0.0508,-0.0352,0.0251,0.0123,0.0573,-0.0532,-0.0557,-0.0314,-0.0465,0.0195,-0.0543,-0.0319,-0.0166,-0.0305]}
{"key":"[Twitter-based traffic information system based on vector representations for words] Recently, researchers have shown an increased interest in harnessing Twitter data for dynamic monitoring of traffic conditions. Bag-of-words representation is a common method in literature for tweet modeling and retrieving traffic information, yet it suffers from the curse of dimensionality and sparsity. To address these issues, our specific objective is to propose a simple and robust framework on the top of word embedding for distinguishing traffic-related tweets against non-traffic-related ones. In our proposed model, a tweet is classified as traffic-related if semantic similarity between its words and a small set of traffic keywords exceeds a threshold value. Semantic similarity between words is captured by means of word-embedding models, which is an unsupervised learning tool. The proposed model is as simple as having only one trainable parameter. The model takes advantage of outstanding merits, which are demonstrated through several evaluation steps. The state-of-the-art test accuracy for our proposed model is 95.9%.","layer":0,"vector":[-0.0186,-0.043,0.0027,0.0281,0.0172,0.0065,0.0805,0.0389,0.015,0.0039,-0.0063,-0.0354,0.0554,0.0522,0.0365,-0.0025,0.0332,0.0225,-0.0313,-0.0588,0.0511,-0.0502,0.0053,-0.0431,0.0287,0.0526,-0.0108,-0.0013,-0.0434,-0.2187,0.0049,-0.0828,0.062,-0.0014,-0.0039,0.007,-0.0322,0.0541,-0.0063,0.0469,-0.0013,-0.0092,-0.0403,-0.0333,-0.0373,-0.0355,-0.0224,-0.0121,-0.0364,-0.054,0.0325,-0.0276,-0.0036,0.0149,0.0077,0.0491,0.0555,0.0559,0.0294,0.0443,0.0233,0.0109,-0.1785,0.0296,0.0254,-0.0145,-0.0292,0.0255,0.0249,0.053,-0.0078,0.0628,-0.0108,0.064,0.0129,0.0217,0.0282,-0.0254,-0.0368,-0.0074,0.0167,-0.0354,-0.0194,-0.0148,-0.0124,-0.0688,-0.0227,-0.0479,0.0311,-0.0334,-0.0569,-0.0367,-0.033,0.0141,-0.0982,-0.0569,0.0369,0.0188,-0.0311,0.1897,-0.0457,0.0851,0.07,-0.026,0.0116,-0.0182,0.0197,-0.0504,-0.0405,-0.003,-0.0286,-0.0367,0.0689,-0.0475,0.0663,0.0161,0.0817,0.0401,-0.0293,0.0105,-0.0771,0.0253,0.0379,-0.032,0.0245,-0.0342,0.0545,0.1274,0.0533,0.0186,-0.0071,0.0451,-0.09,0.0174,-0.0074,0.035,-0.033,0.0245,-0.0235,-0.0232,-0.0158,-0.1074,0.0093,-0.0929,-0.0861,0.1057,-0.0626,-0.0055,-0.0225,-0.0343,-0.0234,-0.0001,-0.0471,-0.0383,-0.0182,0.0702,0.0855,0.045,-0.0004,0.0294,0.0025,-0.0413,-0.0395,0.0602,-0.0038,-0.0814,-0.049,0.0218,0.0013,-0.0768,0.0445,-0.0117,-0.0468,0.0462,0.0951,0.0179,-0.0536,0.0215,0.0196,0.0309,0.0303,-0.0958,-0.0528,0.053,0.0343,-0.0084,-0.0214,-0.0405,0.052,0.019,-0.0053,0.015,-0.0046,-0.0139,-0.0293,-0.0162,-0.0169,-0.0089,0.0171,-0.0568,0.0152,-0.0068,-0.0334,-0.001,-0.0172,0.0253,-0.0217,0.0206,0.0347,0.0071,-0.0043,-0.0353,-0.0038,0.0174,-0.0283,-0.0049,-0.0006,0.0542,0.0295,0.0354,-0.0072,-0.0531,-0.0404,-0.2239,-0.0393,0.01,0.0028,0.0717,-0.0488,0.005,-0.0011,0.0593,0.0766,0.0688,-0.0347,-0.0008,0.0207,0.0362,0.044,0.0394,0.0681,0.0012,-0.0306,-0.0403,0.0361,-0.0399,-0.0761,0.0664,-0.0199,0.1945,0.0222,0.0538,-0.0585,0.0315,0.0106,-0.0237,-0.1336,0.0572,0.0033,0.0473,0.0284,-0.0541,-0.0143,-0.0647,0.0325,0.0009,-0.0677,-0.0479,-0.058,-0.0468,0.0266,-0.0581,0.0048,0.0065,-0.0421,0.0608,0.0183,0.026,-0.0437,-0.039,0.0375,-0.0398,-0.0276,-0.014,-0.0327,0.0572,-0.0777,0.0443,0.0221,-0.0477,-0.0375,0.0151,0.0073,-0.0616,0.1142,-0.0292,0.0294,0.0493,0.0098,0.0151,-0.0238,-0.0705,-0.0059,0.072,-0.0144,0.0985,0.0253,0.0374,0.042,0.0768,-0.0073,0.0084,0.0081,0.0564,-0.0072,0.009,-0.0374,0.0155,-0.0137,-0.2945,0.0208,0.0104,0.0139,-0.0298,-0.0182,0.0626,0.012,-0.0288,-0.0297,0.0248,0.0871,0.0622,-0.0225,-0.0043,0.0289,0.0792,-0.0429,0.0113,-0.0317,0.0356,0.0435,0.225,-0.0557,0.0238,0.0019,-0.0276,0.026,0.0384,0.0044,0.0105,-0.004,0.1085,-0.047,0.0007,0.0317,-0.0532,0.0302,0.0284,0.0152,0.0217,0.0205,-0.0404,-0.0204,0.0339,0.0182,0.0133,-0.0259,-0.0039,0.0416,-0.005,-0.0103,-0.0161,0.024,0.0756,0.0065,-0.0569,-0.055,-0.0248,-0.0502,-0.0121,-0.0643,-0.0269,-0.0002,0.0339]}
{"key":"[Global Pose Estimation with an Attention-based Recurrent Network] The ability for an agent to localize itself within an environment is crucial for many real-world applications. For unknown environments, Simultaneous Localization and Mapping (SLAM) enables incremental and concurrent building of and localizing within a map. We present a new, differentiable architecture, Neural Graph Optimizer, progressing towards a complete neural network solution for SLAM by designing a system composed of a local pose estimation model, a novel pose selection module, and a novel graph optimization process. The entire architecture is trained in an end-to-end fashion, enabling the network to automatically learn domain-specific features relevant to the visual odometry and avoid the involved process of feature engineering. We demonstrate the effectiveness of our system on a simulated 2D maze and the 3D ViZ-Doom environment.","layer":6,"vector":[0.0021,-0.034,0.0773,-0.0329,0.0122,0.0396,0.0362,0.035,0.0227,-0.0431,0.0224,-0.0983,0.0454,0.1031,-0.0226,-0.0068,-0.0454,0.1064,0.0036,-0.0283,0.0303,-0.0223,0.0272,-0.0517,-0.0378,0.0046,-0.0489,-0.0059,-0.0282,-0.2344,0.001,-0.0611,0.043,-0.0342,-0.0342,-0.0113,-0.0147,0.06,-0.0333,0.0399,0.0121,0.0359,-0.0449,-0.059,-0.0053,-0.0495,0.0124,0.006,-0.0122,-0.0592,0.0293,-0.0385,0.0011,0.0174,0.0416,0.0748,0.0623,0.0142,0.0093,0.0236,0.0327,0.0317,-0.1555,0.042,0.0723,0.0305,-0.0245,-0.0155,0.0381,0.0576,-0.0021,0.0415,0.0269,0.0497,0.018,-0.0224,-0.0146,-0.0063,0.0382,-0.0334,0.0129,-0.0209,-0.0366,-0.0038,0.0054,-0.0353,-0.0239,-0.0186,0.0365,-0.0059,-0.0525,-0.004,-0.0533,-0.0091,-0.0558,-0.0184,0.0445,0.0372,-0.0599,0.2034,-0.0431,0.0394,0.0426,-0.011,0.0159,-0.0365,-0.0214,-0.0158,-0.0637,0.0433,-0.0245,-0.0327,-0.01,-0.0063,0.0342,0.0068,0.022,0.0655,0.0045,-0.0056,0.0044,-0.0012,0.0493,-0.0256,0.0384,-0.0534,-0.0119,0.1244,0.0517,0.0265,0.0624,-0.0091,-0.0147,0.0015,-0.0023,0.0375,0.0405,-0.0162,0.0225,-0.0123,-0.0282,-0.0169,0.0182,-0.0871,-0.0407,0.1164,-0.053,-0.0155,-0.0157,-0.0532,-0.0368,0.0307,0.0111,0.024,0.0214,0.0093,0.0386,0.0516,-0.0993,0.0234,-0.0531,-0.0089,-0.0099,0.0714,-0.0005,-0.0957,-0.0465,-0.0231,-0.0235,0.0157,0.0337,0.0265,-0.0162,0.0375,0.0763,0.0594,-0.0944,0.0188,-0.0128,0.0078,-0.0316,-0.0563,-0.003,-0.0151,0.0343,-0.0359,0.0215,-0.0191,-0.0496,0.055,0.0109,0.0568,-0.0138,-0.0036,-0.0411,0.0009,-0.0034,-0.0178,0.0223,-0.0547,0.0605,-0.0059,-0.0229,-0.0085,0.0123,-0.0,-0.005,-0.0148,-0.035,0.0266,-0.0585,0.0008,0.0411,-0.045,-0.0654,-0.0169,0.0235,-0.0,0.0098,0.0354,0.0015,-0.03,-0.0171,-0.2307,-0.0,-0.0101,-0.0005,0.0062,-0.0708,0.0024,-0.005,0.0799,0.03,0.059,-0.0868,0.0058,0.0735,0.0021,0.0425,0.0268,0.0247,-0.0192,-0.0374,0.0452,-0.0073,-0.0244,-0.0743,0.0417,0.0073,0.2556,0.0368,0.0618,-0.0248,0.0252,-0.0056,-0.0409,-0.0957,0.0467,0.0226,0.073,0.0196,-0.0269,-0.0159,-0.0761,0.0259,0.0324,-0.0922,-0.0296,-0.0155,-0.0363,0.0692,0.0048,0.0211,0.0402,-0.0439,0.0185,-0.0315,-0.0417,-0.0419,-0.0342,-0.0303,-0.051,0.0422,0.004,-0.0637,0.004,-0.0221,0.0648,0.0076,0.0137,-0.0703,0.0379,-0.0137,-0.0432,0.0539,0.0512,0.0031,0.063,-0.0001,0.0498,-0.0303,-0.0104,-0.0104,-0.0195,-0.0199,0.0321,0.016,0.0247,-0.0256,0.0787,-0.0379,0.016,-0.0132,0.0412,0.0037,-0.0261,-0.0567,0.0445,0.0244,-0.3161,0.0095,0.0276,0.0446,-0.029,0.0294,0.0423,0.0311,0.0135,0.0039,-0.0267,0.0528,0.0333,-0.0041,-0.0191,0.0446,0.0605,-0.029,0.0664,-0.0975,0.0106,0.056,0.2166,-0.0451,0.0657,0.0385,-0.0434,-0.0154,0.0169,-0.0379,0.0075,-0.0126,0.0586,-0.0598,0.0413,0.0792,-0.0388,0.0225,-0.0053,0.0171,-0.0071,0.0101,0.0279,-0.0157,0.0976,-0.02,-0.0341,0.0027,0.0045,0.0153,-0.0367,-0.004,-0.0247,0.0038,0.0699,0.0256,-0.0223,-0.0553,-0.1037,-0.0174,0.0358,-0.0926,-0.0246,-0.0531,0.0124]}
{"key":"[Online Learning with Pairwise Loss Functions] Efficient online learning with pairwise loss functions is a crucial component in building large-scale learning system that maximizes the area under the Receiver Operator Characteristic (ROC) curve. In this paper we investigate the generalization performance of online learning algorithms with pairwise loss functions. We show that the existing proof techniques for generalization bounds of online algorithms with a univariate loss can not be directly applied to pairwise losses. In this paper, we derive the first result providing data-dependent bounds for the average risk of the sequence of hypotheses generated by an arbitrary online learner in terms of an easily computable statistic, and show how to extract a low risk hypothesis from the sequence. We demonstrate the generality of our results by applying it to two important problems in machine learning. First, we analyze two online algorithms for bipartite ranking; one being a natural extension of the perceptron algorithm and the other using online convex optimization. Secondly, we provide an analysis for the risk bound for an online algorithm for supervised metric learning.","layer":0,"vector":[-0.0339,-0.0352,0.0374,-0.0108,-0.0247,0.0424,0.0461,0.0575,0.0399,-0.0541,0.0045,-0.0304,0.0259,0.0635,-0.0033,0.0151,0.0251,0.0301,-0.0474,0.0125,0.0374,-0.0178,-0.0196,-0.0769,0.0074,-0.0083,-0.0343,-0.0294,-0.029,-0.2312,0.0021,-0.0595,0.0334,-0.016,0.0192,-0.0101,-0.0116,0.051,-0.0457,0.01,0.0484,0.0522,-0.0046,-0.0205,0.0027,0.0136,0.0148,-0.0218,-0.0255,-0.0327,0.0421,-0.0342,0.0203,0.0672,0.0091,0.0564,0.0461,0.0799,-0.0049,0.079,0.0092,0.0236,-0.1917,0.0118,0.0191,0.0275,-0.0485,-0.0048,-0.0166,0.0919,-0.0156,0.0092,0.0085,0.0274,-0.0152,-0.0031,0.0056,-0.0185,-0.0274,0.0046,0.0271,-0.0303,-0.0644,0.0054,-0.0328,-0.0668,0.024,-0.0305,0.039,-0.003,-0.0146,0.0014,-0.0318,0.0226,-0.0537,-0.0027,0.0326,0.0319,-0.0454,0.181,-0.0997,0.065,0.0151,-0.0384,0.0415,-0.0538,-0.0367,-0.0522,0.0293,-0.0519,-0.0286,-0.0073,0.0303,-0.0686,0.0349,0.0581,0.0592,0.0416,-0.0056,-0.0124,-0.0506,0.0177,0.0651,0.0148,0.0574,-0.0363,-0.0002,0.137,0.0292,0.0605,0.049,-0.0481,-0.0682,-0.0183,0.0126,0.0176,0.0133,0.0165,0.0507,0.0026,-0.0393,-0.0588,0.0056,-0.054,-0.0126,0.1794,-0.0052,0.0579,-0.0271,-0.0472,0.0022,-0.0114,-0.0302,-0.0257,0.0172,0.0325,0.0377,0.0133,-0.0448,0.0234,-0.0536,-0.0106,0.0164,0.0955,0.0328,-0.0647,-0.0154,-0.0118,0.0347,-0.0194,0.0612,0.0365,-0.0156,0.0252,0.0535,0.0196,-0.0826,-0.0204,0.0215,0.0125,0.0228,-0.0276,-0.0351,0.0227,0.0443,0.0008,0.024,-0.0566,0.0592,0.035,-0.0195,-0.007,-0.0557,-0.0335,-0.0172,-0.004,-0.0298,-0.0125,0.0301,0.0076,0.0325,-0.0053,-0.0236,0.0189,0.0125,0.001,0.0042,0.0,0.0263,0.0155,-0.0089,-0.024,0.0226,0.0108,-0.0276,-0.0198,0.0766,0.038,0.0026,0.0485,0.0142,-0.0028,-0.036,-0.2398,-0.0512,0.016,-0.0131,0.0408,-0.1015,0.0578,-0.0264,0.0221,0.0728,0.0641,-0.0087,-0.0039,0.0313,-0.023,0.0748,0.0461,0.0012,-0.0123,-0.0066,-0.0376,0.0692,-0.0087,-0.0817,0.0603,-0.0032,0.208,0.0261,0.0347,-0.0712,0.0161,0.054,-0.0035,-0.0675,0.0663,0.0162,0.0251,-0.0394,-0.0478,-0.0255,-0.0201,-0.0183,0.0532,-0.0997,-0.0225,-0.0341,-0.0468,0.0433,-0.0559,0.0381,0.076,-0.0222,0.0866,-0.0506,-0.0046,-0.0493,-0.0906,0.023,-0.0648,0.0243,0.011,-0.0815,0.0092,-0.1057,0.0644,-0.0184,-0.0369,-0.0162,0.0262,-0.0379,-0.0528,0.0935,-0.0053,-0.007,0.0663,-0.0142,0.0527,-0.0499,-0.0376,-0.0147,0.0723,-0.0549,-0.0007,0.0125,0.0106,0.0123,0.1008,0.0029,0.0137,-0.0113,-0.0179,-0.0049,-0.0895,-0.0129,0.0282,-0.0008,-0.2808,0.0373,-0.0014,0.0229,-0.044,-0.0229,0.0376,-0.0132,-0.0655,-0.0328,0.0085,0.0524,0.0261,-0.0425,0.0456,0.0371,0.0143,-0.0496,0.024,-0.025,-0.0058,0.0445,0.2041,-0.0468,0.0382,0.0099,-0.0269,-0.0456,0.0344,-0.0387,0.0645,-0.0025,0.0836,-0.0595,0.0282,0.0943,-0.0134,0.0406,0.0248,-0.0112,-0.0066,-0.0032,-0.0726,-0.0106,0.1143,0.002,-0.0034,-0.0581,-0.0402,0.0065,-0.0527,0.0208,0.003,-0.0085,-0.0018,0.0157,-0.0156,-0.0585,-0.0096,-0.0658,0.0153,-0.0447,-0.0374,0.0035,-0.0209]}
{"key":"[Semi-Supervised Clustering via Information-Theoretic Markov Chain Aggregation] We connect the problem of semi-supervised clustering to constrained Markov aggregation, i.e., the task of partitioning the state space of a Markov chain. We achieve this connection by considering every data point in the dataset as an element of the Markov chain's state space, by defining the transition probabilities between states via similarities between corresponding data points, and by incorporating semi-supervision information as hard constraints in a Hartigan-style algorithm. The introduced Constrained Markov Clustering (CoMaC) is an extension of a recent information-theoretic framework for (unsupervised) Markov aggregation to the semi-supervised case. Instantiating CoMaC for certain parameter settings further generalizes two previous information-theoretic objectives for unsupervised clustering. Our results indicate that CoMaC is competitive with the state-of-the-art.","layer":1,"vector":[-0.0445,-0.0314,0.0253,-0.0038,0.0397,0.0747,0.0624,-0.0039,0.0272,0.0212,0.0355,-0.1005,0.0309,0.091,-0.0004,0.0438,0.0242,0.0884,-0.0423,-0.008,-0.0031,-0.0158,-0.0322,-0.0359,0.0831,0.05,-0.0243,-0.0264,-0.0666,-0.2077,0.0185,-0.0309,0.0809,0.0039,0.0457,0.0044,-0.0113,0.0939,-0.0102,0.0166,0.02,0.0208,-0.0073,-0.0654,-0.0561,-0.0735,-0.0515,-0.0112,-0.0507,-0.0257,-0.023,-0.0031,0.0064,-0.0212,0.0434,0.0408,0.0253,-0.0018,0.0211,0.0063,0.0234,0.0559,-0.1641,0.0652,0.0529,0.019,-0.043,0.0285,0.0573,0.0373,-0.0098,0.0909,-0.0002,0.0267,0.0149,-0.0296,0.0035,-0.0375,-0.0043,0.0155,-0.0156,-0.0585,-0.0517,-0.0112,-0.0284,-0.0704,0.0317,-0.0775,0.0261,-0.0076,-0.0206,0.0202,-0.0181,0.0041,-0.0772,-0.0446,0.0194,0.0384,-0.0057,0.1744,-0.0625,0.0831,0.044,-0.0446,0.0417,-0.0452,-0.0005,-0.0581,-0.0261,-0.0118,-0.0031,0.0209,0.022,-0.0919,0.0586,0.0122,0.0668,0.0508,-0.0055,-0.028,-0.0118,-0.0137,0.0496,0.0353,0.0082,-0.039,0.0017,0.104,0.0511,-0.0185,0.0698,-0.0025,-0.0499,0.0041,0.038,0.0247,0.0008,-0.0215,0.0222,-0.0098,-0.0159,-0.0824,-0.0017,-0.1044,-0.0544,0.1833,-0.0709,0.0103,-0.0539,-0.0264,0.0187,-0.0297,-0.0155,-0.0501,0.0208,0.0671,0.0654,0.0142,-0.0458,0.036,-0.0349,-0.0489,0.0085,0.1493,0.0065,-0.0724,-0.0204,0.0049,0.0049,-0.041,0.0595,0.0871,-0.0088,0.0246,0.06,0.0204,-0.0789,-0.0114,0.0094,0.0089,0.0009,-0.0044,-0.032,0.0596,0.041,-0.0407,-0.0066,-0.0209,0.0154,0.0436,-0.0626,0.0055,0.0004,-0.0192,-0.0076,-0.0326,-0.0019,0.0153,0.0084,-0.0404,0.002,0.0167,-0.0608,0.0748,0.0103,0.0141,-0.025,-0.0099,0.0271,-0.0256,-0.0267,-0.0036,0.0471,-0.013,-0.0224,0.0049,0.0296,0.0188,0.0214,0.0264,0.026,-0.0171,-0.0537,-0.1897,0.0107,-0.0208,0.0083,0.0172,-0.0324,-0.0151,-0.0228,0.0957,0.0686,0.0308,-0.0299,-0.0458,0.0068,-0.0027,0.0518,0.0465,0.0199,-0.0256,0.0523,0.0311,-0.0031,-0.0635,-0.0879,0.0385,-0.0242,0.2291,0.0267,0.0482,-0.0265,0.0167,0.0278,-0.0814,-0.0811,0.0596,0.039,0.0114,-0.038,-0.0208,-0.0413,-0.0062,-0.0111,0.0153,-0.0905,-0.0504,-0.0579,0.0055,0.0026,-0.0316,-0.0189,0.0159,-0.0262,0.0231,-0.0394,-0.017,-0.0351,-0.0619,-0.0132,-0.042,0.0086,0.0426,-0.0631,0.0132,-0.0569,0.0514,-0.0378,-0.0609,0.0136,0.0135,-0.0564,-0.0505,0.0952,-0.0115,-0.0119,0.0348,0.0141,0.0107,-0.0208,-0.0658,-0.0399,0.063,-0.0565,0.0229,0.0595,0.0203,-0.0214,0.0538,-0.0277,0.0445,-0.0108,0.0538,0.0169,-0.0372,0.0281,0.0423,0.0126,-0.2809,0.0086,-0.0027,0.0341,0.0044,-0.0096,0.0664,0.0242,-0.021,-0.0223,0.0248,0.036,0.0549,0.0001,-0.0176,0.0562,0.0401,-0.0651,0.0052,-0.055,0.0181,0.0321,0.2117,-0.0206,0.0249,-0.0025,-0.0363,0.0304,0.051,-0.0326,-0.0204,-0.0163,0.0866,-0.0591,0.0859,0.0678,-0.007,0.0142,0.0098,0.0037,-0.0026,0.023,-0.045,0.0099,0.1051,0.029,-0.0154,-0.082,-0.002,0.0395,-0.0289,-0.0243,-0.0372,0.0094,-0.0017,0.0361,-0.0569,-0.0896,-0.0037,-0.0444,0.0244,-0.0676,-0.0242,0.0165,-0.0134]}
{"key":"[Online Learning to Rank with Feedback at the Top] We consider an online learning to rank setting in which, at each round, an oblivious adversary generates a list of $m$ documents, pertaining to a query, and the learner produces scores to rank the documents. The adversary then generates a relevance vector and the learner updates its ranker according to the feedback received. We consider the setting where the feedback is restricted to be the relevance levels of only the top $k$ documents in the ranked list for $k \\ll m$. However, the performance of learner is judged based on the unrevealed full relevance vectors, using an appropriate learning to rank loss function. We develop efficient algorithms for well known losses in the pointwise, pairwise and listwise families. We also prove that no online algorithm can have sublinear regret, with top-1 feedback, for any loss that is calibrated with respect to NDCG. We apply our algorithms on benchmark datasets demonstrating efficient online learning of a ranking function from highly restricted feedback.","layer":3,"vector":[-0.0689,-0.0225,0.0097,-0.0092,0.0008,0.0177,0.0175,0.0694,0.0172,-0.0059,0.0247,0.002,0.0394,0.0629,-0.0027,0.023,-0.0186,0.058,-0.0464,0.0014,0.0196,-0.0107,-0.0144,-0.0509,0.003,-0.0309,-0.0395,-0.0627,-0.0579,-0.2213,-0.0024,-0.0613,0.0615,0.0179,0.0004,-0.0106,-0.0137,0.0254,-0.0342,-0.0086,0.0462,0.032,-0.013,-0.0368,-0.0293,-0.0251,0.0234,-0.0162,-0.0064,-0.0587,0.0104,-0.022,-0.0239,0.0379,0.0747,0.0088,0.04,0.0478,0.0131,0.0593,0.0685,0.0439,-0.1337,0.0339,-0.006,0.0267,-0.0491,0.0083,-0.005,0.0736,0.0164,0.026,-0.0113,0.0618,0.0289,-0.0001,-0.004,-0.0342,-0.0232,0.0375,0.0065,-0.0297,-0.0649,-0.0032,-0.0138,-0.0798,0.0506,-0.0579,0.0498,0.0285,0.0251,-0.0095,0.0037,0.0431,-0.0429,-0.0342,0.0438,0.0364,-0.035,0.2089,-0.0423,0.0517,0.0137,-0.0561,0.0235,-0.0451,0.0396,-0.032,-0.0135,-0.0446,-0.0364,-0.0285,0.0729,-0.0426,0.0469,0.0462,0.0831,0.0595,-0.0158,-0.0239,-0.0212,0.0468,0.0733,-0.0352,0.0706,-0.0448,-0.0034,0.1176,-0.0017,0.0494,0.024,-0.0857,-0.0508,-0.0361,-0.0002,-0.021,-0.0048,-0.0117,0.0332,-0.0205,-0.0546,-0.0682,-0.014,-0.0501,-0.0227,0.1725,-0.0287,0.0444,-0.0589,-0.0426,-0.0081,-0.0084,-0.0239,-0.0241,0.004,0.0105,0.0507,0.0158,-0.0372,0.0042,-0.0258,-0.017,-0.0114,0.1114,0.0128,-0.0744,-0.0372,0.0242,-0.002,-0.0024,0.0263,0.0113,-0.0427,0.0575,0.0443,0.0017,-0.088,-0.0298,0.0443,0.0153,0.0225,-0.0626,-0.0049,0.044,0.0337,0.0088,-0.0061,-0.0253,0.0621,0.0313,-0.0317,-0.0206,-0.0511,-0.0057,-0.0137,-0.0327,-0.0011,-0.0185,0.022,-0.01,0.0026,-0.0332,-0.0333,0.0374,0.0255,0.0358,0.002,-0.0393,0.0339,0.0257,-0.0398,-0.0084,-0.0152,-0.0353,-0.006,-0.0279,0.0428,0.0338,-0.0389,0.0035,0.0441,-0.0362,-0.0084,-0.2159,-0.0541,-0.0224,0.0005,0.0496,-0.0669,0.0739,-0.0359,0.0165,0.067,0.0301,0.0021,-0.0016,0.0354,-0.021,0.0727,0.017,-0.0026,0.0111,-0.0151,-0.0106,0.0403,-0.0347,-0.0901,0.0501,-0.0076,0.2428,0.0444,0.0183,-0.0097,0.0337,0.0278,-0.0117,-0.0787,0.0649,0.0089,0.0425,-0.0162,-0.0494,-0.0056,-0.0034,0.0022,0.0279,-0.1214,-0.0452,-0.0323,-0.0389,0.0433,-0.0321,0.0461,-0.0026,-0.0189,0.0782,-0.0115,-0.0252,-0.0728,-0.0885,0.0056,-0.0463,0.0577,0.0099,-0.0563,0.01,-0.0841,0.0492,-0.0304,0.0014,-0.0131,0.0504,-0.0606,-0.0504,0.0018,-0.0058,0.0436,0.0348,0.0216,0.0385,-0.0604,-0.0746,-0.0551,0.062,-0.0173,0.0532,-0.0008,0.0293,0.0058,0.1114,0.0172,-0.0029,0.0289,-0.0119,0.0074,-0.0953,-0.0313,0.0457,-0.0042,-0.3132,0.008,0.0128,0.0676,-0.0239,0.0202,0.0561,0.0071,-0.0579,0.0461,0.0272,0.0753,-0.0095,-0.0173,0.0109,0.064,0.0259,-0.0716,0.0644,-0.0199,-0.0007,0.0585,0.2116,-0.0144,0.0435,0.0005,-0.0645,-0.0027,0.0636,-0.0113,0.0502,0.005,0.0686,-0.044,0.0238,0.0695,-0.0111,0.0443,0.0202,-0.0428,-0.0406,0.0093,-0.0593,0.0015,0.0956,0.0028,-0.0151,-0.033,-0.0146,0.0079,-0.0432,0.021,0.0202,0.0098,-0.023,0.0424,-0.0489,-0.0478,-0.0205,-0.0813,-0.0022,-0.0317,-0.0083,0.0076,0.0017]}
{"key":"[Deep Binary Reinforcement Learning for Scalable Verification] The use of neural networks as function approximators has enabled many advances in reinforcement learning (RL). The generalization power of neural networks combined with advances in RL algorithms has reignited the field of artificial intelligence. Despite their power, neural networks are considered black boxes, and their use in safety-critical settings remains a challenge. Recently, neural network verification has emerged as a way to certify safety properties of networks. Verification is a hard problem, and it is difficult to scale to large networks such as the ones used in deep reinforcement learning. We provide an approach to train RL policies that are more easily verifiable. We use binarized neural networks (BNNs), a type of network with mostly binary parameters. We present an RL algorithm tailored specifically for BNNs. After training BNNs for the Atari environments, we verify robustness properties.","layer":5,"vector":[-0.0652,-0.005,0.0082,-0.068,-0.0086,0.0734,0.0338,0.0363,0.0345,-0.04,-0.0162,-0.0831,0.0246,0.0619,-0.0087,-0.0046,-0.0072,0.0615,0.0085,0.0315,0.014,-0.0229,-0.0168,-0.0561,-0.0028,0.0159,-0.0479,-0.049,-0.0474,-0.2357,-0.0,-0.0261,0.0112,-0.0626,0.0081,0.0038,-0.0405,0.018,0.0008,-0.0194,0.051,0.0394,0.0009,-0.0541,-0.015,-0.0494,0.0007,-0.0419,0.0055,-0.0548,0.049,-0.0642,0.0409,0.0303,0.0462,0.0182,0.0432,0.1053,0.0596,0.0292,-0.0134,0.0391,-0.1792,0.032,0.023,0.0587,-0.0178,-0.0308,0.0084,0.0579,-0.022,0.0143,0.0098,0.0428,-0.0054,0.0158,-0.0026,-0.0756,0.0036,-0.0139,0.006,-0.0819,-0.0315,-0.0162,-0.0102,-0.0496,0.0167,-0.0202,-0.0098,-0.0101,-0.0505,0.0101,-0.0005,-0.0062,-0.0619,0.0108,0.0111,-0.0056,-0.0933,0.2101,-0.0055,0.0262,-0.0066,0.0097,0.0498,-0.0156,-0.0307,-0.0328,-0.0315,-0.0169,-0.0559,-0.0391,0.0747,0.0059,0.0168,0.0468,0.0493,0.0105,-0.0011,-0.0259,-0.0017,0.0062,0.056,-0.011,0.0115,-0.0612,-0.0004,0.1483,-0.0012,0.0446,0.0258,-0.041,-0.05,-0.0133,0.0476,0.0515,0.0042,-0.0004,0.0083,0.0217,-0.0448,-0.0049,0.0329,-0.1007,-0.056,0.054,-0.0038,0.0416,-0.0292,-0.014,-0.0409,-0.0111,-0.014,-0.0089,0.0198,0.0272,0.0451,0.0455,-0.0911,-0.0165,0.0087,-0.0378,0.0025,0.11,0.0036,-0.0483,-0.0319,0.0009,0.0172,-0.0303,0.0328,0.02,-0.0468,0.0165,0.0532,0.039,-0.0533,-0.0093,0.0047,0.0308,-0.0091,-0.0804,-0.0349,0.0611,0.057,-0.0009,0.0155,-0.0736,0.0352,0.0205,-0.0409,0.0314,-0.0738,-0.0052,-0.0225,-0.0319,0.002,-0.0292,0.0072,-0.0434,0.0128,-0.0106,-0.0428,0.0339,-0.04,0.0077,-0.0131,-0.0356,0.0641,0.0196,-0.0326,0.0503,0.0646,-0.0273,-0.0088,0.0252,0.0064,0.0055,-0.0061,0.0231,0.0688,-0.0154,-0.0347,-0.2232,-0.0181,-0.0226,-0.0154,0.1023,-0.078,0.0706,0.0005,-0.0078,0.0348,0.058,-0.0299,-0.0434,0.0348,-0.0249,0.0601,0.0556,0.0064,0.0111,0.0115,-0.0633,0.0164,-0.0502,-0.0801,0.0327,0.0218,0.2261,0.0141,0.0711,-0.0013,0.0152,0.0336,-0.0247,-0.0764,0.119,0.0108,0.0495,0.0229,-0.0095,-0.0484,-0.0396,0.0702,-0.0387,-0.1246,-0.0166,-0.0157,-0.0464,0.0478,-0.0505,-0.0015,0.035,-0.0156,0.0117,0.0333,-0.0025,-0.05,-0.0578,0.0201,-0.0382,0.024,0.0041,-0.0775,0.0063,-0.0381,0.0487,0.0204,0.0331,-0.0594,0.0429,0.0141,-0.007,0.0746,0.0179,-0.0361,0.0868,0.0223,-0.0075,-0.0311,-0.0346,-0.0031,0.0443,-0.0169,0.0722,0.051,0.0074,-0.0064,0.0662,0.0023,0.0363,-0.0038,-0.013,-0.0104,-0.082,-0.0156,0.0519,0.0309,-0.2911,0.0797,0.0252,0.0406,-0.0556,-0.0305,0.0591,0.0075,-0.0145,-0.0234,-0.0088,0.0507,0.0873,-0.0099,0.0256,0.0339,0.0652,-0.0376,0.0343,-0.0484,0.0267,0.0322,0.1979,-0.0338,0.0251,0.031,-0.0268,0.008,0.044,0.0018,-0.007,0.0058,0.0675,-0.047,0.0296,0.1084,-0.0362,0.0251,0.0494,0.007,-0.0441,0.0062,-0.0112,-0.0012,0.0855,-0.0203,-0.0239,-0.0145,-0.047,0.023,-0.0474,0.0172,0.0082,-0.0141,0.0227,0.0434,-0.0328,-0.0569,-0.0668,-0.0683,0.0177,-0.0634,0.0709,0.0118,-0.0562]}
{"key":"[Exploiting Chain Rule and Bayes' Theorem to Compare Probability Distributions] To measure the difference between two probability distributions, referred to as the source and target, respectively, we exploit both the chain rule and Bayes' theorem to construct conditional transport (CT), which is constituted by both a forward component and a backward one. The forward CT is the expected cost of moving a source data point to a target one, with their joint distribution defined by the product of the source probability density function (PDF) and a source-dependent conditional distribution, which is related to the target PDF via Bayes' theorem. The backward CT is defined by reversing the direction. The CT cost can be approximated by replacing the source and target PDFs with their discrete empirical distributions supported on mini-batches, making it amenable to implicit distributions and stochastic gradient descent-based optimization. When applied to train a generative model, CT is shown to strike a good balance between mode-covering and mode-seeking behaviors and strongly resist mode collapse. On a wide variety of benchmark datasets for generative modeling, substituting the default statistical distance of an existing generative adversarial network with CT is shown to consistently improve the performance. PyTorch code is provided.","layer":2,"vector":[-0.0637,-0.0223,0.022,-0.0124,0.0235,0.0403,0.012,-0.0544,0.0192,0.0213,-0.0047,-0.0328,0.0251,0.0507,0.0016,0.0477,0.0029,0.0348,-0.0481,0.0016,0.0762,-0.0468,0.0205,-0.0638,0.0381,0.0246,0.0062,-0.0359,-0.0475,-0.2467,0.0077,-0.0622,-0.0073,-0.0533,-0.0534,-0.0356,-0.0552,0.0438,0.0208,0.0432,0.0176,0.0416,-0.0067,-0.0469,-0.0191,-0.021,-0.0524,0.0132,-0.0436,-0.0567,0.0455,-0.0206,0.0448,0.0305,0.024,0.0484,0.062,0.0618,0.0388,0.0585,-0.011,0.0668,-0.1752,0.0286,0.0544,-0.0052,-0.0688,0.0018,-0.0085,0.0411,-0.0212,0.0099,0.0112,0.0576,0.0317,-0.0442,0.0005,-0.0235,-0.0661,0.0206,0.0389,-0.0118,-0.0025,0.0033,-0.0305,-0.0339,0.0305,-0.0554,0.0256,-0.004,-0.0329,-0.0106,-0.0575,0.0543,-0.0657,-0.0089,0.0149,-0.0058,-0.0253,0.1996,-0.0383,0.0415,0.0623,-0.0017,0.0159,-0.03,-0.0156,0.0084,-0.0365,0.0087,0.014,-0.0112,0.0182,-0.0447,0.0125,-0.0148,0.0477,0.0397,-0.0338,-0.0261,-0.0607,0.0192,0.0605,-0.0158,0.001,-0.0682,0.0228,0.1501,0.0395,0.0374,0.0556,-0.0426,-0.0468,-0.0064,0.0391,-0.0049,0.0058,-0.0209,-0.0164,-0.0008,-0.0154,-0.0088,-0.0227,-0.0649,-0.0181,0.1146,0.0191,0.0368,-0.027,-0.0448,0.023,0.0013,0.0213,-0.0413,-0.0169,0.0621,-0.0288,0.0594,-0.0258,0.0279,-0.0335,-0.036,-0.0375,0.117,-0.0215,-0.0871,-0.024,0.0206,0.0118,0.0161,-0.0058,0.0377,-0.0057,0.0188,0.069,0.0207,-0.0646,-0.0233,-0.014,0.0392,0.0318,-0.0698,-0.017,0.0162,0.037,-0.0474,0.0175,-0.0153,0.015,0.0662,0.0332,-0.0097,0.0083,-0.0301,-0.0164,-0.0382,-0.0166,-0.0008,0.0088,-0.0279,0.0254,0.0042,-0.0502,-0.0035,-0.0556,0.0126,-0.0036,-0.0067,0.0378,0.0275,-0.0464,-0.0087,0.0729,-0.0144,-0.0388,-0.0017,-0.0158,0.0615,0.0137,0.0411,-0.0161,-0.044,-0.0022,-0.2264,0.0362,0.0417,0.0046,0.068,-0.085,-0.0196,0.0248,0.0144,0.0601,0.0801,-0.0429,-0.0142,0.0368,-0.0394,0.0303,0.0303,0.0509,-0.0025,-0.0227,-0.0112,0.0149,-0.013,-0.127,0.0427,0.0078,0.1946,0.0386,0.0654,0.0069,0.0205,0.033,-0.0191,-0.077,0.064,0.0024,0.0902,-0.0299,-0.0346,0.0127,-0.0059,0.0265,-0.0071,-0.1206,-0.0315,-0.0773,-0.0262,0.03,-0.0845,0.0316,0.0214,-0.0341,0.0714,-0.0336,-0.0069,-0.0335,-0.0976,0.0318,-0.0521,0.0459,0.0024,-0.0026,0.024,-0.0552,0.0274,-0.0042,-0.0221,-0.0683,0.0467,-0.0059,-0.0151,0.0738,0.0015,0.0521,0.0696,0.004,-0.0026,-0.008,-0.0519,-0.0617,0.0472,-0.0111,0.0511,0.0309,0.014,0.0031,0.0874,0.0111,0.0225,-0.0169,-0.0189,0.0169,-0.0661,-0.0149,0.0145,-0.0042,-0.3181,0.0306,0.0144,0.0597,-0.0561,0.0096,0.1028,0.0209,-0.0315,-0.0558,-0.0154,0.0458,0.0274,0.0058,-0.0066,-0.0071,0.0547,-0.0779,0.0288,-0.0436,0.0249,0.0508,0.219,-0.0416,-0.0095,-0.0271,0.0085,0.0358,0.0201,-0.0405,-0.0004,0.0098,0.072,-0.0722,0.0201,0.0624,-0.0367,0.0592,0.0151,-0.0413,-0.0235,-0.0127,-0.0651,0.0152,0.0983,0.0133,0.0128,-0.0048,0.0007,0.0013,-0.0557,0.0897,-0.0104,0.0176,0.0312,0.0178,-0.0571,-0.0521,-0.0086,-0.0412,0.0428,-0.0522,-0.0126,0.0261,-0.0189]}
{"key":"[Oversampling Log Messages Using a Sequence Generative Adversarial Network for Anomaly Detection and Classification] Dealing with imbalanced data is one of the main challenges in machine/deep learning algorithms for classification. This issue is more important with log message data as it is typically very imbalanced and negative logs are rare. In this paper, a model is proposed to generate text log messages using a SeqGAN network. Then features are extracted using an Autoencoder and anomaly detection is done using a GRU network. The proposed model is evaluated with two imbalanced log data sets, namely BGL and Openstack. Results are presented which show that oversampling and balancing data increases the accuracy of anomaly detection and classification.","layer":2,"vector":[-0.0353,0.0169,0.0153,-0.0583,0.0424,-0.0168,0.0187,-0.001,-0.0113,-0.0288,-0.019,-0.0154,-0.0128,0.0864,0.024,0.0037,0.0201,-0.0112,-0.0222,0.0144,0.032,-0.034,0.0115,-0.042,0.017,0.0313,-0.0387,-0.0608,-0.083,-0.2333,0.0412,-0.0316,0.0565,-0.0217,0.0046,-0.0277,-0.0431,0.0428,-0.0226,0.0648,0.0425,0.0301,-0.0167,-0.0758,-0.0166,-0.0645,-0.0185,-0.0557,0.0109,-0.0412,0.0472,-0.0427,0.0219,0.0231,-0.0038,-0.0123,0.0356,0.021,0.0262,0.0874,0.0475,0.0706,-0.1362,0.053,0.0358,0.037,-0.0328,-0.0275,0.0431,0.0284,-0.0237,0.0128,-0.0105,0.0455,-0.025,0.0507,-0.0046,-0.0657,-0.0228,0.0285,0.0368,-0.0031,-0.0161,-0.0003,0.003,-0.0584,0.0009,-0.036,0.0645,-0.0188,0.0147,0.0075,0.0048,0.0792,-0.0346,-0.037,0.0005,0.0513,-0.0689,0.2117,-0.0264,0.0108,0.0664,-0.0216,0.0259,0.0083,-0.0593,-0.0453,-0.0279,-0.0613,-0.0028,-0.0088,0.0367,0.0013,0.0212,-0.0078,0.0625,0.0228,-0.0495,-0.0027,0.0052,0.0159,0.0579,-0.0794,0.0552,-0.0418,0.0199,0.139,0.036,-0.0091,-0.0066,0.0038,-0.0598,-0.0133,-0.0125,0.0023,-0.0207,0.0228,0.0131,-0.0003,-0.0198,-0.0677,0.0168,-0.0447,-0.0523,0.0958,-0.0407,0.0142,-0.0279,-0.0399,-0.0305,0.0224,-0.0024,-0.0605,0.0479,0.0381,0.0323,0.0442,-0.0359,0.0185,0.0134,-0.0548,-0.0324,0.109,0.0273,-0.1171,-0.0202,0.0192,-0.0008,-0.0284,0.0559,0.0172,-0.038,0.0543,0.0411,0.0173,-0.0229,-0.0405,-0.0494,0.0193,0.0148,-0.0513,-0.0347,0.06,0.0746,-0.0784,0.0024,-0.0225,0.0345,0.0184,-0.0402,0.0038,-0.0174,-0.0235,-0.0315,-0.0285,0.0102,-0.009,0.0056,-0.0683,0.026,0.0408,-0.0427,-0.0241,-0.0243,0.0336,-0.0181,-0.0291,0.0251,0.0242,-0.0246,0.0231,0.0548,-0.0132,-0.0174,-0.012,-0.0123,0.0654,-0.0319,0.0232,0.0214,-0.0122,-0.0296,-0.2934,-0.0078,0.0399,-0.0535,0.0063,-0.0673,0.0201,-0.0027,0.046,0.0714,0.0609,0.0158,0.013,-0.0085,-0.0037,0.0994,-0.0028,0.016,-0.0074,0.0106,-0.0332,0.0159,0.0359,-0.094,0.0375,-0.0007,0.1753,0.0394,0.0661,-0.0092,0.0412,0.0148,-0.0252,-0.0719,0.1026,0.0563,0.045,0.0405,-0.0717,-0.042,-0.0346,0.0442,-0.0047,-0.0903,-0.0106,-0.0236,-0.0462,-0.0155,-0.0589,0.046,0.086,-0.0025,0.051,0.056,0.033,-0.0543,-0.1298,0.0416,-0.0458,-0.0204,0.0116,-0.048,0.0221,-0.0412,0.0484,0.0317,-0.0606,-0.054,0.0328,0.0088,-0.0022,0.1076,-0.011,0.0174,0.0419,0.0029,0.0345,-0.0485,-0.0551,-0.0237,0.0441,-0.0242,0.0297,0.0253,0.016,0.0119,0.0497,0.0254,0.0272,0.0006,0.0213,0.0514,-0.0199,-0.0515,0.0178,0.0123,-0.2765,0.0215,-0.0451,0.0699,-0.0068,0.002,0.0159,0.0384,-0.0481,-0.0169,0.0139,-0.0024,-0.013,-0.0637,0.0034,0.0354,0.0235,-0.0634,0.0251,-0.032,0.0025,0.0292,0.2326,-0.0419,0.0257,0.0041,-0.0128,0.0044,0.0228,-0.0075,0.0337,-0.004,0.1067,0.0023,0.0883,0.0878,-0.0104,0.0023,0.0201,-0.0413,-0.0267,0.0195,-0.0306,-0.0127,0.0862,-0.017,-0.056,-0.0321,0.0328,0.0626,-0.0507,-0.0023,-0.0218,0.034,0.0065,0.0437,-0.053,-0.0586,-0.0383,-0.0523,0.0323,-0.0549,-0.018,-0.0073,-0.0389]}
{"key":"[Deep Equilibrium Optical Flow Estimation] Many recent state-of-the-art (SOTA) optical flow models use finite-step recurrent update operations to emulate traditional algorithms by encouraging iterative refinements toward a stable flow estimation. However, these RNNs impose large computation and memory overheads, and are not directly trained to model such stable estimation. They can converge poorly and thereby suffer from performance degradation. To combat these drawbacks, we propose deep equilibrium (DEQ) flow estimators, an approach that directly solves for the flow as the infinite-level fixed point of an implicit layer (using any black-box solver), and differentiates through this fixed point analytically (thus requiring $O(1)$ training memory). This implicit-depth approach is not predicated on any specific model, and thus can be applied to a wide range of SOTA flow estimation model designs. The use of these DEQ flow estimators allows us to compute the flow faster using, e.g., fixed-point reuse and inexact gradients, consumes $4\\sim6\\times$ times less training memory than the recurrent counterpart, and achieves better results with the same computation budget. In addition, we propose a novel, sparse fixed-point correction scheme to stabilize our DEQ flow estimators, which addresses a longstanding challenge for DEQ models in general. We test our approach in various realistic settings and show that it improves SOTA methods on Sintel and KITTI datasets with substantially better computational and memory efficiency.","layer":2,"vector":[-0.0516,-0.0379,0.0539,-0.0249,0.045,0.0635,0.0006,-0.0109,0.0353,0.0009,0.0316,-0.0636,0.0289,0.0219,0.0001,0.0226,0.0166,0.0468,-0.0363,0.0077,0.0419,-0.0709,-0.0279,-0.0452,0.0425,-0.0339,-0.0332,-0.0431,-0.0367,-0.294,0.0114,0.0121,-0.0054,-0.0095,0.0271,-0.0441,-0.0872,0.0422,-0.0512,0.0359,-0.0061,0.0266,-0.0331,-0.0393,-0.0494,-0.0188,-0.0421,-0.0104,-0.013,-0.0301,0.0184,-0.043,0.024,0.0055,0.0008,0.0245,0.0562,0.0303,0.0328,0.0315,0.0334,0.0181,-0.1715,0.051,0.0401,0.0165,-0.0102,-0.0547,0.006,0.0186,-0.0285,0.0631,0.0248,-0.018,0.0228,-0.0272,0.0325,0.0023,0.0208,-0.0002,0.0458,-0.023,0.0175,-0.0298,0.0122,-0.0299,0.0362,-0.0184,0.0192,-0.0141,-0.0581,-0.0153,-0.0297,0.007,-0.048,0.0091,-0.003,-0.0135,-0.0227,0.2072,-0.029,0.0455,0.0316,0.0075,0.0446,-0.0127,-0.0423,0.0123,-0.0029,0.0163,-0.0253,0.0173,0.089,-0.0469,0.0323,-0.0066,0.0395,0.0206,-0.024,0.0251,0.0072,0.0166,0.0308,0.0041,0.0059,-0.095,0.0385,0.1296,0.0252,0.0649,0.0532,-0.0073,-0.0486,-0.0184,0.0228,0.0233,0.0333,-0.0175,0.0084,-0.0394,-0.0831,-0.0437,0.0165,-0.0928,-0.0673,0.1254,-0.0158,0.0764,-0.0202,-0.0805,-0.0288,0.0347,-0.0335,-0.0148,0.0018,0.0372,0.0268,0.0609,-0.093,0.0316,-0.0343,-0.067,-0.0565,0.0408,-0.0033,-0.0589,-0.0194,-0.0128,-0.014,0.0068,0.0141,0.0183,-0.0265,-0.0309,0.0694,0.0227,-0.0913,0.0245,0.0127,0.0312,0.0058,-0.0491,-0.0525,0.0184,0.0303,-0.0358,-0.0204,-0.0419,0.0001,0.0597,-0.0431,-0.0072,-0.0008,0.0266,0.0214,0.0208,-0.0382,-0.0114,0.0155,-0.0412,0.0112,-0.0178,-0.0618,-0.0091,0.0041,0.0118,-0.0286,0.0263,0.0199,0.0515,-0.0629,-0.0051,0.0636,-0.053,-0.0041,0.0162,-0.0102,-0.0049,-0.0233,0.0087,0.0707,-0.0626,-0.059,-0.2456,-0.0013,0.0151,-0.0412,0.0812,-0.06,0.0389,0.0053,0.0584,0.0964,0.0354,0.0118,0.0239,0.0024,0.0208,0.0407,0.0208,0.0345,0.003,0.0051,-0.029,0.0272,-0.0315,-0.0829,0.0541,0.0156,0.2284,0.0056,0.0295,0.0093,0.0283,0.0212,-0.0211,-0.0788,0.0036,0.0278,0.0955,0.0163,0.0122,-0.0264,-0.0123,0.0039,-0.025,-0.0927,-0.0486,0.0047,-0.0087,0.0243,-0.0481,0.0037,-0.0155,-0.0544,0.0654,-0.0215,-0.0084,0.014,-0.0865,0.0368,-0.0409,0.0337,-0.024,-0.0419,0.0241,-0.0527,0.0835,0.0272,0.0155,-0.082,0.0379,-0.0144,-0.0122,0.0754,-0.0112,0.0277,0.0616,0.0481,0.0554,0.0017,-0.0253,-0.065,0.0771,-0.0318,0.0443,0.0743,0.0214,0.0668,0.1011,-0.0586,-0.0034,-0.0336,0.005,0.0074,-0.0634,-0.018,0.0123,0.0221,-0.2815,-0.0011,0.0002,0.0242,-0.0239,0.0256,0.0802,0.03,-0.0351,-0.0068,-0.0447,0.0588,0.03,0.0156,-0.0012,-0.0024,0.1061,-0.0503,0.0643,-0.0449,0.0001,0.0148,0.1953,-0.0372,-0.0298,0.0326,-0.0627,0.0108,0.049,-0.0408,0.0074,0.0358,0.0716,-0.0426,0.0092,0.0359,-0.015,0.0426,0.0553,0.0262,0.0102,0.0096,0.0012,-0.0187,0.0564,0.0146,-0.029,0.0138,-0.0132,0.0018,-0.0108,0.0025,0.0385,0.0108,0.0326,0.0097,-0.0991,-0.0432,-0.0826,-0.0198,0.0331,-0.0578,-0.0195,-0.0336,-0.0107]}
{"key":"[Identification of Nonlinear Dynamic Systems Using Type-2 Fuzzy Neural Networks -- A Novel Learning Algorithm and a Comparative Study] In order to achieve faster and more robust convergence (especially under noisy working environments), a sliding mode theory-based learning algorithm has been proposed to tune both the premise and consequent parts of type-2 fuzzy neural networks in this paper. Differently from recent studies, where sliding mode control theory-based rules are proposed for only the consequent part of the network, the developed algorithm applies fully sliding mode parameter update rules for both the premise and consequent parts of the type-2 fuzzy neural networks. In addition, the responsible parameter for sharing the contributions of the lower and upper parts of the type-2 fuzzy membership functions is also tuned. Moreover, the learning rate of the network is updated during the online training. The stability of the proposed learning algorithm has been proved by using an appropriate Lyapunov function. Several comparisons have been realized and shown that the proposed algorithm has faster convergence speed than the existing methods such as gradient-based and swarm intelligence-based methods. Moreover, the proposed learning algorithm has a closed form, and it is easier to implement than the other existing methods.","layer":3,"vector":[-0.0805,-0.0195,0.0246,0.0136,0.0125,0.0417,0.0364,0.024,0.0451,-0.0285,-0.0032,-0.046,0.0328,0.0603,-0.0221,-0.0154,0.0088,0.0433,0.0141,0.0216,0.0194,0.0018,-0.0221,-0.0145,0.0101,-0.0002,-0.0552,0.0102,-0.0365,-0.2062,-0.0073,-0.0498,0.0055,0.0132,0.002,-0.0295,-0.0599,0.0459,-0.0079,0.0371,0.0269,0.0247,0.0161,-0.1141,0.0332,-0.0492,-0.0029,-0.0256,0.0023,-0.0538,0.0267,-0.0067,0.0395,0.0118,0.0203,0.078,0.0262,0.0518,0.0408,0.0421,0.0319,0.0322,-0.1968,0.0322,0.0422,0.052,-0.0555,-0.0409,0.052,0.0331,-0.0258,0.0434,0.0155,0.0155,0.0154,-0.0082,-0.0342,-0.0561,0.0207,-0.0075,0.0209,-0.0497,-0.0371,-0.0454,-0.0067,-0.0242,0.052,-0.0683,0.0497,0.034,-0.0174,0.0199,0.0399,0.0522,-0.0408,-0.0006,0.0112,0.0346,-0.0469,0.1721,-0.0677,-0.0155,0.0803,-0.0355,0.0083,-0.0277,-0.0299,-0.0357,-0.0469,-0.0005,-0.028,-0.0226,0.0238,-0.0131,-0.0135,0.0682,0.0069,0.0486,-0.0015,-0.0252,0.0064,0.0138,0.0607,0.003,0.0363,-0.0651,-0.0022,0.112,0.0064,0.0353,0.039,-0.0355,-0.0279,-0.0384,-0.0066,0.0514,0.0225,0.0261,0.0145,-0.0137,-0.012,-0.0847,0.0575,-0.1341,-0.0491,0.0829,-0.0163,-0.0023,-0.0115,-0.0299,-0.0153,0.0198,-0.0786,-0.0163,0.0273,0.0729,0.0744,0.0781,-0.0444,0.0152,-0.037,-0.0332,-0.0771,0.0544,0.0195,-0.1083,-0.0178,-0.0217,0.0422,-0.0218,0.0184,0.0265,-0.0621,0.0006,0.096,0.0399,0.0083,-0.0093,0.0311,0.0289,0.0686,-0.0319,-0.042,-0.0037,0.0302,-0.0591,-0.0268,-0.0756,0.0223,0.0222,-0.0903,-0.0077,-0.0022,0.0112,-0.0226,-0.0304,-0.0027,0.0062,-0.0069,-0.0633,0.0219,-0.0273,-0.0465,0.0221,-0.0365,-0.012,0.0227,0.013,-0.0046,0.0333,-0.0248,0.0144,0.0651,-0.0327,-0.0551,0.025,0.0322,0.0292,-0.0328,0.0539,0.055,0.0213,-0.098,-0.1853,0.0043,-0.0266,-0.0058,0.0595,-0.0436,0.0293,-0.0196,0.0713,0.0045,0.0916,0.06,0.0074,0.0201,-0.0159,0.0383,0.0487,0.0132,-0.0212,0.0119,-0.0124,0.0381,-0.0091,-0.0758,0.0656,-0.0054,0.1473,-0.0545,0.0755,-0.0487,0.0435,0.0381,0.0053,-0.0621,0.0802,0.0026,0.0228,-0.0678,-0.0425,-0.0448,0.0146,-0.0012,-0.0052,-0.0613,0.002,-0.0391,-0.0499,0.0421,-0.0688,-0.0363,0.0804,0.0141,0.0575,-0.0277,-0.021,0.0032,-0.098,0.0138,-0.0513,-0.0294,-0.0155,-0.0773,-0.057,-0.0137,0.0727,-0.0064,-0.0455,-0.0117,0.0552,-0.027,-0.0288,0.1449,0.0446,0.0157,0.0668,0.0124,0.0096,-0.0323,-0.0225,-0.024,0.014,-0.0162,0.0766,0.0494,0.0183,-0.0285,0.0614,-0.0172,0.0679,-0.0457,-0.0198,-0.006,-0.0662,-0.0107,0.0145,0.0365,-0.295,0.0403,0.0175,0.0354,-0.0347,-0.0023,0.0301,-0.0151,-0.0203,-0.0181,0.0,0.0739,0.053,0.016,0.0501,0.0246,0.0131,-0.0357,0.0793,-0.0994,0.0444,0.0861,0.2226,-0.0454,0.047,-0.0236,-0.0215,-0.0037,-0.0015,-0.0048,0.0061,-0.0041,0.0972,-0.109,0.0114,0.0283,-0.0623,0.0254,0.01,-0.0169,-0.0135,0.0343,-0.1013,0.0376,0.1119,-0.0286,-0.0144,-0.0645,-0.0327,0.0304,-0.0339,-0.0115,-0.0163,0.0107,0.0749,0.0084,-0.0306,-0.0315,-0.0754,-0.0168,0.0243,-0.0584,0.0006,0.0057,0.0434]}
{"key":"[Detecting low-complexity unobserved causes] We describe a method that infers whether statistical dependences between two observed variables X and Y are due to a \"direct\" causal link or only due to a connecting causal path that contains an unobserved variable of low complexity, e.g., a binary variable. This problem is motivated by statistical genetics. Given a genetic marker that is correlated with a phenotype of interest, we want to detect whether this marker is causal or it only correlates with a causal one. Our method is based on the analysis of the location of the conditional distributions P(Y|x) in the simplex of all distributions of Y. We report encouraging results on semi-empirical data.","layer":0,"vector":[-0.0603,0.0108,0.0179,0.0008,0.0577,-0.0005,0.072,0.0632,0.046,-0.0478,0.0589,-0.0751,0.0431,0.0574,0.0025,0.0275,-0.017,0.0578,-0.0616,0.023,0.0115,-0.0409,0.0262,-0.0524,0.0283,0.0341,-0.0127,-0.0105,-0.0281,-0.2408,-0.0039,-0.0657,0.0649,-0.0132,0.039,-0.0494,-0.0533,0.0486,0.0421,0.0308,0.0319,0.0368,-0.0168,-0.0561,-0.0202,-0.0754,-0.0198,0.0283,-0.0297,-0.0436,-0.0321,-0.0413,0.0305,0.0485,0.0416,0.0701,0.0709,0.0427,0.0146,0.0528,0.0103,0.0589,-0.1955,0.0184,0.135,0.0443,-0.0262,-0.0172,0.0196,0.0465,-0.018,0.0399,-0.0169,0.0455,-0.0151,-0.0107,0.0132,-0.0164,-0.0639,0.0284,0.0259,-0.0118,-0.0012,0.0194,-0.0278,-0.079,0.025,-0.0654,0.0477,0.0125,-0.0488,-0.0226,0.0176,0.0098,-0.0677,-0.0219,0.0647,-0.0165,0.0016,0.1729,-0.087,0.0271,-0.0075,0.0294,0.0508,-0.033,-0.0239,-0.056,-0.0305,-0.0026,0.0207,-0.0316,0.0454,-0.0377,0.0213,-0.0213,0.0532,0.015,0.0186,-0.0067,-0.0111,-0.0067,0.0297,-0.0092,0.0064,-0.0597,-0.0045,0.1475,0.033,-0.0226,0.0344,-0.0225,-0.0517,-0.0136,-0.0308,-0.0071,-0.0049,0.0086,-0.0151,-0.0055,-0.0471,-0.0538,0.0327,-0.087,-0.054,0.1361,-0.0057,0.0283,-0.034,-0.022,-0.0529,0.0037,-0.0383,-0.076,0.008,0.0207,0.0294,0.022,-0.0327,0.0626,-0.0411,-0.07,-0.0311,0.0868,0.0082,-0.0725,0.0069,0.0143,0.0246,0.0144,0.0382,0.0356,-0.0518,0.0125,0.0501,-0.0056,-0.0737,0.0416,0.0009,0.0318,0.0796,-0.0692,-0.0139,0.0578,0.0576,-0.0014,-0.0191,0.0174,0.0028,0.0318,0.0016,-0.0086,-0.0487,-0.0065,-0.0115,0.0094,-0.0033,-0.0124,0.0251,-0.0165,-0.0102,-0.0184,-0.0372,0.0362,-0.0684,0.0165,-0.0013,0.0158,0.0651,0.0324,0.0121,0.0163,0.0327,0.0265,-0.0502,0.0378,0.0197,0.026,0.0564,0.0548,0.0066,-0.0841,-0.0356,-0.2636,-0.0488,-0.0074,0.0389,0.0167,-0.0578,0.0042,0.0243,0.0325,0.078,0.0341,-0.0373,-0.0379,0.0264,-0.0319,0.0401,-0.0028,0.0227,-0.0055,0.0154,-0.036,0.0031,0.0054,-0.0486,0.0596,-0.002,0.2208,0.026,0.0166,-0.0268,0.0117,-0.0035,-0.0311,-0.0948,0.0428,0.0505,0.0042,-0.0314,-0.0286,-0.0196,-0.0569,0.023,-0.0426,-0.0254,-0.0312,-0.0276,0.003,-0.0072,-0.0588,0.0591,0.0209,-0.0368,0.0688,0.0264,-0.0038,-0.0308,-0.1065,0.0291,-0.0485,0.043,0.0007,-0.0177,0.0252,-0.0855,0.0321,-0.0189,-0.0106,-0.0681,0.0223,-0.0035,0.0071,0.1213,-0.0194,-0.0366,0.0161,0.012,0.0202,-0.0278,-0.0624,-0.0459,0.0741,-0.0465,0.0161,0.0555,-0.0354,-0.0118,0.0898,0.0032,0.053,-0.0052,-0.0061,-0.024,-0.0164,-0.0429,-0.0092,-0.0018,-0.2638,0.04,0.011,-0.031,-0.0288,0.0154,0.057,0.0419,-0.0225,-0.0029,0.0405,0.0101,0.0472,-0.0123,-0.014,0.0497,0.0534,-0.0988,0.0549,-0.0736,0.0024,0.0581,0.1911,-0.024,0.0365,0.0124,-0.035,0.0459,0.0155,-0.006,0.0233,0.0571,0.0866,-0.0881,0.0122,0.044,-0.0467,-0.0175,-0.0023,-0.0261,-0.0077,0.0232,-0.0667,-0.0393,0.1282,-0.0087,-0.0142,-0.0225,0.0165,0.0391,-0.029,0.0011,-0.0184,0.0262,0.0031,0.0153,-0.0393,-0.0184,-0.0097,-0.0571,-0.0014,-0.0357,-0.0175,0.0398,-0.0157]}
{"key":"[ST-ExpertNet: A Deep Expert Framework for Traffic Prediction] Recently, forecasting the crowd flows has become an important research topic, and plentiful technologies have achieved good performances. As we all know, the flow at a citywide level is in a mixed state with several basic patterns (e.g., commuting, working, and commercial) caused by the city area functional distributions (e.g., developed commercial areas, educational areas and parks). However, existing technologies have been criticized for their lack of considering the differences in the flow patterns among regions since they want to build only one comprehensive model to learn the mixed flow tensors. Recognizing this limitation, we present a new perspective on flow prediction and propose an explainable framework named ST-ExpertNet, which can adopt every spatial-temporal model and train a set of functional experts devoted to specific flow patterns. Technically, we train a bunch of experts based on the Mixture of Experts (MoE), which guides each expert to specialize in different kinds of flow patterns in sample spaces by using the gating network. We define several criteria, including comprehensiveness, sparsity, and preciseness, to construct the experts for better interpretability and performances. We conduct experiments on a wide range of real-world taxi and bike datasets in Beijing and NYC. The visualizations of the expert's intermediate results demonstrate that our ST-ExpertNet successfully disentangles the city's mixed flow tensors along with the city layout, e.g., the urban ring road structure. Different network architectures, such as ST-ResNet, ConvLSTM, and CNN, have been adopted into our ST-ExpertNet framework for experiments and the results demonstrates the superiority of our framework in both interpretability and performances.","layer":1,"vector":[-0.0119,-0.0391,0.0108,0.016,0.0436,0.042,0.0405,-0.0173,0.0546,0.0024,-0.0385,-0.0236,0.0178,0.0646,0.02,0.0003,0.0058,0.036,-0.0125,-0.0315,0.0339,-0.0393,-0.0116,-0.084,0.026,0.0212,0.001,-0.0507,-0.0558,-0.2237,0.0238,-0.038,0.0286,0.009,0.0051,-0.0577,-0.0201,0.0884,-0.027,0.0555,0.0139,0.0228,-0.0044,-0.0031,0.0001,-0.0118,-0.0308,0.0106,-0.0503,-0.0256,0.0593,-0.0703,-0.0164,0.0175,0.0222,0.0205,0.0194,0.0253,0.0692,0.032,0.0299,0.0279,-0.1551,0.0575,0.0302,0.0198,-0.0222,0.0038,0.0132,0.0248,0.0067,0.0521,-0.0058,0.0472,0.0288,-0.0055,-0.0066,-0.0064,-0.0085,0.0191,0.0275,-0.0292,-0.0228,-0.0096,0.0267,0.0095,0.012,-0.0058,0.018,-0.0326,-0.0649,-0.0119,-0.0568,0.0354,-0.0362,0.0261,0.0425,-0.0009,-0.0017,0.1958,-0.0512,0.0533,0.0417,0.0201,0.0066,-0.0538,-0.0145,-0.0049,-0.0341,0.0306,-0.0152,-0.0296,0.0731,-0.0085,0.0548,0.0031,0.0552,0.084,-0.049,0.0243,-0.0238,-0.0221,0.0377,-0.0077,0.0214,-0.0673,0.0083,0.154,0.022,0.0338,0.0565,0.0161,-0.0989,-0.0084,0.0206,-0.0136,0.0211,-0.0031,-0.0173,-0.0406,-0.0432,-0.0076,-0.0008,-0.0925,-0.0512,0.083,-0.0413,0.0034,-0.0332,-0.012,-0.0276,-0.0089,-0.0496,-0.0387,-0.0037,0.0448,0.0659,0.0754,-0.0571,0.0424,-0.0189,-0.0393,-0.0794,0.0698,0.0078,-0.1158,-0.0244,0.0084,0.0084,-0.0584,0.0358,-0.0178,-0.0133,0.0589,0.0872,0.0474,-0.0793,0.031,0.0032,0.013,0.0294,-0.0446,-0.0186,0.0696,0.0427,-0.0603,-0.0269,-0.0681,-0.012,0.0583,-0.0304,0.0167,-0.008,0.0057,-0.0246,0.0017,-0.0356,0.0132,0.0142,-0.016,0.0028,-0.0025,-0.0377,-0.0011,-0.0581,0.0233,-0.014,0.0112,0.0127,0.0357,-0.0195,-0.0253,0.0553,-0.0182,0.0306,-0.0022,0.0263,0.0335,-0.0069,0.0,0.0286,-0.0278,-0.0929,-0.2217,-0.0164,0.0411,0.0003,0.0744,-0.0248,0.0401,0.0305,0.0153,0.0909,0.08,-0.0168,-0.0199,0.0246,0.0142,0.0562,0.0077,0.0372,-0.0044,-0.0009,-0.0097,0.0492,-0.0504,-0.1013,0.0283,0.0138,0.176,0.0265,0.0383,-0.0265,0.0236,0.0318,-0.022,-0.1215,0.0581,0.0152,0.1435,-0.0018,-0.0262,-0.0438,-0.0543,-0.0016,-0.0135,-0.0845,-0.0516,-0.0515,-0.0013,0.0296,-0.0524,-0.0049,0.0147,-0.0546,0.0614,-0.0157,0.0238,-0.0223,-0.0877,0.0738,-0.0033,0.0077,-0.0346,-0.0292,0.0052,-0.0812,0.0479,0.0255,-0.0798,-0.0801,-0.0124,0.0049,-0.0269,0.1024,-0.0135,-0.0238,0.0852,-0.0172,0.0305,-0.0055,-0.0277,-0.0167,0.0853,-0.036,0.0586,0.0404,0.0494,0.0239,0.0536,-0.0361,0.015,-0.0247,0.0398,-0.0036,-0.0349,-0.0534,-0.0045,-0.0231,-0.3142,0.0615,-0.0134,0.0246,-0.0424,0.0016,0.0538,-0.0042,-0.01,-0.0205,0.0474,0.0472,0.0648,-0.0387,-0.0131,-0.0319,0.0925,-0.0109,0.0242,-0.035,0.0211,0.0613,0.2309,-0.063,0.0393,0.0044,-0.0634,-0.0252,0.046,-0.0622,0.0195,-0.0363,0.079,-0.063,0.0377,0.0352,-0.0349,0.0554,0.0003,-0.0037,-0.0043,-0.0083,-0.0153,-0.0298,0.0773,-0.0073,-0.0549,-0.0318,0.0281,0.0256,-0.0064,-0.0239,-0.0559,-0.0074,0.0151,0.0306,-0.0386,-0.0471,-0.0396,-0.0034,0.0262,-0.0935,0.0119,0.0,-0.0343]}
{"key":"[On the Optimal Memorization Power of ReLU Neural Networks] We study the memorization power of feedforward ReLU neural networks. We show that such networks can memorize any $N$ points that satisfy a mild separability assumption using $\\tilde{O}\\left(\\sqrt{N}\\right)$ parameters. Known VC-dimension upper bounds imply that memorizing $N$ samples requires $\\Omega(\\sqrt{N})$ parameters, and hence our construction is optimal up to logarithmic factors. We also give a generalized construction for networks with depth bounded by $1 \\leq L \\leq \\sqrt{N}$, for memorizing $N$ samples using $\\tilde{O}(N/L)$ parameters. This bound is also optimal up to logarithmic factors. Our construction uses weights with large bit complexity. We prove that having such a large bit complexity is both necessary and sufficient for memorization with a sub-linear number of parameters.","layer":3,"vector":[-0.0481,-0.0099,0.029,-0.0578,-0.0058,0.0312,0.0514,0.0244,0.0286,-0.021,-0.0277,-0.0218,0.0686,0.099,0.0144,0.0239,-0.0063,0.0427,-0.05,0.0134,0.0822,-0.0569,-0.0202,-0.0265,0.0171,-0.0292,-0.0279,-0.0484,-0.0163,-0.2299,0.0214,-0.02,0.0469,-0.0649,-0.0217,-0.028,-0.0208,0.0142,-0.0348,0.0146,0.0382,0.0529,0.0017,-0.0691,-0.009,-0.0367,-0.0168,-0.0032,0.0309,-0.0415,0.0283,-0.0141,0.0152,0.0357,0.0254,0.0272,0.0401,0.0032,0.0594,0.0302,0.0355,0.0352,-0.1549,0.0301,0.0019,-0.0257,-0.049,-0.0382,-0.0212,0.0789,-0.0173,0.0441,-0.018,-0.0029,0.0012,0.0248,-0.0115,-0.0361,0.0002,-0.0025,0.0117,-0.0135,-0.0317,-0.0053,-0.0015,-0.0726,0.0051,-0.0475,0.0031,-0.0065,-0.0014,-0.0172,-0.0164,-0.0184,-0.0463,-0.0508,0.0692,0.0583,-0.0712,0.2024,-0.0313,0.0453,0.0459,-0.0507,0.0036,-0.0291,-0.0096,-0.0199,-0.0516,-0.0173,-0.0063,-0.0589,0.0278,-0.0271,0.0636,0.0346,0.033,0.0332,-0.0173,0.0229,-0.0175,0.0086,0.0158,-0.0129,0.0049,-0.0613,-0.0223,0.1071,0.0442,0.0746,0.0454,-0.0181,-0.0065,-0.018,0.0236,0.0331,0.0193,-0.0227,0.0134,-0.0115,-0.0462,-0.0169,0.0634,-0.044,-0.0602,0.1239,-0.0365,0.0419,-0.0654,-0.0502,-0.0276,-0.0031,0.0011,-0.0416,0.0439,-0.0014,0.0188,0.0192,-0.0791,0.0201,-0.0013,-0.026,0.015,0.1244,-0.011,-0.051,-0.0163,-0.0141,0.0073,-0.0145,0.07,0.0553,-0.0335,-0.0022,0.046,0.0377,-0.0886,-0.0376,-0.0186,0.0157,0.0082,-0.0562,-0.0177,0.0395,0.0359,-0.0339,0.0183,-0.0203,0.0175,0.0828,-0.0024,0.0198,-0.0648,-0.0155,-0.0632,-0.0244,0.0255,-0.0032,0.0082,-0.0381,-0.0199,0.0189,-0.0541,0.013,0.0546,0.0347,0.0104,-0.0089,0.037,0.0375,-0.0405,-0.0229,0.0691,-0.0576,-0.0348,-0.0007,-0.0008,0.0194,0.0097,0.0225,0.0233,-0.089,-0.0892,-0.2467,-0.033,0.0185,-0.0483,0.1216,-0.1008,0.0559,0.0342,0.0594,0.0457,0.0129,-0.0314,-0.0217,0.015,0.006,0.0434,0.0494,0.0271,-0.0295,0.021,-0.0414,-0.0065,-0.0271,-0.0237,0.0596,-0.0129,0.253,-0.0171,0.0609,-0.0346,0.0479,0.0396,-0.0008,-0.0429,0.0689,-0.0003,0.0458,0.024,0.0096,-0.0563,-0.0042,0.026,0.0172,-0.0931,-0.0717,-0.0465,-0.0351,-0.0338,-0.0395,0.0237,0.0439,-0.0808,0.0528,0.0337,0.0171,-0.0398,-0.0944,0.0012,-0.0777,0.0336,-0.0248,-0.0908,0.0084,-0.0545,0.0419,0.034,-0.0335,-0.0075,0.0446,-0.0072,-0.0138,0.0323,0.0163,-0.0265,0.0593,-0.0243,0.0338,-0.0453,-0.0373,-0.014,0.0673,-0.0764,0.0245,0.0091,0.0064,0.0037,0.0993,-0.0185,0.0114,-0.0019,0.0051,0.0177,-0.0658,-0.0051,-0.007,0.001,-0.2975,0.0578,0.0096,0.0219,-0.0019,0.0263,0.0414,0.0194,-0.0266,0.0294,0.0238,0.0672,0.0512,-0.0029,-0.0209,0.0462,0.0685,-0.0027,0.0659,-0.0484,-0.0063,0.0425,0.1843,-0.0522,0.0178,0.0184,-0.0108,-0.0101,0.0554,-0.0165,0.0229,0.0056,0.0948,-0.0438,0.0696,0.0969,-0.0401,0.0287,0.0414,-0.0147,-0.012,-0.026,-0.0697,-0.0344,0.1102,0.0021,0.01,-0.0481,-0.0166,0.0405,-0.0079,-0.0116,0.0255,0.0059,0.0493,0.0407,-0.0391,-0.0512,-0.0369,-0.0319,0.0299,-0.0608,0.0158,0.0119,-0.0285]}
{"key":"[Linear Convergence of Gradient and Proximal-Gradient Methods Under the Polyak-\\L{}ojasiewicz Condition] In 1963, Polyak proposed a simple condition that is sufficient to show a global linear convergence rate for gradient descent. This condition is a special case of the \\L{}ojasiewicz inequality proposed in the same year, and it does not require strong convexity (or even convexity). In this work, we show that this much-older Polyak-\\L{}ojasiewicz (PL) inequality is actually weaker than the main conditions that have been explored to show linear convergence rates without strong convexity over the last 25 years. We also use the PL inequality to give new analyses of randomized and greedy coordinate descent methods, sign-based gradient descent methods, and stochastic gradient methods in the classic setting (with decreasing or constant step-sizes) as well as the variance-reduced setting. We further propose a generalization that applies to proximal-gradient methods for non-smooth optimization, leading to simple proofs of linear convergence of these methods. Along the way, we give simple convergence results for a wide variety of problems in machine learning: least squares, logistic regression, boosting, resilient backpropagation, L1-regularization, support vector machines, stochastic dual coordinate ascent, and stochastic variance-reduced gradient methods.","layer":0,"vector":[-0.0387,-0.0307,0.0168,-0.0168,-0.0112,0.0262,-0.0302,0.0389,0.057,-0.0264,0.0382,-0.0344,0.023,0.0735,0.0173,0.0355,0.027,0.0312,-0.0664,0.021,0.0264,-0.0514,-0.0162,-0.0828,0.0818,0.0135,-0.0104,-0.0212,-0.0101,-0.2858,0.0035,-0.0532,0.0541,-0.0501,0.0342,-0.0025,-0.0409,0.0671,-0.0514,0.0301,-0.0368,0.02,-0.0641,-0.0375,-0.0009,-0.0537,-0.0434,-0.0431,-0.052,-0.0092,0.0197,-0.0129,0.0436,0.0242,0.0094,0.0109,0.0099,0.0229,0.0592,0.045,-0.001,0.0,-0.2153,0.0349,0.0417,-0.0115,-0.0186,-0.0435,0.0351,0.0887,0.0339,0.023,0.0284,0.0047,-0.0357,0.0237,-0.0179,-0.0386,-0.0187,0.0093,0.0342,-0.0166,-0.0261,-0.0207,-0.0625,-0.0253,0.0228,-0.056,0.0318,0.0233,0.0024,-0.0155,-0.0006,0.0068,-0.0514,-0.0035,0.0457,0.051,-0.043,0.1883,-0.0737,0.1043,0.0403,-0.0159,0.0209,-0.0436,-0.0427,-0.0214,-0.0068,0.0011,-0.0492,0.0082,0.0408,-0.0038,-0.0096,-0.0024,0.0527,0.069,-0.0039,-0.0053,-0.0047,-0.0275,0.0352,0.006,0.0468,-0.0456,0.0055,0.1268,0.0375,0.0487,0.0243,-0.0269,-0.0476,-0.0624,0.0084,0.0204,0.0015,0.035,0.0557,-0.0016,-0.0269,-0.0717,0.0455,-0.0707,-0.0388,0.1253,-0.0329,0.0315,-0.0107,-0.0468,-0.0056,0.0052,-0.0374,0.0091,0.0215,0.0171,0.0337,0.0229,-0.0324,0.0337,-0.0483,-0.061,-0.0034,0.0942,-0.0127,-0.0674,-0.0185,-0.0105,-0.0072,-0.0223,0.0633,0.0415,-0.0201,0.0114,0.0588,0.0222,-0.1011,-0.0403,-0.0061,0.009,-0.0032,-0.022,-0.039,0.0364,0.0426,-0.0076,0.0244,-0.0417,0.0438,0.0195,-0.0385,-0.0287,-0.0076,-0.0308,-0.0075,-0.0265,-0.0383,0.0127,0.0503,-0.0123,0.0126,0.0128,-0.0474,0.0517,0.0077,0.0168,0.0064,-0.0359,0.0583,0.0488,-0.0508,-0.0527,0.0545,-0.0287,-0.0429,0.0297,0.0165,0.0337,-0.0315,0.0592,0.0157,-0.0053,-0.0597,-0.1967,-0.0199,0.0078,0.0197,0.0449,-0.0602,0.027,-0.0362,0.0415,0.074,0.0636,0.0006,-0.0276,0.0497,0.018,0.0594,0.0494,0.0197,0.0217,-0.0227,-0.0103,-0.0158,0.0037,-0.0235,0.0623,-0.0215,0.1904,-0.0058,0.0442,-0.0378,0.0485,0.032,0.0014,-0.083,0.0489,0.0479,0.0756,-0.032,-0.0349,-0.0042,-0.0084,0.0144,0.0222,-0.0847,-0.0786,-0.0582,-0.0498,0.0507,-0.0372,0.0176,0.0334,-0.0054,0.0583,-0.0486,0.0099,-0.0498,-0.1145,0.0512,-0.0305,0.0374,-0.0082,-0.0971,-0.008,-0.0257,0.0771,0.0379,0.0007,0.0037,0.0226,-0.0115,-0.0163,0.094,0.0033,-0.0111,0.0497,0.027,0.0419,0.0138,-0.0371,-0.009,0.0556,-0.019,0.0449,0.0002,0.0287,0.0235,0.0573,-0.0326,-0.0172,-0.016,-0.1108,0.0189,-0.0671,-0.0078,0.0383,0.0278,-0.3064,0.0255,0.0015,0.0008,-0.0604,0.0456,0.048,-0.006,-0.066,0.0173,0.0315,0.057,0.0198,-0.0077,0.0487,0.0438,0.0366,-0.0233,0.0599,-0.0822,0.0194,0.0434,0.1945,-0.0391,-0.0028,0.0134,0.0045,-0.007,0.0333,-0.057,-0.0339,-0.0063,0.0452,-0.0245,0.0696,0.1176,-0.0264,0.0382,0.0112,-0.0418,-0.0054,-0.0011,-0.021,-0.0314,0.0899,-0.0246,-0.0071,-0.0152,0.0264,0.0084,-0.0566,-0.0158,0.0259,-0.0162,0.007,0.0171,-0.0688,-0.0352,-0.0671,-0.0487,0.046,-0.0695,-0.0549,0.0033,-0.0074]}
{"key":"[DeepFall -- Non-invasive Fall Detection with Deep Spatio-Temporal Convolutional Autoencoders] Human falls rarely occur; however, detecting falls is very important from the health and safety perspective. Due to the rarity of falls, it is difficult to employ supervised classification techniques to detect them. Moreover, in these highly skewed situations it is also difficult to extract domain specific features to identify falls. In this paper, we present a novel framework, \\textit{DeepFall}, which formulates the fall detection problem as an anomaly detection problem. The \\textit{DeepFall} framework presents the novel use of deep spatio-temporal convolutional autoencoders to learn spatial and temporal features from normal activities using non-invasive sensing modalities. We also present a new anomaly scoring method that combines the reconstruction score of frames across a temporal window to detect unseen falls. We tested the \\textit{DeepFall} framework on three publicly available datasets collected through non-invasive sensing modalities, thermal camera and depth cameras and show superior results in comparison to traditional autoencoder methods to identify unseen falls.","layer":4,"vector":[-0.038,-0.0091,0.0419,-0.0302,0.0421,0.0359,0.0611,0.0548,0.0408,-0.0413,0.0057,-0.0431,0.0345,0.0833,0.0226,-0.0106,-0.0016,0.0703,-0.0471,0.009,0.0283,-0.0167,0.0208,-0.0196,0.0054,0.0184,0.0025,-0.0703,-0.0805,-0.1858,0.0167,-0.046,0.0618,-0.0235,0.0175,-0.0319,-0.0545,0.0396,-0.0256,0.0008,0.0088,0.0059,0.0033,-0.0523,0.0253,-0.0395,0.0028,-0.0252,0.0004,-0.0286,0.0178,-0.0325,0.0258,0.0445,0.0148,-0.0167,0.0798,0.0339,0.0598,0.072,0.0658,0.0377,-0.1672,0.0267,0.055,-0.0045,-0.0378,-0.0093,0.066,-0.026,-0.0072,0.0576,-0.0085,0.034,-0.0267,-0.0056,-0.0052,-0.0333,0.0016,0.0057,0.0748,0.0042,-0.028,-0.0139,-0.0047,-0.0987,0.04,-0.0551,0.0627,0.0065,-0.0427,-0.0226,-0.0618,0.0358,-0.0726,0.0036,0.0278,0.0299,-0.0681,0.2181,-0.0974,0.0791,0.0592,0.0118,0.035,-0.0467,-0.0012,-0.0714,0.0046,-0.0105,0.0166,0.0054,0.0652,-0.0046,0.0035,-0.0156,0.0086,0.0597,-0.0001,0.0128,0.008,0.0126,0.0898,-0.0524,0.0081,-0.0883,0.0287,0.1275,0.055,0.0168,0.026,-0.0326,-0.0538,-0.0586,0.0227,0.0284,0.0252,0.0153,0.0137,-0.0148,-0.0178,-0.0705,0.0275,-0.0757,-0.0123,0.0743,-0.0611,0.0001,-0.0423,-0.0202,-0.0251,0.026,-0.041,-0.0116,0.0749,0.0156,0.0432,0.0324,-0.0722,-0.0044,-0.0047,-0.0472,-0.0261,0.0973,-0.0052,-0.1196,-0.022,-0.0164,0.019,-0.0487,0.0286,0.0394,-0.026,-0.0182,0.0936,0.0445,-0.1027,-0.01,0.0088,0.0039,0.0111,-0.0259,-0.0007,0.0533,0.0736,-0.032,0.0109,-0.0455,0.0295,0.0644,-0.0438,-0.0184,-0.0502,-0.0267,0.0083,0.029,-0.0358,-0.0142,-0.0061,0.0104,0.0219,-0.0048,0.0169,0.0295,0.0342,0.0299,-0.0162,-0.002,0.0313,0.0052,-0.0155,0.0257,0.0516,-0.0165,-0.0003,-0.0313,-0.0117,0.0356,-0.0495,-0.004,0.0358,-0.0459,-0.0326,-0.2305,-0.0043,0.069,-0.064,0.0127,-0.0343,-0.0186,-0.0444,0.0631,0.0697,0.0901,-0.0627,-0.0054,-0.0042,0.0328,0.0644,0.0145,0.0095,-0.0545,-0.0019,-0.0229,-0.0153,-0.029,-0.097,0.0138,-0.0089,0.2046,0.0289,0.0502,-0.015,0.0065,0.0236,0.0006,-0.0933,0.0376,0.0353,0.0402,0.0097,-0.0759,-0.064,-0.0123,0.0742,0.0229,-0.0877,-0.0265,-0.0169,-0.025,-0.0076,-0.0231,-0.0036,0.047,-0.0359,0.0427,-0.0141,0.013,-0.0207,-0.0713,0.0324,-0.0358,-0.0094,-0.029,-0.0638,0.0399,-0.1009,0.0757,0.022,-0.0704,-0.0646,0.0214,-0.0325,-0.0033,0.109,0.0057,-0.0091,0.0628,0.0014,0.041,-0.0254,-0.0145,-0.024,0.0723,-0.0166,0.0044,0.0188,0.0165,0.0379,0.0668,-0.0267,0.0247,-0.0515,0.0258,0.0187,-0.0855,-0.0311,0.011,-0.0234,-0.3082,0.0731,0.0175,-0.0242,-0.017,-0.0268,0.0485,0.0495,-0.0218,0.0221,-0.0504,0.0037,0.0034,-0.0005,0.0013,0.0409,0.0529,-0.0282,0.0618,-0.0546,0.017,0.0662,0.1846,-0.0316,-0.0177,0.0299,0.0165,-0.0256,0.0408,-0.0047,-0.0284,0.0073,0.0579,-0.0651,0.0478,0.0755,-0.0201,0.0777,0.0044,-0.0179,-0.0086,-0.0352,-0.009,-0.0713,0.0953,-0.0021,-0.017,0.0016,0.0212,0.0478,-0.0233,-0.0326,-0.0187,0.0154,0.0185,0.0375,-0.0435,0.0069,-0.0383,-0.0301,0.0294,-0.0589,-0.0482,-0.0217,-0.0346]}
{"key":"[Adaptive Stress Testing for Autonomous Vehicles] This paper presents a method for testing the decision making systems of autonomous vehicles. Our approach involves perturbing stochastic elements in the vehicle's environment until the vehicle is involved in a collision. Instead of applying direct Monte Carlo sampling to find collision scenarios, we formulate the problem as a Markov decision process and use reinforcement learning algorithms to find the most likely failure scenarios. This paper presents Monte Carlo Tree Search (MCTS) and Deep Reinforcement Learning (DRL) solutions that can scale to large environments. We show that DRL can find more likely failure scenarios than MCTS with fewer calls to the simulator. A simulation scenario involving a vehicle approaching a crosswalk is used to validate the framework. Our proposed approach is very general and can be easily applied to other scenarios given the appropriate models of the vehicle and the environment.","layer":1,"vector":[-0.0885,-0.0135,0.0761,-0.0164,0.0147,0.0987,0.0529,0.0216,0.046,-0.0107,0.0116,-0.012,0.0415,0.0757,0.0291,0.0066,-0.0216,0.031,-0.0114,-0.0364,0.0166,-0.0196,-0.0296,-0.0808,-0.0299,0.0366,-0.0143,-0.0203,-0.0457,-0.2414,-0.0004,-0.0569,0.0363,-0.0265,-0.006,-0.004,-0.0671,0.0606,0.0135,0.0205,0.0354,-0.0071,-0.0423,-0.0686,-0.0155,-0.0473,0.0168,-0.0353,0.0135,-0.0423,-0.0057,-0.052,0.0592,-0.0345,0.0168,0.0097,0.0735,0.0599,0.0578,0.0236,-0.0176,0.0151,-0.1841,0.0744,0.0565,0.0503,-0.0569,-0.0146,0.0539,0.071,-0.0603,0.0404,0.0048,0.0666,0.0435,-0.0154,0.0244,-0.0904,0.0409,-0.012,-0.0112,-0.0464,-0.0631,0.0186,-0.0116,-0.0826,-0.019,-0.0309,0.0746,0.0068,-0.0163,0.0411,0.0175,0.0153,-0.0631,0.0088,0.0159,-0.0294,-0.0409,0.218,-0.0158,0.0616,0.0264,-0.0043,0.0126,0.0229,-0.0273,-0.0493,-0.0427,0.0267,0.0377,-0.0496,0.0696,-0.0099,-0.0374,0.0131,0.07,0.0384,-0.0071,0.0115,-0.0127,-0.0105,0.0568,-0.0232,0.0412,-0.1142,0.0245,0.1573,0.0095,-0.006,0.0406,-0.0707,-0.0566,-0.0216,0.015,0.0041,-0.0165,0.0177,-0.0144,0.028,-0.0447,-0.0369,-0.0064,-0.0953,-0.0437,0.0629,0.0001,0.0552,-0.0479,-0.0491,0.0202,-0.017,0.008,-0.0248,0.0336,0.0357,0.0191,0.0409,-0.0515,0.0199,-0.001,0.0113,0.007,0.1252,-0.0015,-0.0589,-0.0237,0.0149,0.0227,-0.0078,0.0282,0.0429,-0.0434,0.0287,0.0455,-0.0025,-0.0855,0.046,-0.0093,0.0245,0.0125,-0.0288,-0.0362,0.0072,0.0351,-0.0383,-0.012,-0.06,0.0161,0.0075,0.0158,-0.0206,-0.0277,-0.0058,-0.04,-0.01,-0.012,-0.0327,0.0174,-0.0094,0.0095,-0.0234,-0.0468,-0.005,-0.0456,0.0013,-0.0172,-0.0215,0.0646,0.0119,-0.0394,0.0071,0.0801,-0.0315,-0.0119,-0.0236,-0.0119,0.0333,-0.0059,0.0478,0.0216,-0.0097,-0.0034,-0.232,-0.0272,-0.0186,0.0159,0.0322,-0.0251,0.0257,-0.0387,0.011,0.0272,0.0637,-0.0389,0.0199,0.0297,0.0264,0.0506,-0.0133,0.0046,-0.0281,0.0061,-0.0007,0.0312,-0.0197,-0.0959,0.0149,-0.0076,0.2162,0.0095,0.0119,0.0131,-0.0019,0.0504,-0.0087,-0.0689,0.0702,0.004,0.0573,-0.0082,-0.0015,-0.0618,-0.0228,0.0036,-0.0399,-0.0866,-0.005,-0.046,-0.0261,0.0848,-0.0427,-0.0035,0.0009,-0.0254,0.0426,-0.0174,-0.0244,-0.0417,-0.0934,0.006,-0.0086,-0.0343,-0.0035,0.0024,0.0204,-0.0205,0.0508,-0.024,-0.0025,-0.087,-0.0149,-0.0066,0.017,0.0701,0.0226,0.0052,0.0745,-0.0014,0.0247,-0.0105,-0.0224,-0.0348,0.0569,-0.0176,0.0276,0.056,0.0602,-0.0003,0.0436,-0.045,0.0415,-0.0392,0.0191,0.0228,-0.0481,-0.0466,0.0585,0.0082,-0.2757,0.0646,-0.007,0.024,-0.0503,-0.0186,0.0395,0.0204,-0.0576,-0.0201,0.0137,0.1018,0.0658,0.0062,0.0229,0.0145,0.0475,0.007,0.0463,-0.0674,0.0032,0.038,0.2317,-0.0614,0.0417,0.0528,-0.055,0.0073,0.0436,-0.0416,0.0034,0.0011,0.043,-0.0719,0.0631,0.0899,-0.0308,0.0267,0.042,-0.0088,-0.022,0.0232,0.0394,-0.0313,0.089,-0.0107,-0.0237,-0.0451,-0.0136,0.0345,-0.0018,0.0017,-0.0325,-0.0485,0.0498,0.0271,-0.0376,-0.0771,-0.0259,-0.0835,0.0305,-0.0528,0.0277,-0.0098,0.0236]}
{"key":"[A Temporal-Pattern Backdoor Attack to Deep Reinforcement Learning] Deep reinforcement learning (DRL) has made significant achievements in many real-world applications. But these real-world applications typically can only provide partial observations for making decisions due to occlusions and noisy sensors. However, partial state observability can be used to hide malicious behaviors for backdoors. In this paper, we explore the sequential nature of DRL and propose a novel temporal-pattern backdoor attack to DRL, whose trigger is a set of temporal constraints on a sequence of observations rather than a single observation, and effect can be kept in a controllable duration rather than in the instant. We validate our proposed backdoor attack to a typical job scheduling task in cloud computing. Numerous experimental results show that our backdoor can achieve excellent effectiveness, stealthiness, and sustainability. Our backdoor's average clean data accuracy and attack success rate can reach 97.8% and 97.5%, respectively.","layer":2,"vector":[-0.055,-0.0457,-0.0098,-0.0285,0.0088,0.031,0.0433,-0.0281,0.0267,0.0011,0.0001,-0.0129,0.0523,0.0212,-0.0052,-0.0151,-0.0441,0.0397,-0.0419,-0.0096,0.0187,-0.08,-0.0286,-0.0794,0.0098,0.0316,-0.0186,-0.0522,-0.0477,-0.2273,0.001,-0.0533,0.0096,-0.0274,0.0394,-0.0126,-0.0487,0.0525,-0.0187,-0.0006,0.0226,0.0338,0.0151,-0.0534,-0.0262,-0.0599,-0.0399,-0.028,-0.0211,-0.0235,0.0254,0.0003,0.0131,0.0387,0.0511,-0.0287,0.0966,0.0665,0.0435,0.0376,0.0219,0.0381,-0.1408,0.036,0.0698,0.0507,-0.0354,-0.0417,0.0457,0.0136,-0.0388,0.0328,-0.0161,0.0324,0.0135,0.0339,-0.0408,-0.0298,0.0023,0.0075,0.0281,-0.0457,-0.0013,-0.0063,-0.0411,-0.0671,0.0229,-0.0248,0.0643,0.0202,-0.0024,-0.0141,0.0004,0.0148,-0.0583,0.0091,0.0394,0.0256,-0.0805,0.2146,-0.0393,0.0396,-0.0153,-0.0061,0.0719,-0.0077,-0.0227,-0.0484,-0.0194,0.0405,-0.029,-0.0179,0.055,-0.0062,0.01,0.015,0.076,0.0129,0.0057,0.0095,-0.0158,-0.0035,0.0784,-0.0404,0.0182,-0.0826,0.0272,0.1527,0.0204,0.046,0.0094,-0.0875,-0.0069,0.0055,0.04,0.0764,-0.0023,0.0226,0.0442,-0.0359,-0.071,0.0095,0.0297,-0.0676,-0.0084,0.0801,0.0371,0.0127,-0.029,-0.0467,-0.051,0.0225,0.0064,-0.0393,0.0504,0.0409,0.0051,0.0879,-0.0323,0.0042,-0.026,-0.0361,-0.0382,0.117,0.0066,-0.1005,-0.0159,-0.0109,-0.0105,-0.036,-0.0012,0.0312,-0.0286,-0.0057,0.0501,0.0184,-0.0782,-0.0063,-0.0347,0.0305,0.0016,-0.1031,-0.0571,0.0112,0.0506,-0.0556,0.0328,-0.009,-0.0045,-0.002,-0.0525,0.0192,-0.0293,-0.0071,-0.0073,-0.0745,0.0034,-0.0072,-0.0154,-0.0489,-0.0155,-0.0265,-0.0456,0.0136,0.0019,0.0177,-0.0094,-0.0164,0.0533,-0.0072,-0.0169,0.0342,0.0035,-0.033,-0.0284,-0.004,0.0054,0.0472,-0.0024,0.0055,0.0157,0.0376,-0.037,-0.2079,-0.0161,-0.0648,-0.0218,0.0496,-0.0364,0.0306,-0.0163,0.0462,0.0199,0.0452,-0.0245,-0.0263,0.0159,0.0237,0.0517,0.0771,0.0401,-0.0036,0.0083,0.0423,-0.0132,-0.0331,-0.103,0.0374,0.0243,0.2527,0.0354,0.0482,-0.042,0.0236,0.0134,-0.022,-0.1294,0.0666,0.0093,0.0802,-0.0061,-0.0032,-0.0753,-0.049,0.0325,-0.023,-0.0889,-0.0172,-0.0397,-0.0148,0.0651,-0.0842,-0.0143,0.0554,-0.0274,0.0392,0.0307,0.0124,-0.0575,-0.0761,0.0195,-0.0142,0.0493,-0.0011,-0.0365,-0.0225,-0.0783,0.0608,-0.0048,0.0033,-0.077,0.0085,-0.0324,-0.0083,0.0725,-0.0155,-0.0081,0.0662,0.0033,0.0171,-0.0244,-0.0628,-0.0105,0.0796,-0.0354,-0.0263,0.0562,0.049,-0.0239,0.0339,0.0146,0.0491,-0.0249,0.0012,0.0062,-0.0631,-0.0226,0.065,0.0289,-0.3021,0.0664,0.0044,0.0473,0.0045,0.0207,0.0471,0.008,-0.0744,-0.0216,-0.0228,0.0687,0.0075,0.0038,0.0362,-0.009,0.1241,-0.0469,0.0339,-0.0379,0.0152,0.0452,0.2296,-0.0258,0.0285,0.0052,-0.0324,0.0348,0.056,-0.0221,0.0181,0.0025,0.0592,-0.0388,-0.0053,0.0501,0.0067,0.0416,0.036,0.023,-0.0273,0.013,-0.0166,0.028,0.0677,0.0007,-0.0039,-0.0489,0.0331,0.0512,-0.0164,-0.0083,-0.0093,0.0112,0.0154,0.0597,-0.0509,-0.0421,-0.043,-0.0081,0.0101,-0.0547,0.0326,0.0015,-0.0027]}
{"key":"[Bounding the Effects of Continuous Treatments for Hidden Confounders] Observational studies often seek to infer the causal effect of a treatment even though both the assigned treatment and the outcome depend on other confounding variables. An effective strategy for dealing with confounders is to estimate a propensity model that corrects for the relationship between covariates and assigned treatment. Unfortunately, the confounding variables themselves are not always observed, in which case we can only bound the propensity, and therefore bound the magnitude of causal effects. In many important cases, like administering a dose of some medicine, the possible treatments belong to a continuum. Sensitivity models, which are required to tie the true propensity to something that can be estimated, have been explored for binary treatments. We propose one for continuous treatments. We develop a framework to compute ignorance intervals on the partially identified dose-response curves, enabling us to quantify the susceptibility of an inference to hidden confounders. We show with simulations and three real-world observational studies that our approach can give non-trivial bounds on causal effects from continuous treatments in the presence of hidden confounders.","layer":3,"vector":[-0.0126,0.0084,0.0461,0.0174,0.0568,0.0295,0.0369,0.0465,0.0177,-0.0427,0.0709,-0.0337,0.0297,0.0432,-0.007,0.0081,-0.0491,0.0516,-0.0724,0.0615,-0.0053,-0.0665,0.0128,-0.0299,0.0096,0.0327,-0.0549,-0.0377,-0.028,-0.2256,-0.0325,-0.0233,0.0363,-0.0523,0.0057,0.0029,-0.026,0.0378,-0.0033,0.0253,0.0245,0.0381,-0.0142,-0.0428,-0.0748,-0.0372,-0.0247,-0.001,-0.0214,-0.0162,0.0405,-0.0246,0.0284,0.074,0.014,0.0355,0.014,0.0402,0.0279,0.0496,0.0274,-0.0006,-0.1959,0.0559,0.0655,0.0298,-0.0434,-0.0311,0.0492,0.0893,-0.0271,0.0876,0.0155,0.0616,0.0386,-0.0353,0.0075,-0.0072,-0.0611,0.0112,0.0377,-0.0164,-0.0014,0.0088,-0.03,-0.0821,0.0329,-0.0598,0.025,-0.0002,-0.0498,-0.016,-0.0456,-0.0287,-0.1057,0.0193,0.0647,0.0323,-0.0533,0.1797,-0.0123,0.0251,-0.0222,-0.0064,0.0626,-0.0079,-0.035,-0.0126,-0.0031,0.0391,0.0049,-0.0247,0.0459,-0.0361,0.0,0.0561,0.0425,0.0097,0.0348,-0.0342,-0.0281,0.0373,0.049,0.002,0.0122,-0.0432,0.0293,0.1366,0.0305,-0.0201,0.0637,-0.0741,-0.0031,-0.0266,0.0104,-0.01,0.0085,0.0086,0.0012,0.0029,-0.0142,-0.0429,-0.011,-0.123,-0.0779,0.14,-0.0092,0.0564,-0.0555,-0.0257,-0.0012,0.0446,-0.0306,-0.0072,0.0146,-0.0246,0.0036,0.0224,-0.0274,0.0015,-0.0497,-0.0472,-0.0482,0.0906,0.0064,-0.0111,-0.0293,0.0304,0.016,0.0054,0.0315,0.0705,-0.0117,0.0106,0.073,-0.0292,-0.0423,0.0243,0.0098,0.0065,0.0288,-0.0489,-0.0122,0.0322,-0.0012,-0.0195,-0.0255,0.0091,0.0272,0.0693,-0.0312,-0.0186,-0.0707,0.0045,-0.0165,-0.0453,-0.0569,-0.0249,0.018,-0.019,0.0203,0.0217,-0.0268,0.0125,0.0014,0.0265,-0.0495,0.005,0.0937,0.0171,-0.0287,0.0567,0.0756,-0.0124,-0.0454,0.0571,0.0291,0.0006,0.0095,0.043,0.0073,-0.0221,-0.0169,-0.2687,-0.046,0.0076,-0.0205,0.024,-0.0734,-0.0036,-0.0033,0.0141,0.0485,0.0212,-0.0144,-0.0795,0.0367,-0.035,0.0171,0.0166,0.0445,-0.0344,0.0103,-0.0224,0.0085,-0.0618,-0.0586,0.0183,0.019,0.2166,0.019,-0.0134,-0.0313,0.0011,0.0197,-0.0304,-0.1395,0.053,0.0078,-0.0034,0.0,-0.0248,-0.0347,-0.0097,0.0245,-0.0413,-0.0488,-0.064,0.0075,-0.0033,0.048,-0.0574,0.0351,-0.0023,-0.0035,0.055,0.0318,0.0268,-0.0234,-0.134,0.0332,-0.0603,0.0366,-0.0016,-0.0036,0.0068,-0.0915,0.0362,-0.013,-0.0245,-0.0575,0.0512,-0.0105,0.0066,0.1159,-0.0156,-0.0101,0.0157,0.0395,0.055,-0.0564,-0.0461,-0.007,0.081,-0.039,0.0215,0.0335,-0.0184,-0.0024,0.0509,0.0039,0.0561,-0.0351,0.0241,0.012,-0.0536,0.0244,0.0037,0.0148,-0.2706,0.0185,0.0353,-0.0013,-0.0456,0.025,0.0281,0.0195,-0.0554,-0.0564,0.0195,0.0618,0.0732,0.0063,0.0042,0.0086,0.0685,-0.0582,0.034,-0.0413,0.033,0.0243,0.2093,-0.0299,0.0229,0.0483,-0.0465,0.0349,0.0343,0.0165,0.0091,0.0443,0.0434,-0.0411,0.0368,-0.0022,-0.0416,0.0501,0.0095,0.0052,0.008,0.0434,-0.0409,0.0026,0.1111,-0.0552,-0.0501,-0.1065,0.0167,-0.0304,-0.047,0.0222,0.0428,0.0514,0.0439,-0.0167,-0.0457,-0.0158,-0.0166,-0.0322,-0.0404,-0.0343,-0.0547,0.0329,-0.0112]}
{"key":"[Heterogeneous Network Representation Learning: A Unified Framework with Survey and Benchmark] Since real-world objects and their interactions are often multi-modal and multi-typed, heterogeneous networks have been widely used as a more powerful, realistic, and generic superclass of traditional homogeneous networks (graphs). Meanwhile, representation learning (\\aka~embedding) has recently been intensively studied and shown effective for various network mining and analytical tasks. In this work, we aim to provide a unified framework to deeply summarize and evaluate existing research on heterogeneous network embedding (HNE), which includes but goes beyond a normal survey. Since there has already been a broad body of HNE algorithms, as the first contribution of this work, we provide a generic paradigm for the systematic categorization and analysis over the merits of various existing HNE algorithms. Moreover, existing HNE algorithms, though mostly claimed generic, are often evaluated on different datasets. Understandable due to the application favor of HNE, such indirect comparisons largely hinder the proper attribution of improved task performance towards effective data preprocessing and novel technical design, especially considering the various ways possible to construct a heterogeneous network from real-world application data. Therefore, as the second contribution, we create four benchmark datasets with various properties regarding scale, structure, attribute/label availability, and \\etc.~from different sources, towards handy and fair evaluations of HNE algorithms. As the third contribution, we carefully refactor and amend the implementations and create friendly interfaces for 13 popular HNE algorithms, and provide all-around comparisons among them over multiple tasks and experimental settings.","layer":3,"vector":[-0.0282,0.027,-0.022,-0.0391,0.0488,0.0466,0.0147,0.0428,0.0334,-0.0029,-0.0165,-0.0561,0.0718,0.0855,0.0366,-0.0002,0.0237,0.0638,-0.0326,-0.0308,-0.0107,-0.0598,-0.0047,-0.0306,0.0468,0.013,-0.0271,0.0393,-0.0718,-0.262,0.025,-0.0804,0.0675,0.0067,-0.0033,-0.0341,0.0172,0.0505,-0.0226,0.0535,0.0235,0.0242,-0.0279,-0.057,-0.0357,-0.0262,-0.022,-0.0437,-0.0158,-0.0584,0.059,-0.0113,0.0224,0.0367,0.024,0.062,0.071,0.0444,0.0305,0.0601,0.0455,0.0387,-0.1374,0.0321,0.0333,0.0227,-0.0678,0.0363,0.0093,0.0459,0.0196,0.0028,-0.0208,0.0228,0.0265,0.0258,-0.0105,-0.019,-0.03,-0.0284,0.0047,-0.0094,-0.0383,-0.0037,0.0312,-0.0402,0.002,-0.0687,0.0175,0.0073,-0.0302,0.0075,-0.019,0.0136,-0.0325,-0.0139,0.0425,0.0262,-0.0654,0.1898,-0.089,0.011,0.0578,0.0063,0.0251,-0.0377,0.003,-0.0522,-0.0338,0.0211,-0.0239,-0.0067,0.0036,-0.0806,0.0379,-0.0004,0.0606,0.0377,-0.0236,-0.038,-0.0311,0.0297,0.0535,-0.0228,0.0647,-0.0565,0.0136,0.1062,0.0419,0.0377,0.0045,0.0214,-0.0187,-0.0108,-0.0074,0.0694,0.0109,-0.0211,-0.0189,0.017,-0.0007,-0.0429,-0.012,-0.0714,-0.0932,0.1327,-0.0455,-0.0329,0.0029,-0.022,-0.0406,0.0148,-0.0465,-0.0251,-0.0213,0.0475,0.0701,0.038,-0.0602,0.0146,-0.0188,-0.0506,-0.0725,0.0916,0.0448,-0.1245,-0.0211,0.0068,-0.0157,-0.0302,0.0405,0.0528,0.0068,0.0471,0.0827,0.0192,-0.0836,-0.015,0.0025,-0.0171,0.0449,-0.0466,-0.0445,0.0401,0.0047,-0.0516,0.0041,-0.0364,-0.0031,0.0153,-0.0195,0.0362,-0.0097,0.0081,0.0075,-0.0366,-0.0038,-0.0383,-0.0001,-0.0264,0.0367,0.0192,-0.0417,0.0093,-0.0495,0.0142,-0.0335,0.0361,0.0536,-0.0028,-0.0738,0.0034,0.0204,0.0003,-0.0173,-0.0273,0.0413,0.0691,0.0398,0.0204,0.0137,-0.0867,-0.0541,-0.2072,-0.0432,0.0425,-0.0285,0.0514,-0.0385,0.018,-0.0186,0.0686,0.1113,0.0836,0.0041,-0.0353,0.0162,-0.0215,0.0537,0.0501,0.0367,-0.0329,0.0161,-0.0255,0.0023,-0.0122,-0.0478,0.0416,-0.0048,0.2039,0.0113,0.0202,-0.0431,0.0117,0.0489,-0.0567,-0.0893,0.0735,0.0004,0.0509,-0.0019,-0.0625,-0.0398,-0.0649,0.0445,0.0303,-0.1476,0.0099,-0.0142,-0.0505,0.0268,-0.0654,0.0086,0.0629,-0.0347,0.0648,-0.0228,-0.0488,-0.031,-0.0206,0.0412,-0.0605,0.0249,0.0176,-0.0697,-0.008,-0.1,0.0247,-0.011,-0.0396,0.0168,0.0133,-0.0286,-0.0348,0.0916,-0.0089,-0.017,0.0877,-0.0353,0.0142,-0.0091,-0.0055,0.0016,0.0628,-0.0778,0.0637,-0.0021,-0.0158,0.0113,0.099,-0.0161,0.0477,0.0003,0.0176,0.0068,-0.0101,-0.039,0.0287,-0.0246,-0.301,0.044,0.0465,0.0537,-0.0274,-0.0289,0.0191,0.026,-0.0348,0.005,0.0447,0.0176,0.0576,-0.0035,-0.0086,0.0292,0.0599,-0.017,0.0441,-0.0475,0.0084,0.0319,0.2235,-0.0469,0.063,0.0265,-0.003,-0.0007,-0.0003,0.0028,-0.021,0.0272,0.0756,-0.0613,0.0397,0.0601,-0.021,0.0235,0.0106,-0.013,0.0345,-0.0477,-0.0554,-0.0495,0.0386,0.0217,-0.0169,-0.0227,0.0027,0.0104,-0.0319,0.0049,0.0013,-0.0065,0.0341,-0.0002,-0.0003,-0.0217,-0.0688,0.0005,0.0198,-0.0214,-0.0206,-0.0354,-0.0262]}
{"key":"[Random Noise vs State-of-the-Art Probabilistic Forecasting Methods : A Case Study on CRPS-Sum Discrimination Ability] The recent developments in the machine learning domain have enabled the development of complex multivariate probabilistic forecasting models. Therefore, it is pivotal to have a precise evaluation method to gauge the performance and predictability power of these complex methods. To do so, several evaluation metrics have been proposed in the past (such as Energy Score, Dawid-Sebastiani score, variogram score), however, they cannot reliably measure the performance of a probabilistic forecaster. Recently, CRPS-sum has gained a lot of prominence as a reliable metric for multivariate probabilistic forecasting. This paper presents a systematic evaluation of CRPS-sum to understand its discrimination ability. We show that the statistical properties of target data affect the discrimination ability of CRPS-Sum. Furthermore, we highlight that CRPS-Sum calculation overlooks the performance of the model on each dimension. These flaws can lead us to an incorrect assessment of model performance. Finally, with experiments on the real-world dataset, we demonstrate that the shortcomings of CRPS-Sum provide a misleading indication of the probabilistic forecasting performance method. We show that it is easily possible to have a better CRPS-Sum for a dummy model, which looks like random noise, in comparison to the state-of-the-art method.","layer":0,"vector":[-0.038,-0.0537,0.0099,0.0051,0.04,0.0635,0.0478,0.0008,0.0433,0.0014,0.0285,-0.0332,-0.0246,0.0689,0.0054,0.026,-0.0156,-0.0179,0.0009,0.0097,0.0532,0.008,-0.0329,-0.088,0.0733,0.0077,-0.0102,-0.0355,-0.0486,-0.2434,-0.0217,-0.0712,0.0361,-0.0493,0.0252,-0.0508,0.0046,0.0734,0.0093,0.034,0.0163,-0.0229,-0.0432,-0.0863,0.0047,-0.0473,-0.0292,0.014,-0.0492,-0.0241,0.041,-0.0481,0.0278,0.0119,0.0295,0.0547,0.0246,0.0162,0.0645,0.0503,-0.0069,0.0605,-0.2001,0.0338,0.0376,0.0144,-0.037,-0.0438,-0.0097,0.0366,-0.0047,0.0503,0.0688,0.0324,-0.0158,-0.0043,-0.0304,-0.0502,-0.0261,0.0137,0.0228,-0.0403,-0.1033,-0.0212,-0.0034,-0.0292,0.0451,-0.0336,0.0777,-0.0175,-0.0088,-0.013,-0.0244,0.0381,-0.0411,-0.0093,0.0532,0.0207,-0.038,0.195,-0.0341,0.022,0.0522,-0.0293,0.0643,-0.031,-0.044,-0.0393,-0.0147,-0.0338,0.0074,-0.0614,0.021,-0.0526,0.0205,-0.0012,0.0824,0.0355,-0.0018,-0.0249,-0.0289,-0.0059,0.0754,0.0167,0.0035,-0.029,0.0675,0.1797,0.0117,0.0044,0.0333,-0.0282,-0.0534,-0.0094,0.0587,-0.0014,0.0342,0.0165,0.0199,0.0107,-0.0336,-0.0173,-0.0008,-0.084,-0.0663,0.1194,-0.0727,0.0196,0.0232,-0.0321,-0.0346,0.031,-0.0057,-0.074,0.0025,0.033,0.0107,0.0196,-0.0249,0.0055,-0.0359,-0.0211,0.0071,0.0459,0.0185,-0.0545,-0.018,0.0191,0.0477,-0.0323,0.0218,0.0068,-0.0393,0.0223,0.0718,0.0465,-0.048,0.0007,0.0185,-0.0198,0.0248,-0.0277,-0.0224,0.0491,-0.0034,-0.0231,0.0077,-0.0418,0.0145,0.0285,-0.0303,-0.0268,0.0016,0.0219,-0.0174,-0.0178,-0.0047,0.0027,0.0335,-0.0617,-0.0135,-0.0267,-0.0241,-0.0103,-0.0133,0.0519,-0.0205,-0.0188,0.0699,0.0383,0.0086,-0.0064,0.0682,-0.0252,-0.0414,0.0686,0.0314,0.0652,-0.0036,0.0387,-0.0111,-0.0115,-0.0527,-0.1984,0.0444,0.0279,0.01,0.078,-0.0448,0.0404,-0.0052,0.0512,0.0859,0.059,-0.0274,-0.0198,0.0017,-0.0069,0.0377,-0.0213,0.023,-0.0435,0.0222,-0.0115,0.0154,-0.007,-0.0729,0.0434,0.0101,0.1317,0.0086,0.0181,-0.0393,0.0492,0.0167,-0.036,-0.099,0.0749,0.0534,0.0629,-0.0612,-0.0861,-0.0647,-0.0037,0.0604,-0.018,-0.0661,-0.0741,-0.0543,-0.0285,0.0498,-0.0688,0.0277,0.0012,-0.0108,0.0947,-0.028,0.0367,-0.0593,-0.0886,0.0385,-0.0107,0.0095,0.0421,-0.0304,0.0177,-0.059,0.0414,-0.0254,-0.0031,-0.0101,0.0058,0.0006,-0.0549,0.1166,-0.0536,-0.0198,0.0542,-0.0318,0.0198,-0.0409,-0.0276,-0.0265,0.0636,-0.0561,0.067,0.0575,0.0295,0.0088,0.0761,-0.0211,-0.0033,-0.0048,-0.0255,0.0103,-0.0353,0.0089,0.0421,0.0032,-0.3346,0.0272,-0.004,-0.0111,-0.0184,-0.0145,-0.0055,0.0179,-0.0622,0.0131,0.0086,0.0201,0.0806,-0.0363,-0.0103,0.0011,0.04,-0.0514,0.0287,-0.03,0.0301,0.0228,0.2375,-0.0033,0.0289,0.0082,0.0089,0.0227,0.0447,-0.0034,-0.007,-0.0127,0.0835,-0.0757,0.0349,0.0804,0.0037,0.0568,0.0129,-0.0252,0.008,0.0131,-0.0178,-0.0075,0.1198,-0.0021,-0.0443,-0.0459,0.0189,0.0112,-0.0537,-0.0096,-0.0396,-0.0404,0.0103,0.0594,-0.0237,-0.0318,-0.0367,-0.0431,0.0069,-0.041,-0.0325,-0.0089,-0.0079]}
{"key":"[Uncovering the Underlying Physics of Degrading System Behavior Through a Deep Neural Network Framework: The Case of Remaining Useful Life Prognosis] Deep learning (DL) has become an essential tool in prognosis and health management (PHM), commonly used as a regression algorithm for the prognosis of a system's behavior. One particular metric of interest is the remaining useful life (RUL) estimated using monitoring sensor data. Most of these deep learning applications treat the algorithms as black-box functions, giving little to no control of the data interpretation. This becomes an issue if the models break the governing laws of physics or other natural sciences when no constraints are imposed. The latest research efforts have focused on applying complex DL models to achieve a low prediction error rather than studying how the models interpret the behavior of the data and the system itself. In this paper, we propose an open-box approach using a deep neural network framework to explore the physics of degradation through partial differential equations (PDEs). The framework has three stages, and it aims to discover a latent variable and corresponding PDE to represent the health state of the system. Models are trained as a supervised regression and designed to output the RUL as well as a latent variable map that can be used and interpreted as the system's health indicator.","layer":0,"vector":[-0.0627,-0.0106,0.0515,-0.012,0.0437,0.0215,0.025,0.0292,0.0436,-0.0217,0.0004,-0.0388,0.0563,0.0512,-0.0069,0.0093,-0.0134,0.0797,-0.0213,0.0079,0.0217,-0.0027,-0.039,-0.0239,0.0157,0.0438,-0.0256,-0.0164,-0.0441,-0.2441,-0.0067,-0.0452,0.0245,-0.0579,0.0113,-0.034,-0.0218,0.02,0.0058,0.0533,-0.0011,-0.0208,-0.0277,-0.0212,0.0195,-0.0731,0.0056,-0.046,-0.0036,-0.0524,0.0168,-0.0352,0.0146,0.0507,0.0436,0.0259,0.0736,0.0294,0.0377,-0.0111,0.066,0.0024,-0.1789,0.0329,0.0856,0.0203,-0.0286,-0.0128,0.0256,0.0444,-0.0292,0.0007,0.0531,0.0363,-0.0149,0.0078,0.0251,-0.0304,-0.0046,0.0249,0.0645,-0.0019,-0.0193,-0.042,-0.0391,-0.0628,0.0272,-0.0461,0.0515,0.0025,-0.0597,-0.0214,-0.0003,0.0255,-0.0388,-0.0013,0.0271,0.0177,-0.0445,0.1988,-0.0621,0.0283,0.0174,-0.007,0.0064,0.0086,-0.0738,-0.0158,-0.0073,0.0258,-0.0399,-0.0094,0.0266,-0.0271,0.0611,-0.0093,0.0337,0.0599,0.0423,-0.0168,-0.0205,0.0012,0.0632,-0.0029,0.0068,-0.0877,0.0157,0.1582,0.0108,-0.0031,0.0354,-0.0156,-0.0786,-0.0391,0.0122,0.0345,0.06,0.0269,0.0158,0.0033,-0.0724,-0.0374,0.0129,-0.1062,-0.0734,0.1047,-0.0294,-0.0203,-0.0609,-0.0666,-0.0255,0.0336,0.0074,-0.0178,0.0577,0.0204,0.0018,0.0331,-0.0422,0.0326,-0.0425,-0.0703,-0.0537,0.1065,-0.0187,-0.0291,-0.0216,0.0289,0.0158,-0.0295,0.065,0.0186,-0.0178,-0.0279,0.0776,0.0393,-0.0225,-0.0143,-0.0115,0.0351,-0.0034,-0.0208,-0.0392,0.041,0.0427,-0.0515,0.0088,-0.0538,0.0169,0.0468,-0.0337,0.0078,-0.012,0.0128,-0.0015,-0.0241,-0.0628,-0.0036,0.0174,-0.0354,-0.0142,-0.0066,-0.0352,0.0303,-0.0315,0.0286,-0.0098,0.0321,0.0266,-0.0115,-0.0244,-0.0106,0.101,-0.0386,-0.026,-0.0025,-0.0069,0.0325,-0.0083,0.0821,0.0774,-0.0221,-0.0682,-0.2134,-0.0001,-0.0075,-0.0369,0.0737,-0.0621,-0.0022,-0.0369,0.0536,0.0352,0.0537,0.0015,0.0005,0.0032,0.0393,0.0225,0.0513,0.0154,-0.0651,0.0045,-0.0136,-0.0292,0.0085,-0.1257,0.0602,0.0071,0.2319,0.0031,0.039,-0.019,-0.0285,0.0015,-0.0491,-0.0756,0.1056,0.0182,0.0611,0.006,-0.056,-0.0172,-0.0501,0.0149,-0.0056,-0.0531,-0.0252,-0.0319,0.0102,0.009,-0.1076,0.0224,0.0552,-0.0165,0.0531,-0.0021,0.0263,-0.0435,-0.0801,0.0611,-0.0569,-0.0075,-0.0343,-0.0405,0.0003,-0.0577,0.0406,-0.0326,-0.0289,-0.0692,0.0169,-0.0578,-0.0349,0.135,-0.0339,-0.0471,0.0567,0.0087,-0.0122,0.0089,-0.0375,0.0064,0.0667,-0.0286,0.0108,0.0655,0.0282,-0.0101,0.0499,-0.053,-0.0063,-0.0163,-0.0216,0.0123,-0.0376,-0.0341,0.0452,0.0308,-0.2792,0.0476,0.0081,0.0065,-0.0255,-0.0173,0.0225,0.0477,-0.035,0.0352,-0.0133,0.038,0.0504,-0.0402,0.0536,0.0101,0.05,-0.1025,0.0518,-0.0475,0.0257,0.0601,0.1839,-0.0724,0.0714,0.0289,-0.0353,0.0294,0.0469,-0.046,0.0029,0.0205,0.0809,-0.0414,0.0403,0.0916,-0.0197,0.0346,0.0618,0.0409,0.0487,-0.0025,0.0071,-0.0131,0.0903,-0.0238,-0.0567,-0.0508,0.0202,0.0372,-0.0244,0.0236,-0.001,0.0225,0.0213,0.0336,-0.0131,-0.0588,-0.0263,-0.044,0.0152,-0.052,-0.0066,-0.0004,-0.0171]}
{"key":"[A Derivative-Free Method for Solving Elliptic Partial Differential Equations with Deep Neural Networks] We introduce a deep neural network based method for solving a class of elliptic partial differential equations. We approximate the solution of the PDE with a deep neural network which is trained under the guidance of a probabilistic representation of the PDE in the spirit of the Feynman-Kac formula. The solution is given by an expectation of a martingale process driven by a Brownian motion. As Brownian walkers explore the domain, the deep neural network is iteratively trained using a form of reinforcement learning. Our method is a 'Derivative-Free Loss Method' since it does not require the explicit calculation of the derivatives of the neural network with respect to the input neurons in order to compute the training loss. The advantages of our method are showcased in a series of test problems: a corner singularity problem, an interface problem, and an application to a chemotaxis population model.","layer":2,"vector":[-0.104,-0.0227,0.0336,0.0026,0.0167,0.0496,0.0053,0.0257,0.0371,-0.0181,0.0149,-0.0499,0.0573,0.0398,0.0,0.0175,-0.0316,0.0616,-0.0429,0.0401,0.0111,-0.0344,-0.0603,-0.0474,-0.014,-0.0221,-0.0062,-0.0393,-0.0279,-0.2224,0.0442,-0.0577,0.0027,-0.0613,0.025,-0.017,-0.0105,0.0329,-0.0179,0.0042,0.0343,-0.0086,-0.0384,-0.0487,0.0408,-0.0555,0.0156,-0.0351,0.02,-0.0697,0.0454,-0.0229,0.004,-0.0128,0.0396,0.0169,0.0688,0.0328,0.0454,0.0436,0.0148,0.0313,-0.1904,0.1115,0.0315,0.0518,-0.0118,-0.0211,0.0432,0.0836,-0.0239,0.0279,0.0331,0.0273,0.0164,0.0298,-0.0149,-0.0482,0.0018,0.0252,0.0011,-0.0228,-0.0353,-0.0533,0.0081,-0.0562,0.047,-0.0449,0.0161,-0.0207,-0.0213,-0.0189,-0.0644,0.0303,-0.0134,0.0163,0.0339,0.0285,-0.0577,0.2154,-0.0321,-0.008,0.0373,-0.0122,-0.0024,-0.0202,-0.0336,-0.0173,-0.019,0.0088,-0.0178,-0.0238,0.045,-0.0215,-0.0039,-0.0166,0.0135,0.0064,-0.0106,0.011,-0.0385,0.0234,0.03,0.0148,0.0191,-0.077,-0.0211,0.1518,0.0065,0.0178,0.063,-0.011,-0.0514,-0.0469,-0.0049,0.0244,-0.0323,-0.0298,-0.0324,0.0159,-0.0723,-0.03,-0.0032,-0.0801,-0.0564,0.0897,-0.0194,0.0095,-0.0362,-0.0384,-0.0024,0.0452,-0.0416,-0.0269,0.0521,-0.0035,-0.0019,0.0563,-0.078,0.0347,-0.0227,-0.0469,-0.0138,0.1848,-0.0033,-0.0466,-0.0181,-0.0137,0.0151,-0.0492,0.0387,0.066,-0.0369,-0.0284,0.0765,0.0434,-0.0824,0.0011,-0.0285,0.051,0.01,-0.012,-0.0323,0.0183,0.0368,-0.0635,0.0288,-0.0677,0.0014,0.0715,-0.0304,0.0371,-0.0303,-0.0022,-0.0088,-0.0288,-0.027,-0.0303,0.0091,-0.0247,0.0019,-0.0427,-0.042,-0.0047,-0.0239,0.0246,0.0217,0.0111,0.0172,0.0053,-0.041,-0.0108,0.0874,-0.0479,-0.0173,0.0319,-0.0116,0.02,0.0149,0.0701,0.0593,-0.0288,-0.0275,-0.212,-0.0358,-0.0101,-0.0134,0.057,-0.0677,0.0401,-0.0198,0.058,0.0389,0.0553,0.011,-0.0033,0.0156,-0.0075,0.0235,0.0266,0.0229,0.0099,-0.0087,-0.0117,-0.047,-0.0039,-0.0967,0.0385,0.0299,0.2224,0.0387,0.0687,-0.0415,0.0133,0.0193,0.0017,-0.0762,0.0613,0.0076,0.1235,-0.0328,0.0021,-0.0674,-0.0182,0.0372,-0.005,-0.0871,-0.0579,-0.0416,-0.0014,0.0317,-0.0774,0.0114,0.0639,-0.0056,0.0632,0.0126,-0.0028,-0.0326,-0.0648,0.0325,-0.0574,0.0093,-0.0141,-0.0624,0.0268,-0.0127,0.0533,0.0155,0.0052,-0.0523,0.0055,-0.0582,-0.001,0.0862,0.0257,0.0175,0.0539,-0.0079,0.0602,0.031,-0.0537,0.014,0.0656,-0.0302,0.0311,0.0541,0.0301,0.0473,0.0426,-0.0367,0.0027,-0.0405,-0.0179,0.023,-0.0791,-0.0031,0.0022,0.0049,-0.2769,0.0673,0.0052,0.0246,0.0022,-0.0073,0.0241,-0.0064,-0.0681,0.0103,-0.0306,0.0326,0.0556,0.0086,0.0363,0.0049,0.0352,-0.0387,0.0686,-0.0385,0.0537,0.0582,0.2139,-0.0909,0.0377,0.0184,-0.0224,-0.0199,0.0396,-0.0319,-0.0179,0.0175,0.0515,-0.0813,0.0487,0.1104,-0.0078,0.0557,0.0468,0.0177,0.0272,0.0127,0.0097,0.0104,0.0479,0.0195,-0.0138,-0.0252,0.0095,0.0445,-0.0145,0.0307,-0.0086,-0.0187,0.0116,0.0382,-0.0669,-0.0767,-0.0578,-0.0546,-0.0097,-0.0966,0.0192,-0.0131,0.0024]}
{"key":"[Uniform regret bounds over $R^d$ for the sequential linear regression problem with the square loss] We consider the setting of online linear regression for arbitrary deterministic sequences, with the square loss. We are interested in the aim set by Bartlett et al. (2015): obtain regret bounds that hold uniformly over all competitor vectors. When the feature sequence is known at the beginning of the game, they provided closed-form regret bounds of $2d B^2 \\ln T + \\mathcal{O}_T(1)$, where $T$ is the number of rounds and $B$ is a bound on the observations. Instead, we derive bounds with an optimal constant of $1$ in front of the $d B^2 \\ln T$ term. In the case of sequentially revealed features, we also derive an asymptotic regret bound of $d B^2 \\ln T$ for any individual sequence of features and bounded observations. All our algorithms are variants of the online non-linear ridge regression forecaster, either with a data-dependent regularization or with almost no regularization.","layer":3,"vector":[-0.0269,-0.0228,0.046,0.0134,0.0214,0.0232,0.0292,0.0527,0.0544,-0.0036,0.0463,0.0043,0.0071,0.0302,0.025,0.0219,-0.0391,0.0378,-0.0692,0.0261,0.0228,-0.0531,-0.0118,-0.0949,0.0336,0.0082,-0.0543,-0.0586,-0.0301,-0.2335,0.0417,-0.033,0.0445,-0.0351,0.0044,-0.0338,-0.0244,0.0794,-0.0416,0.0532,-0.011,0.0508,-0.009,-0.0515,0.0066,-0.0441,0.0106,-0.0024,-0.0199,-0.038,0.0164,-0.0124,0.0264,0.034,0.024,0.0213,-0.0086,0.0635,0.0097,0.0582,0.0377,-0.0051,-0.1794,0.0483,0.0154,0.0304,-0.0584,0.0027,-0.0115,0.0877,0.0266,0.0158,0.02,0.0455,0.0002,0.0129,0.02,-0.0008,-0.0264,0.015,0.0444,-0.0166,-0.045,-0.0115,-0.0361,-0.0744,0.0286,-0.0735,0.0661,0.0053,-0.019,-0.0174,0.011,-0.0103,-0.0783,-0.0121,0.0525,0.0434,-0.0146,0.1908,-0.0518,0.043,-0.0033,-0.0009,0.0352,-0.0279,-0.0316,-0.0145,-0.0244,-0.026,-0.0083,-0.0151,0.0574,-0.0387,0.0077,-0.0005,0.0547,0.1017,-0.0099,0.0049,-0.0374,0.0401,0.0975,-0.0023,0.0284,-0.0549,0.0194,0.1392,0.0297,0.0338,0.0194,-0.0624,-0.0206,-0.0014,0.0334,-0.0111,0.0506,0.0059,0.0251,-0.0216,-0.0515,-0.0424,0.0405,-0.0777,-0.0293,0.1378,-0.0194,0.0161,-0.0277,-0.0384,0.0305,0.0078,-0.003,-0.032,0.0307,0.0392,0.0336,0.0245,-0.0781,0.0061,-0.0552,-0.0049,0.0099,0.0829,-0.0289,-0.0643,-0.0043,0.0064,0.0035,0.0232,0.0528,-0.0016,-0.0262,-0.0054,0.0796,0.0331,-0.0707,-0.0027,0.0199,0.0057,-0.0188,-0.0122,-0.0136,0.0315,0.0056,-0.0402,0.0116,-0.0277,0.0116,0.0578,-0.0208,-0.0086,-0.0311,0.0121,0.0125,-0.043,-0.0119,-0.0146,0.019,-0.0423,0.0139,-0.0084,-0.0643,0.0084,0.0431,0.0062,0.0332,-0.0217,0.0482,0.0243,-0.037,-0.0209,0.0502,-0.0155,-0.0539,0.0707,0.053,0.0546,0.0065,0.0512,0.019,-0.0056,-0.0309,-0.2154,-0.01,-0.0143,0.0028,0.0603,-0.0569,0.0494,-0.0504,0.0644,0.0708,0.0474,-0.0451,-0.0251,0.008,0.0184,0.0435,0.0314,-0.0047,-0.0587,0.0352,-0.0672,0.0038,-0.0419,-0.0494,0.0511,-0.0043,0.2234,0.0268,0.0275,-0.0527,0.0449,0.0405,0.0195,-0.0437,0.0338,0.0341,0.045,0.0128,-0.0709,-0.0546,-0.0192,0.0061,0.0254,-0.0938,-0.0598,-0.0143,-0.0753,0.0304,-0.0705,0.007,0.0504,-0.0363,0.1045,-0.0559,0.0636,-0.0192,-0.1257,-0.0017,-0.0078,0.0349,-0.0015,-0.0755,-0.0072,-0.0742,0.0594,-0.0093,0.0166,-0.0718,0.0089,-0.0115,-0.0072,0.0429,-0.0497,-0.0089,0.0185,0.0019,0.0208,-0.0622,-0.0711,-0.0288,0.0953,-0.0787,0.0122,0.0175,0.0038,0.021,0.092,-0.0162,0.0248,0.0299,-0.0253,0.001,-0.0887,-0.0147,0.0491,0.0282,-0.3112,0.0263,-0.0073,0.0037,-0.0068,-0.0002,0.0303,-0.0014,-0.0578,0.0133,0.0228,0.0594,0.0495,-0.0412,0.0461,0.0416,0.0512,-0.0493,0.0252,-0.0549,0.0049,0.0495,0.187,-0.0671,0.0283,-0.0107,-0.0525,-0.0443,0.0401,-0.0152,0.0398,0.0037,0.0582,-0.0576,0.0236,0.0627,-0.0505,0.0561,0.0254,-0.0187,-0.0325,0.0065,-0.0492,0.0065,0.1157,0.026,-0.0437,-0.0317,-0.0516,0.0141,-0.0276,0.0207,0.0049,0.0096,-0.0026,0.0172,-0.0446,-0.0422,-0.0312,-0.0465,0.0357,-0.0731,-0.0518,-0.0228,-0.01]}
{"key":"[RDeepSense: Reliable Deep Mobile Computing Models with Uncertainty Estimations] Recent advances in deep learning have led various applications to unprecedented achievements, which could potentially bring higher intelligence to a broad spectrum of mobile and ubiquitous applications. Although existing studies have demonstrated the effectiveness and feasibility of running deep neural network inference operations on mobile and embedded devices, they overlooked the reliability of mobile computing models. Reliability measurements such as predictive uncertainty estimations are key factors for improving the decision accuracy and user experience. In this work, we propose RDeepSense, the first deep learning model that provides well-calibrated uncertainty estimations for resource-constrained mobile and embedded devices. RDeepSense enables the predictive uncertainty by adopting a tunable proper scoring rule as the training criterion and dropout as the implicit Bayesian approximation, which theoretically proves its correctness.To reduce the computational complexity, RDeepSense employs efficient dropout and predictive distribution estimation instead of model ensemble or sampling-based method for inference operations. We evaluate RDeepSense with four mobile sensing applications using Intel Edison devices. Results show that RDeepSense can reduce around 90% of the energy consumption while producing superior uncertainty estimations and preserving at least the same model accuracy compared with other state-of-the-art methods.","layer":0,"vector":[-0.0189,0.0211,0.0776,-0.0446,0.0436,0.0479,0.0458,0.02,0.0623,-0.0225,0.0088,-0.0412,0.0681,0.0392,0.0386,0.0033,-0.013,0.0248,-0.0173,-0.0078,0.0464,-0.0516,0.0169,-0.0525,0.0348,0.0011,-0.0388,-0.0234,-0.0642,-0.2335,0.0093,-0.052,0.037,-0.032,0.0312,-0.0236,-0.0507,0.0335,0.0146,0.0358,-0.0072,0.0192,-0.0213,-0.0553,-0.0184,-0.0704,-0.0135,0.0098,-0.0322,-0.0387,0.0345,-0.0166,0.0189,0.0232,0.0414,0.0552,0.0376,0.0626,0.0786,0.0342,0.021,0.0307,-0.1676,0.0545,0.0433,-0.041,-0.0698,-0.0903,0.002,-0.007,-0.0076,0.0309,0.0372,0.0833,0.0345,0.0044,0.007,-0.0203,-0.0432,0.0274,0.0218,-0.0118,-0.0316,0.0237,0.0112,-0.044,0.0327,-0.0097,0.025,-0.0286,-0.0533,-0.0283,-0.0529,0.018,-0.0681,0.026,0.0398,-0.0157,-0.0847,0.2182,-0.0162,0.0325,0.0272,-0.0368,0.0103,-0.0439,-0.0533,-0.0231,-0.0695,0.0359,-0.0102,-0.0192,0.0318,-0.0135,0.0378,0.0272,0.0776,0.0609,0.0078,-0.0096,-0.0757,0.0063,0.0659,-0.0231,0.0327,-0.0948,0.0275,0.1629,-0.0172,0.0245,-0.0035,-0.0702,-0.0496,-0.0166,0.0773,0.0324,0.0547,-0.0118,0.037,-0.006,-0.0136,-0.0287,0.049,-0.0804,-0.0522,0.085,-0.0294,0.0353,-0.0493,-0.0802,0.0056,0.0484,0.0026,-0.0074,0.0348,0.046,-0.0215,0.0297,-0.0679,-0.0024,-0.0078,-0.041,-0.0158,0.0892,-0.0186,-0.0826,-0.0076,0.0245,0.0094,-0.0432,0.0453,0.042,-0.0291,0.0181,0.0476,0.0313,-0.0939,0.017,-0.0178,0.0276,-0.0081,-0.0096,-0.0086,0.0076,0.0342,-0.0307,0.0159,-0.0726,0.0128,0.0648,-0.0093,-0.0064,-0.0392,-0.0134,0.0091,0.0039,-0.0264,0.0064,0.0442,-0.0506,-0.0003,-0.0263,-0.0323,0.0378,0.0085,0.0284,-0.0178,0.0477,0.0079,0.0375,-0.0083,-0.0175,0.0657,-0.0404,0.0078,-0.0172,0.0378,0.0318,0.008,0.0223,0.0344,-0.0674,-0.0212,-0.2241,0.014,0.0256,-0.0189,0.0318,-0.0442,0.0197,-0.0213,0.0399,0.0413,0.0512,-0.011,0.0011,0.0141,-0.0097,0.0765,0.0064,-0.0071,-0.0541,0.0132,-0.0073,0.0029,-0.0643,-0.092,0.0563,0.0604,0.2147,0.0059,0.0372,-0.0089,0.0285,0.0523,-0.0104,-0.0976,0.0525,0.0263,0.0451,-0.011,-0.0564,-0.0542,-0.0376,0.0332,-0.0123,-0.1369,-0.0653,-0.0347,-0.0415,0.0285,-0.0318,0.0081,0.0219,-0.0179,0.0781,-0.0558,-0.01,-0.0428,-0.0882,0.0829,-0.0409,-0.0062,-0.0311,-0.0521,-0.0337,-0.0308,0.0371,-0.0255,0.0118,-0.053,0.0393,-0.0325,0.0147,0.0947,0.0068,0.0386,0.0427,-0.0185,0.0428,-0.045,0.0021,-0.048,0.0449,-0.0157,0.046,0.053,-0.0033,0.0447,0.0725,-0.0157,0.0418,-0.0313,-0.0072,-0.0035,-0.0425,-0.0474,0.0472,-0.0138,-0.2855,0.0538,0.0064,0.0284,-0.077,-0.006,0.0718,0.0079,-0.06,0.0205,-0.0144,0.0485,0.0564,0.0,0.0086,0.0027,0.0523,-0.0553,0.0413,-0.0864,0.0197,0.0434,0.1813,-0.0474,0.035,0.055,0.0167,0.02,0.0444,-0.0158,-0.0283,-0.008,0.0479,-0.0623,0.0248,0.0925,-0.0169,0.018,0.0345,-0.0006,0.029,0.0224,0.019,-0.0415,0.0793,-0.0207,-0.0498,-0.0188,0.003,0.0306,-0.0086,-0.0348,-0.0435,-0.0089,0.0091,0.0261,-0.0347,-0.0447,-0.0591,-0.0751,0.0555,-0.0375,-0.0019,-0.0169,-0.0142]}
{"key":"[On Learning Sparsely Used Dictionaries from Incomplete Samples] Most existing algorithms for dictionary learning assume that all entries of the (high-dimensional) input data are fully observed. However, in several practical applications (such as hyper-spectral imaging or blood glucose monitoring), only an incomplete fraction of the data entries may be available. For incomplete settings, no provably correct and polynomial-time algorithm has been reported in the dictionary learning literature. In this paper, we provide provable approaches for learning - from incomplete samples - a family of dictionaries whose atoms have sufficiently \"spread-out\" mass. First, we propose a descent-style iterative algorithm that linearly converges to the true dictionary when provided a sufficiently coarse initial estimate. Second, we propose an initialization algorithm that utilizes a small number of extra fully observed samples to produce such a coarse initial estimate. Finally, we theoretically analyze their performance and provide asymptotic statistical and computational guarantees.","layer":1,"vector":[-0.0401,0.0241,0.0077,-0.0445,0.0181,0.0309,0.042,0.0349,0.0258,-0.0209,0.0288,-0.0637,0.0319,0.035,-0.0149,0.0452,0.0412,0.0509,-0.0425,-0.0121,0.0448,-0.0254,0.0297,-0.0438,0.0227,0.0064,-0.009,-0.0782,-0.0482,-0.2275,0.0362,-0.0092,0.0568,-0.0084,0.0227,-0.015,-0.0289,0.0636,-0.0457,0.0784,0.0074,0.008,-0.0308,-0.0056,-0.0207,-0.0488,-0.0353,-0.0412,0.0082,-0.0277,0.0036,-0.0419,0.0073,0.0317,0.0311,-0.0065,0.0222,0.0114,0.0261,0.0262,0.019,0.0581,-0.1473,0.0466,0.074,0.0594,-0.0499,-0.0199,0.032,0.0334,0.0218,0.0575,0.0638,0.0769,-0.0219,-0.0358,0.0136,-0.0545,0.0069,0.0003,0.0162,-0.0161,0.0171,0.0027,-0.0665,-0.091,0.023,-0.0648,0.016,0.01,-0.0849,-0.041,-0.0597,0.0666,-0.0613,-0.0161,0.0492,0.0373,-0.0233,0.2035,-0.0503,0.0553,0.0181,-0.0606,0.0228,-0.061,-0.0152,0.001,-0.0213,0.0201,0.0199,0.0141,0.0572,-0.0458,0.0153,-0.0438,0.0262,0.0521,-0.011,0.0318,0.0097,-0.0271,0.0647,-0.0136,0.0316,-0.0643,0.0115,0.1446,0.0338,0.0589,0.0407,-0.0435,-0.0326,-0.0204,-0.0195,0.0194,0.0135,0.0339,0.0444,0.0104,-0.0555,-0.0812,0.0042,-0.0727,-0.0549,0.1336,-0.0444,0.0603,-0.0661,-0.0395,0.021,0.0024,-0.0053,-0.0136,0.0177,0.0158,0.0941,0.0244,-0.0402,0.0292,-0.0108,-0.0893,-0.0032,0.0879,-0.0216,-0.0594,-0.0552,-0.0207,0.0084,0.0034,0.0631,0.0396,-0.0515,0.0329,0.1145,0.0088,-0.0785,-0.0162,0.0365,0.0038,-0.0151,-0.0622,-0.0518,0.0406,0.0263,-0.0242,0.0135,-0.015,0.0313,0.0235,-0.0431,0.0069,-0.0206,-0.0344,0.0074,-0.0557,-0.0055,-0.0284,-0.0201,-0.0242,0.0045,-0.0349,-0.0274,0.0487,0.055,0.0304,-0.0088,-0.0087,0.0387,0.0294,-0.0101,-0.0094,0.0389,-0.0544,-0.0261,-0.0181,0.0176,-0.0136,0.0101,0.0618,-0.0042,-0.0764,-0.0911,-0.2264,-0.0121,0.0367,0.019,0.0458,-0.0665,0.072,0.0071,0.0441,0.0598,0.0091,-0.0063,-0.001,0.0413,-0.0255,0.0451,0.053,0.0221,-0.0253,0.0198,0.0038,0.0163,-0.0437,-0.0898,0.0498,0.0006,0.2041,-0.0143,0.0055,-0.0404,-0.0306,0.0475,-0.0317,-0.0787,0.0242,-0.0239,0.0288,0.0103,-0.0356,-0.0188,-0.0023,0.0202,-0.0086,-0.0788,-0.0332,-0.0607,-0.069,0.0109,-0.0437,0.0295,0.035,-0.0046,0.0673,-0.059,0.0364,-0.0402,-0.1278,0.0369,-0.0474,0.0511,0.0027,-0.0377,-0.0056,-0.0423,0.05,0.0099,-0.0237,-0.0363,0.0274,-0.0339,-0.0404,0.0805,-0.0135,0.031,0.0217,0.0397,0.0119,-0.0449,-0.0486,0.0215,0.0882,-0.0345,0.0313,0.0336,0.0452,0.0691,0.1178,0.0208,0.0052,-0.0241,0.0199,0.0293,-0.0583,0.0303,0.0457,-0.0042,-0.2935,0.0263,0.0542,0.0375,-0.0344,0.0272,0.0704,0.0062,-0.0366,-0.0026,-0.0389,0.0183,0.0224,-0.0364,-0.0229,0.0556,0.0994,-0.0533,0.0272,-0.0891,0.0016,0.0556,0.2012,-0.059,0.0059,0.0394,-0.016,0.0189,0.0119,-0.0041,0.0257,-0.0114,0.0663,-0.0411,0.0305,0.0596,-0.029,0.038,0.0117,-0.0305,-0.024,-0.0155,-0.0609,-0.0114,0.0917,-0.0271,0.0024,-0.0386,-0.002,0.001,0.0006,-0.0289,0.0169,0.0015,-0.0077,0.0021,-0.0219,-0.039,-0.0203,-0.0213,0.0034,-0.0767,-0.0349,-0.0072,-0.0096]}
{"key":"[BR-NS: an Archive-less Approach to Novelty Search] As open-ended learning based on divergent search algorithms such as Novelty Search (NS) draws more and more attention from the research community, it is natural to expect that its application to increasingly complex real-world problems will require the exploration to operate in higher dimensional Behavior Spaces which will not necessarily be Euclidean. Novelty Search traditionally relies on k-nearest neighbours search and an archive of previously visited behavior descriptors which are assumed to live in a Euclidean space. This is problematic because of a number of issues. On one hand, Euclidean distance and Nearest-neighbour search are known to behave differently and become less meaningful in high dimensional spaces. On the other hand, the archive has to be bounded since, memory considerations aside, the computational complexity of finding nearest neighbours in that archive grows linearithmically with its size. A sub-optimal bound can result in \"cycling\" in the behavior space, which inhibits the progress of the exploration. Furthermore, the performance of NS depends on a number of algorithmic choices and hyperparameters, such as the strategies to add or remove elements to the archive and the number of neighbours to use in k-nn search. In this paper, we discuss an alternative approach to novelty estimation, dubbed Behavior Recognition based Novelty Search (BR-NS), which does not require an archive, makes no assumption on the metrics that can be defined in the behavior space and does not rely on nearest neighbours search. We conduct experiments to gain insight into its feasibility and dynamics as well as potential advantages over archive-based NS in terms of time complexity.","layer":1,"vector":[-0.0779,0.0191,0.0669,-0.0079,0.0181,-0.01,0.0253,0.0401,0.0364,-0.0217,0.0562,-0.0397,0.014,0.0728,-0.0135,-0.0168,-0.0086,0.051,-0.0097,0.0109,0.0409,-0.0328,-0.022,-0.0501,0.0062,0.0483,-0.0489,-0.0509,0.0028,-0.2543,0.0308,-0.0358,0.0634,-0.0492,-0.008,-0.0622,-0.033,0.0788,-0.0666,0.0649,0.0362,0.0342,-0.0007,-0.048,-0.0298,-0.0024,-0.019,-0.0331,-0.0073,-0.076,-0.004,-0.0494,0.0533,0.028,0.0309,0.054,0.0554,0.0312,0.0353,0.0291,0.0556,-0.0009,-0.1196,0.0211,0.0489,0.0207,-0.0491,-0.0473,0.0359,0.0503,-0.0193,0.0432,0.0308,0.0124,0.048,-0.0135,-0.0181,-0.0112,0.0005,-0.0085,0.0049,-0.0314,-0.0285,-0.0203,-0.0341,-0.0486,0.0296,-0.016,0.038,0.0281,-0.0458,0.016,-0.0044,-0.002,-0.0966,-0.0227,0.0155,0.0667,-0.0469,0.2253,-0.0623,0.0542,0.0207,-0.0076,0.0137,-0.0116,-0.0346,-0.0148,-0.0241,-0.04,0.0017,-0.0225,0.0526,-0.0066,0.0126,-0.0072,0.0595,0.0375,0.0034,-0.0062,-0.0451,-0.0183,0.0523,-0.046,0.0108,-0.044,0.0158,0.0972,0.0021,-0.0315,0.0306,-0.0186,-0.0239,-0.0041,0.0013,0.0703,0.0154,-0.0213,-0.0044,-0.0254,0.0084,-0.0036,0.0484,-0.0588,-0.0553,0.1345,-0.0374,-0.0094,-0.0175,-0.0101,-0.0169,0.0301,-0.0427,-0.0461,0.0348,0.0078,0.0346,0.072,-0.0689,0.0093,-0.0728,-0.0417,-0.0014,0.087,-0.0489,-0.0484,-0.0152,-0.0168,-0.0111,-0.0001,0.0193,0.0569,-0.0624,0.064,0.0314,0.008,-0.0634,0.0047,0.02,-0.0005,0.0098,-0.0673,-0.0108,0.0233,0.0532,-0.0513,0.0416,-0.0122,0.0451,0.0315,0.017,-0.0031,-0.0033,-0.0167,-0.0237,-0.0752,0.0177,-0.0261,0.0425,-0.0141,0.0189,0.0234,-0.0374,0.0297,0.0128,0.037,0.0181,-0.0434,0.0817,-0.0003,-0.0315,0.0109,0.0511,-0.0189,-0.0456,-0.0218,0.0236,0.012,0.0134,0.0397,0.0128,-0.0144,-0.0338,-0.2515,-0.0342,0.0244,-0.0094,0.0619,-0.0603,0.035,-0.0128,0.0464,0.0402,0.0035,-0.0389,-0.0325,0.058,-0.0037,0.077,0.0594,0.0134,-0.0044,0.021,0.0015,0.0031,0.0064,-0.0985,0.0317,-0.0456,0.2158,0.049,-0.0122,-0.0449,-0.0032,0.0393,-0.0127,-0.0943,0.0418,0.0335,0.059,0.0008,-0.0197,-0.0517,-0.077,0.0138,0.0097,-0.081,-0.0233,0.0219,-0.0315,0.019,-0.0802,0.0458,0.0464,-0.0088,0.0562,-0.0412,-0.0365,-0.0583,-0.0538,-0.0281,-0.037,0.025,-0.0354,-0.0845,0.0005,-0.0874,0.0889,-0.0153,0.0012,-0.0544,0.0097,-0.0176,-0.0436,0.0978,0.0351,-0.0291,0.0152,0.0124,0.0185,-0.0067,-0.0469,-0.0172,0.0567,-0.0611,0.0573,-0.004,0.0206,-0.0044,0.0926,-0.0325,0.0651,-0.0382,0.0188,0.0302,-0.0153,-0.0258,0.056,0.0483,-0.3045,0.0499,-0.0103,0.0084,-0.0064,0.0341,0.0106,-0.0041,-0.0091,-0.0032,0.0157,0.0504,0.0346,-0.0168,0.0124,0.0247,0.0772,-0.0045,0.0302,-0.0922,0.0343,0.0323,0.2377,-0.0094,0.0512,-0.0231,-0.0402,-0.0354,0.0155,-0.0758,0.0345,-0.0321,0.0731,-0.0727,0.0427,0.0478,-0.0112,0.027,-0.0258,-0.0178,-0.0127,0.0094,-0.0361,-0.0025,0.1113,0.0071,-0.0354,-0.0143,-0.0254,0.0358,-0.015,-0.0013,-0.0195,0.0129,0.0519,0.0311,-0.044,-0.047,-0.0212,-0.0716,0.0418,-0.0079,0.0433,-0.0219,0.0204]}
{"key":"[Wavelet neural operator: a neural operator for parametric partial differential equations] With massive advancements in sensor technologies and Internet-of-things, we now have access to terabytes of historical data; however, there is a lack of clarity in how to best exploit the data to predict future events. One possible alternative in this context is to utilize operator learning algorithm that directly learn nonlinear mapping between two functional spaces; this facilitates real-time prediction of naturally arising complex evolutionary dynamics. In this work, we introduce a novel operator learning algorithm referred to as the Wavelet Neural Operator (WNO) that blends integral kernel with wavelet transformation. WNO harnesses the superiority of the wavelets in time-frequency localization of the functions and enables accurate tracking of patterns in spatial domain and effective learning of the functional mappings. Since the wavelets are localized in both time/space and frequency, WNO can provide high spatial and frequency resolution. This offers learning of the finer details of the parametric dependencies in the solution for complex problems. The efficacy and robustness of the proposed WNO are illustrated on a wide array of problems involving Burger's equation, Darcy flow, Navier-Stokes equation, Allen-Cahn equation, and Wave advection equation. Comparative study with respect to existing operator learning frameworks are presented. Finally, the proposed approach is used to build a digital twin capable of predicting Earth's air temperature based on available historical data.","layer":0,"vector":[-0.0615,-0.0279,0.0367,-0.0047,0.0683,0.011,0.0324,0.0357,0.0223,-0.0283,-0.0508,-0.076,0.0458,0.0261,0.0279,0.0127,-0.025,0.0705,-0.0326,-0.0153,0.0204,0.0109,-0.0011,-0.017,-0.0003,0.0288,-0.0281,0.0119,-0.0164,-0.2133,0.0286,-0.0485,0.0146,-0.0568,0.0324,-0.024,0.021,0.0686,-0.017,0.08,-0.0176,-0.035,-0.002,-0.0319,-0.0064,-0.0513,0.0159,-0.0063,0.0181,-0.0197,0.044,-0.0481,0.0106,0.0127,0.044,0.0115,0.0635,0.0279,0.0555,0.0457,0.0116,0.0119,-0.1687,0.0766,0.0262,0.0181,0.011,-0.0182,-0.0163,0.0264,-0.037,0.0294,-0.0064,-0.0221,0.0238,0.0197,-0.0341,-0.0212,-0.0063,-0.0022,0.0563,-0.0204,-0.054,-0.055,-0.0432,-0.0383,0.037,-0.0466,0.0446,0.0286,-0.0388,-0.0297,-0.0327,0.0297,-0.0481,0.0142,0.0248,0.0451,-0.0215,0.1986,-0.0518,0.0255,0.0483,-0.0221,0.0404,-0.065,-0.0444,-0.0543,-0.0155,-0.0164,-0.047,-0.0151,0.027,-0.0162,0.0249,-0.0438,0.0041,0.0224,0.0016,0.0229,-0.0229,-0.0027,0.0129,0.0074,0.0262,-0.0382,0.0446,0.1246,0.0297,0.0273,0.068,0.0124,-0.046,-0.0038,-0.0215,0.0235,-0.0261,0.0057,-0.0295,-0.0211,-0.0676,-0.0464,0.0236,-0.0906,-0.0458,0.1093,-0.0317,0.0177,-0.065,-0.0269,-0.031,0.0463,-0.076,-0.0093,0.05,0.0551,0.0021,0.0635,-0.0464,0.0109,-0.0474,-0.0595,-0.0212,0.1072,-0.0088,-0.0812,0.0004,0.0428,0.0349,-0.0326,0.0299,0.022,-0.0122,-0.0205,0.1483,0.0225,-0.0301,0.0304,-0.0169,0.0375,0.0201,-0.062,-0.0156,0.0079,0.0499,-0.0698,0.0285,-0.0729,-0.0298,0.0525,-0.0008,0.0118,0.0195,0.0148,-0.0072,-0.0211,0.0039,-0.038,0.0423,-0.0476,0.0298,-0.042,-0.0402,-0.0014,-0.0033,0.0036,0.0372,0.0057,-0.0512,0.0273,-0.0215,0.0062,0.0627,-0.0325,-0.0175,-0.0118,-0.0048,0.0115,0.0055,0.0707,0.0341,-0.0594,-0.0725,-0.2457,-0.0698,0.007,-0.0228,0.0647,-0.0401,0.052,-0.0259,0.0392,0.0563,0.0798,0.0151,0.0208,-0.0024,0.0005,-0.0123,0.0765,0.0369,-0.0454,0.0148,-0.0296,0.0081,-0.0045,-0.0773,0.0674,-0.0146,0.1783,0.0281,0.0511,-0.0602,0.0476,-0.0194,0.0006,-0.0832,0.0311,0.0149,0.076,-0.0495,-0.0249,-0.0854,-0.008,0.0271,0.0152,-0.0425,-0.0372,-0.0258,-0.0252,0.0234,-0.068,0.0048,0.0445,-0.0419,0.0655,-0.0261,-0.0248,-0.051,-0.047,0.0383,-0.0224,0.0319,-0.0011,-0.0792,0.0164,-0.0098,0.0336,0.0461,0.009,-0.0438,0.0441,-0.0389,-0.0224,0.0971,-0.0149,0.0082,0.0435,-0.0039,0.0486,-0.014,-0.0099,-0.0129,0.0761,-0.0372,0.0843,0.0785,0.0406,0.0157,0.0605,-0.0436,-0.0166,-0.0289,-0.0081,-0.0112,-0.0542,-0.026,0.0486,-0.0353,-0.3097,0.022,0.0131,0.0001,-0.0354,0.005,0.0008,-0.0063,-0.0519,0.0003,-0.0572,0.0006,0.0209,0.0076,-0.0096,0.0229,0.0118,-0.0493,0.0755,-0.0337,0.0356,0.0771,0.2339,-0.0206,0.0933,0.0164,-0.0248,-0.0099,0.0131,-0.0358,0.0311,0.0417,0.0839,-0.0798,0.0515,0.0751,-0.0256,0.0485,0.0193,-0.0062,0.0588,-0.0019,-0.0252,-0.0409,0.0861,0.0256,-0.0091,-0.0725,-0.0449,0.0452,0.0037,0.0307,0.0206,0.0008,0.0193,0.0742,-0.0817,-0.0532,-0.0715,-0.0188,0.0349,-0.0706,-0.0251,-0.0047,-0.0012]}
{"key":"[Energy-Based Sequence GANs for Recommendation and Their Connection to Imitation Learning] Recommender systems aim to find an accurate and efficient mapping from historic data of user-preferred items to a new item that is to be liked by a user. Towards this goal, energy-based sequence generative adversarial nets (EB-SeqGANs) are adopted for recommendation by learning a generative model for the time series of user-preferred items. By recasting the energy function as the feature function, the proposed EB-SeqGANs is interpreted as an instance of maximum-entropy imitation learning.","layer":4,"vector":[-0.0792,-0.0153,0.0332,-0.056,-0.0106,0.0401,0.0524,0.0078,0.009,0.0011,0.0021,-0.0098,0.0323,0.0965,0.0224,0.0301,0.0164,0.0167,-0.05,-0.0168,0.0623,-0.0393,0.0006,-0.0752,-0.0058,0.0131,-0.0396,-0.0283,-0.0594,-0.1986,0.0486,-0.056,0.0493,0.0055,-0.048,-0.0627,-0.0318,0.0326,-0.0493,0.0437,0.0237,0.0458,-0.0402,-0.065,0.0171,-0.0216,-0.018,0.0062,0.0031,-0.0343,0.0067,-0.0339,0.0053,0.0354,0.0355,0.0335,0.0353,0.033,0.0203,0.0363,0.0294,0.0449,-0.1593,0.0661,0.0094,0.0283,-0.0015,-0.0385,-0.0279,0.0306,-0.0167,0.056,0.0052,0.0655,-0.0173,0.023,-0.0101,-0.0366,-0.0534,0.0282,0.0011,-0.0155,-0.0498,-0.0293,0.024,-0.0214,0.0177,-0.0265,0.0302,0.0182,-0.0064,-0.0092,-0.0203,0.0587,-0.0484,-0.0406,0.016,0.0537,-0.0569,0.2284,-0.0255,0.078,0.0563,-0.0106,0.0221,-0.0215,-0.0362,-0.0217,-0.0223,0.0075,-0.0609,0.0312,0.0249,-0.043,0.0698,0.0022,0.0464,0.0464,-0.0118,-0.0392,-0.0372,0.0468,0.0591,-0.0316,0.0219,-0.0543,-0.0068,0.0859,0.0373,0.0532,0.0191,-0.023,-0.0761,0.0196,-0.0008,-0.005,0.0178,-0.0513,0.0007,-0.0055,0.0147,-0.0281,-0.0059,-0.0702,-0.0202,0.0739,-0.0297,-0.0017,-0.0807,-0.0067,-0.0274,-0.0164,-0.0028,-0.0244,0.0546,0.0762,0.0584,0.0445,-0.0903,0.0189,-0.0248,-0.0389,-0.0168,0.0729,0.0325,-0.0981,-0.0258,-0.0212,0.0143,-0.0082,0.0142,0.0166,-0.0536,0.0566,0.0974,0.0615,-0.0381,-0.0111,-0.0063,0.028,0.0059,-0.0734,-0.0356,0.051,0.0358,-0.0048,0.0016,-0.0557,0.0063,0.0562,-0.01,0.0159,-0.0074,0.0096,0.021,-0.0266,-0.0347,0.0064,-0.0095,-0.0625,-0.0042,-0.0063,-0.02,-0.0036,-0.0166,0.0221,-0.0306,-0.0307,0.0724,-0.0213,-0.0143,0.0338,0.0245,0.0244,-0.0744,0.001,0.0173,0.0452,0.0379,0.0616,0.0389,-0.0485,-0.0431,-0.286,0.0129,-0.0226,0.0147,0.0631,-0.0941,0.0423,-0.0514,0.0487,0.0429,0.0374,0.0138,-0.0023,0.0425,-0.0007,0.0885,0.0351,-0.0403,-0.0156,-0.0122,-0.0075,0.0425,0.0065,-0.0998,0.0441,-0.0078,0.2195,0.0643,0.0196,-0.0002,0.0815,0.0112,-0.0618,-0.0755,0.0255,0.0331,0.1178,-0.0076,-0.034,-0.0837,-0.0386,0.0317,-0.0239,-0.0783,0.0048,-0.0423,-0.0285,0.0158,-0.0627,0.0613,0.0437,-0.0265,0.0425,-0.0104,-0.047,-0.038,-0.0969,0.0379,-0.0372,0.0255,-0.0256,-0.041,0.0088,-0.0439,0.0276,-0.0197,-0.0077,-0.0119,0.0321,0.0301,-0.0212,0.0553,-0.0184,0.0085,0.0575,0.0133,0.0278,-0.0192,-0.0705,-0.0259,0.0439,-0.0197,0.0491,0.0266,0.0345,-0.0206,0.0314,0.0033,0.0335,0.0062,0.011,0.0431,-0.0887,-0.0262,0.0921,-0.0202,-0.2949,0.0729,-0.006,0.1007,-0.0236,-0.0012,0.0356,0.0228,-0.0096,-0.01,0.0136,-0.0159,0.0389,-0.0462,-0.0107,0.0006,0.0512,-0.0734,0.0032,-0.026,0.0093,0.0302,0.2307,-0.0211,0.0485,-0.0142,-0.0057,-0.0101,0.0021,-0.0295,-0.004,0.0026,0.0837,-0.0291,0.0256,0.0808,-0.0595,0.0148,0.047,-0.0458,-0.0393,-0.0104,-0.0607,-0.001,0.0714,-0.0038,0.0099,-0.0271,-0.0436,0.0113,-0.0334,0.0136,0.002,0.0016,0.0232,0.0689,-0.052,-0.0448,-0.0007,-0.048,0.0047,-0.0385,-0.0172,-0.001,-0.0011]}
{"key":"[DeepAveragers: Offline Reinforcement Learning by Solving Derived Non-Parametric MDPs] We study an approach to offline reinforcement learning (RL) based on optimally solving finitely-represented MDPs derived from a static dataset of experience. This approach can be applied on top of any learned representation and has the potential to easily support multiple solution objectives as well as zero-shot adjustment to changing environments and goals. Our main contribution is to introduce the Deep Averagers with Costs MDP (DAC-MDP) and to investigate its solutions for offline RL. DAC-MDPs are a non-parametric model that can leverage deep representations and account for limited data by introducing costs for exploiting under-represented parts of the model. In theory, we show conditions that allow for lower-bounding the performance of DAC-MDP solutions. We also investigate the empirical behavior in a number of environments, including those with image-based observations. Overall, the experiments demonstrate that the framework can work in practice and scale to large complex offline RL problems.","layer":1,"vector":[-0.0478,0.0096,0.043,-0.0313,-0.0164,0.0333,0.0055,0.0103,0.0084,-0.0173,0.0685,-0.0249,0.0279,0.046,0.0055,0.0097,-0.003,0.0518,-0.0331,0.0309,0.0213,-0.0869,-0.0485,-0.0485,-0.007,0.0209,-0.074,-0.0681,-0.0328,-0.2202,0.0119,-0.0514,0.0255,-0.0154,0.0268,0.0078,-0.0433,0.0462,-0.0169,-0.0,0.0237,0.0071,-0.0251,-0.0797,-0.0455,-0.0443,-0.0468,-0.0494,0.0057,-0.0172,0.0165,-0.0397,-0.0016,0.0088,0.0569,0.0104,0.0569,0.0504,0.0345,0.0331,0.02,0.0609,-0.1409,0.0262,0.0116,0.0131,-0.032,-0.0178,0.0642,0.0644,-0.039,0.038,-0.0249,0.024,0.039,-0.0114,-0.0255,-0.0203,-0.0056,-0.0492,0.0439,-0.0121,-0.017,-0.001,-0.0157,-0.054,0.0361,-0.0596,0.036,0.0104,-0.0362,0.0192,0.0027,0.0097,-0.0634,0.0056,0.0255,0.0483,-0.0821,0.2221,0.0033,0.0486,0.0403,-0.0276,0.0191,-0.048,-0.0323,-0.0158,-0.0239,-0.0109,-0.0085,0.0019,0.026,0.0075,0.0211,0.0211,0.0461,0.026,0.0071,0.0129,-0.0043,0.0064,0.0429,-0.0145,0.0215,-0.0639,-0.0067,0.1603,0.0061,0.0199,0.0543,-0.0668,-0.0177,-0.0113,0.0476,0.0001,0.0917,0.0296,0.0233,0.0024,-0.0655,0.0318,0.0158,-0.0998,-0.0391,0.091,0.0281,0.0379,-0.0518,-0.0367,-0.0178,0.0135,-0.0138,-0.0234,0.0125,0.0108,0.0006,0.0242,-0.0979,0.0019,-0.0421,-0.0825,-0.0038,0.1277,-0.034,-0.0943,-0.0329,-0.031,-0.0063,-0.0299,0.0402,0.0029,-0.0225,0.0179,0.1033,0.0052,-0.1179,-0.0365,-0.0147,0.0145,0.0119,-0.0582,-0.0285,0.0193,0.0004,-0.0506,0.0059,-0.0544,0.0344,-0.0047,0.0048,0.006,-0.0319,-0.0134,-0.0304,-0.0297,-0.0039,-0.0271,0.0337,-0.0128,-0.0092,-0.0399,-0.008,-0.0043,-0.0058,0.0248,0.0041,0.0049,0.0601,0.0166,-0.047,0.0321,0.0276,-0.0194,-0.0155,0.0103,0.0878,0.0289,-0.001,0.0663,0.0035,-0.0536,-0.0111,-0.2407,0.0033,-0.0086,-0.04,0.0468,-0.0543,0.0211,-0.0433,0.0453,0.0734,0.0736,-0.0866,-0.0085,0.0376,-0.0306,0.0597,0.0583,0.0283,-0.0251,0.0064,-0.0099,0.0228,-0.0229,-0.1106,0.0479,-0.0122,0.2409,0.0233,0.0721,-0.0307,0.013,0.0584,0.0016,-0.0974,0.0359,-0.0057,0.0793,-0.0156,-0.0019,-0.037,0.0035,0.0214,0.007,-0.1268,-0.0532,0.0059,-0.0396,0.0377,-0.0629,-0.031,0.0136,-0.05,0.0576,-0.0396,-0.0428,-0.0224,-0.0732,0.0539,-0.0562,0.0347,0.0108,-0.0363,0.0096,-0.0532,0.05,-0.0279,0.0179,-0.0565,0.0737,-0.0339,-0.0019,0.0419,-0.003,0.0229,0.0213,0.0113,0.0153,-0.0205,-0.0528,-0.008,0.0771,-0.0031,0.0075,0.0698,0.0193,0.0116,0.0502,-0.0329,0.0648,0.0079,0.0029,-0.0024,-0.0901,-0.0028,0.0465,0.0049,-0.2862,0.0676,0.0095,0.0,-0.0159,0.0061,0.0313,0.022,-0.0373,-0.0421,0.015,0.0404,0.0185,0.0378,0.0366,0.024,0.0732,-0.0446,0.0696,-0.066,0.0375,0.049,0.2083,-0.0712,0.0443,0.0031,-0.0486,-0.0056,0.0571,-0.0135,0.0007,0.0011,0.0868,-0.0705,0.0469,0.0806,-0.0204,0.0312,0.034,0.0376,-0.0072,0.0149,0.0169,0.005,0.0952,0.0314,-0.0187,-0.0218,-0.0201,0.0513,0.007,-0.0185,-0.0075,-0.0023,0.0321,-0.011,-0.0178,-0.0625,0.0016,-0.0224,0.0256,-0.0326,0.0146,-0.0282,0.007]}
{"key":"[Zero-shot Task Adaptation using Natural Language] Imitation learning and instruction-following are two common approaches to communicate a user's intent to a learning agent. However, as the complexity of tasks grows, it could be beneficial to use both demonstrations and language to communicate with an agent. In this work, we propose a novel setting where an agent is given both a demonstration and a description, and must combine information from both the modalities. Specifically, given a demonstration for a task (the source task), and a natural language description of the differences between the demonstrated task and a related but different task (the target task), our goal is to train an agent to complete the target task in a zero-shot setting, that is, without any demonstrations for the target task. To this end, we introduce Language-Aided Reward and Value Adaptation (LARVA) which, given a source demonstration and a linguistic description of how the target task differs, learns to output a reward / value function that accurately describes the target task. Our experiments show that on a diverse set of adaptations, our approach is able to complete more than 95% of target tasks when using template-based descriptions, and more than 70% when using free-form natural language.","layer":1,"vector":[-0.0538,-0.0156,-0.0323,-0.0134,-0.0184,-0.0001,0.0428,0.0655,-0.0274,0.001,0.0501,-0.0411,0.0321,0.056,0.0174,0.0017,-0.0357,0.052,-0.057,0.0005,0.0449,-0.0624,0.0183,-0.0409,0.0101,0.0169,-0.04,-0.025,-0.0023,-0.201,0.0278,-0.0426,0.043,-0.0352,-0.0262,-0.0139,-0.0928,0.0375,-0.0277,0.022,0.0153,-0.0057,-0.0043,-0.0478,-0.0345,-0.0665,-0.0344,-0.0563,-0.0214,-0.0485,0.018,-0.056,0.0265,-0.0068,0.0494,0.0165,0.0649,0.0636,0.0478,0.0379,-0.0092,0.0289,-0.1816,0.0931,0.0249,0.0472,-0.061,-0.0084,0.0286,0.0461,0.0025,0.0523,0.0365,0.078,0.05,0.0041,-0.0119,-0.0224,-0.0068,-0.0073,0.0045,-0.0477,-0.0333,0.005,-0.0238,-0.0467,0.031,-0.0292,0.0636,0.019,-0.0466,0.0079,-0.0404,0.0193,-0.0561,-0.0236,0.0116,0.009,-0.0605,0.2095,-0.0017,0.0136,0.0178,-0.0281,0.0604,-0.0247,-0.009,-0.0193,-0.0235,-0.008,-0.0611,-0.0073,0.0223,-0.0354,0.0452,0.032,0.094,-0.0059,0.0278,-0.0201,-0.0087,-0.0022,0.0604,-0.0452,0.0169,-0.0733,0.0182,0.1662,0.0228,0.0515,0.0499,-0.0299,-0.0183,-0.0176,-0.0023,0.0171,0.0222,-0.0224,0.0186,0.0513,-0.0212,-0.0244,-0.0117,-0.0677,-0.0325,0.0891,-0.0104,0.0072,-0.0108,-0.0232,-0.0229,0.0154,0.0016,-0.0201,0.0025,0.0098,0.0882,0.0291,-0.0562,0.0022,-0.0089,-0.0474,-0.0578,0.0552,0.0013,-0.075,-0.0342,-0.0056,0.0111,-0.0318,0.0363,0.0427,-0.047,0.003,0.0827,0.037,-0.0769,-0.0076,0.046,-0.0012,0.0573,-0.0957,0.0022,0.0376,-0.0338,-0.0271,-0.0382,-0.0273,0.0575,0.0534,-0.0164,0.0711,-0.0145,-0.029,-0.0282,-0.0437,0.0046,-0.0203,-0.0216,-0.0386,-0.0306,0.017,-0.0532,0.0049,-0.0038,-0.0162,0.0037,0.0002,0.1044,0.0503,-0.0291,0.0135,0.0149,0.0012,-0.0844,-0.006,0.009,0.021,0.0069,0.0086,-0.0229,0.0115,-0.0166,-0.2188,-0.0112,-0.0015,-0.0341,0.0435,-0.0318,0.0542,-0.0135,0.0324,0.0365,0.0515,-0.0903,-0.0402,0.0136,0.023,0.0324,0.0779,0.0209,0.0258,-0.009,0.0287,-0.0051,0.0076,-0.1025,0.0405,-0.0488,0.255,0.0628,0.0534,-0.0177,0.022,0.0457,-0.0355,-0.0976,0.0526,0.0185,0.0895,-0.0262,0.01,-0.0207,0.0043,0.0214,-0.005,-0.0962,-0.0348,-0.004,-0.0586,0.015,-0.0591,0.0288,0.0274,0.0031,0.0175,-0.0106,-0.1022,-0.0356,-0.0523,0.0366,-0.0197,0.0318,0.0325,0.0064,0.0045,-0.0396,0.0496,0.0182,0.0271,-0.0695,0.0389,0.0186,-0.0143,0.0229,-0.0051,-0.0092,0.0038,0.0314,-0.0201,-0.0407,-0.0268,-0.0182,0.0422,-0.0135,0.0075,0.0143,0.0349,0.0124,0.0748,-0.0617,0.0255,0.0047,0.023,0.0122,-0.0267,-0.0064,0.0492,-0.0456,-0.2978,0.0202,0.0448,0.049,-0.0134,0.0125,0.0292,-0.0144,-0.0717,0.0111,-0.0045,0.0013,0.0167,0.0579,-0.0094,0.0473,0.0966,-0.0256,0.0398,-0.0614,0.0166,0.0793,0.2345,-0.0474,0.0522,-0.0096,-0.0145,-0.0258,0.0716,-0.0126,-0.0001,0.0139,0.0899,-0.0437,-0.0002,0.1007,-0.0316,0.032,0.0211,-0.0103,-0.0726,0.0148,-0.0175,-0.0639,0.0782,0.0216,0.0118,-0.0947,-0.0449,0.0462,-0.0115,0.0387,0.0238,-0.0273,0.0418,-0.0177,-0.0234,-0.0548,-0.0603,-0.0129,-0.0096,-0.0334,0.0287,-0.0018,-0.0327]}
{"key":"[DeepVir -- Graphical Deep Matrix Factorization for \"In Silico\" Antiviral Repositioning: Application to COVID-19] This work formulates antiviral repositioning as a matrix completion problem where the antiviral drugs are along the rows and the viruses along the columns. The input matrix is partially filled, with ones in positions where the antiviral has been known to be effective against a virus. The curated metadata for antivirals (chemical structure and pathways) and viruses (genomic structure and symptoms) is encoded into our matrix completion framework as graph Laplacian regularization. We then frame the resulting multiple graph regularized matrix completion problem as deep matrix factorization. This is solved by using a novel optimization method called HyPALM (Hybrid Proximal Alternating Linearized Minimization). Results on our curated RNA drug virus association (DVA) dataset shows that the proposed approach excels over state-of-the-art graph regularized matrix completion techniques. When applied to \"in silico\" prediction of antivirals for COVID-19, our approach returns antivirals that are either used for treating patients or are under for trials for the same.","layer":0,"vector":[-0.0439,-0.0019,0.0307,0.0185,0.0371,0.0528,0.0049,0.0396,0.026,-0.0309,-0.0062,-0.0358,0.0249,0.0551,0.0163,0.0321,0.0109,0.0449,-0.0615,-0.0153,-0.0109,-0.0219,-0.0217,-0.0561,0.0679,0.0181,-0.0165,-0.0391,-0.0762,-0.2445,-0.0095,-0.0053,0.0756,-0.043,0.0041,-0.04,-0.024,0.0505,-0.0506,0.0056,0.0116,0.0533,0.0266,0.0117,-0.0343,-0.0617,-0.0471,-0.0134,-0.0087,-0.0374,0.0242,-0.0466,-0.0106,0.0455,0.0417,0.0238,0.043,0.0082,0.0061,0.0652,0.0409,0.0082,-0.1723,0.0588,0.0842,0.0133,-0.0366,-0.026,0.0512,0.1174,-0.0261,0.0377,-0.005,0.0183,0.0523,0.0145,0.0328,-0.0134,-0.025,-0.015,0.0234,-0.0018,-0.0025,-0.0336,-0.0159,-0.0166,0.0211,-0.035,0.0411,-0.0213,-0.0349,0.011,0.008,0.0304,-0.0795,-0.0302,0.038,0.0338,-0.0465,0.183,-0.0846,0.0266,0.0044,-0.0273,0.0166,-0.0305,-0.0253,-0.0337,-0.0023,-0.002,0.0237,0.0187,0.0491,-0.0627,0.014,0.0141,0.0673,0.0696,-0.0252,0.0116,-0.0203,0.0108,0.0613,0.014,0.0353,-0.0385,-0.0177,0.1327,0.0458,0.0419,0.0713,0.0277,0.0095,-0.0145,0.043,-0.0254,-0.0064,0.004,0.0269,0.0128,-0.0516,-0.0142,-0.001,-0.0938,-0.0557,0.133,-0.0648,0.0071,-0.0309,-0.0573,-0.0186,0.0385,-0.0162,0.0053,0.0203,-0.017,0.0474,-0.0059,-0.0664,0.052,-0.0553,-0.0662,-0.0561,0.182,-0.0303,-0.0751,-0.0115,0.0053,0.0152,0.0316,0.0284,0.0515,-0.009,0.011,0.0474,0.0188,-0.0832,-0.0367,0.0044,0.0146,0.0274,-0.034,-0.0744,0.0205,-0.0038,-0.045,-0.0029,-0.0474,-0.0046,-0.0229,-0.0232,0.046,0.0019,0.0169,-0.0164,-0.0099,-0.0458,-0.0052,0.0099,-0.0442,0.0324,-0.0199,-0.0084,0.0557,-0.0249,0.0245,-0.0055,-0.0514,0.0122,0.0408,-0.0291,-0.0124,0.0217,-0.046,-0.0178,-0.0135,0.0223,0.0138,-0.0164,0.0604,0.0244,-0.0258,-0.0554,-0.2184,-0.0205,-0.0205,-0.0149,0.0349,-0.0828,0.0373,-0.0137,0.041,0.0899,0.029,0.0018,0.0094,0.0051,-0.0411,0.0673,0.0385,0.0436,-0.0101,-0.0317,0.0055,-0.012,0.0075,-0.0285,0.0308,0.0196,0.244,0.0756,0.0057,0.0079,0.0183,0.0398,-0.0432,-0.1078,0.0471,0.0009,0.076,-0.0109,-0.0164,-0.0553,-0.0023,0.0202,-0.011,-0.0623,-0.0116,-0.0197,-0.0405,0.0089,-0.0567,0.0425,0.086,-0.033,0.0286,0.0185,0.0266,-0.0202,-0.0764,0.0298,-0.0218,0.0222,-0.0088,-0.0765,-0.0007,-0.0498,0.0415,-0.0452,0.0166,-0.0189,0.0194,-0.0274,-0.0192,0.0522,-0.0024,0.013,0.0766,0.01,0.014,-0.0104,-0.0714,-0.0224,0.0719,-0.0469,-0.009,-0.0003,0.0389,0.0021,0.0659,-0.0011,0.0294,-0.0161,0.0465,-0.0138,-0.0581,0.013,0.035,-0.0079,-0.3009,0.0284,0.0251,0.028,-0.0102,-0.0198,0.0548,0.0094,-0.0335,-0.0378,0.0253,0.05,0.0452,-0.0331,-0.0157,-0.0229,0.1216,-0.0574,0.0465,-0.057,-0.0152,-0.0096,0.2592,-0.0616,-0.0046,0.0264,-0.0371,-0.0227,-0.0025,-0.0369,-0.0194,0.0188,0.0724,-0.0418,0.0611,0.0663,-0.0215,0.0369,0.022,0.0344,-0.0287,0.0007,-0.0296,-0.0138,0.0457,-0.0571,0.0052,-0.0716,0.0281,0.0363,-0.0129,-0.0093,0.0181,-0.0056,0.0202,-0.0178,-0.0021,-0.0462,-0.0178,-0.0277,-0.0415,-0.0702,-0.0302,0.0011,-0.0176]}
{"key":"[Adaptive Contract Design for Crowdsourcing Markets: Bandit Algorithms for Repeated Principal-Agent Problems] Crowdsourcing markets have emerged as a popular platform for matching available workers with tasks to complete. The payment for a particular task is typically set by the task's requester, and may be adjusted based on the quality of the completed work, for example, through the use of \"bonus\" payments. In this paper, we study the requester's problem of dynamically adjusting quality-contingent payments for tasks. We consider a multi-round version of the well-known principal-agent model, whereby in each round a worker makes a strategic choice of the effort level which is not directly observable by the requester. In particular, our formulation significantly generalizes the budget-free online task pricing problems studied in prior work. We treat this problem as a multi-armed bandit problem, with each \"arm\" representing a potential contract. To cope with the large (and in fact, infinite) number of arms, we propose a new algorithm, AgnosticZooming, which discretizes the contract space into a finite number of regions, effectively treating each region as a single arm. This discretization is adaptively refined, so that more promising regions of the contract space are eventually discretized more finely. We analyze this algorithm, showing that it achieves regret sublinear in the time horizon and substantially improves over non-adaptive discretization (which is the only competing approach in the literature). Our results advance the state of art on several different topics: the theory of crowdsourcing markets, principal-agent problems, multi-armed bandits, and dynamic pricing.","layer":1,"vector":[-0.0909,-0.0474,0.0141,-0.0123,0.0113,0.014,0.0348,-0.0082,0.0186,-0.0069,0.0637,-0.0203,0.0227,0.0461,0.0191,-0.0134,-0.0106,0.0222,0.0013,-0.0167,0.0094,-0.0692,-0.0082,-0.0647,0.0298,-0.0302,-0.0446,-0.0532,-0.038,-0.206,0.0367,-0.0397,0.0214,-0.0073,0.0211,-0.026,-0.0286,0.0367,0.0057,0.0727,0.0353,0.0306,-0.0576,-0.0548,-0.0111,-0.0502,-0.0165,0.0009,-0.0412,-0.0069,0.0008,-0.0543,0.0221,0.0453,0.0162,0.0222,0.0218,0.0542,0.0454,0.0193,0.0537,0.0506,-0.1288,0.0447,0.0564,0.0233,-0.0287,0.0123,-0.0548,0.0499,0.0179,0.0395,-0.0139,0.034,0.0557,0.0095,-0.026,0.0057,0.0104,0.0134,-0.0371,-0.0506,-0.031,0.0299,-0.0637,-0.0568,0.0538,-0.014,0.0732,0.0401,-0.0158,-0.0032,-0.0317,-0.0205,-0.0739,-0.02,0.0461,0.0517,-0.0607,0.2274,-0.0066,0.0398,0.0113,-0.0453,0.0308,-0.072,-0.0471,-0.0231,-0.0142,0.0475,-0.0244,0.0449,0.0633,-0.0143,0.0002,0.0365,0.0135,0.0504,-0.005,-0.0167,-0.0374,0.0052,0.0531,0.021,0.0078,-0.045,0.0147,0.1628,0.0427,0.0283,0.0646,-0.0371,-0.0559,-0.0588,0.0175,0.0299,-0.017,0.0166,0.0139,-0.0097,-0.0552,-0.0204,-0.0243,-0.0997,-0.0301,0.1143,0.0431,0.0597,-0.0274,-0.0343,-0.0259,-0.0057,0.0015,-0.0083,0.0222,0.0036,0.0786,0.0371,-0.0602,0.0649,-0.0026,-0.0473,0.0065,0.1085,-0.0067,-0.1048,-0.006,-0.0439,-0.0122,0.0112,0.0013,0.0282,-0.0506,0.0567,0.083,0.0179,-0.1008,0.0007,0.0,-0.0026,0.0439,-0.008,-0.0284,0.0155,-0.0165,-0.066,-0.0129,0.0034,0.0423,0.0908,-0.0112,0.0134,-0.0063,0.0049,0.0059,-0.0483,-0.0,-0.0178,0.0226,-0.022,0.0195,0.0083,-0.0633,0.012,0.0633,0.0155,0.0324,-0.0155,0.0353,0.0489,-0.0187,0.0296,0.0052,-0.008,-0.0729,0.0134,0.0555,0.0527,-0.0,0.0286,0.0058,0.0028,-0.0127,-0.2171,-0.0157,-0.0352,-0.0023,0.0315,-0.0087,0.0774,-0.0588,-0.0215,0.0777,0.1088,-0.0715,0.0027,0.0388,0.0058,0.0481,0.0018,0.0116,0.0078,-0.0272,0.0005,0.0175,-0.0617,-0.0504,0.0091,0.0221,0.2175,0.0465,-0.0368,-0.0224,0.0321,0.0448,-0.0325,-0.1107,-0.0006,0.0491,0.0578,-0.0227,-0.0173,-0.0326,0.014,0.0091,-0.0087,-0.0811,-0.0804,-0.029,-0.037,-0.0077,-0.0547,0.0085,0.0238,-0.0245,0.0466,-0.034,0.0039,-0.0415,-0.0501,0.028,-0.0215,0.0458,0.0327,-0.0541,0.005,-0.0174,0.0919,0.0236,0.0264,-0.0485,0.0118,-0.0546,0.0171,0.0826,0.0091,0.0255,0.0182,0.01,0.023,-0.0601,-0.0468,0.0079,0.0357,-0.0468,-0.0027,0.0286,-0.0057,0.0123,0.0559,-0.0243,-0.0149,-0.0346,0.0236,0.0307,-0.114,0.0284,0.0379,-0.0109,-0.3168,0.046,-0.0094,0.0292,-0.0544,0.0093,-0.009,0.0198,-0.051,-0.0274,0.037,0.0915,-0.0191,-0.0233,0.0167,0.0259,0.0705,-0.0294,0.0314,-0.0605,0.0253,0.0259,0.2337,-0.0451,0.032,0.0211,-0.0417,-0.0066,0.0395,-0.0304,0.0031,-0.0293,0.0736,-0.0853,0.0612,0.0639,-0.0503,0.0201,-0.0011,-0.0132,-0.0624,0.0351,-0.0097,-0.0202,0.0962,0.048,-0.0344,-0.0913,-0.0011,0.0326,-0.0399,-0.0244,-0.0271,-0.0224,-0.0043,0.0166,-0.0363,-0.0227,-0.0119,-0.0141,0.0154,-0.0516,0.0076,-0.0369,0.0069]}
{"key":"[Intrinsic persistent homology via density-based metric learning] We address the problem of estimating intrinsic distances in a manifold from a finite sample. We prove that the metric space defined by the sample endowed with a computable metric known as sample Fermat distance converges a.s. in the sense of Gromov-Hausdorff. The limiting object is the manifold itself endowed with the population Fermat distance, an intrinsic metric that accounts for both the geometry of the manifold and the density that produces the sample. This result is applied to obtain intrinsic persistence diagrams, which are less sensitive to the particular embedding of the manifold in the Euclidean space. We show that this approach is robust to outliers and deduce a method for pattern recognition in signals, with applications in real data.","layer":0,"vector":[-0.0203,-0.044,0.0006,-0.01,0.0183,0.0231,0.025,0.0458,0.0233,-0.0442,-0.0035,-0.0938,0.0024,0.0818,-0.005,0.0317,-0.0121,0.0962,-0.0333,0.0461,0.0602,-0.0693,-0.0161,-0.031,0.0112,0.043,0.008,-0.013,-0.0494,-0.2536,0.0578,-0.0374,0.0517,-0.0073,0.0229,-0.0507,-0.0236,0.0447,-0.029,0.0534,0.0032,0.042,-0.0204,-0.029,-0.0181,-0.0726,0.0323,-0.0172,-0.0226,-0.0403,0.006,-0.0485,0.0165,0.0297,0.0496,0.0199,0.0983,0.0082,0.0507,0.0412,0.0364,0.0294,-0.1462,0.0372,0.041,0.0497,-0.0643,-0.0235,0.0148,0.0312,0.0043,0.0136,-0.0166,0.0561,0.0181,-0.0199,0.0099,-0.0405,-0.0189,0.0578,-0.0187,-0.0091,-0.0431,-0.0093,-0.049,-0.0109,0.0532,-0.0748,0.0364,-0.0202,-0.0431,-0.0085,-0.0434,0.0319,-0.0778,-0.0506,0.0226,0.0419,0.0387,0.2187,-0.0732,0.047,0.0249,-0.0139,0.0227,-0.0231,-0.0248,-0.0351,-0.0,0.0039,-0.024,0.0092,-0.0012,-0.0313,0.0243,-0.0131,0.0488,0.0641,-0.012,-0.0091,-0.0462,0.024,0.0883,-0.0046,0.0466,-0.063,-0.0072,0.1008,0.0508,0.0154,0.0388,0.0005,-0.0483,-0.0118,-0.0185,0.0325,0.0043,0.0254,0.0193,0.0196,-0.0603,-0.0678,0.0181,-0.0468,-0.057,0.1418,-0.0653,0.0556,-0.0358,-0.0035,-0.0253,0.017,-0.0069,-0.0258,0.0494,0.0142,0.0251,0.0083,-0.063,0.0286,-0.0585,-0.0775,0.0007,0.1266,-0.005,-0.0688,-0.0337,-0.0012,0.0606,-0.007,0.0479,0.032,-0.0148,0.0492,0.0672,0.0341,-0.0817,-0.0263,0.0452,0.0217,0.0114,-0.0517,-0.0455,0.0107,0.0074,-0.0685,-0.0087,-0.0403,0.0608,0.0837,0.0319,0.0238,-0.0042,-0.0594,-0.0193,-0.0675,-0.002,0.0058,0.0271,-0.0546,0.024,-0.0057,-0.005,0.0043,0.0058,0.0051,-0.0214,-0.0213,0.0093,0.0344,-0.0334,-0.0496,-0.0064,-0.0195,0.0205,-0.0297,0.0199,0.0361,0.0114,0.0475,0.0266,-0.053,-0.063,-0.2426,-0.0287,-0.0036,-0.0093,0.0336,-0.0747,0.0643,0.0142,0.0581,0.0526,0.0173,0.0268,-0.0044,0.0271,-0.0281,0.0858,0.0308,0.0406,-0.0694,-0.0083,-0.0197,0.016,-0.0192,-0.0612,0.0622,-0.0536,0.2352,0.0165,0.0263,-0.0514,-0.0116,0.0091,-0.0192,-0.0829,0.0716,0.0343,0.0303,-0.0016,-0.0027,-0.0271,-0.0524,0.0052,0.0333,-0.0472,-0.0314,-0.0137,-0.0331,0.0051,-0.0611,0.0427,0.0548,-0.0277,0.0767,-0.0232,-0.0454,-0.0548,-0.0553,-0.02,-0.0468,0.0318,-0.0091,-0.0516,-0.0266,-0.0233,0.08,-0.0332,-0.024,-0.049,0.0419,-0.0089,-0.0349,0.0469,0.0099,-0.0527,0.0658,-0.0451,0.0067,-0.003,-0.0427,-0.0191,0.0414,-0.0197,0.0345,0.0274,0.0212,0.0574,0.0967,0.0002,0.0177,-0.0097,0.0567,0.0649,-0.0358,-0.0321,0.0253,0.0288,-0.2687,0.0426,-0.0078,0.0284,-0.0207,-0.0098,0.0347,0.0312,-0.0311,-0.0202,0.0217,0.065,0.0239,-0.0396,0.0171,0.0549,0.0374,-0.0806,0.0275,-0.0896,0.0201,0.0136,0.2142,-0.0293,0.0039,-0.0115,-0.0038,0.0034,0.0267,-0.0609,-0.0103,0.0071,0.083,-0.0393,0.0484,0.0688,-0.006,0.048,0.0227,-0.0407,0.0153,-0.0235,-0.0665,-0.0299,0.095,-0.0243,-0.0414,-0.0267,-0.0069,0.0429,0.008,-0.0251,0.0008,0.0217,-0.0126,0.0367,-0.0595,-0.0068,0.0105,-0.0399,-0.037,-0.0507,-0.0303,-0.0302,0.0097]}
{"key":"[Attention-augmented Spatio-Temporal Segmentation for Land Cover Mapping] The availability of massive earth observing satellite data provide huge opportunities for land use and land cover mapping. However, such mapping effort is challenging due to the existence of various land cover classes, noisy data, and the lack of proper labels. Also, each land cover class typically has its own unique temporal pattern and can be identified only during certain periods. In this article, we introduce a novel architecture that incorporates the UNet structure with Bidirectional LSTM and Attention mechanism to jointly exploit the spatial and temporal nature of satellite data and to better identify the unique temporal patterns of each land cover. We evaluate this method for mapping crops in multiple regions over the world. We compare our method with other state-of-the-art methods both quantitatively and qualitatively on two real-world datasets which involve multiple land cover classes. We also visualise the attention weights to study its effectiveness in mitigating noise and identifying discriminative time period.","layer":2,"vector":[-0.0053,-0.0318,0.0354,-0.0312,0.0813,-0.0181,0.0381,-0.0007,0.0512,-0.01,-0.0096,-0.0707,0.0259,0.0957,0.0237,0.0333,-0.0026,0.0631,-0.0134,-0.025,0.0689,0.0092,-0.0039,-0.0072,0.0183,0.0489,-0.0194,-0.0106,-0.0835,-0.2554,0.0357,-0.0306,0.051,-0.0264,-0.036,-0.0334,-0.0508,0.0221,-0.0256,0.0142,0.0156,0.0023,-0.0098,-0.048,-0.081,-0.0815,-0.0529,-0.0649,-0.0221,-0.0438,0.0388,-0.0245,0.0302,0.0141,0.0172,0.065,0.0518,0.0345,0.0376,0.0362,0.0525,0.0177,-0.1615,0.0361,0.0552,0.0266,-0.025,-0.0115,0.0207,-0.0039,0.0046,0.0583,-0.0179,0.0333,0.028,-0.0275,-0.0352,-0.0078,-0.0126,-0.0505,0.0102,0.029,-0.0418,-0.0365,0.0172,-0.0558,-0.0368,-0.008,0.0579,-0.0096,-0.0187,-0.0391,-0.01,0.0157,-0.0627,-0.0014,0.0355,0.0148,-0.0301,0.1764,-0.1062,0.037,0.0397,-0.0283,-0.0092,-0.0608,-0.0292,-0.0151,-0.0264,0.0233,-0.0174,-0.0048,0.0147,0.0153,0.0136,-0.0121,0.0621,0.0045,0.0051,-0.0088,-0.0207,0.0005,0.0229,-0.0356,0.0797,-0.0384,0.0557,0.1269,0.0564,0.0164,0.0696,-0.0294,-0.065,-0.0349,-0.0361,0.0403,0.0333,0.0195,0.0168,-0.0507,-0.0315,-0.0564,0.0185,-0.1008,-0.0411,0.1285,-0.0143,0.0428,-0.0405,-0.0017,-0.0257,-0.0262,0.0235,-0.0176,0.0412,0.0685,0.0262,0.0471,-0.027,0.0263,-0.022,-0.0066,-0.0319,0.0971,0.0196,-0.0961,-0.022,-0.0084,-0.0188,-0.0098,0.0184,0.0166,-0.0026,0.0138,0.1132,0.012,-0.0671,-0.0046,0.0054,-0.0211,0.03,-0.0804,0.0253,0.0498,0.0111,-0.0509,-0.0241,-0.0176,-0.014,0.0143,0.0131,0.0218,-0.0022,0.0128,-0.0107,-0.0503,-0.0227,-0.017,0.0396,-0.0442,0.0515,0.0029,-0.0071,0.0232,0.0229,0.0171,0.0042,0.0022,0.0398,0.0469,-0.0167,0.0315,0.0364,-0.037,-0.0644,-0.0263,0.056,0.0076,0.0029,0.0791,0.0198,-0.0544,-0.0681,-0.2425,-0.0513,0.0176,-0.0283,0.0218,-0.0409,-0.013,0.0144,0.079,0.0768,0.1071,-0.0543,-0.041,0.0631,-0.0238,0.0287,0.0363,0.0692,-0.048,-0.0285,0.0092,0.0325,-0.0113,-0.045,0.0677,-0.0452,0.2009,0.0222,0.022,-0.0573,0.0397,-0.0182,-0.0228,-0.1272,0.0114,-0.0169,0.0497,0.0026,-0.0314,-0.0589,0.0007,-0.0051,-0.0004,-0.0521,-0.0159,-0.0064,-0.0146,0.0553,-0.0078,-0.0114,0.0309,-0.0779,0.0524,0.0194,-0.0135,-0.0445,-0.0925,-0.0126,-0.0238,0.01,0.0193,-0.0456,0.0262,-0.0688,0.0598,-0.004,-0.0441,-0.027,0.0002,-0.0234,-0.0467,0.0667,-0.016,-0.0429,0.0454,-0.0167,0.078,0.0409,-0.0605,-0.0015,0.09,-0.0331,0.0225,-0.0164,0.0386,0.0256,0.0651,-0.053,0.0349,-0.0135,0.0255,0.0138,-0.0626,-0.0351,0.0609,-0.0177,-0.2799,0.0864,0.007,0.0064,-0.0008,-0.0032,0.0148,0.0479,0.0018,0.0123,-0.0396,0.0562,0.0697,-0.0486,-0.0109,0.0535,0.0401,-0.0269,0.0614,-0.0435,0.0356,0.0375,0.2141,-0.0299,0.0172,0.0333,-0.0523,-0.0131,0.017,-0.0166,0.0163,0.047,0.0874,-0.0744,0.0134,0.0847,-0.0045,0.0167,-0.0364,0.0089,-0.0084,0.0174,-0.0064,-0.0175,0.0757,0.0005,-0.0021,-0.0486,-0.0143,0.0199,-0.0126,-0.0009,-0.0365,0.0461,0.0033,0.0207,-0.0514,0.0087,-0.0609,0.0151,0.0544,-0.0983,-0.0563,-0.0078,0.0189]}
{"key":"[Using AntiPatterns to avoid MLOps Mistakes] We describe lessons learned from developing and deploying machine learning models at scale across the enterprise in a range of financial analytics applications. These lessons are presented in the form of antipatterns. Just as design patterns codify best software engineering practices, antipatterns provide a vocabulary to describe defective practices and methodologies. Here we catalog and document numerous antipatterns in financial ML operations (MLOps). Some antipatterns are due to technical errors, while others are due to not having sufficient knowledge of the surrounding context in which ML results are used. By providing a common vocabulary to discuss these situations, our intent is that antipatterns will support better documentation of issues, rapid communication between stakeholders, and faster resolution of problems. In addition to cataloging antipatterns, we describe solutions, best practices, and future directions toward MLOps maturity.","layer":0,"vector":[-0.0411,-0.0436,0.023,-0.0034,0.0422,-0.0059,0.0378,0.0439,0.0058,-0.0373,0.0217,-0.0331,0.0306,-0.0074,0.0011,0.0053,-0.0055,0.0252,-0.04,0.0137,0.0207,-0.0203,-0.0639,-0.0831,0.0755,0.072,-0.0327,-0.052,-0.0729,-0.2322,-0.0085,-0.0305,0.0618,-0.0387,0.0361,0.0156,-0.0488,0.0555,-0.0107,0.0186,0.0318,0.0123,-0.0234,-0.0724,0.0183,-0.0725,0.0087,-0.0075,-0.0526,-0.0137,-0.0215,-0.0883,0.0343,0.0075,0.022,0.0344,0.0637,0.0381,0.0334,0.0478,0.0311,0.0237,-0.169,0.0582,0.0243,0.0235,-0.0036,-0.0183,0.0347,0.037,0.0068,-0.0111,0.0172,0.0721,0.0113,0.0402,-0.0002,-0.0519,0.0274,0.0574,0.0192,-0.0423,-0.0131,-0.0009,-0.0279,-0.0249,0.0489,0.0134,0.0509,-0.031,-0.0106,0.0127,0.008,-0.0047,-0.0224,0.0195,0.0315,0.0234,-0.0677,0.2529,-0.0246,0.0358,0.0045,-0.0135,0.0142,-0.0513,-0.0407,-0.0279,0.0055,-0.0146,-0.0397,0.0055,0.0133,-0.0451,-0.0108,0.002,0.0394,0.0127,0.0396,0.005,-0.0267,0.0338,0.0353,0.0115,-0.0275,-0.0674,0.0368,0.1174,0.023,0.0118,0.02,-0.0287,-0.0637,-0.0211,0.0332,-0.0071,-0.0345,0.0492,0.0537,-0.0307,-0.046,-0.0223,-0.0122,-0.0945,-0.0535,0.1568,-0.0143,0.032,-0.0221,-0.0141,-0.0526,0.0228,-0.0528,-0.0573,-0.0214,0.049,0.0161,0.054,-0.0479,0.0071,-0.0234,-0.0306,-0.0737,0.121,-0.0067,-0.0884,-0.0167,0.0041,-0.0171,0.0169,0.0098,0.0399,-0.0351,0.0047,0.0485,-0.0136,-0.0595,-0.0705,-0.0111,0.0172,0.0406,-0.0252,-0.0461,0.045,0.0692,-0.0606,0.0178,-0.0473,0.0034,0.0367,-0.0127,0.0173,-0.0187,-0.0121,-0.0499,-0.0272,-0.019,0.0353,0.0044,-0.0338,0.0024,0.0201,-0.0342,0.0268,-0.0148,0.0583,-0.0219,0.0054,0.0459,0.025,-0.029,0.0111,0.0204,-0.0181,-0.0404,0.0149,0.0215,0.0599,0.0039,0.0339,0.0208,0.0073,-0.0589,-0.2587,-0.0162,0.0109,-0.0151,0.0247,-0.04,0.0503,-0.0174,0.0007,0.0846,0.0325,-0.0475,-0.0373,-0.0301,-0.0102,0.0434,-0.0013,0.0442,-0.0744,-0.006,-0.0283,0.0103,-0.0156,-0.0689,0.0404,0.0566,0.2264,0.0365,0.0295,-0.0334,0.0446,0.0027,-0.0196,-0.1001,0.0735,0.0186,0.0614,-0.0179,-0.0226,-0.0095,-0.0492,0.0313,-0.0078,-0.097,-0.0458,-0.0103,-0.0541,0.0022,-0.0247,0.0331,-0.0052,-0.0546,0.031,0.0119,0.0399,-0.0332,-0.0733,0.0497,-0.0159,0.0032,0.023,-0.0668,0.0139,-0.0503,0.0572,-0.0301,-0.0021,-0.0426,0.0268,-0.0169,-0.0105,0.1028,-0.0042,-0.0772,0.0247,0.0275,0.026,-0.0734,-0.0732,0.0016,0.096,-0.0365,0.0324,0.0227,0.0516,0.0354,0.073,-0.0193,0.0641,-0.0505,-0.0225,0.0186,-0.0609,0.0075,0.076,0.0047,-0.26,0.0453,0.0168,0.0227,-0.0447,0.0247,0.0343,0.0185,-0.0072,-0.0196,-0.0103,0.0299,0.0593,-0.0449,0.0422,0.0376,0.0736,-0.0455,0.0411,-0.0362,0.0438,0.0602,0.2363,-0.0648,0.0076,0.0406,-0.0096,-0.0011,0.0875,-0.0042,0.0279,-0.0075,0.0675,-0.02,0.0512,0.0208,-0.0133,0.0217,0.0392,-0.0229,-0.0073,0.0114,-0.0127,-0.0223,0.0773,-0.018,0.0288,-0.0347,0.0017,0.0255,-0.0217,-0.0374,-0.0217,0.0107,0.0058,0.0305,-0.0393,-0.063,-0.0225,-0.0362,0.0227,-0.087,-0.0123,0.0159,-0.0055]}
{"key":"[Combining Unsupervised and Supervised Learning for Asset Class Failure Prediction in Power Systems] In power systems, an asset class is a group of power equipment that has the same function and shares similar electrical or mechanical characteristics. Predicting failures for different asset classes is critical for electric utilities towards developing cost-effective asset management strategies. Previously, physical age based Weibull distribution has been widely used to failure prediction. However, this mathematical model cannot incorporate asset condition data such as inspection or testing results. As a result, the prediction cannot be very specific and accurate for individual assets. To solve this important problem, this paper proposes a novel and comprehensive data-driven approach based on asset condition data: K-means clustering as an unsupervised learning method is used to analyze the inner structure of historical asset condition data and produce the asset conditional ages; logistic regression as a supervised learning method takes in both asset physical ages and conditional ages to classify and predict asset statuses. Furthermore, an index called average aging rate is defined to quantify, track and estimate the relationship between asset physical age and conditional age. This approach was applied to an urban distribution system in West Canada to predict medium-voltage cable failures. Case studies and comparison with standard Weibull distribution are provided. The proposed approach demonstrates superior performance and practicality for predicting asset class failures in power systems.","layer":10,"vector":[-0.0363,-0.0285,0.0567,-0.0099,0.0657,0.0394,0.019,0.0001,0.0047,0.0006,0.0324,-0.0534,0.0088,0.0342,-0.0005,0.0281,0.0057,0.0615,-0.0266,0.0048,0.0415,-0.0191,-0.0435,-0.0311,0.0295,0.0226,-0.0136,-0.0501,-0.0586,-0.2561,-0.0033,-0.0805,0.039,-0.0292,0.0021,-0.0137,-0.0517,0.0458,-0.0455,0.0209,0.0105,-0.0207,-0.0038,-0.0808,-0.0326,-0.0931,-0.0054,-0.0197,0.0151,-0.0337,0.0387,-0.0348,0.0354,0.0237,0.0143,0.0638,0.0205,0.0149,0.0803,0.0322,-0.0028,-0.0018,-0.1905,0.0259,0.0519,0.0593,-0.0013,0.0238,0.0064,-0.0031,0.0247,0.0531,-0.0118,0.0437,-0.0115,0.047,0.0022,-0.0276,-0.0789,-0.0055,0.0303,-0.05,-0.0112,-0.0113,-0.0567,-0.0261,-0.0017,-0.0747,0.0597,0.0286,-0.0175,0.0466,-0.004,0.049,-0.0569,-0.041,0.0129,-0.0128,-0.047,0.2079,-0.0591,0.0458,0.0473,-0.0391,-0.0004,-0.0464,-0.0317,-0.0501,-0.0227,-0.0554,0.0061,0.0168,-0.0316,-0.0463,0.0482,-0.0091,0.0432,0.0372,0.0221,-0.0065,-0.0017,0.0072,0.0772,0.0176,0.0472,-0.0813,0.0229,0.1051,0.0028,-0.0081,-0.015,-0.0241,-0.0863,-0.0374,0.0383,0.0079,0.0427,0.011,0.0245,-0.0125,-0.0143,-0.0388,0.0079,-0.0515,-0.0344,0.093,-0.0466,0.0399,-0.023,-0.0384,-0.0006,0.0067,-0.0111,-0.0452,0.042,0.0192,0.0574,0.0494,-0.0329,0.0103,-0.0096,-0.0563,-0.1117,0.1066,-0.0296,-0.0674,-0.0233,0.0035,0.0173,-0.0282,0.0264,0.0793,-0.009,0.0497,0.0975,0.0182,-0.0229,-0.0183,-0.0327,0.0294,0.04,0.0209,-0.0947,0.0371,0.0649,-0.0391,0.0022,-0.0504,0.0152,0.0545,-0.0407,0.0108,-0.0327,-0.0004,-0.0368,-0.004,-0.0347,-0.0187,0.0232,-0.0168,-0.0046,0.0255,-0.0135,0.0314,-0.0445,0.0752,0.0017,-0.0155,0.0519,0.0064,0.0144,-0.0272,0.1128,0.0097,0.0012,0.0466,0.0236,0.0415,0.0265,0.0519,0.0695,0.008,-0.0545,-0.213,0.003,0.004,-0.0213,0.0659,-0.074,0.0061,-0.0247,0.0213,0.036,0.0685,-0.0075,-0.0142,-0.0144,0.0072,0.0379,0.0483,0.0126,-0.0814,0.0053,-0.0092,0.003,0.0001,-0.0682,0.0831,-0.0126,0.1905,-0.0258,0.0264,-0.0208,0.0212,-0.0191,-0.0304,-0.0556,0.1019,0.0526,0.0511,-0.0063,-0.0725,-0.0293,-0.0546,0.013,-0.0013,-0.0241,-0.0279,-0.0623,-0.0223,0.0507,-0.0795,0.0131,0.0648,-0.0262,0.068,0.0168,0.0469,-0.0466,-0.1036,0.0802,0.0042,-0.0114,0.0195,-0.0325,0.0314,-0.0686,0.0554,-0.0658,-0.0564,-0.0508,-0.059,-0.0759,-0.055,0.1163,0.026,-0.0432,0.0359,-0.0054,0.0102,-0.0033,-0.0369,0.0096,0.0928,-0.0687,0.0377,-0.0063,0.0436,-0.0153,0.0333,0.0332,0.0454,0.0175,-0.029,-0.0111,-0.0402,-0.0168,0.0284,0.0136,-0.2608,0.0124,0.0307,0.0268,-0.0696,-0.0018,0.0249,0.0239,0.0318,0.0316,0.0028,0.0391,0.0524,-0.044,-0.0059,-0.0096,0.0512,-0.0604,0.0473,-0.0438,0.05,0.0608,0.1978,-0.0422,0.0561,0.0065,-0.026,0.0398,0.0289,-0.0361,0.0221,0.0223,0.1048,-0.034,0.037,0.0773,0.0184,0.0042,0.0319,-0.0124,-0.0001,0.0203,-0.0708,-0.0465,0.0963,-0.0155,-0.0514,-0.0678,-0.0172,0.0077,-0.0332,0.0251,-0.0128,-0.0052,0.0263,0.0226,-0.0343,-0.0488,-0.0311,-0.062,0.0357,-0.0313,-0.0124,-0.0225,0.005]}
{"key":"[Multimodal Sentiment Analysis with Word-Level Fusion and Reinforcement Learning] With the increasing popularity of video sharing websites such as YouTube and Facebook, multimodal sentiment analysis has received increasing attention from the scientific community. Contrary to previous works in multimodal sentiment analysis which focus on holistic information in speech segments such as bag of words representations and average facial expression intensity, we develop a novel deep architecture for multimodal sentiment analysis that performs modality fusion at the word level. In this paper, we propose the Gated Multimodal Embedding LSTM with Temporal Attention (GME-LSTM(A)) model that is composed of 2 modules. The Gated Multimodal Embedding alleviates the difficulties of fusion when there are noisy modalities. The LSTM with Temporal Attention performs word level fusion at a finer fusion resolution between input modalities and attends to the most important time steps. As a result, the GME-LSTM(A) is able to better model the multimodal structure of speech through time and perform better sentiment comprehension. We demonstrate the effectiveness of this approach on the publicly-available Multimodal Corpus of Sentiment Intensity and Subjectivity Analysis (CMU-MOSI) dataset by achieving state-of-the-art sentiment classification and regression results. Qualitative analysis on our model emphasizes the importance of the Temporal Attention Layer in sentiment prediction because the additional acoustic and visual modalities are noisy. We also demonstrate the effectiveness of the Gated Multimodal Embedding in selectively filtering these noisy modalities out. Our results and analysis open new areas in the study of sentiment analysis in human communication and provide new models for multimodal fusion.","layer":0,"vector":[0.0043,-0.0018,0.0212,-0.006,0.0038,0.0318,0.0354,0.0077,0.0229,-0.0329,0.0202,-0.0629,0.0233,0.0673,0.0741,0.0264,0.0181,0.0062,-0.0611,-0.0104,0.0544,-0.0222,0.0231,-0.0452,0.0196,-0.0254,-0.0385,-0.0245,-0.0508,-0.2095,0.0353,-0.0346,0.0484,-0.0386,-0.0138,0.0117,-0.0301,0.0512,-0.0325,0.0389,-0.0102,-0.0085,-0.0208,-0.0872,-0.0419,-0.0888,-0.0398,-0.029,-0.0195,-0.0327,0.0522,-0.0533,0.0375,0.042,-0.0187,0.022,0.0594,0.0826,0.0178,0.0512,0.0276,0.0642,-0.1675,0.0515,0.0395,0.0652,-0.0148,-0.0199,-0.0309,0.0473,-0.0164,0.0174,0.0503,0.0315,0.0062,0.0447,0.0322,0.0017,-0.0062,-0.0378,0.0262,0.0011,-0.036,-0.0479,-0.0068,-0.0907,-0.0075,-0.0096,0.0075,0.0205,-0.062,-0.0445,-0.0242,0.0242,-0.0312,-0.0204,0.0442,0.0346,-0.0432,0.2019,-0.026,0.0546,0.0563,-0.0851,0.0534,-0.0483,-0.0034,-0.0249,-0.0265,0.0145,-0.0392,-0.0427,0.0803,-0.0226,0.0819,0.0041,0.0394,0.0306,0.0164,-0.0521,-0.013,-0.0347,0.034,-0.0544,0.031,-0.0971,0.0469,0.1097,0.0367,-0.0059,0.0821,0.0118,-0.0279,-0.0512,0.0029,0.0332,-0.0263,-0.0332,0.0578,-0.0132,-0.0424,-0.0573,0.0116,-0.0717,-0.062,0.1022,-0.037,-0.0146,-0.0554,0.0403,-0.0513,-0.0011,-0.0314,-0.0339,0.0565,0.0177,0.0442,0.045,-0.077,0.0697,0.0027,-0.0526,-0.0127,0.0838,0.0062,-0.1186,-0.0227,0.0199,0.049,-0.0311,0.0642,0.0023,-0.0208,0.0077,0.0591,0.0397,-0.058,0.0131,0.0118,0.0108,0.0465,-0.0207,-0.0521,0.0568,0.001,-0.0595,0.0305,-0.0503,0.0284,0.0202,-0.0139,0.0341,-0.0127,0.0163,-0.0347,-0.0218,-0.0137,-0.0113,0.0164,-0.0305,0.0148,-0.027,-0.0482,0.0213,-0.0133,0.0004,-0.0272,0.042,0.0704,0.0113,-0.03,0.0081,0.0504,-0.0233,-0.0212,-0.006,0.0498,0.0138,0.0245,0.0333,0.0096,-0.0189,-0.058,-0.229,-0.0356,0.0326,-0.0164,0.0506,-0.0756,0.0446,-0.0166,0.1079,0.0905,0.0901,-0.0332,-0.0315,0.0034,0.0432,0.0399,0.0215,0.0539,-0.0304,0.0298,-0.0165,-0.0008,-0.008,-0.0812,0.0553,-0.0244,0.1971,0.0175,-0.0016,-0.0325,0.0473,0.0348,-0.0383,-0.1383,0.0619,-0.021,0.0876,0.0302,-0.0705,-0.023,-0.0426,0.0671,0.0125,-0.0978,-0.0319,-0.0264,0.0059,0.0215,-0.0691,-0.008,0.026,-0.0379,0.0546,0.0333,-0.0413,-0.044,-0.0692,-0.0136,-0.0477,0.0189,-0.0131,-0.0096,0.0726,-0.0982,0.0058,0.0048,0.0077,-0.0423,0.0016,-0.0307,-0.0325,0.0668,-0.0176,0.0122,0.0459,0.0007,0.0567,0.0118,-0.0688,-0.0205,0.0699,-0.0148,0.0149,0.0069,-0.0024,-0.0185,0.0736,-0.0036,0.0382,-0.0062,0.0222,0.007,-0.0449,-0.0278,0.0229,0.0364,-0.3141,0.0391,0.0441,-0.0057,0.0055,0.0097,0.035,-0.0037,-0.0559,0.0372,-0.0262,0.0667,0.01,-0.0459,0.007,0.0128,0.0897,-0.0522,0.0235,-0.0327,0.012,0.0357,0.1932,-0.0472,0.0287,-0.0084,-0.0514,-0.0144,-0.0159,-0.0003,0.021,0.0346,0.0802,-0.0487,-0.0253,0.0266,-0.0608,0.026,0.0295,0.007,0.0677,0.0263,-0.0254,-0.0654,0.0665,0.0321,0.0327,-0.0075,-0.0252,0.0285,-0.0113,0.0257,0.0083,0.0422,-0.0001,-0.0084,-0.0192,-0.0386,0.0063,-0.0173,0.0033,-0.0604,-0.0221,-0.0233,-0.0115]}
{"key":"[Formalizing Generalization and Robustness of Neural Networks to Weight Perturbations] Studying the sensitivity of weight perturbation in neural networks and its impacts on model performance, including generalization and robustness, is an active research topic due to its implications on a wide range of machine learning tasks such as model compression, generalization gap assessment, and adversarial attacks. In this paper, we provide the first integral study and analysis for feed-forward neural networks in terms of the robustness in pairwise class margin and its generalization behavior under weight perturbation. We further design a new theory-driven loss function for training generalizable and robust neural networks against weight perturbations. Empirical experiments are conducted to validate our theoretical analysis. Our results offer fundamental insights for characterizing the generalization and robustness of neural networks against weight perturbations.","layer":5,"vector":[-0.0115,-0.0359,0.0121,-0.0452,0.0109,0.0086,0.0293,0.0282,0.0408,-0.0237,0.0082,-0.0204,0.0786,0.0964,0.0133,-0.0202,-0.0029,0.0443,-0.0523,0.0064,0.0271,-0.0169,0.0014,-0.0327,-0.0106,-0.0097,-0.0365,0.0204,-0.0305,-0.2872,0.0055,-0.059,0.0588,-0.0476,0.0225,0.0014,-0.0408,0.0117,-0.033,0.004,0.0268,0.0373,-0.0215,-0.0569,-0.0034,-0.0206,-0.0137,0.0139,-0.0313,-0.0423,0.0426,-0.0343,0.0291,0.0196,0.0206,0.0373,0.0655,0.0626,0.0285,0.0685,-0.0225,0.0513,-0.1872,0.0448,0.0366,-0.01,-0.0784,-0.025,0.0012,0.0541,-0.0174,0.0316,0.0295,0.0517,-0.0099,0.0242,0.0361,-0.0266,-0.0289,0.0339,0.06,-0.0325,-0.0557,-0.0172,-0.017,-0.0545,0.0333,-0.0566,0.0057,0.004,-0.0291,-0.0316,-0.0114,0.0422,-0.0201,-0.0111,0.0625,-0.006,-0.1012,0.1764,-0.039,0.0097,0.0472,-0.0163,0.0665,-0.0109,-0.0528,-0.0529,-0.0081,-0.0146,-0.0118,-0.0006,0.0086,-0.0353,0.0258,0.0525,0.0283,0.031,-0.0192,0.0125,-0.028,-0.0025,0.0325,-0.0017,0.0592,-0.057,-0.012,0.1453,0.0279,0.0273,0.0207,-0.0337,-0.0404,-0.0238,0.0087,0.0327,0.008,0.0254,0.0565,0.0023,-0.0704,-0.0557,0.0215,-0.0842,-0.0705,0.121,-0.0558,0.0243,-0.0064,-0.0035,-0.017,0.008,-0.0345,-0.0283,0.0362,0.0525,0.0244,0.0411,-0.0307,0.0043,-0.0114,-0.0386,-0.0206,0.0623,0.0237,-0.061,-0.0258,0.0092,0.0263,-0.0306,0.0198,0.0297,-0.012,0.0111,0.0492,-0.0054,-0.0703,-0.0055,-0.0075,0.0417,0.0028,-0.0358,-0.0129,0.0263,0.0142,0.0038,-0.0106,-0.0546,0.0669,0.0457,-0.0515,-0.0053,-0.0693,-0.011,-0.0412,-0.0159,-0.0494,-0.0048,0.0022,-0.0263,0.0182,0.0064,-0.0051,-0.002,0.0086,0.01,0.01,0.0034,-0.0158,0.0422,-0.0412,-0.0418,0.047,-0.0561,-0.0168,-0.0106,0.0357,0.0425,-0.0142,0.0534,0.0455,-0.0332,-0.0654,-0.2423,-0.0286,-0.0103,-0.0411,0.1072,-0.1145,0.0693,-0.0128,-0.0113,0.0346,0.0244,0.035,-0.0489,0.0585,-0.0169,0.0707,-0.0145,-0.0194,-0.0275,-0.0269,-0.0774,0.0618,-0.0078,-0.1053,0.0547,0.0067,0.1781,0.0074,0.0711,-0.0392,0.0206,0.0379,-0.0283,-0.0402,0.1047,0.008,0.0693,-0.0336,-0.0246,-0.0299,-0.0436,0.0147,0.0513,-0.1059,-0.0384,-0.0157,-0.0467,0.005,-0.0789,0.0438,0.0523,-0.0217,0.0599,0.0152,0.0105,-0.0431,-0.1066,0.0393,-0.0784,0.0443,0.0141,-0.1056,0.0025,-0.0626,0.0446,0.034,-0.0138,-0.0218,0.0612,0.0041,-0.0159,0.0606,0.0323,0.0076,0.0471,-0.025,-0.0159,-0.0114,-0.041,-0.0307,0.0566,-0.0034,0.0223,0.0079,0.0532,0.0046,0.071,-0.0189,0.0605,-0.0137,0.0182,0.0291,-0.056,-0.0226,0.061,0.0103,-0.282,0.0436,0.0013,0.0485,-0.0423,0.0197,0.0312,-0.008,-0.0705,-0.046,-0.0079,0.0676,0.0556,-0.0126,0.0009,0.0029,0.0373,-0.0526,0.0623,-0.0152,0.0237,0.0658,0.2192,-0.0234,0.0162,0.0277,-0.0291,-0.0005,0.009,-0.0007,0.0282,-0.001,0.1009,-0.0435,0.0406,0.0815,-0.0282,0.0432,0.0301,-0.011,0.0109,-0.011,-0.0479,-0.0014,0.0923,0.0117,-0.0108,-0.0316,-0.0184,0.0184,-0.0275,0.0435,0.0152,-0.0361,0.0535,0.0397,-0.0505,-0.029,-0.0469,-0.0099,0.0377,-0.0162,-0.0476,-0.0176,-0.0313]}
{"key":"[Flexible Modeling and Multitask Learning using Differentiable Tree Ensembles] Decision tree ensembles are widely used and competitive learning models. Despite their success, popular toolkits for learning tree ensembles have limited modeling capabilities. For instance, these toolkits support a limited number of loss functions and are restricted to single task learning. We propose a flexible framework for learning tree ensembles, which goes beyond existing toolkits to support arbitrary loss functions, missing responses, and multi-task learning. Our framework builds on differentiable (a.k.a. soft) tree ensembles, which can be trained using first-order methods. However, unlike classical trees, differentiable trees are difficult to scale. We therefore propose a novel tensor-based formulation of differentiable trees that allows for efficient vectorization on GPUs. We perform experiments on a collection of 28 real open-source and proprietary datasets, which demonstrate that our framework can lead to 100x more compact and 23% more expressive tree ensembles than those by popular toolkits.","layer":0,"vector":[-0.0052,-0.0119,0.0346,-0.0121,0.0503,0.0291,0.0152,0.0376,0.0304,-0.0217,0.0269,-0.0581,0.0256,0.0642,0.0076,-0.0028,-0.0067,0.0386,-0.0651,0.0052,0.012,-0.0363,-0.0138,-0.0411,-0.0042,-0.0004,-0.0326,-0.03,-0.048,-0.2487,0.0248,-0.021,0.0353,-0.0113,0.0166,0.0189,-0.068,0.0192,-0.0492,0.0619,-0.0057,0.0105,-0.0446,-0.0622,0.0271,-0.0873,-0.039,-0.011,-0.02,0.0233,0.0016,-0.0426,0.0326,0.0157,0.0371,0.0337,0.032,0.0008,0.0534,0.02,0.0076,0.0143,-0.1447,0.0571,0.0547,0.0278,-0.0258,0.0071,0.0501,0.0842,-0.0186,0.0406,0.0229,0.0148,0.0196,-0.0012,0.0271,0.0052,0.0005,0.0173,0.0155,-0.0566,-0.0411,-0.0069,0.0139,-0.0359,-0.0012,-0.0255,0.0803,0.0148,-0.0499,0.0002,-0.0324,0.0274,-0.0333,-0.0268,0.0962,0.0334,-0.0647,0.193,-0.0352,0.025,0.0237,-0.0499,0.0107,-0.0566,-0.0344,-0.0307,-0.0407,0.036,0.0141,-0.0515,0.0266,-0.0446,0.035,0.0054,0.0581,0.0459,-0.0324,-0.0045,-0.0517,0.0167,0.0464,-0.0549,0.0434,-0.0556,-0.0195,0.1281,0.0297,0.037,0.0574,-0.0349,-0.0544,0.0124,0.0665,0.0291,0.0062,0.023,0.004,0.0127,-0.0509,-0.0002,0.0287,-0.0983,-0.0472,0.1247,-0.0329,0.0106,-0.0624,0.0086,-0.0277,0.0351,0.007,0.0135,0.0174,0.0038,0.0392,0.0251,-0.0209,-0.0105,0.0115,-0.0623,-0.013,0.1027,0.0045,-0.0597,-0.0269,-0.0031,0.0122,-0.0,0.0935,0.0246,-0.0107,0.0251,0.068,-0.0294,-0.0998,0.0183,0.0056,-0.0124,0.0279,-0.0526,-0.0342,0.0374,0.0214,-0.0302,0.0063,-0.054,0.02,0.0163,-0.0247,0.0147,-0.0172,-0.0154,-0.0279,-0.0042,-0.0322,-0.0182,0.0333,-0.0029,-0.0154,0.0212,-0.0316,-0.0038,-0.0518,0.0331,-0.0417,0.0263,0.0618,0.0205,-0.0146,-0.0123,0.0638,-0.0313,-0.0613,0.0405,0.037,0.0601,0.0045,0.0479,0.0535,-0.0498,-0.0677,-0.2316,-0.0308,0.0018,0.0058,0.0405,-0.0623,0.0447,0.0002,0.0375,0.0728,0.0642,0.0215,-0.0414,0.024,-0.0046,0.0524,0.0263,0.0158,-0.0148,-0.0292,-0.0051,0.0443,-0.0019,-0.0864,0.0094,0.0104,0.2289,0.0341,0.0123,-0.0324,0.0339,0.0679,-0.0125,-0.083,0.0708,0.0468,0.0705,-0.0286,-0.0374,0.0162,0.0007,0.0498,0.0006,-0.1503,-0.0749,-0.0343,-0.0068,0.0295,-0.0223,0.0361,0.0336,-0.0553,0.0395,-0.0127,-0.0232,-0.0791,-0.124,0.0284,-0.03,0.0007,-0.0041,-0.0614,0.0187,-0.0598,0.04,-0.0233,-0.0544,-0.0576,-0.0003,-0.0408,-0.0406,0.0168,-0.0016,-0.0364,0.0643,-0.0084,0.0571,-0.0122,-0.0076,-0.062,0.0647,-0.0292,0.0297,0.0274,0.0387,0.0471,0.0891,-0.0229,0.0284,-0.049,-0.0014,-0.0003,-0.0179,0.0132,0.0541,-0.0028,-0.3041,0.0188,0.0267,0.027,0.0018,-0.0094,0.045,-0.0167,-0.0458,-0.005,-0.0144,0.0486,0.0232,0.009,-0.0303,0.0315,0.109,-0.0354,0.0028,-0.0164,0.0407,0.0802,0.2144,-0.0156,0.0228,0.0207,-0.0603,-0.0087,0.0505,-0.0181,0.0141,-0.0166,0.1185,-0.0569,0.0271,0.0623,-0.0099,0.0499,0.0082,-0.0162,0.0049,-0.0129,-0.0523,-0.0622,0.0766,-0.0161,-0.0169,-0.0518,-0.0038,0.0323,0.0361,0.0116,-0.0577,-0.0447,0.0044,0.0227,0.0037,-0.053,-0.0491,-0.0483,-0.0031,-0.0534,0.0051,-0.0439,-0.035]}
{"key":"[On the Convergence of Bound Optimization Algorithms] Many practitioners who use the EM algorithm complain that it is sometimes slow. When does this happen, and what can be done about it? In this paper, we study the general class of bound optimization algorithms - including Expectation-Maximization, Iterative Scaling and CCCP - and their relationship to direct optimization algorithms such as gradient-based methods for parameter learning. We derive a general relationship between the updates performed by bound optimization methods and those of gradient and second-order methods and identify analytic conditions under which bound optimization algorithms exhibit quasi-Newton behavior, and conditions under which they possess poor, first-order convergence. Based on this analysis, we consider several specific algorithms, interpret and analyze their convergence properties and provide some recipes for preprocessing input to these algorithms to yield faster convergence behavior. We report empirical results supporting our analysis and showing that simple data preprocessing can result in dramatically improved performance of bound optimizers in practice.","layer":5,"vector":[-0.0537,0.0303,0.1036,-0.0086,0.008,-0.0029,0.0085,0.0346,0.0744,-0.0081,0.01,-0.0329,0.007,0.0152,-0.003,0.038,0.0334,0.0327,-0.0551,0.0381,0.0341,-0.0289,-0.046,-0.0962,0.0898,-0.0048,-0.0546,-0.0326,-0.039,-0.2692,0.0189,-0.0415,0.0453,-0.0068,-0.0085,-0.0211,-0.0163,0.045,-0.0381,0.0596,-0.0125,0.0609,-0.0206,-0.0669,0.007,-0.0483,-0.0298,-0.023,-0.0278,-0.0308,0.0119,-0.0525,0.0035,0.0265,0.0163,0.0369,0.0115,0.0143,0.0299,0.0217,-0.0063,0.0651,-0.158,0.0582,0.0318,0.0216,-0.0212,-0.0185,0.02,0.1172,-0.022,0.0493,0.0198,0.0253,0.0012,0.0214,0.005,-0.013,0.0073,-0.0051,-0.0057,-0.0533,-0.0212,0.0137,-0.0385,0.0106,0.0181,-0.0259,0.0546,0.0135,-0.0322,-0.0285,-0.0419,-0.0077,-0.0905,0.0102,0.0208,0.0729,-0.0525,0.1826,-0.0333,0.0358,0.0201,-0.0211,0.0172,-0.0218,-0.0296,-0.019,-0.0166,-0.0068,-0.0326,-0.0306,-0.0255,-0.0506,-0.003,0.0477,0.005,0.0424,-0.0401,-0.0211,-0.0571,0.0058,0.0636,0.0303,0.0557,-0.0774,0.0033,0.144,0.025,0.0472,0.0348,-0.0377,-0.0321,-0.0313,0.0057,0.0392,-0.003,0.0007,0.0535,-0.0097,-0.0497,-0.0685,-0.0155,-0.1272,-0.0526,0.1419,-0.0401,0.061,-0.0552,-0.0622,-0.0294,-0.0119,-0.0156,-0.027,0.0115,0.0484,0.0113,0.0312,-0.0423,0.0192,-0.0511,-0.0195,-0.0285,0.1222,-0.0102,-0.0535,-0.025,-0.0028,0.0139,0.0295,0.0611,0.065,-0.0067,0.0357,0.0484,0.0378,-0.0599,0.0069,-0.0016,0.0067,0.0075,-0.0367,-0.0386,0.0201,0.0569,-0.0468,0.0136,-0.0376,0.0416,0.0128,-0.0518,0.0022,-0.0053,0.0352,-0.0831,-0.0537,-0.0084,0.0046,0.0361,-0.0116,0.0037,0.018,-0.0211,0.0669,-0.0123,-0.0149,-0.0329,0.001,0.0612,0.0539,-0.024,-0.0375,0.0534,-0.0491,-0.065,0.0309,0.026,0.0238,-0.0178,0.0687,0.016,-0.0108,-0.0615,-0.2449,-0.0366,-0.0219,0.0024,0.0511,-0.0901,0.0252,0.0065,0.0443,0.0612,0.0309,-0.019,-0.044,0.0226,-0.0247,0.0055,0.0352,0.0055,-0.0163,0.0374,0.0102,0.0321,-0.0323,-0.0489,0.0277,-0.0169,0.1575,-0.0044,0.0077,-0.0627,0.0155,0.0128,0.0158,-0.049,0.0102,0.0327,0.0484,-0.0121,-0.0088,0.004,-0.0096,0.027,-0.0135,-0.0779,-0.0416,-0.0438,-0.0514,0.0224,-0.0691,0.0018,0.0605,-0.0127,0.029,-0.0551,-0.0055,-0.0413,-0.0652,0.026,-0.0262,0.0499,-0.0048,-0.0878,0.041,-0.0361,0.0488,-0.0191,-0.0086,-0.0091,-0.0059,-0.045,-0.0318,0.0871,-0.0022,0.0344,0.0338,0.0115,0.0226,-0.0128,-0.0177,0.0113,0.0668,-0.0227,0.0795,-0.0116,-0.0031,-0.0005,0.114,-0.0078,0.0222,0.0075,-0.0216,-0.0061,-0.0613,0.0151,0.0477,-0.0012,-0.2992,0.0296,0.0353,-0.0004,-0.0071,0.0618,0.0677,-0.0148,-0.0468,-0.003,-0.0111,0.0616,0.0489,0.016,0.0136,0.0208,0.0712,-0.0518,0.0045,-0.0929,0.003,0.0628,0.2428,-0.035,-0.0032,0.033,-0.0299,-0.0102,0.0097,-0.0554,0.0095,-0.0095,0.0648,-0.0742,0.0454,0.0927,-0.0444,0.0892,0.0445,-0.0005,0.0314,0.0376,-0.015,-0.0143,0.0637,-0.0039,-0.0206,0.0014,0.0058,0.0043,-0.048,0.0233,0.0226,0.0141,0.042,0.0371,-0.077,-0.0278,-0.0297,-0.0433,0.0393,-0.0551,-0.0474,0.0039,0.0163]}
{"key":"[Task2Vec: Task Embedding for Meta-Learning] We introduce a method to provide vectorial representations of visual classification tasks which can be used to reason about the nature of those tasks and their relations. Given a dataset with ground-truth labels and a loss function defined over those labels, we process images through a \"probe network\" and compute an embedding based on estimates of the Fisher information matrix associated with the probe network parameters. This provides a fixed-dimensional embedding of the task that is independent of details such as the number of classes and does not require any understanding of the class label semantics. We demonstrate that this embedding is capable of predicting task similarities that match our intuition about semantic and taxonomic relations between different visual tasks (e.g., tasks based on classifying different types of plants are similar) We also demonstrate the practical value of this framework for the meta-task of selecting a pre-trained feature extractor for a new task. We present a simple meta-learning framework for learning a metric on embeddings that is capable of predicting which feature extractors will perform well. Selecting a feature extractor with task embedding obtains a performance close to the best available feature extractor, while costing substantially less than exhaustively training and evaluating on all available feature extractors.","layer":0,"vector":[-0.0006,-0.0464,0.0056,0.003,0.0488,0.0256,0.0658,0.0088,-0.0297,0.0163,0.0286,-0.1011,0.0461,0.0811,0.06,0.013,0.0227,0.0741,-0.0448,-0.0096,0.0104,-0.0469,-0.0273,-0.0493,-0.0011,0.032,-0.0443,-0.0058,-0.057,-0.2416,0.0279,-0.0637,0.0481,0.0402,0.0122,-0.0433,-0.0454,0.0307,-0.0353,0.0077,0.0068,-0.0009,-0.0487,-0.0677,-0.0532,-0.0433,-0.0359,-0.0403,-0.011,-0.0279,0.011,-0.0579,0.0215,0.039,0.003,0.0685,0.0508,0.01,0.0599,0.0705,0.0327,0.0059,-0.1411,0.0599,0.0115,0.0458,-0.0623,0.0083,0.0239,0.0363,-0.02,-0.0101,-0.0393,0.0522,-0.0121,-0.0102,-0.0245,-0.0237,0.0005,-0.0137,0.0481,-0.0157,0.0197,0.0103,-0.0026,0.0184,0.0006,-0.0288,0.0626,0.0264,-0.0273,-0.0138,-0.0606,-0.011,-0.0577,-0.0189,0.0605,0.0226,-0.0337,0.1774,-0.0752,0.0191,0.0733,-0.0606,0.0487,-0.0607,-0.044,-0.0156,-0.0532,-0.0177,-0.0022,0.0175,-0.02,-0.0337,0.0417,-0.0138,0.0664,0.036,-0.0124,0.0027,-0.0276,-0.0018,0.036,-0.048,0.041,-0.0538,0.0298,0.1469,0.0554,0.0044,0.0692,-0.0258,-0.0316,-0.0036,0.0114,0.0477,0.014,0.0043,-0.0067,0.0131,-0.0282,-0.0369,0.0008,-0.0597,-0.0954,0.1331,-0.0406,-0.0023,-0.0151,-0.0418,-0.0002,0.0297,-0.0257,-0.0074,0.011,0.0315,0.0406,0.0363,-0.0469,0.0166,0.011,-0.079,-0.0097,0.0895,0.0277,-0.0793,-0.0312,-0.0514,-0.0317,-0.0093,0.0459,0.0339,0.0023,0.023,0.0918,0.0454,-0.0935,0.0022,0.0041,-0.0287,0.0439,-0.0779,-0.0125,0.0085,0.0431,-0.0204,0.0147,-0.0759,0.0094,0.035,0.0295,0.0289,0.0143,0.0453,-0.0095,-0.0114,-0.0058,0.0222,-0.0205,-0.0275,0.0001,0.0178,-0.0048,0.0432,-0.0112,0.0046,-0.0393,0.0429,0.0665,0.0256,-0.052,-0.0107,-0.0017,-0.0063,-0.028,-0.0161,0.022,0.0516,0.0399,0.0439,0.032,-0.0693,-0.032,-0.2307,-0.0303,0.0342,-0.0206,-0.0066,-0.0672,-0.0209,-0.019,0.0358,0.0537,0.0541,-0.0446,-0.0143,0.0064,0.0274,0.0602,0.0586,-0.0032,-0.0355,-0.0468,-0.0079,0.01,0.0268,-0.0705,0.0348,-0.0324,0.2443,0.0602,-0.0093,-0.0343,0.0365,0.0343,-0.0648,-0.1313,0.0316,-0.008,0.044,-0.0131,-0.0187,-0.0055,-0.0355,0.0302,0.0123,-0.1241,-0.0224,-0.0212,-0.0026,0.0105,-0.0449,0.0249,0.0587,-0.0175,0.0291,-0.0215,-0.0654,0.0073,-0.0776,-0.0168,-0.0847,0.0208,0.0277,-0.0297,0.0598,-0.0794,0.0816,0.0209,-0.0366,-0.0608,0.0196,-0.0241,-0.0367,0.0709,-0.0319,-0.0054,0.0607,-0.0088,0.0159,-0.0202,-0.0192,-0.0198,0.0564,0.0066,0.0277,-0.0075,0.0702,0.0217,0.0871,-0.0227,0.0467,-0.0078,0.0284,-0.0003,-0.0127,-0.0105,0.0623,0.0461,-0.2827,0.0661,0.0239,0.0424,-0.0145,-0.0106,0.0767,-0.0027,-0.0367,-0.0175,0.0095,0.0148,0.0519,0.0093,-0.0188,0.0427,0.084,-0.0168,0.0278,-0.026,-0.0188,0.0441,0.2225,-0.0362,0.0173,-0.0281,-0.0239,-0.038,0.0035,0.0087,0.0327,0.0242,0.0919,-0.0435,0.0339,0.0827,-0.0165,0.0107,0.0384,-0.0114,0.0272,-0.0415,-0.0582,-0.0319,0.0793,-0.0035,0.013,-0.022,-0.0431,0.0222,-0.0218,-0.0078,-0.0257,-0.0345,0.0231,-0.0182,-0.0503,-0.0244,-0.0847,-0.0308,0.0511,-0.0583,0.0387,-0.0077,0.0183]}
{"key":"[Whitening for Self-Supervised Representation Learning] Most of the current self-supervised representation learning (SSL) methods are based on the contrastive loss and the instance-discrimination task, where augmented versions of the same image instance (\"positives\") are contrasted with instances extracted from other images (\"negatives\"). For the learning to be effective, many negatives should be compared with a positive pair, which is computationally demanding. In this paper, we propose a different direction and a new loss function for SSL, which is based on the whitening of the latent-space features. The whitening operation has a \"scattering\" effect on the batch samples, avoiding degenerate solutions where all the sample representations collapse to a single point. Our solution does not require asymmetric networks and it is conceptually simple. Moreover, since negatives are not needed, we can extract multiple positive pairs from the same image instance. The source code of the method and of all the experiments is available at: https://github.com/htdt/self-supervised.","layer":0,"vector":[-0.0592,-0.0393,-0.0257,-0.0324,0.0245,0.0365,0.0295,0.0115,-0.0142,-0.0136,0.0231,-0.0601,0.0629,0.0776,-0.0253,0.0172,0.0076,0.1022,-0.0635,-0.0325,-0.0096,-0.0389,-0.0425,-0.0215,0.0428,-0.0482,-0.0397,-0.0159,-0.0248,-0.2738,0.0312,-0.0595,0.0034,-0.0007,0.0547,-0.0527,0.0148,0.0634,-0.0689,0.0058,0.0659,0.004,0.0033,-0.0185,-0.0512,-0.04,0.0025,-0.0578,-0.0136,-0.0579,0.0494,-0.0479,0.0156,0.027,0.0008,0.0524,0.0387,0.0481,0.0198,-0.0133,0.0477,0.0505,-0.1537,0.054,0.0433,0.0364,-0.0668,0.0108,0.0393,0.0375,-0.0238,0.0161,0.043,0.0063,-0.0278,-0.0017,0.0542,0.0002,-0.0199,-0.0271,0.0021,0.0155,-0.0461,-0.0125,0.0053,-0.043,0.0059,-0.05,0.0313,-0.0048,-0.048,-0.0306,-0.0383,0.0475,-0.0583,-0.0578,0.0187,0.033,-0.027,0.2279,-0.0339,0.0096,0.0895,-0.0235,0.0006,-0.0649,-0.0543,-0.0356,-0.0636,-0.0304,-0.0356,-0.011,-0.0109,-0.0515,0.0371,-0.0108,0.0766,0.0589,0.0017,0.0175,0.0129,-0.0051,0.007,-0.0189,0.0448,-0.0604,0.0461,0.1252,0.0063,0.0329,-0.0052,-0.0281,-0.0206,-0.0014,-0.0059,0.0616,0.031,0.0218,-0.005,-0.0002,-0.0194,-0.0544,0.0073,-0.068,-0.0514,0.1073,-0.0374,0.0264,-0.0012,-0.0182,-0.0135,0.0216,-0.0286,-0.0326,-0.0014,0.0592,0.0417,0.0319,-0.0713,-0.0027,-0.0246,-0.0553,-0.0012,0.1042,0.0307,-0.0705,-0.0147,0.0034,0.0175,-0.0087,0.0423,0.0093,0.0003,0.0417,0.0817,0.012,-0.0859,-0.0072,0.0117,-0.0039,0.0428,-0.0652,-0.0118,0.0394,0.0452,-0.0454,0.0077,-0.0286,0.0355,0.0446,0.0022,0.0094,-0.0053,-0.0094,-0.0021,-0.0121,0.0226,-0.0374,-0.0094,0.0087,0.0122,0.0236,-0.0335,0.0504,0.0222,0.0238,-0.0029,-0.0227,0.0323,0.0207,-0.0712,0.0116,0.0595,-0.0169,0.0106,-0.0271,-0.0171,0.0459,0.0147,0.0225,0.0172,-0.0725,-0.0675,-0.2103,0.002,0.0456,-0.0339,0.0144,-0.0919,0.0295,0.0465,0.0743,0.101,0.0611,-0.0127,-0.0267,0.0048,0.0092,0.0216,0.066,0.0642,0.0206,-0.0238,-0.0405,0.0558,0.0159,-0.0654,0.0903,-0.0292,0.2512,0.0546,0.0036,-0.0489,0.0055,-0.0117,-0.0848,-0.0956,0.038,0.0212,0.0552,-0.0166,-0.05,-0.031,-0.005,0.0533,0.0357,-0.0698,0.0061,-0.0335,-0.0379,0.0271,-0.0564,0.0479,0.0618,-0.05,0.0208,-0.0345,-0.0066,-0.02,-0.0499,0.0128,-0.06,0.0004,0.0237,-0.077,0.0291,-0.0923,0.0643,0.0175,-0.0415,-0.0157,0.0276,-0.0407,-0.0201,0.0808,-0.0353,-0.0,0.0333,0.005,-0.0182,-0.0152,-0.0724,-0.0096,0.0467,-0.0281,0.0332,0.0044,0.0354,0.0372,0.0683,0.0284,0.0304,-0.0308,-0.0571,0.0036,-0.0527,0.0155,0.0744,-0.0053,-0.2525,0.0414,-0.0116,0.0764,-0.0415,0.0343,0.0299,0.0161,-0.0571,-0.0596,-0.0274,0.0335,0.0528,0.0187,0.0187,0.0094,0.0412,-0.0261,0.0687,-0.0335,0.0141,0.0098,0.2114,-0.0299,0.0219,0.0227,-0.0388,-0.0155,0.0264,-0.0356,0.0253,0.0136,0.0881,-0.0392,-0.0165,0.0858,-0.0314,0.0033,-0.0062,-0.0151,0.0235,0.0035,-0.0453,-0.0753,0.0951,-0.0075,0.0109,0.0029,-0.0201,0.0213,-0.0124,0.0565,0.0006,-0.0451,0.0149,0.0009,-0.0619,-0.0454,-0.0268,-0.0238,0.0623,-0.0278,-0.0314,-0.0075,0.0396]}
{"key":"[Two-stage Training for Chinese Dialect Recognition] In this paper, we present a two-stage language identification (LID) system based on a shallow ResNet14 followed by a simple 2-layer recurrent neural network (RNN) architecture, which was used for Xunfei (iFlyTek) Chinese Dialect Recognition Challenge and won the first place among 110 teams. The system trains an acoustic model (AM) firstly with connectionist temporal classification (CTC) to recognize the given phonetic sequence annotation and then train another RNN to classify dialect category by utilizing the intermediate features as inputs from the AM. Compared with a three-stage system we further explore, our results show that the two-stage system can achieve high accuracy for Chinese dialects recognition under both short utterance and long utterance conditions with less training time.","layer":2,"vector":[-0.079,-0.0296,0.0221,-0.0143,-0.0091,0.0353,0.016,-0.0076,0.0044,-0.0546,0.0291,-0.0443,0.0531,0.029,0.0293,0.0208,0.0511,0.037,-0.0147,0.0003,-0.0017,-0.0179,-0.0166,-0.0379,0.0008,0.0075,-0.001,-0.0399,-0.0271,-0.2291,0.0424,-0.0039,0.0128,-0.0307,-0.0046,-0.0409,-0.0424,0.0193,-0.0245,0.0273,0.0232,0.0624,0.0551,-0.0802,0.0261,-0.0332,-0.0399,-0.039,-0.0212,-0.0297,-0.0111,-0.019,0.0341,0.0068,-0.0071,0.0844,0.0584,0.0457,0.0356,0.0611,0.0351,0.0043,-0.1964,0.0383,0.0046,0.0603,-0.035,-0.0308,0.0413,-0.0148,-0.0162,0.0362,0.0029,0.0377,-0.0161,-0.0303,0.0494,-0.0246,0.0235,-0.0052,0.025,0.0201,-0.0182,-0.0352,0.0177,-0.0188,-0.0183,-0.0285,-0.0261,-0.029,-0.0309,0.0072,-0.0042,0.0673,-0.0392,-0.0288,0.0131,0.0339,-0.0431,0.198,-0.0284,-0.0148,-0.0062,-0.0725,0.0487,-0.0107,-0.0751,-0.0306,-0.0149,0.0211,-0.0094,-0.0128,0.038,-0.0093,0.0642,0.0105,0.0763,0.0473,-0.0178,0.0116,-0.009,0.0075,0.0105,-0.0501,-0.0139,-0.0677,0.047,0.0991,0.0384,0.0786,0.0634,-0.033,0.0003,-0.0034,0.0199,0.0101,0.0194,-0.0075,-0.0101,-0.0076,-0.038,-0.0923,0.0213,-0.077,-0.0913,0.1159,-0.0468,0.0115,-0.0651,-0.0222,-0.0189,0.0037,0.0005,-0.0795,0.0655,0.0212,0.0828,0.0541,-0.044,0.0097,-0.0352,-0.085,-0.073,0.0807,0.0471,-0.1188,-0.0661,-0.0095,0.0444,-0.0482,0.0618,0.0285,-0.0336,-0.0009,0.0306,0.0386,-0.0606,-0.0243,-0.0028,0.0295,0.046,-0.0599,-0.0333,0.0166,0.0449,-0.0492,-0.0008,-0.0756,-0.0079,0.0589,-0.0505,0.0445,0.0236,0.0115,-0.0159,-0.0253,0.0036,-0.0234,0.0043,-0.0623,0.0337,-0.0062,-0.0081,0.0146,0.0037,0.0021,-0.0012,0.014,0.0215,0.0272,-0.0354,-0.0005,0.0614,-0.022,0.0004,0.0248,0.0276,0.0581,0.0045,0.0574,0.0404,-0.0075,-0.0617,-0.2437,-0.0192,0.0576,-0.0347,0.0838,-0.0568,0.0068,0.0289,0.0716,0.0413,0.0193,0.0143,-0.0001,0.0368,-0.017,0.0861,0.053,0.0043,0.0188,-0.002,0.0616,0.0202,-0.0508,-0.0806,0.0385,-0.0057,0.1888,-0.0062,0.0629,-0.0367,0.0624,-0.0054,-0.0475,-0.1036,0.1044,0.0171,0.0843,-0.0184,-0.0234,-0.0553,-0.027,-0.0099,0.0176,-0.0545,-0.0169,-0.0174,-0.0609,0.0054,-0.0453,-0.0168,0.0206,0.0415,0.0559,0.0235,-0.0334,-0.0064,-0.1241,0.0106,-0.0435,-0.0142,0.0137,-0.033,0.0309,-0.017,0.0479,0.044,-0.0432,-0.055,-0.0033,-0.027,-0.0876,0.0757,0.014,0.0017,0.0409,-0.0103,0.016,-0.0819,-0.041,-0.0109,0.0731,-0.0149,0.04,0.0331,0.0304,0.0275,0.0686,0.0019,0.0299,-0.0098,0.0167,0.0359,-0.0165,-0.021,0.0345,0.0249,-0.3079,0.04,-0.0025,0.0531,-0.0213,0.0375,0.0147,-0.0021,-0.0555,-0.0226,0.0168,0.0488,0.0572,-0.0644,0.002,0.0466,0.0986,-0.0406,0.0496,0.0019,0.017,0.0454,0.1741,-0.0126,0.0611,-0.0554,-0.0276,-0.0263,0.0165,-0.0334,-0.0038,-0.0087,0.0866,-0.0837,-0.0044,0.0271,-0.0337,0.0283,0.0222,0.011,-0.0289,0.0295,-0.0892,-0.0417,0.0865,-0.0163,0.0068,-0.0492,-0.0263,0.0286,-0.0324,0.0054,-0.0081,0.0479,0.0313,0.04,-0.0449,-0.0491,-0.0292,-0.0157,0.0308,-0.0704,0.0025,0.0029,-0.0006]}
{"key":"[Identifying Documents In-Scope of a Collection from Web Archives] Web archive data usually contains high-quality documents that are very useful for creating specialized collections of documents, e.g., scientific digital libraries and repositories of technical reports. In doing so, there is a substantial need for automatic approaches that can distinguish the documents of interest for a collection out of the huge number of documents collected by web archiving institutions. In this paper, we explore different learning models and feature representations to determine the best performing ones for identifying the documents of interest from the web archived data. Specifically, we study both machine learning and deep learning models and \"bag of words\" (BoW) features extracted from the entire document or from specific portions of the document, as well as structural features that capture the structure of documents. We focus our evaluation on three datasets that we created from three different Web archives. Our experimental results show that the BoW classifiers that focus only on specific portions of the documents (rather than the full text) outperform all compared methods on all three datasets.","layer":0,"vector":[-0.0376,0.0208,0.0255,0.013,0.0632,-0.0013,0.0352,0.0208,-0.0071,-0.0498,-0.0014,0.0301,0.0376,0.0487,0.0042,0.0216,-0.019,0.0299,-0.0814,0.0285,0.0695,-0.0068,0.0139,-0.0141,0.0173,0.0317,-0.0465,-0.0158,-0.0394,-0.2284,0.0047,-0.0853,0.0567,-0.0302,0.046,-0.0256,0.011,0.0483,-0.0766,0.0419,0.0207,0.0019,-0.055,-0.0219,-0.0472,-0.0345,-0.0024,-0.0394,-0.0319,-0.0348,0.0301,-0.0245,-0.0076,0.0452,-0.0304,0.0425,0.0606,0.0179,0.0125,0.0295,0.0495,0.0517,-0.1645,0.0619,0.0139,0.0465,-0.0626,-0.0099,0.0144,0.0528,-0.0032,0.0508,-0.0313,0.0667,-0.0055,0.021,0.0327,-0.0192,-0.0114,0.0141,0.0152,-0.0024,-0.0524,-0.0159,-0.0505,-0.0636,-0.0075,-0.0575,0.019,-0.0075,-0.0281,0.004,0.01,0.0346,-0.1018,-0.0368,0.0087,0.0557,-0.0012,0.2207,-0.0618,0.0579,0.0777,-0.074,0.0114,-0.0169,0.025,-0.0312,-0.0041,-0.03,-0.0262,-0.0146,-0.0002,-0.0286,0.0637,0.0013,0.0502,0.0576,-0.0186,-0.0227,-0.0238,0.039,0.0793,-0.0161,0.0076,-0.0253,0.0605,0.0984,0.0454,0.0201,0.0157,-0.0007,-0.0362,-0.0219,0.0261,0.0319,0.0214,0.027,0.0054,-0.0165,-0.0266,-0.0538,0.0092,-0.0452,-0.1229,0.1348,-0.0395,0.0237,-0.0637,-0.0685,0.0095,0.0419,0.0043,-0.0466,0.0377,0.0254,0.0098,0.0591,-0.0317,-0.016,-0.0053,-0.0318,-0.0288,0.1084,0.0207,-0.083,-0.0321,0.008,-0.0141,-0.0098,0.0544,0.0325,-0.0411,0.0454,0.0779,0.0064,-0.044,-0.0108,0.0169,0.008,0.0364,-0.0683,-0.0783,0.0199,0.065,-0.0354,-0.0215,-0.0486,0.0504,0.0622,0.002,0.0493,-0.013,-0.0296,0.0118,-0.0214,-0.022,0.0006,-0.0313,-0.0641,0.0062,0.0227,-0.0269,0.0056,-0.0035,-0.0038,-0.0418,-0.001,0.0364,-0.0203,-0.0468,-0.0158,0.0428,-0.0265,-0.035,-0.052,0.0426,-0.0008,-0.0037,0.073,0.0398,-0.1194,-0.0285,-0.2218,0.0058,-0.0162,-0.0118,0.0772,-0.0687,0.0789,-0.0021,0.0286,0.058,0.001,-0.0032,-0.0493,-0.0032,-0.0013,0.0751,0.0422,0.0416,-0.0468,0.034,-0.0106,-0.0215,-0.0018,-0.0593,0.0231,0.019,0.1926,0.0585,0.0105,-0.0382,0.0021,0.0016,-0.0262,-0.1086,0.0536,0.0189,0.0483,0.024,-0.0266,0.0101,-0.0376,0.046,0.0421,-0.0886,-0.0133,-0.0235,-0.0668,-0.0209,-0.0462,-0.003,0.0223,-0.0078,0.0321,0.0429,-0.0347,0.0026,-0.0632,0.0315,-0.0275,-0.0032,-0.0024,-0.0694,0.0602,-0.0654,0.031,0.0004,-0.0399,-0.0069,0.0279,-0.0851,-0.0374,0.0734,-0.0363,-0.0129,0.0235,0.0103,0.0434,-0.0097,-0.0253,-0.0498,0.0623,-0.0316,0.0491,0.0221,0.0113,0.0265,0.0402,-0.0323,0.0383,0.0023,0.0115,0.0083,-0.032,-0.0193,0.0166,0.0222,-0.318,0.0487,0.0361,0.0275,0.0071,0.0199,0.0498,0.0178,0.0001,-0.0015,0.0204,0.0331,0.0144,-0.0834,0.0061,0.058,0.0549,-0.0183,0.046,-0.0398,0.0006,0.0491,0.2408,-0.036,-0.0077,-0.0076,-0.0116,0.0319,-0.0,-0.0036,0.015,-0.025,0.1318,-0.0434,0.0018,0.0988,-0.028,0.0203,-0.0127,0.0005,-0.0114,0.016,-0.1003,-0.0343,0.0838,0.0027,0.0055,-0.0415,0.0057,0.0157,-0.0275,0.009,-0.0288,0.0,0.0571,0.0081,-0.0157,-0.0383,-0.0352,-0.0609,0.0095,-0.0403,-0.019,0.0035,0.0044]}
{"key":"[SocialGuard: An Adversarial Example Based Privacy-Preserving Technique for Social Images] The popularity of various social platforms has prompted more people to share their routine photos online. However, undesirable privacy leakages occur due to such online photo sharing behaviors. Advanced deep neural network (DNN) based object detectors can easily steal users' personal information exposed in shared photos. In this paper, we propose a novel adversarial example based privacy-preserving technique for social images against object detectors based privacy stealing. Specifically, we develop an Object Disappearance Algorithm to craft two kinds of adversarial social images. One can hide all objects in the social images from being detected by an object detector, and the other can make the customized sensitive objects be incorrectly classified by the object detector. The Object Disappearance Algorithm constructs perturbation on a clean social image. After being injected with the perturbation, the social image can easily fool the object detector, while its visual quality will not be degraded. We use two metrics, privacy-preserving success rate and privacy leakage rate, to evaluate the effectiveness of the proposed method. Experimental results show that, the proposed method can effectively protect the privacy of social images. The privacy-preserving success rates of the proposed method on MS-COCO and PASCAL VOC 2007 datasets are high up to 96.1% and 99.3%, respectively, and the privacy leakage rates on these two datasets are as low as 0.57% and 0.07%, respectively. In addition, compared with existing image processing methods (low brightness, noise, blur, mosaic and JPEG compression), the proposed method can achieve much better performance in privacy protection and image visual quality maintenance.","layer":7,"vector":[-0.0075,0.0018,-0.002,-0.0406,0.0471,0.0107,0.0672,-0.0256,0.0166,0.0017,0.0166,-0.0137,0.0289,0.0325,0.01,0.026,0.0362,0.02,-0.0766,0.0079,0.002,-0.0536,-0.0369,-0.0704,0.0157,-0.0002,-0.0281,-0.0722,-0.0663,-0.1871,0.0194,-0.0769,0.0121,-0.0212,0.0286,-0.0297,-0.0313,0.0162,-0.0784,0.0371,0.0144,0.0283,-0.0307,-0.0935,-0.0419,0.0165,-0.0374,-0.0364,0.0057,-0.0253,0.0572,-0.0325,0.0253,0.0455,0.0249,0.0594,0.1044,0.044,0.017,0.0159,0.0451,0.0345,-0.1409,0.0452,0.0302,0.0541,-0.0173,-0.0033,0.0368,-0.0117,-0.0096,0.0728,0.0323,0.0166,0.0029,0.0335,0.0051,-0.0401,-0.0489,-0.0347,0.0163,0.0212,0.0228,0.0233,-0.0112,-0.047,0.0466,-0.0765,0.0757,-0.0133,-0.0603,-0.0291,0.0163,0.0436,-0.0449,-0.0158,-0.0085,0.013,-0.0743,0.2171,-0.06,0.0324,0.0638,-0.0738,0.0407,-0.0333,-0.0218,-0.0346,-0.0184,0.042,-0.0083,-0.0448,0.0605,-0.0215,0.074,-0.0055,0.0449,0.0384,-0.0153,0.002,-0.001,0.022,0.0796,0.0305,0.0412,-0.0451,0.0062,0.1405,0.031,0.0225,0.0245,-0.0137,-0.0236,-0.0202,0.0066,0.0204,-0.0359,0.0341,0.0228,-0.0531,-0.0194,-0.0167,0.0296,-0.0482,-0.006,0.1123,0.0138,0.0479,-0.0343,-0.0305,0.0155,0.0329,-0.0522,0.0019,0.0307,0.014,0.0218,0.0862,-0.0491,0.0171,-0.0149,-0.0631,-0.0472,0.0902,0.0266,-0.1144,0.0151,0.0227,0.0043,-0.0357,-0.0044,0.0192,-0.0415,0.0174,0.0789,0.0341,-0.0626,0.0012,-0.0122,-0.0026,-0.0173,-0.0555,-0.0498,0.0179,0.0356,-0.0363,0.0212,-0.0606,0.0157,0.0549,-0.0432,0.0068,-0.0854,-0.0127,-0.0446,-0.0236,-0.0494,-0.0706,-0.0298,-0.0081,0.0082,-0.0408,-0.0289,0.0111,-0.0027,0.0351,0.0186,-0.0643,-0.0014,0.0423,-0.0735,-0.0372,0.0089,-0.0184,-0.0091,-0.0141,0.0277,0.0313,-0.0038,0.0239,0.0465,-0.0383,-0.0532,-0.2548,0.0055,-0.0109,-0.0294,0.0389,-0.095,0.0387,-0.0036,0.0726,0.0278,0.1168,-0.0422,-0.0126,0.0445,0.018,0.0606,0.0262,0.0288,-0.0316,-0.0214,-0.0331,0.0538,0.031,-0.0863,0.0691,0.0082,0.2108,0.0451,0.0302,-0.018,0.016,0.0556,-0.0176,-0.1393,0.0326,0.0217,0.026,-0.017,-0.0494,-0.0275,0.014,0.0552,0.0157,-0.0924,-0.0005,-0.0613,-0.0258,0.0277,-0.0709,0.0296,0.0563,-0.0034,0.0355,-0.0133,0.0017,-0.0363,-0.1001,0.0078,-0.0474,0.0727,-0.019,-0.0682,-0.0048,-0.0709,0.0758,0.021,-0.071,-0.0706,0.0099,0.0037,-0.0054,0.0828,0.0585,-0.0101,0.0173,-0.0014,0.0585,-0.0204,-0.0563,-0.031,0.1059,0.0348,0.0037,0.0591,0.0458,0.0442,0.1082,0.0065,-0.0057,-0.0519,0.0418,-0.0168,-0.079,-0.0508,0.0527,-0.0163,-0.2967,-0.0113,-0.0107,0.0721,-0.0202,0.0597,0.0334,0.0508,-0.0226,-0.0345,-0.0296,0.0339,0.0441,-0.0117,-0.0195,0.0291,0.0145,-0.0103,0.0315,0.0079,0.0007,-0.0044,0.1681,-0.049,0.0053,0.0002,-0.0167,0.0259,0.0132,-0.027,-0.0205,0.0367,0.0571,-0.0408,0.0097,0.0519,-0.0281,0.0082,0.0043,-0.0017,-0.0292,0.0206,-0.0275,-0.0067,0.0963,-0.0304,0.0006,0.0222,0.0419,0.0109,-0.0233,-0.0287,-0.0056,-0.0042,0.0525,0.0253,-0.0739,-0.0023,-0.0073,-0.0064,0.0017,-0.0021,-0.0457,0.015,-0.0068]}
{"key":"[Zeroth-order non-convex learning via hierarchical dual averaging] We propose a hierarchical version of dual averaging for zeroth-order online non-convex optimization - i.e., learning processes where, at each stage, the optimizer is facing an unknown non-convex loss function and only receives the incurred loss as feedback. The proposed class of policies relies on the construction of an online model that aggregates loss information as it arrives, and it consists of two principal components: (a) a regularizer adapted to the Fisher information metric (as opposed to the metric norm of the ambient space); and (b) a principled exploration of the problem's state space based on an adapted hierarchical schedule. This construction enables sharper control of the model's bias and variance, and allows us to derive tight bounds for both the learner's static and dynamic regret - i.e., the regret incurred against the best dynamic policy in hindsight over the horizon of play.","layer":1,"vector":[-0.0362,-0.0297,0.0417,-0.0294,-0.0397,0.0588,-0.0103,0.0282,0.0316,-0.014,0.0308,-0.0171,0.0286,0.0774,0.0044,0.0072,0.01,0.0231,-0.0247,0.0227,0.0341,-0.0699,-0.0015,-0.0706,0.0379,0.019,-0.0408,-0.0213,-0.0379,-0.2503,0.0358,-0.0503,0.0201,-0.0239,-0.014,-0.0276,-0.0469,0.0579,-0.0476,0.046,0.0079,0.032,-0.0399,-0.0705,0.0021,-0.0564,-0.0133,0.0127,-0.0282,-0.0498,0.0186,-0.0347,0.0222,0.0146,0.0464,0.0413,0.041,0.0601,0.0359,0.0737,0.0127,0.0025,-0.159,0.0357,0.0513,0.0206,-0.0508,0.0247,0.0189,0.0735,-0.0264,0.0013,0.029,0.0701,0.0439,-0.05,0.0228,-0.0296,-0.0065,-0.0172,0.0167,-0.0507,-0.0393,0.0019,-0.0197,-0.0618,-0.0093,-0.05,0.0659,0.0307,-0.0359,-0.0213,-0.0074,0.0018,-0.0634,0.0191,0.0408,0.034,-0.0493,0.2082,-0.0453,0.0437,0.0425,-0.0488,0.0056,-0.052,-0.0122,0.0036,0.0136,-0.0431,-0.0062,0.0014,0.0454,-0.0361,0.0429,0.0413,0.0663,0.0214,0.031,-0.0163,-0.0402,0.0098,0.0622,-0.0296,0.0063,-0.0946,0.0181,0.1581,0.0109,0.0463,0.0321,-0.0982,-0.0238,-0.018,0.0266,0.0079,-0.0019,-0.0085,0.0236,-0.0055,-0.0586,-0.0327,-0.0043,-0.0964,-0.0267,0.1612,0.0429,0.0282,-0.0323,-0.0148,-0.0133,-0.041,-0.0052,-0.0293,-0.0207,0.0268,0.0159,0.0243,-0.0343,0.0014,-0.0634,-0.037,-0.004,0.1071,-0.0265,-0.0533,-0.0237,-0.0287,0.0327,-0.0174,0.0441,-0.022,-0.0195,0.0084,0.1,0.0437,-0.088,-0.0437,0.032,0.0221,0.0209,-0.0372,-0.0109,0.0041,0.0443,0.0154,0.0265,-0.0119,0.0285,0.015,-0.0327,-0.0213,-0.0045,-0.0333,0.0036,-0.051,0.0377,-0.0201,0.0481,0.0044,0.0185,-0.0081,-0.054,0.0543,0.0324,0.0367,0.0075,-0.0243,0.0515,0.017,-0.0345,-0.02,0.0357,-0.0161,-0.044,0.027,0.0683,0.0474,-0.0337,0.0346,0.0238,-0.0211,-0.0332,-0.2189,-0.0009,-0.0344,-0.0093,0.0331,-0.0449,0.0694,-0.0593,0.0443,0.0814,0.0536,-0.042,-0.0081,0.0244,0.0035,0.0646,0.0452,0.0196,-0.0196,-0.0023,-0.0301,0.0385,-0.0433,-0.0652,0.0804,-0.0424,0.2024,0.0327,0.0858,-0.0187,0.0597,0.0728,0.0134,-0.0631,0.0202,0.0095,0.0447,-0.0447,-0.0614,-0.0558,-0.0175,-0.0113,-0.0027,-0.0988,-0.0472,-0.0296,-0.0631,0.0488,-0.0701,-0.0121,0.0411,-0.0307,0.0272,-0.0486,-0.0284,-0.0454,-0.0902,0.0395,-0.0578,0.0349,0.0472,-0.0214,0.0009,-0.0808,0.0464,-0.0041,0.0124,-0.0368,0.0253,-0.0437,-0.0337,0.0439,-0.0053,0.0196,0.0165,0.0198,0.0314,-0.0373,-0.0678,0.0193,0.0468,-0.0749,0.0256,0.0507,0.0365,-0.013,0.0673,-0.0222,-0.0038,0.0041,-0.0187,0.0016,-0.0922,0.0111,0.0562,0.0016,-0.2917,0.0343,0.0203,0.0013,-0.0194,-0.0019,0.0288,-0.0139,-0.0385,0.025,0.0156,0.0714,0.0023,0.0454,0.0265,0.0313,0.0571,-0.0368,0.0471,-0.0714,0.0283,0.0875,0.1943,-0.05,0.0357,0.0084,-0.0275,-0.0099,0.0216,-0.0317,0.028,0.0049,0.077,-0.0599,0.0616,0.0545,-0.03,0.0615,-0.0253,-0.0008,-0.0402,0.0065,-0.0188,0.0204,0.1073,0.0213,-0.0264,-0.0278,-0.0215,0.008,-0.0024,0.0456,0.0234,0.0178,0.005,0.0058,-0.033,-0.0959,-0.0627,-0.0457,0.0272,-0.0268,-0.0227,-0.0378,0.004]}
{"key":"[Gradient Flows for Regularized Stochastic Control Problems] This paper studies stochastic control problems regularized by the relative entropy, where the action space is the space of measures. This setting includes relaxed control problems, problems of finding Markovian controls with the control function replaced by an idealized infinitely wide neural network and can be extended to the search for causal optimal transport maps. By exploiting the Pontryagin optimality principle, we identify suitable metric space on which we construct gradient flow for the measure-valued control process along which the cost functional is guaranteed to decrease. It is shown that under appropriate conditions, this gradient flow has an invariant measure which is the optimal control for the regularized stochastic control problem. If the problem we work with is sufficiently convex, the gradient flow converges exponentially fast. Furthermore, the optimal measured valued control admits Bayesian interpretation which means that one can incorporate prior knowledge when solving stochastic control problem. This work is motivated by a desire to extend the theoretical underpinning for the convergence of stochastic gradient type algorithms widely used in the reinforcement learning community to solve control problems.","layer":3,"vector":[-0.0319,-0.0127,0.0556,-0.0099,-0.0336,0.0281,0.0506,0.0409,0.0739,0.0242,0.0046,-0.0249,0.0198,0.1289,0.023,0.0681,-0.0207,0.024,-0.0408,0.0014,0.0381,-0.0351,0.0272,-0.0766,0.0127,0.0326,-0.0446,-0.0581,-0.041,-0.2231,0.0379,-0.0626,0.0076,-0.0737,0.0328,-0.0229,-0.0376,0.056,-0.0414,0.0457,0.0075,0.0563,-0.0483,-0.0601,-0.0176,-0.0682,0.0013,-0.0297,-0.0555,-0.0163,0.0161,0.0095,0.0361,0.019,0.0489,0.0205,0.0255,0.0488,0.0378,0.0264,-0.0194,0.0156,-0.1923,0.0601,0.0513,0.0484,-0.0289,0.0037,0.0209,0.0485,-0.0648,0.0424,-0.0199,0.0752,0.0533,-0.0226,-0.0045,-0.0198,-0.0075,0.0317,0.0334,-0.0192,-0.0479,-0.0105,-0.0205,-0.0576,0.034,-0.0497,0.0557,0.0344,-0.0175,-0.0077,0.0092,0.0236,-0.0151,0.0042,0.0183,0.0427,-0.0294,0.1791,-0.021,0.0571,0.0311,0.018,0.053,0.0032,-0.043,-0.0085,-0.0089,0.0289,-0.0472,-0.0156,0.0563,-0.0405,0.0065,0.0144,0.0058,0.0569,-0.048,-0.0087,-0.0434,-0.0079,0.0775,-0.0076,0.0283,-0.0746,-0.0144,0.1674,0.0115,0.0134,0.0663,-0.0392,-0.0431,-0.0429,0.0221,-0.0116,-0.0112,-0.045,0.0267,-0.0138,-0.047,-0.0196,0.0002,-0.183,-0.079,0.1098,-0.022,0.0304,-0.0198,-0.0266,0.0003,-0.0233,0.0041,-0.0324,-0.0175,0.0717,0.0342,0.0435,-0.0911,0.007,-0.0349,-0.0223,-0.0317,0.1032,-0.0286,-0.0494,-0.0128,-0.0395,-0.0048,-0.0175,0.0142,-0.0159,-0.0198,0.0105,0.0705,0.0375,-0.0948,-0.0008,-0.0257,0.007,0.0289,-0.0428,-0.002,-0.0008,0.0401,-0.0174,0.0516,-0.0377,-0.0199,0.0719,0.0052,-0.0098,0.0091,-0.0417,-0.0216,-0.0568,-0.0084,-0.0252,-0.0023,-0.0139,0.0043,-0.0191,-0.0451,0.0382,-0.0184,0.0137,-0.0493,-0.0102,0.0246,0.0099,-0.0503,0.0098,0.0432,-0.0331,-0.0322,0.0568,0.0152,-0.0159,-0.0226,0.0171,0.0518,0.0239,-0.0584,-0.2244,-0.0282,-0.0572,0.0177,0.0299,-0.057,0.0054,-0.0229,0.047,0.0509,0.0712,-0.0315,-0.0116,0.0157,0.0226,0.0545,0.0731,0.0376,-0.0121,0.0053,0.0001,-0.0003,-0.0254,-0.0618,0.0864,-0.0479,0.1976,0.0222,0.1086,-0.0251,0.0193,0.02,0.0095,-0.0453,0.038,0.0236,0.0666,-0.0063,-0.0015,-0.0408,0.022,0.0138,-0.0441,-0.0781,-0.026,-0.0428,-0.0197,0.025,-0.0735,-0.0264,0.0277,0.0084,0.0291,-0.0256,-0.0118,0.008,-0.0601,-0.0032,-0.0262,0.0427,-0.0016,-0.0382,0.0142,-0.0476,0.033,0.0112,0.0385,-0.0447,-0.0057,0.0072,0.0007,0.0417,-0.0157,-0.0081,0.0396,0.0111,-0.005,0.0231,-0.0405,0.0014,0.0562,-0.0247,0.0391,0.0614,-0.0029,0.0167,0.0883,-0.0313,0.0006,-0.0036,0.0196,0.0042,-0.0752,0.0106,0.0339,0.0127,-0.2947,0.0596,0.0139,0.0245,-0.0651,-0.0031,0.0589,-0.0192,-0.0717,0.0061,0.0224,0.0455,0.043,0.016,0.0068,0.0167,0.0555,-0.026,0.0887,-0.0717,-0.0088,0.0594,0.2387,-0.0376,0.0291,-0.0034,-0.0004,0.0068,0.0404,-0.053,0.0237,0.0283,0.041,-0.0713,0.0791,0.081,-0.0129,0.0037,0.0144,-0.0029,-0.0224,-0.0052,-0.0096,0.001,0.0779,0.0121,-0.0318,-0.0308,-0.0217,0.0064,0.001,0.0515,-0.0134,-0.0475,0.025,0.02,-0.0664,-0.0859,-0.022,-0.0597,-0.0277,-0.1096,0.0308,-0.0007,-0.0043]}
{"key":"[Multiscale Residual Mixture of PCA: Dynamic Dictionaries for Optimal Basis Learning] In this paper we are interested in the problem of learning an over-complete basis and a methodology such that the reconstruction or inverse problem does not need optimization. We analyze the optimality of the presented approaches, their link to popular already known techniques s.a. Artificial Neural Networks,k-means or Oja's learning rule. Finally, we will see that one approach to reach the optimal dictionary is a factorial and hierarchical approach. The derived approach lead to a formulation of a Deep Oja Network. We present results on different tasks and present the resulting very efficient learning algorithm which brings a new vision on the training of deep nets. Finally, the theoretical work shows that deep frameworks are one way to efficiently have over-complete (combinatorially large) dictionary yet allowing easy reconstruction. We thus present the Deep Residual Oja Network (DRON). We demonstrate that a recursive deep approach working on the residuals allow exponential decrease of the error w.r.t. the depth.","layer":1,"vector":[-0.049,0.0199,0.0118,-0.0386,0.039,0.0434,0.023,0.0247,0.0268,-0.0175,0.0105,-0.0614,0.0367,0.0436,0.0461,-0.0088,0.017,0.0653,-0.0614,0.0015,0.0417,-0.0344,-0.0185,-0.0511,0.0531,-0.0083,-0.0445,-0.0488,-0.0292,-0.2401,-0.0177,-0.0007,0.0448,-0.0341,0.0272,-0.031,-0.0389,0.0422,-0.0354,0.0685,-0.0052,0.0264,-0.0135,-0.0663,-0.0274,-0.0214,-0.0445,0.0004,-0.0051,-0.0182,0.072,-0.0247,-0.0116,0.0539,0.0375,0.025,0.0505,0.0239,0.0367,0.02,0.0389,0.0658,-0.2105,0.0446,0.0415,0.0077,0.0122,-0.0007,0.0182,0.0458,0.0062,0.0677,0.0588,0.0243,-0.004,-0.0041,-0.0254,0.0003,-0.0132,0.0005,0.0052,0.0162,0.0335,-0.0576,-0.0451,-0.0477,0.0241,-0.0317,0.0318,-0.0101,-0.0519,-0.0209,-0.0047,-0.0175,-0.0424,0.0067,0.0339,0.0215,-0.0117,0.2087,-0.0539,0.0446,0.0689,-0.0246,0.0021,-0.0351,-0.0013,-0.014,-0.0069,0.0528,0.0056,-0.0223,0.028,-0.0214,0.0152,-0.0241,0.0354,0.085,-0.0186,0.0162,0.0215,-0.0272,0.064,-0.0383,0.0363,-0.0403,-0.0262,0.1084,0.049,0.0698,0.0103,-0.0239,-0.0353,-0.0583,0.0621,0.0054,0.0015,-0.0153,0.0112,0.0136,-0.0804,-0.0506,0.0122,-0.0515,-0.0337,0.1119,0.0019,0.0342,-0.0579,-0.0332,0.0005,0.0295,-0.0243,-0.012,0.0041,0.0481,0.0321,0.0237,-0.0404,0.0477,-0.0239,-0.0706,-0.0358,0.112,-0.0492,-0.0925,-0.0609,0.0042,-0.0074,0.0403,0.0352,0.0208,-0.0367,-0.0039,0.0834,0.0264,-0.0852,-0.0252,-0.0049,0.0277,0.0224,-0.0683,-0.064,0.0322,0.0607,-0.0399,0.0114,-0.0169,0.043,0.0139,-0.0495,0.026,-0.038,-0.0035,0.0165,-0.0327,-0.0112,-0.025,0.0038,-0.0385,0.0275,-0.0108,-0.0437,0.057,-0.0041,0.018,0.0104,-0.0043,0.0045,0.0448,-0.031,-0.0004,0.073,-0.0523,-0.0174,-0.043,0.0073,0.0132,-0.0149,0.0341,0.0177,-0.144,-0.0728,-0.2156,0.0252,0.0221,-0.0095,0.04,-0.0326,0.0478,-0.0033,0.0471,0.0521,0.0191,0.027,0.0134,-0.0149,-0.0201,0.0556,0.0501,0.0357,-0.0461,-0.0126,-0.0103,-0.0177,-0.0112,-0.0552,0.05,-0.003,0.2223,-0.011,0.0273,0.0078,0.0328,0.0642,-0.0298,-0.0926,0.0814,-0.0424,0.0979,0.0093,-0.0434,-0.0485,-0.0078,-0.0409,0.0102,-0.0628,-0.0323,-0.0473,-0.0509,0.0141,-0.0445,0.0201,0.0332,-0.05,0.0247,-0.0181,-0.0436,-0.0025,-0.0915,0.0226,-0.0751,0.016,0.0083,-0.0161,-0.0332,-0.0561,0.0509,0.0189,-0.0153,-0.0294,-0.0232,-0.0333,-0.022,0.0643,-0.0117,0.0222,0.0886,-0.0141,0.0181,-0.0017,-0.0993,0.0054,0.0747,-0.0321,0.0352,0.0198,0.0558,0.0671,0.0992,-0.0161,-0.0127,-0.0072,0.0261,0.0172,-0.0907,-0.0036,0.0244,0.0134,-0.3044,0.0382,0.0065,0.0371,-0.0119,0.0105,0.0008,0.0336,-0.0033,-0.0453,-0.0421,0.0397,0.0358,-0.0735,-0.0114,0.0474,0.0789,-0.0267,0.0712,-0.0441,-0.0111,0.1083,0.1993,-0.0675,0.0432,0.0055,-0.0073,0.0133,0.038,-0.0575,0.0302,0.0108,0.0526,-0.0399,0.0054,0.0521,-0.0314,0.0344,0.0374,-0.0064,0.0191,-0.0651,-0.0389,-0.0499,0.0981,-0.0214,-0.0099,-0.0359,0.0388,-0.0031,-0.0392,-0.0177,-0.0117,-0.0202,0.0124,0.0284,-0.0159,-0.0352,-0.0166,-0.005,0.0386,-0.0505,-0.0029,-0.0056,-0.0021]}
{"key":"[Improving the Robustness of Adversarial Attacks Using an Affine-Invariant Gradient Estimator] As designers of artificial intelligence try to outwit hackers, both sides continue to hone in on AI's inherent vulnerabilities. Designed and trained from certain statistical distributions of data, AI's deep neural networks (DNNs) remain vulnerable to deceptive inputs that violate a DNN's statistical, predictive assumptions. Before being fed into a neural network, however, most existing adversarial examples cannot maintain malicious functionality when applied to an affine transformation. For practical purposes, maintaining that malicious functionality serves as an important measure of the robustness of adversarial attacks. To help DNNs learn to defend themselves more thoroughly against attacks, we propose an affine-invariant adversarial attack, which can consistently produce more robust adversarial examples over affine transformations. For efficiency, we propose to disentangle current affine-transformation strategies from the Euclidean geometry coordinate plane with its geometric translations, rotations and dilations; we reformulate the latter two in polar coordinates. Afterwards, we construct an affine-invariant gradient estimator by convolving the gradient at the original image with derived kernels, which can be integrated with any gradient-based attack methods. Extensive experiments on ImageNet, including some experiments under physical condition, demonstrate that our method can significantly improve the affine invariance of adversarial examples and, as a byproduct, improve the transferability of adversarial examples, compared with alternative state-of-the-art methods.","layer":5,"vector":[-0.0368,-0.051,-0.0353,-0.0019,0.0009,0.0274,0.0489,0.0274,-0.0327,0.0245,0.0079,-0.073,0.0228,0.0463,-0.0205,0.0211,-0.0033,0.0523,-0.0724,0.0456,0.0325,-0.0263,0.0363,-0.0354,0.0009,0.0296,-0.0197,-0.0234,-0.033,-0.2439,0.0129,-0.0415,0.0246,-0.0494,-0.0034,-0.0368,-0.0766,0.0584,-0.0308,0.0233,0.0016,0.0641,-0.0308,-0.0762,-0.027,-0.0386,-0.0447,0.0193,-0.011,-0.0307,0.0172,-0.0395,0.0298,0.0432,0.0784,-0.011,0.1015,0.0353,0.0324,0.0927,0.0173,0.0322,-0.1377,0.0552,0.0382,0.0562,-0.0299,-0.0585,-0.0246,0.0332,-0.0069,0.0334,0.0294,0.0071,-0.003,0.0202,-0.028,-0.0297,-0.0039,-0.0059,0.0448,0.007,-0.0033,0.0047,-0.0212,-0.0299,0.0348,-0.0372,0.1037,-0.0044,-0.0106,0.0191,-0.0377,0.0462,-0.0416,0.0003,0.0295,0.0039,-0.0459,0.2015,-0.0545,0.009,0.0423,-0.045,0.0316,-0.0133,-0.0537,-0.0442,-0.0165,0.0181,-0.0267,-0.0171,-0.0034,-0.0068,0.0217,-0.0378,0.0596,0.0274,-0.0368,-0.0037,-0.0289,0.0166,0.0392,0.0188,0.0587,-0.065,-0.0086,0.1758,0.0578,0.0495,0.0121,-0.0118,-0.029,-0.0061,0.013,0.0411,-0.0161,0.0292,0.0033,0.01,-0.0681,-0.0105,0.0211,-0.0132,-0.0119,0.1158,-0.043,0.028,-0.037,-0.0014,0.0132,0.024,-0.0684,-0.0088,0.0052,0.0258,-0.0229,0.0618,-0.0719,-0.0146,-0.0402,-0.0326,-0.0462,0.1221,0.0245,-0.05,-0.0066,0.0088,0.0192,-0.0237,0.0185,-0.0096,-0.0114,-0.0036,0.0574,-0.0152,-0.0647,-0.0428,0.0019,0.0214,0.0073,-0.0566,-0.0158,0.0185,0.0562,0.0011,0.0195,-0.0378,0.0381,0.0418,-0.0621,0.0584,-0.0593,-0.0169,-0.0702,-0.026,-0.0125,0.0051,-0.0118,-0.0403,0.0081,-0.0032,-0.041,0.0004,-0.0005,0.0519,0.0014,-0.0379,-0.0084,0.0744,-0.0057,-0.0199,0.021,-0.0679,-0.023,-0.0033,-0.0141,0.069,-0.0295,0.0179,0.0138,-0.0398,-0.0545,-0.2238,-0.0237,-0.0277,-0.0442,0.0381,-0.1172,0.0375,-0.0188,0.0694,0.0265,0.0542,-0.0165,0.0272,0.012,0.0005,0.0833,-0.0128,0.0627,-0.0258,-0.0044,-0.0583,0.0421,0.0036,-0.0856,0.0395,0.0345,0.2412,0.0387,0.0295,-0.0443,0.0094,0.0049,0.0174,-0.085,0.0459,-0.0175,0.043,0.0178,-0.0286,-0.0048,-0.0322,0.0235,0.0375,-0.0956,-0.0227,-0.0429,-0.0571,0.0484,-0.0354,0.0461,0.0285,-0.0048,0.07,-0.026,0.0133,-0.0295,-0.0662,0.0238,-0.0399,0.0584,-0.0202,-0.0607,0.0303,-0.0851,0.0899,0.0091,-0.0092,-0.075,0.0497,0.008,-0.0237,0.0722,0.0602,0.0153,0.0663,0.017,0.0332,0.0103,-0.0643,-0.0379,0.0181,0.0096,-0.0023,-0.0082,0.0413,-0.041,0.0512,0.0029,0.0759,-0.0508,-0.0271,-0.0142,-0.071,-0.0248,0.0522,0.0443,-0.3092,0.008,0.0461,0.0626,-0.0228,-0.0064,0.058,0.0212,-0.0687,0.0099,-0.0431,0.0309,0.0378,-0.0139,0.021,-0.009,0.0729,-0.0674,0.0416,-0.0512,0.0221,0.0474,0.218,-0.0298,-0.0179,-0.0215,-0.0339,0.0291,-0.0001,-0.0233,0.0244,0.0103,0.0384,-0.0126,0.0547,0.0701,-0.0213,-0.0084,0.0073,-0.02,-0.0255,0.0451,-0.0236,0.0402,0.0763,-0.0134,-0.0164,-0.0016,0.0457,0.0019,-0.0086,0.0049,0.0142,-0.0167,0.0654,0.0181,-0.0544,-0.0185,-0.0598,-0.0101,0.0114,-0.0509,-0.0475,-0.0059,-0.0446]}
{"key":"[Squeeze All: Novel Estimator and Self-Normalized Bound for Linear Contextual Bandits] We propose a novel algorithm for linear contextual bandits with $O(\\sqrt{dT \\log T})$ regret bound, where $d$ is the dimension of contexts and $T$ is the time horizon. Our proposed algorithm is equipped with a novel estimator in which exploration is embedded through explicit randomization. Depending on the randomization, our proposed estimator takes contribution either from contexts of all arms or from selected contexts. We establish a self-normalized bound for our estimator, which allows a novel decomposition of the cumulative regret into additive dimension-dependent terms instead of multiplicative terms. We also prove a novel lower bound of $\\Omega(\\sqrt{dT})$ under our problem setting. Hence, the regret of our proposed algorithm matches the lower bound up to logarithmic factors. The numerical experiments support the theoretical guarantees and show that our proposed method outperforms the existing linear bandit algorithms.","layer":2,"vector":[-0.0406,0.0252,0.0417,-0.0458,0.0297,-0.0173,0.081,0.0311,0.0261,-0.0351,0.0273,-0.0023,0.0281,0.052,0.029,0.002,-0.0079,0.0332,-0.0579,0.0251,0.0563,-0.1127,0.003,-0.0825,0.06,0.0053,-0.0444,-0.0791,-0.0122,-0.1953,0.013,-0.0275,0.0069,-0.0698,-0.0107,-0.0164,-0.0108,0.0532,0.0012,0.0831,0.0367,0.0705,-0.0537,-0.0759,-0.0313,-0.0491,-0.0107,0.0023,-0.025,-0.0256,-0.0261,-0.0092,0.0742,0.0276,0.0067,0.0106,0.0097,0.0313,0.0505,0.0257,0.0127,0.0419,-0.1536,0.0152,0.0332,-0.0099,-0.0614,-0.0011,0.0188,0.0505,-0.0005,-0.0035,0.0172,0.0534,0.0553,-0.0095,0.0145,-0.0563,-0.0008,0.0091,0.0016,-0.028,-0.0503,0.0181,-0.0202,-0.0778,0.0083,-0.0382,0.0833,0.0243,-0.033,-0.0083,-0.0257,-0.0075,-0.0761,-0.0524,0.0517,0.0328,-0.052,0.1907,-0.0213,0.053,0.0085,0.0047,0.0298,-0.0222,-0.0156,-0.0066,-0.0142,-0.0243,0.0245,-0.0318,0.0596,-0.029,-0.0066,-0.0187,0.0606,0.0759,0.0101,0.0082,-0.0547,0.0252,0.0554,-0.0248,0.0144,-0.0701,0.009,0.1389,0.0202,0.0199,-0.0,-0.0551,-0.011,-0.0022,-0.0017,-0.0043,-0.0334,0.0418,0.0339,-0.0505,-0.0503,-0.0086,0.0461,-0.0842,-0.0477,0.1162,0.0092,0.016,-0.0367,-0.0424,-0.0037,-0.0108,-0.021,-0.0021,0.0377,0.0387,0.0346,0.0376,-0.0597,0.0344,-0.0595,-0.0375,0.0397,0.0929,0.0017,-0.0826,-0.0087,0.0495,0.0008,0.0332,0.0229,0.0024,-0.0704,0.0114,0.0918,0.0398,-0.0686,0.002,-0.0078,0.004,0.0035,-0.0124,-0.0132,0.0037,0.0091,-0.0571,0.0103,-0.0456,0.0473,0.0395,-0.013,-0.0002,-0.0076,-0.013,-0.013,-0.0589,0.0099,0.0063,0.0155,-0.0009,0.0181,-0.0076,-0.0651,-0.0209,0.048,0.0283,0.0212,-0.0472,0.0342,0.0047,-0.0367,0.0074,0.067,-0.0053,-0.0535,0.0336,0.0554,0.0385,-0.0436,0.0341,0.0292,-0.0098,-0.0042,-0.2548,-0.0033,-0.0225,0.0095,0.0386,-0.041,0.0382,-0.0519,0.0482,0.0619,0.0514,-0.0614,-0.0117,0.021,0.0134,0.0605,0.019,0.0009,-0.0382,0.0153,-0.0384,0.0292,-0.0219,-0.0212,0.0297,0.0122,0.2599,0.0375,0.013,-0.0444,0.0667,0.0381,0.0053,-0.1033,-0.0082,0.0299,0.0428,0.0012,-0.0232,-0.0776,0.001,0.0064,-0.0108,-0.0573,-0.0796,-0.0425,-0.053,0.0241,-0.0351,0.0103,0.0386,-0.0196,0.0379,-0.0426,-0.0165,-0.0472,-0.0741,0.016,-0.0351,0.0343,-0.0036,-0.0732,0.0278,-0.0813,0.0298,-0.0296,0.0365,-0.0465,0.0258,-0.0256,-0.0166,0.0108,-0.0049,0.0019,0.0194,0.0208,0.0315,-0.0564,-0.0287,-0.0059,0.0642,-0.0752,0.0399,0.0362,-0.0221,0.0203,0.0824,0.0166,0.0191,-0.0026,-0.0183,0.0183,-0.0751,-0.0063,0.0955,-0.0099,-0.3217,0.0662,0.0594,0.0112,-0.0128,0.0643,0.0467,-0.0048,-0.023,-0.0064,0.0238,0.0831,0.032,-0.0311,0.0357,0.0146,0.0865,-0.0416,0.0467,-0.0583,0.0393,0.0522,0.2023,-0.0482,0.0253,0.0212,-0.0468,-0.005,0.0389,0.0069,-0.0031,-0.0008,0.0908,-0.0928,0.0518,0.0557,-0.0266,0.0318,0.0005,-0.0296,-0.0473,0.0116,-0.025,-0.0258,0.1012,0.0092,-0.0026,-0.0351,-0.0501,0.0315,-0.0172,-0.0052,0.0007,-0.034,0.0138,0.025,-0.0283,-0.0606,-0.014,-0.0081,-0.0042,-0.0495,-0.007,-0.0446,0.0251]}
{"key":"[Provable Certificates for Adversarial Examples: Fitting a Ball in the Union of Polytopes] We propose a novel method for computing exact pointwise robustness of deep neural networks for all convex $\\ell_p$ norms. Our algorithm, GeoCert, finds the largest $\\ell_p$ ball centered at an input point $x_0$, within which the output class of a given neural network with ReLU nonlinearities remains unchanged. We relate the problem of computing pointwise robustness of these networks to that of computing the maximum norm ball with a fixed center that can be contained in a non-convex polytope. This is a challenging problem in general, however we show that there exists an efficient algorithm to compute this for polyhedral complices. Further we show that piecewise linear neural networks partition the input space into a polyhedral complex. Our algorithm has the ability to almost immediately output a nontrivial lower bound to the pointwise robustness which is iteratively improved until it ultimately becomes tight. We empirically show that our approach generates distance lower bounds that are tighter compared to prior work, under moderate time constraints.","layer":2,"vector":[-0.0423,-0.0625,0.02,-0.0413,0.0089,0.0461,0.0551,0.0137,-0.0024,-0.0144,-0.0149,-0.0543,0.0386,0.0972,-0.0329,0.0228,0.0128,0.073,-0.0518,0.0669,0.0409,-0.0556,0.0011,-0.0576,0.0213,0.0038,-0.0079,-0.0213,-0.0195,-0.2755,-0.0171,-0.0274,0.0153,-0.0168,-0.0162,-0.0124,-0.0363,0.035,-0.0481,0.0437,0.037,0.0226,-0.0385,-0.0751,0.0155,-0.0303,0.005,0.028,0.0068,-0.0593,0.0254,-0.0742,0.0376,0.0295,0.0384,0.0207,0.0488,0.0407,0.0309,0.0439,0.0123,0.0281,-0.1466,0.0126,0.0319,-0.0026,-0.0452,-0.0478,-0.02,0.063,0.0345,0.0399,0.0157,0.0134,0.0088,0.0189,0.0111,-0.054,-0.0383,0.0331,0.0243,-0.0066,-0.0602,0.0137,-0.0148,-0.0278,0.0293,-0.0161,0.0066,-0.0082,0.0074,-0.0071,-0.0329,0.0116,-0.0506,-0.0583,0.0487,-0.0024,-0.0449,0.1836,-0.0644,0.0402,0.0455,-0.0172,0.0512,-0.0121,-0.036,-0.0758,0.0053,-0.0002,-0.0325,-0.0006,-0.0,-0.023,0.0114,0.0144,0.0486,0.0197,-0.0324,-0.031,-0.0603,0.0412,0.051,0.0068,0.0337,-0.0431,0.006,0.1286,0.0345,0.0944,0.0224,-0.0041,-0.0041,-0.0139,0.0336,0.0241,0.0135,0.0266,0.0169,0.0025,-0.0476,-0.0513,0.0572,-0.053,-0.0275,0.1555,-0.0532,-0.0014,-0.007,-0.0486,-0.0016,0.0049,-0.0519,-0.0036,0.0297,0.0322,-0.0046,0.0083,-0.0379,-0.0296,-0.0151,-0.0762,-0.0218,0.1289,-0.0006,-0.0773,-0.0098,-0.0085,0.0421,-0.0207,0.036,0.0245,0.0126,0.0313,0.0453,0.0222,-0.1012,-0.0345,0.0153,0.0277,-0.0199,-0.0723,-0.0138,0.0126,0.0203,-0.0019,0.0318,-0.0245,0.0392,0.0336,-0.0513,0.036,-0.0756,-0.0296,-0.031,-0.0427,0.0198,0.0149,0.0168,-0.0217,-0.0023,0.0077,-0.0438,0.0448,0.0664,0.0243,-0.0131,0.0058,0.0102,0.0675,-0.0442,-0.0446,0.0305,-0.113,-0.0169,0.0077,0.0438,0.0048,-0.0612,0.0383,0.0243,-0.0457,-0.0729,-0.2126,-0.0143,-0.0125,-0.0335,0.0909,-0.0767,0.0465,-0.0324,0.0227,0.0299,0.0798,-0.0154,-0.0229,0.0618,-0.0073,0.0349,-0.007,0.0276,-0.0442,0.0222,-0.0428,0.0166,-0.0843,-0.0331,0.066,0.0155,0.2328,0.056,0.0605,-0.0187,0.0249,0.009,-0.0343,-0.0646,0.0819,0.0141,0.0489,-0.0055,-0.0146,-0.0273,-0.047,0.0295,0.0165,-0.1004,-0.0401,-0.0262,-0.0197,0.054,-0.0417,0.0245,0.0816,-0.0396,0.0454,-0.003,0.0163,-0.0498,-0.0765,0.0437,-0.039,0.0515,0.0214,-0.0783,-0.0036,-0.0268,0.0656,0.0607,0.0037,-0.033,0.0333,0.0046,-0.0056,0.0477,0.0386,0.0284,0.0599,-0.0412,0.0608,0.0239,-0.0198,-0.0305,0.0695,-0.0131,0.0246,-0.0078,0.0321,0.0058,0.0552,-0.0356,0.0516,-0.0329,-0.0003,0.0144,-0.0641,-0.0084,0.0224,0.0088,-0.2944,0.0038,0.0017,0.0141,-0.0776,-0.0007,0.0178,0.0144,-0.0364,-0.0075,0.0161,0.0935,0.0176,-0.0393,-0.0357,0.0135,0.0358,-0.0413,0.0797,-0.0686,-0.0053,0.0618,0.2145,-0.0525,0.0165,0.0271,-0.0008,0.0,0.0033,0.0014,0.0031,0.0223,0.0509,-0.0318,0.0154,0.138,-0.02,0.0076,0.0132,-0.0023,-0.0307,0.0173,-0.0063,-0.0289,0.0669,0.0086,-0.0265,0.0002,0.0182,0.0051,-0.0431,0.0004,0.0172,0.0004,-0.0435,0.0362,-0.0354,-0.05,-0.0737,-0.0466,0.046,-0.063,-0.0098,-0.0168,-0.0221]}
{"key":"[Adversarial TCAV -- Robust and Effective Interpretation of Intermediate Layers in Neural Networks] Interpreting neural network decisions and the information learned in intermediate layers is still a challenge due to the opaque internal state and shared non-linear interactions. Although (Kim et al, 2017) proposed to interpret intermediate layers by quantifying its ability to distinguish a user-defined concept (from random examples), the questions of robustness (variation against the choice of random examples) and effectiveness (retrieval rate of concept images) remain. We investigate these two properties and propose improvements to make concept activations reliable for practical use. Effectiveness: If the intermediate layer has effectively learned a user-defined concept, it should be able to recall --- at the testing step --- most of the images containing the proposed concept. For instance, we observed that the recall rate of Tiger shark and Great white shark from the ImageNet dataset with \"Fins\" as a user-defined concept was only 18.35% for VGG16. To increase the effectiveness of concept learning, we propose A-CAV --- the Adversarial Concept Activation Vector --- this results in larger margins between user concepts and (negative) random examples. This approach improves the aforesaid recall to 76.83% for VGG16. For robustness, we define it as the ability of an intermediate layer to be consistent in its recall rate (the effectiveness) for different random seeds. We observed that TCAV has a large variance in recalling a concept across different random seeds. For example, the recall of cat images (from a layer learning the concept of tail) varies from 18% to 86% with 20.85% standard deviation on VGG16. We propose a simple and scalable modification that employs a Gram-Schmidt process to sample random noise from concepts and learn an average \"concept classifier\". This approach improves the aforesaid standard deviation from 20.85% to 6.4%.","layer":1,"vector":[0.0031,-0.0327,-0.03,0.0018,0.0018,0.029,0.0657,0.0275,0.0025,-0.0115,0.0023,-0.0478,0.0618,0.0948,0.0018,0.0337,0.0259,0.0327,-0.0434,0.008,-0.0077,-0.0216,0.0369,-0.0245,-0.0022,0.0143,-0.0173,-0.0146,-0.0561,-0.2478,-0.0185,-0.0842,0.0248,-0.0268,-0.0006,-0.0215,-0.0808,0.0462,-0.0293,0.0587,0.0491,0.0215,-0.0354,-0.0727,-0.013,0.0052,-0.0151,0.0024,-0.0262,-0.0236,0.0287,-0.0575,0.001,0.0434,0.0162,0.0543,0.0708,0.0304,0.0392,0.0627,0.043,0.0594,-0.1582,0.064,0.0326,0.0268,-0.0833,0.0009,0.0097,0.0314,0.0107,0.0391,0.0255,0.0834,-0.0046,0.0373,0.0035,-0.0361,-0.0397,-0.0247,0.0355,0.0143,0.0261,0.0014,-0.0152,-0.006,-0.0136,-0.0537,0.044,-0.0077,0.0289,0.0024,-0.0408,0.0457,-0.0291,-0.0224,0.0494,0.0237,-0.0512,0.2096,-0.0419,-0.0044,0.0128,-0.0491,0.0585,-0.0109,-0.0717,-0.0343,0.0058,-0.0119,-0.0047,-0.0456,0.0051,-0.0371,0.0371,-0.0137,0.0454,-0.0106,-0.0082,-0.0197,-0.0666,0.0306,0.0277,-0.0297,0.0278,-0.0348,0.0486,0.1092,0.052,0.0173,0.0571,-0.0468,-0.0214,0.0278,0.031,0.0221,0.0032,0.0181,-0.0197,-0.0118,-0.0239,-0.029,0.0272,-0.0812,-0.0611,0.1324,-0.0222,0.0315,-0.0259,-0.0049,0.0034,0.0176,-0.0139,-0.0134,0.023,0.044,0.0231,0.0409,-0.0199,0.0011,-0.0015,-0.1011,-0.0305,0.0809,0.0117,-0.0393,-0.0449,-0.0067,-0.0048,-0.0587,0.0281,-0.0051,-0.0305,0.0341,0.0347,0.035,-0.0941,-0.0319,-0.0164,0.0124,0.0086,-0.0732,-0.0217,0.0408,0.0242,-0.0364,0.0281,-0.0399,0.016,0.0483,-0.0332,-0.0054,-0.0283,-0.0205,-0.0475,-0.0426,-0.0129,-0.0089,0.0094,-0.0527,0.001,-0.0309,-0.0408,0.0141,-0.0563,0.0728,0.0201,-0.0225,0.0673,0.0267,-0.0342,-0.0256,0.0297,-0.0126,-0.0328,-0.0359,-0.0211,0.0787,-0.0108,0.0145,0.0751,-0.0692,-0.0371,-0.2802,-0.0039,-0.0283,-0.0431,0.0663,-0.0956,0.0439,-0.0226,0.0453,0.0102,0.0359,0.0015,-0.012,-0.0018,0.0227,0.0674,-0.0051,0.0078,-0.0295,0.0445,-0.0627,0.0314,0.0193,-0.0773,0.0311,-0.0308,0.2293,0.032,0.0309,-0.0174,0.0345,0.038,-0.0485,-0.0664,0.06,0.0012,0.1003,0.0078,-0.0272,-0.0208,-0.0249,0.0428,0.0167,-0.1129,-0.0525,-0.0354,-0.0266,0.0095,-0.057,0.0075,0.0185,-0.0414,0.0566,0.011,0.0156,-0.0238,-0.1279,0.0474,-0.0496,0.0258,-0.0161,-0.0316,-0.0006,-0.053,0.0536,0.0032,0.0104,-0.0552,0.0201,-0.0349,-0.0232,0.0535,0.0437,-0.0116,0.0432,-0.0034,0.0241,-0.06,-0.0437,-0.0057,0.0569,-0.0061,0.0244,-0.0442,0.0827,0.0108,0.066,-0.0313,0.0541,0.0069,0.0065,-0.0027,-0.0361,-0.0298,0.0354,0.0223,-0.2801,0.0396,0.0314,0.079,-0.006,0.0387,0.0282,-0.0037,-0.0443,-0.0076,-0.0164,-0.0066,0.061,-0.0331,-0.0176,0.0749,0.0522,-0.0507,0.06,-0.0343,0.0315,0.0137,0.2237,-0.0665,0.0063,0.016,-0.0264,0.0217,0.0281,-0.0233,0.0362,0.0429,0.0494,-0.0515,0.02,0.07,-0.0383,0.0381,0.0335,-0.0335,-0.0301,0.0221,-0.0648,-0.0207,0.0905,0.0029,0.0547,-0.0339,-0.0293,0.0496,-0.0172,-0.0394,0.0128,-0.0123,0.0713,0.031,-0.0205,-0.059,-0.0224,-0.0087,0.0125,-0.0096,0.0045,0.0013,-0.019]}
{"key":"[Optimal Demand Response Using Device Based Reinforcement Learning] Demand response (DR) for residential and small commercial buildings is estimated to account for as much as 65% of the total energy savings potential of DR, and previous work shows that a fully automated Energy Management System (EMS) is a necessary prerequisite to DR in these areas. In this paper, we propose a novel EMS formulation for DR problems in these sectors. Specifically, we formulate a fully automated EMS's rescheduling problem as a reinforcement learning (RL) problem, and argue that this RL problem can be approximately solved by decomposing it over device clusters. Compared with existing formulations, our new formulation (1) does not require explicitly modeling the user's dissatisfaction on job rescheduling, (2) enables the EMS to self-initiate jobs, (3) allows the user to initiate more flexible requests and (4) has a computational complexity linear in the number of devices. We also demonstrate the simulation results of applying Q-learning, one of the most popular and classical RL algorithms, to a representative example.","layer":5,"vector":[-0.0683,0.0168,0.0377,0.022,0.0072,0.0255,0.0216,0.0174,0.017,-0.0019,0.0162,-0.0384,-0.0,0.0667,0.0643,0.0078,0.0094,0.022,-0.0089,0.0193,0.0581,-0.0301,-0.0561,-0.0869,0.0166,-0.0456,-0.0163,-0.0094,-0.04,-0.2376,0.0146,-0.0199,0.0399,-0.032,0.0163,-0.0032,-0.0358,0.0365,-0.0217,0.0371,0.0253,0.0121,0.038,-0.0765,-0.0328,-0.0369,-0.003,-0.0379,-0.0376,-0.0296,0.047,0.0153,-0.0189,0.0322,0.0403,0.0187,0.0469,0.0529,0.0708,0.0283,0.0108,0.0384,-0.169,0.071,0.031,0.0008,-0.0199,0.0111,0.0243,0.0636,-0.0453,0.0468,0.0008,0.0136,0.0079,-0.0042,-0.0039,-0.0322,0.0059,-0.0274,0.0211,-0.0661,-0.0695,0.0047,-0.0282,-0.0334,0.0099,-0.0199,0.0715,0.0531,-0.0417,-0.0092,-0.0325,0.025,-0.0577,-0.0268,0.0467,0.0005,-0.0946,0.2059,0.016,0.0645,0.0115,-0.0163,0.0257,-0.0435,-0.0396,-0.0452,-0.0615,-0.0145,-0.0362,-0.0322,0.0324,-0.0251,0.055,0.0079,0.0207,0.075,0.0254,-0.0157,-0.0161,0.0429,0.0554,-0.016,0.0274,-0.0833,0.0194,0.1215,-0.0294,0.0516,0.0336,-0.0475,-0.0057,-0.0134,0.0467,0.0712,0.057,-0.0291,0.0318,0.0273,-0.006,-0.007,-0.0058,-0.1464,-0.0213,0.0939,-0.0097,0.0356,-0.0523,-0.0194,0.0236,0.0034,-0.0429,-0.0433,0.0243,0.0535,0.0396,0.0389,-0.0202,-0.0145,-0.0365,-0.0217,-0.0483,0.0797,-0.0167,-0.0792,-0.0178,-0.01,-0.0106,-0.0071,0.0559,0.045,-0.078,0.0316,0.106,0.0151,-0.0734,-0.0145,-0.0052,-0.0057,0.011,-0.0614,-0.0049,-0.0019,0.0715,-0.0165,0.0299,-0.037,-0.0188,0.0096,-0.0418,0.0099,-0.0043,0.0103,0.0022,-0.021,-0.0242,0.0095,0.0177,-0.0099,-0.0127,0.0147,-0.0412,0.0455,-0.012,0.0577,-0.0446,-0.001,0.0881,-0.0038,0.0184,0.0185,0.0812,-0.0676,-0.0506,0.0459,0.0102,0.0174,-0.0107,0.0458,0.0297,0.0529,-0.0389,-0.2324,0.005,-0.0114,-0.0261,0.0393,-0.0385,0.0499,-0.0385,0.0036,0.0436,0.0652,-0.0514,-0.0234,0.0401,-0.0167,0.0497,0.0483,0.0226,-0.0689,-0.0322,-0.0055,-0.0023,-0.017,-0.1064,0.0507,-0.018,0.2154,-0.0053,0.0358,-0.0013,0.0578,0.003,-0.0562,-0.1029,0.0339,0.0694,0.065,0.0166,0.0033,-0.0578,-0.0245,0.0479,-0.0422,-0.0554,-0.0184,-0.0185,-0.0234,0.0403,-0.0983,-0.0176,0.0269,-0.0189,0.0447,-0.0264,0.0212,-0.0309,-0.0797,0.0728,-0.0268,0.0232,0.0195,-0.0453,0.011,-0.031,0.0411,-0.0325,-0.0023,0.0116,-0.0111,-0.0252,-0.0217,0.0649,0.004,0.0028,0.0197,0.0146,0.0173,-0.0198,-0.0256,-0.0504,0.0728,-0.055,0.0599,0.0284,0.0013,-0.0303,0.0257,0.019,0.0136,-0.0087,-0.0451,0.0286,-0.0132,-0.0117,0.0396,0.0179,-0.315,0.0379,0.0291,0.0305,-0.0428,0.0266,-0.0184,0.0199,-0.0152,0.0392,-0.0254,0.0395,0.033,0.0196,0.0148,-0.0305,0.0678,-0.0589,0.0154,-0.067,0.0288,0.0639,0.2186,-0.0593,0.0705,0.0261,-0.0259,0.0259,0.0372,-0.0225,-0.026,-0.0117,0.0777,-0.0519,0.0494,0.0346,-0.0515,0.0239,0.0149,0.0162,-0.0568,0.0097,-0.0294,0.0036,0.1031,0.0123,-0.0536,-0.0959,-0.0265,0.0283,-0.005,-0.0093,-0.0386,0.0098,-0.008,0.0055,-0.0778,-0.0565,-0.0325,-0.0682,0.0319,0.0017,0.0271,-0.0352,0.0213]}
{"key":"[Search and Rescue with Airborne Optical Sectioning] We show that automated person detection under occlusion conditions can be significantly improved by combining multi-perspective images before classification. Here, we employed image integration by Airborne Optical Sectioning (AOS)---a synthetic aperture imaging technique that uses camera drones to capture unstructured thermal light fields---to achieve this with a precision/recall of 96/93%. Finding lost or injured people in dense forests is not generally feasible with thermal recordings, but becomes practical with use of AOS integral images. Our findings lay the foundation for effective future search and rescue technologies that can be applied in combination with autonomous or manned aircraft. They can also be beneficial for other fields that currently suffer from inaccurate classification of partially occluded people, animals, or objects.","layer":2,"vector":[-0.0181,0.0062,0.0612,-0.0312,0.1124,0.0601,0.0759,0.0019,-0.0093,-0.0136,0.0452,-0.0795,0.0147,0.0676,0.0347,0.0083,0.0116,0.0547,-0.0014,0.0234,0.0054,-0.0253,-0.025,-0.0456,-0.0115,0.0564,-0.0433,-0.0266,-0.0218,-0.1841,-0.0094,-0.0277,0.0598,-0.0271,0.0129,-0.0236,-0.042,0.0699,-0.037,0.0362,0.008,-0.035,-0.032,-0.0719,-0.0178,-0.0188,-0.0074,-0.0008,-0.0014,-0.0555,0.0142,-0.0409,0.0051,0.0703,0.0204,-0.0066,0.0428,0.0471,0.0649,0.0197,0.0611,0.0236,-0.1914,0.0327,0.0426,0.0217,-0.0176,-0.0742,0.0716,-0.0269,-0.0285,0.0701,0.025,-0.0009,-0.0118,-0.0173,-0.0001,-0.0419,-0.0202,-0.0092,0.0166,-0.0149,-0.0503,0.005,0.0031,-0.0399,0.0464,-0.0901,0.0249,0.0031,-0.0611,-0.0277,-0.063,0.0016,-0.0705,-0.0262,0.0352,0.0347,-0.0307,0.2134,-0.0106,0.0291,0.0632,-0.0426,0.0335,-0.0216,-0.0366,-0.0416,-0.0428,-0.0165,0.0331,0.0006,0.044,0.0349,0.0072,0.0175,0.0665,0.0476,-0.0051,-0.0194,-0.0418,-0.0361,0.0374,-0.0307,0.0217,-0.0597,0.0723,0.1211,0.047,0.0367,0.0682,-0.0137,-0.0088,-0.0886,-0.0038,0.023,-0.0089,0.0152,0.0319,-0.0315,0.0077,-0.0717,0.0393,-0.0732,-0.002,0.0994,-0.0717,0.0817,-0.0572,-0.045,0.0054,0.0276,-0.0152,-0.0178,0.0236,0.0168,0.0671,0.0544,-0.0532,0.0363,-0.0176,-0.0526,-0.0376,0.0893,0.0376,-0.0903,0.0029,-0.0049,-0.0014,0.0388,0.0384,0.0067,-0.0102,-0.0105,0.0527,0.0108,-0.0933,0.0036,-0.0104,-0.0071,-0.0144,-0.0584,-0.03,-0.0083,0.0807,-0.0181,-0.0392,-0.0223,0.0078,0.0445,-0.0204,0.0248,-0.0193,-0.0557,-0.0085,0.0175,-0.0057,-0.0234,0.0219,-0.0096,0.0619,-0.0129,-0.0291,0.024,0.0516,0.0119,0.0147,-0.0089,0.0083,0.0386,-0.0378,-0.0093,0.0472,-0.0103,-0.0699,-0.0348,0.0459,0.0218,-0.0417,0.0674,-0.0146,-0.0654,-0.0455,-0.2506,-0.0001,-0.0265,-0.0191,0.0089,-0.0616,-0.0083,-0.0013,0.0415,0.058,0.072,-0.0708,0.0183,0.067,-0.0138,0.0414,0.0105,-0.0234,-0.0559,0.0013,-0.0206,0.0664,-0.0394,-0.0486,0.0409,-0.0564,0.2552,0.0644,0.0345,-0.0043,0.0109,-0.0048,-0.0457,-0.0897,0.0502,0.0457,0.0446,-0.016,-0.0329,-0.017,-0.0222,0.0368,0.0208,-0.06,-0.0327,-0.0733,-0.0701,0.0283,-0.0345,0.0164,0.0119,-0.0347,0.0314,-0.013,-0.0463,-0.0613,-0.0691,0.0036,-0.0483,0.0135,0.0153,-0.0076,0.0084,-0.0775,0.1065,0.0024,-0.0495,-0.0397,0.0276,-0.0513,-0.019,0.0829,-0.0107,0.0021,0.0682,0.009,0.0668,-0.0269,-0.0085,-0.0193,0.0774,-0.0256,0.0116,0.0185,0.0664,0.0157,0.0399,-0.0389,0.0205,-0.0493,0.0179,0.0307,-0.0462,-0.0287,0.0422,-0.0059,-0.2811,0.0624,-0.0067,-0.0226,-0.0472,0.0334,0.0604,0.0457,-0.0017,-0.0022,-0.0329,0.0028,0.0635,-0.0116,0.0114,0.0791,0.0017,-0.03,0.0073,-0.0734,0.0148,-0.0134,0.2056,-0.0458,-0.0008,0.0339,-0.0066,-0.0359,-0.0011,-0.0356,0.0602,0.0146,0.0663,-0.0515,0.0328,0.0653,0.0107,0.0575,0.0118,-0.0503,-0.027,-0.0036,0.0432,-0.0145,0.0648,0.0217,-0.0322,0.0158,0.0135,0.0172,-0.014,0.0111,-0.0533,-0.0065,0.0386,0.0531,-0.0694,-0.0004,-0.0332,-0.0169,0.0088,-0.0181,-0.0279,-0.0166,0.0169]}
{"key":"[DeepSZ: Identification of Sunyaev-Zel'dovich Galaxy Clusters using Deep Learning] Galaxy clusters identified from the Sunyaev Zel'dovich (SZ) effect are a key ingredient in multi-wavelength cluster-based cosmology. We present a comparison between two methods of cluster identification: the standard Matched Filter (MF) method in SZ cluster finding and a method using Convolutional Neural Networks (CNN). We further implement and show results for a `combined' identifier. We apply the methods to simulated millimeter maps for several observing frequencies for an SPT-3G-like survey. There are some key differences between the methods. The MF method requires image pre-processing to remove point sources and a model for the noise, while the CNN method requires very little pre-processing of images. Additionally, the CNN requires tuning of hyperparameters in the model and takes as input, cutout images of the sky. Specifically, we use the CNN to classify whether or not an 8 arcmin $\\times$ 8 arcmin cutout of the sky contains a cluster. We compare differences in purity and completeness. The MF signal-to-noise ratio depends on both mass and redshift. Our CNN, trained for a given mass threshold, captures a different set of clusters than the MF, some of which have SNR below the MF detection threshold. However, the CNN tends to mis-classify cutouts whose clusters are located near the edge of the cutout, which can be mitigated with staggered cutouts. We leverage the complementarity of the two methods, combining the scores from each method for identification. The purity and completeness of the MF alone are both 0.61, assuming a standard detection threshold. The purity and completeness of the CNN alone are 0.59 and 0.61. The combined classification method yields 0.60 and 0.77, a significant increase for completeness with a modest decrease in purity. We advocate for combined methods that increase the confidence of many lower signal-to-noise clusters.","layer":1,"vector":[-0.0247,-0.0182,-0.0497,0.0317,0.0399,0.0444,0.0103,0.0249,0.0349,-0.0083,0.0226,-0.0688,0.0068,0.051,0.0028,0.0118,0.0172,0.0115,-0.0717,-0.0062,-0.0198,-0.0211,-0.0107,-0.025,0.0819,0.0041,-0.0209,-0.0256,-0.1066,-0.2578,-0.0077,-0.0028,0.0698,-0.0286,0.0069,-0.0374,0.0294,0.0448,-0.0688,0.0056,0.0132,-0.0314,-0.0417,-0.0699,-0.0589,-0.0547,-0.0498,-0.0173,-0.0132,-0.0468,0.0117,-0.0496,-0.0173,0.0788,0.0009,0.0599,0.0456,-0.0048,0.0524,0.0031,0.0145,0.0044,-0.1963,0.0471,0.0356,0.0082,0.0003,-0.0569,0.0489,0.0183,-0.0227,0.0991,0.0129,0.0255,-0.0233,0.0329,-0.002,-0.0304,-0.0068,0.0142,-0.0355,-0.0191,-0.0247,-0.0061,-0.0381,-0.0393,-0.0006,-0.016,0.0238,0.013,-0.0631,0.0376,-0.0291,0.0439,-0.0807,-0.0032,0.0247,0.0022,0.0261,0.2137,-0.0481,0.0342,0.0456,-0.0457,0.0395,-0.0197,-0.0413,0.0148,-0.0131,-0.0064,-0.0132,-0.0444,-0.0024,0.0077,-0.0144,-0.0079,0.0837,0.0486,0.0343,-0.0154,-0.0245,-0.0132,0.078,-0.0022,0.0502,-0.0391,0.0495,0.0949,0.0347,0.0233,0.0271,-0.0237,-0.0467,0.0175,0.0297,0.0644,0.0183,0.0095,0.0064,-0.0324,-0.0551,-0.0443,0.0111,-0.0795,-0.0223,0.1101,-0.0616,-0.0056,-0.0109,-0.0389,-0.0137,0.0416,-0.0576,-0.0116,0.043,0.0125,0.0452,0.0446,-0.0437,0.0368,0.0045,-0.0328,-0.029,0.1344,0.0266,-0.05,-0.0342,-0.0042,-0.0006,-0.0015,-0.0064,0.048,-0.0337,0.0102,0.0858,0.0044,-0.107,0.0352,-0.0209,0.0406,-0.0559,-0.0093,-0.0429,0.0279,0.048,-0.0577,-0.0152,-0.0542,-0.0075,0.0111,-0.0589,0.0402,-0.0047,0.0053,-0.0074,-0.0818,-0.0103,0.0115,-0.0146,-0.0419,-0.0116,0.0452,-0.0149,0.0025,0.0064,0.0146,-0.0031,0.012,0.056,0.0413,-0.024,-0.0253,0.0546,-0.0361,-0.0164,-0.0106,0.007,-0.0077,-0.0426,0.0702,0.0514,-0.06,-0.0953,-0.2437,0.0111,0.0084,-0.0144,0.077,-0.0679,0.0317,0.0058,0.0439,0.0455,0.0357,0.0133,0.0004,0.0139,-0.0089,0.0775,0.0041,0.0568,-0.0565,-0.0155,-0.024,0.0333,-0.0579,-0.0603,0.0349,0.0173,0.1855,0.0312,0.0474,-0.0287,0.0233,0.0272,-0.0114,-0.0471,0.0254,0.0277,0.0948,0.0176,0.0005,-0.0241,-0.0122,0.0063,0.0061,-0.0849,0.0135,0.0073,-0.0023,0.0552,-0.0449,0.0071,0.0198,-0.0039,-0.0034,-0.0195,0.0336,-0.0053,-0.1557,0.0133,-0.0557,0.0063,-0.0161,-0.03,0.0278,-0.0847,0.0738,0.0141,-0.0592,-0.0108,0.0543,-0.0398,-0.0201,0.0626,-0.0425,0.0126,0.0868,0.0148,0.0781,-0.0389,-0.0398,-0.0259,0.104,0.0184,0.0419,0.0667,0.0335,0.0326,0.0492,-0.0361,0.0486,-0.0464,0.0067,0.0067,-0.0221,-0.0152,0.0075,-0.0255,-0.2779,0.0609,0.0075,0.0135,0.0122,0.0098,0.045,0.0163,-0.0336,-0.031,-0.0134,0.0368,0.0235,-0.0363,-0.0021,0.0744,0.0451,-0.0438,0.0625,-0.029,0.0031,0.0222,0.2212,-0.0305,0.0007,0.0629,-0.05,-0.0291,0.019,-0.0133,0.0391,0.0081,0.0681,-0.0208,0.014,0.078,0.0093,0.0142,0.0343,0.0096,0.0015,0.0615,-0.0375,-0.015,0.1036,-0.0401,-0.0084,-0.0704,-0.027,-0.0079,-0.039,0.0153,-0.016,0.0013,0.0299,0.0218,-0.0818,-0.0218,-0.0445,-0.029,0.0336,-0.0506,-0.0156,-0.0029,-0.0086]}
{"key":"[A machine-learning framework for daylight and visual comfort assessment in early design stages] This research is mainly focused on the assessment of machine learning algorithms in the prediction of daylight and visual comfort metrics in the early design stages. A dataset was primarily developed from 2880 simulations derived from Honeybee for Grasshopper. The simulations were done for a shoebox space with a one side window. The alternatives emerged from different physical features, including room dimensions, interior surfaces reflectance, window dimensions and orientations, number of windows, and shading states. 5 metrics were used for daylight evaluations, including UDI, sDA, mDA, ASE, and sVD. Quality Views were analyzed for the same shoebox spaces via a grasshopper-based algorithm, developed from the LEED v4 evaluation framework for Quality Views. The dataset was further analyzed with an Artificial Neural Network algorithm written in Python. The accuracy of the predictions was estimated at 97% on average. The developed model could be used in early design stages analyses without the need for time-consuming simulations in previously used platforms and programs.","layer":6,"vector":[-0.0517,0.0009,0.0695,0.0051,0.0517,0.0673,-0.0084,0.0343,0.0234,0.0114,-0.0002,-0.054,0.0172,0.0432,0.0138,-0.0102,0.0231,0.0275,-0.0239,0.0037,0.041,0.0053,0.0031,-0.1044,0.0046,0.0344,-0.0234,-0.0335,-0.0528,-0.2011,-0.0337,-0.0383,0.0577,-0.0161,0.0278,-0.0121,0.0076,0.054,-0.0355,0.0316,0.0303,-0.0418,-0.0192,-0.0431,-0.0386,-0.056,-0.0091,-0.0104,-0.0475,-0.0405,0.0187,-0.0515,-0.0103,0.0296,0.0325,0.0459,0.0414,0.0289,0.0587,0.0214,0.043,0.0433,-0.2134,0.0548,0.035,0.0328,-0.0783,-0.0448,0.0143,0.0428,-0.0504,0.0028,0.0392,0.0241,0.0316,0.0084,0.0099,-0.0614,0.0507,-0.0036,0.0048,-0.0199,-0.0495,0.0072,0.0136,-0.0042,-0.017,-0.0021,0.0413,0.025,-0.0322,-0.0242,-0.0605,0.0058,-0.0337,-0.0199,0.0137,0.02,-0.042,0.216,-0.0527,0.0368,0.0245,-0.027,0.0418,-0.0619,-0.018,-0.0608,-0.0564,0.0069,-0.0349,0.023,0.0222,0.0136,0.027,0.0083,0.0039,0.0345,0.0439,-0.0199,-0.0429,0.0105,0.0599,-0.0065,0.018,-0.0484,0.0456,0.1235,-0.0029,0.036,0.0486,-0.031,-0.0017,-0.0277,0.0385,0.0847,0.0435,0.0167,0.0263,-0.0267,-0.0444,-0.0461,0.0578,-0.0786,-0.0391,0.1204,-0.0294,0.0356,-0.0391,0.0168,0.0087,0.0283,-0.0281,-0.0087,0.0381,0.0479,-0.0103,0.0706,-0.0131,0.0348,0.0355,-0.0209,-0.0392,0.0429,-0.0086,-0.1056,-0.048,0.012,-0.0208,0.0159,0.0484,0.0597,-0.0696,0.058,0.067,0.0027,-0.0626,0.0033,0.0045,0.0173,0.0453,-0.0323,-0.0405,0.0189,0.0939,-0.0545,-0.0001,-0.0281,0.0058,0.052,-0.0471,-0.0491,-0.0524,0.0331,-0.043,0.0111,-0.0335,-0.0195,0.0015,-0.0088,-0.0005,-0.0208,-0.0376,0.0288,0.0076,0.021,0.0077,0.0136,0.0831,0.0133,-0.0473,-0.0049,0.0609,-0.0464,-0.0312,0.0192,-0.0073,0.0256,0.0315,0.048,0.0025,-0.0415,-0.1105,-0.2278,0.0296,0.0386,-0.0335,0.0488,-0.0653,0.0197,-0.0581,0.0162,-0.0142,0.1169,-0.0037,-0.009,0.0146,-0.0559,0.0437,0.036,0.0252,-0.082,-0.0635,-0.0106,0.0246,0.0039,-0.0938,0.0189,-0.0146,0.1891,-0.012,0.0205,-0.0638,0.0448,-0.0077,-0.0437,-0.1072,0.0839,0.0631,0.065,-0.0223,-0.0373,-0.0586,0.0038,0.0786,-0.0298,-0.0846,-0.0452,-0.0656,-0.0184,0.023,-0.0475,-0.0067,-0.002,-0.0236,0.0696,-0.0007,-0.033,-0.0496,-0.0832,0.0441,0.0087,0.0417,0.0143,-0.0603,0.0707,-0.044,0.038,-0.0478,-0.0118,-0.0385,0.0238,-0.0282,-0.0247,0.115,0.0071,-0.0609,0.0817,-0.0012,0.0572,0.013,-0.0018,-0.0135,0.0214,-0.0625,-0.0044,0.0227,0.0531,0.0173,0.0255,-0.0326,0.0344,-0.0631,-0.0238,0.0168,-0.0384,-0.0043,0.028,0.0295,-0.2683,0.0455,-0.0235,0.0265,-0.0192,-0.0299,0.0396,0.0092,-0.0057,-0.0281,-0.0044,0.0213,0.0168,-0.0319,0.0309,-0.0011,0.0713,-0.0509,0.049,-0.0623,0.0246,0.0547,0.243,-0.0569,0.0329,0.0634,-0.0238,0.0017,0.0192,0.0075,0.0284,0.0178,0.119,-0.0519,0.0253,0.0646,-0.0384,-0.0086,-0.0031,0.0095,0.0244,0.0295,0.0032,-0.0141,0.1203,0.0012,-0.0087,-0.0358,-0.0113,-0.0137,-0.0479,0.0233,-0.0313,-0.021,-0.003,-0.0057,-0.0355,-0.0123,-0.0493,-0.0013,0.0001,-0.0447,0.007,-0.0115,0.0216]}
{"key":"[Distributed Deep Convolutional Compression for Massive MIMO CSI Feedback] Massive multiple-input multiple-output (MIMO) systems require downlink channel state information (CSI) at the base station (BS) to achieve spatial diversity and multiplexing gains. In a frequency division duplex (FDD) multiuser massive MIMO network, each user needs to compress and feedback its downlink CSI to the BS. The CSI overhead scales with the number of antennas, users and subcarriers, and becomes a major bottleneck for the overall spectral efficiency. In this paper, we propose a deep learning (DL)-based CSI compression scheme, called DeepCMC, composed of convolutional layers followed by quantization and entropy coding blocks. In comparison with previous DL-based CSI reduction structures, DeepCMC proposes a novel fully-convolutional neural network (NN) architecture, with residual layers at the decoder, and incorporates quantization and entropy coding blocks into its design. DeepCMC is trained to minimize a weighted rate-distortion cost, which enables a trade-off between the CSI quality and its feedback overhead. Simulation results demonstrate that DeepCMC outperforms the state of the art CSI compression schemes in terms of the reconstruction quality of CSI for the same compression rate. We also propose a distributed version of DeepCMC for a multi-user MIMO scenario to encode and reconstruct the CSI from multiple users in a distributed manner. Distributed DeepCMC not only utilizes the inherent CSI structures of a single MIMO user for compression, but also benefits from the correlations among the channel matrices of nearby users to further improve the performance in comparison with DeepCMC. We also propose a reduced-complexity training method for distributed DeepCMC, allowing to scale it to multiple users, and suggest a cluster-based distributed DeepCMC approach for practical implementation.","layer":2,"vector":[-0.0442,-0.0169,0.0127,-0.0262,0.0421,0.0576,-0.003,0.0437,0.054,-0.0188,0.0298,-0.0437,0.0486,0.0682,0.039,0.0342,-0.0,-0.0085,-0.055,0.0091,0.047,-0.0284,-0.0342,-0.0384,0.0116,-0.0359,-0.0592,-0.052,-0.066,-0.2338,0.026,-0.0036,0.0954,-0.0112,0.0247,-0.083,-0.0302,0.0093,-0.0484,0.0465,0.0406,0.0447,-0.0197,-0.0217,-0.0463,-0.0336,-0.0495,0.0211,0.0155,-0.0387,0.0868,-0.0164,-0.0585,0.0473,0.0422,0.0187,0.0489,0.0801,0.0736,0.0566,0.0734,0.0098,-0.1848,0.0468,0.038,0.0117,-0.0244,0.037,0.007,0.039,-0.0319,0.0442,0.0496,0.0234,0.0237,0.0133,-0.0109,-0.0187,-0.0037,0.0447,0.0229,-0.0349,-0.0399,-0.0255,0.0128,-0.0567,0.0039,-0.0357,-0.0017,-0.014,-0.0561,0.001,-0.0362,0.0557,-0.1013,0.0165,-0.0295,0.0244,-0.0712,0.1832,-0.0211,0.009,0.0395,-0.0543,-0.0016,-0.0006,0.0236,-0.0133,-0.0049,0.0097,-0.0327,-0.0375,0.0173,0.0062,0.0424,0.0286,0.0451,0.0674,-0.0188,-0.0171,-0.0091,0.0054,0.022,-0.0306,0.0576,-0.0505,-0.0428,0.163,0.0153,0.0554,0.0522,0.0287,-0.0444,-0.0211,0.0175,0.0384,0.0314,-0.0299,0.0083,0.0341,-0.0121,-0.0601,0.0343,-0.056,-0.0565,0.0752,-0.036,0.0298,-0.0164,-0.0128,-0.0294,0.0385,-0.0169,-0.0198,0.0357,-0.0066,0.0094,0.075,-0.0692,0.0128,-0.0124,-0.0088,-0.02,0.106,0.0282,-0.0941,-0.0234,0.0115,0.045,-0.0582,0.0191,0.0069,-0.0643,-0.0142,0.0515,0.045,-0.1037,-0.0327,0.0206,0.0096,-0.0215,-0.0245,0.02,0.0131,0.0342,-0.048,0.0071,-0.0492,0.0157,0.0,-0.0471,0.005,-0.0475,0.0058,-0.0084,-0.0364,-0.0244,0.0204,-0.0026,0.0202,-0.0092,0.0055,-0.0427,0.0411,0.0157,0.073,0.0002,-0.0262,0.0103,0.0142,-0.0033,-0.0319,0.1215,-0.0455,0.0003,-0.0096,0.0276,0.0589,-0.0316,0.0206,0.0326,-0.0593,-0.0869,-0.2009,0.0012,0.0243,-0.045,-0.0006,-0.079,0.0305,-0.0241,0.0365,0.0748,0.0443,0.0016,-0.047,-0.0114,-0.0116,0.0562,0.0257,0.0644,-0.0217,-0.0058,-0.0142,0.0523,0.0229,-0.0734,0.0538,-0.012,0.1951,0.0326,0.095,0.0258,0.0228,0.0543,-0.0152,-0.0894,-0.0191,0.0098,0.0804,0.0264,-0.0008,-0.0055,0.0147,-0.0256,-0.0144,-0.0865,-0.014,-0.0291,-0.0196,0.0101,-0.1138,-0.0163,0.0546,-0.0538,0.0169,0.0,0.0114,-0.0039,-0.1302,0.02,-0.0505,-0.0118,-0.0202,-0.0167,-0.0083,-0.0846,0.0283,0.008,0.0163,-0.0227,0.0396,-0.0434,0.0065,0.0509,0.0202,0.0056,0.0757,-0.0324,0.0398,-0.0117,-0.0355,-0.0625,0.1109,0.0018,0.0351,0.032,0.0303,0.0251,0.0728,0.0348,0.0267,-0.0539,-0.0019,0.0413,-0.0475,-0.0336,-0.0086,-0.0515,-0.2856,0.0356,0.0279,0.0014,-0.0537,-0.016,0.0799,0.0068,-0.0875,0.0285,-0.0014,0.0039,0.0394,-0.0347,0.0505,-0.0008,0.0313,-0.0296,0.0379,-0.0611,0.0082,0.0131,0.1597,0.0008,0.0373,0.0303,-0.048,0.0581,0.026,-0.0222,0.025,0.0252,0.0864,-0.0804,0.0057,0.1011,-0.0021,0.0844,0.0371,0.0584,-0.0254,0.002,-0.0091,-0.0286,0.0943,-0.0074,-0.0185,-0.058,-0.0115,-0.0064,-0.0389,-0.0015,-0.0359,-0.0596,0.0028,0.0088,-0.0795,-0.0722,-0.0189,-0.0164,0.016,-0.061,-0.0657,-0.0204,-0.0375]}
{"key":"[Learning Transferable Graph Exploration] This paper considers the problem of efficient exploration of unseen environments, a key challenge in AI. We propose a `learning to explore' framework where we learn a policy from a distribution of environments. At test time, presented with an unseen environment from the same distribution, the policy aims to generalize the exploration strategy to visit the maximum number of unique states in a limited number of steps. We particularly focus on environments with graph-structured state-spaces that are encountered in many important real-world applications like software testing and map building. We formulate this task as a reinforcement learning problem where the `exploration' agent is rewarded for transitioning to previously unseen environment states and employ a graph-structured memory to encode the agent's past trajectory. Experimental results demonstrate that our approach is extremely effective for exploration of spatial maps; and when applied on the challenging problems of coverage-guided software-testing of domain-specific programs and real-world mobile applications, it outperforms methods that have been hand-engineered by human experts.","layer":0,"vector":[-0.0409,-0.0039,0.0435,0.0002,0.0078,0.0395,0.0217,0.0217,0.0135,-0.0214,0.0217,-0.0422,0.0746,0.0804,-0.0057,0.0108,-0.0698,0.0535,-0.024,-0.0316,0.0407,-0.0029,-0.0033,-0.0709,0.0024,0.0588,-0.0411,-0.0602,-0.0407,-0.237,0.0004,-0.056,0.0269,-0.028,-0.0161,-0.0131,0.0147,0.0472,-0.0144,0.0215,0.0468,-0.0107,-0.0241,-0.0526,-0.0397,-0.0497,0.011,-0.0402,-0.0086,-0.0466,-0.0025,-0.0392,0.0168,0.0155,0.0485,0.066,0.0489,0.0635,0.0384,0.0208,0.0284,0.0141,-0.1172,0.0511,0.07,0.0734,-0.0687,-0.0243,0.0473,0.065,0.003,0.032,0.0036,0.0565,0.0764,-0.0062,-0.0239,-0.0132,0.0212,-0.037,0.0046,-0.0528,0.0021,0.0202,-0.0337,-0.0684,-0.0027,-0.0131,0.0623,0.0269,-0.0153,0.0146,-0.0028,0.0272,-0.0522,-0.0045,0.062,0.0368,-0.0106,0.2176,-0.0466,0.0295,0.0143,0.024,-0.017,-0.0433,0.0041,-0.0021,-0.042,-0.0091,-0.0284,-0.0396,0.0219,-0.0237,-0.0216,0.0227,0.0568,0.0503,-0.0159,-0.011,-0.0192,0.0099,0.0126,-0.0608,0.0394,-0.081,0.0303,0.1205,0.0125,-0.0087,0.039,-0.0268,-0.0099,0.027,0.0025,0.0242,-0.0025,-0.0196,0.0057,0.0109,-0.0451,0.0562,0.045,-0.1131,-0.0602,0.1008,0.0007,0.03,-0.0534,-0.0319,-0.0277,0.0188,-0.017,-0.004,-0.0045,0.0373,0.0358,0.0466,-0.0471,0.046,-0.0587,-0.0354,-0.0437,0.1178,-0.0087,-0.0679,-0.031,0.0071,0.0118,-0.0036,0.0142,0.0654,-0.0716,0.028,0.0491,-0.0001,-0.0973,0.0025,-0.007,-0.0352,0.0052,-0.0532,-0.0006,0.0397,0.0524,-0.0409,0.0072,-0.0079,-0.0023,0.0285,0.0003,0.0549,-0.0028,0.0009,-0.0615,0.0005,-0.0075,-0.0196,0.0051,-0.0122,0.0304,-0.0748,-0.0536,-0.0005,-0.0266,0.0169,-0.0219,-0.0352,0.0168,0.0118,-0.0686,0.0066,0.0522,-0.0151,-0.0048,-0.0276,-0.0103,0.0126,0.0236,0.0503,0.0119,-0.0402,-0.017,-0.2349,-0.0126,-0.0127,0.0012,0.0462,-0.0577,0.0051,-0.0211,0.0613,0.034,0.0533,-0.0957,-0.0321,0.0562,-0.0032,0.0284,0.0077,0.0299,-0.0345,0.0111,0.0098,-0.0141,-0.0294,-0.1064,0.0339,-0.0104,0.2747,0.0368,0.0184,-0.0015,0.0101,-0.0127,-0.0352,-0.1493,0.0378,0.0288,0.0465,0.0049,0.0038,-0.0659,-0.0199,0.0029,-0.0422,-0.0829,-0.0103,-0.021,-0.0113,0.0133,-0.0074,-0.0023,0.0156,-0.0403,0.0175,0.004,-0.0471,-0.0652,-0.0621,0.0217,-0.044,0.044,0.0103,-0.0391,0.0194,-0.0271,0.0788,-0.0035,0.0169,-0.0691,0.0615,0.0014,-0.0072,0.0036,0.0109,-0.0264,0.035,0.0397,0.0462,-0.0018,-0.0306,-0.0382,0.0271,-0.0409,0.0048,-0.009,0.0293,0.0052,0.0616,-0.0396,0.053,-0.0161,0.0354,0.0251,-0.0818,-0.0469,0.0971,-0.0564,-0.2841,0.0424,0.0282,0.0536,-0.0473,0.0249,0.0607,0.0053,-0.0555,0.0027,0.0267,0.0627,0.0596,-0.0181,0.0175,0.0584,0.0998,0.0021,0.0353,-0.0876,0.0202,0.0459,0.2262,-0.0472,0.0586,0.0399,-0.0565,-0.0114,0.0224,0.0293,0.0197,0.0096,0.0449,-0.0519,0.0349,0.0747,0.0,0.0289,0.0066,0.0112,-0.0048,-0.0243,-0.0153,0.0201,0.0748,0.0092,-0.0249,-0.0231,-0.0058,0.0407,0.0019,-0.0219,-0.0129,-0.0,0.0629,0.0304,-0.0367,-0.0534,-0.0648,-0.0665,-0.0023,-0.0824,0.0577,-0.0176,0.0008]}
{"key":"[Kernel Bi-Linear Modeling for Reconstructing Data on Manifolds: The Dynamic-MRI Case] This paper establishes a kernel-based framework for reconstructing data on manifolds, tailored to fit the dynamic-(d)MRI-data recovery problem. The proposed methodology exploits simple tangent-space geometries of manifolds in reproducing kernel Hilbert spaces and follows classical kernel-approximation arguments to form the data-recovery task as a bi-linear inverse problem. Departing from mainstream approaches, the proposed methodology uses no training data, employs no graph Laplacian matrix to penalize the optimization task, uses no costly (kernel) pre-imaging step to map feature points back to the input space, and utilizes complex-valued kernel functions to account for k-space data. The framework is validated on synthetically generated dMRI data, where comparisons against state-of-the-art schemes highlight the rich potential of the proposed approach in data-recovery problems.","layer":0,"vector":[-0.0316,-0.0235,0.048,-0.0129,-0.0042,0.0108,0.0133,0.0671,0.0145,-0.0394,0.0141,-0.0964,0.0269,0.0219,0.0021,-0.0039,0.0261,0.0758,-0.0178,0.041,0.0043,-0.0635,0.0024,-0.0173,-0.0159,-0.0018,0.0178,-0.0226,-0.0344,-0.262,0.0192,-0.0453,0.0657,-0.0461,0.0224,-0.0469,-0.0215,0.0591,-0.0405,0.0509,0.0191,0.0256,-0.0031,0.0041,-0.0685,-0.0385,-0.0106,-0.0303,0.0253,-0.0175,0.0245,0.011,0.0132,0.0546,0.0433,0.0524,0.0811,0.0431,0.0481,0.0478,0.0172,0.0166,-0.1525,0.0701,0.0517,0.0178,-0.0272,-0.0417,0.0541,0.0565,-0.0048,0.0431,0.0017,-0.0123,0.0364,-0.028,-0.0002,-0.0232,-0.0274,0.03,0.0014,0.0381,-0.0253,0.0051,-0.037,-0.0305,0.0013,-0.0585,0.0384,-0.0122,-0.0449,-0.023,-0.0481,0.015,-0.0948,-0.0235,0.0085,0.0659,-0.0145,0.2075,-0.0764,0.0447,0.0845,-0.0302,-0.0121,-0.0642,-0.0304,-0.0317,-0.0007,-0.0184,-0.0008,-0.0243,0.0327,-0.0282,0.046,-0.0262,0.0521,0.0673,-0.0204,-0.0101,-0.0288,0.0533,0.0506,-0.0307,0.0429,-0.062,-0.0067,0.1311,0.0683,0.0377,-0.0116,0.0054,-0.0179,-0.051,-0.0236,-0.0349,0.0194,-0.0026,-0.0098,0.0381,-0.0298,-0.073,0.0141,-0.0312,-0.0452,0.136,-0.0572,-0.004,-0.0845,-0.0046,-0.0046,0.0353,-0.0574,0.0167,-0.01,0.0053,-0.0156,0.0205,-0.0199,0.0307,-0.037,-0.0887,-0.0491,0.1472,-0.0308,-0.0303,-0.022,0.014,0.0297,0.002,0.0457,0.0203,-0.0382,0.0093,0.1121,0.0327,-0.0582,0.0063,0.022,0.0111,0.0574,-0.0606,-0.0555,0.0202,-0.0147,-0.0406,-0.0396,-0.0049,0.0218,0.0346,-0.0042,-0.018,-0.0197,-0.0284,-0.0195,-0.0627,-0.0197,-0.0161,0.0315,-0.0259,0.0133,-0.0234,-0.0556,0.04,-0.0104,-0.0107,0.0109,0.0038,-0.0191,0.0616,0.0021,-0.009,0.0547,-0.0663,-0.0388,-0.0247,0.0046,0.0069,-0.01,0.0573,0.0381,-0.0533,-0.0471,-0.2088,0.0026,-0.0254,0.0495,0.03,-0.0817,0.047,-0.0349,0.093,0.0434,0.0377,0.0215,-0.0513,0.0354,-0.0308,0.0326,0.0161,0.0123,-0.06,-0.0664,-0.017,0.0042,-0.0194,-0.0762,0.0794,-0.0426,0.2232,0.0296,0.0393,-0.0278,0.0019,0.0266,-0.0105,-0.0802,0.0833,0.0256,0.0588,-0.0065,-0.054,-0.0086,-0.0237,-0.0592,0.029,-0.0479,0.0027,-0.0253,-0.0756,-0.017,-0.0529,0.0135,0.0491,-0.0295,0.0254,-0.0353,-0.0223,-0.019,-0.0771,-0.0219,-0.0252,0.0122,0.0193,-0.0735,0.015,-0.0401,0.0867,0.0165,0.0155,-0.0423,0.0096,-0.034,-0.0145,0.0821,0.0373,0.0358,0.0801,0.0139,0.0164,0.0019,-0.0204,-0.0455,0.0709,-0.0354,0.0379,0.0521,0.0542,0.0399,0.078,-0.0364,-0.0436,-0.0496,-0.0369,0.0067,-0.0797,-0.0192,0.0375,0.0277,-0.3026,0.033,0.0113,-0.005,0.0045,0.0443,0.0004,0.0259,-0.0177,-0.0233,0.0014,0.0595,0.0461,0.0263,0.0412,0.0371,0.0732,-0.0471,0.0293,-0.0702,-0.001,0.0654,0.1756,0.0018,0.0195,0.032,-0.0036,-0.0144,0.0152,-0.017,0.0192,-0.0163,0.0636,-0.0356,0.0687,0.0902,-0.0509,0.0678,-0.0066,-0.0361,0.0449,0.0215,-0.027,-0.0259,0.0867,-0.0019,-0.0084,-0.0409,-0.0017,0.0154,0.0035,-0.0034,0.0177,0.0003,0.0538,0.0498,-0.0084,-0.0658,-0.0224,-0.0027,-0.0122,-0.0423,-0.0285,-0.0253,-0.0367]}
{"key":"[Lipschitz Lifelong Reinforcement Learning] We consider the problem of knowledge transfer when an agent is facing a series of Reinforcement Learning (RL) tasks. We introduce a novel metric between Markov Decision Processes (MDPs) and establish that close MDPs have close optimal value functions. Formally, the optimal value functions are Lipschitz continuous with respect to the tasks space. These theoretical results lead us to a value-transfer method for Lifelong RL, which we use to build a PAC-MDP algorithm with improved convergence rate. Further, we show the method to experience no negative transfer with high probability. We illustrate the benefits of the method in Lifelong RL experiments.","layer":7,"vector":[-0.0504,0.013,0.0192,-0.0504,-0.0066,0.0701,0.0474,0.0613,0.0838,-0.0135,0.005,-0.0555,0.0539,0.0728,0.0091,0.0419,-0.0475,0.0677,-0.0324,0.0156,0.0614,-0.0651,0.0012,-0.0299,0.0019,0.0191,-0.0672,-0.0654,-0.0341,-0.2277,0.0208,-0.0514,-0.021,-0.0421,0.0018,0.0116,-0.0551,0.0166,-0.02,0.0615,0.074,0.0354,-0.0066,-0.0541,-0.0253,-0.0784,-0.0082,-0.0569,-0.0119,-0.0425,-0.0206,-0.0004,0.0224,0.0352,0.0344,0.0419,0.0401,0.0434,0.0234,0.0191,-0.0022,0.0083,-0.1826,0.0621,0.0269,0.0303,-0.0414,0.0042,0.0091,0.031,-0.0626,0.0279,-0.0144,0.0379,0.034,-0.021,0.0113,-0.0528,-0.0312,0.0081,-0.0007,-0.0492,-0.0636,-0.0169,-0.036,-0.01,-0.003,-0.0779,0.0204,0.0349,-0.0133,0.0262,-0.0094,0.0453,-0.0595,-0.0033,0.0777,0.0147,-0.0513,0.1897,-0.0329,0.0316,-0.0081,-0.0357,0.0563,0.0408,-0.051,-0.0286,-0.0072,0.0026,-0.0545,-0.0296,0.0414,-0.0311,-0.0245,0.0656,0.054,0.0526,-0.0227,-0.0095,-0.0418,0.0364,0.0454,-0.0044,0.0137,-0.072,0.001,0.1563,-0.0177,-0.0016,0.0499,-0.0652,-0.0325,-0.0046,0.0473,0.0561,0.03,-0.022,0.0382,-0.0392,-0.0094,0.0001,0.0028,-0.1103,-0.0617,0.1129,-0.0025,0.0505,-0.0017,0.0104,-0.0001,-0.0106,0.0192,-0.0145,-0.0089,0.0509,0.0504,0.0389,-0.0684,-0.0277,-0.0079,-0.0602,-0.0205,0.1272,-0.0278,-0.0517,-0.0523,-0.0475,0.0136,0.0016,0.0205,0.0657,-0.012,0.0386,0.0766,-0.0032,-0.08,0.0042,0.0414,-0.0094,0.0101,-0.0308,-0.0394,0.0111,0.0264,-0.0097,0.0038,-0.0556,0.0538,0.026,-0.0121,0.0082,-0.0135,0.0069,-0.0286,-0.0308,-0.004,0.0008,0.0185,-0.03,-0.0001,-0.0469,-0.0739,-0.025,0.0096,0.0109,-0.0086,0.0019,0.1077,-0.0082,-0.0439,0.0221,0.0208,0.0042,-0.0377,0.0143,0.0301,0.0126,0.0002,-0.0093,0.0167,0.076,-0.0228,-0.2275,-0.0074,-0.0111,-0.0169,0.0503,-0.0247,0.0323,-0.0199,0.028,0.0498,0.0547,-0.0627,-0.021,0.0191,-0.0317,0.0459,0.0598,0.0334,-0.0534,0.0157,-0.0244,-0.0279,-0.0227,-0.0942,0.0713,-0.0374,0.2269,0.0051,0.0319,-0.0063,0.013,0.045,-0.0206,-0.0708,0.0376,0.0005,0.0443,-0.0202,0.0117,-0.0285,0.0127,0.0107,-0.0395,-0.0848,-0.0249,-0.0387,-0.0321,0.0127,-0.0278,-0.0016,0.0129,-0.0056,0.0039,-0.0287,-0.0055,-0.0446,-0.0471,0.0268,-0.0334,0.0151,-0.0065,-0.0016,0.0374,-0.0143,0.0863,-0.0113,0.0109,-0.0223,0.0305,-0.0214,-0.0272,0.0548,0.0027,-0.0168,0.0501,0.0135,-0.002,-0.0078,-0.0698,-0.0233,0.0504,-0.0725,0.0316,0.0448,0.0309,-0.0011,0.0779,-0.0197,0.0063,-0.0152,0.0063,-0.0111,-0.0807,-0.0025,0.0394,0.0009,-0.3058,0.033,-0.0094,0.0309,-0.0329,0.0407,0.0565,-0.0054,-0.0798,0.0082,0.0362,0.0713,0.0496,0.0137,0.0334,0.0094,0.0469,-0.0227,0.0318,-0.0605,0.0251,0.0301,0.2316,-0.0283,0.0445,0.0189,-0.0443,0.0161,0.0513,-0.0354,-0.0072,0.0302,0.0543,-0.043,0.0575,0.1035,-0.0489,0.0678,-0.0084,0.0172,-0.0031,-0.0269,0.0057,0.0106,0.1241,0.004,-0.0217,-0.0849,-0.0693,0.0494,-0.0227,-0.0025,0.0124,-0.0182,0.0241,0.0269,-0.0162,-0.0559,-0.016,-0.0566,0.0097,-0.0757,0.0364,-0.0106,0.0167]}
{"key":"[Explainable Agents Through Social Cues: A Review] The issue of how to make embodied agents explainable has experienced a surge of interest over the last three years, and, there are many terms that refer to this concept, e.g., transparency or legibility. One reason for this high variance in terminology is the unique array of social cues that embodied agents can access in contrast to that accessed by non-embodied agents. Another reason is that different authors use these terms in different ways. Hence, we review the existing literature on explainability and organize it by (1) providing an overview of existing definitions, (2) showing how explainability is implemented and how it exploits different social cues, and (3) showing how the impact of explainability is measured. Additionally, we present a list of open questions and challenges that highlight areas that require further investigation by the community. This provides the interested reader with an overview of the current state-of-the-art.","layer":0,"vector":[0.0011,-0.0128,0.0097,-0.0272,0.02,0.014,0.0727,0.0274,0.0179,-0.0119,0.0131,-0.0412,-0.0032,0.0242,0.0577,-0.0325,-0.0233,0.0509,-0.037,0.0126,0.0373,-0.0377,-0.0052,0.0123,-0.0024,0.0696,-0.0557,-0.0443,-0.042,-0.2074,0.0211,-0.0452,0.0288,-0.0168,-0.014,-0.0097,-0.0393,0.0234,-0.0487,0.0235,0.0463,0.0081,-0.0141,-0.0388,-0.0461,-0.0565,-0.0128,-0.0211,-0.0419,-0.0672,0.0194,-0.0176,0.0098,0.0008,0.0283,0.0178,0.0921,0.0903,0.0044,0.0136,0.0542,0.0467,-0.1264,0.0947,0.076,0.0435,-0.0321,-0.0064,0.0206,-0.0006,0.0056,0.018,0.0279,0.0369,0.0402,-0.0474,-0.0119,-0.05,0.0291,-0.0296,-0.0225,-0.0236,-0.0248,-0.0053,-0.0244,-0.0266,0.0457,-0.0221,0.0257,-0.0013,-0.0596,0.0437,-0.0456,-0.0215,-0.0353,-0.0333,0.0547,0.0047,-0.0713,0.2027,-0.0386,-0.0083,0.0464,0.004,0.0407,0.0012,-0.011,-0.0675,-0.0059,0.0246,-0.0732,0.0055,0.0398,0.0077,0.0429,0.0214,0.0481,-0.0078,0.0158,-0.0673,0.0083,0.0291,0.0538,-0.0011,0.031,-0.0525,0.0534,0.1457,0.0314,0.0165,0.0692,-0.0228,-0.0312,0.0046,0.0117,0.0206,0.0026,0.0189,0.0072,0.0106,0.0062,-0.0374,-0.0265,-0.0925,-0.0673,0.0812,-0.0475,-0.0056,-0.032,0.0714,-0.0473,0.0027,-0.0312,-0.0277,0.0202,0.0173,0.0592,0.0583,-0.0816,0.0458,0.0123,-0.0707,-0.0188,0.0379,0.0128,-0.0884,-0.0103,0.0107,0.0536,-0.0504,0.0147,0.0332,-0.0593,0.0137,0.0603,0.0328,-0.0753,0.0228,-0.0152,-0.0007,0.043,-0.0883,-0.0506,0.075,0.03,-0.0415,-0.0193,-0.0576,0.0613,0.0618,0.0348,0.0211,-0.0428,0.0109,-0.0478,0.0102,0.0078,-0.0813,-0.0716,-0.0334,-0.0055,-0.0025,-0.0505,0.0149,-0.0087,0.0415,0.0126,0.0052,0.089,0.0411,-0.0504,0.0551,-0.0178,-0.0356,-0.0345,0.0016,0.0254,0.0088,0.0062,0.0024,0.0051,-0.0178,-0.0551,-0.2101,0.0186,0.0298,-0.0429,0.0257,-0.0573,0.0544,-0.0206,0.0331,0.0448,0.0736,-0.0303,-0.0531,0.0118,-0.0057,0.0374,0.01,0.0143,-0.0282,0.0218,0.0207,0.0092,-0.0033,-0.0977,0.0066,0.0376,0.2502,0.0665,0.0289,-0.0044,0.0251,0.0325,-0.0748,-0.1445,0.0613,0.0439,0.036,-0.0731,0.0002,-0.045,-0.036,0.049,0.0001,-0.0971,-0.0456,-0.0112,-0.0421,0.0306,-0.0228,0.0056,0.0348,-0.0032,0.0393,0.0174,-0.0598,-0.0497,-0.0305,0.0101,-0.0179,0.0899,-0.0159,0.0026,0.0284,-0.077,0.0807,0.0315,-0.031,-0.0473,0.021,-0.013,-0.0292,0.1166,-0.0128,-0.0408,0.0803,0.0225,0.0058,-0.0412,-0.0385,-0.0343,0.0567,0.0043,0.0074,0.0049,0.0038,-0.0325,0.0586,-0.0653,0.0371,-0.0376,0.0164,0.0316,-0.0516,-0.0325,0.033,-0.0373,-0.2991,0.0233,0.0275,0.0657,-0.064,0.0224,-0.0042,0.0354,-0.0387,0.0106,0.0166,0.0297,0.0215,0.0496,0.0018,0.0643,0.0563,-0.0553,0.0426,-0.0814,0.0229,0.0025,0.2473,-0.0061,0.0279,0.0199,-0.0062,-0.0363,0.0347,-0.027,0.0302,-0.0277,0.0656,-0.0489,0.0237,0.0277,-0.0255,0.0362,0.0339,-0.0393,-0.0173,-0.0165,0.0072,-0.0284,0.0854,0.0357,-0.0641,-0.0616,0.0023,0.0244,-0.027,-0.0048,-0.0369,-0.0185,0.0454,0.0029,-0.0333,-0.0067,-0.0235,-0.0024,0.0162,-0.0387,0.0327,0.0121,-0.0033]}
{"key":"[Semi-Supervised Natural Language Approach for Fine-Grained Classification of Medical Reports] Although machine learning has become a powerful tool to augment doctors in clinical analysis, the immense amount of labeled data that is necessary to train supervised learning approaches burdens each development task as time and resource intensive. The vast majority of dense clinical information is stored in written reports, detailing pertinent patient information. The challenge with utilizing natural language data for standard model development is due to the complex nature of the modality. In this research, a model pipeline was developed to utilize an unsupervised approach to train an encoder-language model, a recurrent network, to generate document encodings; which then can be used as features passed into a decoder-classifier model that requires magnitudes less labeled data than previous approaches to differentiate between fine-grained disease classes accurately. The language model was trained on unlabeled radiology reports from the Massachusetts General Hospital Radiology Department (n=218,159) and terminated with a loss of 1.62. The classification models were trained on three labeled datasets of head CT studies of reported patients, presenting large vessel occlusion (n=1403), acute ischemic strokes (n=331), and intracranial hemorrhage (n=4350), to identify a variety of different findings directly from the radiology report data; resulting in AUCs of 0.98, 0.95, and 0.99, respectively, for the large vessel occlusion, acute ischemic stroke, and intracranial hemorrhage datasets. The output encodings are able to be used in conjunction with imaging data, to create models that can process a multitude of different modalities. The ability to automatically extract relevant features from textual data allows for faster model development and integration of textual modality, overall, allowing clinical reports to become a more viable input for more encompassing and accurate deep learning models.","layer":10,"vector":[-0.0171,-0.0052,-0.0125,0.001,0.0103,0.0239,0.0094,0.0524,0.0049,0.0267,-0.0024,-0.0562,0.0148,0.0482,-0.011,0.0179,-0.0041,0.0217,-0.0105,0.0479,0.0198,-0.011,0.0152,-0.0357,0.0233,0.0376,-0.0352,-0.044,-0.0508,-0.2211,0.0045,-0.0669,0.0564,-0.0145,0.0314,-0.0501,-0.0562,0.0452,-0.0278,0.0412,0.0215,-0.0016,-0.0085,-0.0363,0.0152,-0.0273,-0.0263,-0.0182,0.0237,-0.0176,0.0322,-0.0523,0.0056,0.0273,0.0245,0.0085,0.0391,0.0077,0.0387,0.0589,0.0444,0.0706,-0.1737,0.0757,-0.0111,0.0163,-0.0892,0.0307,0.0061,0.0408,-0.0032,0.0224,0.0033,0.0586,-0.0048,-0.0123,-0.0118,-0.0171,0.0355,0.0173,0.022,0.0013,-0.0605,-0.024,-0.0227,-0.0729,-0.0063,-0.0896,0.0169,-0.0227,-0.0363,-0.0058,-0.0262,0.0461,-0.0874,-0.0371,0.0278,0.0465,-0.085,0.1793,-0.0461,0.0167,0.0287,-0.051,0.0255,-0.0326,0.0,-0.0335,-0.0351,0.026,0.0018,0.0072,0.0482,-0.0519,0.0596,0.0077,0.082,0.032,-0.0245,0.0147,-0.0033,0.0046,-0.0165,-0.0225,0.0204,-0.0508,0.0759,0.1294,0.0615,0.0049,0.0827,0.0207,-0.0255,-0.0045,0.0083,0.0004,0.014,-0.0135,0.0081,0.0071,-0.0083,-0.0517,-0.0288,-0.0794,-0.099,0.1206,-0.0685,-0.0087,-0.064,-0.0511,-0.0145,0.0234,-0.0178,-0.0051,0.0354,0.0331,0.0537,0.0217,-0.06,0.0114,0.0383,-0.078,-0.0369,0.1104,0.012,-0.0672,-0.0419,-0.0024,0.0411,-0.021,0.0773,0.0522,0.0061,0.0109,0.0773,0.0731,-0.0619,-0.0399,-0.0108,0.0001,0.0364,-0.0111,-0.0322,0.0689,0.011,-0.0439,0.0045,-0.0457,0.0848,0.0166,-0.0164,0.0306,-0.0061,-0.0021,-0.0135,-0.0435,-0.0066,0.0021,-0.0244,-0.0321,0.0122,0.0146,-0.0564,0.038,0.0284,-0.0242,-0.0178,0.0264,0.0535,-0.0005,-0.0426,0.0534,0.0581,0.0051,-0.024,0.0069,0.0135,0.0248,-0.0148,0.0314,0.0231,-0.0373,-0.0237,-0.2118,-0.0191,0.0539,-0.0315,0.0478,-0.0652,0.0174,-0.0016,0.0637,0.0647,0.0027,-0.0091,-0.0601,-0.0345,-0.01,0.0175,0.0679,0.0333,-0.0336,0.0168,0.039,0.0056,-0.0067,-0.0988,0.0222,0.017,0.2357,0.0084,0.072,-0.0363,0.0079,0.0155,-0.0095,-0.1223,0.062,0.0189,0.0502,0.026,-0.0429,0.0174,-0.0417,0.0109,-0.0155,-0.0932,-0.0443,-0.0288,-0.0404,-0.0285,-0.041,0.0681,0.0327,-0.0825,0.0423,0.0452,-0.0203,-0.0185,-0.1254,0.0198,-0.046,-0.0156,0.026,-0.0295,0.0007,-0.0968,0.0109,0.0314,-0.0443,-0.0055,0.0299,-0.0164,-0.0315,0.0987,-0.0249,-0.0095,0.06,0.0235,0.0028,-0.0512,-0.0151,-0.0312,0.086,-0.0322,0.0923,0.0232,0.0127,0.0176,0.0563,-0.0187,0.0,-0.0466,-0.0251,0.0106,-0.0352,-0.0347,0.0044,-0.0009,-0.2971,0.0431,-0.0165,-0.0042,-0.0018,0.0109,0.0071,0.0422,-0.0076,0.0077,-0.012,0.0228,0.0495,-0.0526,-0.0368,0.0183,0.0969,-0.0605,0.0162,0.0134,0.0004,0.0483,0.1906,-0.0324,0.0451,-0.0049,-0.0478,0.0371,0.0427,0.0297,0.0567,-0.0039,0.1231,-0.0107,0.0429,0.1134,-0.0214,0.0295,0.0291,-0.0043,0.025,0.0313,-0.0515,-0.0384,0.0576,0.019,-0.0177,-0.0584,-0.0379,0.006,-0.0678,-0.0167,-0.0435,0.0157,0.0709,0.0229,-0.0248,-0.0779,-0.0167,-0.0396,-0.0054,-0.0612,-0.0425,0.0336,-0.0443]}
{"key":"[Image-Based Feature Representation for Insider Threat Classification] Insiders are the trusted entities in the organization, but poses threat to the with access to sensitive information network and resources. The insider threat detection is a well studied problem in security analytics. Identifying the features from data sources and using them with the right data analytics algorithms makes various kinds of threat analysis possible. The insider threat analysis is mainly done using the frequency based attributes extracted from the raw data available from data sources. In this paper, we propose an image-based feature representation of the daily resource usage pattern of users in the organization. The features extracted from the audit files of the organization are represented as gray scale images. Hence, these images are used to represent the resource access patterns and thereby the behavior of users. Classification models are applied to the representative images to detect anomalous behavior of insiders. The images are classified to malicious and non-malicious. The effectiveness of the proposed representation is evaluated using the CMU CERT data V4.2, and state-of-art image classification models like Mobilenet, VGG and ResNet. The experimental results showed improved accuracy. The comparison with existing works show a performance improvement in terms of high recall and precision values.","layer":1,"vector":[-0.0501,-0.0388,-0.0353,-0.0018,0.0648,-0.0131,0.0892,0.0238,-0.0151,0.019,0.0467,-0.0274,0.0417,0.0182,0.002,0.0049,0.0155,0.0305,-0.009,0.0141,0.0015,-0.0349,0.0079,-0.0758,0.0039,-0.0294,-0.0309,0.0203,-0.0547,-0.1957,0.0296,-0.1042,0.0391,-0.0289,0.0133,-0.0592,-0.0433,0.0755,-0.0221,0.049,0.0169,0.0158,-0.0471,-0.0976,-0.0904,-0.0991,-0.0166,-0.0555,0.0039,-0.0527,0.0467,-0.0206,-0.0115,0.0613,0.0174,0.0416,0.0751,0.0365,0.0444,0.0608,0.0453,0.0277,-0.1545,0.0356,0.0205,0.0131,-0.0333,0.0038,0.0199,0.0098,-0.0264,0.0257,0.0065,0.0084,0.0046,-0.0129,-0.0242,-0.0121,-0.0567,0.0106,-0.0078,0.0131,-0.0159,-0.0043,0.0227,-0.0163,0.0237,-0.0649,0.0928,0.0007,-0.0088,-0.0092,-0.0157,-0.0068,-0.0455,-0.0737,0.0331,0.0238,-0.0707,0.2215,-0.0738,-0.0212,0.0625,-0.0413,0.0557,-0.0097,-0.0099,-0.0162,-0.0135,-0.0019,0.024,-0.0022,-0.0196,-0.057,0.0522,-0.0189,0.0291,0.0262,0.0136,0.004,0.0178,0.0443,0.0254,-0.0245,0.0249,-0.0827,0.0542,0.1253,0.048,0.0425,0.0169,0.0031,-0.0492,0.0208,0.0157,0.0653,-0.0066,0.0404,-0.0147,-0.0008,-0.0532,-0.0581,0.0281,-0.0738,-0.0451,0.1241,-0.0445,0.033,-0.0169,-0.041,0.0041,0.0273,-0.0345,-0.0394,-0.0244,-0.0166,0.0215,0.0502,-0.0495,0.0066,-0.0078,-0.0602,-0.0437,0.1136,0.0197,-0.0906,-0.0033,-0.0242,-0.026,-0.0356,0.0045,0.0117,-0.0178,0.0299,0.0545,-0.0321,-0.0266,-0.0457,-0.0121,0.0136,0.0588,-0.0646,-0.0618,0.0307,0.0324,-0.0276,0.0253,-0.0123,0.0297,0.0308,-0.0154,0.0416,-0.0238,-0.0075,-0.0681,-0.0433,-0.0292,-0.0041,-0.0233,-0.0723,0.0351,0.0302,-0.0459,0.028,-0.0353,0.0296,-0.0071,-0.0031,0.0082,0.0609,-0.016,-0.0052,0.0314,-0.0101,0.0189,-0.0124,0.0361,0.0961,0.0527,0.0216,0.1054,-0.0067,-0.0885,-0.257,-0.0141,0.0034,-0.0204,0.0147,-0.0871,0.0301,-0.0209,0.039,0.0183,0.0821,0.0119,-0.0442,0.0272,0.0081,0.0455,0.0395,0.0417,-0.0341,-0.0017,-0.0225,0.0304,-0.0182,-0.0785,0.0287,-0.0004,0.1799,0.0251,0.014,-0.0492,0.0046,0.031,-0.0394,-0.0746,0.0543,0.0158,0.0104,0.0094,-0.0433,0.0007,-0.0347,0.0452,-0.0054,-0.0521,0.0278,-0.0376,-0.0034,0.0396,-0.0821,0.023,0.0317,-0.0131,0.0333,0.0211,0.0189,-0.0253,-0.0334,0.0442,-0.0367,0.0769,-0.0485,-0.0644,0.0097,-0.1011,0.0382,-0.0528,-0.0346,-0.0277,0.0097,-0.0492,-0.027,0.1344,0.0138,-0.0694,0.0778,0.03,0.0466,-0.0451,-0.0411,-0.005,0.0311,-0.0098,0.065,0.0063,0.0366,0.0197,0.0534,0.019,0.0283,-0.0381,0.0292,-0.0123,-0.02,-0.0205,0.0404,0.0479,-0.3021,0.041,-0.0216,0.0332,-0.0149,0.0004,0.054,0.0057,-0.0435,-0.0117,0.0449,0.0083,0.0526,-0.043,-0.0052,0.0069,0.0325,-0.0413,0.0529,-0.0061,0.0095,0.0182,0.2533,-0.015,0.0345,0.0451,-0.0041,0.0104,0.0069,0.0171,0.064,-0.0175,0.096,-0.0303,0.0155,0.0463,-0.018,0.0393,0.0005,-0.0339,0.0072,0.0121,-0.0404,-0.017,0.09,-0.0156,-0.0041,-0.0275,0.0267,0.0348,-0.0301,-0.0325,-0.0438,0.0151,0.039,0.0455,-0.0504,-0.0337,-0.0114,-0.0415,0.05,-0.0498,-0.0191,0.0001,0.0135]}
{"key":"[Symphony Generation with Permutation Invariant Language Model] In this work, we present a symbolic symphony music generation solution, SymphonyNet, based on a permutation invariant language model. To bridge the gap between text generation and symphony generation task, we propose a novel Multi-track Multi-instrument Repeatable (MMR) representation with particular 3-D positional embedding and a modified Byte Pair Encoding algorithm (Music BPE) for music tokens. A novel linear transformer decoder architecture is introduced as a backbone for modeling extra-long sequences of symphony tokens. Meanwhile, we train the decoder to learn automatic orchestration as a joint task by masking instrument information from the input. We also introduce a large-scale symbolic symphony dataset for the advance of symphony generation research. Our empirical results show that our proposed approach can generate coherent, novel, complex and harmonious symphony compared to human composition, which is the pioneer solution for multi-track multi-instrument symbolic music generation.","layer":0,"vector":[-0.0441,-0.0061,0.0164,-0.0544,-0.0241,0.0305,-0.0579,0.0065,0.0357,-0.0264,0.007,-0.0379,0.0465,-0.002,0.0381,0.008,0.0185,0.0493,-0.0535,0.004,0.0642,0.0053,-0.0398,-0.0823,-0.0013,0.0422,-0.0373,-0.0158,-0.023,-0.2203,-0.0107,-0.0291,0.0754,-0.025,-0.0163,-0.0395,-0.0705,0.0456,-0.0435,0.0673,0.0376,0.0303,-0.0309,-0.0468,-0.0233,-0.0515,-0.0535,-0.0436,-0.011,-0.017,0.0213,-0.0226,0.006,0.07,0.0081,0.0457,0.0406,0.0608,0.0171,0.0367,0.0139,0.0232,-0.1502,0.0628,0.0092,0.043,-0.0232,0.0013,-0.0276,0.0378,-0.0664,0.0002,0.0567,0.0253,0.0231,-0.0198,0.0031,-0.0347,-0.0523,0.039,0.023,-0.0502,-0.063,-0.0041,-0.0185,-0.0342,-0.0347,-0.0023,-0.0132,0.0188,-0.0968,-0.0126,0.0056,0.0283,-0.0503,-0.0338,0.0383,0.0591,-0.028,0.199,-0.0418,0.0017,0.0391,-0.0719,0.0366,-0.0166,-0.0123,0.0017,-0.0027,-0.0179,0.0086,-0.0321,0.024,-0.0519,0.0308,0.0571,0.0435,0.0553,-0.0115,-0.0077,-0.0171,0.0355,0.0184,-0.0058,0.0562,-0.047,0.0533,0.0972,0.042,0.0519,0.0652,0.0264,-0.0543,-0.0383,-0.0216,-0.0112,-0.0115,-0.0401,0.0439,0.0208,-0.008,-0.0197,0.0393,-0.0663,-0.039,0.1257,-0.0272,0.0291,-0.0511,0.013,-0.013,0.0248,-0.0075,-0.0183,0.0318,0.0403,0.0268,0.0225,-0.0648,0.0043,0.0098,-0.0374,-0.0048,0.0943,0.0086,-0.0989,0.0005,0.0269,0.0242,-0.0204,-0.0047,0.0205,-0.0598,0.042,0.1042,0.0351,-0.0371,0.0144,0.0335,0.0421,0.0262,-0.0743,-0.0013,0.0348,0.0286,-0.0401,0.0138,-0.0609,0.0235,0.0255,0.0087,0.084,0.0142,-0.0087,-0.0325,-0.065,-0.0044,-0.0248,-0.0329,-0.0176,0.013,-0.0086,-0.0374,0.0266,0.0339,-0.0075,0.0079,-0.0181,0.023,0.046,-0.0025,0.0014,0.0656,0.0292,-0.0228,-0.019,-0.0546,0.0723,0.0369,0.0224,-0.0289,-0.0514,-0.0891,-0.2465,0.0068,0.023,0.0066,0.0312,-0.0216,0.0263,-0.0241,0.0393,0.0361,0.0552,0.0152,-0.021,0.0331,-0.0328,0.0381,-0.0073,0.0228,0.025,-0.0095,0.0038,0.0154,-0.0149,-0.0914,0.0221,-0.0589,0.2019,0.0241,0.0521,-0.0427,0.0203,0.034,-0.0495,-0.0829,0.0174,0.0453,0.0658,0.0382,-0.0353,-0.043,-0.0547,-0.0069,0.0054,-0.0566,-0.0224,-0.0609,-0.0331,-0.0334,0.0121,0.0457,0.0358,-0.038,0.0523,0.009,-0.0097,-0.0559,-0.1005,0.0252,-0.0227,0.0307,0.0364,-0.0252,0.0449,-0.0501,0.0073,0.0212,-0.0217,-0.0209,0.054,-0.0497,0.0206,0.0512,-0.0061,-0.0222,0.0922,0.0248,0.0082,-0.059,0.0212,-0.011,0.0626,-0.014,0.0277,-0.0239,-0.0107,0.0069,0.0801,0.0266,0.0256,-0.0732,-0.0111,0.0193,-0.0409,0.0091,-0.0099,-0.0024,-0.3356,0.0361,-0.0076,0.0248,-0.0367,-0.0097,0.0467,0.0137,-0.1045,0.0026,-0.0131,0.0455,0.0161,-0.0464,0.0211,0.0462,0.1187,-0.0521,0.0366,-0.0525,0.0122,0.0609,0.2492,0.0245,-0.0033,-0.0125,0.0021,0.024,0.0462,0.0025,-0.0055,-0.0271,0.0969,-0.0285,0.0018,0.0326,-0.03,0.0331,0.0492,-0.0279,-0.0104,-0.0132,-0.0788,-0.063,0.1284,0.0186,0.0012,-0.0311,0.0197,0.0394,-0.0006,-0.0004,-0.0334,-0.0048,0.0366,0.026,-0.0506,-0.0365,0.0114,-0.0196,0.0124,-0.0742,-0.0363,0.0263,-0.059]}
{"key":"[Verified Probabilistic Policies for Deep Reinforcement Learning] Deep reinforcement learning is an increasingly popular technique for synthesising policies to control an agent's interaction with its environment. There is also growing interest in formally verifying that such policies are correct and execute safely. Progress has been made in this area by building on existing work for verification of deep neural networks and of continuous-state dynamical systems. In this paper, we tackle the problem of verifying probabilistic policies for deep reinforcement learning, which are used to, for example, tackle adversarial environments, break symmetries and manage trade-offs. We propose an abstraction approach, based on interval Markov decision processes, that yields probabilistic guarantees on a policy's execution, and present techniques to build and solve these models using abstract interpretation, mixed-integer linear programming, entropy-based refinement and probabilistic model checking. We implement our approach and illustrate its effectiveness on a selection of reinforcement learning benchmarks.","layer":3,"vector":[-0.0905,-0.0121,0.0241,-0.0507,-0.0212,0.0922,0.0739,-0.0157,0.0482,0.0002,0.0323,-0.0523,0.0314,0.09,-0.0038,0.0322,-0.0267,0.0544,-0.0004,-0.0088,0.0353,-0.0706,-0.0175,-0.0693,0.0039,0.0686,-0.0096,-0.0338,-0.0233,-0.2337,0.0158,-0.0488,-0.0013,-0.0552,0.0166,-0.025,-0.0493,0.0246,0.0003,0.0233,0.0346,0.0272,-0.024,-0.0675,0.0237,-0.0302,0.0061,-0.0348,-0.0388,-0.0377,0.051,-0.0251,0.0444,0.0221,0.0457,0.0293,0.0996,0.0941,0.0437,0.0328,0.006,0.0193,-0.1625,0.0745,0.0189,0.0461,-0.0288,-0.0525,0.0077,0.0632,-0.0409,0.0472,0.0071,0.0372,-0.0191,-0.0012,0.0008,-0.062,-0.0086,0.0313,-0.0088,-0.0544,-0.032,0.0231,-0.0213,-0.0762,0.0312,-0.008,0.0357,-0.0189,-0.0368,0.0098,-0.0191,0.0174,-0.0286,0.0258,0.017,0.007,-0.0704,0.194,-0.0096,0.0358,-0.0056,0.0334,0.0592,-0.021,-0.0313,-0.0203,-0.0223,0.0026,-0.0469,-0.0115,0.0629,-0.022,-0.0067,0.0292,0.0599,0.0386,-0.0154,-0.0483,-0.0363,0.0035,0.0663,0.0096,0.002,-0.0779,-0.0143,0.1531,0.0221,0.0306,0.0218,-0.0605,-0.0347,-0.0367,0.0562,0.0136,0.0085,0.0051,0.0325,-0.0018,-0.0294,0.0255,-0.0059,-0.1019,-0.0354,0.1034,-0.0047,0.0256,-0.0509,-0.0087,-0.0284,0.0219,0.0198,-0.0568,0.0336,0.0085,0.0187,0.0345,-0.0171,-0.007,-0.0144,-0.0617,-0.0129,0.1214,-0.0126,-0.0633,-0.0228,-0.0227,-0.0125,-0.0284,0.0395,0.0309,-0.028,0.017,0.0197,0.04,-0.0831,0.0085,0.0076,0.0236,0.0019,-0.0573,0.0033,0.0096,0.0194,-0.0473,0.0137,-0.0636,-0.0034,0.0202,-0.0113,0.0098,-0.0197,-0.0094,-0.0247,-0.0573,-0.0258,-0.0187,0.0173,-0.0589,-0.0045,-0.0148,-0.0771,0.0112,-0.0239,0.0461,-0.0643,0.0067,0.0715,0.022,-0.0378,0.0502,0.0323,-0.0273,0.0039,0.023,0.001,0.0498,-0.006,0.0022,0.0277,-0.0073,-0.0182,-0.2085,-0.016,-0.0344,-0.0036,0.0442,-0.0571,0.0404,-0.0458,0.0057,0.043,0.0396,-0.0141,-0.0397,0.0011,-0.0005,0.0701,0.024,0.0067,-0.0469,0.0299,-0.0006,0.0094,-0.0642,-0.1079,0.0108,0.0012,0.2409,0.0218,0.068,0.0125,0.031,0.0575,-0.0183,-0.1149,0.0738,0.0241,0.0739,0.0036,0.0066,-0.0582,0.011,0.0695,-0.0361,-0.1123,-0.0236,-0.068,-0.014,0.0586,-0.042,-0.0202,0.0224,-0.0227,0.0104,-0.0056,-0.0359,-0.0131,-0.0733,0.0353,-0.0139,0.0446,-0.0051,-0.0308,0.0195,-0.0187,0.0454,-0.0044,0.0366,-0.061,0.04,0.0183,0.0093,0.0582,0.0031,-0.0106,0.0465,0.0176,0.0299,-0.0208,-0.0563,-0.0272,0.0531,-0.0317,0.05,0.0715,0.0355,-0.0206,0.039,-0.0196,0.0262,0.0035,0.0198,-0.0092,-0.0681,0.0217,0.0426,0.011,-0.2993,0.0551,0.0047,0.0289,-0.0449,-0.0112,0.0542,-0.0191,-0.0818,-0.0065,0.0386,0.0564,0.0697,-0.0052,-0.0004,0.0367,0.0712,-0.041,0.0595,-0.0695,0.005,-0.0046,0.218,-0.0377,0.0345,0.0066,-0.014,0.0438,0.0505,-0.0148,-0.0204,0.0083,0.0702,-0.0304,0.0633,0.1215,-0.0336,0.0401,0.0087,0.003,-0.0332,0.0009,0.0175,0.0195,0.0717,-0.0198,-0.0483,-0.022,-0.0245,0.0452,-0.0582,-0.0133,-0.028,-0.0372,0.0255,0.0322,-0.0443,-0.0847,-0.0279,-0.0406,-0.0011,-0.0536,0.0453,-0.0431,-0.0368]}
{"key":"[Scalable Generative Models for Graphs with Graph Attention Mechanism] Graphs are ubiquitous real-world data structures, and generative models that approximate distributions over graphs and derive new samples from them have significant importance. Among the known challenges in graph generation tasks, scalability handling of large graphs and datasets is one of the most important for practical applications. Recently, an increasing number of graph generative models have been proposed and have demonstrated impressive results. However, scalability is still an unresolved problem due to the complex generation process or difficulty in training parallelization. In this paper, we first define scalability from three different perspectives: number of nodes, data, and node/edge labels. Then, we propose GRAM, a generative model for graphs that is scalable in all three contexts, especially in training. We aim to achieve scalability by employing a novel graph attention mechanism, formulating the likelihood of graphs in a simple and general manner. Also, we apply two techniques to reduce computational complexity. Furthermore, we construct a unified and non-domain-specific evaluation metric in node/edge-labeled graph generation tasks by combining a graph kernel and Maximum Mean Discrepancy. Our experiments on synthetic and real-world graphs demonstrated the scalability of our models and their superior performance compared with baseline methods.","layer":5,"vector":[-0.0126,-0.0242,0.0177,-0.0321,0.0194,0.0387,-0.0378,0.006,0.0312,-0.0048,0.014,-0.0633,0.0642,0.0801,0.0361,0.0633,0.0179,0.0201,-0.0509,-0.0228,0.0402,-0.0265,-0.0108,-0.0704,0.0458,0.027,0.0001,-0.0355,-0.0464,-0.233,0.0237,-0.0425,0.0974,-0.0102,-0.0049,-0.0587,0.0106,-0.0039,-0.0185,0.0207,0.0166,0.0129,-0.0699,-0.0302,-0.0242,-0.0167,-0.0477,-0.0343,-0.0481,-0.0265,0.0316,-0.0654,0.0329,0.0104,0.0609,0.0581,0.0544,-0.0136,0.0459,0.0505,0.0209,0.0629,-0.1141,0.0244,0.0471,0.0238,-0.0487,-0.0041,-0.0126,0.0779,0.0136,0.0287,0.0129,0.0519,0.0322,0.0021,-0.014,-0.0026,-0.007,-0.0055,-0.0128,-0.065,-0.0367,-0.0332,0.0155,-0.0196,-0.0248,0.0066,0.0383,0.0431,-0.024,-0.0077,-0.0382,0.0041,-0.0809,-0.0252,0.0111,0.0043,-0.0453,0.2012,-0.0461,0.0359,0.0759,-0.0269,0.0188,-0.0186,0.007,-0.0332,-0.0273,-0.0081,0.0005,-0.0462,-0.0223,-0.0457,0.0184,-0.0314,0.1021,0.0371,-0.0418,-0.0212,-0.0285,0.0519,0.0199,-0.039,0.0468,-0.0538,0.0026,0.1346,0.0627,0.0416,0.0271,0.043,-0.0375,0.006,0.0024,-0.0139,0.0416,-0.0184,0.0152,-0.0067,0.0196,0.0257,-0.0095,-0.0489,-0.0594,0.1311,-0.0425,0.0082,-0.0474,-0.0427,-0.0332,-0.0046,-0.0069,-0.0337,0.0008,0.0536,0.0226,0.0307,-0.0391,0.0065,-0.0222,0.0054,-0.0559,0.0943,0.0217,-0.1011,-0.021,0.005,-0.0118,0.0173,0.026,0.0502,-0.0196,0.0525,0.0671,0.0449,-0.0767,-0.014,0.0443,0.0075,0.0482,-0.0506,-0.0229,0.0469,0.0258,-0.0717,0.0001,-0.0179,0.0035,0.053,-0.0125,0.0332,0.0188,-0.0248,-0.0336,-0.0468,-0.0337,-0.0361,0.0224,-0.0321,0.0376,-0.0111,-0.034,-0.0246,-0.0593,0.0063,-0.0374,0.0087,0.0177,-0.0124,-0.0745,0.0082,0.0457,0.0133,-0.0714,0.0051,0.0119,0.0436,0.005,0.0543,0.0214,-0.045,-0.0317,-0.2118,-0.0017,0.0244,-0.0219,0.0772,-0.0633,0.0228,0.0181,0.0395,0.0556,0.0473,-0.003,-0.0402,-0.02,-0.0112,0.0426,-0.0213,0.0345,-0.0012,-0.0397,0.0288,0.0299,0.016,-0.1152,0.0007,-0.0078,0.2362,0.0138,0.019,-0.0469,0.0307,0.0506,-0.0442,-0.099,0.0549,0.064,0.0476,-0.0036,-0.0389,-0.0051,-0.0247,0.0193,-0.007,-0.1222,-0.0121,-0.0153,-0.016,0.0071,-0.0243,0.009,0.0524,-0.0361,0.087,0.0136,-0.0137,-0.0878,-0.1403,0.0272,-0.0475,0.0369,0.0401,-0.0626,0.031,-0.0372,0.0562,0.0118,-0.0431,-0.0453,-0.0187,-0.0245,0.0068,0.0643,0.0276,0.0118,0.0648,0.0297,-0.0148,-0.035,-0.0449,-0.0422,0.0234,-0.0346,0.0649,0.0085,0.024,0.0257,0.0704,0.0157,0.0299,-0.0354,0.0043,0.0238,-0.0589,-0.0513,0.0467,-0.0503,-0.3194,0.0406,0.0017,0.0714,-0.0191,0.0174,0.0432,0.068,-0.042,0.0111,0.0264,0.0616,0.0315,-0.0321,-0.0393,0.0499,0.0634,-0.0303,0.0436,-0.0425,0.0361,-0.0082,0.2017,-0.0295,0.0515,0.026,-0.0172,-0.0254,0.0606,-0.0308,-0.0476,-0.0107,0.0836,-0.043,0.0693,0.0793,-0.0537,0.0565,0.0148,-0.0296,0.0213,-0.0046,-0.0335,-0.019,0.0649,0.0096,-0.0192,-0.0447,0.0152,0.0438,-0.046,0.0174,-0.0113,0.0386,0.044,0.0053,-0.0105,-0.03,-0.057,-0.045,-0.0135,-0.0794,-0.0053,0.0083,-0.0433]}
{"key":"[Lower Bounds for Policy Iteration on Multi-action MDPs] Policy Iteration (PI) is a classical family of algorithms to compute an optimal policy for any given Markov Decision Problem (MDP). The basic idea in PI is to begin with some initial policy and to repeatedly update the policy to one from an improving set, until an optimal policy is reached. Different variants of PI result from the (switching) rule used for improvement. An important theoretical question is how many iterations a specified PI variant will take to terminate as a function of the number of states $n$ and the number of actions $k$ in the input MDP. While there has been considerable progress towards upper-bounding this number, there are fewer results on lower bounds. In particular, existing lower bounds primarily focus on the special case of $k = 2$ actions. We devise lower bounds for $k \\geq 3$. Our main result is that a particular variant of PI can take $\\Omega(k^{n/2})$ iterations to terminate. We also generalise existing constructions on $2$-action MDPs to scale lower bounds by a factor of $k$ for some common deterministic variants of PI, and by $\\log(k)$ for corresponding randomised variants.","layer":1,"vector":[-0.0977,-0.0142,0.0331,-0.0525,-0.045,0.0638,0.0593,0.057,0.0623,0.0203,0.0384,-0.0226,0.0184,0.0448,-0.0079,0.0206,-0.065,0.0187,-0.0177,0.003,0.0372,-0.0625,-0.0222,-0.0517,0.0446,0.0112,-0.0175,-0.0717,-0.0319,-0.25,0.0468,-0.0302,0.034,-0.0336,0.0022,-0.0181,-0.0186,0.0579,-0.0364,0.0441,0.0779,0.0599,0.0084,-0.1009,-0.0126,-0.0538,-0.0334,-0.0209,-0.0308,-0.0626,-0.022,-0.0111,0.0398,0.031,0.0578,-0.0048,0.024,0.0617,-0.0041,0.013,-0.016,0.0241,-0.2043,0.0345,0.0244,0.0537,-0.0146,-0.0354,0.0188,0.1031,-0.062,0.0544,0.0271,0.0335,0.0063,-0.0143,-0.009,-0.032,0.0021,0.0216,-0.0277,-0.0866,-0.0255,0.007,-0.0298,-0.0502,0.0061,-0.0333,0.0258,0.0408,0.0032,0.0222,-0.0069,-0.0237,-0.0467,0.0174,0.0485,0.0542,-0.0546,0.1887,-0.0168,0.0476,0.0063,0.0036,0.0406,-0.0376,-0.0213,-0.031,-0.037,-0.0199,-0.0136,-0.0128,0.0586,-0.0389,-0.0006,0.0259,0.0604,0.0466,-0.0362,-0.0211,0.0232,0.0182,0.0739,-0.0035,0.0207,-0.0412,-0.0077,0.1382,0.0293,0.0168,0.0267,-0.0646,0.0034,-0.0373,0.0142,-0.0104,0.0307,0.0096,0.0102,-0.0269,-0.0302,-0.0716,-0.0007,-0.1242,-0.0008,0.1606,-0.0276,0.0518,-0.0512,-0.0313,0.0173,0.0066,-0.0139,-0.0441,0.0141,0.0291,0.033,0.0406,-0.0359,0.013,-0.0275,-0.0516,-0.0117,0.0888,-0.0178,-0.0681,0.0103,-0.0345,-0.029,-0.0121,0.0372,0.0637,-0.0074,-0.0235,0.0625,0.0033,-0.0905,0.0151,0.0246,0.0313,0.0256,-0.0263,-0.0495,-0.0211,0.0238,-0.0014,-0.0014,0.0132,0.0423,0.008,-0.0211,0.0018,0.0011,0.0364,-0.0581,-0.0674,-0.0339,-0.0006,0.0407,-0.0185,0.0411,-0.0042,-0.0612,0.0106,0.0028,-0.0026,-0.0442,0.0292,0.0231,0.0122,-0.0354,0.0302,0.0441,-0.0119,-0.0196,0.0232,0.0119,0.0428,-0.0387,0.0042,0.0313,-0.0179,-0.0158,-0.2145,-0.0202,-0.0209,0.0101,0.0776,-0.0727,0.0135,-0.0539,0.0234,0.054,0.0201,-0.0539,-0.0418,0.0457,-0.0089,0.0396,0.0115,0.0447,-0.0268,0.0147,0.0156,0.0084,-0.0359,-0.09,0.0512,0.0137,0.2288,0.0345,0.0375,0.0089,0.0414,0.066,-0.0452,-0.0652,0.05,0.0236,0.0445,-0.0296,0.0109,-0.0643,0.0164,0.0327,-0.0524,-0.1077,-0.0201,-0.0525,-0.019,0.0419,-0.0368,-0.0196,0.0365,-0.0065,-0.0169,-0.0332,-0.0014,-0.0041,-0.0814,0.0087,-0.0022,0.0698,-0.025,-0.0558,0.0164,-0.021,0.0543,-0.0332,0.0517,-0.0287,0.0036,-0.0144,-0.0086,0.0498,0.0267,-0.0179,0.0054,0.034,-0.0037,-0.0009,-0.0459,-0.0573,0.0717,-0.0683,0.03,0.0251,-0.008,-0.0289,0.0534,0.0152,0.0191,-0.0199,0.0125,-0.0021,-0.0425,0.0581,0.0173,-0.0122,-0.3018,0.0306,0.0159,0.0052,-0.0092,0.0351,0.0831,-0.0019,-0.071,0.0021,0.0207,0.0937,0.0242,0.0267,0.0047,0.0341,0.0482,-0.0121,0.053,-0.0836,0.0621,0.0344,0.1929,-0.0335,0.037,0.0076,-0.0008,0.029,0.0063,-0.025,0.002,-0.0105,0.0627,-0.062,0.0937,0.0621,-0.0468,0.0498,0.0233,0.0081,-0.0148,-0.0244,-0.0118,-0.0008,0.0996,-0.0014,-0.0687,-0.0746,0.0007,0.0453,-0.0414,0.0038,-0.016,-0.0217,0.0381,0.0592,-0.04,-0.0656,-0.0422,-0.0408,0.0232,-0.0552,0.0096,-0.0034,0.0272]}
{"key":"[Breaking the Communities: Characterizing community changing users using text mining and graph machine learning on Twitter] Even though the Internet and social media have increased the amount of news and information people can consume, most users are only exposed to content that reinforces their positions and isolates them from other ideological communities. This environment has real consequences with great impact on our lives like severe political polarization, easy spread of fake news, political extremism, hate groups and the lack of enriching debates, among others. Therefore, encouraging conversations between different groups of users and breaking the closed community is of importance for healthy societies. In this paper, we characterize and study users who break their community on Twitter using natural language processing techniques and graph machine learning algorithms. In particular, we collected 9 million Twitter messages from 1.5 million users and constructed the retweet networks. We identified their communities and topics of discussion associated to them. With this data, we present a machine learning framework for social media users classification which detects \"community breakers\", i.e. users that swing from their closed community to another one. A feature importance analysis in three Twitter polarized political datasets showed that these users have low values of PageRank, suggesting that changes are driven because their messages have no response in their communities. This methodology also allowed us to identify their specific topics of interest, providing a fully characterization of this kind of users.","layer":3,"vector":[-0.0129,-0.0034,-0.0038,0.0061,0.0846,-0.0138,0.0547,0.0559,-0.0163,-0.0208,0.0132,-0.0202,0.0154,0.0287,0.0595,0.0184,0.014,0.0303,-0.0496,0.0018,-0.0131,-0.0271,0.0222,-0.0444,0.0409,0.02,-0.0312,-0.0602,-0.0687,-0.1898,0.0262,-0.0445,0.083,-0.0382,0.0029,-0.022,-0.0154,-0.0042,-0.0411,0.0954,-0.004,-0.0041,-0.0174,-0.0443,-0.0091,-0.0271,0.0013,-0.0311,-0.0421,-0.0676,0.0072,-0.0294,0.0285,0.0117,0.0608,0.0318,0.0417,0.0516,0.0586,0.0254,0.069,0.0709,-0.1838,0.0672,0.0282,0.0264,-0.0289,0.0053,0.0082,0.0458,-0.0348,0.079,0.0431,0.0386,-0.0102,0.0253,-0.0181,0.0001,-0.0173,0.0205,0.0202,-0.0219,-0.0285,-0.0216,-0.0199,-0.0309,0.0367,-0.0156,0.0203,0.0196,-0.0419,0.0187,0.054,0.0365,-0.0636,-0.0505,0.0145,-0.0061,-0.0555,0.2153,-0.0635,0.0093,0.0459,-0.0529,0.0082,-0.0443,-0.0136,-0.0365,-0.0003,-0.0096,-0.0349,-0.0485,0.0251,-0.0372,0.0555,-0.0336,0.0954,0.0308,-0.0139,0.0209,-0.056,0.0483,0.0286,-0.0239,0.0515,-0.0291,0.0188,0.1266,0.0522,-0.0114,0.0366,0.0268,-0.0596,-0.0137,0.0053,0.0193,-0.0073,-0.0027,0.0138,0.0137,-0.0246,-0.0414,-0.0071,-0.0768,-0.061,0.1397,-0.0903,-0.0148,-0.0457,-0.0205,-0.0382,-0.0065,-0.0954,-0.0323,0.0152,0.0078,0.0647,0.0321,-0.0381,0.0081,0.0277,-0.0581,-0.0747,0.1021,0.0407,-0.1077,-0.0188,-0.0059,-0.0186,-0.0161,0.0562,0.0251,-0.0022,0.0439,0.0527,0.0252,-0.0386,-0.0382,0.0198,0.0235,0.0312,-0.0102,-0.0778,0.0684,0.0053,-0.0216,-0.0161,-0.0612,0.0676,0.0167,-0.009,0.0198,-0.0491,-0.0265,-0.0406,-0.0235,-0.0031,-0.0345,0.0056,-0.0162,-0.0108,0.0294,-0.0985,0.031,-0.0229,0.0307,-0.0091,0.0115,0.0554,-0.0064,-0.005,-0.0137,0.0054,-0.0235,-0.0298,-0.0594,0.0542,0.0297,0.0451,0.0542,0.0662,0.0024,-0.0476,-0.2267,-0.0598,0.0196,-0.0117,0.0292,-0.0684,0.0318,-0.046,0.0584,0.0809,0.0931,-0.0337,-0.0493,0.0404,0.0287,0.0322,0.0177,0.0418,-0.0215,0.0011,0.0003,0.0058,0.0057,-0.0903,0.0273,0.004,0.2303,0.0511,0.0269,-0.0231,0.038,0.0467,-0.0488,-0.1333,0.0704,0.0457,0.0492,-0.0388,-0.0322,0.024,0.0053,0.0301,-0.0062,-0.0722,-0.0262,-0.0661,-0.0119,-0.004,-0.0291,0.0292,0.0475,0.0075,0.0564,0.0669,-0.0288,-0.0373,-0.0786,0.0398,-0.0173,-0.0062,-0.0149,-0.0323,0.0212,-0.0708,0.0554,0.0318,-0.0442,-0.0182,0.0187,-0.0149,-0.0172,0.0793,0.0096,-0.0305,0.0407,-0.0235,0.0036,-0.0671,-0.0439,-0.0285,0.0656,-0.0189,0.0621,0.0068,-0.025,-0.0204,0.0404,-0.0128,0.0213,-0.0092,0.0296,0.0366,-0.0845,-0.0604,0.028,-0.0173,-0.2753,0.0409,0.0097,0.0349,0.0275,0.0455,0.0775,0.0609,-0.0596,0.0028,0.0395,0.0797,0.0261,-0.0421,-0.0095,0.0307,0.036,-0.0406,-0.0139,-0.0184,0.0227,-0.0007,0.2023,-0.0253,0.055,-0.0002,0.0068,-0.0007,-0.0003,-0.0227,-0.0532,-0.0123,0.0698,-0.0421,0.0447,-0.0089,-0.0425,-0.0144,0.0274,-0.0154,-0.0175,0.0228,-0.0365,-0.0375,0.0883,-0.0345,-0.0188,-0.0887,0.0332,0.0353,-0.026,0.0023,-0.0487,0.0588,0.025,0.0441,-0.0694,0.0131,0.0195,-0.0611,-0.0321,-0.0525,-0.0174,-0.0011,0.0177]}
{"key":"[DIPPA: An improved Method for Bilinear Saddle Point Problems] This paper studies bilinear saddle point problems $\\min_{\\bf{x}} \\max_{\\bf{y}} g(\\bf{x}) + \\bf{x}^{\\top} \\bf{A} \\bf{y} - h(\\bf{y})$, where the functions $g, h$ are smooth and strongly-convex. When the gradient and proximal oracle related to $g$ and $h$ are accessible, optimal algorithms have already been developed in the literature \\cite{chambolle2011first, palaniappan2016stochastic}. However, the proximal operator is not always easy to compute, especially in constraint zero-sum matrix games \\cite{zhang2020sparsified}. This work proposes a new algorithm which only requires the access to the gradients of $g, h$. Our algorithm achieves a complexity upper bound $\\tilde{\\mathcal{O}}\\left( \\frac{\\|\\bf{A}\\|_2}{\\sqrt{\\mu_x \\mu_y}} + \\sqrt[4]{\\kappa_x \\kappa_y (\\kappa_x + \\kappa_y)} \\right)$ which has optimal dependency on the coupling condition number $\\frac{\\|\\bf{A}\\|_2}{\\sqrt{\\mu_x \\mu_y}}$ up to logarithmic factors.","layer":0,"vector":[-0.0799,-0.0146,0.0375,-0.0419,-0.0576,0.0096,0.0151,0.0247,0.043,-0.0089,-0.0126,-0.0708,0.0414,0.0779,-0.0182,0.0448,0.0214,0.065,-0.0414,0.0639,0.0269,-0.001,-0.0549,-0.0893,0.0455,-0.0384,-0.0619,-0.0438,0.0091,-0.2376,0.0036,-0.0195,0.0449,-0.0473,-0.0284,-0.0145,0.0005,0.049,-0.0393,0.0071,0.0457,0.0162,-0.0115,-0.0238,-0.041,-0.0413,-0.0144,-0.0426,-0.0074,-0.0415,0.0133,-0.0135,0.038,-0.0106,0.0601,0.0328,0.0429,0.0274,0.0034,0.059,0.003,0.0282,-0.162,0.0546,0.0562,-0.0036,0.0096,-0.0433,-0.0051,0.1353,-0.0086,0.0184,-0.0064,-0.0065,-0.0062,0.0182,-0.0056,-0.019,-0.0159,-0.0327,-0.0233,-0.032,-0.0021,0.0301,0.0202,-0.0737,0.0339,-0.0309,0.043,0.0024,-0.0143,-0.0174,0.0361,0.0042,-0.0632,-0.0011,-0.0016,0.0322,-0.0891,0.2101,-0.0489,0.0306,0.0249,-0.0525,0.0408,-0.024,-0.0521,-0.0504,-0.0304,-0.0296,-0.0327,-0.0342,0.0656,-0.0191,0.0061,-0.0002,0.0569,-0.0019,-0.0218,0.0127,-0.0644,0.0381,0.0312,0.0397,0.0364,-0.0604,0.0204,0.0924,0.0325,0.0302,0.0372,-0.0079,-0.0399,-0.0571,-0.0245,-0.0177,-0.0476,0.0331,0.0283,-0.0222,-0.0325,-0.0807,0.0267,-0.1034,-0.0313,0.0992,-0.041,0.0308,-0.0756,-0.0381,-0.0101,-0.0405,-0.0252,0.0175,-0.0238,0.0258,0.0085,0.0508,-0.0684,0.0475,0.0044,-0.0085,-0.0033,0.114,-0.007,-0.074,0.0089,0.013,0.007,-0.0167,0.0275,0.0467,-0.0194,0.0132,0.1084,-0.0121,-0.0696,0.0144,-0.0162,0.018,0.0551,-0.0148,-0.0292,0.0443,0.0469,-0.074,0.0073,-0.0258,0.0246,0.0328,-0.0373,-0.0304,-0.0721,0.0016,-0.0061,-0.0438,0.0085,-0.0235,0.0219,-0.0054,0.0463,-0.0071,-0.0749,0.0516,0.004,-0.0223,0.0104,-0.0344,0.0266,0.0322,-0.0416,-0.0131,0.0552,-0.0236,-0.0038,0.0307,0.0233,-0.0034,-0.0585,0.0629,0.0695,0.0206,-0.0369,-0.2224,-0.0092,-0.0023,-0.0126,0.0462,-0.0504,0.0476,-0.0094,0.0264,0.0962,0.1091,-0.0055,-0.0446,0.0663,-0.0223,0.0627,0.0128,-0.0069,0.0027,-0.0248,-0.036,-0.0022,-0.041,-0.0411,0.0806,-0.0059,0.2324,0.0513,-0.0068,0.008,0.0321,-0.0266,-0.028,-0.0321,0.0363,0.0113,0.1055,-0.0572,-0.0012,-0.0578,0.044,0.0499,-0.0309,-0.0016,-0.0277,-0.0112,-0.02,0.0252,-0.0348,-0.0008,0.0801,-0.007,0.0236,-0.046,0.0408,-0.0715,-0.0364,0.0114,-0.016,0.0482,-0.0198,-0.0605,0.001,-0.0189,0.0193,0.0409,0.0185,-0.0013,0.0503,-0.0364,-0.0027,0.03,0.0014,0.0067,0.0606,0.0203,0.1118,0.0341,0.0044,-0.0375,0.0748,-0.086,0.0386,-0.0039,-0.0016,0.0121,0.0497,-0.0317,0.0308,-0.0287,0.0243,0.0047,-0.0804,-0.0068,0.0262,0.027,-0.3023,0.0067,0.0385,-0.0455,-0.0393,0.0458,0.0101,0.014,-0.0766,-0.0055,0.048,0.0559,0.035,-0.0277,0.0282,-0.0292,0.0385,-0.0563,0.0437,-0.0736,-0.0181,0.0287,0.1989,-0.0501,0.071,0.0162,0.0053,-0.0013,0.0355,-0.0226,-0.0171,0.0087,0.0939,-0.0728,0.0543,0.0991,0.0161,0.05,0.0176,0.0169,-0.0244,0.0156,-0.0559,-0.0223,0.0611,-0.0077,-0.0371,-0.0316,0.0125,0.0109,-0.0055,0.0256,0.0274,0.0064,0.0354,-0.0243,-0.0382,-0.0543,-0.0386,-0.0296,0.0007,-0.0602,-0.0456,0.0196,0.0226]}
{"key":"[Finite-Time Analysis of Asynchronous Stochastic Approximation and $Q$-Learning] We consider a general asynchronous Stochastic Approximation (SA) scheme featuring a weighted infinity-norm contractive operator, and prove a bound on its finite-time convergence rate on a single trajectory. Additionally, we specialize the result to asynchronous $Q$-learning. The resulting bound matches the sharpest available bound for synchronous $Q$-learning, and improves over previous known bounds for asynchronous $Q$-learning.","layer":4,"vector":[-0.0942,-0.023,0.0059,0.011,-0.0312,0.04,0.0078,0.0457,0.0676,0.0013,0.0475,-0.0319,0.0299,0.089,0.0253,0.0329,-0.0337,-0.0082,-0.0498,-0.0224,0.0403,-0.0188,-0.0142,-0.0202,0.0128,-0.045,-0.0404,-0.0787,-0.008,-0.2478,0.0071,-0.0418,-0.0167,-0.0142,0.0017,-0.0381,-0.0308,0.0588,-0.0092,0.0423,0.034,0.0441,-0.0688,-0.0844,-0.0252,-0.0727,-0.0172,-0.0407,-0.0311,0.0023,-0.0152,-0.007,0.0344,0.0243,0.0001,0.0295,0.0073,0.0478,0.024,0.0285,-0.0179,0.0127,-0.1646,0.0382,0.0315,-0.0098,-0.0218,-0.0271,0.0264,0.0513,-0.0106,0.0345,-0.0099,0.0525,0.0329,-0.0091,-0.0013,-0.0484,-0.0007,0.0339,-0.0189,-0.0474,-0.0533,0.004,-0.0618,-0.0262,0.0335,-0.0592,0.0308,0.0168,-0.0396,-0.0544,-0.0392,0.0134,-0.0697,-0.0038,0.016,0.0538,-0.0383,0.1676,-0.0324,0.0694,0.0118,-0.0133,0.0225,-0.0119,-0.0206,-0.0684,-0.0122,0.0011,-0.0359,-0.019,0.0498,-0.0609,0.0166,0.0464,0.051,0.0259,-0.0041,-0.0141,-0.0276,0.0005,0.0513,0.0164,0.0189,-0.0634,0.0116,0.1376,0.0377,0.0547,0.049,-0.0369,-0.0056,-0.0293,0.0154,0.0334,-0.0027,-0.0081,0.0639,-0.0141,-0.0284,-0.0914,0.0191,-0.058,-0.0603,0.1017,0.007,0.0817,-0.0536,-0.0437,-0.0021,-0.0097,-0.002,-0.0599,0.0512,0.0547,0.0785,0.0159,-0.0602,-0.0045,-0.0641,-0.0763,0.0269,0.0992,0.0066,-0.0503,-0.004,-0.0135,0.0192,-0.0286,0.021,0.0322,-0.0538,0.0209,0.0858,0.0259,-0.0462,-0.0098,0.0143,0.0113,-0.0149,-0.0553,-0.026,0.0138,0.0468,-0.0286,0.0468,0.0054,0.0286,0.0221,-0.0109,-0.0285,-0.0267,-0.0032,-0.0453,-0.061,0.0061,-0.0218,0.0505,-0.0515,-0.0162,-0.0222,-0.0646,0.0191,0.0356,-0.0134,-0.0562,0.0138,0.0337,0.0357,-0.011,-0.004,0.0413,-0.0221,-0.0081,0.0772,0.0381,0.0412,-0.031,0.0214,0.0189,0.0052,-0.0578,-0.2339,-0.0135,0.011,0.0301,0.0499,-0.0489,0.0172,-0.033,0.0418,0.0526,0.0188,0.0225,0.0128,-0.0039,-0.0221,0.0579,0.0527,0.0288,0.0042,-0.0072,-0.035,0.0402,-0.0701,-0.0864,0.0659,-0.0146,0.2097,0.013,0.0804,-0.0294,0.0305,0.0446,-0.0234,-0.0437,0.0372,0.0403,0.0864,-0.0068,-0.0006,-0.0295,0.0213,0.0378,0.0034,-0.1,-0.0307,-0.0055,-0.0309,0.0273,-0.0532,-0.0313,0.0432,-0.0301,0.0519,-0.0337,-0.023,-0.0205,-0.0536,0.0144,-0.0281,0.0695,-0.0074,-0.0548,0.0057,0.0044,0.107,0.0064,-0.0249,-0.0498,0.0152,-0.0303,0.0119,0.0463,-0.0322,0.0331,0.0732,0.0199,0.0494,-0.0189,-0.055,-0.0129,0.0772,-0.0721,0.0451,0.0208,-0.0283,0.0101,0.0693,-0.0002,-0.0003,0.0352,0.0143,0.0232,-0.0543,0.0015,0.032,-0.045,-0.3197,0.027,0.0044,0.0307,-0.0466,0.0147,0.0601,-0.0172,-0.0702,0.0328,-0.0094,0.1034,0.0488,0.0371,-0.0055,0.043,0.0872,-0.0255,0.0416,-0.0603,-0.0052,0.0242,0.2068,-0.0546,0.0253,0.0374,-0.0419,-0.0042,0.0385,-0.0286,0.0067,-0.0092,0.0666,-0.0566,0.058,0.0512,-0.0403,0.0796,-0.0077,0.0011,0.0219,0.0119,-0.0247,-0.0022,0.1289,-0.0164,-0.0405,-0.0806,-0.0078,0.0286,-0.0457,0.0178,0.0048,-0.0128,0.0151,0.016,-0.0345,-0.0358,-0.0399,-0.0692,0.0501,-0.0647,-0.0177,0.0116,0.0149]}
{"key":"[Machine Teaching of Active Sequential Learners] Machine teaching addresses the problem of finding the best training data that can guide a learning algorithm to a target model with minimal effort. In conventional settings, a teacher provides data that are consistent with the true data distribution. However, for sequential learners which actively choose their queries, such as multi-armed bandits and active learners, the teacher can only provide responses to the learner's queries, not design the full data. In this setting, consistent teachers can be sub-optimal for finite horizons. We formulate this sequential teaching problem, which current techniques in machine teaching do not address, as a Markov decision process, with the dynamics nesting a model of the learner and the actions being the teacher's responses. Furthermore, we address the complementary problem of learning from a teacher that plans: to recognise the teaching intent of the responses, the learner is endowed with a model of the teacher. We test the formulation with multi-armed bandit learners in simulated experiments and a user study. The results show that learning is improved by (i) planning teaching and (ii) the learner having a model of the teacher. The approach gives tools to taking into account strategic (planning) behaviour of users of interactive intelligent systems, such as recommendation engines, by considering them as boundedly optimal teachers.","layer":2,"vector":[-0.0695,-0.0071,0.0116,-0.0278,-0.0118,0.0208,0.0333,0.0423,0.0055,0.0135,0.0377,-0.0285,0.0061,0.0588,0.0026,0.0331,-0.0274,0.0361,0.0032,-0.0154,0.037,-0.0286,-0.0531,-0.0908,0.0147,0.0315,-0.03,-0.068,-0.0193,-0.1783,0.0379,-0.0359,0.0546,-0.0251,-0.033,-0.0203,-0.0012,0.0665,0.0141,0.065,0.0239,-0.012,-0.0427,-0.0618,-0.0062,-0.0008,0.02,-0.0508,-0.0346,-0.0295,-0.028,-0.0445,0.0248,0.0285,0.0316,0.0262,0.0047,0.074,0.0184,0.0308,-0.0127,0.024,-0.1603,0.0876,-0.0034,0.0599,-0.0676,0.0159,0.0092,0.0644,-0.003,0.0348,0.013,0.0749,0.0126,-0.0057,0.008,0.0021,0.0265,0.0002,-0.0519,-0.0583,-0.033,-0.0147,-0.0014,-0.0552,0.005,-0.0715,0.0686,-0.0137,-0.0333,-0.021,-0.0173,0.0134,-0.0305,-0.0087,0.0269,0.0374,-0.0642,0.2013,-0.0365,0.0426,0.0393,-0.0091,0.0198,-0.0357,-0.0295,-0.0228,0.0144,-0.0146,-0.0331,-0.0289,0.0563,-0.028,-0.0052,0.0551,0.042,0.0176,-0.0128,-0.0157,0.0176,0.0127,0.0984,-0.0308,0.0308,-0.0695,-0.0025,0.123,0.0115,0.0425,0.0464,-0.0687,-0.0734,-0.0152,0.0354,0.0283,-0.0129,-0.0024,0.0512,-0.0073,-0.0511,-0.0265,0.0157,-0.1106,-0.0269,0.1156,-0.0263,0.0471,-0.0551,-0.0229,0.0025,0.0211,-0.0126,-0.0497,-0.0042,0.082,0.0787,0.033,-0.0607,-0.0093,-0.0705,-0.0786,-0.025,0.1043,0.0143,-0.0334,-0.0137,0.0198,0.0354,-0.0316,0.071,0.0608,-0.0146,0.0397,0.0784,0.0091,-0.092,-0.0187,0.0029,0.0055,0.0307,-0.0115,0.0029,0.0393,0.0123,-0.0064,-0.012,-0.0383,0.0454,0.0612,-0.0123,0.0303,-0.0106,-0.0166,-0.049,-0.0575,0.0181,-0.003,0.008,-0.033,0.0022,-0.0247,-0.0539,0.0115,-0.001,0.0143,0.0132,-0.0035,0.0499,-0.0088,-0.0087,-0.0063,0.0299,-0.0461,-0.083,0.0204,0.0673,0.0651,0.0148,0.0608,-0.0096,0.0002,-0.0192,-0.2454,-0.0076,-0.0004,-0.0145,0.0578,-0.0507,0.0506,-0.0551,0.0226,0.0562,0.0501,-0.0249,0.0099,0.0126,-0.0265,0.0571,0.0212,0.0323,-0.0335,-0.0273,0.008,-0.0086,-0.0243,-0.0901,0.0233,0.0017,0.229,0.0521,0.0515,-0.0309,0.0732,0.0436,-0.019,-0.0853,0.0293,-0.0021,0.0668,-0.0316,0.0272,-0.0527,0.0257,0.0155,-0.0229,-0.0879,-0.0794,-0.048,-0.0418,0.0401,-0.0365,0.0269,-0.012,-0.0169,0.0335,-0.0453,-0.0688,-0.0073,-0.0897,0.0312,-0.0284,0.058,0.0286,-0.0542,-0.0345,-0.0577,0.0374,-0.0506,0.016,-0.0082,0.0548,-0.0209,-0.0132,0.0516,0.0224,-0.0169,0.0122,0.004,0.0468,-0.0604,-0.0505,0.0015,0.0574,-0.089,0.0311,0.0425,0.0287,-0.0116,0.0704,-0.0229,0.0221,0.0094,-0.0242,0.0259,-0.0547,0.0338,0.0342,-0.0258,-0.292,0.0244,0.0007,0.05,-0.0273,0.0332,0.0415,0.0033,-0.0397,-0.0249,0.0097,0.0697,0.0054,-0.0294,-0.0085,0.0434,0.0715,-0.0494,0.0132,-0.0794,-0.0018,0.0679,0.2302,-0.0514,0.0516,0.0247,-0.0332,-0.0019,0.0332,-0.0549,0.0453,-0.0088,0.0974,-0.0415,0.0128,0.0906,-0.0263,0.0505,0.0269,-0.0184,-0.0506,0.0072,-0.0496,-0.0132,0.0925,0.0295,0.005,-0.0789,-0.0318,0.0248,0.0327,-0.0358,-0.0226,0.0052,0.0336,0.0431,-0.0611,-0.0487,-0.027,-0.0202,0.0328,-0.0679,0.0139,-0.0276,0.0135]}
{"key":"[Towards robust and domain agnostic reinforcement learning competitions] Reinforcement learning competitions have formed the basis for standard research benchmarks, galvanized advances in the state-of-the-art, and shaped the direction of the field. Despite this, a majority of challenges suffer from the same fundamental problems: participant solutions to the posed challenge are usually domain-specific, biased to maximally exploit compute resources, and not guaranteed to be reproducible. In this paper, we present a new framework of competition design that promotes the development of algorithms that overcome these barriers. We propose four central mechanisms for achieving this end: submission retraining, domain randomization, desemantization through domain obfuscation, and the limitation of competition compute and environment-sample budget. To demonstrate the efficacy of this design, we proposed, organized, and ran the MineRL 2020 Competition on Sample-Efficient Reinforcement Learning. In this work, we describe the organizational outcomes of the competition and show that the resulting participant submissions are reproducible, non-specific to the competition environment, and sample/resource efficient, despite the difficult competition task.","layer":1,"vector":[-0.0452,-0.0443,-0.0044,-0.014,-0.0027,0.0121,0.0031,0.0433,0.0361,-0.0132,0.0006,-0.0386,0.0007,0.083,-0.0074,0.0185,-0.0257,0.0665,0.0165,0.0266,0.0064,-0.0532,-0.0329,-0.0604,-0.0181,0.0199,-0.0312,-0.0741,-0.038,-0.241,0.0528,-0.0584,0.0083,-0.0607,0.0186,-0.0057,-0.0386,0.0267,-0.0485,0.0307,0.0431,0.0077,0.0036,-0.0791,-0.0103,-0.0519,-0.0177,-0.0159,-0.0413,-0.0033,0.0207,-0.0329,0.0157,-0.0027,0.0518,0.0058,0.0458,0.0573,0.049,0.0524,0.004,0.0767,-0.1396,0.0654,0.0312,0.0799,-0.0473,0.008,-0.0094,0.0638,-0.0062,0.07,0.0165,0.0463,0.0051,0.0268,-0.0208,-0.0374,-0.0055,-0.0191,0.0148,-0.0666,-0.0509,0.0019,-0.04,-0.0642,0.0256,-0.0436,0.0671,0.0098,-0.0277,0.0322,-0.0271,0.0315,-0.0649,0.0074,0.0478,0.0063,-0.0726,0.2104,-0.0332,0.049,-0.0224,-0.0378,0.0501,-0.0684,-0.0392,-0.0086,-0.029,0.0331,-0.0123,-0.0293,0.0657,-0.0092,0.0168,0.0322,0.0658,0.0005,0.0108,-0.0061,-0.034,-0.017,0.0189,-0.0051,0.0273,-0.0336,0.0221,0.1462,0.0006,0.0107,0.0561,-0.0447,-0.0393,-0.0135,0.0353,0.0156,-0.0181,0.0136,0.015,0.0419,-0.0124,0.0097,0.0409,-0.1104,-0.0601,0.0775,0.0242,0.0418,-0.0163,-0.0241,-0.0182,0.0035,0.0008,-0.0412,0.0134,0.0398,0.032,0.0579,-0.044,-0.0207,-0.0204,-0.045,0.0047,0.1325,-0.0229,-0.0939,-0.0407,-0.0005,0.0196,-0.0195,0.0139,-0.0051,-0.0605,0.0255,0.0666,-0.0102,-0.1,-0.0154,0.0223,-0.0063,0.0365,-0.0651,-0.0477,0.0112,0.0685,-0.0574,0.0015,-0.0392,0.0418,0.0284,-0.0676,0.0227,-0.0102,0.0413,-0.0303,-0.0296,0.023,-0.0685,0.0125,-0.0005,-0.0268,0.0215,-0.052,0.008,0.0019,0.0203,0.0313,-0.0451,0.0402,0.023,-0.0387,0.003,-0.0005,-0.0291,-0.0219,0.0109,0.0476,0.0357,-0.0027,0.0207,0.051,0.026,-0.0176,-0.2436,-0.0216,-0.0359,0.0233,0.0841,-0.0345,0.0547,-0.0106,0.0308,0.0547,0.0542,-0.0269,-0.0271,0.0433,-0.0178,-0.0024,0.0408,0.0286,0.0046,-0.0172,0.0073,0.0058,0.012,-0.0823,0.0385,0.0021,0.2246,0.0613,0.0141,-0.0047,0.0123,0.0609,-0.0489,-0.097,0.0293,0.0359,0.0347,-0.0123,0.0122,-0.0579,-0.0237,0.0554,-0.0045,-0.1363,-0.0149,-0.0394,-0.0719,0.0548,-0.0434,0.0649,0.0443,-0.0319,0.0826,-0.014,-0.023,-0.0135,-0.1016,0.0259,-0.0216,0.0128,0.0153,-0.0365,0.0218,-0.028,0.0503,0.0031,0.0357,-0.0583,0.0762,0.0114,-0.0132,-0.0065,0.0208,-0.0201,0.0211,0.025,-0.0088,-0.0119,-0.0425,0.0012,0.0596,-0.0494,0.0361,0.0714,0.0244,0.0167,0.073,0.0082,0.0254,-0.043,-0.0012,0.0408,-0.0664,-0.0335,0.0527,0.0143,-0.3077,0.0238,0.0606,0.053,-0.0209,0.0048,0.0401,0.014,-0.058,-0.005,0.0217,0.0791,0.0053,0.0469,0.0111,0.0314,0.0831,-0.0356,0.0415,-0.0449,0.0133,0.0585,0.2051,-0.0586,0.0348,0.0103,-0.0572,-0.0155,0.0128,-0.0403,0.0131,-0.0035,0.0604,-0.0578,-0.0085,0.0748,-0.0425,0.0129,0.0059,-0.0023,-0.0741,-0.0004,0.0092,-0.0004,0.0715,0.0078,0.003,-0.0526,-0.0314,0.0484,-0.0397,-0.0309,-0.0365,-0.0409,0.0052,0.0269,-0.0286,-0.0707,-0.0338,-0.0422,0.0116,-0.03,0.024,0.0176,-0.0226]}
{"key":"[Multidimensional counting grids: Inferring word order from disordered bags of words] Models of bags of words typically assume topic mixing so that the words in a single bag come from a limited number of topics. We show here that many sets of bag of words exhibit a very different pattern of variation than the patterns that are efficiently captured by topic mixing. In many cases, from one bag of words to the next, the words disappear and new ones appear as if the theme slowly and smoothly shifted across documents (providing that the documents are somehow ordered). Examples of latent structure that describe such ordering are easily imagined. For example, the advancement of the date of the news stories is reflected in a smooth change over the theme of the day as certain evolving news stories fall out of favor and new events create new stories. Overlaps among the stories of consecutive days can be modeled by using windows over linearly arranged tight distributions over words. We show here that such strategy can be extended to multiple dimensions and cases where the ordering of data is not readily obvious. We demonstrate that this way of modeling covariation in word occurrences outperforms standard topic models in classification and prediction tasks in applications in biology, text modeling and computer vision.","layer":1,"vector":[-0.0107,-0.0073,-0.0113,0.0039,0.0299,0.0421,0.0477,-0.0023,0.0363,-0.0132,0.0087,0.0175,0.0294,0.0839,0.0348,0.024,0.0257,0.0002,-0.0755,-0.0091,0.0375,-0.0233,0.01,-0.0173,0.0145,0.0425,-0.0534,-0.0556,-0.0248,-0.2442,-0.0156,-0.0394,0.0376,-0.0112,0.0492,-0.0142,-0.0311,0.077,-0.0197,0.0413,0.0074,0.0128,-0.0204,-0.0368,-0.0496,-0.048,-0.0168,0.0277,-0.02,-0.047,0.0207,-0.0776,-0.0233,0.0145,0.0714,0.0362,0.0436,-0.027,0.0177,0.0475,0.0325,0.0407,-0.1692,0.0865,0.0425,-0.0073,-0.0346,0.0156,-0.0022,0.0845,-0.0219,0.0363,0.0335,0.0836,0.0424,-0.0092,0.0101,-0.0397,-0.0191,0.0113,-0.0052,-0.0166,-0.0267,0.0095,-0.0533,-0.0508,0.0067,-0.0389,0.0079,-0.0213,-0.0328,-0.0058,-0.023,0.0199,-0.0658,-0.0424,0.0137,0.0213,0.0181,0.2038,-0.0448,0.0229,0.0673,-0.0277,0.0204,-0.0451,-0.0411,-0.076,-0.0416,-0.0125,-0.0015,0.0011,0.0472,-0.0405,0.0565,-0.0262,0.076,0.0363,-0.0072,0.0266,-0.037,0.0207,0.0226,-0.0013,0.019,-0.0165,0.0417,0.1245,0.0495,-0.0267,0.0321,0.0285,-0.0511,-0.0225,0.0441,0.0057,-0.0001,0.0136,0.0354,-0.0392,-0.036,-0.0509,0.0092,-0.1085,-0.0686,0.1729,-0.0758,0.0241,-0.0592,-0.0171,-0.0184,0.0077,0.0095,-0.0752,0.0109,0.0165,0.0577,0.0093,-0.0148,0.0253,0.0273,-0.0188,-0.0128,0.103,0.0028,-0.0728,-0.0106,0.0124,0.0403,-0.0463,0.0531,0.0015,-0.0137,0.0475,0.0741,0.02,-0.0525,-0.0017,0.0471,0.0085,0.0413,-0.0527,-0.0486,0.0899,0.0189,-0.0504,0.0239,-0.0314,0.0419,0.0266,-0.0001,0.0257,0.0215,0.0119,-0.0345,-0.0339,-0.0432,0.0049,0.0176,-0.055,-0.0189,-0.0027,-0.0455,0.0114,0.0072,-0.0107,0.0097,-0.0198,0.0377,0.0155,-0.0474,-0.0151,-0.0103,-0.0338,-0.0199,-0.0294,-0.0096,0.0429,0.0002,0.0054,0.0435,-0.0893,0.0014,-0.2531,-0.0305,-0.0124,-0.0238,0.0485,-0.0324,0.0593,-0.0287,0.0536,0.11,0.0168,-0.0394,-0.0002,-0.0153,0.0133,0.0342,0.0306,0.0054,-0.0156,-0.0138,-0.0098,0.0116,-0.0166,-0.1182,0.0313,-0.0094,0.2141,0.0873,-0.0268,0.0047,0.0365,0.0122,-0.0135,-0.0735,0.0612,0.0327,0.0534,0.0055,-0.0782,-0.0168,-0.0747,0.0293,0.0114,-0.0734,-0.0505,-0.0446,-0.0219,0.0043,-0.0524,0.0155,0.0138,-0.0643,0.0431,0.0168,0.0103,-0.0334,-0.0607,0.012,-0.01,-0.0227,-0.0192,-0.0248,0.043,-0.0549,0.0027,0.0132,-0.0396,-0.0185,0.0056,-0.0199,-0.018,0.0708,-0.0102,-0.0251,0.0319,0.0087,-0.0031,-0.0014,-0.0433,-0.0395,0.0906,-0.029,0.0564,0.0113,0.065,-0.0055,0.0859,-0.0017,0.0179,0.0134,0.0241,0.0439,-0.0435,-0.0138,0.0345,-0.0035,-0.298,0.0522,0.0085,0.0206,0.0326,0.0274,-0.0055,0.0639,-0.0159,-0.0008,0.0067,0.0663,0.034,-0.0446,-0.0162,0.0456,0.0795,-0.0698,0.0284,-0.0618,-0.0009,0.0364,0.2198,-0.0183,0.0224,-0.0239,0.0037,0.0287,-0.0187,-0.0033,0.0219,0.0036,0.1131,-0.0198,0.0239,0.0541,-0.0254,0.0793,0.0258,-0.0239,-0.0509,0.0308,-0.0531,-0.0386,0.0864,0.0007,0.0012,-0.0545,-0.0171,0.025,-0.0266,-0.0137,-0.015,0.0519,-0.0021,0.0084,-0.0189,-0.0409,0.0015,-0.0794,-0.0098,-0.1004,-0.0188,-0.0239,-0.0206]}
{"key":"[On the Covariance-Hessian Relation in Evolution Strategies] We consider Evolution Strategies operating only with isotropic Gaussian mutations on positive quadratic objective functions, and investigate the covariance matrix when constructed out of selected individuals by truncation. We prove that the covariance matrix over $(1,\\lambda)$-selected decision vectors becomes proportional to the inverse of the landscape Hessian as the population-size $\\lambda$ increases. This generalizes a previous result that proved an equivalent phenomenon when sampling was assumed to take place in the vicinity of the optimum. It further confirms the classical hypothesis that statistical learning of the landscape is an inherent characteristic of standard Evolution Strategies, and that this distinguishing capability stems only from the usage of isotropic Gaussian mutations and rank-based selection. We provide broad numerical validation for the proven results, and present empirical evidence for its generalization to $(\\mu,\\lambda)$-selection.","layer":0,"vector":[-0.0753,-0.018,-0.0066,0.0287,0.0218,0.0141,0.0576,0.0238,0.0223,0.0287,0.0366,-0.0246,0.0235,0.0546,-0.005,0.042,0.0149,0.0283,-0.0596,0.0184,0.0228,-0.0395,0.0039,-0.0723,0.0542,0.0107,-0.0474,-0.0307,-0.0159,-0.2329,0.0159,-0.0136,0.0641,-0.0652,-0.0053,0.0062,-0.0249,0.0507,-0.0146,0.046,-0.007,0.0327,-0.0384,-0.0492,-0.0365,-0.0301,-0.0239,-0.0012,-0.0582,-0.0657,-0.0137,-0.0273,-0.0111,0.0192,0.0473,0.0567,0.0631,0.009,0.0346,0.05,0.013,0.0535,-0.1827,0.0453,0.0488,0.0472,0.0037,-0.0545,0.0424,0.0732,-0.0092,0.0378,0.007,-0.0019,0.0363,0.0179,-0.0379,0.0123,-0.01,0.0257,0.0583,-0.0312,-0.0404,0.0211,-0.007,-0.0671,0.0404,-0.0119,0.0745,0.0471,0.0083,0.0167,-0.041,-0.0172,-0.0873,-0.0008,0.0225,0.0031,-0.0296,0.1661,-0.0376,0.0185,0.0049,-0.0136,0.0307,-0.008,-0.0634,-0.0355,0.0146,0.0089,0.0171,-0.0128,0.0027,0.0177,-0.0357,-0.0088,0.0135,0.0355,-0.0061,-0.0105,-0.0697,0.0025,0.0805,-0.0326,0.0244,-0.0345,-0.0047,0.1309,0.0299,-0.0004,0.0319,-0.0228,-0.0501,-0.0484,-0.0191,-0.0084,-0.0386,0.0304,0.0154,0.0189,-0.0565,-0.0351,0.024,-0.0837,-0.0543,0.1312,-0.0411,0.0469,-0.0412,-0.0127,0.0007,0.0209,-0.0211,-0.036,0.0056,0.0487,-0.0112,0.0414,-0.0296,0.0295,-0.0734,-0.0156,-0.0171,0.1336,-0.0116,-0.0456,-0.0543,0.0368,0.0263,-0.0095,0.0096,0.0104,-0.0514,0.0345,0.1037,-0.0029,-0.1351,0.0013,0.0207,-0.0112,0.0407,-0.0356,-0.0196,0.0806,0.0471,-0.0215,-0.0298,-0.057,0.0202,0.0349,-0.049,-0.0138,0.0241,-0.0255,-0.0587,-0.0081,-0.0149,0.0165,0.0468,0.0441,-0.0045,0.0313,-0.0496,-0.0078,-0.0096,0.0091,0.0261,-0.0403,0.0315,0.0622,-0.0179,-0.0266,0.0089,0.0002,-0.0793,0.0148,-0.0022,0.0257,-0.0019,0.0639,0.0194,-0.0529,-0.0579,-0.23,-0.0285,-0.0611,-0.0205,0.0547,-0.0711,0.075,-0.0212,0.0595,0.0496,0.0093,0.0025,-0.0371,0.0278,-0.021,0.0245,0.0347,0.0115,-0.0031,-0.034,0.0227,0.0263,-0.025,-0.0893,0.0806,-0.0295,0.2062,0.0461,0.0398,0.0253,0.0249,-0.0131,-0.0179,-0.027,0.0442,0.0694,0.0722,-0.0637,-0.0131,-0.0307,0.0337,0.0074,-0.0411,-0.0867,-0.0659,-0.01,-0.0346,0.0936,-0.0483,0.0098,0.0863,-0.0064,0.0912,-0.0506,-0.0068,-0.0597,-0.0953,0.0088,-0.0321,0.0574,0.0114,-0.0618,0.0149,-0.0419,0.0601,-0.0348,0.0098,-0.0513,0.0273,-0.0301,-0.0068,0.0357,0.0194,-0.0027,0.0327,-0.0045,0.0516,-0.0158,-0.0515,-0.0353,0.0115,-0.0667,0.0467,0.0099,0.029,-0.0076,0.0491,-0.0486,0.0036,-0.0079,-0.0401,0.0126,-0.0666,-0.0247,0.0139,0.0035,-0.2852,0.0508,0.0446,0.0067,-0.0451,0.0099,0.0263,0.0185,-0.0222,-0.0094,0.0012,0.0323,0.0381,0.0176,0.0463,0.0335,0.0642,-0.0644,0.0544,-0.0792,0.0196,0.0026,0.2166,-0.025,0.0542,0.0144,-0.0471,0.0404,0.0036,-0.0421,0.0199,0.0409,0.1032,-0.0731,0.0655,0.0729,-0.0205,0.0163,-0.0131,-0.001,-0.0428,0.0283,-0.0306,0.0177,0.132,-0.0375,-0.0038,-0.0685,-0.0376,0.0361,-0.0125,0.0253,-0.0418,-0.0217,0.0359,0.0112,-0.0592,-0.0422,-0.0197,-0.0307,0.0678,-0.027,-0.01,0.0078,0.0089]}
{"key":"[Learning Multi-Layer Transform Models] Learned data models based on sparsity are widely used in signal processing and imaging applications. A variety of methods for learning synthesis dictionaries, sparsifying transforms, etc., have been proposed in recent years, often imposing useful structures or properties on the models. In this work, we focus on sparsifying transform learning, which enjoys a number of advantages. We consider multi-layer or nested extensions of the transform model, and propose efficient learning algorithms. Numerical experiments with image data illustrate the behavior of the multi-layer transform learning algorithm and its usefulness for image denoising. Multi-layer models provide better denoising quality than single layer schemes.","layer":3,"vector":[-0.0214,-0.0146,0.0208,-0.0159,0.0374,0.0536,-0.0089,0.0124,0.0165,-0.0148,-0.0077,-0.0713,0.0486,0.0317,0.0444,0.0214,0.0159,0.0602,-0.0435,-0.0278,0.0295,-0.0417,0.0105,-0.0283,0.0404,0.0179,0.0216,-0.0689,-0.0532,-0.247,-0.0016,-0.0376,0.0443,0.0147,0.0183,-0.005,-0.0345,0.0741,-0.0587,0.0522,0.0057,0.0089,-0.0116,-0.0767,-0.0395,-0.0408,-0.0142,-0.0635,-0.0004,-0.0336,0.0263,-0.0378,0.0171,0.0232,0.0382,-0.0044,0.026,0.0562,0.0694,0.0592,-0.0054,0.0683,-0.1679,0.0609,0.0368,0.0389,-0.0342,-0.0631,0.0083,0.0324,-0.0082,0.016,-0.0275,0.0119,0.0112,-0.0321,-0.0083,-0.0109,-0.0151,0.014,0.0193,-0.0677,-0.0054,-0.0384,-0.0162,-0.0652,0.0236,-0.0876,0.0297,0.0052,-0.0437,-0.0246,-0.0291,0.0542,-0.0628,-0.0168,0.0342,0.0357,-0.0142,0.1836,-0.0443,0.0004,0.0479,-0.0516,0.0281,-0.0602,-0.0338,-0.0181,-0.0445,0.0192,-0.0475,-0.0,0.0274,-0.0338,-0.0007,-0.0394,0.0501,0.0734,-0.0475,-0.0363,-0.0223,-0.0104,0.0336,0.0003,0.0675,-0.0691,0.0307,0.1291,0.0835,0.0644,0.0545,-0.0044,-0.0145,-0.0152,-0.0059,0.0251,-0.0047,-0.0188,-0.0268,0.0128,-0.0687,-0.0705,0.0268,-0.0512,-0.0545,0.1104,-0.0413,0.0116,-0.0983,-0.0359,-0.0763,0.0136,-0.0104,-0.0058,0.0404,0.0333,0.0005,0.0496,-0.0516,0.0139,0.0017,-0.1038,-0.0116,0.09,0.0004,-0.0434,-0.0486,-0.0473,0.0077,0.007,0.0577,0.0108,-0.0236,0.019,0.0989,0.0169,-0.0622,0.0076,0.0143,0.0279,0.0348,-0.0557,-0.0278,0.0398,0.0368,-0.0288,0.011,-0.0104,0.0342,0.0418,-0.0667,-0.0021,-0.0187,-0.0259,-0.0328,-0.0369,-0.0054,-0.0088,-0.0143,-0.0446,0.0049,-0.0191,-0.0212,0.0242,-0.0303,0.0283,0.0109,-0.0083,-0.0149,0.0569,0.0293,-0.0021,0.1068,-0.0085,0.0071,0.0102,0.0546,0.0564,-0.0207,0.057,0.0316,-0.0602,-0.1007,-0.2007,-0.0519,0.0335,0.0067,0.0602,-0.0629,0.036,-0.0127,0.0538,0.0751,0.0447,-0.0185,-0.0175,0.0214,0.0231,0.0478,0.0235,0.0351,-0.0161,-0.0257,-0.042,0.0427,0.0511,-0.0871,0.0624,0.0137,0.1993,-0.0404,0.0257,0.023,0.0305,0.0293,-0.012,-0.048,0.0548,0.0245,0.066,-0.0079,-0.034,-0.0192,-0.0163,-0.0028,-0.0042,-0.0811,-0.0271,-0.0298,-0.069,0.0296,-0.0756,0.002,0.0208,-0.003,0.0861,-0.0447,0.0097,-0.0296,-0.1166,0.0219,-0.0486,0.0282,0.0262,-0.0357,0.0155,-0.1038,0.0356,0.0303,-0.0182,-0.0673,0.0034,-0.0509,-0.0165,0.0916,0.0329,0.0047,0.0082,0.0331,0.0257,-0.0154,-0.0545,-0.0645,0.0602,-0.0301,0.0463,0.0013,0.0726,0.0198,0.111,0.0013,0.017,-0.0266,-0.0034,-0.0128,-0.0337,0.0083,0.0369,-0.0223,-0.2989,-0.0111,0.0552,0.0429,0.0086,0.0195,0.0219,0.042,-0.0504,0.0179,-0.0581,0.008,0.051,0.0031,0.0107,0.0754,0.0606,-0.0491,0.0657,-0.0199,-0.0064,0.0387,0.214,-0.0188,0.0039,0.0375,-0.0287,0.0164,0.0093,-0.0187,0.0451,0.0447,0.0529,-0.0363,0.047,0.0946,-0.0807,0.0559,-0.0348,0.0007,0.0147,-0.0052,-0.041,-0.0096,0.0903,0.0191,-0.0018,0.0104,-0.027,0.006,-0.0196,0.0433,0.016,-0.0245,0.0275,0.024,-0.0321,-0.0412,-0.0228,-0.0033,0.0418,-0.061,-0.0351,-0.0077,-0.045]}
{"key":"[Variational autoencoders in the presence of low-dimensional data: landscape and implicit bias] Variational Autoencoders are one of the most commonly used generative models, particularly for image data. A prominent difficulty in training VAEs is data that is supported on a lower-dimensional manifold. Recent work by Dai and Wipf (2020) proposes a two-stage training algorithm for VAEs, based on a conjecture that in standard VAE training the generator will converge to a solution with 0 variance which is correctly supported on the ground truth manifold. They gave partial support for that conjecture by showing that some optima of the VAE loss do satisfy this property, but did not analyze the training dynamics. In this paper, we show that for linear encoders/decoders, the conjecture is true-that is the VAE training does recover a generator with support equal to the ground truth manifold-and does so due to an implicit bias of gradient descent rather than merely the VAE loss itself. In the nonlinear case, we show that VAE training frequently learns a higher-dimensional manifold which is a superset of the ground truth manifold.","layer":3,"vector":[-0.0277,-0.0478,0.0183,0.0176,0.0194,0.0247,-0.0392,0.0234,-0.0099,0.019,0.0265,-0.0705,0.0575,0.0769,0.0186,0.0277,0.0414,0.0466,-0.0339,0.0423,0.033,-0.0321,-0.0061,-0.0438,-0.0236,-0.0218,-0.0084,-0.0527,0.0,-0.2724,0.0445,-0.039,0.0637,-0.0163,0.0021,-0.0629,-0.0583,0.0275,-0.0427,0.0447,0.0123,0.0094,-0.0292,-0.0166,-0.0328,-0.0312,0.0117,-0.0121,-0.0392,-0.0441,0.0279,-0.0167,0.0014,0.0233,-0.0015,0.0246,0.0795,0.0609,0.0425,0.0529,0.0342,0.0371,-0.1751,0.078,0.027,-0.0087,-0.0647,-0.0239,0.0049,0.0148,0.0037,0.0149,0.0073,0.0201,0.0392,-0.0224,0.0212,0.0233,-0.0423,0.0187,0.0369,-0.0181,-0.0374,0.0239,-0.0099,-0.02,0.0514,-0.0318,0.0484,-0.0093,-0.0128,-0.0442,-0.0448,0.0349,-0.0326,-0.0214,0.0065,0.0611,-0.0399,0.1809,-0.0572,0.0069,0.0299,-0.0484,0.0317,-0.0815,-0.0517,0.0013,0.0002,-0.0304,0.0114,-0.0321,0.0136,-0.0513,-0.0073,-0.0441,0.0703,0.0306,-0.0439,0.003,-0.0573,0.009,0.043,-0.0439,0.0281,-0.0465,0.0188,0.1633,0.0676,0.0342,0.0277,0.0005,-0.0095,-0.0499,-0.01,-0.0057,0.0229,0.0214,0.0113,0.0038,-0.0414,-0.075,-0.0029,-0.0423,-0.0291,0.1066,-0.0826,0.0287,-0.0585,-0.0111,0.0458,0.031,-0.0409,-0.0228,0.0392,0.0488,-0.0111,0.0181,-0.0691,0.0514,-0.028,-0.046,-0.0468,0.1103,-0.0115,-0.0425,-0.0406,-0.0046,0.0729,0.0034,0.0454,0.0188,-0.019,0.0031,0.0631,0.0312,-0.085,0.0298,-0.0098,0.0311,-0.0111,-0.0335,0.0022,0.061,0.0122,-0.0015,0.0256,-0.0384,0.0642,0.0191,0.0093,0.016,-0.0378,-0.0217,0.0056,-0.0209,-0.0169,-0.0237,0.0114,-0.0166,0.0249,0.0088,-0.0296,0.0555,-0.0217,0.02,-0.0047,-0.0185,0.0565,0.0329,-0.0229,-0.0441,0.033,-0.0306,-0.0413,-0.0136,-0.0305,-0.0001,-0.0399,0.0413,-0.0051,-0.0615,-0.0341,-0.2118,-0.0162,-0.0106,-0.0686,0.0364,-0.0673,0.0358,-0.0327,0.0422,0.0427,0.0255,-0.0134,0.0178,0.0339,-0.0067,0.0641,0.0126,0.0402,-0.0275,-0.003,-0.0061,0.0023,-0.0041,-0.0967,0.0719,0.0108,0.2548,0.0074,0.0433,-0.025,0.043,0.0365,-0.0097,-0.0817,0.092,-0.0277,0.0505,-0.0211,-0.0177,-0.0116,-0.0109,-0.0093,0.0266,-0.1072,-0.0558,-0.0757,-0.0703,0.0179,-0.0673,0.0184,0.0576,-0.0278,0.0514,-0.021,-0.023,-0.0217,-0.1007,0.019,-0.0355,0.0662,-0.0011,-0.0748,0.0081,-0.0368,0.0751,0.049,-0.0169,-0.0331,0.0426,0.0021,-0.0145,0.0716,-0.011,0.0018,0.0867,0.0197,0.0082,0.0093,-0.0494,-0.0494,0.0437,-0.0386,0.0319,0.0288,0.055,0.01,0.0721,-0.04,0.0206,-0.0172,-0.0129,0.047,-0.0809,0.0015,0.0182,-0.0093,-0.2865,-0.0102,0.0315,0.0232,-0.018,0.0211,0.0389,0.0095,-0.0399,-0.0399,0.0181,0.0389,0.0547,-0.0008,0.0507,0.0623,0.0847,-0.0659,0.0616,-0.0391,0.0031,0.0651,0.2235,-0.0044,-0.0102,-0.0171,-0.0446,-0.0038,0.046,-0.0176,-0.0253,0.0403,0.1033,-0.0116,0.0329,0.0608,-0.0625,0.0348,0.0012,-0.0278,0.0029,0.0119,-0.0054,-0.0235,0.0736,-0.0126,0.0558,-0.0148,-0.0385,0.0183,0.0036,-0.0052,0.0248,0.0016,0.0101,0.0378,-0.0598,-0.071,-0.021,-0.0208,0.0269,-0.0619,-0.047,-0.0234,-0.0303]}
{"key":"[GUIGAN: Learning to Generate GUI Designs Using Generative Adversarial Networks] Graphical User Interface (GUI) is ubiquitous in almost all modern desktop software, mobile applications, and online websites. A good GUI design is crucial to the success of the software in the market, but designing a good GUI which requires much innovation and creativity is difficult even to well-trained designers. Besides, the requirement of the rapid development of GUI design also aggravates designers' working load. So, the availability of various automated generated GUIs can help enhance the design personalization and specialization as they can cater to the taste of different designers. To assist designers, we develop a model GUIGAN to automatically generate GUI designs. Different from conventional image generation models based on image pixels, our GUIGAN is to reuse GUI components collected from existing mobile app GUIs for composing a new design that is similar to natural-language generation. Our GUIGAN is based on SeqGAN by modeling the GUI component style compatibility and GUI structure. The evaluation demonstrates that our model significantly outperforms the best of the baseline methods by 30.77% in Frechet Inception distance (FID) and 12.35% in 1-Nearest Neighbor Accuracy (1-NNA). Through a pilot user study, we provide initial evidence of the usefulness of our approach for generating acceptable brand new GUI designs.","layer":2,"vector":[-0.0579,-0.0039,0.0273,-0.0274,0.0027,0.0412,0.0117,0.0059,-0.0124,0.0034,-0.0314,-0.0396,0.0944,0.049,0.0141,-0.0096,0.0127,-0.0148,-0.0239,-0.0237,0.0547,-0.0221,0.0151,-0.074,0.0001,0.001,-0.0044,-0.0598,-0.0087,-0.2098,0.0349,-0.0141,0.0533,-0.0278,-0.0307,-0.0269,-0.0584,0.048,0.0054,0.0528,0.0075,0.0453,-0.049,-0.0561,-0.0219,-0.0287,-0.0528,-0.0259,-0.0098,-0.0133,0.0291,-0.0581,0.0149,0.0358,0.013,0.0186,0.0405,0.027,-0.0023,0.0458,0.025,0.0294,-0.1688,0.1045,0.0361,-0.0051,-0.0197,-0.0238,0.011,0.0209,-0.0353,0.0421,-0.0029,0.0322,0.0104,-0.0178,0.0198,-0.0245,-0.036,0.012,-0.0119,0.0122,-0.0205,0.0132,-0.0144,0.0115,0.0122,-0.0037,0.0477,0.0369,0.0038,-0.0238,-0.0428,0.0317,-0.0437,-0.0175,0.0047,0.0027,-0.0839,0.2381,-0.0765,0.0117,0.085,-0.0296,0.0316,-0.0083,-0.0357,0.0077,-0.0508,0.0111,-0.0301,0.0132,-0.0145,0.0007,0.031,-0.0162,0.0015,0.0112,-0.0109,-0.0287,-0.0471,0.0152,0.015,-0.0534,0.0333,-0.0457,-0.0281,0.1129,0.0078,0.0312,0.036,0.018,-0.0205,0.0332,0.0319,-0.0059,0.0153,0.0184,-0.0088,0.0211,-0.0383,-0.0061,0.0159,-0.0688,-0.0164,0.1204,-0.0177,0.075,-0.0262,-0.0134,-0.0178,0.0258,-0.049,0.0066,0.0293,0.0771,0.0201,0.0822,-0.0289,0.0245,-0.0014,-0.0738,-0.0721,0.0579,0.0017,-0.105,0.0014,0.047,0.0258,0.0048,-0.0176,0.0364,-0.0543,0.0646,0.0809,0.0397,-0.0802,-0.0119,0.0176,0.0576,0.0375,-0.0344,-0.0276,0.0236,0.0016,-0.0593,0.0309,-0.0498,0.0279,0.0326,0.0072,0.0381,-0.0181,-0.0022,-0.0271,-0.0404,0.0019,-0.0285,0.0125,-0.0421,0.0179,-0.0016,-0.0438,-0.0329,-0.0279,0.0074,-0.0007,-0.0204,0.0769,0.0332,-0.0767,0.016,0.1014,0.0106,-0.0124,-0.0397,-0.0137,0.0649,-0.0007,0.0402,0.0142,-0.0548,-0.0599,-0.2523,0.0207,0.0136,-0.0231,0.0121,-0.0542,0.0155,0.0121,0.0345,0.0026,0.0311,-0.0135,0.0028,0.0515,-0.0289,0.0172,-0.0161,0.0236,-0.0319,-0.0571,0.0033,0.0296,0.0162,-0.1209,0.0003,0.0001,0.2276,0.0391,0.0467,-0.0015,0.0769,0.0377,-0.0261,-0.1085,0.0243,0.0824,0.0529,-0.0037,-0.0439,-0.0111,-0.0235,0.0408,0.0167,-0.1152,-0.0194,-0.0561,-0.0329,-0.0249,-0.0646,0.0369,0.0087,-0.0431,0.0709,0.0185,-0.0341,-0.0787,-0.1467,0.0675,-0.0492,0.0256,0.0092,-0.0562,0.0452,-0.0686,0.0186,0.0209,-0.0381,-0.0771,0.0378,0.0038,-0.0026,0.096,-0.0169,0.0041,0.0623,-0.0084,0.0451,-0.0317,-0.0585,-0.0417,0.0041,-0.0051,0.0813,0.0251,0.0228,-0.0016,0.0602,-0.0115,0.0099,-0.0646,0.0137,0.0175,-0.0537,-0.0205,0.0262,0.0072,-0.2899,0.0691,-0.0201,0.0686,-0.0431,0.0065,0.0743,0.0304,-0.0598,-0.0102,0.014,-0.0038,0.0447,-0.0489,0.0285,0.0194,0.0813,-0.031,0.0551,-0.0248,0.0034,0.0195,0.2242,-0.0597,0.0201,0.006,0.0191,-0.0056,0.0492,0.0222,0.0179,-0.0079,0.1158,0.0025,0.0319,0.0839,-0.0463,-0.0051,-0.0033,-0.0391,-0.0487,0.0572,-0.0345,-0.015,0.0627,-0.0302,0.0064,0.008,-0.0142,0.0229,-0.0295,-0.0164,-0.0385,0.0024,0.0417,0.0206,-0.0147,-0.0128,-0.0204,0.0001,0.0075,-0.0334,0.0039,0.0115,-0.0161]}
{"key":"[Fast Estimation of Information Theoretic Learning Descriptors using Explicit Inner Product Spaces] Kernel methods form a theoretically-grounded, powerful and versatile framework to solve nonlinear problems in signal processing and machine learning. The standard approach relies on the \\emph{kernel trick} to perform pairwise evaluations of a kernel function, leading to scalability issues for large datasets due to its linear and superlinear growth with respect to the training data. Recently, we proposed \\emph{no-trick} (NT) kernel adaptive filtering (KAF) that leverages explicit feature space mappings using data-independent basis with constant complexity. The inner product defined by the feature mapping corresponds to a positive-definite finite-rank kernel that induces a finite-dimensional reproducing kernel Hilbert space (RKHS). Information theoretic learning (ITL) is a framework where information theory descriptors based on non-parametric estimator of Renyi entropy replace conventional second-order statistics for the design of adaptive systems. An RKHS for ITL defined on a space of probability density functions simplifies statistical inference for supervised or unsupervised learning. ITL criteria take into account the higher-order statistical behavior of the systems and signals as desired. However, this comes at a cost of increased computational complexity. In this paper, we extend the NT kernel concept to ITL for improved information extraction from the signal without compromising scalability. Specifically, we focus on a family of fast, scalable, and accurate estimators for ITL using explicit inner product space (EIPS) kernels. We demonstrate the superior performance of EIPS-ITL estimators and combined NT-KAF using EIPS-ITL cost functions through experiments.","layer":0,"vector":[-0.0466,-0.0555,0.047,-0.0363,0.0518,0.013,0.0649,0.0617,0.038,-0.0081,0.0202,-0.0636,0.0449,0.0006,0.0324,0.0211,0.055,0.0488,-0.0511,-0.0255,0.0724,-0.0292,-0.0411,-0.0293,0.0304,0.0086,0.0001,-0.0164,-0.0387,-0.2603,0.0296,-0.0533,0.0756,0.0061,0.0083,-0.0572,-0.0468,0.0765,-0.0388,0.0228,-0.0092,0.0094,-0.0382,-0.0429,-0.0269,-0.0891,-0.0359,-0.0511,-0.028,-0.027,0.0186,-0.0191,0.048,0.0182,0.0797,0.0213,0.0425,0.0227,0.0521,0.0388,0.0304,0.0407,-0.1747,0.0648,0.0269,0.0162,-0.0465,-0.0542,0.0445,0.0289,-0.0204,0.0832,-0.0006,0.0271,-0.0087,-0.0326,-0.0025,0.0015,0.0022,0.0222,0.0203,-0.0369,-0.0622,-0.0248,-0.0325,-0.0737,-0.0009,-0.0313,0.0807,0.0064,-0.0839,-0.0241,-0.0553,-0.0019,-0.0376,-0.0144,-0.0217,0.0091,-0.0271,0.1882,-0.0142,0.0721,0.0204,-0.046,0.0307,-0.0877,-0.0118,-0.0098,-0.0463,-0.0148,-0.0103,-0.0226,-0.0034,-0.0323,0.0332,-0.0093,0.0748,0.0264,0.0185,-0.0255,-0.0248,0.0038,0.0249,0.0286,0.018,-0.0416,-0.0195,0.1312,0.0405,0.0512,0.0433,-0.0394,-0.0731,-0.0068,-0.0278,0.0554,0.0373,0.0436,0.0219,-0.0038,-0.0033,-0.062,0.0313,-0.0498,-0.0772,0.126,-0.0271,0.0209,-0.0632,0.0094,-0.0037,0.0344,-0.016,-0.052,0.0414,0.0033,-0.009,0.0347,-0.037,0.0509,-0.0594,-0.0421,0.0113,0.088,0.0012,-0.0612,-0.0451,0.0173,0.0295,-0.0099,0.0489,0.0216,-0.0309,-0.0017,0.0898,0.0239,-0.0856,-0.0172,0.0381,0.014,0.0031,-0.065,-0.0143,0.0212,0.0449,-0.0432,0.0018,0.0123,0.0419,0.044,-0.0193,0.0072,-0.0015,-0.0126,-0.0342,-0.0419,0.004,-0.0194,0.0195,-0.038,0.018,-0.0158,-0.051,0.0671,-0.0237,0.019,-0.0304,-0.0298,0.0283,-0.0019,0.0128,-0.0241,0.0552,-0.0077,-0.0061,-0.004,-0.0161,0.028,-0.0039,0.0309,0.0363,-0.046,-0.0852,-0.239,-0.0224,0.0115,-0.0076,0.0406,-0.0653,0.0893,-0.012,0.0689,0.1108,0.0254,0.024,-0.0347,0.0621,0.016,0.0529,0.0813,0.008,-0.005,-0.0054,0.0278,0.0442,0.0016,-0.0873,0.0522,-0.0544,0.1806,-0.0173,0.0502,-0.0197,0.0235,0.0825,-0.0055,-0.0572,0.0604,0.0421,0.0527,-0.0101,-0.0428,-0.029,0.0221,-0.0403,0.0101,-0.0642,-0.0517,-0.0278,-0.022,0.0387,-0.0553,-0.0007,0.0581,-0.012,0.0663,-0.0544,-0.0053,-0.0508,-0.0418,0.0048,-0.0214,0.0384,0.0177,-0.0602,0.0144,-0.0398,0.0525,0.0001,-0.0179,-0.0295,0.0344,-0.0256,-0.0581,0.0769,0.0291,0.0203,0.056,-0.0103,0.0114,-0.044,-0.0367,0.0018,0.0735,-0.0184,0.0332,0.0242,0.0099,0.0034,0.0983,-0.0375,0.0007,-0.0295,-0.0232,0.0357,-0.0621,-0.021,0.0667,-0.0235,-0.3173,-0.019,-0.0076,0.0046,-0.0159,-0.007,-0.0068,-0.0002,-0.035,0.0025,-0.0347,0.0498,0.001,0.0022,0.0407,0.0693,0.0663,-0.0669,0.053,-0.0326,0.0314,0.0441,0.1901,0.0039,0.0181,-0.0038,-0.0701,-0.0161,0.0088,-0.0134,0.0359,-0.0207,0.0482,-0.0249,0.0189,0.0658,-0.0035,0.0379,-0.0066,-0.0291,0.0368,0.001,-0.0384,-0.033,0.0765,-0.0244,-0.0224,-0.036,-0.0261,0.0049,0.0044,0.0322,0.0193,0.0523,0.0627,0.0236,-0.0767,-0.0436,-0.0485,-0.0692,0.0304,-0.0601,-0.0425,0.0137,0.0206]}
{"key":"[Ensemble Hyperspectral Band Selection for Detecting Nitrogen Status in Grape Leaves] The large data size and dimensionality of hyperspectral data demands complex processing and data analysis. Multispectral data do not suffer the same limitations, but are normally restricted to blue, green, red, red edge, and near infrared bands. This study aimed to identify the optimal set of spectral bands for nitrogen detection in grape leaves using ensemble feature selection on hyperspectral data from over 3,000 leaves from 150 Flame Seedless table grapevines. Six machine learning base rankers were included in the ensemble: random forest, LASSO, SelectKBest, ReliefF, SVM-RFE, and chaotic crow search algorithm (CCSA). The pipeline identified less than 0.45% of the bands as most informative about grape nitrogen status. The selected violet, yellow-orange, and shortwave infrared bands lie outside of the typical blue, green, red, red edge, and near infrared bands of commercial multispectral cameras, so the potential improvement in remote sensing of nitrogen in grapevines brought forth by a customized multispectral sensor centered at the selected bands is promising and worth further investigation. The proposed pipeline may also be used for application-specific multispectral sensor design in domains other than agriculture.","layer":2,"vector":[-0.0433,0.0242,0.0322,-0.0374,0.0867,0.026,0.0292,0.0363,0.011,0.0167,-0.0068,-0.0823,0.014,0.0379,0.0296,-0.0312,0.0445,0.0199,-0.0439,0.0256,-0.0091,0.0056,-0.0484,-0.0684,0.0603,0.0155,-0.0595,-0.027,-0.0752,-0.2389,0.0173,0.0053,0.075,-0.0429,-0.031,0.01,-0.0126,0.0174,-0.0491,0.0418,0.0183,-0.0297,-0.0199,-0.0276,-0.0277,-0.064,0.0099,0.0034,0.0058,-0.0479,0.0249,-0.049,0.0184,0.0355,0.0321,0.0508,0.0296,0.033,0.0289,0.0369,0.0822,0.0096,-0.1863,0.0481,0.0389,0.0194,-0.0616,-0.0128,0.053,0.0257,0.0028,0.0558,0.0239,0.0383,0.0183,0.0151,-0.0035,-0.0767,-0.0244,-0.0039,0.0278,0.0121,-0.0687,0.008,0.0232,-0.0284,-0.0141,-0.0353,0.0646,0.0276,-0.0397,-0.0224,-0.0265,-0.0136,-0.0839,-0.035,0.0265,-0.0077,-0.0209,0.1934,-0.0434,-0.0041,0.0088,-0.0472,0.0051,-0.032,-0.0887,-0.0244,-0.0234,0.0138,0.0392,-0.0406,0.0142,-0.0195,-0.0284,-0.0109,0.0637,0.0249,-0.016,0.0096,-0.0038,0.0137,0.0779,-0.0319,0.0255,-0.0266,-0.0016,0.1205,0.0076,0.0473,0.0577,-0.0299,-0.0319,-0.0675,-0.0127,0.0511,0.056,0.008,-0.0072,0.008,-0.0576,-0.0741,0.0411,-0.075,-0.0194,0.0938,-0.0486,0.0501,-0.0502,-0.0267,-0.0453,0.0366,-0.0324,-0.0223,0.0457,0.0592,-0.0061,0.0461,-0.0367,0.0396,-0.0376,-0.0584,0.0295,0.1075,-0.0278,-0.0908,0.0281,-0.0023,-0.0194,-0.0313,0.0367,0.0493,-0.0256,0.0329,0.1011,0.0375,-0.0301,0.02,-0.0051,-0.0134,0.0445,-0.0155,-0.0169,0.0468,0.0727,-0.013,0.0043,-0.0195,-0.0106,0.0265,-0.0526,-0.0286,-0.0177,-0.0132,-0.0227,0.0159,-0.0081,-0.0121,0.0224,-0.053,0.0475,-0.0191,-0.0217,0.0504,0.012,0.0091,0.0359,-0.0148,0.0013,-0.001,0.0258,0.0077,0.0317,-0.0106,-0.1175,-0.0032,-0.0116,0.0674,0.0136,0.057,0.0522,-0.0553,-0.1162,-0.2463,-0.027,0.0536,0.036,0.0707,-0.0005,0.0377,-0.0053,0.064,0.0747,0.0682,0.015,-0.0151,0.0231,-0.0054,0.039,0.0496,0.0408,-0.0297,0.0049,0.0044,0.0127,-0.005,-0.0706,0.0238,-0.016,0.1374,0.0117,0.0129,-0.0356,0.0286,0.0189,-0.0563,-0.0761,0.0286,-0.0025,0.032,0.0054,-0.0891,-0.0348,0.0558,0.0514,-0.0284,-0.1177,-0.0344,-0.0466,-0.0081,0.0273,-0.0446,-0.0058,0.0431,0.03,0.0391,0.0008,0.0468,-0.0528,-0.102,0.0416,-0.0231,0.0121,0.0004,-0.075,0.0069,-0.0707,0.0747,0.0296,-0.0077,-0.0106,0.0656,-0.0287,-0.0159,0.0979,-0.0198,-0.032,0.0711,0.0406,0.0161,-0.0404,-0.0113,-0.033,0.0807,-0.0414,0.0224,0.0106,0.0087,0.0403,0.087,-0.0281,0.0162,-0.0311,0.0473,0.0226,-0.0611,-0.0217,0.0878,0.0119,-0.29,0.0315,-0.0181,0.014,-0.0099,0.0296,-0.012,0.0152,-0.0528,0.0032,0.0033,0.02,0.0492,-0.0094,0.0058,0.0353,0.0276,-0.0556,0.0215,-0.0553,0.0242,0.0321,0.2136,-0.0598,-0.0025,0.0426,-0.0428,-0.0055,-0.0133,-0.0401,0.0865,0.0193,0.0909,-0.0554,0.0488,0.0562,0.0202,-0.0126,-0.0181,-0.0347,0.0085,-0.0027,-0.0573,-0.0416,0.1008,-0.0249,-0.0237,-0.0113,-0.0063,0.0594,-0.0431,0.0132,-0.0691,0.001,-0.0179,-0.0037,-0.0855,-0.0365,-0.0266,-0.0303,0.0397,-0.0405,-0.025,0.001,-0.0249]}
{"key":"[Federated Self-supervised Learning for Heterogeneous Clients] Federated Learning has become an important learning paradigm due to its privacy and computational benefits. As the field advances, two key challenges that still remain to be addressed are: (1) system heterogeneity - variability in the compute and/or data resources present on each client, and (2) lack of labeled data in certain federated settings. Several recent developments have tried to overcome these challenges independently. In this work, we propose a unified and systematic framework, \\emph{Heterogeneous Self-supervised Federated Learning} (Hetero-SSFL) for enabling self-supervised learning with federation on heterogeneous clients. The proposed framework allows collaborative representation learning across all the clients without imposing architectural constraints or requiring presence of labeled data. The key idea in Hetero-SSFL is to let each client train its unique self-supervised model and enable the joint learning across clients by aligning the lower dimensional representations on a common dataset. The entire training procedure could be viewed as self and peer-supervised as both the local training and the alignment procedures do not require presence of any labeled data. As in conventional self-supervised learning, the obtained client models are task independent and can be used for varied end-tasks. We provide a convergence guarantee of the proposed framework for non-convex objectives in heterogeneous settings and also empirically demonstrate that our proposed approach outperforms the state of the art methods by a significant margin.","layer":10,"vector":[0.0033,-0.0545,-0.0161,-0.0633,0.0296,0.0296,0.0122,0.0355,0.0239,-0.0487,0.0204,-0.0363,0.0224,0.0927,0.005,0.0215,-0.0037,0.0485,-0.0284,-0.0163,-0.0099,-0.0662,-0.054,-0.0233,0.0048,0.0245,-0.0596,-0.0363,-0.0465,-0.2256,0.0366,-0.0657,0.0087,0.0035,0.0357,-0.0382,0.0014,0.0592,-0.0484,0.0272,0.0135,0.0275,-0.0166,0.0101,-0.029,-0.0555,-0.0209,-0.018,-0.0603,0.0035,0.0489,-0.0118,0.0056,0.0467,-0.0109,0.0836,0.0484,0.0388,0.0389,-0.0177,0.0163,0.0609,-0.1735,0.0663,0.0244,0.0528,-0.0484,-0.0087,0.0146,0.0459,0.0018,0.0742,0.0114,-0.008,0.0115,-0.0028,0.003,0.0182,0.007,0.03,-0.0192,-0.0188,-0.0654,-0.0275,-0.0276,-0.0079,0.0005,-0.0518,0.0326,0.031,-0.036,-0.0049,0.0161,0.0269,-0.0365,0.0155,0.0198,0.0542,-0.0606,0.1911,-0.0528,0.0486,0.0597,-0.0352,0.0181,-0.0273,-0.04,-0.0413,-0.0192,-0.001,-0.0542,-0.0154,-0.008,-0.0593,0.0296,-0.0066,0.0804,0.0174,-0.0117,0.0004,0.0285,0.0056,0.0496,-0.0245,0.0451,-0.0621,0.0279,0.1336,0.0279,0.0417,0.0103,-0.0134,-0.0197,-0.0204,0.0295,0.0874,-0.0103,-0.0246,0.024,0.0072,-0.0058,-0.0643,0.0175,-0.0692,-0.0332,0.146,0.0039,0.0427,-0.065,-0.0417,-0.0323,0.0091,-0.0504,-0.0063,0.024,0.0353,0.0501,0.033,-0.0719,-0.0119,-0.0592,-0.0373,-0.0224,0.1439,0.0217,-0.1246,-0.0236,0.0328,0.0168,-0.0458,0.0417,0.0351,-0.0127,0.0382,0.0249,0.0634,-0.0862,-0.0085,0.0107,0.0108,0.0052,-0.04,-0.0313,0.0397,-0.0211,-0.0468,0.0216,-0.0604,0.0023,0.0279,-0.0295,0.0132,-0.017,-0.0343,0.0084,-0.0242,0.0277,-0.0098,0.0127,0.0002,-0.0103,0.0399,-0.0154,0.0458,0.0106,-0.0063,0.0031,0.0085,0.03,-0.0152,-0.0302,0.0186,0.0152,0.0237,-0.0441,0.0069,0.0153,0.0985,0.0038,0.0449,0.046,-0.015,-0.058,-0.1835,-0.0066,0.0345,-0.0581,0.0083,-0.022,0.0592,0.0075,0.0281,0.1066,0.0871,-0.0207,-0.0646,0.0378,-0.0418,0.0356,0.0642,0.0522,-0.0148,0.009,-0.0112,0.0376,0.0175,-0.0675,0.0597,0.0191,0.2286,0.0105,0.0135,-0.0475,0.0036,0.0287,-0.0577,-0.1405,0.03,-0.0378,0.0544,-0.0095,-0.047,-0.0443,-0.0346,0.0054,0.0511,-0.1724,-0.0344,-0.0579,-0.0406,0.0358,-0.1111,0.0338,0.0414,-0.0577,0.0459,-0.0282,-0.018,-0.0074,-0.0241,0.0516,-0.0361,0.0125,0.003,-0.0638,0.0151,-0.0661,0.0898,-0.026,-0.0399,0.0041,0.0467,-0.046,-0.0067,0.0699,-0.0389,-0.008,0.0487,0.0059,0.0481,0.008,-0.0464,-0.0257,0.0821,-0.0479,0.0804,-0.0086,-0.0112,0.0256,0.0424,0.0394,0.0135,-0.0497,-0.0073,0.0281,-0.039,-0.0058,0.0641,0.0005,-0.2899,-0.0073,0.0073,0.0347,-0.062,0.0003,0.0672,0.0346,-0.0518,0.0146,0.0131,0.022,0.0169,0.0081,0.0106,0.0437,0.0671,-0.0413,0.046,-0.0853,0.0076,0.0476,0.2121,-0.035,0.0456,0.0285,-0.0515,0.0088,0.0425,0.0116,-0.0147,-0.0208,0.0617,-0.0382,0.028,0.0621,-0.0035,0.004,0.0174,-0.0077,0.0075,-0.0485,-0.0192,-0.0327,0.0676,0.0376,-0.0211,-0.024,-0.0228,0.0111,0.0073,-0.0173,-0.013,-0.0179,0.0317,-0.011,-0.0427,-0.0197,-0.0542,-0.0305,0.0147,-0.0224,-0.0612,-0.0357,-0.0034]}
{"key":"[Order-Planning Neural Text Generation From Structured Data] Generating texts from structured data (e.g., a table) is important for various natural language processing tasks such as question answering and dialog systems. In recent studies, researchers use neural language models and encoder-decoder frameworks for table-to-text generation. However, these neural network-based approaches do not model the order of contents during text generation. When a human writes a summary based on a given table, he or she would probably consider the content order before wording. In a biography, for example, the nationality of a person is typically mentioned before occupation in a biography. In this paper, we propose an order-planning text generation model to capture the relationship between different fields and use such relationship to make the generated text more fluent and smooth. We conducted experiments on the WikiBio dataset and achieve significantly higher performance than previous methods in terms of BLEU, ROUGE, and NIST scores.","layer":3,"vector":[-0.0192,0.0349,0.0315,-0.0383,0.0046,0.0339,-0.0004,0.0178,0.0234,-0.0286,-0.0101,-0.0231,0.0497,0.0566,-0.0032,-0.0173,0.0059,0.0282,-0.0546,0.0104,0.0347,-0.0495,-0.0236,-0.0533,0.0058,-0.0247,-0.061,-0.0405,-0.0184,-0.2171,-0.0077,-0.0479,0.058,0.0033,-0.0388,-0.0281,-0.0366,0.0639,-0.0151,0.0102,0.0209,-0.0149,-0.0314,-0.035,0.0412,-0.0132,-0.0119,-0.0181,-0.042,-0.0501,0.0002,-0.0345,-0.0139,0.0068,0.0412,0.0241,0.0423,0.0302,0.0399,0.083,0.035,0.0473,-0.1927,0.0946,0.0277,0.0582,-0.07,0.034,-0.0152,0.0274,-0.0376,0.0254,0.054,0.0938,0.0356,-0.0247,-0.007,-0.0347,0.0057,0.0225,0.0023,-0.0022,-0.0618,-0.0096,0.0044,0.0003,0.0023,-0.0408,-0.0397,0.0395,-0.0268,-0.0213,0.0113,0.0399,-0.0685,-0.0339,-0.0062,0.0184,-0.0417,0.211,-0.0375,0.0243,0.0348,-0.0423,0.0098,-0.0049,-0.0253,-0.0265,-0.0453,-0.0068,-0.0199,-0.0027,-0.0142,-0.0258,0.031,0.0268,0.1188,0.0228,-0.0473,-0.0225,-0.0048,0.0565,0.0212,-0.03,0.038,-0.0502,0.0714,0.1242,0.0521,0.0382,0.0348,-0.004,-0.0743,-0.0293,-0.0164,-0.0015,-0.0044,-0.0201,0.002,-0.0262,-0.0117,-0.0607,-0.0016,-0.0572,-0.0806,0.1408,-0.033,0.0219,-0.0725,-0.0343,-0.0302,0.0107,0.0218,-0.0798,0.006,0.0377,0.0452,0.0474,-0.0469,-0.0442,0.0213,-0.0618,-0.0616,0.1171,0.0193,-0.0948,-0.0403,0.0371,0.0433,-0.0365,0.0663,-0.0107,-0.0422,0.0722,0.0515,0.0538,-0.0083,0.0004,0.0171,0.0331,0.0679,-0.0518,-0.0151,0.0572,0.0143,-0.0657,0.0117,-0.0236,0.0203,0.029,-0.0102,0.0473,0.0076,-0.001,-0.06,-0.0387,-0.0444,0.0039,0.026,-0.0462,-0.0103,0.0109,-0.0889,0.0242,-0.0076,-0.0156,0.0133,-0.0199,0.0956,0.0248,-0.0294,-0.0167,0.06,-0.0235,-0.0455,-0.0189,-0.0134,0.0125,0.0428,0.053,0.0041,-0.0373,-0.0131,-0.2133,0.0286,0.0031,-0.0396,0.0162,-0.0523,0.0147,-0.0151,0.0439,0.077,0.0244,-0.0315,-0.0036,0.0076,-0.0673,0.0617,0.0484,0.0033,0.0025,0.0195,0.0271,-0.007,-0.011,-0.1267,0.0057,-0.027,0.221,0.0539,0.0538,-0.0213,0.0765,0.0124,-0.0352,-0.1004,0.0566,0.0238,0.0568,-0.018,-0.0244,-0.0188,0.0044,0.0345,0.0068,-0.0736,-0.0293,-0.063,-0.0378,-0.0157,-0.0115,0.0423,0.0305,-0.0438,0.0628,0.0233,-0.0046,-0.0537,-0.1139,-0.0007,-0.0257,0.0023,0.0295,-0.022,-0.0028,-0.0267,-0.0038,0.0268,-0.0317,0.0064,0.0373,-0.0411,-0.002,0.073,-0.0277,0.0313,0.0478,0.0512,-0.0135,-0.0478,-0.0428,-0.0338,0.0513,-0.0508,0.1002,-0.0215,0.0445,0.0024,0.0643,-0.0222,0.0287,0.0326,0.0214,0.0334,-0.0654,0.0136,0.0342,-0.017,-0.2798,0.0558,-0.0221,0.0199,0.0367,0.0424,0.0366,0.016,-0.0718,0.0329,0.0002,0.0256,0.0108,-0.0655,0.0063,0.0386,0.0612,-0.0605,0.0312,-0.0665,0.0122,0.0223,0.2228,0.0025,0.0314,-0.0125,-0.0061,-0.0126,0.0544,0.0071,0.022,-0.0232,0.1123,-0.0016,0.036,0.0802,-0.0036,0.0424,0.012,-0.028,-0.0292,0.0217,-0.0606,-0.0265,0.086,0.0212,-0.0068,-0.0377,0.0132,0.0259,-0.051,0.0068,-0.0565,0.0207,0.036,-0.0053,-0.0241,-0.0388,-0.0298,-0.0288,-0.0233,-0.0735,0.0039,0.0416,-0.0247]}
{"key":"[Matrix completion and extrapolation via kernel regression] Matrix completion and extrapolation (MCEX) are dealt with here over reproducing kernel Hilbert spaces (RKHSs) in order to account for prior information present in the available data. Aiming at a faster and low-complexity solver, the task is formulated as a kernel ridge regression. The resultant MCEX algorithm can also afford online implementation, while the class of kernel functions also encompasses several existing approaches to MC with prior information. Numerical tests on synthetic and real datasets show that the novel approach performs faster than widespread methods such as alternating least squares (ALS) or stochastic gradient descent (SGD), and that the recovery error is reduced, especially when dealing with noisy data.","layer":0,"vector":[-0.0532,-0.0186,0.0556,0.0031,0.0293,0.0194,-0.006,0.0577,0.0447,-0.0206,0.0235,-0.0465,0.002,0.0068,0.0631,0.0378,0.0121,0.0829,-0.0454,-0.0322,-0.015,-0.0145,-0.0104,-0.0679,0.0228,0.0277,-0.0097,-0.0918,-0.0514,-0.2657,0.0045,-0.0335,0.08,-0.0301,0.0353,-0.0472,-0.0201,0.0367,-0.0234,0.0486,-0.0338,0.0084,0.0116,0.0051,-0.0491,-0.0521,-0.0544,-0.0248,0.0015,-0.0222,0.0164,0.0001,0.0352,0.045,0.0589,0.0246,0.0978,0.0163,0.0301,0.0347,0.0205,-0.0001,-0.1993,0.0451,0.0844,0.0081,0.0103,-0.0475,0.0282,0.0879,-0.0412,0.0398,0.0237,0.0351,-0.0037,-0.0094,0.029,-0.028,-0.0586,0.0316,0.0343,-0.024,-0.0114,-0.0188,-0.0552,-0.0151,-0.0013,-0.0537,0.0363,0.0777,-0.0348,-0.0411,-0.03,0.0251,-0.0499,0.0223,0.0276,0.0469,-0.0128,0.2261,-0.0663,0.0586,0.0421,-0.0127,-0.0003,-0.0754,-0.0455,-0.0011,-0.0087,-0.0169,-0.0071,-0.0151,0.0307,-0.0183,0.018,-0.0199,0.048,0.0556,-0.0226,0.0087,-0.0361,0.0382,0.0267,-0.0011,0.0521,-0.0624,-0.0127,0.1264,0.0532,0.0751,0.0616,-0.0347,-0.0327,-0.052,-0.0087,-0.0174,0.0009,0.0122,0.0018,0.0077,-0.045,-0.0587,0.0075,-0.0795,-0.0482,0.1259,-0.0522,0.0154,-0.0618,0.0075,0.0012,0.0362,-0.0338,0.0091,0.0316,0.0054,0.0051,-0.0116,-0.059,0.0469,-0.0483,-0.0839,-0.0404,0.121,0.0325,-0.0681,-0.007,0.024,0.0452,-0.0019,0.0348,-0.0102,-0.0152,-0.006,0.0433,0.0117,-0.0644,0.0694,-0.0021,0.0091,0.0346,-0.0584,-0.0704,0.0361,0.0178,-0.0728,0.0298,-0.0414,-0.0033,0.0139,-0.0254,0.0034,-0.0053,0.0089,-0.0365,-0.0426,-0.0581,-0.0275,0.0473,-0.0266,0.0366,-0.004,-0.0349,0.0557,-0.014,0.0356,0.0299,-0.0112,0.0169,0.0239,-0.0107,-0.0593,0.0852,-0.0367,-0.0221,0.026,0.0264,0.0593,-0.0119,0.0327,0.0136,-0.0448,-0.0649,-0.2305,-0.0144,0.0042,-0.0232,0.0318,-0.0865,0.0744,-0.0201,0.0343,0.054,0.0328,0.0094,-0.0247,0.0354,-0.0047,0.0434,0.0314,0.0019,-0.0367,-0.0037,-0.0193,-0.0009,0.0012,-0.0222,0.0599,-0.0318,0.1925,0.0299,0.0468,-0.0405,0.0018,0.0228,-0.0462,-0.073,0.0664,0.0146,0.1128,0.0279,-0.0414,-0.0486,-0.0067,0.0335,0.0188,-0.0556,-0.0019,-0.0414,-0.0463,0.0255,-0.0341,0.0237,0.0559,-0.0401,0.0622,-0.0419,-0.0003,0.0004,-0.1041,0.0037,-0.008,0.0403,0.0239,-0.0911,-0.0008,-0.0538,0.042,0.0166,0.0017,-0.0375,0.0082,-0.052,-0.0191,0.0842,-0.0025,-0.0173,0.0392,0.0061,0.0035,0.0073,-0.0337,0.0146,0.0791,-0.0301,0.0562,0.0394,0.0469,0.0386,0.0635,-0.0301,0.0097,-0.0279,-0.0234,-0.0102,-0.0459,-0.0014,0.0468,0.0025,-0.2908,-0.0043,0.0059,-0.0166,-0.0337,-0.0354,0.0203,0.0233,-0.0532,0.0032,-0.0412,0.055,0.0358,-0.0188,0.0497,0.0157,0.0842,-0.0604,0.0413,-0.0345,-0.0156,0.0283,0.2021,-0.0744,0.0246,0.0072,-0.0331,-0.0138,0.0389,-0.049,0.0384,0.0045,0.0597,-0.069,0.0472,0.0473,-0.0322,0.0577,-0.0015,0.0026,0.0245,0.0014,-0.0137,-0.0156,0.0749,-0.0024,-0.0035,-0.0443,0.003,0.0163,-0.0189,0.0274,-0.0133,0.0224,0.0414,0.0218,-0.0633,-0.0325,-0.0465,-0.0194,-0.0042,-0.0587,-0.0359,-0.0077,-0.0059]}
{"key":"[An Accelerated Variance-Reduced Conditional Gradient Sliding Algorithm for First-order and Zeroth-order Optimization] The conditional gradient algorithm (also known as the Frank-Wolfe algorithm) has recently regained popularity in the machine learning community due to its projection-free property to solve constrained problems. Although many variants of the conditional gradient algorithm have been proposed to improve performance, they depend on first-order information (gradient) to optimize. Naturally, these algorithms are unable to function properly in the field of increasingly popular zeroth-order optimization, where only zeroth-order information (function value) is available. To fill in this gap, we propose a novel Accelerated variance-Reduced Conditional gradient Sliding (ARCS) algorithm for finite-sum problems, which can use either first-order or zeroth-order information to optimize. To the best of our knowledge, ARCS is the first zeroth-order conditional gradient sliding type algorithms solving convex problems in zeroth-order optimization. In first-order optimization, the convergence results of ARCS substantially outperform previous algorithms in terms of the number of gradient query oracle. Finally we validated the superiority of ARCS by experiments on real-world datasets.","layer":1,"vector":[-0.0576,0.0137,0.0574,-0.0092,-0.0037,0.0489,-0.002,0.05,0.0523,-0.0155,0.04,-0.0569,-0.0035,0.0712,-0.0373,0.0155,0.0614,0.0604,-0.0494,0.0107,0.0593,-0.0469,-0.0286,-0.08,0.0696,-0.0035,-0.0225,-0.0364,-0.0125,-0.2776,0.0274,-0.0593,0.0253,-0.0178,-0.0009,-0.0038,-0.0312,0.0761,-0.0607,0.0377,0.0303,-0.0072,-0.0478,-0.0652,-0.0079,-0.0491,-0.0487,-0.0072,-0.0301,-0.0353,0.0339,-0.0571,0.03,0.0269,0.0479,0.0407,0.0462,0.0358,0.0344,0.0272,0.0317,0.02,-0.1576,0.0555,0.0693,0.0407,-0.0098,-0.0269,0.0301,0.0836,-0.0072,0.0437,0.0785,0.0288,0.0024,-0.0325,0.0345,-0.0068,-0.0104,-0.011,0.0121,-0.0268,-0.0482,-0.0147,-0.0268,-0.0067,-0.0055,-0.0495,0.0526,0.0018,-0.0207,-0.0341,0.0105,0.0291,-0.0099,0.0379,0.0134,-0.0021,-0.0485,0.2014,-0.0391,0.0494,0.0296,-0.0476,-0.0088,-0.0613,-0.0306,-0.0131,-0.0238,-0.0202,-0.0134,-0.0426,0.0716,-0.0326,0.011,0.0264,0.0387,0.0319,-0.0258,0.0253,-0.0247,0.0141,0.0523,0.0209,0.0046,-0.0606,0.0092,0.1316,-0.0123,0.0178,0.0463,-0.0663,-0.0113,-0.0324,0.0208,0.0242,-0.0099,0.0073,-0.01,0.0018,-0.033,-0.0369,0.0064,-0.1037,-0.035,0.1432,-0.0529,0.0257,-0.035,-0.0458,0.0154,-0.0074,-0.0152,-0.0328,-0.0021,0.0252,0.0152,0.0546,-0.0336,0.0215,-0.0547,-0.0337,-0.0235,0.0834,0.0185,-0.108,0.0084,0.0093,0.0137,-0.0314,0.0387,0.0215,-0.0588,0.0347,0.1247,0.0304,-0.0096,-0.0232,-0.0283,0.0095,0.0037,-0.0139,-0.0131,0.0726,0.0048,-0.0288,0.0386,-0.0198,0.0071,0.0406,-0.0774,-0.0428,-0.0054,0.0014,-0.0163,-0.0147,0.0182,0.0097,-0.0071,0.0261,0.0198,-0.017,-0.0721,0.0737,0.0408,-0.0168,-0.0352,-0.0653,0.0414,0.0096,-0.031,-0.0152,0.0644,-0.0404,-0.0214,-0.0059,0.0245,0.0071,-0.0087,0.1015,0.0153,0.0175,-0.0462,-0.2187,-0.0199,-0.0314,-0.005,0.0201,-0.0469,0.0391,-0.027,0.0618,0.0614,0.0843,-0.0187,-0.0287,0.0309,-0.0287,0.044,0.0329,0.0076,-0.0178,-0.0306,-0.0,0.0379,-0.0104,-0.0928,0.0709,0.0005,0.1915,0.0123,0.0667,-0.0711,0.0303,-0.0114,0.0432,-0.0591,0.0527,0.0076,0.0596,-0.0434,-0.0638,-0.0228,-0.0064,0.0492,-0.0059,-0.0951,-0.0128,-0.0401,-0.0609,0.0265,-0.063,0.0307,0.0431,-0.0104,0.0605,-0.0139,0.0164,-0.0545,-0.0872,0.0036,-0.0627,0.0315,0.001,-0.0955,-0.0176,-0.0423,0.0372,-0.0318,-0.0241,-0.0039,0.0286,-0.0382,-0.0151,0.0716,-0.0109,0.0139,0.0311,0.04,0.0347,-0.0123,-0.0509,-0.0486,0.0653,-0.0343,0.0624,0.0102,-0.003,0.0112,0.1135,-0.0227,0.0074,0.0005,-0.0216,0.0352,-0.0924,-0.0028,0.0565,-0.016,-0.262,0.0185,0.008,-0.0223,-0.0196,0.0178,0.0544,-0.0029,-0.0365,0.0348,-0.0255,0.071,0.0261,0.0003,0.0605,-0.0241,0.0425,-0.0286,0.0377,-0.0478,0.032,0.0866,0.215,-0.0012,0.0065,0.0051,0.0118,-0.0124,0.0168,-0.0326,-0.0118,-0.0457,0.0709,-0.0711,0.0276,0.0764,-0.0377,0.0477,-0.002,-0.0207,-0.0264,0.0254,-0.0508,0.0125,0.1285,0.0163,-0.0147,-0.0095,0.0008,-0.0096,-0.0536,0.0574,0.021,-0.0067,0.0274,-0.0321,-0.0533,-0.0659,-0.0373,-0.0075,0.0251,-0.0602,-0.059,-0.0028,-0.0122]}
{"key":"[FedSEAL: Semi-Supervised Federated Learning with Self-Ensemble Learning and Negative Learning] Federated learning (FL), a popular decentralized and privacy-preserving machine learning (FL) framework, has received extensive research attention in recent years. The majority of existing works focus on supervised learning (SL) problems where it is assumed that clients carry labeled datasets while the server has no data. However, in realistic scenarios, clients are often unable to label their data due to the lack of expertise and motivation while the server may host a small amount of labeled data. How to reasonably utilize the server labeled data and the clients' unlabeled data is thus of paramount practical importance. In this paper, we propose a new FL algorithm, called FedSEAL, to solve this Semi-Supervised Federated Learning (SSFL) problem. Our algorithm utilizes self-ensemble learning and complementary negative learning to enhance both the accuracy and the efficiency of clients' unsupervised learning on unlabeled data, and orchestrates the model training on both the server side and the clients' side. Our experimental results on Fashion-MNIST and CIFAR10 datasets in the SSFL setting validate the effectiveness of our method, which outperforms the state-of-the-art SSFL methods by a large margin.","layer":0,"vector":[-0.0146,-0.0377,0.0044,-0.0191,0.0408,0.0297,0.0286,0.033,0.0119,-0.0319,0.0301,-0.0527,0.0503,0.0641,-0.0049,0.0191,0.0249,0.0611,-0.0651,-0.0064,0.014,-0.0295,-0.0575,-0.0754,0.0362,0.0556,-0.0318,-0.0294,-0.0794,-0.2209,0.052,-0.0535,0.0333,0.0104,0.0529,-0.0188,-0.0532,0.0559,-0.0359,0.051,0.0138,-0.0101,-0.0197,-0.0154,0.0002,-0.0466,0.0116,-0.0117,-0.0181,-0.0141,0.0663,-0.0199,-0.0266,0.0361,-0.0202,0.0476,0.0246,0.0309,0.0319,-0.0343,0.0482,0.0409,-0.1393,0.0612,0.0441,0.0757,-0.0491,-0.0001,0.0355,0.0346,0.011,0.0491,0.0196,0.03,-0.0147,-0.0071,-0.0126,0.007,0.0032,0.0401,0.016,-0.0137,-0.0335,-0.0256,-0.0213,-0.0417,-0.0248,-0.0549,0.0537,-0.0084,-0.0421,0.0077,0.0237,0.0119,-0.0458,-0.0417,-0.0075,0.0347,-0.0711,0.2073,-0.0687,0.0657,0.0363,-0.0389,0.0311,-0.0579,-0.0235,-0.0279,-0.0265,-0.015,-0.0536,-0.0153,0.0247,-0.0781,0.0239,0.0188,0.116,0.0646,-0.0386,-0.0184,0.0156,-0.0309,0.0627,-0.0221,0.0195,-0.0564,0.0074,0.1292,0.0034,0.0159,0.0318,-0.0344,-0.0193,-0.0067,0.0189,0.0616,0.0121,-0.0067,0.0275,-0.0015,-0.0246,-0.0739,0.0168,-0.0594,-0.0267,0.1297,0.0043,0.0869,-0.0689,-0.0297,-0.0056,0.0138,-0.0162,-0.0171,0.0443,0.0262,0.0622,0.0529,-0.0422,-0.0091,-0.0429,-0.0496,0.032,0.1174,0.0497,-0.1145,-0.0179,0.0153,0.0157,-0.0382,0.022,0.0367,-0.0065,0.0328,0.066,0.0522,-0.0442,-0.0055,-0.0156,-0.0088,-0.004,-0.0225,-0.0556,0.0493,0.0014,-0.0283,0.0194,-0.0733,-0.0027,0.0578,-0.0541,0.017,0.0014,0.0026,-0.0009,-0.0124,0.0395,-0.0144,0.0142,-0.0304,-0.0062,0.0119,-0.0269,0.0338,-0.0031,0.0407,0.0038,0.0014,0.0579,-0.0131,-0.0372,0.0396,0.0318,-0.0055,-0.0712,0.0005,0.0082,0.0337,-0.0271,0.0545,0.032,0.0052,-0.0675,-0.2098,-0.0132,0.04,-0.048,0.0135,-0.0397,0.0602,0.0005,0.0311,0.1123,0.0817,-0.0194,-0.0864,0.0409,-0.0374,0.0375,0.0597,0.0548,-0.0352,0.0032,-0.0258,0.0386,0.01,-0.0723,0.0546,0.0324,0.186,0.0299,0.0161,-0.0779,0.0049,0.0204,-0.051,-0.1397,0.0413,-0.001,0.023,-0.0454,-0.0329,-0.0229,-0.0189,0.0312,0.0583,-0.1472,-0.0427,-0.0555,-0.0336,-0.0122,-0.0823,0.0652,0.0367,-0.0414,0.0552,0.0164,-0.0218,-0.0234,-0.0494,0.0319,-0.029,0.034,0.0197,-0.0394,0.0154,-0.0837,0.0371,-0.0325,-0.066,0.0137,0.0431,-0.0712,-0.0403,0.0695,0.0261,-0.004,0.0538,0.0221,0.0069,-0.0248,-0.0581,-0.0263,0.1075,-0.0106,0.051,0.006,-0.0109,0.0248,0.0732,0.0465,0.0269,-0.0405,0.0064,0.0153,-0.0505,-0.0141,0.0541,0.0148,-0.292,-0.0101,-0.0028,0.0583,-0.05,-0.0019,0.04,0.0533,-0.0394,-0.0036,0.0192,0.0577,0.0219,0.011,-0.0063,0.0203,0.0415,-0.0899,0.0492,-0.0686,0.0424,0.0416,0.2102,-0.0298,0.0234,0.051,-0.0615,0.0262,0.0181,-0.0031,-0.0168,-0.0227,0.0699,-0.0451,0.0157,0.0431,-0.0083,-0.0322,-0.004,-0.0246,-0.0108,-0.0098,-0.0157,0.0088,0.0769,0.0272,-0.0638,-0.0358,-0.0172,0.0064,0.0223,-0.0169,-0.0266,-0.0215,0.0223,0.0146,-0.0619,-0.0343,-0.0295,-0.0213,0.0017,-0.0701,-0.0583,-0.0123,-0.0103]}
